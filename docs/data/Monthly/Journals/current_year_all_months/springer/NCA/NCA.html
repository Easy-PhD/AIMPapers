<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>NCA</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="nca">NCA - 974</h2>
<ul>
<li><details>
<summary>
(2025). Estimate remaining useful life for predictive railways maintenance based on LSTM autoencoder. <em>NCA</em>, <em>37</em>(27), 22967-22978. (<a href='https://doi.org/10.1007/s00521-021-06051-1'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Nowadays, frequent maintenance and repair of mechanical equipment whose goal is to deter the suspension time of railway infrastructures are proven to be ineffectual. It also results in loss of reliability as well as consuming unnecessary means and costs, since at least half of the precautionary maintenance activities are considered redundant. Despite this spending, operators are struggling to adequately maintain their assets—resulting in unacceptably frequent delays and cancellations and low levels of satisfaction among rail users. Thanks to the increasing availability and sophistication of advanced analytics, operators have a significant opportunity to create solutions to long-standing maintenance challenges. The role of predictive maintenance and especially that of Design-Out Maintenance constitute the necessary procedure that can predict in time any hardware failures, while reducing the damage or wearing down of the overall operational equipment. This can increase the effectiveness of the railways, while significantly reducing the overall expenditure needed for the repair and maintenance of the industry’s infrastructure. This paper proposes a predictive railways maintenance strategy based on deep learning techniques. Specifically, and in order to achieve the exact remaining useful life of the railway equipment, a hybrid neural architecture of long short-term memory autoencoder network is used. The purpose of this suggested architecture is the automatic feature extraction of dynamic time series and their utilization on a prediction model, which can predict with high accuracy the remaining useful life of the railway’s mechanical equipment.},
  archive      = {J_NCA},
  author       = {Hu, Liqiang and Dai, Guoyong},
  doi          = {10.1007/s00521-021-06051-1},
  journal      = {Neural Computing and Applications},
  month        = {9},
  number       = {27},
  pages        = {22967-22978},
  shortjournal = {Neural Comput. Appl.},
  title        = {Estimate remaining useful life for predictive railways maintenance based on LSTM autoencoder},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). I-SAMARAH, an incremental constrained clustering applied to remote sensing images. <em>NCA</em>, <em>37</em>(27), 22941-22965. (<a href='https://doi.org/10.1007/s00521-025-11161-1'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Automatically extracting knowledge from diverse datasets is a valuable task that helps experts explore new types of data while reducing the time spent on manual annotations. This is particularly important for emerging fields such as emergency management and environmental monitoring. Traditional unsupervised methods often struggle to capture experts’ intuitions or integrate non-formalized knowledge. On the other hand, supervised methods typically require a substantial amount of prior knowledge to function effectively. Constrained clustering, a semi-supervised approach, addresses these challenges by allowing experts to incorporate their knowledge into the clustering process. However, it often yields suboptimal results because it is difficult for experts to provide constraints that are both informative and coherent. Building on the idea that it is easier to critique than to construct, this article introduces a novel method called I-Samarah, an incremental constrained clustering approach. This method alternates between a clustering phase, where expert-provided constraints are applied, and a critique phase, where experts provide feedback on the clustering results. Through an iterative process, the method refines the clusters, improving their alignment with expert knowledge. We empirically demonstrate the effectiveness of I-Samarah using remote sensing image time series, comparing it to other constrained clustering methods in terms of result quality and to supervised methods in terms of annotation efficiency.},
  archive      = {J_NCA},
  author       = {Lafabrègue, Baptiste and Gançarski, Pierre},
  doi          = {10.1007/s00521-025-11161-1},
  journal      = {Neural Computing and Applications},
  month        = {9},
  number       = {27},
  pages        = {22941-22965},
  shortjournal = {Neural Comput. Appl.},
  title        = {I-SAMARAH, an incremental constrained clustering applied to remote sensing images},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Online forecasting using neighbor-based incremental learning for electricity markets. <em>NCA</em>, <em>37</em>(27), 22923-22940. (<a href='https://doi.org/10.1007/s00521-024-10876-x'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Electricity market forecasting is very useful for the different actors involved in the energy sector to plan both the supply chain and market operation. Nowadays, energy demand data are data coming from smart meters and have to be processed in real-time for more efficient demand management. In addition, electricity prices data can present changes over time such as new patterns and new trends. Therefore, real-time forecasting algorithms for both demand and prices have to adapt and adjust to online data in order to provide timely and accurate responses. This work presents a new algorithm for electricity demand and prices forecasting in real-time. The proposed algorithm generates a prediction model based on the k-nearest neighbors algorithm, which is incrementally updated in an online scenario considering both changes to existing patterns and adding new detected patterns to the model. Both time-frequency and error threshold based model updates have been evaluated. Results using energy demand from 2007 to 2016 and prices data for different time periods from the Spanish electricity market are reported and compared with other benchmark algorithms.},
  archive      = {J_NCA},
  author       = {Melgar-García, L. and Gutiérrez-Avilés, D. and Rubio-Escudero, C. and Troncoso, A.},
  doi          = {10.1007/s00521-024-10876-x},
  journal      = {Neural Computing and Applications},
  month        = {9},
  number       = {27},
  pages        = {22923-22940},
  shortjournal = {Neural Comput. Appl.},
  title        = {Online forecasting using neighbor-based incremental learning for electricity markets},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multi-classification of brain tumors using proposed hybrid quantum–classical integrated neural network (HQCINN) models: Shallow and deep circuit approaches. <em>NCA</em>, <em>37</em>(27), 22891-22922. (<a href='https://doi.org/10.1007/s00521-025-11522-w'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Early detection and accurate classification of brain tumors using MRI scans are crucial for effective diagnosis and treatment planning. However, with the growing patient population and the increasing volume of MRI data, as well as limitations like noise in image data and poor resolution, accurate and rapid diagnosis becomes challenging. To address these issues, AI systems are needed to support radiologists by offering a second opinion. Recent advancements in deep learning (DL) have significantly improved MRI-based brain tumor diagnosis. Despite these improvements, challenges such as the need for higher computational power, difficulty processing large and high-resolution datasets, and limitations of classical vector space. However, quantum computing and quantum computing-based AI methods, by leveraging properties such as superposition and entanglement, have the potential to process data in parallel, handle higher-dimensional data more efficiently, and solve certain problems that classical methods struggle with, more quickly and efficiently. In this study, we proposed four different hybrid quantum–classical integrated neural network (HQCINN) models featuring various multilayer parameterized quantum circuit architectures, which we refer to as “shallow and deep circuits,” designed based on properties such as “entanglement capability, circuit loss, and the number of trainable parameters.” These models aim to distinguish between glioma, meningioma, pituitary and non-tumor classes. The performance of these models was compared to classical DL models, revealing that quantum models provide higher accuracy and lower loss values with fewer parameters. Additionally, when the HQCINN model with the best performance was applied to a brain tumor dataset consisting of CT images, it demonstrated consistent performance across different patient data distributions and imaging modalities, thereby showing strong generalization capability. These results suggest that HQCINN approaches could provide significant advantages in medical imaging tasks, particularly in complex datasets like brain tumor classification.},
  archive      = {J_NCA},
  author       = {Akpinar, Emine and Islam, Sardar M. N. and Oduncuoglu, Murat},
  doi          = {10.1007/s00521-025-11522-w},
  journal      = {Neural Computing and Applications},
  month        = {9},
  number       = {27},
  pages        = {22891-22922},
  shortjournal = {Neural Comput. Appl.},
  title        = {Multi-classification of brain tumors using proposed hybrid quantum–classical integrated neural network (HQCINN) models: Shallow and deep circuit approaches},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A lightweight convolutional neural network based on u shape structure and attention mechanism for anterior mediastinum segmentation. <em>NCA</em>, <em>37</em>(27), 22875-22889. (<a href='https://doi.org/10.1007/s00521-025-11515-9'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To automatically detect anterior mediastinum lesions (AMLs) in the anterior mediastinum (AM), an automatic segmentation model designed explicitly for AM regions in chest computed tomography (CT) scans is required. Due to the low prevalence of AML, reviewing large CT datasets retrospectively is time-consuming. Developing an artificial intelligence (AI) model to identify the AM region can help radiologists manage workloads and improve diagnostic accuracy. In this paper, we introduce a U-shaped network architecture with two attention mechanisms to maintain long-range dependencies and enhance localization. We propose a parallel multi-head self-attention (MHSA) module called wide-MHSA (W-MHSA), along with a dilated depth-wise parallel path connection (DDWPP) to support upsampling stages. Additionally, an expanding convolution block is combined with W-MHSA in the encoder to ensure a lightweight design. Our proposed model demonstrates superior segmentation performance compared to state-of-the-art networks, showing strong potential for clinical application in AM lesion detection workflows.},
  archive      = {J_NCA},
  author       = {Soleimani-Fard, Sina and Jeong, Won Gi and Ferri Ripalda, Francis and Sasani, Hasti and Choi, Younhee and Deiva, S. and Jin, Gong Yong and Ko, Seok-bum},
  doi          = {10.1007/s00521-025-11515-9},
  journal      = {Neural Computing and Applications},
  month        = {9},
  number       = {27},
  pages        = {22875-22889},
  shortjournal = {Neural Comput. Appl.},
  title        = {A lightweight convolutional neural network based on u shape structure and attention mechanism for anterior mediastinum segmentation},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Demand forecasting using KAN-RNN. <em>NCA</em>, <em>37</em>(27), 22857-22874. (<a href='https://doi.org/10.1007/s00521-025-11514-w'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In today’s highly competitive business environment, organizations continuously strive to maintain their competitiveness and achieve sustainable profit margins to support long-term growth and development. Accurate demand forecasting has become a critical tool for decision-makers, as it allows better resource allocation, inventory management, and strategic planning. Recurrent deep learning methods, which use gating mechanisms to maintain an internal state aligned with time series data, are among the most widely used approaches to improve forecast accuracy. Despite their success, these models still exhibit significant untapped potential that could be realized by rethinking the design of their gating mechanisms. To address this, we introduce a novel demand forecasting method inspired by Kolmogorov–Arnold networks (KANs), featuring a modified recurrent architecture with a restructured gating mechanism. This innovation leverages KAN principles to enhance the model’s capacity to capture intricate temporal dependencies and adapt to evolving demand patterns. Experimental evaluations demonstrate that the proposed method outperforms state-of-the-art approaches, highlighting its ability to provide more accurate and reliable demand forecasting results.},
  archive      = {J_NCA},
  author       = {Mejía-Muñoz, Jose-Manuel and Mederos, Boris and Avelar, Liliana and Díaz-Román, José David and Cruz-Mejia, Oliverio},
  doi          = {10.1007/s00521-025-11514-w},
  journal      = {Neural Computing and Applications},
  month        = {9},
  number       = {27},
  pages        = {22857-22874},
  shortjournal = {Neural Comput. Appl.},
  title        = {Demand forecasting using KAN-RNN},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Efficient deep neural unification in symbolic processing. <em>NCA</em>, <em>37</em>(27), 22827-22855. (<a href='https://doi.org/10.1007/s00521-025-11512-y'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Unification is indispensable for inferences in symbolic processing. The authors propose a neural network-based solution to perform efficient unification. Symbolic processing in conventional artificial intelligence has strong inference abilities but is not well suited for handling large amounts of ambiguous data. Contrastingly, neural networks can easily handle large amounts of ambiguous data but are not well suited for making complex inferences. Therefore, the authors realized the unification of the knowledge base, including ambiguous data, using a network that combines a memory network and recurrent neural network. The novelty of the proposed network is that matching, which is a unification process, is highly efficient and substitution, which is a unification process, is robust. The proposed network enables highly efficient matching by grouping multiple terms and processing them in a memory network. Furthermore, it can handle unknown words even during substitution because it uses a recurrent neural network to perform substitution. The experimental results show that the proposed network can achieve more efficient unification of ambiguous data than the baseline. This study combines symbolic processing and deep learning and suggests that it contributes to the realization of complex inferences from large amounts of ambiguous data, which has proven challenging in conventional research. Furthermore, the use of unification, which handles large amounts of ambiguous data, facilitates the development of inference systems with human-interactive interfaces. This allows humans to obtain inference results without knowing the representations of the knowledge base.},
  archive      = {J_NCA},
  author       = {Honda, Hiroshi and Hagiwara, Masafumi},
  doi          = {10.1007/s00521-025-11512-y},
  journal      = {Neural Computing and Applications},
  month        = {9},
  number       = {27},
  pages        = {22827-22855},
  shortjournal = {Neural Comput. Appl.},
  title        = {Efficient deep neural unification in symbolic processing},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). MB-ViT: MBConv vision transformer with time–frequency feature fusion for bearing fault diagnosis. <em>NCA</em>, <em>37</em>(27), 22801-22825. (<a href='https://doi.org/10.1007/s00521-025-11509-7'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Roller bearings play a crucial role in mechanical systems, where their operational condition directly impacts system performance and lifespan. However, detecting early-stage bearing faults during routine maintenance remains a challenge due to the cost and technical limitations of current fault diagnosis methods, often resulting in reduced accuracy. To address this issue, this paper proposes a bearing fault diagnosis method based on the MBConv Vision Transformer (MB-ViT) with frequency feature fusion. Specifically, we introduce a novel approach that transforms bearing fault signals into RGB images by combining Continuous Wavelet Transform (CWT) and Gramian Angular Summation Field (GASF) images, thereby enhancing feature representation and improving fault recognition. Additionally, recognizing the limitations of the traditional Vision Transformer (ViT) in capturing local features, we introduce the MB-Multi-Head Self-Attention (MB-MSA) module to overcome this challenge. Experimental results using data from Case Western Reserve University and Xi’an Jiaotong University show that feature fusion significantly improves fault diagnosis accuracy, while the MB-MSA module enhances both diagnostic precision and robustness. MB-ViT achieves an accuracy of 99.90 $$\%$$ in bearing fault diagnosis tasks, demonstrating its superior performance. In summary, the proposed model outperforms existing ViT-based methods, providing a promising solution for bearing fault diagnosis and supporting the advancement of next-generation industrial technologies. The code and data are available at https://github.com/viivan/MB-vit .},
  archive      = {J_NCA},
  author       = {Xiao, Gang and Yao, Junbo and Zhong, Liubing and Xiao, Zhongcheng and Lu, JiaWei},
  doi          = {10.1007/s00521-025-11509-7},
  journal      = {Neural Computing and Applications},
  month        = {9},
  number       = {27},
  pages        = {22801-22825},
  shortjournal = {Neural Comput. Appl.},
  title        = {MB-ViT: MBConv vision transformer with time–frequency feature fusion for bearing fault diagnosis},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Dependency relationship-enhanced graph convolutional network for aspect-based sentiment analysis. <em>NCA</em>, <em>37</em>(27), 22775-22800. (<a href='https://doi.org/10.1007/s00521-025-11508-8'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Aspect-based sentiment analysis task aims at predicting the sentiment polarity of a specific aspect in a sentence. Recent works have shown that attention-based and syntax-based approaches have gradually become mainstream methods. However, attention-based models may erroneously utilize unrelated context words as cues for prediction in sentences with long-range word dependency information. Besides, methods based on graph neural networks have been applied to model syntactic structure information, although great outcomes have been achieved, these methods are overly dependent on the precision of the syntactical dependency tree, which may lead to suboptimal dependencies between words and thus introduce noise. Effectively incorporating semantic relevance information and syntactic structure information remains a challenging task. To address the shortcomings referred to, we propose the dependency relationship-enhanced graph convolutional network (DREGCN) model, which utilizes a dual channel to integrate semantic relevance information and syntactic structure information. Specifically, in the syntactic channel, we preserve the original dependency tree to obtain global syntactic information, while introducing an aspect-oriented reconstruction tree to capture local syntactic information. Additionally, in contrast to previous studies where context words and aspect words were modeled separately, we propose cosine networks in the semantic channel to enhance information interaction between contexts and aspects. The experimental results show that our DREGCN model has a strong advantage on the three publicly available datasets.},
  archive      = {J_NCA},
  author       = {Tian, Xiaohui and Liu, Fang’ai and Zhuang, Xuqiang and Zhang, Yuling and Gao, Xuejian},
  doi          = {10.1007/s00521-025-11508-8},
  journal      = {Neural Computing and Applications},
  month        = {9},
  number       = {27},
  pages        = {22775-22800},
  shortjournal = {Neural Comput. Appl.},
  title        = {Dependency relationship-enhanced graph convolutional network for aspect-based sentiment analysis},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). UDA-student network and the role of pixel-space relationships in pseudo-label optimization. <em>NCA</em>, <em>37</em>(27), 22755-22773. (<a href='https://doi.org/10.1007/s00521-025-11507-9'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Due to the expensive and time-consuming nature of obtaining pixel-level annotations for real-world images in semantic segmentation, using readily available synthetic data to train models is practical. This allows the model to adapt to real-world images without needing additional annotations. This process has been explored extensively in the context of Unsupervised Domain Adaptation (UDA). Despite numerous studies proposing novel adaptation strategies, many have largely overlooked the role of the student network and the crucial impact of spatial relationships between pixels on pseudo-label generation. To address pseudo-label noise and enhance the quality of pseudo-labels, three simple yet crucial modification modules are employed: (1) Student Network Reverse-Guiding The Teacher Network: Replace low-confidence pseudo-labels generated by the teacher network at the current position with high-confidence pseudo-labels generated by the student network. (2) Pixel-Space Self-Modification: Leverage the spatial distribution characteristics between pixels by replacing unreliable low-probability labels within a specified range with high-probability labels identified by different categories of pixels within the same range. (3) Small Connected Component Elimination: Identify small connected domains within a specified range that are below a certain threshold and replace the labels within these domains with the label having the highest number of pixels in the surrounding area. In summary, the improvements introduced by these three modules increased the mIoU from GTA to Cityscapes to 76.8 and from Synthia to Cityscapes to 67.9, demonstrating the effectiveness of the proposed modules in enhancing model segmentation performance.},
  archive      = {J_NCA},
  author       = {Zhang, Hao and He, LingMin and Huo, WanLi},
  doi          = {10.1007/s00521-025-11507-9},
  journal      = {Neural Computing and Applications},
  month        = {9},
  number       = {27},
  pages        = {22755-22773},
  shortjournal = {Neural Comput. Appl.},
  title        = {UDA-student network and the role of pixel-space relationships in pseudo-label optimization},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). An ontology-based adaptive tutoring system for learning business english idioms. <em>NCA</em>, <em>37</em>(27), 22725-22753. (<a href='https://doi.org/10.1007/s00521-025-11506-w'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This research presents an intelligent tutoring system (ITS) designed for the adaptive learning of Business English idioms. It addresses the problem of limited personalization and static content delivery in traditional language learning platforms. The objective is to design a hybrid ITS that adapts to individual learner performance, knowledge level, and emotional feedback. The proposed system integrates rule-based classification, ontology-driven knowledge structuring, semantic similarity algorithms, and sentiment analysis using a BERT-based deep learning model. Learners are initially categorized using pre-assessment rules, and idioms are then recommended based on difficulty, domain relevance, and previous learning outcomes. Feedback is analyzed in real time to guide dialogue and content adaptation. The methodology was evaluated through controlled user interaction scenarios. Results indicate a 25% improvement in quiz performance and a 30% increase in learner engagement, compared to a baseline non-adaptive version. Learners received personalized recommendations, adaptive quizzes, and emotionally responsive system messages, improving motivation and retention. However, limitations include the scalability of rule-based logic, difficulties in culturally interpreting idioms, and occasional inaccuracies in sentiment detection. Future work will explore reinforcement learning for dynamic adaptation and multilingual support for broader applicability. In conclusion, the system demonstrates that combining symbolic and deep learning techniques in ITS significantly enhances the learning experience. It offers a replicable model for personalized, intelligent, and emotionally aware instruction in domain-specific language learning.},
  archive      = {J_NCA},
  author       = {Ali, Rehan S. and Abouel-Ela, Magdy and Eldakhly, Nabil M.},
  doi          = {10.1007/s00521-025-11506-w},
  journal      = {Neural Computing and Applications},
  month        = {9},
  number       = {27},
  pages        = {22725-22753},
  shortjournal = {Neural Comput. Appl.},
  title        = {An ontology-based adaptive tutoring system for learning business english idioms},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). AdversarialGait: Direction invariant gait recognition with adversarial learning. <em>NCA</em>, <em>37</em>(27), 22707-22724. (<a href='https://doi.org/10.1007/s00521-025-11505-x'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Gait recognition is an emerging biometric technology that identifies individuals based on their unique walking patterns with gait sequences and variations serving as strong biometric features. Unlike other biometric modalities, it can operate over long distances without requiring active participation from the subjects, thus having wide application in security and surveillance. The performance of gait recognition can be significantly affected by variations such as view angle, posture, clothing and occlusion. Despite the advances in deep learning, these variations still pose a challenge. Specifically, a person’s appearance differs based on walking directions, impacting the accuracy of gait recognition. Existing works primarily address appearance-level variation and do not explicitly suppress the influence of walking direction. We propose a novel approach to remove the influence of walking direction in gait recognition via an adversarial process. We introduce a three module framework—the Feature Extraction Network (FEN), Gait Recognition Network (GRN) and the Direction Estimation Network (DEN). We implement an adversarial training paradigm with walking direction as the adversarial parameter in which we simultaneously train the FEN to enhance recognition capabilities of GRN while hindering DEN’s ability to estimate the walking direction, thus learning direction irrelevant gait recognition features. We train our model on the benchmark datasets, CASIA-B and OU-MVLP. Furthermore, we provide the first experimental demonstration showing that adversarial learning actively avoids direction-related features forming more compact clusters with better inter-class separability while maintaining comparable accuracy to state-of-the-art gait recognition models.},
  archive      = {J_NCA},
  author       = {Raj, Hilton and Vishnuram, A. V. and Raman, Rahul},
  doi          = {10.1007/s00521-025-11505-x},
  journal      = {Neural Computing and Applications},
  month        = {9},
  number       = {27},
  pages        = {22707-22724},
  shortjournal = {Neural Comput. Appl.},
  title        = {AdversarialGait: Direction invariant gait recognition with adversarial learning},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Surrogate-assisted evolutionary sampling particle swarm optimization for high-dimensional expensive optimization. <em>NCA</em>, <em>37</em>(27), 22689-22705. (<a href='https://doi.org/10.1007/s00521-023-08661-3'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Surrogate-assisted evolutionary algorithms have been widely employed for solving expensive optimization problems. To address high-dimensional expensive optimization problems, we propose an evolutionary sampling-assisted particle swarm optimization method, termed ESPSO. ESPSO consists of some evolutionary sampling-assisted strategies. It first improves the initialized population with some elite samples by evolutionary sampling. Secondly, during the optimization process, the method builds a local radial basis function model using the personal historical optimal data of the population to approximate the objective function landscape. Finally, surrogate-assisted local search and surrogate-assisted trust region search are designed to find promising candidate solutions for replacing individuals in the population to accelerate the search process. Behavioral research experiments of ESPSO verified these strategies have led to improvements in the search efficiency of the algorithm in various aspects, such as initialization, population update, and optimal solution promotion. We compared ESPSO with five state-of-the-art SAEAs using 18 benchmark functions, which show that ESPSO outperforms the other compared SAEAs and get the best average ranking of 2.194.},
  archive      = {J_NCA},
  author       = {Huang, Kuihua and Zhen, Huixiang and Gong, Wenyin and Wang, Rui and Bian, Weiwei},
  doi          = {10.1007/s00521-023-08661-3},
  journal      = {Neural Computing and Applications},
  month        = {9},
  number       = {27},
  pages        = {22689-22705},
  shortjournal = {Neural Comput. Appl.},
  title        = {Surrogate-assisted evolutionary sampling particle swarm optimization for high-dimensional expensive optimization},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). LEM-PSO: A lightweight evolutionary-state-driven multiple information learning particle swarm optimization algorithm. <em>NCA</em>, <em>37</em>(27), 22667-22688. (<a href='https://doi.org/10.1007/s00521-025-11083-y'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Particle swarm optimization (PSO) has been widely used, in which each particle selects its learning sample relying on fitness information. Intuitively, fitness-based selection strategy is beneficial to optimization. However, excessive reliance on fitness information may cause premature convergence of the whole population. To solve the defects of PSO, a lightweight evolutionary-state-driven multiple information learning particle swarm optimization algorithm (LEM-PSO) is proposed. In the new proposed LEM-PSO, firstly, a lightweight multiple information learning strategy is proposed. Then, adaptive evolutionary-state adjustment mechanism is proposed. Finally, local optimum warning operation is used to help the stagnant population to jump from local optimums. The comprehensive performance of LEM-PSO is compared with seven popular PSO variants on CEC2013, CEC2017 and two engineering problems, and the results confirm the firmness of LEM-PSO.},
  archive      = {J_NCA},
  author       = {Yang, Xu and Li, Hongru},
  doi          = {10.1007/s00521-025-11083-y},
  journal      = {Neural Computing and Applications},
  month        = {9},
  number       = {27},
  pages        = {22667-22688},
  shortjournal = {Neural Comput. Appl.},
  title        = {LEM-PSO: A lightweight evolutionary-state-driven multiple information learning particle swarm optimization algorithm},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A coevolutionary artificial bee colony for training feedforword neural networks. <em>NCA</em>, <em>37</em>(27), 22649-22666. (<a href='https://doi.org/10.1007/s00521-024-10910-y'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A coevolutionary artificial bee colony (CoABC) trainer based on a hybrid encoding mode is proposed to optimize the network structure and connection weights of a single-hidden layer feedforward network (SLFN). In the proposed CoABC, an integrated population (or colony) with double subpopulations, one of which is responsible for evolution of the network structure encoded as a binary vector, and the other of which is in charge of evolution of the connection weights encoded as a real-number vector, is utilized to coordinate the evolution of two subpopulations. Two types of updating formulas for binary and continuous variables in employed bees phase and onlooker bees phase are developed to enhance the search capability of CoABC. The CoABC can self-organize the structure and weights of a SLFN. In the experiments, 22 benchmark classification datasets are employed to evaluate the proposed CoABC trainer. The results show that the CoABC based on the hybrid encoding mode can train the optimal SLFNs for classification tasks with average test accuracy of 85.12%. Also, the proposed CoABC trainer outperforms original ABC-based trainers and other compared metaheuristic trainers as well as gradient-based trainers. Compared with all other algorithms, the proposed algorithm ranks top 1 in average test accuracy.},
  archive      = {J_NCA},
  author       = {Zhang, Li and Li, Hong and Gao, Weifeng},
  doi          = {10.1007/s00521-024-10910-y},
  journal      = {Neural Computing and Applications},
  month        = {9},
  number       = {27},
  pages        = {22649-22666},
  shortjournal = {Neural Comput. Appl.},
  title        = {A coevolutionary artificial bee colony for training feedforword neural networks},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Software defect prediction based on multi-filter wrapper feature selection and deep neural network with attention mechanism. <em>NCA</em>, <em>37</em>(27), 22621-22648. (<a href='https://doi.org/10.1007/s00521-024-10902-y'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Software defect prediction (SDP) models rely on various software metrics and defect data to identify potential defects in new software modules. However, the performance of these predictive models can be negatively impacted by irrelevant, redundant metrics and the imbalanced nature of defect datasets. Additionally, the previous studies mainly use conventional machine learning (ML) techniques, but their predictive performance is not superior enough. Addressing these issues is crucial to improve the accuracy and effectiveness of SDP models. This study presents a novel approach to SDP using a multi-filter wrapper feature selection technique (MFWFS). To identify a subset of relevant and informative features, we leverage the combination of filter techniques—Information gain (IG), Chi-square (CS), and Relief-F (RF) method, and a wrapper technique—Opposition-Based Whale Optimization Algorithm (OBWOA). One-dimensional-Convolutional Neural Network (CNN) with an attention mechanism is employed to enhance the classification performance of the predictive model by efficiently integrating the selected characteristics into abstract deep semantic features. We undertake experiments on seventeen open-source software datasets on four performance measures—AUC, G-mean, F-measure, and MCC and compare the obtained results with existing state-of-the-art ML and hybrid algorithms. The experimental findings demonstrate the greater efficiency of our approach, highlighting the usefulness of the multi-filter wrapper feature selection technique and 1D-CNN with attention to SDP.},
  archive      = {J_NCA},
  author       = {Malhotra, Ruchika and Chawla, Sonali and Sharma, Anjali},
  doi          = {10.1007/s00521-024-10902-y},
  journal      = {Neural Computing and Applications},
  month        = {9},
  number       = {27},
  pages        = {22621-22648},
  shortjournal = {Neural Comput. Appl.},
  title        = {Software defect prediction based on multi-filter wrapper feature selection and deep neural network with attention mechanism},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Differential evolution using multi-strategy for the improvement of optimization performance. <em>NCA</em>, <em>37</em>(27), 22593-22620. (<a href='https://doi.org/10.1007/s00521-024-10781-3'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Differential evolution (DE) is an effective population-based optimization approach that has been widely used to deal with scientific and engineering problems. However, the performance of DE method is largely dependent on its trial vector produce strategy, namely, mutation strategy, crossover operation and its corresponding control parameters. As claimed by the ‘No free Lunch theorem’, each mutation or crossover strategy has its fatal flaws; hence, the DE method having a single operation strategy cannot solve all types of optimization problems. Therefore, we propose a novel multi-strategy DE (MS-DE) in this study. First, the proposed algorithm uses combined mutation strategies including two powerful mutation strategies and selects them in a probabilistic way. Second, an improved crossover operation is introduced to tackle the stagnation problem. When a stagnation occurs, DE employs the top p-best vector to conduct crossover operation. Third, the control parameters are tuned in novel adaptation schemes. Finally, a local search is utilized in the proposed method to accelerate the convergence. The proposed MS-DE method is examined on CEC2017 test suite, and experiment results confirm its outperformance over several state-of-the-art DE methods. Furthermore, the proposed MS-DE is applied to two constrained engineering problems. The comparison results on these two problems also demonstrate the efficiency of our proposed MS-DE.},
  archive      = {J_NCA},
  author       = {Liu, Nengxian and Luo, Jianbin and Chang, Jie and Pan, Jeng-Shyang},
  doi          = {10.1007/s00521-024-10781-3},
  journal      = {Neural Computing and Applications},
  month        = {9},
  number       = {27},
  pages        = {22593-22620},
  shortjournal = {Neural Comput. Appl.},
  title        = {Differential evolution using multi-strategy for the improvement of optimization performance},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Lung infection detection and classification using the integration of the improved grasshopper and the remora optimization approaches with improved SVM. <em>NCA</em>, <em>37</em>(27), 22573-22591. (<a href='https://doi.org/10.1007/s00521-024-10624-1'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Infectious lung diseases, such as pneumonia and COVID-19, pose significant threats to global health, with high mortality rates and substantial burdens on healthcare systems. Accurate and timely diagnosis is crucial for effective management and treatment. This study addresses the limitations of existing diagnostic methods by proposing advanced techniques based on computer-aided diagnosis systems and enhanced machine-learning algorithms. The methodology involves the development of novel algorithms for image enhancement, segmentation, feature selection, and classification. A kurtosis-based multi-thresholding grasshopper optimization algorithm is proposed for image segmentation, reducing complexity and enhancing the accuracy of lesion identification. An improved rider optimization algorithm is also introduced for feature selection, aiming to prioritize relevant features and reduce dimensionality effectively. Furthermore, an enhanced support vector machine (SVM) algorithm for lesion classification is presented, utilizing linear mapping to generate feature scores for regions of interest. This facilitates the evaluation of the loss function and improves classification results. The approach’s effectiveness is demonstrated using datasets comprising chest X-ray and CT scan images from the LIDC-IDRI and Montgomery datasets. The improved optimization algorithms were trained and tested over the chest X-ray and CT scan image datasets. An improved SVM classified the lesions with an accuracy of 99.9% for chest X-ray images and 99.8% for CT scan images. The results proved that the improved SVM adequately classifies lung diseases from the chest X-ray and CT scan images. The findings suggest that the proposed methodologies significantly enhance the accuracy and efficiency of diagnosing pneumonia and COVID-19 from medical images. By addressing the limitations of existing diagnostic techniques, this research contributes to improving healthcare practices and ultimately reducing the burden of infectious lung diseases on a global scale.},
  archive      = {J_NCA},
  author       = {Bhimavarapu, Usharani},
  doi          = {10.1007/s00521-024-10624-1},
  journal      = {Neural Computing and Applications},
  month        = {9},
  number       = {27},
  pages        = {22573-22591},
  shortjournal = {Neural Comput. Appl.},
  title        = {Lung infection detection and classification using the integration of the improved grasshopper and the remora optimization approaches with improved SVM},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Artificial hummingbird algorithm with chaotic-opposition-based population initialization for solving real-world problems. <em>NCA</em>, <em>37</em>(27), 22529-22572. (<a href='https://doi.org/10.1007/s00521-024-10621-4'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The artificial hummingbird algorithm is a global search mechanism with many applications in engineering design, but it tends to stall in high-dimensional problems with locally optimal solutions. To address this issue, this paper enhances an artificial hummingbird algorithm by using a chaos map and opposition-based method for population initialization to combat the lack of population diversity, the imbalance between exploration and exploitation, and the algorithm’s premature convergence. The randomness of chaos maps has been leveraged to prevent solutions trapped in local optima and facilitate faster convergence. Moreover, an opposition-based population can serve as a better initial solution and accelerate convergence when compared to random initialization. Two numerical test suites are used to evaluate the efficacy of the proposed algorithm: 50 benchmark functions and the CEC 2018 benchmark test suite. The outcomes are compared to eight other cutting-edge metaheuristic algorithms. Wilcoxon rank sum test, Friedman test, and mean absolute error are used to conduct additional statistical analysis on the data. Moreover, experiments are conducted on the aforementioned 57 real-world optimization problems to demonstrate the efficacy of the proposed method. The outcomes are contrasted to the algorithms SASS, MAgES, EnMODE, and COLSHADE (which won the CEC2020 Competition on Real-World Single-Objective Constrained Optimization). All quantitative and qualitative results on benchmark functions, statistical tests, as well as real-world optimization problem results demonstrate that the proposed algorithm is competitive and preferable to the metaheuristics considered in the experiments. Hence it is concluded that the proposed algorithm balances exploration and exploitation more effectively and the population initialization technique is conducive to augmenting the search capabilities of the artificial hummingbird algorithm.},
  archive      = {J_NCA},
  author       = {Kaur, Sumandeep and Kaur, Lakhwinder and Lal, Madan},
  doi          = {10.1007/s00521-024-10621-4},
  journal      = {Neural Computing and Applications},
  month        = {9},
  number       = {27},
  pages        = {22529-22572},
  shortjournal = {Neural Comput. Appl.},
  title        = {Artificial hummingbird algorithm with chaotic-opposition-based population initialization for solving real-world problems},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Deep learning at the service of metaheuristics for solving numerical optimization problems. <em>NCA</em>, <em>37</em>(27), 22493-22528. (<a href='https://doi.org/10.1007/s00521-024-10610-7'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Integrating deep learning methods into metaheuristic algorithms has gained attention for addressing design-related issues and enhancing performance. The primary objective is to improve solution quality and convergence speed within solution search spaces. This study investigates the use of deep learning methods as a generative model to learn historical content, including global best and worst solutions, solution sequences, function evaluation patterns, solution space characteristics, population modification trajectories, and movement between local and global search processes. An LSTM-based architecture is trained on dynamic optimization data collected during the metaheuristic optimization process. The trained model generates an initial solution space and is integrated into the optimization algorithms to intelligently monitor the search process during exploration and exploitation phases. The proposed deep learning-based methods are evaluated on 55 benchmark functions of varying complexities, including CEC 2017 and compared with 13 biology-based, evolution-based, and swarm-based metaheuristic algorithms. Experimental results demonstrate that all the deep learning-based optimization algorithms achieve high-quality solutions, faster convergence rates, and significant performance improvements. These findings highlight the critical role of deep learning in addressing design issues, enhancing solution quality, trajectory, and performance speed in metaheuristic algorithms.},
  archive      = {J_NCA},
  author       = {Oyelade, Olaide N. and Ezugwu, Absalom E. and Saha, Apu K. and Thieu, Nguyen V. and Gandomi, Amir H.},
  doi          = {10.1007/s00521-024-10610-7},
  journal      = {Neural Computing and Applications},
  month        = {9},
  number       = {27},
  pages        = {22493-22528},
  shortjournal = {Neural Comput. Appl.},
  title        = {Deep learning at the service of metaheuristics for solving numerical optimization problems},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). M2M-net: Multi-objective neural architecture search using dynamic M2M population decomposition. <em>NCA</em>, <em>37</em>(27), 22473-22491. (<a href='https://doi.org/10.1007/s00521-024-10595-3'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Evolutionary multi-objective neural architecture search (MO-NAS) is an efficient solution for automating the design of deep neural network architectures, aiming to explore a diverse range of objectives. However, the sensitivity of objectives in MO-NAS varies over generations, leading to a search imbalance, and local search plays a crucial role in this combinatorial optimization problem with a discrete search space. In this paper, we propose M2M-Net, a dynamic self-adaptive (Multi-objective to Multi-objective) M2M population decomposition-based evolutionary algorithm for NAS. M2M-Net leverages dynamic self-adaptive M2M population decomposition to overcome the search imbalance in MO-NAS. The subpopulation-based search within M2M-Net facilitates local search through crossover and mutation. Additionally, M2M-Net incorporates a proxy model to reduce computational cost in architecture evaluation and utilizes the channel attention mechanism to improve the accuracy of proxy model evaluation. Experimental studies on CIFAR-10 and CIFAR-100 datasets validate the effectiveness and efficiency of M2M-Net. Comparisons and analysis demonstrate that M2M-Net achieves comparable performance to state-of-the-art NAS methods while utilizing fewer computational resources.},
  archive      = {J_NCA},
  author       = {Tan, Zhiwen and Guo, Daqi and Chen, Junan and Chen, Lei},
  doi          = {10.1007/s00521-024-10595-3},
  journal      = {Neural Computing and Applications},
  month        = {9},
  number       = {27},
  pages        = {22473-22491},
  shortjournal = {Neural Comput. Appl.},
  title        = {M2M-net: Multi-objective neural architecture search using dynamic M2M population decomposition},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A hybrid algorithm considering continuous transportation for flexible job shop scheduling problem with finite transportation resources. <em>NCA</em>, <em>37</em>(27), 22451-22471. (<a href='https://doi.org/10.1007/s00521-024-10580-w'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Traditional flexible job shop scheduling ignores transportation or considers production and transportation separately. With factory digitalization and the widespread use of automated guided vehicles (AGVs), the isolated scheduling of production and transportation is no longer sufficient to meet increased productivity demands. Thus, integrated scheduling has become an inevitable option. Previous research has not sufficiently explored the domain knowledge of flexible job shop scheduling problem with finite transportation resources (FJSP-T), and thus the optimal solution cannot be found in an acceptable time using meta-heuristic algorithms. This paper explores FJSP-T to enhance the efficiency of the entire production system, and the objective is to minimize the makespan. The transportation situations of FJSP-T are analyzed, and it has been identified that the key to solving the problem is considering continuous transportation of AGVs. Further, the active decoding method and the initialization method considering continuous transportation are designed. A hybrid algorithm (HA) is proposed, which incorporates local search into the genetic algorithm, and various neighborhood structures for local search are designed. Finally, the superiority of the active decoding method is proved experimentally, and the algorithm performance is tested on two sets of famous benchmark instances (including 67 instances). Compared with other state-of-the-art reported algorithms, the proposed method obtains the new best solutions for 6 instances, and all solutions are not lower than previous results. Meanwhile, the computational time is only a few seconds. As a result, the proposed HA has significantly improved in solving FJSP-T regardless of the solution accuracy and the computational time.},
  archive      = {J_NCA},
  author       = {Wang, Qingzheng and Gao, Liang and Yu, Yanbin and Xiang, Zhimou and Yao, Youjie and Li, Xinyu and Zhou, Wei},
  doi          = {10.1007/s00521-024-10580-w},
  journal      = {Neural Computing and Applications},
  month        = {9},
  number       = {27},
  pages        = {22451-22471},
  shortjournal = {Neural Comput. Appl.},
  title        = {A hybrid algorithm considering continuous transportation for flexible job shop scheduling problem with finite transportation resources},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Empowering white shark optimizer for dimensionality reduction with case study of apple disease prediction. <em>NCA</em>, <em>37</em>(27), 22421-22449. (<a href='https://doi.org/10.1007/s00521-024-10577-5'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Feature selection (FS) plays a crucial role in developing classification models by reducing the number of features used while improving their predictive power. It is a challenging problem that can be viewed as an NP-hard optimization task. To tackle this problem, powerful wrapper-based metaheuristic algorithms are employed, as they have the ability to search for nearly optimal feature subsets in the vast search space. However, these algorithms often face challenges such as getting trapped in local optima and striking a balance between exploration and exploitation. To address these challenges, this study proposes an improved version of the white shark optimizer (WSO) called the improved WSO (IWSO). The IWSO incorporates two efficient strategies, namely opposition-based learning (OBL) and Gaussian mutation (GM), to overcome the limitations of the original method. OBL enhances exploration by considering opposite solutions, while GM prevents premature convergence and improves the exploitation capabilities of the algorithm. The effectiveness of the proposed IWSO is evaluated using various benchmark datasets and assessed using standard evaluation metrics. The results of the experiments demonstrate that the IWSO is capable of discovering new optimal solutions across different test cases. Furthermore, the proposed algorithm is applied to a real-world problem involving the identification of apple diseases, further validating its effectiveness.},
  archive      = {J_NCA},
  author       = {Sami, Aya and Barakat, Sherif I. and Mostafa, Reham R.},
  doi          = {10.1007/s00521-024-10577-5},
  journal      = {Neural Computing and Applications},
  month        = {9},
  number       = {27},
  pages        = {22421-22449},
  shortjournal = {Neural Comput. Appl.},
  title        = {Empowering white shark optimizer for dimensionality reduction with case study of apple disease prediction},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A parallel hybrid approach integrating clonal selection with artificial bee colony for logistic regression in spam email detection. <em>NCA</em>, <em>37</em>(27), 22401-22419. (<a href='https://doi.org/10.1007/s00521-024-10505-7'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Spam emails are sent to recipients for advertisement and phishing purposes. In either case, it disturbs recipients and reduces communication quality. Addressing this issue requires classifying emails on servers as either spam or ham. Numerous methods have been proposed for this classification task. Among them, logistic regression (LR) stands out for its simplicity, speed, and ease of implementation. However, LR suffers from low detection rates caused by the gradient descent algorithm used in its training phase. To overcome this limitation, we propose a novel method based on the clonal selection algorithm (CSA), renowned for its success in optimization problems due to its local and global search capabilities. Despite CSA’s effective optimization performance, it suffers from robustness and slow training time. Therefore, the CSA and artificial bee colony (ABC) algorithms are hybridized to improve CSA’s robustness and are parallelized to reduce the training time significantly. This hybrid method is employed to optimize the weights of LR by minimizing the cost at the output of LR. The empirical results denote that the proposed method, named CSA–ABC–LR, yields better classification performance compared to state-of-the-art models reported by previous studies, demonstrating an accuracy rate of 99.13% on the Enron-1 dataset, 99.22% on the CSDMC2010 dataset, and 94.49% on the Spambase dataset.},
  archive      = {J_NCA},
  author       = {Dedeturk, Bilge Kagan and Akay, Bahriye},
  doi          = {10.1007/s00521-024-10505-7},
  journal      = {Neural Computing and Applications},
  month        = {9},
  number       = {27},
  pages        = {22401-22419},
  shortjournal = {Neural Comput. Appl.},
  title        = {A parallel hybrid approach integrating clonal selection with artificial bee colony for logistic regression in spam email detection},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). An evolutionary swarm intelligence optimizer based on probabilistic distribution. <em>NCA</em>, <em>37</em>(27), 22387-22399. (<a href='https://doi.org/10.1007/s00521-023-09299-x'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this study, we propose a novel approach to balance exploitation and exploration. The proposed approach is the Evolutionary Swarm Intelligence (ESI) optimizer, which combines an exploration-biased strategy with an exploitation-biased operator. The algorithm is built based on the collective behavior of biological groups, imitating their intelligence behavior. The biological evolutionary process, inspired by genetic algorithms, is applied to every individual in the algorithm. Both swarm intelligence and genetic algorithms have been widely used in practical problems, and their reliability has been proven. ESI is characterized by both spatial group intelligence behavior and temporal biological evolution. To test the performance of ESI, we used a classic test set from IEEE CEC2017 and 22 practical problems from IEEE CEC2011. The popular training tests of the dendritic neuron model were also included in the control trials. We compared ESI with some typical swarm intelligence algorithms and classic algorithms to evaluate its performance and ability to solve practical problems. The experimental results show that ESI outperforms other algorithms in terms of basic performance and the ability to solve practical problems.},
  archive      = {J_NCA},
  author       = {Yang, Yifei and Yang, Haichuan and Li, Haotian and Tang, Zheng and Gao, Shangce},
  doi          = {10.1007/s00521-023-09299-x},
  journal      = {Neural Computing and Applications},
  month        = {9},
  number       = {27},
  pages        = {22387-22399},
  shortjournal = {Neural Comput. Appl.},
  title        = {An evolutionary swarm intelligence optimizer based on probabilistic distribution},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). An adaptive grey wolf optimization with differential evolution operator for solving the discount {0–1} knapsack problem. <em>NCA</em>, <em>37</em>(27), 22369-22385. (<a href='https://doi.org/10.1007/s00521-023-09075-x'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The discount {0–1} knapsack problem (D {0–1} KP) is a new variant of the knapsack problem. It is an NP-hard problem and also a binary optimization problem. As a new intelligent algorithm that imitates the leadership function of wolves, the grey wolf optimizer (GWO) can solve NP problems more effectively than accurate algorithms. At the same time, the GWO has fewer parameters, faster calculations, and easier implementation than other intelligent algorithms. This paper introduces a method of adaptively updating the prey position of wolves and a differential evolution operator with a scaling factor that adaptively changes according to the number of iterations, and selects which operator to use for iteration by the value of the search agent parameter. Finally, it combines the improved greedy repair operator based on D {0–1} KP to form the adaptive grey wolf optimization with differential evolution operator (de-AGWO). The experimental results of the standard test function prove that the algorithm in this paper has a significant improvement in function optimization performance. And the experimental results of D {0–1} KP shows that the proposed algorithm yields superior solution outcomes, except for unrelated datasets, and exhibits significant advantages when solving strongly correlated datasets. Finally, it is verified that more than 80% of the iterations utilize the grey wolf evolution operator, highlighting that the core of the algorithm remains the GWO.},
  archive      = {J_NCA},
  author       = {Wang, Zijian and Fang, Xi and Gao, Fei and Xie, Liang and Meng, Xianchen},
  doi          = {10.1007/s00521-023-09075-x},
  journal      = {Neural Computing and Applications},
  month        = {9},
  number       = {27},
  pages        = {22369-22385},
  shortjournal = {Neural Comput. Appl.},
  title        = {An adaptive grey wolf optimization with differential evolution operator for solving the discount {0–1} knapsack problem},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). UAV path planning in mountain areas based on a hybrid parallel compact arithmetic optimization algorithm. <em>NCA</em>, <em>37</em>(27), 22353-22368. (<a href='https://doi.org/10.1007/s00521-023-08983-2'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Unmanned Aerial Vehicle (UAV) path planning is one of the core components of its entire autonomous control system. The main challenge lies in efficiently obtaining an optimal flight route in complex environments, especially in mountain areas. To address this, we propose a novel version of arithmetic optimization algorithm (AOA), named parallel and compact AOA (PCAOA). In PCAOA, the compact technique can save the memory of UAV and shorten the calculation time, and the parallel technique can quicken the convergence speed and improve the solution accuracy. In addition, the flight path generated by PCAOA is smoothed with cubic B-spline curves, making the path suitable for a UAV. The performance of PCAOA is demonstrated on 23 benchmark functions. Experimental results show that PCAOA achieves competitive results. Finally, the simulation studies are conducted to verify that PCAOA can successfully acquire a feasible and effective route in different mountain areas.},
  archive      = {J_NCA},
  author       = {Wang, Ruo-Bin and Wang, Wei-Feng and Geng, Fang-Dong and Pan, Jeng-Shyang and Chu, Shu-Chuan and Xu, Lin},
  doi          = {10.1007/s00521-023-08983-2},
  journal      = {Neural Computing and Applications},
  month        = {9},
  number       = {27},
  pages        = {22353-22368},
  shortjournal = {Neural Comput. Appl.},
  title        = {UAV path planning in mountain areas based on a hybrid parallel compact arithmetic optimization algorithm},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A modified whale optimization algorithm with multi-strategy mechanism for global optimization problems. <em>NCA</em>, <em>37</em>(27), 22339-22352. (<a href='https://doi.org/10.1007/s00521-023-08287-5'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Whale Optimization Algorithm (WOA) is an outstanding nature-inspired algorithm widely used to solve many complex engineering optimization problems. However, WOA has a poor balance in exploration and exploitation, which converges to local optimum easily. This article proposes a Modified Whale Optimization Algorithm (MWOA) with multi-strategy mechanism, which introduces the elite reverse learning strategy, nonlinear convergence factor, DE/rand/1 mutation strategy and Lévy flight disturbance strategy. MWOA can improve the convergent ability and maintain the balance of exploitation and exploration to avoid local optimum. Compared with WOA, PSO, MFO, SOA, SCA and other four WOA variants on the CEC2017 benchmark suite, MWOA has strong competitiveness and can better improve the efficiency of WOA according to the experimental results and analysis.},
  archive      = {J_NCA},
  author       = {Li, Mingyuan and Yu, Xiaobing and Fu, Bingbing and Wang, Xuming},
  doi          = {10.1007/s00521-023-08287-5},
  journal      = {Neural Computing and Applications},
  month        = {9},
  number       = {27},
  pages        = {22339-22352},
  shortjournal = {Neural Comput. Appl.},
  title        = {A modified whale optimization algorithm with multi-strategy mechanism for global optimization problems},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Single-shot policy explanation to improve task performance via semantic reward coaching. <em>NCA</em>, <em>37</em>(26), 22315-22337. (<a href='https://doi.org/10.1007/s00521-025-11477-y'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Communication is crucial for synchronizing expectations and knowledge within teams. For robots to effectively collaborate with or provide actionable decision-support or coaching to humans, it is critical that they be able to generate intelligible explanations to reconcile differences between their understanding of the world and that of their collaborators. In this work we present Single-shot Policy Elicitation for Augmenting Rewards (SPEAR), a novel sequential optimization algorithm that uses semantic explanations derived from combinations of planning predicates to augment human agents’ reward functions, driving their policies to exhibit more optimal behavior by modeling humans as reinforcement learning (RL) agents and reconciling disparities in their reward function. We present an experimental validation of the policy manipulation capabilities of SPEAR in a practically grounded application and a performance analysis of SPEAR across a suite of domains with increasingly complex state spaces and predicate counts. SPEAR demonstrates substantial improvements in runtime and addressable problem size, enabling an expert agent to leverage its own expertise to communicate actionable information to improve human performance. Through a series of human subjects studies, we demonstrate SPEAR’s potential to improve human policies and reduce cognitive load, all while enhancing interpretability, task awareness, and promoting active thinking patterns among users. Finally, we apply SPEAR in a robot-to-robot policy manipulation scenario, showcasing its applicability in robot-robot collaborations.},
  archive      = {J_NCA},
  author       = {Tabrez, Aaquib and Leonard, Ryan and Hayes, Bradley},
  doi          = {10.1007/s00521-025-11477-y},
  journal      = {Neural Computing and Applications},
  month        = {9},
  number       = {26},
  pages        = {22315-22337},
  shortjournal = {Neural Comput. Appl.},
  title        = {Single-shot policy explanation to improve task performance via semantic reward coaching},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Predicting early ASD traits of adults and toddlers using machine learning and deep learning with explainable AI and optimization. <em>NCA</em>, <em>37</em>(26), 22287-22314. (<a href='https://doi.org/10.1007/s00521-025-11064-1'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Autism spectrum disorder (ASD) is a complex neurodevelopmental condition characterized by challenges in social interaction, communication difficulties, repetitive behaviors, and a range of strengths and differences in cognitive abilities. Early ASD diagnosis using machine learning and deep learning techniques is crucial for preventing its severity and long-term effects. The articles published in this area have only applied different machine learning algorithms, and a notable gap observed is the absence of an in-depth analysis in terms of hyperparameter tuning and the type of dataset used in this context. This study investigated predictive modeling for ASD traits by leveraging two distinct datasets: (i) a raw CSV dataset with tabular data and (ii) an image dataset with facial expression. This study aims to conduct an in-depth analysis of ASD trait prediction in adults and toddlers by doing hyper optimized and interpreting the result through explainable AI. In the CSV dataset, a comprehensive exploration of machine learning and deep learning algorithms, including decision trees, Naive Bayes, random forests, support vector machines (SVM), k-nearest neighbors (KNN), logistic regression, XGBoost, and ANN, was conducted. XGBoost emerged as the most effective machine learning algorithm, achieving an accuracy of 96.13%. The deep learning ANN model outperformed the traditional machine learning algorithms with an accuracy of 99%. Additionally, an ensemble model combining a decision tree, random forest, SVM, KNN, and logistic regression demonstrated superior performance, yielding an accuracy of 96.67%. The XGBoost model, utilized in hyperparameter optimization for CSV data, exhibited a substantial accuracy increase, reaching 98%. For the image dataset, advanced deep learning models, such as ResNet50, VGG16, Boosting, and Bagging, were employed. The bagging model outperformed the others, achieving an impressive accuracy of 99%. Subsequent hyperparameter optimization was conducted on both the CSV and image datasets. Similarly, the boosting model for the image dataset achieved remarkable 100% accuracy post-tuning. As a further contribution, explainable AI has been incorporated into the study through SHAP (Shapley Additive exPlanations) analysis, providing insights into feature importance for both image and CSV data.},
  archive      = {J_NCA},
  author       = {Rahman, Md. Ashiqur and Hossain, Md. Mamun and Singh, Sondip Poul and Sharmin, Nusrat},
  doi          = {10.1007/s00521-025-11064-1},
  journal      = {Neural Computing and Applications},
  month        = {9},
  number       = {26},
  pages        = {22287-22314},
  shortjournal = {Neural Comput. Appl.},
  title        = {Predicting early ASD traits of adults and toddlers using machine learning and deep learning with explainable AI and optimization},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Experiential explanations for reinforcement learning. <em>NCA</em>, <em>37</em>(26), 22255-22285. (<a href='https://doi.org/10.1007/s00521-024-10951-3'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Reinforcement learning (RL) systems can be complex and non-interpretable, making it challenging for non-AI experts to understand or intervene in their decisions. This is due in part to the sequential nature of RL in which actions are chosen because of their likelihood of obtaining future rewards. However, RL agents discard the qualitative features of their training, making it difficult to recover user-understandable information for “why” an action is chosen. We propose a technique Experiential Explanations to generate counterfactual explanations by training influence predictors along with the RL policy. Influence predictors are models that learn how different sources of reward affect the agent in different states, thus restoring information about how the policy reflects the environment. Two human evaluation studies revealed that participants presented with Experiential Explanations were better able to correctly guess what an agent would do than those presented with other standard types of explanation. Participants also found that Experiential Explanations are more understandable, satisfying, complete, useful, and accurate. Qualitative analysis provides information on the factors of Experiential Explanations that are most useful and the desired characteristics that participants seek from the explanations.},
  archive      = {J_NCA},
  author       = {Alabdulkarim, Amal and Singh, Madhuri and Mansi, Gennie and Hall, Kaely and Ehsan, Upol and Riedl, Mark O.},
  doi          = {10.1007/s00521-024-10951-3},
  journal      = {Neural Computing and Applications},
  month        = {9},
  number       = {26},
  pages        = {22255-22285},
  shortjournal = {Neural Comput. Appl.},
  title        = {Experiential explanations for reinforcement learning},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Survey on deep learning in multimodal medical imaging for cancer detection. <em>NCA</em>, <em>37</em>(26), 22239-22254. (<a href='https://doi.org/10.1007/s00521-023-09214-4'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The task of multimodal cancer detection is to determine the locations and categories of lesions by using different imaging techniques, which is one of the key research methods for cancer diagnosis. Recently, deep learning-based object detection has made significant developments due to its strength in semantic feature extraction and nonlinear function fitting. However, multimodal cancer detection remains challenging due to morphological differences in lesions, interpatient variability, difficulty in annotation, and imaging artifacts. In this survey, we mainly investigate over 150 papers in recent years with respect to multimodal cancer detection using deep learning, with a focus on datasets and solutions to various challenges such as data annotation, variance between classes, small-scale lesions, and occlusion. We also provide an overview of the advantages and drawbacks of each approach. Finally, we discuss the current scope of work and provide directions for the future development of multimodal cancer detection.},
  archive      = {J_NCA},
  author       = {Tian, Yan and Xu, Zhaocheng and Ma, Yujun and Ding, Weiping and Wang, Ruili and Gao, Zhihong and Cheng, Guohua and He, Linyang and Zhao, Xuran},
  doi          = {10.1007/s00521-023-09214-4},
  journal      = {Neural Computing and Applications},
  month        = {9},
  number       = {26},
  pages        = {22239-22254},
  shortjournal = {Neural Comput. Appl.},
  title        = {Survey on deep learning in multimodal medical imaging for cancer detection},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Characterization of the algae growth on the RTV-coated insulator surface based on the corrosion expansion and the image segmentation algorithm. <em>NCA</em>, <em>37</em>(26), 22229-22238. (<a href='https://doi.org/10.1007/s00521-024-10728-8'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Image recognition technology for RTV (room temperature vulcanized)-coated silicone rubber insulators with algae fouling is gradually gaining popularity due to time-consuming and laborious nature of manual inspection. However, there is still an issue with the accuracy of image recognition. In this paper, a corrosion and expansion algorithm is applied for digital image processing to accurately characterize algae growth on silicone rubber insulator surfaces. After pre-processing, two common indexes are used: EXG (excess green index) and HVI (hague vegetation index). The effect of image segmentation is studied, and then the characteristic green value is used to measure algae greenness. The method for measuring algae cell density is also described. By calculating the characteristic green value of collected insulator images, the corresponding algae cell density is determined, and the algae growth degree is characterized along with the algae coverage rate. Therefore, the erosion and dilation algorithm is an essential part of image pre-processing. Based on the above work, the relationship curve between the characteristic green value and algae cell density is accurately obtained. The research results are of great significance for characterization of algae pollution on insulator surfaces using image methods.},
  archive      = {J_NCA},
  author       = {Yang, Shifang and Zhang, Zexuan and Chen, Tianyu and Liu, Yunpeng and Jia, Zhidong and Xie, Jun},
  doi          = {10.1007/s00521-024-10728-8},
  journal      = {Neural Computing and Applications},
  month        = {9},
  number       = {26},
  pages        = {22229-22238},
  shortjournal = {Neural Comput. Appl.},
  title        = {Characterization of the algae growth on the RTV-coated insulator surface based on the corrosion expansion and the image segmentation algorithm},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Incorporating prior knowledge for domain generalization traffic flow anomaly detection. <em>NCA</em>, <em>37</em>(26), 22215-22228. (<a href='https://doi.org/10.1007/s00521-024-10632-1'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Traffic flow anomaly detection is crucial for traffic management and reducing adverse impacts. However, due to the lack of labeled information for anomaly events and the highly complex nature of road networks, practitioners find it difficult to detect anomaly information. Currently, many anomaly detection methods perform well when there is no statistical difference between training and testing data (same road), ignoring the unique traffic patterns of different road and the dependency on prior knowledge, which are difficult to dynamically adapt to the detection model. In order to effectively integrate prior knowledge and generalize this knowledge to different road for anomaly detection, we use domain-specific prior knowledge as the initial weights of convolutional kernels, fine-tuning the convolutional kernel weights using classification loss, allowing prior knowledge to adapt to extracting anomaly features. To further increase the generalization capability of features, we perturb the extracted features with Gaussian noise specific to the direction of the abnormal class during training, enabling the anomaly detection classifier to have domain generalization performance across roads with different traffic patterns. We compare the proposed model with six baseline methods on real and synthetic datasets. The results demonstrate that the proposed model can effectively detect anomalies and outperforms the baseline methods.},
  archive      = {J_NCA},
  author       = {Chen, Bo and Fang, Min and Wei, HaoJie},
  doi          = {10.1007/s00521-024-10632-1},
  journal      = {Neural Computing and Applications},
  month        = {9},
  number       = {26},
  pages        = {22215-22228},
  shortjournal = {Neural Comput. Appl.},
  title        = {Incorporating prior knowledge for domain generalization traffic flow anomaly detection},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Behaviour-based trust assessment for the internet of things systems using multi-classifier ensemble learning and Dempster–Shafer fusion. <em>NCA</em>, <em>37</em>(26), 22191-22214. (<a href='https://doi.org/10.1007/s00521-025-11273-8'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the rapid proliferation of the Internet of Things (IoT), robust trust management has become imperative to ensure security in these IoT systems. Prior machine learning approaches to IoT trust management have exhibited suboptimal performance, failing to capture the dynamic behaviour and complex discriminative features of IoT devices. To address these challenges, we design an evocative trust management scheme for user’s authentication based on Dempster–Shafer’s evidence theory, which can persuade the normal activities of IoT device systems. We establish a set of discriminating features to predict the trustworthiness of a network node by assessing its observed behaviours. These behaviours being assessed encompass several characteristics such as throughput, delay, jitter and network latency. As such, nodes that demonstrate elevated data transmission rates and have anomalous traffic patterns could potentially be categorised as untrustworthy. In addition, the existence of persistently high latency will impede trust prediction algorithms, and this will consequently alter the overall behaviour of the node, ultimately affecting the trustworthiness of the entire network. We also design a framework for the fusion of evidence based on the belief degree and reputation-based evidence to avoid misclassification resulting in evidence conflicting. The resultant fusion outcomes are transformed into category labels which serve as the prediction outcome of the multi-classifier ensemble scheme. We evaluate our proposed scheme with and without the best discriminative features on performance metrics including accuracy, precision, recall, F1-score, detection rate and false alarm rate. Comprehensive experiments on a transformed UNSW-NB15 dataset demonstrate the better performance of our proposed framework, especially in the application of evidence conflicting.},
  archive      = {J_NCA},
  author       = {Aaqib, Muhammad and Ali, Aftab and Chen, Liming and Nibouche, Omar},
  doi          = {10.1007/s00521-025-11273-8},
  journal      = {Neural Computing and Applications},
  month        = {9},
  number       = {26},
  pages        = {22191-22214},
  shortjournal = {Neural Comput. Appl.},
  title        = {Behaviour-based trust assessment for the internet of things systems using multi-classifier ensemble learning and Dempster–Shafer fusion},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). WLAN: Water leakage-aware network for water leakage identification in metro tunnels. <em>NCA</em>, <em>37</em>(26), 22179-22189. (<a href='https://doi.org/10.1007/s00521-024-10564-w'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Due to the environmental factors, tunnel linings often experience water leakage, which affects the structural safety and shorten the operation life of the tunnel. While some computer vision-based studies aim to replace manual inspections, they still encounter challenges such as limited segmentation accuracy. In this study, we introduce a Water Leakage-Aware Network (WLAN) for tunnel defects inspection, enhancing the accuracy of water leakage segmentation and mitigating estimation errors in the predicted area. Two novel modules, the Attention-Guided Feature Fusion (AGFF) and the Auxiliary Boundary Awareness (ABA), are devised to provide supplementary information for segmentation masks and improve network perception of water leakage boundaries, respectively. Experimental evaluations showcase that WLAN outperforms existing approaches, establishing a new state-of-the-art standard in tunnel water leakage segmentation.},
  archive      = {J_NCA},
  author       = {Wang, Yuliang and Huang, Kai and Sun, Lei and Gao, Jianwei and Guo, Zhiwei and Chen, Xiaohan},
  doi          = {10.1007/s00521-024-10564-w},
  journal      = {Neural Computing and Applications},
  month        = {9},
  number       = {26},
  pages        = {22179-22189},
  shortjournal = {Neural Comput. Appl.},
  title        = {WLAN: Water leakage-aware network for water leakage identification in metro tunnels},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). RSMA-assisted SHAPTINs: Secrecy performance under imperfect hardware and channel estimation errors. <em>NCA</em>, <em>37</em>(26), 22161-22178. (<a href='https://doi.org/10.1007/s00521-024-10526-2'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Satellite high aerial platform terrestrial integrated networks have become the hot topic these years, which have been regarded as the major part of the intelligence of things for future networks. During this work, we investigate the secrecy performance for rate-splitting multiple access-assisted satellite high aerial platform terrestrial integrated networks. Besides, imperfect hardware and channel estimation errors are considered in the secrecy networks. Moreover, to enhance the energy utilization efficiency, rate-splitting multiple access scheme is utilized into the considered network, which is prior to that of the non-orthogonal multiple access scheme. What's more, to enhance the satellite transmission, multiple high aerial platforms are used to forward the transmission along with multiple eavesdroppers. In addition, the direct transmission link is not considered in the secrecy networks due to the heavy fading and obstacles. Relied on the former considerations, the exact and asymptotic analysis for the secrecy performance is further obtained to confirm the rightness of the analysis. Finally, some representative Monte Carlo simulations are carried out to validate the obtained results.},
  archive      = {J_NCA},
  author       = {Feng, Zhou and Kefeng, Guo and Jian, Cheng and Sunder Ali, Khowaja and Kapal, Dev and Reddy, Gadekallu Thippa and Hussam Al, Hamadi},
  doi          = {10.1007/s00521-024-10526-2},
  journal      = {Neural Computing and Applications},
  month        = {9},
  number       = {26},
  pages        = {22161-22178},
  shortjournal = {Neural Comput. Appl.},
  title        = {RSMA-assisted SHAPTINs: Secrecy performance under imperfect hardware and channel estimation errors},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). An efficient deep learning model for early disease detection in vegetable crops. <em>NCA</em>, <em>37</em>(26), 22141-22160. (<a href='https://doi.org/10.1007/s00521-025-11179-5'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Plant diseases significantly threaten global food security and economic stability by reducing crop yields, increasing production costs, and exacerbating food shortages. Early and precise detection of plant diseases is essential for mitigating these risks. This study introduces a lightweight deep learning model for early and efficient disease detection in key vegetable crops, including eggplant, potato, tomato, and soybean. The model leverages a comprehensive dataset containing 23 classes of healthy and diseased plant images. With only 388,055 parameters and a model size of 1.48 MB, it achieves an impressive accuracy of 92.75%, outperforming 17 state-of-the-art deep learning models in terms of performance, efficiency, and size. The novelty lies in the model’s compact architecture, making it highly suitable for deployment in resource-constrained environments such as mobile and edge devices. The primary objective is to provide a scalable and cost-effective solution for early disease detection to enhance agricultural productivity. The findings underscore the model’s superiority over traditional methods and emphasize its potential for real-world applications in smart and sustainable agriculture systems.},
  archive      = {J_NCA},
  author       = {Bhola, Amit and Kumar, Prabhat},
  doi          = {10.1007/s00521-025-11179-5},
  journal      = {Neural Computing and Applications},
  month        = {9},
  number       = {26},
  pages        = {22141-22160},
  shortjournal = {Neural Comput. Appl.},
  title        = {An efficient deep learning model for early disease detection in vegetable crops},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). PreSA: An intelligent blockchain-based platform for monitoring and predicting water quality for smart aquaculture. <em>NCA</em>, <em>37</em>(26), 22129-22140. (<a href='https://doi.org/10.1007/s00521-024-10877-w'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Water quality is an important factor for the survival of most living things, such as the production process of intensive aquaculture systems. In addition, it is crucial to protect fishes from any possible catastrophe caused by pollution. In this context, it is essential to monitor, control and predict water quality to have high-quality fish farming water. In this research, machine learning (ML) and blockchain technologies offer efficient and dependable solution for smart aquaculture providing greater control, management and security. In this paper, we proposed using ML and blockchain to develop an intelligent platform collecting and predicting water pollution using trophic index (TRIX). TRIX index is a metric for assessing the trophic state of an aquatic ecosystem and understanding ecological health. Autoregressive integrated moving average (ARIMA), random forest (RF) and K-nearest neighbor (KNN) models were used to predict TRIX and to help control centers for quick decision-making and real-time interventions. Different evaluation metrics have been used to identify the best ML model. Blockchain is used to secure data and ensure alerts traceability. The evaluation confirms that RF model provides better accuracy compared to other ML models. So, this study provides a secure water quality prediction system to early detect pollution and improve water quality in aquaculture environment.},
  archive      = {J_NCA},
  author       = {Hachicha, Marwa and Ben Halima, Riadh and Frikha, Tarek},
  doi          = {10.1007/s00521-024-10877-w},
  journal      = {Neural Computing and Applications},
  month        = {9},
  number       = {26},
  pages        = {22129-22140},
  shortjournal = {Neural Comput. Appl.},
  title        = {PreSA: An intelligent blockchain-based platform for monitoring and predicting water quality for smart aquaculture},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Temporal forecasting of plant height and canopy diameter from RGB images using a CNN-based regression model for ornamental pepper plants (Capsicum spp.) growing under high-temperature stress. <em>NCA</em>, <em>37</em>(26), 22107-22128. (<a href='https://doi.org/10.1007/s00521-024-10502-w'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Being capable of accurately predicting morphological parameters of the plant weeks before achieving fruit maturation is of great importance in the production and selection of suitable ornamental pepper plants. The objective of this article is evaluating the feasibility and assessing the performance of CNN-based models using RGB images as input to forecast two morphological parameters: plant height and canopy diameter. To this end, four CNN-based models are proposed to predict these morphological parameters in four different scenarios: first, using as input a single image of the plant; second, using as input several images from different viewpoints of the plant acquired on the same date; third, using as input two images from two consecutive weeks; and fourth, using as input a set of images consisting of one image from each week up to the current date. The results show that it is possible to accurately predict both plant height and canopy diameter. The RMSE for a forecast performed 6 weeks in advance to the actual measurements was below 4.5 cm and 4.2 cm, respectively. When information from previous weeks is added to the model, better results can be achieved and as the prediction date gets closer to the assessment date the accuracy improves as well.},
  archive      = {J_NCA},
  author       = {Ruiz-Gonzalez, Ruben and do Nascimento, Antonia Maiara Marques and Santos, Marcos Bruno da Costa and Porto, Rutten Kécio Soares de Brito and Medeiros, Artur Mendes and dos Santos, Fábio Sandro and Martínez-Martínez, Víctor and Barroso, Priscila Alves},
  doi          = {10.1007/s00521-024-10502-w},
  journal      = {Neural Computing and Applications},
  month        = {9},
  number       = {26},
  pages        = {22107-22128},
  shortjournal = {Neural Comput. Appl.},
  title        = {Temporal forecasting of plant height and canopy diameter from RGB images using a CNN-based regression model for ornamental pepper plants (Capsicum spp.) growing under high-temperature stress},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Analyzing toxicity in arabic social media: A study of regional dialects, sentiments, and toxic topics on X/Twitter. <em>NCA</em>, <em>37</em>(26), 22083-22105. (<a href='https://doi.org/10.1007/s00521-025-11503-z'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Online social networks have become the most popular medium for interpersonal communication, emotional expression, and information exchange. Leveraging artificial intelligence technologies, this study aims to analyze and address the issues of toxicity and abusiveness prevalent on X (formerly known as Twitter), specifically within Arabic-speaking communities. Data for this research was collected using the X API and meticulously annotated by native Arabic speakers to create a balanced dataset of toxic and non-toxic tweets. These tweets were categorized across various topics, including politics, sports, economics, religion, technology, and more. We employed topic analysis, dialect identification, sentiment analysis, and data frequency distribution techniques to examine the dataset. Our findings indicate that topics related to politics, religion, and pornography contain the highest levels of toxicity. Additionally, dialect identification reveals significant regional variations, with the most toxic content appearing in the Libyan, Egyptian, and Yemeni dialects. Sentiment analysis shows a predominance of negative sentiments in toxic tweets. Moreover, the data frequency distribution highlights commonly used abusive terms and phrases. This study underscores the critical need to address toxicity on social media platforms across different languages and cultures, providing valuable insights into the nature and distribution of harmful content in Arabic-speaking regions.},
  archive      = {J_NCA},
  author       = {Hatem, Loay and Omar, Ahmed and Farghaly, Heba Mamdouh and Ali, Abdelmgeid A.},
  doi          = {10.1007/s00521-025-11503-z},
  journal      = {Neural Computing and Applications},
  month        = {9},
  number       = {26},
  pages        = {22083-22105},
  shortjournal = {Neural Comput. Appl.},
  title        = {Analyzing toxicity in arabic social media: A study of regional dialects, sentiments, and toxic topics on X/Twitter},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). ESMPS: An efficient stock market prediction system based on optimized and ensemble deep learning architecture. <em>NCA</em>, <em>37</em>(26), 22057-22082. (<a href='https://doi.org/10.1007/s00521-025-11502-0'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the advancements in deep learning architectures (DLAs), different application domains, including healthcare, communication, agriculture, finance, etc., get benefits in many terms. Its inclusion in stock markets in terms of prediction of closing prices can help the investors with better planning and other potential tasks such as portfolio management and decision process. However, the dynamic, unstable, and unknown patterns of stocks pose challenging situations for DLAs and increase the computational overhead. To deal with such challenges, this research work proposes an efficient stock market prediction system (ESMTS) with an ensemble DLA architecture. The convolutional neural network (CNN) in the ensemble model extracts meaningful information, such as trends and correlations in stocks. The recurrent neural network (RNN) focuses on capturing temporal dependencies and sequential patterns of stocks. This ensemble model can enhance generalization, versatility in data handling, and resilience to data shifts. Additionally, an enhanced krill herd optimization (EKH-Opt) is proposed to improve computational efficiency. The proposed model is evaluated using a small-scale and large-scale dataset of stocks from Yahoo Finances. The historical information is extracted from January 2014 to December 2023, and different technical indicators are also computed for better trend analysis and improved predictive power of the system. Different performance indicators such as mean squared error (MSE), R-squared (R2), mean squared logarithmic error (MSLE), explained variance score (EVS), and mean absolute error (MAE) are used for performance analysis and comparison. The proposed system demonstrates its effectiveness with an overall performance based on MSE of 0.0002, R2 of 0.99, MSLE of 0.00009, EVS of 0.99, and MAE of 0.0078.},
  archive      = {J_NCA},
  author       = {Singh, Shobhita and Khanna, Divya and Bhatia, B. S.},
  doi          = {10.1007/s00521-025-11502-0},
  journal      = {Neural Computing and Applications},
  month        = {9},
  number       = {26},
  pages        = {22057-22082},
  shortjournal = {Neural Comput. Appl.},
  title        = {ESMPS: An efficient stock market prediction system based on optimized and ensemble deep learning architecture},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Swarm intelligence for handling out-of-vocabulary in arabic dialect identification with different representations. <em>NCA</em>, <em>37</em>(26), 22029-22055. (<a href='https://doi.org/10.1007/s00521-025-11501-1'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the rise in popularity of social networks and programs that let users connect instantaneously, communication has become more dynamic. So, regularly occurring new words affect the quality of representation models and make spelling errors. As the natural language processing applications depend on vector representations of texts, out-of-vocabulary (OOV) terms are unfamiliar to the models and must be handled with degrading their quality. For this, we present an OOV handling approach based on four swarm intelligence techniques, ant colony optimization, chicken swarm optimization, gray wolf optimization, and particle swarm optimization. In this study, three word embedding models have been used to obtain the representation of words. The performance of the proposed methods is evaluated on three tasks, dialect identification, sentiment analysis, sarcasm detection, and the results show that the suggested methods are promising for handling OOV and demonstrated high performance in all experiments. GWO-OOV-SVM achieved a 53.43% F1-score for dialect identification, while CSO-OOV-SVM achieved 75.66% and 57.68% F1-scores for sentiment analysis and sarcasm detection respectively, exceeding other models.},
  archive      = {J_NCA},
  author       = {Sobhy, Mahmoud and AbuElAtta, Ahmed H. and El-Sawy, Ahmed A. and Nayel, Hamada},
  doi          = {10.1007/s00521-025-11501-1},
  journal      = {Neural Computing and Applications},
  month        = {9},
  number       = {26},
  pages        = {22029-22055},
  shortjournal = {Neural Comput. Appl.},
  title        = {Swarm intelligence for handling out-of-vocabulary in arabic dialect identification with different representations},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A novel GldReLU activation function with enhanced RESNET50 for classification of X-ray images. <em>NCA</em>, <em>37</em>(26), 21997-22028. (<a href='https://doi.org/10.1007/s00521-025-11500-2'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {GldReLU, as a novel activation function, is proposed in this research work. The number of activation functions have been investigated for Deep Learning (DL) models to improve the performance. The Rectified Linear Unit (ReLU) is a widely used activation function. Although a number of alternatives to ReLU have been used to enhance training performance and stability and to understand the ways in which ReLU interacts with different optimization strategies, weight initialization techniques, and network architectures, we propose a novel activation function, GldReLU. By scaling the ReLU function with the Golden Ratio phi (φ). The ReLU can be replaced with GldReLU in DL models. The proposed GldReLU is investigated with Residual Network (RESNET50), Visual Geometry Group (VGG16) and a customized Artificial Neural Network (ANN) model, and the outcomes are compared with every ReLU variant. The experiment is conducted with 118 real-time Chest X-ray images and 10,166 images from Kaggle for multi-classification. Furthermore more the experiment is conducted with 225 Teeth root X-ray images for binary classification. The Caltech-101 dataset is used and the results are compared with ReLU and GldReLU. Then for the text data Pima Indian Diabetes Dataset has been utilized. The comparison is done with the benchmark dataset, Cifar10. The findings demonstrated that accuracy, GldReLU outperforms ReLU and its variants.Cifar10 dataset with RESNET50 was 83% of accuracy and with modified RESNET50, is of 87%.The accuracy for the classification of real-time Chest X-ray images in RESNET50 is 85% and in modified RESNET50 is 89%. The Caltech-101 dataset is also taken for evaluation. The test accuracy improvement was 10% in GldReLU when compared with ReLU in the network. Besides this, the accuracy of the customized Artificial Neural Network (ANN) for the Pima Indian Diabetes Dataset with four hidden layers and ReLU AF is 71%, while the accuracy with GldReLU is 82%. Thus, the proposed novel GldReLU outperforms in the model with all datasets.},
  archive      = {J_NCA},
  author       = {Lakshmi, P. Pankaja and Sivagami, M.},
  doi          = {10.1007/s00521-025-11500-2},
  journal      = {Neural Computing and Applications},
  month        = {9},
  number       = {26},
  pages        = {21997-22028},
  shortjournal = {Neural Comput. Appl.},
  title        = {A novel GldReLU activation function with enhanced RESNET50 for classification of X-ray images},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). An evaluation of a pre-trained transformer-based self-distillation model (DINOv2) for cross-domain plant species identification. <em>NCA</em>, <em>37</em>(26), 21969-21995. (<a href='https://doi.org/10.1007/s00521-025-11499-6'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Plant species identification is a fundamental process in botany and agriculture sector. In recent years, deep neural networks have become the primary approach for automating this task, providing valuable insights into biodiversity, ecological systems, and agricultural practices. Along with more discoveries in plant species, training a deep neural network becomes very challenging as the cost required to collect and annotate plant samples is expensive and impractical. Despite the lack of labelled plant samples, recent studies have explored the potential of leveraging publicly available and systematically annotated plant specimens in herbaria coupled with field images for plant species identification through cross-domain adaptation techniques. However, the accuracy of these methods remains unsatisfactory, motivating the exploration of alternative approaches. In this paper, we evaluated the feasibility of employing a pre-trained transformer-based self-distillation model (DINOv2) for cross-domain plant species identification tasks. We trained our model with the PlantCLEF2020 dataset comprised of approximately 320 k herbarium and field images representing 997 plant species. Our approach leverages the advanced feature extraction capabilities of DINOv2, which enhances cross-domain adaptation by effectively bridging the gap between herbarium and field images, achieving a 17.7% improvement over the best model proposed in previous work, that employs ensembles of Siamese network architectures with triplet loss (HFTL-ENS and OSM-ENS).},
  archive      = {J_NCA},
  author       = {Ong, Chin Ann and Tay, Fei Siang and Then, Yi Lung and McCarthy, Chris},
  doi          = {10.1007/s00521-025-11499-6},
  journal      = {Neural Computing and Applications},
  month        = {9},
  number       = {26},
  pages        = {21969-21995},
  shortjournal = {Neural Comput. Appl.},
  title        = {An evaluation of a pre-trained transformer-based self-distillation model (DINOv2) for cross-domain plant species identification},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A novel method of heterogeneous parallel machine learning by CPU–TPU for molecular dynamics. <em>NCA</em>, <em>37</em>(26), 21949-21967. (<a href='https://doi.org/10.1007/s00521-025-11498-7'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, a heterogeneous parallel machine learning molecular dynamics (MLMD) calculation method based on both central processing unit (CPU) and SOPHON BM1684X tensor processing unit (TPU) is proposed. The method aims to offer a new hardware deployment approach for advanced MLMD algorithms, alleviating the constraints imposed by the severe "memory wall" and "power wall" bottlenecks caused by the separation of storage units and computing units inherent in von Neumann architecture-based machines at the hardware level. By decomposing complex MD simulation tasks into subtasks that can be processed in parallel on both CPU and TPU, this method enhances computational efficiency while maintaining high precision. Specifically, the potential energy surface fitting task in MD simulation is deployed on the TPU, leveraging its parallel processing capabilities to accelerate computations. Meanwhile, load balancing between the CPU and TPU is achieved by executing other computational tasks on the CPU. Experimental results demonstrate significant improvements in computational speed, energy efficiency, and the size of computable systems compared to the non-heterogeneous CPU-only system, indicating that heterogeneous parallel computing is an effective method for accelerating MD simulations.},
  archive      = {J_NCA},
  author       = {Zhang, Yujia and Zhang, Xin and Zheng, Gang and Mo, Pinghui and Zhao, Zhuoying and Li, Chenyang and Tang, Kai and Liu, Jie},
  doi          = {10.1007/s00521-025-11498-7},
  journal      = {Neural Computing and Applications},
  month        = {9},
  number       = {26},
  pages        = {21949-21967},
  shortjournal = {Neural Comput. Appl.},
  title        = {A novel method of heterogeneous parallel machine learning by CPU–TPU for molecular dynamics},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Surfing the bitcoin waves: Comprehensive trend forecasting with various trader types. <em>NCA</em>, <em>37</em>(26), 21931-21948. (<a href='https://doi.org/10.1007/s00521-025-11496-9'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cryptocurrency trading is becoming increasingly popular worldwide, with many individuals seeking to maximize their profits. One approach they are exploring is following the actions of successful investors, automated bots, or whale traders. The insights that can help traders make better decisions may be uncovered by analyzing their behavior and impact. This study examines the effectiveness of this strategy and aims to understand how various types of traders affect the Bitcoin market, including fundamental aspects like price fluctuations over time. Additionally, we aim to identify patterns that regular traders can follow to enhance their chances of success in cryptocurrency trading. We employed a time-series forecasting method, which involves analyzing past price movements and other critical factors to predict future trends. To ensure the robustness and reliability of our findings, we utilized various advanced techniques, such as machine learning, deep learning, and traditional time-series forecasting models. These powerful tools enable us to make more accurate predictions and provide strong evidence for our research conclusions. The study demonstrates that some models, such as linear regression and random forest regression, did not perform well with features related to "whales," "bots," and "top traders." However, models like XGBoost Regression and Transformer showed positive effects. This suggests that, for now, traders should focus more on basic features like "open," "high," and "low" prices rather than these other factors. As advanced models like XGBoost and Transformer continue to develop, these features may become more important. While it is important to consider different features, relying on traditional indicators currently seems prudent.},
  archive      = {J_NCA},
  author       = {Ateş, Can Ali and Çoban, Emre and Gurgen Erdogan, Tugba},
  doi          = {10.1007/s00521-025-11496-9},
  journal      = {Neural Computing and Applications},
  month        = {9},
  number       = {26},
  pages        = {21931-21948},
  shortjournal = {Neural Comput. Appl.},
  title        = {Surfing the bitcoin waves: Comprehensive trend forecasting with various trader types},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). OVST: Online video stabilization with two-stage training transformer. <em>NCA</em>, <em>37</em>(26), 21909-21929. (<a href='https://doi.org/10.1007/s00521-025-11494-x'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Video stabilization aims to mitigate or eliminate the shake presented within video frames. Existing online video stabilization technologies rely on information from future frames, which may introduce a lag during real-time video stabilization. To surmount this hurdle, an online video stabilization model called OVST is proposed, which leverages solely historical video frames and enhanced by an attention mechanism. To simplify the complexity of model training and enhance robustness, a two-stage training strategy is proposed to decouple the fitting of real poses and the stabilization of virtual poses, and a hybrid stabilization loss with interframe soft constraints is designed, which effectively regulates the changes in camera poses between adjacent frames through interframe displacement, angular distortion, and cropping rate, thereby suppressing the distortion effects caused by excessive pose smoothing while balancing stability and cropping rate. Experiments demonstrate the superiority of the proposed OVST method over existing state-of-the-art techniques, achieving a stability metric of 0.8878 and a distortion metric of 0.9870.},
  archive      = {J_NCA},
  author       = {Wu, Xing and Zhu, Yimin and Zhang, Han and Song, Jun and Yao, Junfeng and Zhu, Dong and Qian, Quan and Guo, Yike},
  doi          = {10.1007/s00521-025-11494-x},
  journal      = {Neural Computing and Applications},
  month        = {9},
  number       = {26},
  pages        = {21909-21929},
  shortjournal = {Neural Comput. Appl.},
  title        = {OVST: Online video stabilization with two-stage training transformer},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A generalized novel approach for plant disease detection based on SimCLR and patch-based analysis. <em>NCA</em>, <em>37</em>(26), 21867-21908. (<a href='https://doi.org/10.1007/s00521-025-11493-y'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Detecting plant diseases is essential for maintaining crop health and maximizing agricultural productivity. While recent advancements in machine learning and deep learning methods for plant disease diagnosis show significant promise, they also present challenges—particularly related to data annotation and the limitations of task-specific models, which often struggle to generalize across different plant diseases. Traditional methods face additional obstacles due to their dependence on crop-specific models and the need for manual inspections, resulting in inefficiencies and limited scalability. This study proposes a generalized approach for efficiently identifying a wide range of plant diseases. By integrating patch-based analysis with the self-supervised learning technique SimCLR, the method enables farmers and researchers to detect unhealthy leaves across various crop species efficiently. To evaluate the effectiveness of our approach, we trained and tested the model using the well-regarded PlantVillage dataset, known for its extensive and diverse representation. Our approach achieved an accuracy of 97.57%. Moreover, when tested on novel datasets, the model achieved an accuracy of 99.22%, demonstrating its robustness and strong generalization capability to previously unseen data.},
  archive      = {J_NCA},
  author       = {Faheem Saidahmd, Mohamed T. and Elbasiony, Reda M. and Bastwesy, Marwa R. M. and Hagar, Asmaa A. M.},
  doi          = {10.1007/s00521-025-11493-y},
  journal      = {Neural Computing and Applications},
  month        = {9},
  number       = {26},
  pages        = {21867-21908},
  shortjournal = {Neural Comput. Appl.},
  title        = {A generalized novel approach for plant disease detection based on SimCLR and patch-based analysis},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Enhancing predictive accuracy of nano-additive concrete gamma ray attenuation at high temperatures using AI-based models. <em>NCA</em>, <em>37</em>(26), 21833-21866. (<a href='https://doi.org/10.1007/s00521-025-11491-0'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This study delves into predicting the residual gamma-ray linear attenuation coefficient (Rµ) values of concrete incorporating nano-additives, specifically nanocarbon tubes (NCTs) and nano-alumina (NAl), under elevated temperatures using various artificial intelligence (AI) models. Four AI-based prediction models of artificial neural networks (ANN), fuzzy logic models (FLM), water cycle algorithm (WCA), and genetic algorithm (GA) were trained using available literature data, which includes experimental results of 104 post-heating µ values varying by temperature degree, temperature exposure period, nanomaterial type, and nanomaterial replacement ratio. Results showed that ANN and FLM models demonstrated strong potential for predicting Rµ values, achieving coefficient of determination (R2) values of 0.989 and 0.999, respectively, for the training datasets. However, their practical application is limited by the challenge of formulating concise and direct prediction equations. Conversely, metaheuristic algorithms such as WCA and GA yield highly accurate predictions and enable the derivation of robust predictive equations. The developed equations using WCA and GA demonstrated excellent performance, achieving high R2 values of 0.959 and 0.907, respectively, for the training datasets. Moreover, these models exhibited superior validation for residual Rµ values after elevated temperatures exposure, with mean absolute errors (MAEs) of 0.0322 and 0.0501 for training, 0.049 and 0.054 for validation, and 0.0499 and 0.0575 for testing datasets, respectively. Furthermore, sensitivity analysis using Shapley Additive Explanation (SHAP) was conducted to elucidate the impact and relationship between the input variables and the outputs of Rµ values. The SHAP results indicated that temperature degree exerted the most significant influence on Rµ values, followed by %NCTs, %NAl, and finally, time of exposure. The average absolute SHAP values for these variables were 2.1, 1.7, 1.6, and 1, respectively. This study’s findings emphatically underscore the effectiveness of AI-based models in predicting concrete radiation shielding behavior. Crucially, it provides valuable insights into the intricate, nonlinear relationships among the various variables that govern this behavior.},
  archive      = {J_NCA},
  author       = {Mahmoud, Alaa A. and El-Sayed, Alaa A. and Aboraya, Ayman M. and N.Fathy, Islam and Nabil, Islam M.},
  doi          = {10.1007/s00521-025-11491-0},
  journal      = {Neural Computing and Applications},
  month        = {9},
  number       = {26},
  pages        = {21833-21866},
  shortjournal = {Neural Comput. Appl.},
  title        = {Enhancing predictive accuracy of nano-additive concrete gamma ray attenuation at high temperatures using AI-based models},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Two-stage APT malware propagation model in computer networks. <em>NCA</em>, <em>37</em>(26), 21805-21832. (<a href='https://doi.org/10.1007/s00521-025-11490-1'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Early detection and prevention of advanced persistent threats (APT) is a critical challenge in cybersecurity. This paper presents an innovative approach using dual susceptible–infected–recovered (Dual-SIR) model to predict the two-stage spread of APT malware within networks. The first SIR model addresses infections at the first stage—device and user level, serving as a precursor to server compromise. The second SIR model focuses on the second stage of propagation—server infections, where sensitive organizational data is stored. Experimental results demonstrate the effectiveness of our proposed model not only for APT malware but also for other types of malware. Our work significantly contributes to the field of cybersecurity by offering a more accurate and proactive method for predicting malware spread. Additionally, this approach has potential applications in forecasting the dissemination of malware in wireless sensor networks and the spread of malicious information on social media platforms.},
  archive      = {J_NCA},
  author       = {Do Xuan, Cho and Tran, Hai-Anh and Nguyen Thi, Lan Phuong and Nguyen, Kim-Khoa},
  doi          = {10.1007/s00521-025-11490-1},
  journal      = {Neural Computing and Applications},
  month        = {9},
  number       = {26},
  pages        = {21805-21832},
  shortjournal = {Neural Comput. Appl.},
  title        = {Two-stage APT malware propagation model in computer networks},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A novel approach using explainable prediction of default risk in peer-to-peer lending based on machine learning models. <em>NCA</em>, <em>37</em>(26), 21783-21803. (<a href='https://doi.org/10.1007/s00521-025-11489-8'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Online peer-to-peer (P2P) lending has expanded substantially during the previous decade globally. However, this quick expansion poses several potential risks as loan default risk in P2P lending remains unavoidable. As P2P lending has grown in both size and complexity, the challenges have also multiplied, leading to several complications, including high number of features, low-performing classification models and imbalanced dataset. Furthermore, machine learning models encounter another challenging issue known as the black-box problem. To overcome these challenges, the present work introduces a novel approach that involves tackling the dataset balancing issue using synthetic minority oversampling technique (SMOTE), employing carefully selected feature selection approaches (maximum relevance minimum redundancy (MRMR), sequential forward selection (SFS) and adaptive boosting (AdaBoost)) and machine learning such as nonlinear model (K-nearest neighbour (KNN)), tree-based model (random forest (RF)) and deep learning (multi-layer perceptron (MLP)). Compared to the previous studies, the present results showed that RF exhibited outstanding performance of 0.94, 0.94 and 0.99 in accuracy, F1-score and AUC, respectively. To address the black-box issue of the prediction model, enhance its interpretability and boost user trust, local interpretable model-agnostic explanations (LIME) and Shapley additive explanations (SHAP) explainable machine learning models were applied to the RF prediction model to elucidate its results. Furthermore, LIME and SHAP explainable machine learning models were applied to the RF prediction model, both with and without SMOTE resampling, to examine the influence of SMOTE resampling on the interpretability analysis of the RF prediction outcomes.},
  archive      = {J_NCA},
  author       = {Atef, Markus and Ouf, Shimaa and Seoud, Wafaa and Gabr, Menna Ibrahim},
  doi          = {10.1007/s00521-025-11489-8},
  journal      = {Neural Computing and Applications},
  month        = {9},
  number       = {26},
  pages        = {21783-21803},
  shortjournal = {Neural Comput. Appl.},
  title        = {A novel approach using explainable prediction of default risk in peer-to-peer lending based on machine learning models},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A fault diagnosis method combining dual information with sample attention mechanism under small samples. <em>NCA</em>, <em>37</em>(26), 21761-21781. (<a href='https://doi.org/10.1007/s00521-025-11487-w'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The implementation of deep learning in fault diagnosis is challenging due to the small sample, since the deficiency of labeled fault data constrains the model’s efficacy. To address this challenge, a new similarity filtering-based pseudo-label learning approach (SFPL) is proposed based on dual information and sample attention mechanism. SFPL utilizes unlabeled data for pre-training through data similarity calculations. It also includes a sample attention mechanism that assigns weights to the samples to improve the efficiency of the model’s learning. Additionally, a data filtering mechanism based on cosine similarity is introduced to enhance the quality of pseudo-labels. These pseudo-labels are used to fine-tune the model for high-accuracy fault diagnosis. Validation experiments on two datasets show that the suggested method can achieve high accuracy and stability with only a few labeled samples.},
  archive      = {J_NCA},
  author       = {Pang, Jiachen and Han, Tian and Li, Peng},
  doi          = {10.1007/s00521-025-11487-w},
  journal      = {Neural Computing and Applications},
  month        = {9},
  number       = {26},
  pages        = {21761-21781},
  shortjournal = {Neural Comput. Appl.},
  title        = {A fault diagnosis method combining dual information with sample attention mechanism under small samples},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). DAPrompt: Deterministic assumption prompt learning for event causality identification. <em>NCA</em>, <em>37</em>(26), 21743-21759. (<a href='https://doi.org/10.1007/s00521-025-11486-x'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Event causality identification (ECI) aims at determining whether there is a causal relation between two event mentions. Conventional prompt learning designs a prompt template to first predict an answer word and then maps it to the final decision. Unlike conventional prompts, we argue that predicting an answer word may not be a necessary prerequisite for the ECI task. Instead, we can first make a deterministic assumption on the existence of causal relation between two events and then evaluate its rationality to either accept or reject the assumption. The design motivation is to try the most utilization of the encyclopedia-like knowledge embedded in a pre-trained language model. In light of such considerations, we propose a deterministic assumption prompt learning model, called DAPrompt, for the ECI task. In particular, we design a simple deterministic assumption template concatenating with the input event pair, which includes two masks as predicted events tokens. We use the probabilities of predicted events to evaluate the assumption rationality for the final event causality decision. Experiments on the EventStoryLine corpus validate our design objective in terms of significant performance improvements over the state-of-the-art algorithms.},
  archive      = {J_NCA},
  author       = {Xiang, Wei and Zhan, Chuanhong and Zhang, Qing and Wang, Bang},
  doi          = {10.1007/s00521-025-11486-x},
  journal      = {Neural Computing and Applications},
  month        = {9},
  number       = {26},
  pages        = {21743-21759},
  shortjournal = {Neural Comput. Appl.},
  title        = {DAPrompt: Deterministic assumption prompt learning for event causality identification},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Detecting deception: Employing deep neural networks for fraudulent review detection on amazon. <em>NCA</em>, <em>37</em>(26), 21715-21742. (<a href='https://doi.org/10.1007/s00521-025-11485-y'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the era of e-commerce dominance, an increase in fake reviews on online shopping platforms compromises the integrity of consumer feedback systems. This study focuses on Amazon, a leading e-commerce platform in the USA, where fake reviews have become a significant concern. Given the limited availability of authentic datasets for analysis, we propose a novel methodology to differentiate between genuine and fraudulent reviews across verified and non-verified purchases. Our approach utilizes the bootstrap distribution of cosine similarity values, providing a robust statistical foundation for review classification. We present a comprehensive framework integrating convolutional neural networks with word embedding and emotion-mining techniques through natural language processing, using a novel loss function. This multifaceted approach enhances detection accuracy and offers insights into the linguistic and emotional markers of fake reviews. Our method demonstrates exceptional performance, achieving an accuracy rate of over 96% in distinguishing fake reviews from user reviews. This study contributes to the growing research on online review authenticity and offers practical implications for e-commerce platforms, regulatory bodies, and consumers. This research aims to foster trust in online marketplaces and protect consumers from misleading information by providing a powerful tool for fake review detection.},
  archive      = {J_NCA},
  author       = {Thilini Jayasinghe, J. M. and Dassanayaka, Sachith},
  doi          = {10.1007/s00521-025-11485-y},
  journal      = {Neural Computing and Applications},
  month        = {9},
  number       = {26},
  pages        = {21715-21742},
  shortjournal = {Neural Comput. Appl.},
  title        = {Detecting deception: Employing deep neural networks for fraudulent review detection on amazon},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Adaptive learning of instance representatives in dual spaces for medical image classification. <em>NCA</em>, <em>37</em>(26), 21695-21714. (<a href='https://doi.org/10.1007/s00521-025-11481-2'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Medical image classification plays a vital role in AI-aided medical diagnosis and is often addressed as a multiple instance learning (MIL) issue (i.e., each sample is a bag of instances). For medical images, the disease area or the discriminative area is usually smaller than the whole tissue. In other words, most of the instances in a bag are irrelevant and could interfere with the bag label inference. To address this issue, we add an instance representative selection process before MIL and propose a novel MIL approach named dual space multiple instance representative learning (DSMIRL). DSMIRL consists of two core steps: adaptive instance representative selection (AIRS) and multiple instance representative learning (MIRL). In AIRS, we group and score instances, and then meticulously devise group-wise or instance-wise strategies to select the final collection of instance representatives. The group-wise approaches only preserve the group of instances with the highest instance score as the instance representatives, while the instance-wise ones select the top-k ranked instances in each group to yield the final instance representation collection. In MIRL, we perform aggregations on the selected instance representatives. These aggregations are carried out in both label and feature spaces, so as to further exploit the complementary information of these two spaces. It is worthwhile to point out that this MIRL step can be also flexibly replaced by other existing MIL approaches and enables further improving them. Extensive experiments on four medical image datasets demonstrate the promising performance of DSMIRL over the state-of-the-art MIL approaches and also validate the effectiveness of boosting other MIL by DSMIRL.},
  archive      = {J_NCA},
  author       = {Zhu, Xiang and Huang, Sheng and Tang, Wenhao and Zhang, Yi and Zhang, Xiaoxian and Liu, Chen and Zhang, Xiahong},
  doi          = {10.1007/s00521-025-11481-2},
  journal      = {Neural Computing and Applications},
  month        = {9},
  number       = {26},
  pages        = {21695-21714},
  shortjournal = {Neural Comput. Appl.},
  title        = {Adaptive learning of instance representatives in dual spaces for medical image classification},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). LSHADE-NGS: Enhancing Q-coverage in directional sensor networks through navigated generation search. <em>NCA</em>, <em>37</em>(26), 21659-21694. (<a href='https://doi.org/10.1007/s00521-025-11479-w'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Directional sensors play a critical role in enabling precise data collection and effective monitoring within wireless sensor networks (WSNs). Despite their effectiveness in tasks like surveillance and environmental observation, challenges remain in prolonging network life and ensuring multi-coverage, particularly Q-coverage. Multi-coverage issues arise in over-provisioned (excess sensors) and under-provisioned (insufficient sensors) environments, leading to inefficient resource utilization or coverage gaps. Achieving effective multi-coverage necessitates strategic resource allocation to maintain comprehensive monitoring while avoiding redundancy. This paper addresses the Q-coverage optimization problem in directional sensor networks with adjustable orientations by assessing whether the environment is over-provisioned or under-provisioned and dynamically determines the operational status (active or inactive) and orientation of each sensor to achieve two main objectives: maximizing network coverage balancing in under-provisioned environments and minimizing the number of active sensors in over-provisioned settings. To meet these goals, we introduce LSHADE-navigated generation search (LSHADE-NGS), a novel enhancement of the LSHADE algorithm designed to navigate each generation toward more promising search spaces. The proposed algorithm integrates several innovative components: a refined initialization method to improve the diversity of the starting population, a heuristic adjustment JADE mutation (HA-JADE) to dynamically adjust mutation strategies, and a greedily-jumped binomial crossover mechanism on the directional array (GJ-Bi) to enhance convergence speed. The algorithm’s effectiveness is evaluated using multiple metrics, including the Q-balancing index, distance index, coverage quality, power consumption, and the count of active sensors. The experimental results reveal significant enhancements in solution quality achieved by LSHADE-NGS, underscoring the method’s superiority over existing approaches and illustrating its distinct advantages compared to the conventional LSHADE framework.},
  archive      = {J_NCA},
  author       = {Thanh, Binh Huynh Thi and Van Duc, Cuong and Van, Son Nguyen and La Van, Quan and Thi, Hanh Nguyen},
  doi          = {10.1007/s00521-025-11479-w},
  journal      = {Neural Computing and Applications},
  month        = {9},
  number       = {26},
  pages        = {21659-21694},
  shortjournal = {Neural Comput. Appl.},
  title        = {LSHADE-NGS: Enhancing Q-coverage in directional sensor networks through navigated generation search},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A fairness scale for real-time recidivism forecasts using a national database of convicted offenders. <em>NCA</em>, <em>37</em>(26), 21607-21657. (<a href='https://doi.org/10.1007/s00521-025-11478-x'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This investigation explores whether machine learning can predict recidivism while addressing societal biases. To investigate this, we obtained conviction data from the UK’s Police National Computer (PNC) on 346,685 records between January 1, 2000, and February 3, 2006 (His Majesty’s Inspectorate of Constabulary in Use of the Police National Computer: An inspection of the ACRO Criminal Records Office. His Majesty’s Inspectorate of Constabulary, Birmingham, https://assets-hmicfrs.justiceinspectorates.gov.uk/uploads/police-national-computer-use-acro-criminal-records-office.pdf , 2017). We generate twelve machine learning models—six to forecast general recidivism, and six to forecast violent recidivism—over a 3-year period, evaluated via fivefold cross-validation. Our best-performing models outperform the existing state-of-the-arts, receiving an area under curve (AUC) score of 0.8660 and 0.8375 for general and violent recidivism, respectively. Next, we construct a fairness scale that communicates the semantic and technical trade-offs associated with debiasing a criminal justice forecasting model. We use this scale to debias our best-performing models. Results indicate both models can achieve all five fairness definitions because the metrics measuring these definitions—the statistical range of recall, precision, positive rate, and error balance between demographics—indicate that these scores are within a one percentage point difference of each other. Deployment recommendations and implications are discussed. These include recommended safeguards against false positives, an explication of how these models addressed societal biases, and a case study illustrating how these models can improve existing criminal justice practices. That is, these models may help police identify fewer people in a way less impacted by structural bias while still reducing crime. A randomized control trial is proposed to test this illustrated case study, and further directions explored.},
  archive      = {J_NCA},
  author       = {Verrey, Jacob and Neyroud, Peter and Sherman, Lawrence and Ariel, Barak},
  doi          = {10.1007/s00521-025-11478-x},
  journal      = {Neural Computing and Applications},
  month        = {9},
  number       = {26},
  pages        = {21607-21657},
  shortjournal = {Neural Comput. Appl.},
  title        = {A fairness scale for real-time recidivism forecasts using a national database of convicted offenders},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Distributed optimization with faulty nodes: Robust aggregation in hyperbolic space. <em>NCA</em>, <em>37</em>(26), 21563-21605. (<a href='https://doi.org/10.1007/s00521-025-11475-0'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The increasing deployment of distributed machine learning models necessitates robust optimization methods that can tolerate adversarial or faulty nodes. In this work, we propose a robust gradient aggregation method for distributed stochastic gradient descent that leverages hyperbolic geometry. Specifically, local gradients computed at individual nodes are embedded into hyperbolic space using the Poincaré ball model, and their geometric median is computed as a robust aggregate. This aggregated gradient is then mapped back to Euclidean space for the gradient update. We also show that existing robust gradient aggregation methods like Krum can be improved using hyperbolic space. Compared to existing robust aggregation methods, our hyperbolic approach offers improved separation of outlier updates. We provide theoretical convergence guarantees and validate our method on benchmark datasets as well as on a traffic forecasting task, demonstrating its efficacy in mitigating Byzantine failures in distributed federated learning environments.},
  archive      = {J_NCA},
  author       = {Ghosh, Subhas Kumar and Vittamsetti, Vijay Monic},
  doi          = {10.1007/s00521-025-11475-0},
  journal      = {Neural Computing and Applications},
  month        = {9},
  number       = {26},
  pages        = {21563-21605},
  shortjournal = {Neural Comput. Appl.},
  title        = {Distributed optimization with faulty nodes: Robust aggregation in hyperbolic space},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Cancer progression inference using a finite-state model to allow recurrences and losses of mutations. <em>NCA</em>, <em>37</em>(26), 21545-21562. (<a href='https://doi.org/10.1007/s00521-025-11474-1'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The inference of cancer evolutionary histories is a key step for the understanding and treatment of the disease; thus, many tools had been developed in the last decade to address this important problem. However, methods for inferring tumor phylogenies need to strike a balance between keeping reasonable running times and employing sophisticated evolution models. Binary characters, such as single-nucleotide variants and known mutations, which is our focus, is an example of a simple model that is able to capture most relevant cases—but not copy number variants. On binary characters, most methods are designed for simpler models where mutations can only be accumulated under the infinite sites assumption; however, those models tend to be too simplistic for real case scenarios. While the most explored direction in the context of binary characters is to allow mutation losses, in this paper, we introduce an even more general model, where each mutation can be acquired and lost more than once. We describe this model, provide a simulated annealing approach exploiting this novel evolutionary framework, and show its accuracy on different sets of experimental evaluations when compared to less general models, and demonstrate potential application to real data.},
  archive      = {J_NCA},
  author       = {Ciccolella, Simone and Patterson, Murray and Hajirasouliha, Iman and Della Vedova, Gianluca},
  doi          = {10.1007/s00521-025-11474-1},
  journal      = {Neural Computing and Applications},
  month        = {9},
  number       = {26},
  pages        = {21545-21562},
  shortjournal = {Neural Comput. Appl.},
  title        = {Cancer progression inference using a finite-state model to allow recurrences and losses of mutations},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Experimental validation of optimized performance of microbial fuel cell-based horned lizard algorithm and artificial intelligence. <em>NCA</em>, <em>37</em>(26), 21519-21544. (<a href='https://doi.org/10.1007/s00521-025-11473-2'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Microbial fuel cell (MFC) has shown promise for simultaneous wastewater treatment and electrical power production. Improving the performance of MFC is the objective of this study. Firstly, three input parameters including Ni (mg/m2), COD (mg/L) and aeration (mL/min) are investigated experimentally to measure the performance index of the MFC. The performance index of MFC includes the power density (PD), COD removal (CODr) and coulombic efficiency (CE). Secondly, using the experimental data, an adaptive neuro-fuzzy inference system (ANFIS) model was created to simulate the MFC in terms of Ni, COD and aeration. To assess the modelling stage, the results are compared with ANOVA. For the PD model, the predicted R2 increased from 0.902 to 0.93 by around 3.1% compared to ANOVA, whereas for the ANFIS model for the CODr, the predicted R2 increased from 0.58 to 0.81 by around 39.6% compared to ANOVA. For the ANFIS model of the CE, the predicted R2 value increased from 0.81 to 0.91 by around 12.3% compared to ANOVA. This demonstrated the robustness of ANFIS model of the MFC. Thirdly, the optimal values of Ni, COD and aeration are identified based on integration between ANFIS model of the MFC and the horned lizard algorithm.},
  archive      = {J_NCA},
  author       = {Rezk, Hegazy and Ghasemi, Mostafa},
  doi          = {10.1007/s00521-025-11473-2},
  journal      = {Neural Computing and Applications},
  month        = {9},
  number       = {26},
  pages        = {21519-21544},
  shortjournal = {Neural Comput. Appl.},
  title        = {Experimental validation of optimized performance of microbial fuel cell-based horned lizard algorithm and artificial intelligence},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Blind image super-resolution using swin transformer with unsupervised degradation and sparse attention. <em>NCA</em>, <em>37</em>(26), 21493-21517. (<a href='https://doi.org/10.1007/s00521-025-11471-4'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the past few years, significant advancements in convolutional neural networks (CNNs) have significantly propelled the field of image super-resolution (SR) research. Nonetheless, many current SR techniques are limited in effectively addressing real-world data degradation, particularly in blind scenarios characterized by multi-modal, spatially variant, and unknown distributions. Based on this issue, we propose a degradation-aware Swin Transformer with sparse attention for blind SR. In this model, we proposed a degradation-aware residual Swin Transformer sparse attention block that is based on the Swin transformer layer, the non-local sparse attention (NLSA), and the degradation-aware convolutional (DA Cov). The Swin Transformer solves CNN’s problems because it has the ability to process images of large size and extract long-range dependency, which works as a local attention mechanism. Moreover, the NLSA is utilized to solve problems combined with non-local attention, which works as a global attention mechanism. Also, it prevents the model from attending to noisy and less informative locations by partitioning the deep feature pixels into different groups. The DA Cov is used to integrate the degraded kernel with extracted features. Moreover, our model shows superior visual quality and reconstruction accuracy with an efficient number of parameters and Mult-Adds. For example, on the Set5 dataset with a kernel size of 0.06 and a scaling factor of $$\times$$ 4, our model achieved a 0.1 dB improvement in PSNR compared to DRAN.},
  archive      = {J_NCA},
  author       = {Gendy, Garas and Sabor, Nabil},
  doi          = {10.1007/s00521-025-11471-4},
  journal      = {Neural Computing and Applications},
  month        = {9},
  number       = {26},
  pages        = {21493-21517},
  shortjournal = {Neural Comput. Appl.},
  title        = {Blind image super-resolution using swin transformer with unsupervised degradation and sparse attention},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). UniBERT: Adversarial training for language-universal representations. <em>NCA</em>, <em>37</em>(26), 21473-21492. (<a href='https://doi.org/10.1007/s00521-025-11470-5'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper presents UniBERT, a compact multilingual language model that uses an innovative training framework that integrates three components: masked language modeling, adversarial training, and knowledge distillation. Pre-trained on a meticulously curated Wikipedia corpus spanning 107 languages, UniBERT is designed to reduce the computational demands of large-scale models while maintaining competitive performance across various natural language processing tasks. Comprehensive evaluations on four tasks, named entity recognition, natural language inference, question answering, and semantic textual similarity, demonstrate that our multilingual training strategy, enhanced by an adversarial objective, significantly improves cross-lingual generalization. Specifically, UniBERT models show an average relative improvement of 7.72% over traditional baselines, which achieved an average relative improvement of only 1.12%, and statistical analysis confirms the significance of these gains (p value = 0.0184). This work highlights the benefits of combining adversarial training and knowledge distillation to build robust and scalable language models, thus advancing the field of multilingual and cross-lingual natural language processing.},
  archive      = {J_NCA},
  author       = {Avram, Andrei-Marius and Lupaşcu, Marian and Cercel, Dumitru-Clementin and Mironică, Ionuţ and Trăuşan-Matu, Ştefan},
  doi          = {10.1007/s00521-025-11470-5},
  journal      = {Neural Computing and Applications},
  month        = {9},
  number       = {26},
  pages        = {21473-21492},
  shortjournal = {Neural Comput. Appl.},
  title        = {UniBERT: Adversarial training for language-universal representations},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A lightweighted densely connected network for insect-pest identification in cotton crop. <em>NCA</em>, <em>37</em>(26), 21459-21472. (<a href='https://doi.org/10.1007/s00521-025-11469-y'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cotton, popularly known as “White-Gold” in India, accounts for about 23% of global production and significantly contributes to the Indian economy. However, it is highly susceptible to the infestation of various harmful insect-pests, which can cause substantial crop damage and yield reductions of up to 40–50%. In this context, we developed a lightweighted densely connected deep learning model for identifying insect-pests of cotton crop using RGB images. We collected 5,559 images of insect-pest infested cotton crops under natural field conditions across Indian agricultural farms. However, for enhancing the training images and to minimize the risk of overfitting the model, we applied a variety of image augmentation methods, including flipping, rotation, zooming, etc. The proposed model, with 83 layers, including four dense blocks and three transition layers, achieved around 99.24% of classification accuracy having 15 s/epoch of training time, outperforming the other pretrained models. Furthermore, Grad-CAM visualization technique was used for demonstrating the model’s effectiveness and efficiency in multiclass classification of cotton insect-pest images. This model offers a practical tool for farmers to manage pest infestations and improve cotton crop yield.},
  archive      = {J_NCA},
  author       = {Kumari, Shalini and Marwaha, Sudeep and Haque, Md. Ashraful and Sachan, Harsh and Deb, Chandan Kumar and Dahiya, Shashi and Arora, Alka and Shashank, P. R.},
  doi          = {10.1007/s00521-025-11469-y},
  journal      = {Neural Computing and Applications},
  month        = {9},
  number       = {26},
  pages        = {21459-21472},
  shortjournal = {Neural Comput. Appl.},
  title        = {A lightweighted densely connected network for insect-pest identification in cotton crop},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A GARCH-temporal fusion transformer model for the volatility prediction of exchange traded funds. <em>NCA</em>, <em>37</em>(26), 21435-21458. (<a href='https://doi.org/10.1007/s00521-025-11468-z'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Forecasting the volatility of financial assets—defined as the degree of their price variation over time—is a trending topic in financial research. Enhancing prediction accuracy is crucial in this field, as an asset’s volatility is widely used to assess the risk associated with its returns. In this context, we introduce a novel hybrid model that integrates traditional econometric techniques, specifically GARCH models, with the Temporal Fusion Transformer (TFT), a cutting-edge deep learning architecture. We designed such a model for analyzing Exchange Traded Funds (ETFs) composed of assets from the S&P 500, a benchmark index tracking 500 large US companies therefore reflecting the overall health and trends of the stock market, across various sectors. Utilizing volatility proxies such as historical volatility (HV) and the Garman–Klass (GK) method, our study demonstrates that the hybrid GARCH-TFT model significantly outperforms alternative models in forecasting the GK proxy and achieves performance comparable to the stand-alone TFT model for HV, underscoring the potential of merging machine learning approaches with traditional econometric methods to enhance predictive precision in volatile financial markets.},
  archive      = {J_NCA},
  author       = {Petrosino, Lorenzo and Bacco, Luca and Salvati, Giuliano and Merone, Mario and Papi, Marco},
  doi          = {10.1007/s00521-025-11468-z},
  journal      = {Neural Computing and Applications},
  month        = {9},
  number       = {26},
  pages        = {21435-21458},
  shortjournal = {Neural Comput. Appl.},
  title        = {A GARCH-temporal fusion transformer model for the volatility prediction of exchange traded funds},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A simple contrastive embedding framework for low-resource fake news detection. <em>NCA</em>, <em>37</em>(26), 21407-21433. (<a href='https://doi.org/10.1007/s00521-025-11467-0'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Low-resource fake news detection aims at discerning between true and false claims from low-resource languages with scarce benchmark datasets. In this resource-constrained scenario, fake news data collected from online hoax reporting system is inherently skewed because human fact checkers mainly sample claims that are more likely to be fake or false. Instead of training end-to-end classifier on the extremely imbalanced dataset, our study investigates a simple framework based on contrastive learning and stacking-based ensemble learning as an alternate fake news classification pipeline for Indonesian language. Our empirical result shows that by combining contrastive-based embedding model—Contrast-BERT and ensemble of multilayer perceptrons (MLPs) in inference stage, we improve the precision score in fake news classification up to 26.64%, while maintaining accuracy and recall scores of above 75%, given extreme class imbalance ratio 1:24. Contrast-BERT is also superior to its counterparts in unsupervised topic clustering and evidence retrieval by nearly twofold. Furthermore, we observe that contrastive-based model follows a similar performance trend in Indonesian clickbait benchmark dataset. Contrast-BERT is more accurate and precise at predicting samples than end-to-end BERT classifier by up to 47%, given training subset with extreme imbalance ratio $$\ge$$ 1:19.},
  archive      = {J_NCA},
  author       = {Ni’mah, Iftitahu and Wijayanti, Rini and Santosa, Agung and Jarin, Asril and Sampurno, Tri and Teduh Uliniansyah, Mohammad and Fang, Meng and Menkovski, Vlado and Pechenizkiy, Mykola},
  doi          = {10.1007/s00521-025-11467-0},
  journal      = {Neural Computing and Applications},
  month        = {9},
  number       = {26},
  pages        = {21407-21433},
  shortjournal = {Neural Comput. Appl.},
  title        = {A simple contrastive embedding framework for low-resource fake news detection},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A stacked ensemble deep learning framework for alzheimer’s severity ranking and classification using MRI scans. <em>NCA</em>, <em>37</em>(26), 21381-21405. (<a href='https://doi.org/10.1007/s00521-025-11465-2'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Alzheimer’s disease (AD) is a progressive neurodegenerative disorder that significantly impacts cognitive function and quality of life. The early diagnosis and an accurate severity assessment of AD are always the point of target to the medical fraternity. To fulfill the dual objective this study presents a stacked ensemble deep learning framework for the automated classification and severity ranking of Alzheimer’s disease using MRI scans. The framework integrates multiple deep learning models including EfficientNet-B7, Xception, and Inception-ResNet-V2 to extract rich spatial features from MRI scans and is subsequently integrated using a rank-based fusion approach. The proposed method is evaluated on large-scale MRI OASIS and ADNI datasets of Alzheimer’s patients. According to experimental results, the suggested ensemble performs well on the OASIS dataset, achieving 97.8% accuracy, 96.5% sensitivity, and 98.2% specificity. The model outperforms the current single-model baselines with an accuracy of 98.1%, sensitivity of 97.3%, and specificity of 98.5% on the ADNI dataset. Additionally, the framework is excellent at assessing the severity of Alzheimer’s disease, making it a trustworthy tool for clinical decision support.},
  archive      = {J_NCA},
  author       = {Pandey, Nidhi and Sharma, Oshin},
  doi          = {10.1007/s00521-025-11465-2},
  journal      = {Neural Computing and Applications},
  month        = {9},
  number       = {26},
  pages        = {21381-21405},
  shortjournal = {Neural Comput. Appl.},
  title        = {A stacked ensemble deep learning framework for alzheimer’s severity ranking and classification using MRI scans},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A new approach to hiding sensitive itemsets based on gray wolf optimization algorithm. <em>NCA</em>, <em>37</em>(26), 21363-21379. (<a href='https://doi.org/10.1007/s00521-025-11460-7'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Protecting privacy in data mining has become a critical issue due to the increasing capabilities of data storage and analysis, particularly in domains involving personal data such as healthcare, banking, and commerce. Many techniques are used to protect sensitive data in data mining. However, these techniques often result in substantial information loss and reduced data utility. Therefore, a key challenge in privacy-preserving data mining (PPDM) is to develop techniques that can hide sensitive information with minimal impact on the original data. This paper introduces a new approach for protecting sensitive itemsets in association rule mining with transactions modifications rather transaction deletion. The proposed approach minimizes the impact on the original dataset while selectively hiding sensitive itemsets depending on the strategies of the gray wolf optimization (GWO) algorithm. The proposed approach introduces a novel algorithm termed GWOHSI (GWO for hiding sensitive itemset) to identify and hide sensitive itemsets with minimal side effects. The goal of this algorithm is to determine the optimal number of transactions should be modified for each item that contributes to sensitive itemsets, thus reducing their support to below the minimum support threshold. Comprehensive experiments are conducted to evaluate the performance of the proposed approach in terms of hiding failure, number of non-sensitive itemsets affected, data dissimilarity and execution time. Four datasets were used for evaluation, the results showed that the proposed approach effectively hides all sensitive itemsets (achieved a 100% hiding ratio), while minimizing the affected non-sensitive itemsets. The execution time of GWOHSI algorithm is considered satisfactory, and it is consistent across almost all datasets.},
  archive      = {J_NCA},
  author       = {Jumaa, Alaa Khalil},
  doi          = {10.1007/s00521-025-11460-7},
  journal      = {Neural Computing and Applications},
  month        = {9},
  number       = {26},
  pages        = {21363-21379},
  shortjournal = {Neural Comput. Appl.},
  title        = {A new approach to hiding sensitive itemsets based on gray wolf optimization algorithm},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Correction: Improved hybrid feature extractor in lightweight convolutional neural network for postharvesting technology: Automated oil palm fruit grading. <em>NCA</em>, <em>37</em>(25), 21361. (<a href='https://doi.org/10.1007/s00521-025-11303-5'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_NCA},
  author       = {Junos, Mohamad Haniff and Mohd Khairuddin, Anis Salwa and Abu Talip, Mohamad Sofian and Kairi, Muhammad Izhar and Siran, Yosri Mohd},
  doi          = {10.1007/s00521-025-11303-5},
  journal      = {Neural Computing and Applications},
  month        = {9},
  number       = {25},
  pages        = {21361},
  shortjournal = {Neural Comput. Appl.},
  title        = {Correction: Improved hybrid feature extractor in lightweight convolutional neural network for postharvesting technology: Automated oil palm fruit grading},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Correction: The FCM-guided deep learning model for low-frequency oscillation damping for electric power networks. <em>NCA</em>, <em>37</em>(25), 21359-21360. (<a href='https://doi.org/10.1007/s00521-025-11301-7'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_NCA},
  author       = {Shafiullah, Md},
  doi          = {10.1007/s00521-025-11301-7},
  journal      = {Neural Computing and Applications},
  month        = {9},
  number       = {25},
  pages        = {21359-21360},
  shortjournal = {Neural Comput. Appl.},
  title        = {Correction: The FCM-guided deep learning model for low-frequency oscillation damping for electric power networks},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Correction: DNACoder: A CNN-LSTM attention-based network for genomic sequence data compression. <em>NCA</em>, <em>37</em>(25), 21357-21358. (<a href='https://doi.org/10.1007/s00521-025-11271-w'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_NCA},
  author       = {Sheena, K. S. and Nair, Madhu S.},
  doi          = {10.1007/s00521-025-11271-w},
  journal      = {Neural Computing and Applications},
  month        = {9},
  number       = {25},
  pages        = {21357-21358},
  shortjournal = {Neural Comput. Appl.},
  title        = {Correction: DNACoder: A CNN-LSTM attention-based network for genomic sequence data compression},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Correction: Performance analysis of machine learning algorithms for hybrid power generation prediction. <em>NCA</em>, <em>37</em>(25), 21353-21356. (<a href='https://doi.org/10.1007/s00521-025-11233-2'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_NCA},
  author       = {Sarıışık, Gencay and Öğütlü, Ahmet Sabri},
  doi          = {10.1007/s00521-025-11233-2},
  journal      = {Neural Computing and Applications},
  month        = {9},
  number       = {25},
  pages        = {21353-21356},
  shortjournal = {Neural Comput. Appl.},
  title        = {Correction: Performance analysis of machine learning algorithms for hybrid power generation prediction},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Correction: Object search by a concept-conditioned object detector. <em>NCA</em>, <em>37</em>(25), 21351-21352. (<a href='https://doi.org/10.1007/s00521-025-11232-3'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_NCA},
  author       = {Rigoni, Davide and Serafini, Luciano and Sperduti, Alessandro},
  doi          = {10.1007/s00521-025-11232-3},
  journal      = {Neural Computing and Applications},
  month        = {9},
  number       = {25},
  pages        = {21351-21352},
  shortjournal = {Neural Comput. Appl.},
  title        = {Correction: Object search by a concept-conditioned object detector},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Correction to: Improving paraphrase generation using supervised neural-based statistical machine translation framework. <em>NCA</em>, <em>37</em>(25), 21349. (<a href='https://doi.org/10.1007/s00521-024-09650-w'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_NCA},
  author       = {Razaq, Abdur and Shah, Babar and Khan, Gohar and Alfandi, Omar and Ullah, Abrar and Halim, Zahid and Ur Rahman, Atta},
  doi          = {10.1007/s00521-024-09650-w},
  journal      = {Neural Computing and Applications},
  month        = {9},
  number       = {25},
  pages        = {21349},
  shortjournal = {Neural Comput. Appl.},
  title        = {Correction to: Improving paraphrase generation using supervised neural-based statistical machine translation framework},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Correction: Knowledge distillation vulnerability of DeiT through CNN adversarial attack. <em>NCA</em>, <em>37</em>(25), 21347. (<a href='https://doi.org/10.1007/s00521-023-09412-0'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_NCA},
  author       = {Hong, Inpyo and Choi, Chang},
  doi          = {10.1007/s00521-023-09412-0},
  journal      = {Neural Computing and Applications},
  month        = {9},
  number       = {25},
  pages        = {21347},
  shortjournal = {Neural Comput. Appl.},
  title        = {Correction: Knowledge distillation vulnerability of DeiT through CNN adversarial attack},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Fortifying EV charging stations: AI-powered detection and mitigation of DDoS attacks using personalized federated learning. <em>NCA</em>, <em>37</em>(25), 21311-21346. (<a href='https://doi.org/10.1007/s00521-025-11452-7'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Electric vehicle charging stations (EVCS) are becoming more and more common, so it is imperative to protect these systems from cyberattacks, especially Distributed Denial-of-Service (DDoS) assaults. The objective of this study is to enhance the model interpretability and detection accuracy of DDoS attacks in EVCS using the Personalized Federated Learning (PFL) technique. The research makes use of an IoT attack dataset with 33 attacks that were carried out over 105 devices in a topology that was divided into seven different categories. Using the Firefly Algorithm, the suggested PFL method selects a subset of features wisely to maximize the performance of the classification model. Promising outcomes are seen in the evaluation of several machine learning models, such as Random Forest, Gradient Boosting Machine (GBM), K-Nearest Neighbors, and Multilayer Perceptron. GBM and Random Forest demonstrate their promise for efficient DDoS detection in EVCS by achieving high accuracy rates of 99% and 98%, respectively, in detecting DDoS attacks. The overall detection performance is further improved by the feature selection model, which also increases the efficiency and interpretability of the classification model. These results imply that machine learning models can improve the security and resilience of EVCS against DDoS attacks when combined with the PFL technique.},
  archive      = {J_NCA},
  author       = {Talaat, Fatma M. and Khoudier, Mohamed Mohsen Elsaid and Moawad, Ibrahim F. and El-Ghamry, Amir},
  doi          = {10.1007/s00521-025-11452-7},
  journal      = {Neural Computing and Applications},
  month        = {9},
  number       = {25},
  pages        = {21311-21346},
  shortjournal = {Neural Comput. Appl.},
  title        = {Fortifying EV charging stations: AI-powered detection and mitigation of DDoS attacks using personalized federated learning},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Improving arabic sentiment analysis with negation and speculation auxiliary tasks. <em>NCA</em>, <em>37</em>(25), 21297-21309. (<a href='https://doi.org/10.1007/s00521-025-11451-8'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In today's era of big data, there is a growing need for computational techniques that can streamline people’s lives by generating, processing, and efficiently understanding textual data. Natural Language Processing (NLP), a branch of Artificial Intelligence, is continuously advancing in the development of deep learning models for these purposes. One crucial aspect of NLP is the detection of negation and speculation in text, particularly in a morphologically rich language like Arabic, as these phenomena can significantly alter the polarity and factuality of the text’s meaning. Addressing negation and speculation is crucial for improving the effectiveness of various NLP applications, including sentiment analysis, machine translation, and biomedical information retrieval. While many research studies have explored these challenges in English, Spanish, and Chinese, Arabic remains unexplored due to its complexity and lack of annotated corpora. This paper proposes a Multi-Task Learning (MTL) model that classifies sentiment analysis as the main task while using negation and speculation detection as auxiliary tasks to enhance contextual understanding of the main task. We trained, validated, and tested the proposed model using the Negation Speculation Arabic Review (NSAR) corpus, a pre-annotated corpus for negation and speculation. The experimental results demonstrate that our model achieves an enhancement of 5% and 3% in F1 over the baseline (F1 = 72%) for negation and speculation, respectively, when evaluated on a well-known benchmarked Arabic sentiment analysis dataset.},
  archive      = {J_NCA},
  author       = {Mahany, Ahmed and Ghoniemy, Said and Khaled, Heba},
  doi          = {10.1007/s00521-025-11451-8},
  journal      = {Neural Computing and Applications},
  month        = {9},
  number       = {25},
  pages        = {21297-21309},
  shortjournal = {Neural Comput. Appl.},
  title        = {Improving arabic sentiment analysis with negation and speculation auxiliary tasks},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A novel NLP-driven approach for enriching artefact descriptions, provenance, and entities in cultural heritage. <em>NCA</em>, <em>37</em>(25), 21275-21296. (<a href='https://doi.org/10.1007/s00521-025-11449-2'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Despite the availability of numerous open datasets on cultural heritage, limited research has focussed on structuring and normalising this type of data, particularly through the extraction of entities from unstructured texts. This step is crucial for enriching, analysing, and understanding these complex datasets. This study presents a procedure designed to streamline the creation of domain-specific datasets for training natural language processing models and evaluates their performance across three distinct datasets generated using this procedure. A zero-shot learning model, the Generalist and Lightweight Model for Named Entity Recognition, was assessed alongside pre-trained spaCy models on three datasets created in the framework of the European Union-funded Research Intelligence Technology for Heritage and Market Security project: one containing provenance information on artefacts from North American museums, another detailing stolen cultural goods in Romania, and a third with structured yet unclassified data on WWII-looted Polish art. Further training of spaCy models on these newly defined datasets revealed that fine-tuned models significantly outperform their non-fine-tuned counterparts, with the best results from the Transformer model fine-tuned on provenance data. This success can be largely attributed to the standardised conventions in provenance research. In contrast, the model fine-tuned on descriptive information performed poorly, likely due to extensive descriptions containing non-essential data that increased model uncertainty. This work highlights the potential of automating entity extraction to build knowledge graphs for cultural object databases, enabling advanced analytical approaches such as Network Analysis.},
  archive      = {J_NCA},
  author       = {Ferro, Sara and Giovanelli, Riccardo and Leeson, Madison and De Bernardin, Michela and Traviglia, Arianna},
  doi          = {10.1007/s00521-025-11449-2},
  journal      = {Neural Computing and Applications},
  month        = {9},
  number       = {25},
  pages        = {21275-21296},
  shortjournal = {Neural Comput. Appl.},
  title        = {A novel NLP-driven approach for enriching artefact descriptions, provenance, and entities in cultural heritage},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). DEPP: Dictionary embedded probabilistic priors for scene text image super-resolution. <em>NCA</em>, <em>37</em>(25), 21259-21273. (<a href='https://doi.org/10.1007/s00521-025-11441-w'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Scene text image super-resolution (STISR), often considered a preliminary step for scene text recognition, refers to the task of enhancing the resolution of text embedded in natural scene images and plays a vital role in various applications. Most of the existing STISR methods either leverage deep convolutional neural networks by regarding text images as natural scene images or use a text recognizer’s feedback as guidance to the STISR process. However, since the text recognition is initially done on low-resolution images, it is mostly inaccurate, more so as the length of the words increases, thus degrading the super-resolution process. In this paper, we introduce DEPP which utilizes dictionary embedding (DE) based probabilistic priors calculated from a large English text corpus consisting of both alphabets and digits. The initial state and the bigram probabilities obtained are fused with the probability obtained from the recognizer, before passing it onto a single image super-resolution (SISR) block. By integrating DE as a prior and implementing a modified perceptual loss, the method effectively captures the contextual information of text, enabling more accurate super-resolution and visually pleasing results. Experimental results on the benchmark TextZoom dataset demonstrate that our DEPP framework achieves superior performance compared to most existing approaches, particularly for medium and long-length words, as measured by text recognition accuracy. Since DEPP uses the text recognition attributes to rectify or guide the super-resolution process, it makes our method more domain-inspired and task-aware, compared to usual black box deep learners.},
  archive      = {J_NCA},
  author       = {Bhattacharya, Avigyan and Basu, Subhadip and Chakraborti, Tapabrata},
  doi          = {10.1007/s00521-025-11441-w},
  journal      = {Neural Computing and Applications},
  month        = {9},
  number       = {25},
  pages        = {21259-21273},
  shortjournal = {Neural Comput. Appl.},
  title        = {DEPP: Dictionary embedded probabilistic priors for scene text image super-resolution},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Explainable deep learning techniques for wind speed forecasting in coastal areas: Integrating model configuration, regularization, early stopping, and SHAP analysis. <em>NCA</em>, <em>37</em>(25), 21219-21257. (<a href='https://doi.org/10.1007/s00521-025-11433-w'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Accurate wind prediction is critical across engineering disciplines. For coastal infrastructure, it determines wave loads and storm surge resilience, directly impacting millions in vulnerable low-lying regions. The energy sector relies on precise forecasts to optimize wind farm output and stabilize power grids, while agriculture uses wind data to time pesticide applications and protect crops. Aviation and shipping industries leverage predictions for fuel-efficient routing and hazard avoidance, and urban engineers integrate wind models for skyscraper design and air pollution management. As climate change amplifies wind extremes, advancing predictive capabilities has become an urgent cross-sector priority for adaptive planning and risk mitigation. In coastal applications, empirical wave models (e.g., SWAN and WAVEWATCH III) heavily depend on accurate wind inputs, where errors can lead to underestimation of extreme events and compromise structural safety. This study introduces a novel deep learning framework, integrating advanced data preprocessing, structured neural networks, and explainable AI techniques, to enhance short-term (hourly) wind speed forecasting for coastal engineering applications, addressing the gap in region-specific deep learning frameworks for operational forecasting. The proposed method in this study addresses critical gaps in traditional methods by combining physical constraints with data-driven learning. It presents an innovative framework for wind speed data processing and prediction, integrating deep learning architectures with comprehensive meteorological analysis. Our research implements a sophisticated neural network model that processes high-frequency wind data from Bowen, incorporating multiple environmental parameters through a systematic data pipeline. The methodology encompasses three key components: (1) advanced data preprocessing, including time series standardization and cyclical feature encoding; (2) a deep learning architecture featuring three hidden layers (128-64-32 nodes) with ReLU activation and dropout regularization; and (3) comprehensive performance evaluation using five-fold cross-validation. The model achieved remarkable accuracy metrics: R2 = 0.957, RMSE = 0.449 m/s, demonstrating robust performance across varying weather conditions. Analysis revealed distinct performance patterns across wind speed ranges (low-speed MAE: 0.295 m/s; high-speed MAE: 0.433 m/s). The SHAP (SHapley Additive exPlanations) analysis provided deeper insights into feature importance and model interpretability, revealing Wind Direction (0.713 SHAP value) as the most influential predictor, followed by Relative Humidity (0.609) and Barometric Pressure (0.563). Temporal features (month, hour, and day) exhibited lower but consistent influence (SHAP values < 0.239). This research advances the field of environmental data science by providing: (1) a reproducible framework for wind speed prediction, (2) insights into feature significance and model behavior, and (3) practical applications for renewable energy planning and meteorological forecasting. The demonstrated methodology offers a foundation for future research in environmental modeling and time series prediction.},
  archive      = {J_NCA},
  author       = {Durap, Ahmet},
  doi          = {10.1007/s00521-025-11433-w},
  journal      = {Neural Computing and Applications},
  month        = {9},
  number       = {25},
  pages        = {21219-21257},
  shortjournal = {Neural Comput. Appl.},
  title        = {Explainable deep learning techniques for wind speed forecasting in coastal areas: Integrating model configuration, regularization, early stopping, and SHAP analysis},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Novel approach for crowd counting combining VGG16 and efficientnetb7 for optimal performance in harsh weather. <em>NCA</em>, <em>37</em>(25), 21193-21217. (<a href='https://doi.org/10.1007/s00521-025-11426-9'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Accurate crowd counting in highly congested scenes is essential for public safety and effective resource management. The proposed method utilizes the VGG16 architecture enhanced with pretrained weights from EfficientNetB7 and compares its performance against the VGG16, ResNET50, MCNN, and VGG19 models. These models are trained and tested on the ShanghaiTech Part A and Part B datasets, representing densely and sparsely populated scenes under fog and rain, and bad weather conditions. The philosophy of this paper is based on achieving an optimal balance between enhancing image details and reducing noise to ensure high-quality feature extraction. Introducing a novel approach that uses various preprocessing techniques, including cubic interpolation and sharpening filters, to improve the quality and detail of images. After applying bad, harsh weather environments, such as rain and fog, the study analyzed the effect of preprocessing on five algorithms. The optimum results were achieved when concatenating EfficientNetB7 with VGG16 and using standalone VGG16. In contrast, VGG19 showed the poorest performance. This highlights the effectiveness of certain architectures under preprocessing enhancements. Combining the feature extraction power of EfficientNetB7 with VGG16 helps the model handle different crowd densities better. This makes the system more effective at analyzing crowds with varying numbers of people. Experiments show that the VGG16 model with EfficientNetB7 pretrained weights performs much better than the other five models in calculating error loss and the counting process. On the ShanghaiTech Part A and Part B datasets, it achieved a mean absolute error (MAE) of 102.83 compared to an MAE of 142.26 for VGG16 and an MAE of 173.06 for VGG19. This shows that EfficientNetB7 helps VGG16 handle different crowd scales and densities in complex environments more effectively. This highlights the importance of VGG16 in extracting features and achieving strong performance in the crowd counting process, which can be used as a concatenated with a strong architecture. In addition, Mall datasets scored the best metrics with a mean absolute error (MAE) of 0.78 and a mean square error (MSE) of 1.00549. Additionally, two datasets were used in testing, JHU-CROWD + + + and UCF-QNRF. The models were tested under different weather conditions, including rainy and foggy environments. Overall, the testing results were quite satisfactory.},
  archive      = {J_NCA},
  author       = {Elsepae, Heba F. and El-Rabaie, El-Sayed M. and Hamad, Ehab K. I. and El-Hoseny, Heba M.},
  doi          = {10.1007/s00521-025-11426-9},
  journal      = {Neural Computing and Applications},
  month        = {9},
  number       = {25},
  pages        = {21193-21217},
  shortjournal = {Neural Comput. Appl.},
  title        = {Novel approach for crowd counting combining VGG16 and efficientnetb7 for optimal performance in harsh weather},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Geometric multimodal learning to support prostate cancer diagnosis on limited and multicentric bi-parametric MRI data. <em>NCA</em>, <em>37</em>(25), 21173-21192. (<a href='https://doi.org/10.1007/s00521-025-11413-0'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The classification of clinically significant prostate cancer (csPCa) lesions remains one of the most important challenges in prostate cancer diagnosis. For this, multimodal convolutional neural networks (CNNs) have achieved outstanding results. Nevertheless, the data used in these studies may only partially represent the total burden of csPCa cases. Hence, it is necessary to design reliable models that perform well in limited data scenarios and involving information from different centers (multicentric). A deep Riemannian geometric learning architecture was introduced to capture the intermediate relationships between bi-parametric MRI (bp-MRI) deep representations coded from a 3D multimodal convolutional backbone and considering their geometry. For this, several multimodal bp-MRI fusion strategies were explored to assess their ability to classify csPCa lesions in scenarios where the percentage of available training data was progressively reduced and multicentric data were involved. The proposed method outperformed baseline CNN techniques with an AUC-ROC of 0.96. More remarkably, the method remained stable even only using 10% of the available training data. Additionally, considering multicentric information, this approach also demonstrates generalization ability by losing only 5.4% of the AUC testing data from different acquisition centers, compared to the 10.4% loss of the baseline method. A new deep learning-based method that improves generalization under scenarios with limited data translates to better support for clinicians in accurately classifying csPCa lesions on unseen data.},
  archive      = {J_NCA},
  author       = {Olmos, Juan A. and Manzanera, Antoine and Martínez, Fabio},
  doi          = {10.1007/s00521-025-11413-0},
  journal      = {Neural Computing and Applications},
  month        = {9},
  number       = {25},
  pages        = {21173-21192},
  shortjournal = {Neural Comput. Appl.},
  title        = {Geometric multimodal learning to support prostate cancer diagnosis on limited and multicentric bi-parametric MRI data},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Hardware implementation of a quantized CNN model for early detection of skin cancer cells using hls4ml tool. <em>NCA</em>, <em>37</em>(25), 21147-21171. (<a href='https://doi.org/10.1007/s00521-025-11411-2'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Like many other cancers, early diagnosis of skin cancer plays an essential role in a patient's survival. The survival rate for different types of skin cancer varies significantly. Early diagnosis of malignant lesions, however, could be expensive and challenging. CNNs (convolutional neural networks) have been utilized in many works in recent researches for medical applications, including cancer detection. Research has shown that skin cancer diagnosis based on CNN classifiers can be as accurate as a dermatologists’ diagnosis. In this paper, several different CNN models, all with similar parameters except different filters for each convolution layer, and different neurons for the first fully connected layer, have been implemented on FPGA. The HAM10000 is utilized as the training dataset, and the SMOTE data augmentation method is applied to it. Models have been implemented using the hls4ml tool, which is an open-source software mainly designed to deploy machine learning models on FPGA. The presented models in this study all have been trained in two different floating-point and quantized format pairs, using Keras and Qkeras frameworks, respectively. Afterward, both versions are fed as input to the hls4ml library and synthesized for FPGA implementations. All the models have been compared thoroughly for accuracy and hardware resource consumption. In the end, considering resource consumption and accuracy simultaneously, a quantized model with a software validation accuracy of 95.50% and hardware-emulated accuracy of 94.90% is proposed to be implemented in the stated conditions.},
  archive      = {J_NCA},
  author       = {Arefi, Ehsan and Mousazadeh, Morteza},
  doi          = {10.1007/s00521-025-11411-2},
  journal      = {Neural Computing and Applications},
  month        = {9},
  number       = {25},
  pages        = {21147-21171},
  shortjournal = {Neural Comput. Appl.},
  title        = {Hardware implementation of a quantized CNN model for early detection of skin cancer cells using hls4ml tool},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Extended dissipativity-based finite-time contractive boundedness for delayed discrete-time neural networks via event-triggered approach. <em>NCA</em>, <em>37</em>(25), 21121-21146. (<a href='https://doi.org/10.1007/s00521-025-11399-9'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we introduce a new performance index that examines the extended dissipative criteria in a finite-time contractive boundedness framework for the delayed discrete-time neural networks (DT-NNs). Initially, to avoid unnecessary resource consumption and to govern the information broadcast, the network-induced delay-dependent event-triggered state estimation approach is employed for the considered DT-NNs. In addition, a new discrete inequality for the single summable term is derived based on the generalized free-weighting-matrix inequality and parameter-dependent reciprocally convex inequality. Novel delay-square dependent Lyapunov-based sufficient conditions are employed to obtain the enhanced finite-time extended dissipative performance based on derived summation inequality. Furthermore, an example is provided in both numerical and simulation domains to exemplify the effectiveness of the proposed theoretical approach.},
  archive      = {J_NCA},
  author       = {Adhira, B. and Nagamani, G.},
  doi          = {10.1007/s00521-025-11399-9},
  journal      = {Neural Computing and Applications},
  month        = {9},
  number       = {25},
  pages        = {21121-21146},
  shortjournal = {Neural Comput. Appl.},
  title        = {Extended dissipativity-based finite-time contractive boundedness for delayed discrete-time neural networks via event-triggered approach},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Optimized intrusion detection with deep learning classification models. <em>NCA</em>, <em>37</em>(25), 21091-21119. (<a href='https://doi.org/10.1007/s00521-025-11383-3'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The increasing sophistication of cyber threats necessitates advanced intrusion detection systems (IDS) capable of adaptive and precise threat mitigation. This study presents an optimized deep learning (DL)-based IDS leveraging a deep neural network (DNN) with rectified linear unit (ReLU) activations and a tabular model utilizing the fastai framework. Both models were trained and evaluated on the NSL-KDD dataset following extensive preprocessing, including feature scaling, outlier handling, and class balancing. The fastai model achieved an accuracy of 84.19%, precision of 85.37%, recall of 83.92%, and F1-score of 84.64%, outperforming the DNN, which attained 79.14% accuracy, 81.25% precision, 78.60% recall, and 79.90% F1-score. Automated feature engineering and tenfold cross-validation were applied to enhance generalization and stability. The results demonstrate that deep learning provides a scalable, high-accuracy IDS framework capable of addressing the dynamic and evolving nature of cyber threats.},
  archive      = {J_NCA},
  author       = {Eldakhly, Nabil M.},
  doi          = {10.1007/s00521-025-11383-3},
  journal      = {Neural Computing and Applications},
  month        = {9},
  number       = {25},
  pages        = {21091-21119},
  shortjournal = {Neural Comput. Appl.},
  title        = {Optimized intrusion detection with deep learning classification models},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A generalized leaf disease recognition of various crops and vegetables through computer vision and machine learning. <em>NCA</em>, <em>37</em>(25), 21069-21089. (<a href='https://doi.org/10.1007/s00521-025-11382-4'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Agricultural production is crucial for global economies, yet crop diseases significantly threaten productivity and food security. Traditional manual diagnosis methods are labor-intensive and subjective, leading to inaccurate pesticide applications. Leveraging deep learning, recent advancements have improved disease identification accuracy. However, existing models lack generalization across multiple crops and vegetable diseases. To address this gap, a comprehensive dataset comprising 11 common leaf diseases, such as Anthracnose, Bacterial Blight, Bacterial Spot, Bacterial Wilt, Blast, Downy Mildew, Early Blight, Late Blight, Mosaic, Powdery Mildew, and Rust, affecting 20 widely consumable crops and vegetables, such as rice, wheat, corn, tea, coffee, soybean, potato, tomato, carrot, black gram, pea, cassava, sugarcane, bottle gourd, pepper bell, brinjal, lettuce, cabbage, cauliflower, and cucumber, was curated. Data augmentation techniques expanded the dataset to 16,800 images to enhance the robustness by reducing the overfitting of the deep learning disease recognition model. The dataset is publicly available on GitHub for research purposes. In addition to dataset preparation, this paper introduces a generalized deep learning model for efficient recognition of the leaf diseases of crops and vegetables. Emphasis is placed on optimizing hyperparameters for well-known pre-trained deep learning models, including DenseNet-121, ResNet-50, VGG-16, VGG-19, and Inception-V4. Experimental results using the dataset demonstrate that DenseNet-121 achieved a classification accuracy of 97.58%, surpassing the other models.},
  archive      = {J_NCA},
  author       = {Sultana, Nusrat and Sharmin, Sabrina and Uddin, Mohammad Shorif},
  doi          = {10.1007/s00521-025-11382-4},
  journal      = {Neural Computing and Applications},
  month        = {9},
  number       = {25},
  pages        = {21069-21089},
  shortjournal = {Neural Comput. Appl.},
  title        = {A generalized leaf disease recognition of various crops and vegetables through computer vision and machine learning},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Cultural history optimization algorithm: A new human-inspired metaheuristic algorithm for engineering optimization problems. <em>NCA</em>, <em>37</em>(25), 21009-21068. (<a href='https://doi.org/10.1007/s00521-025-11379-z'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Meta-heuristic optimization methods are popular today, but they still face many problems, such as early convergence, weak scalability, and high computing cost. As engineering problems grow larger and more complex, the need for an optimizer that can search broadly, converge quickly, and keep the computation affordable becomes even more urgent. To address these issues, this paper introduces a novel human inspired metaheuristic algorithm, the cultural history optimization algorithm (CHOA), based on cultural history principles. CHOA’s performance is evaluated against 47 benchmark functions and the CEC06−2019 test suite, encompassing large-scale unimodal, multimodal, and fixed-dimension functions. Results demonstrate CHOA’s strong exploration and exploitation capabilities, achieving global optima with rapid convergence and manageable computational cost. Performance metrics, including mean cost, standard deviation, convergence acceleration, and computational burden, are compared with established metaheuristics, highlighting effectiveness. Moreover, Wilcoxon rank-sum tests confirm CHOA’s statistical superiority. As a large-scale design optimization problem, CHOA and state-of-the-art algorithms are applied to optimize a permanent magnet synchronous motor, showcasing CHOA’s local optima avoidance and scalability. Finally, the paper describes a graphical user interface (GUI) developed for CHOA to facilitate its practical application.},
  archive      = {J_NCA},
  author       = {Sharifi, Tohid and Mirsalim, Mojtaba and Soleimanian Gharehchopogh, Farhad and Mirjalili, Seyedali},
  doi          = {10.1007/s00521-025-11379-z},
  journal      = {Neural Computing and Applications},
  month        = {9},
  number       = {25},
  pages        = {21009-21068},
  shortjournal = {Neural Comput. Appl.},
  title        = {Cultural history optimization algorithm: A new human-inspired metaheuristic algorithm for engineering optimization problems},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Reinforcement learning-based decision support system for an agile humanitarian relief chain. <em>NCA</em>, <em>37</em>(25), 20983-21007. (<a href='https://doi.org/10.1007/s00521-025-11352-w'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Rapid item distribution to applicants, increasing decision-making speed, improving decision-making quality, and aiding in planning and prioritizing central distribution point establishment. During severe crises like earthquakes, decisions on locating, allocating, and distributing vital items are crucial for humanitarian relief managers. Items are dispatched to demand areas via central distribution points. Due to limited resources during crises, all central distribution points cannot be set up simultaneously. Hence, prioritizing their establishment is essential. Setting up these points significantly enhances response time and service quality. Agile relief systems are pivotal in improving service quality. Without decision-making tools, comprehensive planning for such issues is challenging. Hence, this study aims to provide a reinforcement learning-based hybrid decision support system for humanitarian relief chains during the crisis to support us in decision-making based on environmental conditions. In this instance, there will be a boost in the velocity and efficiency of the decision-making process. The Q-learning method is the core of processing and computations. The Q-learning method was compared with a random walk, ε -greedy, and simulated annealing algorithms for a simulated problem with high iterations. The comparison results indicate that the algorithm under analysis provides proper efficiency in the routing process. This algorithm was used as the major core of the study to design a hybrid decision support system for earthquake crises.},
  archive      = {J_NCA},
  author       = {Javadi, Babak and Noori, Hossein and Aghaabdollahian, Behnaz},
  doi          = {10.1007/s00521-025-11352-w},
  journal      = {Neural Computing and Applications},
  month        = {9},
  number       = {25},
  pages        = {20983-21007},
  shortjournal = {Neural Comput. Appl.},
  title        = {Reinforcement learning-based decision support system for an agile humanitarian relief chain},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Aftina: Enhancing stability and preventing hallucination in AI-based islamic fatwa generation using LLMs and RAG. <em>NCA</em>, <em>37</em>(25), 20957-20982. (<a href='https://doi.org/10.1007/s00521-025-11229-y'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Question–answering (QA) systems face considerable challenges when involved in Islamic fatwas due to the complexity and sensitivity of the data. Such problems involve providing accurate and reliable responses, managing hallucinations and inaccurate responses, and maintaining the stability of the generated responses. Prior studies have concentrated mainly on collecting and preprocessing Islamic datasets or developing retrieval-based QA systems, overlooking the precision and reliability required for fatwa issuance. To address this issue, we propose a QA approach utilizing advanced retrieval-augmented generation (RAG), which is enhanced by a re-ranker to increase response stability, eliminate hallucinations, and prioritize the most appropriate and exact answer. This enhancement significantly improves response stability and reduces hallucinations by improving the data used for answer generation. We conducted experiments across three setups: (1) base LLM, (2) LLM with RAG, and (3) LLM with RAG and re-ranker. The third method of LLM with RAG includes a re-ranker for knowledge retrieval, which improves the process and ensures relevant and trustworthy data. This differentiates it from the second method, which uses a retrieval model. The Flash re-ranker retrieves the most relevant data, which increases the response stability and trustworthiness. Evaluations using BERTScore, hallucination, completeness, and irrelevance metrics demonstrated that the third experiment LLM with RAG and re-ranker outperformed other setups, providing precise, stable, and dependable answers. This research contributes a robust methodology to improve AI-driven fatwa systems, guaranteeing higher precision and trustworthiness in Islamic QA systems.},
  archive      = {J_NCA},
  author       = {Mohammed, Marryam Yahya and Ali, Sama Ayman and Ali, Salma Khaled and Majeed, Ayad Abdul and Mohamed, Ensaf Hussein},
  doi          = {10.1007/s00521-025-11229-y},
  journal      = {Neural Computing and Applications},
  month        = {9},
  number       = {25},
  pages        = {20957-20982},
  shortjournal = {Neural Comput. Appl.},
  title        = {Aftina: Enhancing stability and preventing hallucination in AI-based islamic fatwa generation using LLMs and RAG},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A panorama of text summarization research: Bibliometric trends and developments (2000–2024). <em>NCA</em>, <em>37</em>(25), 20917-20956. (<a href='https://doi.org/10.1007/s00521-025-11562-2'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A detailed bibliometric analysis of text summarization research from 2000 to 2024 is performed in this study. It tracks the growth of the field, identifies key contributors, and examines collaborative networks that have shaped the development of text summarization technologies. The analysis shows a significant rise in publication volume, particularly in the last decade, driven by advancements in natural language processing and AI. China, India, and the USA emerge as major contributors, with notable institutions and researchers leading the field. Research themes have evolved from foundational topics to advanced AI-driven approaches like deep learning and abstractive summarization. The study also highlights the global and collaborative nature of research, with extensive partnerships across institutions and countries. The findings offer valuable insights into the current state and future directions of text summarization research, providing a solid foundation for further exploration and development. Data are gathered from major academic databases like Scopus, ACM, and IEEE Xplore and filtered through a rigorous selection process to create a comprehensive dataset. Bibliometric methods analyzed publication trends, key sources, and geographical contributions. The structural topic model identified principal research themes, while the Mann–Kendall test examined trends in these themes over time. Social network analysis visualized collaborations among scholars and institutions. Key research themes include multihead attention mechanisms, graph-based semantic analysis, and topic modeling techniques, with emerging trends highlighting interest in self-supervised learning, zero-shot learning, and transformer models. This study offers valuable insights and serves as a useful resource for researchers and practitioners, enhancing the understanding of current and future directions in text summarization.},
  archive      = {J_NCA},
  author       = {Kumari, Namrata and Singh, Pardeep},
  doi          = {10.1007/s00521-025-11562-2},
  journal      = {Neural Computing and Applications},
  month        = {9},
  number       = {25},
  pages        = {20917-20956},
  shortjournal = {Neural Comput. Appl.},
  title        = {A panorama of text summarization research: Bibliometric trends and developments (2000–2024)},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Auto language identifier and text aligner using neural network. <em>NCA</em>, <em>37</em>(25), 20897-20916. (<a href='https://doi.org/10.1007/s00521-025-11390-4'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Natural Language Processing aims to facilitate computers to comprehend and refine human languages for some real-world as well as valuable objectives. While thousands of languages are spoken worldwide, language translation has been an utmost demanding and thought-provoking area of research. Researchers have been fruitful not only in representing these languages using machines but also in examining the elementary structure of these languages as reported by Niraj Aswani (in: Aligning sentences and words using English–Hindi bilingual parallel corpora). This research aims to develop a bilingual automatic AI-based text aligner using neural networks. The system inputs a text file from the user which comprises bilingual miscellaneous sentences. The bilingual text is processed based on the text sentences. The.csv file downloaded as the output consists of sentences placed adjacent to their equivalent sentences. In this, we will be working primarily with two languages English and Hindi. In this paper, we present a method for aligning English sentences with their corresponding Hindi translations at the sentence level, utilizing natural language processing and AI techniques. This approach aims to address a significant challenge in developing language models for various Indian languages, primarily due to the scarcity of aligned parallel bilingual data. In the results section, we will demonstrate the accuracy and efficiency of these models for English and Hindi, with potential applications for other Indian languages as well. The methodologies described above are typically based on either sentence length or word correspondences. Sentence-length-based approaches are generally faster and offer reasonable accuracy, while word correspondence methods tend to be more precise but are significantly slower, often relying on cognates or a bilingual lexicon. Our technique synthesizes and enhances these approaches, creating a system designed to align sentences in an English–Hindi corpus. This method achieves high accuracy at a relatively low computational cost, aiming to produce large-scale, high-quality aligned sentences between English and Hindi.},
  archive      = {J_NCA},
  author       = {Singh, Shashi Pal and Tiwari, Ritu and Sharma, Sanjeev},
  doi          = {10.1007/s00521-025-11390-4},
  journal      = {Neural Computing and Applications},
  month        = {9},
  number       = {25},
  pages        = {20897-20916},
  shortjournal = {Neural Comput. Appl.},
  title        = {Auto language identifier and text aligner using neural network},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). HFCANet: Heterogeneous feature driven cascade association network for multiple object tracking. <em>NCA</em>, <em>37</em>(25), 20879-20895. (<a href='https://doi.org/10.1007/s00521-025-11518-6'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Existing multi-object tracking (MOT) methods integrate object motion and appearance features to improve tracking performance. However, these methods often exhibit sub-optimal tracking performance when facing objects with diverse poses, similar appearances, or nonlinear motions. Especially in crowded scenarios, the frequent occlusion between objects leads to an increase in the discontinuity of the trajectories, thus lagging the tracking performance. Motivated by this, we propose a heterogeneous feature driven cascade association network (HFCANet). Specifically, we design a triple heterogeneous feature extraction (THFE) module to capture more discriminative features of objects, which can realize accurate object representation even in the presence of drastic changes in scale, pose, and background. In addition, to retrieve the fragmented trajectory of the occluded object for more complete tracking, we introduce an easy-to-hard cascade association policy that analyzes heterogeneous cues to determine whether the detected object can be re-corrected into the trajectory. HFCANet comprehensively considers the heterogeneous cues of objects, encompassing the nonlinear motion and the richness of limb movements. This characteristic makes it especially well-suited for fulfilling tracking requirements in scenes with dense pedestrians and frequent occlusions. Extensive experiments and ablation studies conducted on MOT17, MOT20, and DanceTrack demonstrate the effectiveness of our proposed HFCANet, where ours has achieved better performance compared to existing state-of-the-art (SOTA) methods.},
  archive      = {J_NCA},
  author       = {Li, Hui and Guo, Ying and Qin, Su and Li, Rui and Gao, Ying},
  doi          = {10.1007/s00521-025-11518-6},
  journal      = {Neural Computing and Applications},
  month        = {9},
  number       = {25},
  pages        = {20879-20895},
  shortjournal = {Neural Comput. Appl.},
  title        = {HFCANet: Heterogeneous feature driven cascade association network for multiple object tracking},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A survey on 1-bit quantized large language models. <em>NCA</em>, <em>37</em>(25), 20823-20878. (<a href='https://doi.org/10.1007/s00521-025-11529-3'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Large language models (LLMs) have gained significant popularity, with various models demonstrating different domains and intelligence. Deep learning methods, especially neural networks, are used to build LLMs, and their training requires enormous volumes of data. They have significantly advanced natural language processing (NLP), enabling applications like chatbots, virtual assistants, language translation, and content generation. However, training LLMs is computationally intensive due to large model sizes, extensive datasets, iterative processes, specialized hardware, and high energy consumption. To address these challenges, quantization has been introduced. This process reduces the precision of numerical values, such as weights and activations, thereby decreasing memory and computational requirements. But this technique can affect model performance negatively, therefore, recent research focuses on minimizing accuracy loss. Techniques like mixed precision training and adaptive quantization have been developed to balance efficiency and performance. This paper surveys the existing 1-bit quantization approaches, providing insights and recommendations for future work. The goal is to enable the efficient and cost-effective deployment of LLMs without compromising their performance, broadening their accessibility and applicability.},
  archive      = {J_NCA},
  author       = {Tripathi, Kritika and Malik, Devanshi and Akshat, Abhi and Lata, Kusum},
  doi          = {10.1007/s00521-025-11529-3},
  journal      = {Neural Computing and Applications},
  month        = {9},
  number       = {25},
  pages        = {20823-20878},
  shortjournal = {Neural Comput. Appl.},
  title        = {A survey on 1-bit quantized large language models},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Best practices for responsible machine learning in credit scoring. <em>NCA</em>, <em>37</em>(25), 20781-20821. (<a href='https://doi.org/10.1007/s00521-025-11520-y'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The widespread use of machine learning in credit scoring has brought significant advancements in risk assessment and decision-making. However, it has also raised concerns about potential biases, discrimination, and lack of transparency in these automated systems. This tutorial paper performed a non-systematic literature review to guide best practices for developing responsible machine learning models in credit scoring, focusing on fairness, reject inference, and explainability. We discuss definitions, metrics, and techniques for mitigating biases and ensuring equitable outcomes across different groups. Additionally, we address the issue of limited data representativeness by exploring reject inference methods that incorporate information from rejected loan applications. Finally, we emphasize the importance of transparency and explainability in credit models, discussing techniques that provide insights into the decision-making process and enable individuals to understand and potentially improve their creditworthiness. By adopting these best practices, financial institutions can harness the power of machine learning while upholding ethical and responsible lending practices.},
  archive      = {J_NCA},
  author       = {Valdrighi, Giovani and M. Ribeiro, Athyrson and S. B. Pereira, Jansen and Guardieiro, Vitoria and Hendricks, Arthur and Miranda Filho, Décio and Nieto Garcia, Juan David and F. Bocca, Felipe and B. Veronese, Thalita and Wanner, Lucas and Medeiros Raimundo, Marcos},
  doi          = {10.1007/s00521-025-11520-y},
  journal      = {Neural Computing and Applications},
  month        = {9},
  number       = {25},
  pages        = {20781-20821},
  shortjournal = {Neural Comput. Appl.},
  title        = {Best practices for responsible machine learning in credit scoring},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Enhanced deep-style interpreter for automatic synthesis of annotated medical images. <em>NCA</em>, <em>37</em>(25), 20755-20780. (<a href='https://doi.org/10.1007/s00521-025-11516-8'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Creating an annotated medical image dataset is challenging and traditionally reliant on labor-intensive manual annotations. Additionally, these datasets often present substantial imbalances regarding sensing devices, class of medical disorders, and patient ethnicity and phenotype. Recently, there has been a research interest in mitigating these issues by employing data augmentation with generative models. However, the quality of images and semantics in medical image datasets are critical for computer vision tasks such as image segmentation. This paper presents DatasetGAN2-ADA, which aims to mitigate these difficulties by presenting an innovative deep-style interpreter robust against anomalous synthesis and designed to automate annotated image generation entirely. By leveraging the capabilities of StyleGAN2-ADA with an improved architecture of DatasetGAN and an enhanced execution framework integrated with an anomaly detector based on custom features, we propose a combined strategy for eliminating flawed synthetic images and masks. Furthermore, we propose exploiting image projections and preexisting semantics, eliminating the need for manual annotations to train our deep-style interpreter. The experimental results obtained with a magnetic resonance image (MRI) dataset demonstrate that DatasetGAN2-ADA is strongly effective in improving the efficiency and quality of synthetic generation, rejecting the synthesis of a substantial amount of low-quality images and masks. Then, an extension of this method is evaluated for detecting anomalous latent vectors a priori of the image synthesis, achieving up to 95.24% precision and illustrating its compelling potential for practical applications in medical imaging.},
  archive      = {J_NCA},
  author       = {Pacheco dos Santos Lima Junior, Marcos Sergio and Ortiz-de-Lazcano-Lobato, Juan Miguel and Fernández-Rodríguez, José David and López-Rubio, Ezequiel},
  doi          = {10.1007/s00521-025-11516-8},
  journal      = {Neural Computing and Applications},
  month        = {9},
  number       = {25},
  pages        = {20755-20780},
  shortjournal = {Neural Comput. Appl.},
  title        = {Enhanced deep-style interpreter for automatic synthesis of annotated medical images},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A systematic literature review on the application of artificial intelligence techniques for rock strength estimation. <em>NCA</em>, <em>37</em>(25), 20721-20753. (<a href='https://doi.org/10.1007/s00521-025-11517-7'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper presents a systematic literature review on the prediction of unconfined compressive strength (UCS) and elastic modulus (E) with artificial intelligence (AI) models. The study categorises three essential parts: (1) a combination of physical and mechanical properties, (2) mechanical properties, and (3) physical properties as input parameters for AI models in estimating UCS and E. The review selection was based on search keywords using title-abstract, full-text, and keywords from Scopus and Web of Science online database libraries. A total of 131 peer-reviewed research articles published from 2014 to 2024 were critically reviewed to provide answers to research-related questions related to current advancements in the prediction of UCS and E with AI models. Among the AI technologies analysed, artificial neural networks (ANN) and ANN-based models stand out as the most used AI algorithms; other algorithms, including ANFIS, RF, SVM, and XGBoost model, have been used at significant levels in predicting UCS and E with high prediction accuracy of R2 greater 0.90 with minimum mean error margins. The ANN (24.7%), ANFIS (11.7%), and RF (7.6%) have been essentially employed in many research studies to predict rock strength. The study combined mechanical and physical properties with AI models at approximately 59%, and after that, mechanical properties at 23.6%. The efficiency of AI algorithms and their application is associated with the usage of data and input parameters. This review recommends future study gaps and places emphasis on integrating rock mechanics, physical laws (Mohr–Coulomb and Hoek–Brown failure criteria) and adaptive AI techniques to advance the adaptability and reliability in predicting rock strength and deformation characteristics.},
  archive      = {J_NCA},
  author       = {Akosah, Stephen and Gratchev, Ivan and Gidigasu, Solomon S. R.},
  doi          = {10.1007/s00521-025-11517-7},
  journal      = {Neural Computing and Applications},
  month        = {9},
  number       = {25},
  pages        = {20721-20753},
  shortjournal = {Neural Comput. Appl.},
  title        = {A systematic literature review on the application of artificial intelligence techniques for rock strength estimation},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Effective non-random extreme learning machine. <em>NCA</em>, <em>37</em>(25), 20691-20719. (<a href='https://doi.org/10.1007/s00521-025-11519-5'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The extreme learning machine (ELM) is a growing statistical technique widely applied to regression problems. In essence, ELMs are single-layer neural networks where the hidden layer weights are randomly sampled from a specific distribution, while the output layer weights are learned from the data. Two of the key challenges with this approach are the architecture design, specifically determining the optimal number of neurons in the hidden layer, and the method’s sensitivity to the random initialization of hidden layer weights. This paper introduces a new and enhanced learning algorithm for regression tasks, the Effective Non-Random ELM (ENR-ELM), which simplifies the architecture design and eliminates the need for random hidden layer weight selection. The proposed method incorporates concepts from signal processing, such as basis functions and projections, into the ELM framework. We introduce two versions of the ENR-ELM: the approximated ENR-ELM and the incremental ENR-ELM. Experimental results on both synthetic and real datasets demonstrate that our method overcomes the problems of traditional ELM while maintaining comparable predictive performance.},
  archive      = {J_NCA},
  author       = {De Canditiis, Daniela and Veglianti, Fabiano},
  doi          = {10.1007/s00521-025-11519-5},
  journal      = {Neural Computing and Applications},
  month        = {9},
  number       = {25},
  pages        = {20691-20719},
  shortjournal = {Neural Comput. Appl.},
  title        = {Effective non-random extreme learning machine},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A novel medical image security and compression based on multiple-order fractional quaternion hahn moments and 2D-chaotic map. <em>NCA</em>, <em>37</em>(25), 20663-20690. (<a href='https://doi.org/10.1007/s00521-025-11523-9'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Objectives: Medical image analysis is essential for quick and accurate diagnosis and treatment. Nowadays, the uses of medical images for the treatment of patients and diagnosis purposes are sent over the internet. So, they should be protected from cyber attackers. These medical images are sensitive to any minor changes, and the data volume is rapidly increasing. Thus, security and storage costs must be considered in medical images. Traditional encryption and compression methods are ineffective for encrypting medical images due to their high execution time and algorithm complexity. Methods: This paper proposes a novel 2D chaotic map and generates the GS sequence in the multiple-order fractional quaternion Hahn moments matrix for generating encryption keys and improving security. The proposed algorithm uses the 2D chaotic map to generate the private key and diffusion process. The pixel values of the original images in the proposed schemes are shuffled using Mersenne Twister (MT) to improve the security of medical images. In this proposed scheme, the Differential Huffman Compression (DHC) method is used for lossless compression while performing XOR-based encryption. Findings: The proposed model has been tested on different color medical images, namely the Computed Tomography (CT) dataset, and Magnetic Resonance Imaging (MRI) dataset. It has been evaluated using performance metrics, such as entropy, key space, histogram analysis, key sensitivity, robustness analysis, correlation, and similarity analysis. The outcomes demonstrate that the proposed scheme is more effective than the other comparable schemes. Novelty: This research pioneers the study of innovative medical image security and compression techniques, married with methods development based on multiple-order fractional quaternion Hahn moments, 2D-Chaotic Map, and DHC for handling the storage cost of medical images. Our findings highlight a crucial aspect of healthcare namely the secure and efficient transfer of medical data, specifically between the radiology department and radiologists. This is vital for patient care, diagnostic accuracy, and maintaining data privacy.},
  archive      = {J_NCA},
  author       = {El Ogri, Omar and EL-Mekkaoui, Jaouad and Benslimane, Mohamed},
  doi          = {10.1007/s00521-025-11523-9},
  journal      = {Neural Computing and Applications},
  month        = {9},
  number       = {25},
  pages        = {20663-20690},
  shortjournal = {Neural Comput. Appl.},
  title        = {A novel medical image security and compression based on multiple-order fractional quaternion hahn moments and 2D-chaotic map},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). An explainable deep learning-based panoptic segmentation for brain tumor diagnosis. <em>NCA</em>, <em>37</em>(25), 20639-20662. (<a href='https://doi.org/10.1007/s00521-025-11459-0'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Brain tumor segmentation (BTS) is a critical task for the accurate diagnosis and treatment of brain tumors. Manual detection poses significant challenges due to the complex anatomy and the variations in tumor types, sizes, shapes, and locations. Computer-aided diagnostic techniques have gained popularity for assisting healthcare providers in diagnosing diseases and improving the consistency and accuracy of their findings. This study introduces a novel hybrid PA-ResNet50 deep learning approach for explainable panoptic brain tumor segmentation (PBTS), integrating both instance and semantic segmentation to achieve detailed tumor boundary delineation while addressing uncertainties in brain imaging. Unlike conventional segmentation methods, our approach leverages panoptic segmentation with uncertainty modeling, enhancing both interpretability and robustness. The primary objective is to reduce the uncertainties in brain imaging, thereby increasing tumor identification accuracy and boosting the confidence of medical professionals. Evaluation results demonstrate that the proposed framework enhances both the interpretability of the results and the precision of brain tumor segmentation. Our model achieved an accuracy of 99.3 and 98.7%, Dice scores of 99.29 and 98.85% and computational times of 13 and 27 s for the BraTS 2019 and BraTS 2021 datasets, respectively. This method not only improves segmentation precision but also enhances the interpretability and reliability of tumor diagnoses, providing a trustworthy, explainable AI-driven solution for clinical decision-making.},
  archive      = {J_NCA},
  author       = {Shaheema, Berlin and Muppalaneni, Naresh Babu and Devi, K. Suganya},
  doi          = {10.1007/s00521-025-11459-0},
  journal      = {Neural Computing and Applications},
  month        = {9},
  number       = {25},
  pages        = {20639-20662},
  shortjournal = {Neural Comput. Appl.},
  title        = {An explainable deep learning-based panoptic segmentation for brain tumor diagnosis},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). An integrated biomedical images security approach to secure healthcare system. <em>NCA</em>, <em>37</em>(25), 20617-20637. (<a href='https://doi.org/10.1007/s00521-025-11389-x'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Smart healthcare holds immense potential to revolutionize the healthcare industry, promoting patient-centric care, preventive medicine, and improved health outcomes. By harnessing the power of advanced technologies and data-driven solutions, healthcare providers can deliver more efficient, accessible, and personalized healthcare services, ultimately transforming the approach and experience of healthcare. Biomedical images have now become the new support for better diagnosis in the medical field. In the area of image security, perceptual hashing provides a powerful approach to enhancing the security of medical images by creating compact and unique representations of their visual content. This paper proposes a framework for smart healthcare where biomedical images are secured with perceptual hashing. In this framework, an authentication module is also deployed to verify the identity of smart users allowed to access the biomedical images over the edge or cloud layers. The performance analysis of the hashing module is evaluated using the structural similarity index measure (SSIM), peak signal-to-noise ratio (PSNR), bit error rate (BER), and Hausdorff distance. Additionally, the performance analysis of the authentication module is evaluated in terms of a system accuracy of 89% and a probability of identification of 0.45 to establish authentication.},
  archive      = {J_NCA},
  author       = {Shreya, Shashi and Chatterjee, Kakali and Singh, Ashish},
  doi          = {10.1007/s00521-025-11389-x},
  journal      = {Neural Computing and Applications},
  month        = {9},
  number       = {25},
  pages        = {20617-20637},
  shortjournal = {Neural Comput. Appl.},
  title        = {An integrated biomedical images security approach to secure healthcare system},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). TLBEMSE: Design of a transfer learning-based bioinspired ensemble model for preemptive detection of stress and emotional disorders. <em>NCA</em>, <em>37</em>(25), 20591-20616. (<a href='https://doi.org/10.1007/s00521-025-11160-2'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Electroencephalogram signals are used to depict emotional and stress disorders. To overcome issues of existing models, novel transfer learning-based bioinspired ensemble model for preemptive detection of stress and emotional disorders is discussed. The proposed model includes features of mel-frequency cepstral coefficient, iVector, cosine, Fourier and wavelet components. A combination of these features is processed via gray wolf optimization which aims at variance maximization across. The selected features are converted into 2D representation and processed via a transfer learning-based convolutional neural network model combining ResNet 101, MobileVNet, and YoLo models. The classified results from these models are further cross-validated via use of ensemble classification that combines Naïve Bayes, support vector machine, random forest, logistic regression, and multilayer perceptron models. These classifiers perform several post-processing tasks involving identification of disease spread probability, estimation of future diseases, etc. The proposed model was trained on DEAP and interface datasets, compared w.r.t. various state-of-the-art methods, in relation to accuracy, recall, precision, area under the curve, and delay performance. Based on this performance, proposed model’s effectiveness was noticed, showcasing 8.5% higher accuracy, 8.3% higher precision, 5.9% better recall, 4.5% better AUC, and 14.9% faster classification performance, which makes it highly useful for clinical deployments.},
  archive      = {J_NCA},
  author       = {Hole, Komal Rajendra and Anand, Divya and Mohanty, Sachi Nandan and Rathore, Rajkumar Singh and lvarez, Roberto Marcelo},
  doi          = {10.1007/s00521-025-11160-2},
  journal      = {Neural Computing and Applications},
  month        = {9},
  number       = {25},
  pages        = {20591-20616},
  shortjournal = {Neural Comput. Appl.},
  title        = {TLBEMSE: Design of a transfer learning-based bioinspired ensemble model for preemptive detection of stress and emotional disorders},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Enhancing patient-independent detection of freezing of gait in parkinson’s disease with deep adversarial network. <em>NCA</em>, <em>37</em>(25), 20569-20589. (<a href='https://doi.org/10.1007/s00521-025-11068-x'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Freezing of gait (FoG) refers to sudden, relatively brief episodes of gait arrest in Parkinson’s disease, known to manifest in the advanced stages of the condition. Events of freezing are associated with tumbles, traumas, and psychological repercussions, significantly impacting the patient’s quality of life. The use of accelerometer data derived from sensors put on a patient’s body to detect FoG has previously been proposed using convolutional neural network (CNN)-based deep learning algorithms. Here, we combine a CNN + Long short-term Memory (LSTM)-attention model to detect FoG episodes—a first for the detection of FoG using the accelerometer data. CNN facilitates automatic feature extraction from the accelerometer data itself. The output from the CNN is fed to the LSTM network, which is known for capturing sequential information. Further, the attention mechanism introduces relative focus on individual sub-sequences during the training of the LSTM network. The proposed model is made patient independent using adversarial training. The proposed model achieved improvements of +7.60% (without adversarial training) and + 8.36% (with adversarial training) in the sensitivity values over state of the art. This is obtained with little or no compromise in specificity values. The corresponding improvements in accuracy values are +3.81 and +2.23%, respectively. These findings imply that the proposed method can detect FoG gaits satisfactorily and can be effective in achieving accurate monitoring and gait assistance for Parkinson’s disease patients during daily life and rehabilitation therapy.},
  archive      = {J_NCA},
  author       = {Fahad, Md Shah and Ranjan, Ashish and Kumar, Gautam},
  doi          = {10.1007/s00521-025-11068-x},
  journal      = {Neural Computing and Applications},
  month        = {9},
  number       = {25},
  pages        = {20569-20589},
  shortjournal = {Neural Comput. Appl.},
  title        = {Enhancing patient-independent detection of freezing of gait in parkinson’s disease with deep adversarial network},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). MFI3D: Masked face identification with 3D face reconstruction and deep learning. <em>NCA</em>, <em>37</em>(25), 20551-20567. (<a href='https://doi.org/10.1007/s00521-024-10582-8'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Masked face identification (MFI) aims to identify human faces that are obscured by masks or occluding objects. Today’s common practice of wearing masks imposes significant barriers to facial recognition systems that rely on an unobstructed view of the face. To address this challenge, this paper introduces MFI 3D-based deep learning model (MFI3D) to identify occluded faces. Three main architectural components provide a set of discriminant features to decide the face identity, which are synthetic face masking, face unmasking with generator and discriminator, and 3D face reconstruction. The MFI3D pipeline begins by creating a syntactic mask for the face image, simulating real-world situations where the face is partially covered by the mask. Then, effective face detection is applied using a generator that learns to generate unmasked images that are indistinguishable from true unmasked images, and a discriminator that learns to distinguish real images from fake images. As a result, the MFI model can learn to reconstruct facial features from partially masked faces. The use of 3D face reconstruction techniques to generate a detailed model of faces leverages 3D geometry to extract facial features that are not visible in 2D image, providing a superior visual facial representation. Finally, the reconstructed face is matched against a collection of known people to determine their identity. Extensive experiments were conducted on facial datasets reconstructed orderly to build a diverse collection of 3D reconstructed facial images with a benchmarking ground truth. The experimental results show the superiority of the proposed MFI3D model in identifying people with occluded faces, achieving a precision of 83.50%.},
  archive      = {J_NCA},
  author       = {Alzu’bi, Ahmad and Albalas, Firas and Al-Hadhrami, Tawfik and Albashayreh, Amjad and Younis, Lojin Bani},
  doi          = {10.1007/s00521-024-10582-8},
  journal      = {Neural Computing and Applications},
  month        = {9},
  number       = {25},
  pages        = {20551-20567},
  shortjournal = {Neural Comput. Appl.},
  title        = {MFI3D: Masked face identification with 3D face reconstruction and deep learning},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Spatiotemporal context feedback bidirectional attention network for breast cancer segmentation based on DCE-MRI. <em>NCA</em>, <em>37</em>(25), 20535-20549. (<a href='https://doi.org/10.1007/s00521-024-10528-0'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Breast cancer is a highly heterogeneous, both between patients (inter-tumor) and within individual tumors (intra-tumor), leads to indistinct boundaries and varying sizes, shapes, appearances and densities of tumors. Current 3D structural imaging-based methods face challenges to segment breast cancer. The key ingredient to the problem is how to exploit the temporal correlations of 4D functional imaging that depict the heterogeneity of vascular permeability in cancer for accurate tumor segmentation. In this paper, we propose a unique spatiotemporal context feedback bidirectional attention network, which segments breast cancer by modeling dynamic contrast-enhanced dependency to exploit pharmacokinetics feature representations. Specifically, we design a temporal context feedback encoder to learn pharmacokinetics feature representations, which embeds bidirectional temporal attention for bidirectionally propagating contextual semantics across time sequences. Additionally, learned representations are fed into a temporal context feedback decoder to obtain a voxel-level classification of breast tumors. Experimental results demonstrated that the proposed method outperforms recent tumor segmentation methods. Furthermore, our approach achieves competitive results on a small training data and avoids the over-fitting phenomenon due to the model-driven skill to capture dynamic contrast-enhanced temporal correlations.},
  archive      = {J_NCA},
  author       = {Pan, Xiang and Lv, Tianxu and Liu, Yuan and Li, Ningjun and Li, Lihua and Zhang, Yan and Ni, Jianming and Jiang, Chunjuan},
  doi          = {10.1007/s00521-024-10528-0},
  journal      = {Neural Computing and Applications},
  month        = {9},
  number       = {25},
  pages        = {20535-20549},
  shortjournal = {Neural Comput. Appl.},
  title        = {Spatiotemporal context feedback bidirectional attention network for breast cancer segmentation based on DCE-MRI},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Lung disease classification using deep learning and genetic algorithm. <em>NCA</em>, <em>37</em>(25), 20519-20534. (<a href='https://doi.org/10.1007/s00521-024-10527-1'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Lung disorders are medical conditions that disrupt the lungs and their capacity to function normally. One fatal lung disease is a collapsed lung where the lung collapses partially or fully due to diseases like pneumothorax and atelectasis. The conventional approach to detecting such diseases is time-consuming, often requiring extensive manual analysis by trained experts, leading to delays in diagnosis and treatment. Computer-aided diagnostics have the potential to aid doctors in enhancing the consistency of diagnoses while also optimizing time efficiency. In this study, we enhance each lung X-ray image with three image enhancement techniques (i) contrast-limited adaptive histogram equalization (CLAHE), (ii) discrete wavelet transform (DWT), and (iii) gamma correction (GC) in parallel. A 3-channel convolutional neural network (CNN) then uses those enhanced images to extract features. The extracted features are further optimized using a genetic algorithm to improve the efficiency of the classification models. Our proposed model is validated with a dataset containing 9391 X-ray images to achieve an average precision, recall, and F1-score of 98, 97, and 98% and best accuracy of 99.33% which shows an improvement of 2.03% in terms of accuracy over the existing state-of-the-art methods.},
  archive      = {J_NCA},
  author       = {Chutia, Upasana and Tewari, Anand Shanker and Singh, Jyoti Prakash},
  doi          = {10.1007/s00521-024-10527-1},
  journal      = {Neural Computing and Applications},
  month        = {9},
  number       = {25},
  pages        = {20519-20534},
  shortjournal = {Neural Comput. Appl.},
  title        = {Lung disease classification using deep learning and genetic algorithm},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Retinal fundus image enhancement using an ensemble framework for accurate glaucoma detection. <em>NCA</em>, <em>37</em>(25), 20499-20517. (<a href='https://doi.org/10.1007/s00521-024-10500-y'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Retinal fundus imaging plays a crucial role in the diagnosis of ophthalmic diseases such as glaucoma, a significant cause of vision loss worldwide. Accurate detection of glaucoma using image processing, machine learning, and deep learning approaches depends on the effectiveness with which the retinal fundus images are captured. Poor-quality images with artifacts, including uneven illumination, blur, and color distortion, can lead to incorrect diagnoses. In this work, we propose an end-to-end glaucoma detection model based on the ensemble of image enhancement networks, segmentation networks, and image classification networks. The proposed approach consists of an improved version of generative adversarial network (GAN) called the cycle consistency GAN (cycle-GAN) for image quality enhancement, U-Net for optic cup and optic disc segmentation, and support vector machine for image classification. The cycle-GAN model uses autoencoders as generators and a deep convolutional neural network (CNN) as discriminators to generate high-quality fundus images. The cup-to-disc ratio, a popular feature, is utilized to categorize fundus images as either glaucomatous or non-glaucomatous. We use six imbalanced datasets for experimental analysis of the proposed ensemble model, including ORIGA, ACRIMA, DRISTI-GS, REFUGE, Messidor, and Mendeley. The experimental findings demonstrate that the proposed ensemble model works better than individual models such as GAN, Autoencoder, deep CNN, and also from existing methods. The proposed method not only reduces the artifacts from fundus images but also solves the problem of imbalanced datasets for accurate glaucoma detection. The experimental results show maximum accuracy, precision, recall, and F-measure values of 0.968, 0.821, 0.974, and 0.891, respectively.},
  archive      = {J_NCA},
  author       = {Lenka, Satyabrata and Mayaluri, Zefree Lazarus and Panda, Ganapati},
  doi          = {10.1007/s00521-024-10500-y},
  journal      = {Neural Computing and Applications},
  month        = {9},
  number       = {25},
  pages        = {20499-20517},
  shortjournal = {Neural Comput. Appl.},
  title        = {Retinal fundus image enhancement using an ensemble framework for accurate glaucoma detection},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Attaining an IoMT-based health monitoring and prediction: A hybrid hierarchical deep learning model and metaheuristic algorithm. <em>NCA</em>, <em>37</em>(25), 20481-20498. (<a href='https://doi.org/10.1007/s00521-023-09293-3'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Internet of Medical Things (IoMT) visualizes a network of medical devices and society adopting wireless communications to enable interchange of healthcare data. IoMT is utilized to gather real-time data from medical equipment and sensors. This enables possibility for continuous health monitoring and prediction. There is concern related to potential privacy and safety hazards connected with the group and transmission of sensitive health data over the network. This study proposes a hybrid hierarchical deep learning (DL) model enhanced with features and a metaheuristic algorithm to achieve health monitoring and prediction based on IoMT. The information gained from the analysis helps to identify important features for prediction. The feature selection phase applies Self-regularized Quantum Coronavirus Optimization Algorithm (SQCOA) to prioritize important features for prediction. The prediction phase includes Optimized Long Short-Term Memory (OLSTM) and Hierarchical Convolutional Spiking Neural Network (HCSNN) for feature learning and performance prediction, respectively. The proposed model is simulated by adopting MATLAB. The model attains the highest accuracy of 98%.},
  archive      = {J_NCA},
  author       = {Shukla, Prashant Kumar and Alqahtani, Ali and Dwivedi, Ashish and Alqahtani, Nayef and Shukla, Piyush Kumar and Alsulami, Abdulaziz A. and Pamucar, Dragan and Simic, Vladimir},
  doi          = {10.1007/s00521-023-09293-3},
  journal      = {Neural Computing and Applications},
  month        = {9},
  number       = {25},
  pages        = {20481-20498},
  shortjournal = {Neural Comput. Appl.},
  title        = {Attaining an IoMT-based health monitoring and prediction: A hybrid hierarchical deep learning model and metaheuristic algorithm},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). AMIAC: Adaptive medical image analyzes and classification, a robust self-learning framework. <em>NCA</em>, <em>37</em>(25), 20451-20479. (<a href='https://doi.org/10.1007/s00521-023-09209-1'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Adaptive self-learning is a promising technique in medical image analysis that enables deep learning models to adapt to changes in image distribution over time. As medical image data can vary due to factors like imaging equipment and patient demographics, adaptive self-learning becomes valuable for maintaining the accuracy and robustness of deep learning frameworks. Initially trained on a large dataset, the framework can adapt to new modalities using transfer learning, adaptive learning, and incremental learning, incorporating both manual and auto (CNN-based) features. Adaptive self-learning offers various benefits, including improved model accuracy and efficiency, reducing the need for manual retraining. However, challenges such as the risk of overfitting, acquiring relevant manual features, and careful monitoring need to be addressed. Combining manual features and pretrained CNN models can enhance performance in medical image analysis tasks such as tumor classification, lesion detection, and cancer segmentation. Manual features capture specific image characteristics, while pretrained CNN models automatically learn abstract features from extensive datasets. Combining these approaches provides additional information that neither approach alone can capture. In this study, we present a unique framework for adaptive self-learning in medical image analysis and classification. The proposed framework analyzes image modality, applies preprocessing techniques, and acquires both manual and CNN-based pertinent features. However, careful tuning and experimentation are essential to determine the optimal combination of manual features with the appropriate CNN model architecture. The recommended adaptive self-learning framework achieves a high averaged F1-score $$97.35\pm 0.39$$ and precision of $$97.19 \pm 0.51$$ , as well as its fantabulous abstraction and accuracy of $$97.81 \pm 0.35$$ , implying that it might be used to build a pathologist’s aid tool.},
  archive      = {J_NCA},
  author       = {Iqbal, Saeed and Qureshi, Adnan N. and Aurangzeb, Khursheed and Alhussein, Musaed and Haider, Syed Irtaza and Rida, Imad},
  doi          = {10.1007/s00521-023-09209-1},
  journal      = {Neural Computing and Applications},
  month        = {9},
  number       = {25},
  pages        = {20451-20479},
  shortjournal = {Neural Comput. Appl.},
  title        = {AMIAC: Adaptive medical image analyzes and classification, a robust self-learning framework},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Improved content-based brain tumor retrieval for magnetic resonance images using weight initialization framework with densely connected deep neural network. <em>NCA</em>, <em>37</em>(25), 20437-20450. (<a href='https://doi.org/10.1007/s00521-023-09149-w'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Content-based medical image retrieval (CBMIR) systems can assist and help doctors and radiologists in reliable diagnosis by retrieving relevant medical cases from past cases that share similarities with the current case. Therefore, there is a need for improved retrieval accuracy in CBMIR systems. Retrieving similar Brain Tumor magnetic resonance imaging (MRI) slices from the same class as the query is inherently challenging. For different types of tumors, there is no class-specific structure, size or shape. Further, MRIs being present in multiple views increase discrepancy in interview retrieval. This leads to high inter-class similarity but at the same time high intra-class variations. Skewed data quantities for Brain Tumor types like Meningioma further add to another challenge. It is necessary to formulate rich and generic MRI representations to address these issues. Toward the same, it is essential to model spatial contexts on a multi-scale across the tumor-affected regions and enhances the feature representational learning by extracting generic features. Hence, we propose a Weight Initialization Framework with Densely Connected Networks to improve generalization for Brain Tumor MRI retrieval. The proposed framework uplifts DenseNet-based models for feature extraction as they incorporate feature reuse and feature learning in a multi-scale manner. Further, a weight Initialization Framework (WIF) is used for improvising the representational learning. Specifically, WIF initializes weights of the DenseNet model by transfer learning-based adaptation, which then involves freezing the initial few layers. The freezing step ensures rich low-level features, even for long-tailed classes like Meningioma, while the remaining trainable layers are fine-tuned to incorporate domain-inherent feature learning. The proposed approach outperforms state-of-the-art by a margin of 1.70% and 1.69% on standard mAP and p@10, respectively. Concretely, when DenseNet and WIF are jointly employed, a stark increment in performance for the Meningioma class is observed, suggesting the generalizability of the framework.},
  archive      = {J_NCA},
  author       = {Singh, Vibhav Prakash and Verma, Aman and Singh, Dushyant Kumar and Maurya, Ritesh},
  doi          = {10.1007/s00521-023-09149-w},
  journal      = {Neural Computing and Applications},
  month        = {9},
  number       = {25},
  pages        = {20437-20450},
  shortjournal = {Neural Comput. Appl.},
  title        = {Improved content-based brain tumor retrieval for magnetic resonance images using weight initialization framework with densely connected deep neural network},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Optimizing wind power forecasting with hybrid neural networks: Insights from brazil. <em>NCA</em>, <em>37</em>(24), 20409-20436. (<a href='https://doi.org/10.1007/s00521-025-11464-3'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Brazil stands out globally for its high dependence on renewable energy sources within its power grid, which accounted for approximately 82% of its electricity generation in 2021, with hydroelectric power alone contributing 60.2%. Since the early 2000 s, the Brazilian government has undertaken efforts to diversify the country’s energy matrix and reduce reliance on hydropower. Programs such as PROINFA have played a key role in supporting the development of renewable energy plants. As a result, Brazil’s installed wind power capacity has grown substantially, from 8,725.88 MW in 2015 to 25,631.7 MW in 2022. The National Energy Plan 2050 projects that wind capacity could exceed 110 GW by mid-century. Efficient operation of the National Interconnected System depends on accurate forecasting of wind patterns. The analysis compares Deep Learning architectures, including Convolutional Neural Networks, Long Short-Term Memory, a hybrid Convolutional Autoencoder–Long Short-Term Memory, against classical state-of-the-art Machine Learning methods commonly used in regression tasks, Support Vector Regression, K-Nearest Neighbors, Multilayer Perceptron, Extreme Gradient Boosting (XGBoost), and Ridge Regression (Ridge). Models are trained on wind regime data from Bahia (Brazil), with hyperparameter optimization performed via Bayesian methods. Results indicate that the CNN-ALSTM architecture achieves superior predictive performance relative to the benchmark models.},
  archive      = {J_NCA},
  author       = {Freire, José Péricles and Rubio, Lihki and Velasquez, Carlos E.},
  doi          = {10.1007/s00521-025-11464-3},
  journal      = {Neural Computing and Applications},
  month        = {8},
  number       = {24},
  pages        = {20409-20436},
  shortjournal = {Neural Comput. Appl.},
  title        = {Optimizing wind power forecasting with hybrid neural networks: Insights from brazil},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Predictive maintenance based on IIoT and machine learning aligned with industry 4.0: A case study in waste-water treatment plant. <em>NCA</em>, <em>37</em>(24), 20383-20407. (<a href='https://doi.org/10.1007/s00521-025-11463-4'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In industrial applications, maintenance strategies such as corrective, preventive, and predictive maintenance (Pdm) are essential for ensuring equipment reliability and availability. The Pdm which focuses on anticipating faults before they occur, has evolved from traditional threshold-based methods to advanced techniques powered by Industry 4.0 technologies. This paper presents an enhanced Pdm methodology by integrating the machine learning (ML) and the industrial internet of things (IIoT) to improve the fault prediction and minimize the unexpected failures. The proposed methodology utilizes real-time data collected by programmable logic controllers (PLCs) from field sensors which monitor key parameters such as temperature, vibration, and current. Historical data stored in a database server is processed using Node-Red for visualization and predictive analysis, enabling accurate failure predictions and remaining useful life estimation based on the health indicator value using regression analysis. The developed model generates automated alerts and reports via email and provides real-time insights into equipment health. These reports will encapsulate detailed information regarding the equipment healthiness and the estimated remaining useful life (RUL). To validate the effectiveness of this methodology, a case study was conducted on heavy-duty equipment in a wastewater treatment plant. The experimental results demonstrate precise failure point estimation with different regression methodologies and highlight the effectiveness of the proposed Pdm framework in improving industrial asset management, reducing downtime, and equipment reliability.},
  archive      = {J_NCA},
  author       = {Abd Elhaleem, Sameh and Zanfal, Abdallah and Hamdy, Mohamed},
  doi          = {10.1007/s00521-025-11463-4},
  journal      = {Neural Computing and Applications},
  month        = {8},
  number       = {24},
  pages        = {20383-20407},
  shortjournal = {Neural Comput. Appl.},
  title        = {Predictive maintenance based on IIoT and machine learning aligned with industry 4.0: A case study in waste-water treatment plant},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Detecting image forgery: A deep learning framework with feature pyramid integration for inpainting detection. <em>NCA</em>, <em>37</em>(24), 20365-20381. (<a href='https://doi.org/10.1007/s00521-025-11461-6'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Inpainting is a technique used to modify the visual components of photographs. It involves removing a specific part of an image and replacing it with new information that reduces any visible inconsistencies. This subject is of great importance; when used maliciously, inpainting can alter images undetectably, affecting areas like legal proceedings, news authenticity, and personal privacy. Hence, this paper addresses the challenge of detecting image inpainting, where a robust end-to-end deep learning framework for this purpose is presented. The proposed approach leverages advanced feature extraction techniques to enhance the model’s effectiveness, in addition to utilizing feature pyramid and a residual network backbone, that are tailored for image inpainting detection. The use of feature pyramid is highly beneficial for reducing intra-class differences by integrating features at multi-scale levels. In addition, a custom public dataset for detecting inpainting, consisting of (12k) images, was created to serve as a foundation for such work. The results obtained from assessments, with a detection accuracy of over (>90%), on the recently created benchmark dataset highlight the strength and reliability of the proposed deep learning approach. These findings demonstrate the model’s strong ability to accurately detect picture manipulation and represent a significant improvement in the tools accessible to analysts who are concerned with protecting the authenticity of digital images.},
  archive      = {J_NCA},
  author       = {Aly, Soha Ahmed Ehssan and Emara, Omar and Mahmoud, Hamza and Bekhet, Saddam},
  doi          = {10.1007/s00521-025-11461-6},
  journal      = {Neural Computing and Applications},
  month        = {8},
  number       = {24},
  pages        = {20365-20381},
  shortjournal = {Neural Comput. Appl.},
  title        = {Detecting image forgery: A deep learning framework with feature pyramid integration for inpainting detection},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Conditional diffusion to enhance performance of object detection in unbalanced data engineering drawings. <em>NCA</em>, <em>37</em>(24), 20335-20364. (<a href='https://doi.org/10.1007/s00521-025-11458-1'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To train deep learning models, it is necessary to have a large amount of data available, preferably with balanced classes. Models often struggle to achieve acceptable performance because unbalanced classes drastically reduce the model’s generalization capacity. In the case of applying deep learning for digitizing engineering drawings (EDs) of the rail networks, rare symbols are common, as some drawings are used to describe specific scenarios, resulting in unbalanced datasets. In this paper, a synthetic ED generator is proposed to augment the database of relay-based railway interlocking systems (RRIS). The synthetic EDs are created based on RRIS schematic rules and probabilities, considering the performance of the model using original data, new samples are created based on the results that need to be improved. Considering conditional diffusion, the proposed method achieves a mean average precision 11.48% higher than the results of the model using only original data. Other compared models had limited variability in the generated samples and struggled to deal with unbalanced classes. The proposed approach overcame these problems and proved to be a promising technique for generating synthetic drawings.},
  archive      = {J_NCA},
  author       = {Stefenon, Stefano Frizzo and Cristoforetti, Marco and Cimatti, Alessandro},
  doi          = {10.1007/s00521-025-11458-1},
  journal      = {Neural Computing and Applications},
  month        = {8},
  number       = {24},
  pages        = {20335-20364},
  shortjournal = {Neural Comput. Appl.},
  title        = {Conditional diffusion to enhance performance of object detection in unbalanced data engineering drawings},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Edge-supervised convolutional neural network for histopathological classification of oral cancer images. <em>NCA</em>, <em>37</em>(24), 20315-20334. (<a href='https://doi.org/10.1007/s00521-025-11457-2'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Oral cancer is the most prevalent type of cancer, and early detection is crucial for reducing mortality rates. This creates opportunities for advancing computer image analysis tools that alleviate the workload of pathologists. In recent years, deep learning has been utilized to automate the detection of various pathologies through digital images. Consequently, this paper presents an edge-supervised deep learning framework for the automated classification of oral cancer histopathology images. The proposed approach aims to enhance nuclear edges and pixel intensities, convert the histopathology image to grayscale, and extract its edges. Afterward, the resulting output is processed through a sequence of newly proposed feature extraction blocks that integrate the image and edge features. This study examines two datasets: The first includes normal subjects and individuals with oral squamous cell carcinoma (OSCC), while the second comprises OSCC, leukoplakia without dysplasia, and leukoplakia with dysplasia. The experimental results indicate that the proposed framework outperforms convolutional neural networks (CNN) and vision transformer models, with the first dataset achieving a balanced classification accuracy (BCC) of 96.80%. The second dataset, on the other hand, revealed that 90.66% of BCCs were presence or absence for dysplasia, and 92.46% were positive for leukoplakia without dysplasia, leukoplakia with dysplasia, and OSCC. Other performance metrics were also calculated, including recall, sensitivity, F1-score, and specificity. The findings suggest that the framework is a promising diagnostic tool to assist pathologists in classifying oral cancer images more accurately.},
  archive      = {J_NCA},
  author       = {Gab Allah, Ahmed M. and Gaballa, Mohamed M. S. and Elshennawy, Nada M. and Elkholy, Amr},
  doi          = {10.1007/s00521-025-11457-2},
  journal      = {Neural Computing and Applications},
  month        = {8},
  number       = {24},
  pages        = {20315-20334},
  shortjournal = {Neural Comput. Appl.},
  title        = {Edge-supervised convolutional neural network for histopathological classification of oral cancer images},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Improvement in the classification of EMG signals through a convolutional neural network. <em>NCA</em>, <em>37</em>(24), 20299-20313. (<a href='https://doi.org/10.1007/s00521-025-11456-3'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The study highlights the crucial role of electromyogram signals (EMG) in recognizing hand and finger movements, and their application in controlling prosthetic limbs. Focusing on the development of human–machine interactions and rehabilitation devices, particularly robotic prostheses. This paper introduces an innovative model utilizing a convolutional neural network (CNN) for classifying fundamental hand grip movements. By converting EMG signals into channel-specific image spectrograms, the model achieved an unprecedented $$100\%$$ accuracy in classifying 6 different movements, evaluated on the EMG hand gesture dataset from the UCI public repository. The results show superior performance compared to advanced methods, demonstrating the model’s potential as a cost-effective and precise control unit for accurately classifying hand grips from EMG signals.},
  archive      = {J_NCA},
  author       = {Benavides-Álvarez, Cesar and Rodríguez-Martínez, Eduardo and Avilés-Cruz, Carlos and Zúñiga-López, Arturo and Ferreyra-Ramírez, Andrés and Aguilar-Sánchez, Miriam},
  doi          = {10.1007/s00521-025-11456-3},
  journal      = {Neural Computing and Applications},
  month        = {8},
  number       = {24},
  pages        = {20299-20313},
  shortjournal = {Neural Comput. Appl.},
  title        = {Improvement in the classification of EMG signals through a convolutional neural network},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). FolkRAG: A retrieval-augmented generation system for cultural heritage materials. <em>NCA</em>, <em>37</em>(24), 20281-20297. (<a href='https://doi.org/10.1007/s00521-025-11455-4'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Archival collections, such as those at the Library of Congress (LoC), often suffer from fragmented metadata and disconnected discovery tools, making research navigation challenging. These collections—comprising catalog records, finding aids, and digital surrogates—frequently lack uniform semantic links, forcing researchers to piece together information from disparate sources. Current discovery systems are limited in their ability to support intuitive or exploratory inquiries, requiring users to possess significant technical expertise or rely on librarian assistance. To address these challenges, this study introduces FolkRAG, a proof-of-concept system that leverages retrieval-augmented generation (RAG). RAG is an advanced natural language processing framework that combines large language models with external information retrieval systems, enhancing access to archival materials through natural language queries and citation-supported responses. FolkRAG is designed to query public data from the American Folklife Center at the LoC, optimizing vector store parameters and integrating advanced retrieval strategies, including the core principles of sentence window retrieval, hypothetical document embedding, and re-ranking. By bridging intellectual and semantic gaps in archival metadata, FolkRAG provides researchers with a cohesive interface for discovering catalog records, finding aids, and digital objects. This approach not only upholds key values such as reliability, transparency, and contextual accuracy but also redefines archival search as a more intuitive and exploratory experience. By showcasing how metadata integration from diverse data sources is used to construct a vector database that powers advanced RAG systems, FolkRAG demonstrates the potential to improve access to cultural heritage materials for both novice and expert users without sacrificing archival practices.},
  archive      = {J_NCA},
  author       = {Kelly, Paul and Schild, Jonathan and Jafari, Amir},
  doi          = {10.1007/s00521-025-11455-4},
  journal      = {Neural Computing and Applications},
  month        = {8},
  number       = {24},
  pages        = {20281-20297},
  shortjournal = {Neural Comput. Appl.},
  title        = {FolkRAG: A retrieval-augmented generation system for cultural heritage materials},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Optimized decision making for stent placement: A comparative analysis using ResNet-50 and advanced metaheuristic algorithms. <em>NCA</em>, <em>37</em>(24), 20253-20280. (<a href='https://doi.org/10.1007/s00521-025-11454-5'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Heart disease is a leading cause of death globally, and continuous efforts are being made to treat this challenging condition. One of the promising combats that could advantage many patients is stent placement, which is a small mesh tube inserted into coronary arteries to maintain blood flow in patients with heart disease. Accurately deciding whether a stent is needed from coronary angiogram datasets is challenging due to the complex nature process of decision making. Although many metaheuristic and optimization algorithms have been proposed in this determination process, their outcomes have been less than encouraging in identifying stent necessity. One of the primary objectives of this study is establishing a significant indicator for stent necessity in hearth disease infected by analyzing an image dataset that includes stent and non-stent cases. Moreover, the study aimed to enhance the accuracy of stent necessity assessments in these patients using ResNet-50 for feature extraction integrating with six optimization algorithms. Five evaluation metrics: accuracy, specificity, sensitivity, precision, and the confusion matrix were utilized to assess the performance of these methods. Among those algorithms, Learner Performance-Based Behavior with Simulated Annealing (LPBSA), when integrated with ResNet-50 in this study and referred to as LPBSA-(ResNet-50), obtained 100% across the evaluation metrics. In comparison, particle swarm optimization with ResNet-50, denoted as PSO-(ResNet-50), demonstrated competitive performance throughout the evaluation process, except the sensitivity metric, obtaining 75%.},
  archive      = {J_NCA},
  author       = {Hamad, Dana Rasul and Rashid, Tarik A.},
  doi          = {10.1007/s00521-025-11454-5},
  journal      = {Neural Computing and Applications},
  month        = {8},
  number       = {24},
  pages        = {20253-20280},
  shortjournal = {Neural Comput. Appl.},
  title        = {Optimized decision making for stent placement: A comparative analysis using ResNet-50 and advanced metaheuristic algorithms},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Optimizing 1D CNN models for accurate °Brix prediction. <em>NCA</em>, <em>37</em>(24), 20241-20251. (<a href='https://doi.org/10.1007/s00521-025-11453-6'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The accurate prediction of °Brix values, which measure soluble solids concentration in agricultural products, is crucial for quality control and product development in the food industry. Traditional methods for °Brix measurement are often time-consuming and destructive, highlighting the need for rapid, nondestructive alternatives. This study addresses this challenge by applying near-infrared (NIR) spectroscopy combined with machine learning techniques to predict °Brix values. Specifically, we explore the efficacy of convolutional neural networks (CNNs), particularly 1D CNNs, in analyzing NIR spectral data. The research involves optimizing CNN models through Bayesian optimization to enhance predictive performance. We compare these models with partial least squares regression (PLSR) using various preprocessing techniques, including multiplicative scatter correction (MSC) and spectral derivatives. Our results demonstrate that 1D CNNs, especially when combined with MSC and spectral derivatives, outperform PLSR in terms of predictive accuracy and generalization to new data. This study highlights the potential of advanced machine learning models for improving nondestructive chemical property prediction in the food industry. The findings suggest that CNNs offer a robust alternative to traditional methods, with implications for both practical applications and theoretical advancements in spectroscopy and predictive modeling. Future research should focus on expanding the dataset and exploring multitask learning approaches to further enhance model capabilities.},
  archive      = {J_NCA},
  author       = {Paiva-Peredo, Ernesto Alonso and Huánuco-Chipana, María and Ausejo-Gonzales, Fenixe and Acuna-Condori, Kevin and Trujillo, Wiliam},
  doi          = {10.1007/s00521-025-11453-6},
  journal      = {Neural Computing and Applications},
  month        = {8},
  number       = {24},
  pages        = {20241-20251},
  shortjournal = {Neural Comput. Appl.},
  title        = {Optimizing 1D CNN models for accurate °Brix prediction},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Physics-informed neural network with symbolic regression for deriving analytical approximate solutions to nonlinear partial differential equations. <em>NCA</em>, <em>37</em>(24), 20205-20240. (<a href='https://doi.org/10.1007/s00521-025-11450-9'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This work explores an interpretable approach based on physics-informed neural networks (PINNs) combined with symbolic regression (SR) to determine mathematical expressions for the predicted solutions of nonlinear partial differential equations that describe arterial blood flow influenced by external magnetic fields. PINNs are excellent at capturing the underlying physics, but can be computationally intensive and hard to interpret. The predictive power of PINN has been combined with evolutionary symbolic regression using PySR, an open-source Python module that employs genetic programming and customizable mathematical operators, including trigonometric, exponential, and arithmetic functions. This hybrid approach enables the derivation of concise, transparent mathematical expressions that closely replicate the behavior of these complex systems. This blend of PINNs and symbolic regression helps us to better understand how pulsatile blood flow and magnetic fields interact in the viscoelastic arterial circulation. The comparison graphs of the SR model and the PINN-predicted data at different time scales signify a better fit for the discovered mathematical expressions with data. As illustrated by the low mean squared error and statistical validation on residual losses, the symbolic expressions are extremely accurate and quick enough for real-time execution. Additionally, the solutions provided by the PINN are validated numerically to demonstrate the effectiveness of the proposed method. Our results demonstrate that combining symbolic regression with PINNs provides practical and interpretable solutions in biofluid mechanics, offering a more transparent and reliable alternative to traditional methods.},
  archive      = {J_NCA},
  author       = {Das, Joy and Bhaumik, Bivas and De, Soumen and Changdar, Satyasaran},
  doi          = {10.1007/s00521-025-11450-9},
  journal      = {Neural Computing and Applications},
  month        = {8},
  number       = {24},
  pages        = {20205-20240},
  shortjournal = {Neural Comput. Appl.},
  title        = {Physics-informed neural network with symbolic regression for deriving analytical approximate solutions to nonlinear partial differential equations},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). VisioDECT: A novel approach to drone detection using CBAM-integrated YOLO and GELAN-E models. <em>NCA</em>, <em>37</em>(24), 20181-20204. (<a href='https://doi.org/10.1007/s00521-025-11448-3'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Unmanned aerial vehicles have revolutionized logistics, environmental monitoring, and aerial surveillance. Their widespread use has created security concerns, specifically regarding illegal spying, smuggling, and hazardous substance movement. Maintaining public safety and protecting sensitive locations requires effective drone detection and payload assessment. Our article proposes a vision-based system for real-time drone identification and classification utilizing YOLOv5, YOLOv8, and GELAN-E deep learning models, enhanced with novel attention mechanisms and interpretability techniques. By integrating the Convolutional Block Attention Module into the YOLO architecture, which is named AttnYOLO, the system enhances feature extraction and focuses on the most relevant regions in an image. This improvement in spatial and channel attention significantly boosts detection performance, particularly for small and occluded drones. Additionally, we employ Gradient-weighted Class Activation Mapping (EigenCAM) for visualizing model focus during detection, increasing the system’s transparency and interpretability. VisioDECT comprises 20,924 annotated photographs of six drone models in overcast, sunny, and evening circumstances. Under cloudy conditions, DenseNet201 achieved 100% classification accuracy, while DarkNet53 and InceptionV3 reached 99.99% and 99.9%, respectively. In evening scenarios, InceptionV3 had 100% accuracy, followed by DarkNet53 with 99.98%. Our proposed model, GELAN-E, excelled in detection and classification. In overcast settings, GELAN-E outperformed YOLOv8 with an accuracy of 0.988, a recall of 0.994, and a mAP50-95 score of 0.688. For evening conditions, GELAN-E achieved a higher mAP50-95 score of 0.642 compared to YOLOv8. These results demonstrate that the inclusion of attention mechanisms, along with visual interpretability, enhances drone detection performance, particularly in low-light and challenging environments, making this system ideal for real-time drone detection in civilian and military applications .},
  archive      = {J_NCA},
  author       = {Sakib Bin Islam, Md. and Chowdhury, Muhammad E. H. and Hasan-Zia, Mazhar and Kashem, Saad Bin Abul and Majid, Molla E. and Ansaruddin Kunju, Ali K. and Khandakar, Amith and Ashraf, Azad and Nashbat, Mohammad},
  doi          = {10.1007/s00521-025-11448-3},
  journal      = {Neural Computing and Applications},
  month        = {8},
  number       = {24},
  pages        = {20181-20204},
  shortjournal = {Neural Comput. Appl.},
  title        = {VisioDECT: A novel approach to drone detection using CBAM-integrated YOLO and GELAN-E models},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Data-driven deep neural control for chattering reduction in a novel caterpillar robot mechanism. <em>NCA</em>, <em>37</em>(24), 20161-20179. (<a href='https://doi.org/10.1007/s00521-025-11447-4'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper proposes a novel data-driven control method for a robotic mechanism inspired by caterpillar locomotion. A caterpillar robot is designed by accurately modeling its complex and nonlinear motion dynamics. To handle these nonlinearities, Koopman operator theory is employed to derive a linear data-driven dynamic model. A fractional-order sliding-mode controller is then developed to regulate the linearized system effectively. Considering the frequent external disturbances encountered by the caterpillar robot, an adaptive Radial Basis Function neural network is integrated to estimate these disturbances and mitigate the chattering phenomenon. Simulation results demonstrate that the proposed control method achieves high tracking accuracy with reduced chattering, confirming its effectiveness and potential for practical applications in robotic systems requiring robust and precise control. This work underscores the advantages of integrating Koopman theory, fractional-order sliding-mode control, and adaptive neural networks for bio-inspired robotic mechanisms.},
  archive      = {J_NCA},
  author       = {Rahmani, Mehran and Uplap, Apoorva Rahul and Redkar, Sangram},
  doi          = {10.1007/s00521-025-11447-4},
  journal      = {Neural Computing and Applications},
  month        = {8},
  number       = {24},
  pages        = {20161-20179},
  shortjournal = {Neural Comput. Appl.},
  title        = {Data-driven deep neural control for chattering reduction in a novel caterpillar robot mechanism},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Integrating plant growth monitoring in a precision intrarow hoeing tool through canopy cover segmentation. <em>NCA</em>, <em>37</em>(24), 20139-20160. (<a href='https://doi.org/10.1007/s00521-025-11445-6'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Compared to broadcast applications, precision crop farming (PCF) can decrease environmental impact and increase biodiversity by its potential to reduce chemical inputs. High-throughput field phenotyping (HTFP) uses technology to reveal spatial and temporal variability in crop fields. It is mainly used for crop breeding but also has potential in PCF. We integrated HTFP in a precision intrarow hoeing tool by continuously monitoring the cauliflower growth through canopy cover segmentation. A dataset of 53,483 cauliflower canopy segmentation labels was generated. The plant centres detected by the intrarow hoeing tool to avoid plant contact were used to select the Segment Anything model semantic labels representing the canopy cover. This automated few-point labelling strategy enabled 79% of the images to be immediately generated correctly. Compared to other YOLOv8 models trained for agricultural applications in literature, the YOLOv8 model yielded an excellent mean average precision (mAP0.5) for bounding box selection and segmentation of 97.2% and 96.8%, respectively, on the test set of 27,100 images. The YOLOv8 model yielded an F1 score of 0.94, which was identical to the F1 score of the Mask R-CNN model and performed the segmentation five times faster. Additionally, the large dataset was used to quantify the number of labels required for good YOLOv8 model performance for this application. Based on the YOLO segmentations, the canopy cover area was calculated and used to determine the growth curves of 87 cauliflower plants in a crop field, reflecting the local field conditions and supporting decision-making for precise crop management. This study is, to our knowledge, the first to reuse data collected during the operation of a precision hoeing machine for crop monitoring and demonstrates a cost-effective integration of HTFP into precision farming machinery without additional hardware costs. This approach allows precision farming equipment to generate a continuous stream of data, providing farmers with valuable insights into their fields. Because the data are a by-product of an existing field operation, the farmers have a supportive monitoring tool at no additional cost.},
  archive      = {J_NCA},
  author       = {Willekens, Axel and Wyffels, Francis and Pieters, Jan G. and Cool, Simon R.},
  doi          = {10.1007/s00521-025-11445-6},
  journal      = {Neural Computing and Applications},
  month        = {8},
  number       = {24},
  pages        = {20139-20160},
  shortjournal = {Neural Comput. Appl.},
  title        = {Integrating plant growth monitoring in a precision intrarow hoeing tool through canopy cover segmentation},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Detoxifying digital information: Harnessing neural networks for cleaner data. <em>NCA</em>, <em>37</em>(24), 20127-20137. (<a href='https://doi.org/10.1007/s00521-025-11444-7'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the contemporary digital landscape, the exponential growth of information dissemination has led to an unprecedented influx of data, posing significant challenges in veracity, bias, and Noise. Amidst this maelstrom, the quest for detoxifying digital information has become paramount. This conceptual paper delves into the pivotal Role played by Neural Networks in this endeavor, elucidating their transformative potential in combating misinformation, bias, and Noise. Drawing upon interdisciplinary insights, we explore Neural Networks’ capacity to discern patterns, mitigate biases, and refine data, thereby fostering a paradigm shift toward cleaner and more trustworthy digital information. We showcase Neural Networks’ efficacy in diverse domains through case studies and examples, from fake news detection to signal refinement in communication systems. However, we also confront the ethical considerations and challenges inherent in deploying Neural Networks for data detoxification, emphasizing the importance of upholding fairness, transparency, and individual autonomy. Ultimately, by navigating these challenges with prudence and foresight, we can harness the transformative power of Neural Networks to foster a digital environment characterized by integrity, transparency, and trustworthiness.},
  archive      = {J_NCA},
  author       = {Bonal, Raghavendra B.},
  doi          = {10.1007/s00521-025-11444-7},
  journal      = {Neural Computing and Applications},
  month        = {8},
  number       = {24},
  pages        = {20127-20137},
  shortjournal = {Neural Comput. Appl.},
  title        = {Detoxifying digital information: Harnessing neural networks for cleaner data},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Damage detection in aircraft engine borescope inspection using deep learning. <em>NCA</em>, <em>37</em>(24), 20105-20126. (<a href='https://doi.org/10.1007/s00521-025-11443-8'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Aircraft engine inspection is a key pillar of aviation safety as it helps to maintain adequate performance standards to ensure engine airworthiness. In addition, it is also vital for asset value retention. Borescope inspection is currently the most widely used visual inspection method for aircraft engines. However, borescope inspection is a time-consuming, subjective, and complex process that heavily depends on the experience and attention level of the inspector. Moreover, the cost savings of airlines and the maintenance, repair, and overhaul (MRO) centers expose pressure and workload on inspectors. These factors make an automated system to support damage detection during borescope inspection necessary in order to mitigate potential risks. In this paper, we propose a deep learning-based automated damage detection framework that employs aircraft engine borescope inspection images. Faster R-CNN-based deep learning model with Inception v2 feature extractor is utilized for the present architecture. Due to the limited number of images, data augmentation and other overfitting methods are also employed. The framework supports crack, burn, nick, and dent damage types across all modules of turbofan engines. It is trained and validated with moderate to complex borescope images obtained from the field. The framework achieves 92.64% accuracy for crack, 92.05% for nick or dent, and 81.14% for burn damage classes, with an overall 88.61% average accuracy.},
  archive      = {J_NCA},
  author       = {Uzun, Ismail and Tolun, Mehmet Resit and Sari, Filiz and Alpaslan, Ferda Nur},
  doi          = {10.1007/s00521-025-11443-8},
  journal      = {Neural Computing and Applications},
  month        = {8},
  number       = {24},
  pages        = {20105-20126},
  shortjournal = {Neural Comput. Appl.},
  title        = {Damage detection in aircraft engine borescope inspection using deep learning},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A novel approach to solar radiation forecasting using a SARIMA–LSTM hybrid model with seasonal trend loess decomposition. <em>NCA</em>, <em>37</em>(24), 20069-20103. (<a href='https://doi.org/10.1007/s00521-025-11442-9'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Solar radiation forecasting plays a crucial role in mitigating climate change, ensuring stable and reliable utilization of solar energy and conducting feasibility studies for solar energy projects. Capturing temporal trends in non-stationary solar radiation data often requires multiple methodologies. In this context, this study proposes an Seasonal Trend Loess based hybrid model that combines the advantages of Seasonal Autoregressive Integrated Moving Average (SARIMA), which effectively explains linear relationships, and Long Short-Term Memory (LSTM), which excels in capturing nonlinear dependencies. The Loess smoothing technique, which is particularly effective for time series with high seasonal variability, enables the decomposition of the time series into seasonal, trend and residual components for separate modeling. Furthermore, to assess the model's stability across all periods and evaluate its predictive performance, tenfold cross-validation was applied to the time series. The study utilized hourly measured solar irradiation data for Afyonkarahisar, Turkey. When compared to standalone SARIMA and LSTM forecasting models, the proposed hybrid model achieved improvements of 37.84% in RMSE, 37.45% in NRMSE, 34.03% in MAE, and 4.42% in R2 metrics.},
  archive      = {J_NCA},
  author       = {Yeşil, Feyza Nur and Serttaş, Tuba Nur},
  doi          = {10.1007/s00521-025-11442-9},
  journal      = {Neural Computing and Applications},
  month        = {8},
  number       = {24},
  pages        = {20069-20103},
  shortjournal = {Neural Comput. Appl.},
  title        = {A novel approach to solar radiation forecasting using a SARIMA–LSTM hybrid model with seasonal trend loess decomposition},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). LDFormer: Lightweight dehazing transformer. <em>NCA</em>, <em>37</em>(24), 20049-20068. (<a href='https://doi.org/10.1007/s00521-025-11440-x'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we introduce a novel lightweight transformer-based network, LDFormer, specifically designed for single image dehazing. LDFormer employs encoder-decoder architecture to efficiently restore haze-free images while tackling the critical challenge of computational complexity. To optimize feature transfer efficiency, we incorporated a selective feedforward network (SFN) within each transformer module (TM), which significantly reduces the computational burden by selectively processing relevant features, while maintaining high performance in dehazing tasks. Moreover, to improve the attention process in LDFormer, we introduced a cross-layer attentive integrated module (CAIM). Through the refinement of the network's emphasis on critical data, CAIM facilitates more effective feature extraction and usage at different transformer module levels. This combination enhances the network’s ability to capture detailed feature representations, leading to superior dehazing quality. Comprehensive tests conducted on multiple benchmark datasets confirm the effectiveness of our network. The results demonstrate that LDFormer not only surpasses current state-of-the-art (SOTA) techniques in terms of restoration quality but also offers significantly improved computational efficiency. Our network emerges as a robust solution for single image dehazing, excelling in both quantitative measurements and visual clarity, thus proving to be a highly effective and efficient choice for real-world dehazing applications.},
  archive      = {J_NCA},
  author       = {Pushpalatha, D. and Prithvi, P.},
  doi          = {10.1007/s00521-025-11440-x},
  journal      = {Neural Computing and Applications},
  month        = {8},
  number       = {24},
  pages        = {20049-20068},
  shortjournal = {Neural Comput. Appl.},
  title        = {LDFormer: Lightweight dehazing transformer},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Adaptive filter-enabled electrocardiogram signals-based myocardial infarction detection using convolutional neural network ensemble model. <em>NCA</em>, <em>37</em>(24), 20027-20048. (<a href='https://doi.org/10.1007/s00521-025-11439-4'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Myocardial Infarction (MI), commonly known as heart attack, is still one of the most frequent and devastating cardiovascular conditions in the world, accounting for a huge percentage of death rates. MI mortality rates are still at concerning levels, placing a significant strain on healthcare systems throughout the globe. Improving patient outcomes and initiating timely medical treatments depend on the timely and accurate diagnosis of myocardial infarction (MI). Electrocardiography (ECG) is a vital diagnostic technique in cardiology that records electrical activity to provide a non-invasive way to evaluate the heart’s health. ECG signals can help detect anomalies suggestive of MI and offer important insights into heart function. In this work, we present a new method for MI diagnosis based on an ensemble of convolutional neural networks (CNNs) trained on ECG signal data. With the aid of ensemble learning and deep learning, our model can accurately identify MI. With a 99.12% accuracy rate, 99.71% specificity, and 99.11% precision, our proposed approach demonstrates encouraging outcomes in successfully detecting MI patients and even patients with a history of MI. This CNN-Ensemble model is pertinent because it has the potential to give medical practitioners a solid and trustworthy tool for the rapid and accurate diagnosis of MI.},
  archive      = {J_NCA},
  author       = {Shastri, Sourabh and Kumar, Sachin and Mansotra, Vibhakar},
  doi          = {10.1007/s00521-025-11439-4},
  journal      = {Neural Computing and Applications},
  month        = {8},
  number       = {24},
  pages        = {20027-20048},
  shortjournal = {Neural Comput. Appl.},
  title        = {Adaptive filter-enabled electrocardiogram signals-based myocardial infarction detection using convolutional neural network ensemble model},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multimodal few-shot learning for plant disease detection with contrastive pre-training and query addressal. <em>NCA</em>, <em>37</em>(24), 20005-20026. (<a href='https://doi.org/10.1007/s00521-025-11438-5'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Every year, the global agrarian economy faces severe losses inflicted by plant diseases. Overcoming this is a significant challenge due to the expertise required in plant disease identification and classification. While machine learning applications are not new to solving this problem, they rely on heavy, non-local datasets which is a hurdle when they have to be deployed in specific regions. To address this, we scrape a novel dataset, tailored to the Indian state of Tamil Nadu. To provide additional context to the images, we propose to develop a second mode of data, text descriptions, which are more abundantly available and can be augmented using large language models. Given the lack of large, structured data for the task, we intend to use a few-shot learning (FSL) approach. Specifically, we propose to use a two-step approach for FSL that incorporates contrastive pre-training to extract better latent features and a prototype-based classification network to perform disease detection. The detected disease combined with the farmers’ question is used to query an advisory document using a retrieval augmented generation pipeline. Our approach achieved an accuracy of 93% in disease classification and a context precision score of 0.87 in query resolution. This shows great potential to contribute to the food security and sustainable agriculture landscape of Tamil Nadu.},
  archive      = {J_NCA},
  author       = {Pranith, P and Yeshwanth, Varsha and Thenmozhi, Durairaj},
  doi          = {10.1007/s00521-025-11438-5},
  journal      = {Neural Computing and Applications},
  month        = {8},
  number       = {24},
  pages        = {20005-20026},
  shortjournal = {Neural Comput. Appl.},
  title        = {Multimodal few-shot learning for plant disease detection with contrastive pre-training and query addressal},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A new deep decoder type cascaded feedforward neural network for critical heat flux prediction in power reactors. <em>NCA</em>, <em>37</em>(24), 19975-20003. (<a href='https://doi.org/10.1007/s00521-025-11436-7'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Critical Heat Flux (CHF) is a fundamental parameter for ensuring the performance, reliability, safety, and economic viability of water-cooled nuclear reactors. Despite its significance, the absence of a deterministic theory for CHF prediction poses a substantial challenge in thermal engineering. Consequently, various experimental and numerical models have been developed, yet no universally accepted model comprehensively addresses the wide range of flow conditions encountered in practical applications. The accurate prediction of CHF remains a complex and critical task. This work introduces a novel Deep Decoder Type Cascaded feedforward Neural Network (DD-CFNN) to predict CHF across a broad spectrum of operating conditions. The DD-CFNN architecture leverages the strengths of feedforward neural networks by incorporating connections from input and preceding layers to all subsequent layers, significantly enhancing its modeling capacity. Furthermore, the decoder type cascaded structure enables progressive enhancement of the feature space, optimizing its predictive performance. A new dataset, derived from extensive CHF data available in the literature, was developed for training and testing the proposed model. The performance of the DD-CFNN variants was evaluated extensively, with the best-performing model achieving a Root Mean Squared Error (RMSE) of 5.17% and a correlation coefficient of 0.997, indicating exceptional predictive accuracy. The model’s performance was benchmarked against standard machine learning algorithms such as Support Vector Machine (SVM) and k-Nearest Neighbors (KNN). Predictive robustness and reliability were further validated using various holdout and cross-validation techniques. The results demonstrate the superiority of the proposed DD-CFNN in predicting CHF and recommend its potential application as a robust monitoring tool to ensure the safe operation of nuclear power reactors.},
  archive      = {J_NCA},
  author       = {Khalid, Rehan Zubair and Ullah, Atta and Khan, Asifullah and Inayat, Mansoor Hameed and Al-Dahhan, Muthanna H.},
  doi          = {10.1007/s00521-025-11436-7},
  journal      = {Neural Computing and Applications},
  month        = {8},
  number       = {24},
  pages        = {19975-20003},
  shortjournal = {Neural Comput. Appl.},
  title        = {A new deep decoder type cascaded feedforward neural network for critical heat flux prediction in power reactors},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Smart traffic management system using YOLOv11 for real-time vehicle detection and dynamic flow optimization in smart cities. <em>NCA</em>, <em>37</em>(24), 19957-19974. (<a href='https://doi.org/10.1007/s00521-025-11434-9'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Modern metropolitan environments continue to face a major problem with traffic congestion, which increases travel time, fuel consumption, and pollution. The Smart Traffic Management System (STMS) suggested in this paper uses an improved YOLO-based deep learning model for dynamic traffic flow optimization and real-time vehicle recognition. To evaluate real-time traffic footage, precisely identify different vehicle kinds, and calculate traffic density, the system combines computer vision and artificial intelligence (AI). This analysis enables an intelligent control system to dynamically modify traffic signals and reroute automobiles to alleviate congestion. Real-world traffic datasets are used to assess the suggested system, which shows excellent detection accuracy and responsiveness in real time. Comparative findings demonstrate how effective the strategy is in enhancing traffic flow and reducing bottlenecks. The results show that combining adaptive signal control and AI-driven real-time traffic monitoring can greatly improve urban mobility and sustainability, opening the door to smarter and more effective cities. According to experimental data, the suggested system's mean Average Precision (mAP) of 92.4% indicates its high level of vehicle identification accuracy. Additionally, the model maintains an Intersection over Union (IoU) score of 0.85, guaranteeing accurate vehicle localization. The technology also efficiently adjusts to different traffic situations, minimizing congestion and improving traffic flow in real time.},
  archive      = {J_NCA},
  author       = {Talaat, Fatma M. and El-Balka, Rana Mohamed and Sweidan, Sara and Gamel, Samah Adel and Al-Zoghby, Aya M.},
  doi          = {10.1007/s00521-025-11434-9},
  journal      = {Neural Computing and Applications},
  month        = {8},
  number       = {24},
  pages        = {19957-19974},
  shortjournal = {Neural Comput. Appl.},
  title        = {Smart traffic management system using YOLOv11 for real-time vehicle detection and dynamic flow optimization in smart cities},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multimodal deep learning model for bitcoin price prediction with news and market prices. <em>NCA</em>, <em>37</em>(24), 19921-19956. (<a href='https://doi.org/10.1007/s00521-025-11432-x'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Bitcoin volatility has posed significant challenges to investors, making it a focal point for researchers. With increasing use of the internet, daily news is considered to have a substantial influence on bitcoin prices. Traditional methods relying solely on technical indicators or social sentiment often fail to capture the nuanced relationships between these factors, leaving a significant gap in accurately predicting price movements. In this research, we predict bitcoin prices by leveraging news data extracted from Common Crawl News dataset, in conjunction with bitcoin prices obtained from Coinbase Application Programming Interface. We propose a multimodal deep learning model named, Generative Pre-Trained Transformer 3.5 (GPT3.5) enhanced Convolutional Neural Network (CNN) Positional encoding-based Transformer Encoder (GPT-CNN-PTEN), designed to predict Bitcoin prices during both bullish and bearish market phases. To achieve this, we employ transformer encoder layers with CNN positional encoding to capture relations in the text and bitcoin prices and handle temporal dependence associated with bitcoin prices. The word embeddings generated using the Ada002 model from the text summarized by GPT3.5 turbo model captures the context of lengthy news articles into concise and meaningful inputs to the model. The dataset from October 2022 to January 2023 was used to train the model. The model was tested on three distinct datasets representing various market conditions. The results are noteworthy achieving mean squared error of 0.001, mean absolute percentage error of 7%. Moreover, the model demonstrated its predictive power by accurately forecasting a bull run hours before it occurred in January 2023.},
  archive      = {J_NCA},
  author       = {Vardhan, Gadige Vishnu and Subburaj, Brindha},
  doi          = {10.1007/s00521-025-11432-x},
  journal      = {Neural Computing and Applications},
  month        = {8},
  number       = {24},
  pages        = {19921-19956},
  shortjournal = {Neural Comput. Appl.},
  title        = {Multimodal deep learning model for bitcoin price prediction with news and market prices},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Intelligent classification of muscular paralysis using optimized convolutional neural network layers from raw electromyography signals. <em>NCA</em>, <em>37</em>(24), 19895-19920. (<a href='https://doi.org/10.1007/s00521-025-11431-y'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Loss of muscle power is paralysis, and the major reason is brain-injured nerve connections. Muscle electrical activity is characterized by electromyography (EMG) signals. EMG signals help classify muscular paralysis, and statistics using EMG data from various settings detect paralysis. Due to the difficulty of EMG signal processing, short research has been done on EMG-based artificial intelligence (AI) techniques for muscular paralysis classification. The AI techniques like deep learning (DL) have garnered interest in such applications. However, designing the DL-based approach for the EMG-based muscular paralysis classification is a challenging research problem. Novel DL-based automatic muscular paralysis classification (AMPC) is proposed using the efficient deep learning (EDL) model called AMPC-EDL. The AMPC-EDL model is composed of EMG signal pre-processing, feature engineering, and classification phases. In pre-processing, the input EMG signal is wavelet-based filtered to remove artifacts, noise, and missing data to improve classification. In the feature engineering phase, the one-dimensional (1D) convolutional neural network (CNN) layers are designed for the automatic feature extraction from the input pre-processed 1D EMG signal. CNN layers are designed efficiently to improve the overall classification accuracy and reduce the computational overhead, vanishing gradient, and overfitting problems. Finally, the SoftMax classifier is utilized, followed by the fully connected layer (FCL), to accurately categorize the input EMG signal as either indicative of muscular paralysis or normal. The simulation results show the AMPC-EDL model has improved the overall accuracy by 4.78% and reduced the time complexity by 17.45% compared to existing methods.},
  archive      = {J_NCA},
  author       = {Awachat, Harshita S. and Rewatkar, Rajendra and Reddy, K. T. V.},
  doi          = {10.1007/s00521-025-11431-y},
  journal      = {Neural Computing and Applications},
  month        = {8},
  number       = {24},
  pages        = {19895-19920},
  shortjournal = {Neural Comput. Appl.},
  title        = {Intelligent classification of muscular paralysis using optimized convolutional neural network layers from raw electromyography signals},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Neural leader–follower swarm control algorithm for underwater vehicles. <em>NCA</em>, <em>37</em>(24), 19867-19893. (<a href='https://doi.org/10.1007/s00521-025-11429-6'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The paper deals with the problem of controlling a swarm of underwater vehicles where vehicle sensors cannot distinguish swarm vehicles from other objects, for example, obstacles. The paper assumes a leader–follower strategy, according to which a swarm of low-cost follower vehicles equipped only with short-range navigation follows the leader along a desired trajectory defined in 2D space. Although the trajectory is defined in 2D space, vehicles can change the depth at which they move, resulting in a temporary loss of sensor data. The paper also considers the possibility of a hybrid swarm consisting of vehicles with different characteristics and a variable formation in which the vehicles move. The vehicles are controlled using neural networks that determine the direction of movement and speed of vehicles and are supplied with data from sensors with different operation ranges. The effectiveness of the networks was verified in simulation conditions. Verification tests have shown that the vehicles are able to: (i) independently form an organized swarm in the initial phase of the operation, (ii) move in two different formations despite interference affecting the operation of sensors and making it difficult to maintain the formation, (iii) cooperate within one swarm despite differences in the maneuverability of individual swarm members, and (iv) change the mission depth as a group; however, it is recommended to carry out the depth change maneuver only on straight sections of the trajectory.},
  archive      = {J_NCA},
  author       = {Praczyk, Tomasz},
  doi          = {10.1007/s00521-025-11429-6},
  journal      = {Neural Computing and Applications},
  month        = {8},
  number       = {24},
  pages        = {19867-19893},
  shortjournal = {Neural Comput. Appl.},
  title        = {Neural leader–follower swarm control algorithm for underwater vehicles},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). An exhaustive examination of the thermal and mechanical properties of nanofluids containing graphene oxide nanoparticles through the utilization of artificial neural network. <em>NCA</em>, <em>37</em>(24), 19839-19866. (<a href='https://doi.org/10.1007/s00521-025-11428-7'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Nanofluid has sparked considerable interest due to its intriguing thermal, mechanical, and optical attributes. This study used Machine learning (ML) to investigate Casson nanofluid flow due to a curved stretching sheet with graphene oxide nanoparticles in engine oil base liquid under the magnetic field and variable thermal conductivity effects. Key physical phenomena including heat source/sink, Joule heating, and activation energy effects are incorporated, with Brownian and thermophoresis diffusivity impacts employed according to the Buongiorno model. The present work employs the homotopy analysis method (HAM) for simulation, with results trained using artificial neural network (ANN) applications. Using the Levenberg-Marquard Scheme-based Backpropagation Method (NN- BLMS) within feed-forward neural networks, the results obtained from the ANN are validated. Our finding shows the ANN model reliably predicts nanofluid behavior with 94% accuracy based on regression analysis. Heat transfer rate increases by 34.99% when the Eckert number rises from 0.1 to 0.7. Also, the nanofluid velocity decreases due to magnetic field effects and Lorentz force resistance. Higher heat and mass transfer rates are predicted for the Brownian motion parameter, while fluid concentration decreases with Schmidt number. ANN implementation enables faster convergence and greater efficiency compared to alternative models. In the present ANN model, the HAM method reduced computational time with a mean squared error (MSE) below 10−4. These findings are valuable for optimizing nanofluid applications in engineering and industry, particularly in heat exchangers, microfluidic cooling systems, solar thermal collectors, and biomedical drug delivery.},
  archive      = {J_NCA},
  author       = {Awais, Muhammad and Ahmad, Shahzad and Junaid, Muhammad Sheraz and Ramzan, Muhammad and Almehizia, Abdulrahman A. and Zen, Amer Alhaj},
  doi          = {10.1007/s00521-025-11428-7},
  journal      = {Neural Computing and Applications},
  month        = {8},
  number       = {24},
  pages        = {19839-19866},
  shortjournal = {Neural Comput. Appl.},
  title        = {An exhaustive examination of the thermal and mechanical properties of nanofluids containing graphene oxide nanoparticles through the utilization of artificial neural network},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Neural network estimation of vacancy binding energy to screw dislocations: A case study for iron and tungsten. <em>NCA</em>, <em>37</em>(24), 19817-19838. (<a href='https://doi.org/10.1007/s00521-025-11425-w'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Materials science plays an important role in the field of fusion research. We focus on the binding energy of vacancy-type defects to screw dislocations. These defects produced by irradiation are known to affect the mechanical properties of the material. Traditional techniques, such as density functional theory or molecular dynamics simulations, can be used to study these defects. However, a combinatorial number of cases need to be analyzed to study the binding energy when several vacancies are present, which quickly becomes infeasible. To address this combinatorial issue, we present a neural network solution. From a subset of cases we can train a model, which in turn can predict the energy in a fraction of the time compared to traditional techniques. However, we have to deal with large uncertainties in our predictions. This is addressed by using uncertainty quantification techniques, such as mixture density networks. We present results for a large iron dataset and for a reduced tungsten dataset, in which our solution is shown to benefit from transfer learning. Therefore, we can use the model to analyze different materials while avoiding the cost of generating new large datasets and training the model from scratch. We see a mean absolute percentage error of 7.5% for the iron case and 9.6% for the reduced tungsten case.},
  archive      = {J_NCA},
  author       = {Cattelan, Bruno O. and Lindblad, Victor and Granberg, Fredric and Nurminen, Jukka K.},
  doi          = {10.1007/s00521-025-11425-w},
  journal      = {Neural Computing and Applications},
  month        = {8},
  number       = {24},
  pages        = {19817-19838},
  shortjournal = {Neural Comput. Appl.},
  title        = {Neural network estimation of vacancy binding energy to screw dislocations: A case study for iron and tungsten},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Enhanced child face anonymization using YOLO framework for real-time privacy protection in videos. <em>NCA</em>, <em>37</em>(24), 19793-19816. (<a href='https://doi.org/10.1007/s00521-025-11424-x'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In an era of pervasive digital connectivity, protecting children's privacy in publicly shared videos has become a crucial concern. To address this problem, this paper presents an approach that leverages advanced object detection models to identify and anonymize children's faces in real-time images and videos. A custom dataset of children's faces was collected and manually labeled from publicly available websites to train and evaluate the models. In addition to detection, the approach includes a blurring mechanism to anonymize detected faces effectively, enhancing privacy protection. This paper explores the performance of three versions of the YOLO (You Only Look Once) object detection family YOLOv5, YOLOv8, and YOLOv9 by comparing their detection performance across various epochs and model sizes. Our results demonstrate the exceptional effectiveness of YOLOv9, particularly its medium-sized variant, which outperformed the other models, achieving the highest mAP@0.5 of 0.963. The findings strongly suggest that YOLOv9 is the most suitable model for real-time face anonymization, offering a promising solution to safeguard children's privacy in digital media. Furthermore, the framework aims to balance privacy concerns with usability in dynamic video environments.},
  archive      = {J_NCA},
  author       = {Ahmed, Hunar A. and Ahmed, Mohammed H. and Majidpour, Jafar and Omer, Saman M.},
  doi          = {10.1007/s00521-025-11424-x},
  journal      = {Neural Computing and Applications},
  month        = {8},
  number       = {24},
  pages        = {19793-19816},
  shortjournal = {Neural Comput. Appl.},
  title        = {Enhanced child face anonymization using YOLO framework for real-time privacy protection in videos},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Training neural networks with a self-adaptive ant colony algorithm. <em>NCA</em>, <em>37</em>(24), 19773-19791. (<a href='https://doi.org/10.1007/s00521-025-11423-y'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {$$\hbox {ACO}_{\mathbb {R}}$$ is a well-established ant colony optimization algorithm that has been applied to neural network training. We present an approach for the dynamic adaptation of the $$\hbox {ACO}_{\mathbb {R}}$$ algorithm’s search intensification/diversification parameter q, based on using several pre-specified parameter configurations, which we call personalities. Before an ant begins to generate a candidate solution, it stochastically adopts a personality based on the relative past success of the different personalities. The success of a personality is measured, in turn, by the relative quality of previous solutions generated by ants adopting that personality. The premise of our approach is that some personalities will be more appropriate than others for different phases of the search. This paper follows up on previous work which used a similar approach to adapting $$\hbox {ACO}_{\mathbb {R}}$$ ’s search width parameter $$\xi$$ . We evaluate our proposal experimentally in the context of training feedforward neural networks for classification using 65 benchmark datasets from the University of California Irvine (UCI) repository. Our experimental results indicate that our proposal produces better predictive accuracy than standard $$\hbox {ACO}_{\mathbb {R}}$$ , to a statistically significant extent.},
  archive      = {J_NCA},
  author       = {Abdelbar, Ashraf M. and Wunsch, Donald C.},
  doi          = {10.1007/s00521-025-11423-y},
  journal      = {Neural Computing and Applications},
  month        = {8},
  number       = {24},
  pages        = {19773-19791},
  shortjournal = {Neural Comput. Appl.},
  title        = {Training neural networks with a self-adaptive ant colony algorithm},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Accurate multi-phase unsupervised and supervised approach to fault detection in power transmission networks. <em>NCA</em>, <em>37</em>(24), 19751-19772. (<a href='https://doi.org/10.1007/s00521-025-11422-z'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Transmission line faults can cause both power loss and failure. To mitigate the effects of such faults, the energy supply must quickly identify and remove faults, as well as ensure grid restoration after a failure occurs. As a result, it is critical to design a system capable of quickly and reliably identifying and eliminating errors. The level of fault identification accuracy is an important indicator for ensuring the reliability of main equipment in power systems, such as generators and transformers. This paper proposes a two-phase approach for identifying faults in transmission systems. In the first phase, unsupervised learning techniques like K-Mean clustering are used to assign labels to datasets for transmission line fault classification. During the second phase, four machine learning techniques called logistic regression (LR), decision tree classifier (DTC), random forest classifier (RFC), and XGBoost Classifier (XGB) are employed to identify faults. Applications are validated on fault detection datasets. The tested approach provides an efficient model for fault detection and classification in transmission lines, as well as a productive framework for fault detection prediction based on machine learning and ensemble learning methods. The experimental simulation results from this study show an accuracy of 83.6% for LR, 99% for DTC, 99% for RFC, and 99.9% for XGB, LightGBM, and CatBoost at 0.99995%. The paper's findings demonstrate the effectiveness of machine learning and ensemble learning techniques in accurately identifying and classifying transmission line faults at competitive performance indices.},
  archive      = {J_NCA},
  author       = {Rezk, Nermeen G. and El-Sehiemy, Ragab A. and Attia, Abdel-Fatah and El-Behery, Heba},
  doi          = {10.1007/s00521-025-11422-z},
  journal      = {Neural Computing and Applications},
  month        = {8},
  number       = {24},
  pages        = {19751-19772},
  shortjournal = {Neural Comput. Appl.},
  title        = {Accurate multi-phase unsupervised and supervised approach to fault detection in power transmission networks},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Advancements in global optimization with an empowered capuchin search algorithm. <em>NCA</em>, <em>37</em>(24), 19707-19749. (<a href='https://doi.org/10.1007/s00521-025-11421-0'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Capuchin search algorithm (CSA) is a newly matured meta-heuristic algorithm based on the natural roaming habits of capuchin monkeys while foraging. The key flaws of this meta-heuristic include convergence to local optimums as well as early convergence. To get around these issues, this study presents a new variant of CSA, referred to as heterogeneous comprehensive opposite learning-based CSA (HCOLCSA), which combines heterogeneous comprehensive learning (HCL) and opposite-based learning (OBL) strategies. The HCL strategy is presented to modify the velocity term of all capuchins for the purpose of improving the exploration and exploitation behaviors in HCOLCSA, while the OBL strategy is embedded into HCOLCSA to boost its exploration and exploitation capabilities of the search space and prevent the trapping of local optimums. To verify the performance of the developed HCOLCSA algorithm, it was evaluated on a set of 29 benchmark functions of the IEEE CEC-2017 test group for dimensions 10, 30, 50, and 100, and then applied to the benchmark functions of the IEEE CEC-2011 evolutionary algorithm competition to demonstrate its reliability and suitability to solve real-world problems. Friedman’s tests, Holm’s tests, and convergence analysis were performed to assess the strength of HCOLCSA compared to others. The numerical findings demonstrate that in more than 92 percent of cases, HCOLCSA produced better results and earned the highest ranking. The results show that HCOLCSA outperforms competing algorithms, demonstrating that it can solve real-world problems represented by CEC-2011 standard functions.},
  archive      = {J_NCA},
  author       = {Braik, Malik and Kassaymeh, Sofian and Almiani, Muder and Albashish, Dheeb and Awadallah, Mohammed and Bataineh, Bilal and Al-Hiary, Heba},
  doi          = {10.1007/s00521-025-11421-0},
  journal      = {Neural Computing and Applications},
  month        = {8},
  number       = {24},
  pages        = {19707-19749},
  shortjournal = {Neural Comput. Appl.},
  title        = {Advancements in global optimization with an empowered capuchin search algorithm},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Weight-space noise for privacy-robustness trade-offs in federated learning. <em>NCA</em>, <em>37</em>(24), 19687-19705. (<a href='https://doi.org/10.1007/s00521-025-11420-1'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Federated learning systems face a fundamental tension between adversarial robustness and privacy preservation. Traditional adversarial training methods enhance robustness but can compromise privacy by making models more susceptible to inference attacks. This paper introduces a weight-space noise approach to address privacy-robustness trade-offs in federated learning. Our method combines server-side federated adversarial adaptation with distributed Gaussian noise injection during model transmission, providing formal differential privacy guarantees while maintaining adversarial robustness. We establish theoretical foundations through weight-space smoothing analysis, demonstrating certified robustness bounds and privacy composition properties across multiple communication rounds. The key insight is that adding Gaussian noise to model weights (rather than inputs) enables simultaneous privacy protection and robustness certification, via randomized-smoothing theory adapted to the weight domain. Experimental evaluation on medical imaging datasets (pathology, meningioma, and glioma classification) shows that our approach achieves robustness comparable to conventional adversarial training methods, while requiring fewer retraining samples and providing stronger privacy guarantees. The distributed noise mechanism reduces attack success rates by up to 27% compared to baseline methods, while maintaining computational efficiency with only 4–5% overhead per federated round. Our theoretical analysis provides certified $$\ell _2$$ robustness guarantees and establishes privacy bounds that degrade gracefully with the number of communication rounds.},
  archive      = {J_NCA},
  author       = {Darzi, Erfan and Sijtsema, Nanna M. and van Ooijen, Peter},
  doi          = {10.1007/s00521-025-11420-1},
  journal      = {Neural Computing and Applications},
  month        = {8},
  number       = {24},
  pages        = {19687-19705},
  shortjournal = {Neural Comput. Appl.},
  title        = {Weight-space noise for privacy-robustness trade-offs in federated learning},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Integrating dynamic evolutionary fuzzy multilevel thresholding with differential evolution for enhanced precision in complex image segmentation tasks. <em>NCA</em>, <em>37</em>(24), 19653-19686. (<a href='https://doi.org/10.1007/s00521-025-11419-8'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Achieving high precision in image segmentation within complex and noisy environments remains an ongoing challenge in image processing. This paper introduces the differential evolution fuzzy multilevel thresholding (DEFMT) model, a ground-breaking algorithm designed to overcome these challenges by combining the strengths of fuzzy multilevel thresholding (FMT) and differential evolution (DE). The DEFMT model addresses inherent segmentation ambiguities through fuzzy logic while leveraging DE for adaptive and dynamic threshold optimization. Using the Euro SAT Dataset (ES), the DEFMT model is meticulously evaluated to demonstrate its ability to achieve superior segmentation accuracy. Key contributions include an innovative integration of fuzzy logic for capturing intricate image details and DE’s optimization capabilities, which enable precise parameter tuning tailored to the dataset’s complexity. The methodology incorporates rigorous preprocessing for noise reduction and data uniformity, setting the foundation for reliable segmentation. Additionally, the model’s performance is validated using the ResNet152V2 architecture, achieving a remarkable 95% classification accuracy. Benchmark comparisons underscore the DEFMT model’s adaptability and superiority over traditional and state-of-the-art techniques. By tackling segmentation challenges head-on, this work establishes DEFMT as a transformative approach with broad applicability, particularly in precision-critical domains such as medical imaging and remote sensing.},
  archive      = {J_NCA},
  author       = {Muthukumaraswamy, P. and Krishnamoorthy, R.},
  doi          = {10.1007/s00521-025-11419-8},
  journal      = {Neural Computing and Applications},
  month        = {8},
  number       = {24},
  pages        = {19653-19686},
  shortjournal = {Neural Comput. Appl.},
  title        = {Integrating dynamic evolutionary fuzzy multilevel thresholding with differential evolution for enhanced precision in complex image segmentation tasks},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Deep learning-based stacked models for cyber-attack detection in industrial internet of things. <em>NCA</em>, <em>37</em>(24), 19617-19651. (<a href='https://doi.org/10.1007/s00521-025-11418-9'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cyber-attack detection is crucial for securing Industrial Internet of Things (IIoT) systems. This study introduces advanced deep learning methodologies to identify potential cyber-attacks effectively in IIoT devices. Three novel stacked deep learning architectures, namely the StackMean, StackMax, and StackRF algorithms. These architectures aggregate and enhance the results of individual deep learning models. Specifically, StackMean computes average predicted class probabilities, StackMax selects maximum predicted class probabilities for more aggressive predictions, and StackRF leverages a random forest to aggregate base models. Theoretical analysis suggests that the proposed stacked deep learning model can boost detection accuracy compared to standalone single deep learning models. Moreover, these stacked models offer increased robustness against adversarial attacks by reducing reliance on specific neural network structures. Additionally, the synthetic minority oversampling technique (SMOTE) algorithm is integrated to address class imbalance challenges in the training dataset. Performance validation is conducted using three publicly available datasets. The detection performance is evaluated using five statistical scores. The results consistently indicate the superiority of the proposed stacked deep learning models over existing techniques. The effectiveness of the SMOTE algorithm is demonstrated through its ability to expand decision regions and minimize false negative signals during attack predictions. In addition, a statistical test is employed to compare the accuracy of individual models with the stacked models, demonstrating that the stacked models exhibit improved accuracy. By combining cutting-edge stacked deep learning architectures with strategic data augmentation techniques, this research significantly contributes to the robustness of cyber-attack detection within IIoT systems.},
  archive      = {J_NCA},
  author       = {Wu, Wang and Fouzi, Harrou and Benamar, Bouyeddo and Sidi-Mohammed, Senouci and Ying, Sun},
  doi          = {10.1007/s00521-025-11418-9},
  journal      = {Neural Computing and Applications},
  month        = {8},
  number       = {24},
  pages        = {19617-19651},
  shortjournal = {Neural Comput. Appl.},
  title        = {Deep learning-based stacked models for cyber-attack detection in industrial internet of things},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Integrating machine learning models and explainable AI to predict DTaP vaccine demand in rural primary care. <em>NCA</em>, <em>37</em>(24), 19597-19616. (<a href='https://doi.org/10.1007/s00521-025-11416-x'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Forecasting the demand for diphtheria, tetanus, and pertussis (DTaP) vaccines is crucial to efficiently allocating resources, reducing vaccine waste, and improving health outcomes, particularly in rural and underprivileged areas. Challenges specific to these areas include limited access to healthcare facilities, inconsistent supply chains, and a lack of data on vaccination rates and patient demographics. This study presents a predictive modeling framework that is capable of improving resource planning at a federally qualified health center (FQHC) in upstate New York by better predicting demand for the DTaP vaccine from the rural population it serves. Using a large dataset of 160,289 patient visits from 2021 to 2023, patient demographic, clinical, insurance, visit-specific, and temporal features were extracted and incorporated into multiple machine learning (ML) models. Multiple ML algorithms—such as XGBoost, CatBoost, and histogram-based gradient boosting— were benchmarked against widely used baseline models including random forests, logistic regression, and decision trees. CatBoost was the top-performing model, with a receiver operating characteristic–area under the curve (ROC-AUC) score of 99.8% and an F1 score of 89.0% at a decision threshold of 67.2%. Seasonal decomposition revealed peaks during back-to-school periods, highlighting temporal dynamics. In addition, SHapley Additive exPlanations (SHAP) analysis improved model interpretability, allowing the identification of age, insurance type, visit type, and previous immunization history as the key predictors that enhance the models’ performance. The predictive modeling framework delivers very accurate forecasts and actionable insights while opening black-box models to guide immunization strategies and optimize resource planning by the existing healthcare system.},
  archive      = {J_NCA},
  author       = {Yaeesh, Osamah and Nimri, Shoog and Wang, Yong and Chen, Shaw-Ree and Kinter, Karen and Renodin-Mead, Danielle and Khasawneh, Mohammad T.},
  doi          = {10.1007/s00521-025-11416-x},
  journal      = {Neural Computing and Applications},
  month        = {8},
  number       = {24},
  pages        = {19597-19616},
  shortjournal = {Neural Comput. Appl.},
  title        = {Integrating machine learning models and explainable AI to predict DTaP vaccine demand in rural primary care},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Underspecification and uncertainty in deep learning models: Is there a connection?. <em>NCA</em>, <em>37</em>(24), 19579-19595. (<a href='https://doi.org/10.1007/s00521-025-11415-y'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep learning has led to significant state-of-the-art results in a panoply of fields with its pattern recognition ability. Even though the research community has strongly benefited from this, these models showcase characteristics that hinder real-world application. One of the major hurdles is the difficulty in pinpointing what a neural network does not know along with their generalisation capabilities. In this context, the term underspecification has been coined, which describes the generation of different predictors with similar in-domain accuracy but diverging results in OOD. In this paper, we characterise the underspecification distribution and study its connection with epistemic uncertainty. We propose the average-metric epistemic uncertainty that transforms the epistemic uncertainty to the underspecification space. We perform a set of experiments using both LeNet and ResNet18 to solve classification problems on CIFAR-10 and Tiny-ImageNet, respectively. We verify that the average-metric epistemic uncertainty is able to accurately predict, on average, 95% of the predictors that can be obtained from a single architecture. In order to improve the interpretability of neural networks, we suggest utilising the range estimated by the average-metric epistemic uncertainty alongside the accuracy to characterise future state-of-the-art models.},
  archive      = {J_NCA},
  author       = {Ramos Ferreira, Filipa M. M. and Rossetti, Rosaldo J. F.},
  doi          = {10.1007/s00521-025-11415-y},
  journal      = {Neural Computing and Applications},
  month        = {8},
  number       = {24},
  pages        = {19579-19595},
  shortjournal = {Neural Comput. Appl.},
  title        = {Underspecification and uncertainty in deep learning models: Is there a connection?},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Hybrid GA-ConvLSTM for data-driven prediction of climate variables: A case study of the most biodiverse cities in india. <em>NCA</em>, <em>37</em>(24), 19549-19578. (<a href='https://doi.org/10.1007/s00521-025-11414-z'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Climate variables affect ecosystems’ structure, sustainability, and biodiversity. Monitoring and analysis of climate variables are essential for the management of ecosystems and the conservation of biodiversity. Temperature affects the distributions, habitats, and phonological events of living things in ecosystems. Humidity is essential for meeting plants’ water needs and the habitat’s quality and health. Dew point is essential for water quality, balance, plant health, and microclimate. Wind speed is essential for pollinating plants, microhabitats, and the health of living things. For such reasons, accurate prediction of climate variables using artificial intelligence methods can effectively predict the future of ecosystems and biodiversity and develop future strategic plans. This study aimed to predict the climate variables of temperature, humidity, dew point, and wind speed of Bhubaneshwar, Dehradun, Guwahati, Mysore, and Nagpur, among the cities with the highest biodiversity in India. It aimed to determine the long-term impacts of climate change on managing ecosystems and biodiversity protection. The hyperparameters of the ConvLSTM model, developed using CNN and LSTM, were optimized with a genetic algorithm, and the GA-ConvLSTM hybrid model was created. GA-ConvLSTM was tested with XGBoost, SVM, ANFIS, CNN, LSTM, and base ConvLSTM using approximately 15 years of hourly, up-to-date, and real-time climate variable data of cities. This work is one of the pioneering studies using GA to optimize deep learning hybrid models for multi-city climate prediction through extensive long-term high-resolution datasets. The experimental results reveal that GA-ConvLSTM outperforms conventional models, producing an R2 score exceeding 0.9 for each city across all climate variables. GA-ConvLSTM produced temperature predictions with 0.833 RMSE and 0.995 R2 for Bhubaneswar, humidity predictions with 3.461 RMSE and 0.990 R2, dew point predictions with 0.632 RMSE and 0.995 R2, and wind speed predictions with 2.278 RMSE and 0.993 R2. The experimental findings confirm the improved performance over conventional models due to SVM and XGBoost failing to achieve these R2 values and high RMSE scores. The research meets three principal obstacles because of the combination of stochastic climate datasets and complex nonlinear patterns, which also encounter limitations in conventional methods for managing scale-dependency relationships and manual parameter adjustments. This approach delivers three main strengths because it combines CNN and LSTM networks through genetic programming while tracking temporal and spatial relationships and processing noisy data automatically, leading to improved results compared to other methods. Unlike other published models in the field, the hybrid model GA-ConvLSTM combines optimized CNN and LSTM elements with genetic algorithm elements. The proposed model received its first application in this research for multi-city climate prediction through training with 15 years of hourly data. Standard methods encounter challenges when detecting long-term dependencies, but GA-ConvLSTM successfully addresses this issue.},
  archive      = {J_NCA},
  author       = {Utku, Anıl and Akyol, Sinem},
  doi          = {10.1007/s00521-025-11414-z},
  journal      = {Neural Computing and Applications},
  month        = {8},
  number       = {24},
  pages        = {19549-19578},
  shortjournal = {Neural Comput. Appl.},
  title        = {Hybrid GA-ConvLSTM for data-driven prediction of climate variables: A case study of the most biodiverse cities in india},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Fire and smoke detection using two-stream spatiotemporal network. <em>NCA</em>, <em>37</em>(24), 19523-19547. (<a href='https://doi.org/10.1007/s00521-025-11412-1'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The detection of fire and smoke plays a crucial and essential role in modern surveillance scenarios to ensure public safety and effective monitoring. Autonomous detection systems face significant challenges in dealing with the complexity of visual patterns, including deformations, unusual camera angles, and seasonal variations. This paper introduces an innovative method to overcome these challenges by integrating spatiotemporal characteristics and an attention model for detecting fire and smoke in videos. To effectively incorporate spatial and temporal information, the proposed methodology described in this paper employs a two-stream deep learning architecture that utilizes a pretrained MobileNet model and includes a separable convolutional LSTM (SepConvLSTM) approach. One stream focuses on frame differences between adjacent frames, while the other stream takes input as background-suppressed frames. Simple and efficient input preprocessing techniques capture motion and generate background-suppressed frames by suppressing nonmoving backgrounds. These preprocessing techniques facilitate the extraction of discriminative features vital to fire and smoke detection. The proposed approach for autonomous detection of fire and smoke in videos involves handling 2D features without flattening them into a 1D vector. Instead, convolution operations are performed on each ConvLSTM gate, allowing the aggregation of frame-level features while retaining spatial information. Furthermore, introducing a depthwise separable convolution technique to generate the SepConvLSTM enhances the robustness of the extracted spatiotemporal features while reducing the number of parameters. Three unique fusion methods are employed to integrate the feature maps obtained from the two streams. Extensive experiments were conducted on five publicly available datasets to evaluate the proposed methodology. The achieved accuracy (ACC) scores for the Mivia, FireNet, BCKO, Visifire, and ViSOR datasets are promising, standing at 97.82%, 100.00%, 98.32%, 97.22%, and 94.33%, respectively. Notably, the model achieved zero false alarm rates for the Mivia dataset, with compact final model sizes of 3.6 MB. The findings presented in this paper demonstrate significant progress compared to existing state-of-the-art research in fire and smoke detection. This improvement can be attributed to the higher accuracy achieved in detection performance and the effective optimization of model parameters, resulting in reduced model complexity and a smaller final model size. The proposed method exhibits the potential to provide a suitable and reliable solution for detecting fire and smoke in surveillance videos and has the potential to enhance efforts in monitoring and improving public safety.},
  archive      = {J_NCA},
  author       = {Khan, Rafaqat Alam and Bajwa, Usama Ijaz and Raza, Rana Hammad and Anwar, Muhammad Waqas},
  doi          = {10.1007/s00521-025-11412-1},
  journal      = {Neural Computing and Applications},
  month        = {8},
  number       = {24},
  pages        = {19523-19547},
  shortjournal = {Neural Comput. Appl.},
  title        = {Fire and smoke detection using two-stream spatiotemporal network},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). CNN-SLSTM framework for human activity recognition using wearable sensor data. <em>NCA</em>, <em>37</em>(23), 19501-19522. (<a href='https://doi.org/10.1007/s00521-025-11410-3'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Human activity recognition (HAR) has become a prevalent research topic in Artificial Intelligence. Recently, numerous deep learning (DL) methods have been extensively applied to HAR, due to their effective automatic feature extraction abilities and to tackle HAR challenges like time-series raw data. In this paper, a Convolutional Neural Network (CNN) integrated with Stacked Long Short-Term Memory (SLSTM) is proposed to predict human activity recognition by extracting spatial and temporal features from the HAR data. The gathered raw data from portable sensors is first processed through a deep neural network that includes two convolutional layers, followed by two LSTM layers whose output is then passed to a dense layer, followed by a fully connected layer with a Softmax activation function to classify input data. The accuracy of the suggested model is 99.86% and 94.44% on the PAMAP2 and UCI-HAR datasets, respectively. The offered network shows improvements on both temporal and spatial dimensions, enhancing the recognition rate when testing the two datasets. The accuracy is substantially enhanced associated with state-of-the-art methods.},
  archive      = {J_NCA},
  author       = {Krishnaleela, P. and Prakash, R. Meena},
  doi          = {10.1007/s00521-025-11410-3},
  journal      = {Neural Computing and Applications},
  month        = {8},
  number       = {23},
  pages        = {19501-19522},
  shortjournal = {Neural Comput. Appl.},
  title        = {CNN-SLSTM framework for human activity recognition using wearable sensor data},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Hybrid deep CNN model for multi-class classification of skin lesion. <em>NCA</em>, <em>37</em>(23), 19479-19499. (<a href='https://doi.org/10.1007/s00521-025-11409-w'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Skin cancer is widely known as the most frequent cancer type worldwide. Early cancer detection is critical as it might cause death if not diagnosed in the early stages. It can be identified through open eyes, but due to great inter-class similarities and intra-class variances, it is difficult to notice. Because of the global prevalence of skin cancers, deep learning is used to create various automated methods to assist physicians in early skin lesion diagnosis. The primary goal of this study is to propose a new tested and calibrated model of deep learning for multiple class categorization of skin lesions. The research community developed numerous Computer-Aided Diagnostic systems (CADs) to support lesion detection. However, this study proposes an automated system of CAD designed for multi-class classification of skin lesions. The dataset used by the authors is HAM1000, and a comprehensive study was performed on three pre-trained Convolutional Neural Network (CNN) models and three hybrid CNN models. To increase efficacy and performance, the Model is suggested and it is multilayered that is designed with great care having multiple with varied sizes of filter, but fewer parameters too. The models’ effectiveness was tested on parameters like precision, the phenomenon of recall, and the score of F1. Maximum accuracy is 91.63% which is reported through a model which is hybrid, and it is proposed in this paper.},
  archive      = {J_NCA},
  author       = {Khan, Mohammad Ahmar and Rastogi, Deependra and Johri, Prashant and Al-Taani, Ahmad and Baghela, Vishwadeepak Singh and Kumud},
  doi          = {10.1007/s00521-025-11409-w},
  journal      = {Neural Computing and Applications},
  month        = {8},
  number       = {23},
  pages        = {19479-19499},
  shortjournal = {Neural Comput. Appl.},
  title        = {Hybrid deep CNN model for multi-class classification of skin lesion},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Efficient few-shot human activity recognition via meta-learning and data augmentation. <em>NCA</em>, <em>37</em>(23), 19461-19477. (<a href='https://doi.org/10.1007/s00521-025-11408-x'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the field of Human Activity Recognition (HAR), the rapid evolution of wearable devices necessitates models that are generalizable and can adapt to entirely new subjects and activities with very limited labeled data. Conventional deep learning models, constrained by their reliance on large training datasets and limited adaptability to novel scenarios, face challenges in these settings. This paper introduces a novel few-shot HAR strategy employing meta-learning, which facilitates rapid adaptation to unseen subjects and activities using minimal annotated samples. Our method augments time series data with a range of transformations, each assigned a learnable weight, enabling the model to prioritize the most effective augmentations and discard the irrelevant ones. Throughout the meta-training phase, the model learns to identify an optimally weighted combination of these transformations, significantly improving the model’s adaptability and generalization to new situations with scarce labeled data. During meta-testing, this knowledge enables the model to efficiently learn from and adapt to a very limited set of labeled samples from completely new subjects undertaking entirely new activities. Extensive experiments on various HAR datasets demonstrate our method’s enhanced adaptability and generalization to tasks never encountered during training, achieving a performance improvement of up to $$2\%$$ across all datasets. These results affirm MADA’s potential for real-world applications characterized by limited data availability.},
  archive      = {J_NCA},
  author       = {Vettoruzzo, Anna and Bouguelia, Mohamed-Rafik and Rögnvaldsson, Thorsteinn},
  doi          = {10.1007/s00521-025-11408-x},
  journal      = {Neural Computing and Applications},
  month        = {8},
  number       = {23},
  pages        = {19461-19477},
  shortjournal = {Neural Comput. Appl.},
  title        = {Efficient few-shot human activity recognition via meta-learning and data augmentation},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Computational fairness in adaptive neural networks. <em>NCA</em>, <em>37</em>(23), 19445-19460. (<a href='https://doi.org/10.1007/s00521-025-11407-y'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Adaptive neural networks offer a promising approach to reduce the computational cost of neural network inference. These models dynamically allocate computational resources based on each individual input, leading to a significant reduction in average computational cost. In this paper, we show that adaptive computation can have unintended consequences, potentially leading to a significantly worse user experience for certain groups. We train several state-of-the-art adaptive neural networks for the task of age prediction on the FairFace dataset and demonstrate that the average computational cost varies considerably across different user demographics, including age, race and gender. Additionally, we analyze the properties of images that influence the computational cost and show that these factors partially explain the observed demographic differences in performance.},
  archive      = {J_NCA},
  author       = {Leroux, Sam and Cornelissen, Ciem and Sharma, Vishisht and Simoens, Pieter},
  doi          = {10.1007/s00521-025-11407-y},
  journal      = {Neural Computing and Applications},
  month        = {8},
  number       = {23},
  pages        = {19445-19460},
  shortjournal = {Neural Comput. Appl.},
  title        = {Computational fairness in adaptive neural networks},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A novel approach to wiring network diagnosis utilizing time domain reflectometry and one-dimensional convolutional neural networks. <em>NCA</em>, <em>37</em>(23), 19423-19444. (<a href='https://doi.org/10.1007/s00521-025-11406-z'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The timely and precise diagnosis of wiring network faults is crucial for the reliable and safe operation of industrial systems. This study introduces an innovative approach to wiring network diagnosis, which employs time domain reflectometry (TDR) and a one-dimensional convolutional neural network (1D-CNN). The principal advantages of the TDR-1D-CNN-based approach include the following: (1) the compact architectural configuration of the 1D-CNN, which solely performs one-dimensional convolutions, rendering it suitable for real-time fault detection and monitoring; (2) the cost-effective and practical implementation in real-time hardware; (3) the capability to function without any predetermined transformation, hand-crafted feature extraction, or feature selection; and (4) the capability of detecting, locating, and characterizing faults in wiring networks, as well as estimating the number of faults within the wiring network. The effectiveness and feasibility of the TDR- and 1D-CNN-based fault diagnosis approach are demonstrated on representative wiring network structures, achieving at least 99% accuracy and macroaverage sensitivity, and surpassing the performance of existing diagnostic methods.},
  archive      = {J_NCA},
  author       = {Goudjil, Abdelhak and Smail, Mostafa Kamel and Bouchekara, Houssem R. E. H. and Pichon, Lionel and Pouliquen, Mathieu and Pigeon, Eric},
  doi          = {10.1007/s00521-025-11406-z},
  journal      = {Neural Computing and Applications},
  month        = {8},
  number       = {23},
  pages        = {19423-19444},
  shortjournal = {Neural Comput. Appl.},
  title        = {A novel approach to wiring network diagnosis utilizing time domain reflectometry and one-dimensional convolutional neural networks},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). The hyperdimensional transform for distributional modeling, regression and classification. <em>NCA</em>, <em>37</em>(23), 19393-19422. (<a href='https://doi.org/10.1007/s00521-025-11405-0'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Hyperdimensional computing (HDC) is an increasingly popular computing paradigm with immense potential for future intelligent applications. Although the main ideas already took form in the 1990s, HDC recently gained significant attention, especially in the field of machine learning and data science. Next to efficiency, interoperability, and explainability, HDC offers attractive properties for generalization as it can be seen as an attempt to combine connectionist ideas from neural networks with symbolic aspects. While the advantages of HDC have been proven empirically, conceptual motivations behind its representations for machine learning models remain somewhat unclear. The approaches often appear ad hoc engineered, offering limited insight into their possible effectiveness. In recent work, we introduced the hyperdimensional transform, outlining theoretical foundations for representing functions and distributions as high-dimensional holographic vectors. Here, we present the potential of the hyperdimensional transform to a broad data science audience and bridge the gap between the mathematical foundations of the transform and the state-of-the-art HDC approaches for machine learning. Besides providing insight into the current state-of-the-art, we show how the hyperdimensional transform leads to a broad, novel, and well-founded toolbox. Next to the standard regression and classification tasks of machine learning, our discussion includes various aspects of statistical modeling, such as representing, learning and deconvolving distributions, sampling, Bayesian inference, and uncertainty estimation.},
  archive      = {J_NCA},
  author       = {Dewulf, Pieter and De Baets, Bernard and Stock, Michiel},
  doi          = {10.1007/s00521-025-11405-0},
  journal      = {Neural Computing and Applications},
  month        = {8},
  number       = {23},
  pages        = {19393-19422},
  shortjournal = {Neural Comput. Appl.},
  title        = {The hyperdimensional transform for distributional modeling, regression and classification},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Torque ripple minimization and maximum power point tracking of wind turbine doubly fed induction generator based on bonobo optimization algorithm. <em>NCA</em>, <em>37</em>(23), 19371-19392. (<a href='https://doi.org/10.1007/s00521-025-11404-1'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {One of the most attractive sources of renewable energy in the world is wind power because of the predicted increase in energy production from wind energy and its good integration with the electrical grid. To cope with the growing need for energy, more robust control strategies are required for wind energy conversion systems (WECSs) for enhancing the overall system efficiency and power quality and achieving a more stable operation of the system. Therefore, for grid-connected doubly fed induction generator (DFIG)-based WECS, an optimization method, based on bonobo optimizer (BO), is presented in this paper to obtain 19 control parameters, which are the parameters of the proportional–integral (PI) controllers of rotor side and grid side converters, capacitance and voltage of the DC bus, the values of a delta connected LC filter for rotor side converter (RSC) and L filter for grid side converter (GSC). The objective of optimization problem is formulated to realize the maximum power point tracking (MPPT) with minimum DFIG torque ripple. Fixed parameters of PI controller and the values of filters and DC bus capacitance and voltage may lead to instability of system at certain wind speeds. Therefore, the optimization variables are obtained for a wide range of wind speeds (4–25 m/s). The optimum power curve and pitch angle values of the wind turbine has been estimated to get the reference values of DFIG mechanical power to have MPPT under different wind speeds. The BO results are compared with a well-known optimization algorithm; particle swarm optimizer (PSO) to verify the accuracy of results. The Matlab environment is used to simulate the 2.4 MW DFIG-based WECS.},
  archive      = {J_NCA},
  author       = {Mostafa, M. Abdelateef and El-Hay, Enas A. and ELkholy, Mahmoud M.},
  doi          = {10.1007/s00521-025-11404-1},
  journal      = {Neural Computing and Applications},
  month        = {8},
  number       = {23},
  pages        = {19371-19392},
  shortjournal = {Neural Comput. Appl.},
  title        = {Torque ripple minimization and maximum power point tracking of wind turbine doubly fed induction generator based on bonobo optimization algorithm},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Data augmentation and feature extraction using deep learning for motor imagery EEG-based brain–computer interface classification. <em>NCA</em>, <em>37</em>(23), 19339-19369. (<a href='https://doi.org/10.1007/s00521-025-11403-2'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Motor imagery EEG-based brain–computer interfaces (BCIs) have recently been developed for communicating between the brain and external devices. One of the most difficult issues in BCI is classifying the brain activity of motor imagery. To achieve optimal results, the most suitable feature extraction and classification approach must be used because of the poor signal-to-noise ratio, low spatial resolution, and variation in brain activity among subjects in EEG. Gathering enough EEG data takes a lot of time and labor since EEG signals are nonstationary and variable. The main contribution of the paper is to propose models based on deep learning to augment EEG trials to improve EEG classification performance for multi-class motor imagery BCI even when small amounts of EEG data are provided for training. To augment the EEG data in this investigation, generative adversarial models and autoencoders were employed. Then, using various autoencoders, discriminative features were extracted from all the training data and augmented signals. The final step involved classifying multi-class motor imagery of the EEG BCI using deep learning and probabilistic graphical models. For the purpose of classifying EEG signals, the effectiveness of our various feature extraction and data augmentation models was examined using a standard BCI benchmark dataset in terms of classification accuracy, Cohen’s kappa, F1-score, and recall. The suggested model outperforms other cutting-edge models with an average classification accuracy of 90.20%, a kappa value of 0.98, and a recall value of 0.99 on the BCI Competition IV dataset 2a. The proposed model is subject-independent since it can be integrated into a single model by employing training on all subjects. Therefore, the proposed models may be of considerable interest for BCI applications in the real world.},
  archive      = {J_NCA},
  author       = {Anjerani, Marzieh and Pedram, Mir Mohsen and Mirzarezaee, Mitra},
  doi          = {10.1007/s00521-025-11403-2},
  journal      = {Neural Computing and Applications},
  month        = {8},
  number       = {23},
  pages        = {19339-19369},
  shortjournal = {Neural Comput. Appl.},
  title        = {Data augmentation and feature extraction using deep learning for motor imagery EEG-based brain–computer interface classification},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). BotMHG: A hybrid deep learning-based graphical approach to detect botnets using graph neural networks and graph attention networks on topological and temporal features. <em>NCA</em>, <em>37</em>(23), 19303-19337. (<a href='https://doi.org/10.1007/s00521-025-11402-3'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The work presents a novel approach for the detecting botnets using graph-based deep learning techniques, recognizing the increasing evolution of cyber threats day-by-day. There is a need for robust botnet detection mechanisms, since bots are constantly evolving and increasingly strengthening to avoid the detection systems. Nowadays, the bots are available for rent that can be used to launch severe attacks on critical infrastructures such as power grid, irrigation, gas distribution, finance, healthcare, and rail networks. The existing rule-based detections can be deceived with dynamic changes in the flow statistics of the network. In this work, BotMHG, a hybrid graph-based system for botnet mitigation is proposed, by modeling network activities as graphs. The network traffic flow is used to form a network graph. The topological and temporal features are extracted from this network graph. The proposed hybrid model is constructed using alternating layers of graph neural network and graph attention network that can capture the relationships among the bots. The model is trained on the extracted features to classify a node. Our approach preprocesses the data to remove inconsistency and class imbalance in the datasets. We train and validate the constructed model on benchmark datasets like CTU-13 and BoT-IoT. Our findings illustrate that the BotMHG model exhibits high accuracy and low false positive rate. The comparative analyses against current flow feature-based methods finds the higher performance in botnet detection by combining graph topological and temporal features. The proposed model is validated based on its statistical significance using Friedman’s, McNemar, Wilcoxon’s, and Mann–Whitney U tests at 5% significant level and shows that the performance is highly significant.},
  archive      = {J_NCA},
  author       = {Mohan, H. G. and Kumar, Jalesh},
  doi          = {10.1007/s00521-025-11402-3},
  journal      = {Neural Computing and Applications},
  month        = {8},
  number       = {23},
  pages        = {19303-19337},
  shortjournal = {Neural Comput. Appl.},
  title        = {BotMHG: A hybrid deep learning-based graphical approach to detect botnets using graph neural networks and graph attention networks on topological and temporal features},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multi-level thresholding segmentation for brain tumor detection using optimized deep learning approach. <em>NCA</em>, <em>37</em>(23), 19279-19302. (<a href='https://doi.org/10.1007/s00521-025-11398-w'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Medical image segmentation and classification are essential for diagnosing brain tumors, which carry a high risk due to the intricate nature of the brain. In this study, we introduce a multi-level thresholding (MLT) approach to achieve high-quality segmentation. The proposed method utilizes deep learning techniques, employing the weighted means of vectors optimization (INFO) algorithm to determine MLT threshold values. Additionally, we fine-tune the hyperparameters of the long Short-term memory (LSTM) neural network, a powerful deep learning architecture, using the INFO algorithm. This LSTM network is employed for classifying the segmented images and detecting brain tumors. The effectiveness of the proposed deep learning approach is assessed on a Kaggle dataset comprising 253 MRI images, including 98 non-tumor and 155 tumor images. We employ performance metrics, including peak signal-to-noise ratio, structural similarity Index, sensitivity, specificity, and accuracy, to evaluate the quality of the deep learning-based segmentation and classification process. The proposed deep learning-based approach consistently outperforms other methods in terms of producing high-quality segmented tumor images and exhibits enhanced detection and classification performance. These findings highlight the effectiveness of the proposed approach in segmenting and classifying brain tumor images.},
  archive      = {J_NCA},
  author       = {Ewees, Ahmed A. and Ismail, Fatma H. and Labeeb, Nada S. and Gaheen, Marwa A.},
  doi          = {10.1007/s00521-025-11398-w},
  journal      = {Neural Computing and Applications},
  month        = {8},
  number       = {23},
  pages        = {19279-19302},
  shortjournal = {Neural Comput. Appl.},
  title        = {Multi-level thresholding segmentation for brain tumor detection using optimized deep learning approach},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). An artificial neural network-based automated identification system for selection of appropriate turbulence model for numerical simulation of tube thermal enhancement using nanofluids. <em>NCA</em>, <em>37</em>(23), 19231-19278. (<a href='https://doi.org/10.1007/s00521-025-11397-x'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The choice of appropriate turbulence model for numerical simulation of tube thermal enhancement using nanofluids by the means computational fluid dynamics (CFD) remains crucial since turbulence models contribute to numerical errors, and minimizing these errors is highly desirable for ensuring maximum accuracy of the simulated results. This selection process, however, via CFD is significantly time-consuming, computationally expensive, problematic and requires tremendous efforts. The difficulty is further augmented when a large range of input variables are involved in the process. Despite the availability of machine learning assisted CFD in recent years, till date, no convenient technique exists to facilitate the choice of turbulence models for simulation of nanofluids tube thermal enhancement with maximum accuracy. As the first stepping stone toward rectifying the issue, this work—via employment of a classification modeling-based surrogate model using artificial neural networks (ANN)—develops an automated system for identifying the most appropriate turbulence model for heat transfer enhancement simulation of tube flows with nanofluids, in order to ensure optimal results of numerical simulation. Three versions of the k-ε turbulence model coupled with the multiphase mixture model—which by far remain as the most heavily used turbulence and flow models—were considered for a wide range of nanofluid configurations and degree of turbulences. The statistical analysis of results shows that the classifier with optimal ANN architecture can detect the most suitable turbulence model for each nanofluid configuration and degree of turbulence with validation and testing accuracies of 94% and 93.55%, respectively. Additionally, it exhibits exceptional discriminatory ability, high robustness and reliable efficiency for both seen and unseen sets of data.},
  archive      = {J_NCA},
  author       = {Al Mahmud, Suaib and Noor, Wazed Ibne and Ibrahim, Azhar Mohd and Ismail, Ahmad Faris},
  doi          = {10.1007/s00521-025-11397-x},
  journal      = {Neural Computing and Applications},
  month        = {8},
  number       = {23},
  pages        = {19231-19278},
  shortjournal = {Neural Comput. Appl.},
  title        = {An artificial neural network-based automated identification system for selection of appropriate turbulence model for numerical simulation of tube thermal enhancement using nanofluids},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Precise fraud detection and risk management with explainable artificial intelligence. <em>NCA</em>, <em>37</em>(23), 19199-19229. (<a href='https://doi.org/10.1007/s00521-025-11396-y'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The exponential growth of online shopping and other digital payment methods has led to a meteoric rise in the use of credit cards. Analyzing consumer data for the purpose of detecting and preventing fraud has been greatly facilitated by machine learning (ML). But most real-world credit card data have irrelevant and redundant features, which makes ML classifiers not work as well. In this research paper, a new approach for detecting and managing risk and segmenting customers in financial systems is introduced, called RiskNet. The proposed approach consists of three main modules, which are (i) data preprocessing module (DPM), (ii) feature selection module (FSM), and (iii) risk predictive module. For the first module (e.g., DPM), the input data are cleaned and preprocessed to be suitable for the next module. Next, during the second module (e.g., FSM) the most important and effective features are selected using the proposed feature selection method called improved whale optimization algorithm. Then, these features are fed to the used classifiers to detect credit card fraud. Finally, to assess the efficacy of RiskNet, experiments were conducted on a real-world financial dataset, and the results demonstrated that RiskNet outperforms existing state of the art. RiskNet delivers precise risk predictions and personalized risk management strategies for each customer, enhancing the overall stability of the financial system.},
  archive      = {J_NCA},
  author       = {Talaat, Fatma M. and Medhat, T. and Shaban, Warda M.},
  doi          = {10.1007/s00521-025-11396-y},
  journal      = {Neural Computing and Applications},
  month        = {8},
  number       = {23},
  pages        = {19199-19229},
  shortjournal = {Neural Comput. Appl.},
  title        = {Precise fraud detection and risk management with explainable artificial intelligence},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Ganetic loss for generative adversarial networks with a focus on medical applications. <em>NCA</em>, <em>37</em>(23), 19177-19197. (<a href='https://doi.org/10.1007/s00521-025-11384-2'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Generative adversarial networks (GANs) are machine learning models that are used to estimate the underlying statistical structure of a given dataset and, as a result, can be used for a variety of tasks, such as image generation or anomaly detection. Despite their initial simplicity, designing an effective loss function for training GANs remains challenging, and various loss functions have been proposed, aiming to improve the performance and stability of generative models. In this study, loss function design for GANs is presented as an optimization problem solved using the Genetic Programming approach. Initial experiments were carried out using a small Deep Convolutional GAN model and the MNIST dataset in order to search experimentally for an improved loss function. The functions found were evaluated on CIFAR-10, with the best function, named GANetic loss, showing better performance and stability compared to the losses commonly used for GAN training. To further evaluate its general applicability to more challenging problems, GANetic loss was applied to two medical applications: image generation and anomaly detection. Experiments were performed with histopathological, gastrointestinal, and glaucoma images to evaluate the GANetic loss in medical image generation, resulting in improved image quality compared to the baseline models. The GANetic loss used for polyp and glaucoma images showed a strong improvement in the detection of anomalies. In summary, the GANetic loss function was evaluated on multiple datasets and applications, where it consistently outperformed alternative losses. Moreover, GANetic loss leads to stable training and reproducible results, a known weak spot of GANs. Code: https://github.com/ZKI-PH-ImageAnalysis/GANetic-Loss .},
  archive      = {J_NCA},
  author       = {Akhmedova, Shakhnaz and Körber, Nils},
  doi          = {10.1007/s00521-025-11384-2},
  journal      = {Neural Computing and Applications},
  month        = {8},
  number       = {23},
  pages        = {19177-19197},
  shortjournal = {Neural Comput. Appl.},
  title        = {Ganetic loss for generative adversarial networks with a focus on medical applications},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Enhancing dung beetle optimization algorithm with hybrid multi-strategy and its engineering applications. <em>NCA</em>, <em>37</em>(23), 19123-19175. (<a href='https://doi.org/10.1007/s00521-025-11371-7'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {An enhanced dung beetle optimization algorithm with hybrid multi-strategy (PCTDBO) is proposed, to balance the convergence speed and diversity of the algorithm. Firstly, a convex lens imaging opposition-based learning strategy is employed to reduce the probability of the algorithm falling into local optima, enhancing the algorithm's global exploration capability. Secondly, inspired by the prairie dog optimization (PDO) algorithm, the strategy of alerting similar predators is used to replace the foraging stage strategy of the dung beetle optimization algorithm, to improve the excessive randomness during the foraging stage, which may lead to instability and slow convergence or even failure to converge. Lastly, to prevent the algorithm from getting stuck in local optima during the exploitation phase, tent chaotic mapping and Cauchy mutation are introduced to enhance the algorithm's ability to escape from local optima. To validate the effectiveness of PCTDBO, benchmark and CEC2017 test functions are tested and compared with state-of-the-art algorithms, and the experimental findings demonstrate that PCTDBO brings about notable enhancements in both convergence speed and optimization accuracy, while also showcasing strong robustness. Additionally, the algorithm has been successfully applied to five practical engineering application problems, and the results indicate that PCTDBO demonstrates good applicability and effectiveness in solving project optimization problems.},
  archive      = {J_NCA},
  author       = {Ma, Zhihai and Liu, Sheng and Xu, Lan},
  doi          = {10.1007/s00521-025-11371-7},
  journal      = {Neural Computing and Applications},
  month        = {8},
  number       = {23},
  pages        = {19123-19175},
  shortjournal = {Neural Comput. Appl.},
  title        = {Enhancing dung beetle optimization algorithm with hybrid multi-strategy and its engineering applications},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). MDA: A multi-agent framework for data analysis task. <em>NCA</em>, <em>37</em>(23), 19103-19122. (<a href='https://doi.org/10.1007/s00521-025-11369-1'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent advancements in large language models (LLMs) have significantly improved their performance across various natural language processing tasks, with remarkable strides in complex tasks. These models demonstrate exceptional generalization abilities and reasoning skills, enabling them to handle multifaceted tasks that involve external knowledge retrieval and logical inference. In this paper, we propose Multi-agent based Data Analysis (MDA), an innovative multi-agent framework that leverages LLMs for complex data analysis tasks. Our approach integrates three key modules: (1) Input Analysis Module identifies user intent and retrieves relevant external knowledge; (2) Generation Module employs an iterative Chain-of-Thought method to generate and refine multiple analysis paths; (3) Refinement Module assesses and enhances the final output using a constructive criticism mechanism. We validate the effectiveness of MDA through extensive experiments on a Chinese high education dataset, demonstrating superior performance compared to baseline methods. Additionally, an ablation study confirms the contribution of each module to the overall success of the framework. This work highlights the potential of LLM-based systems in advancing data analysis methodologies and tackling complex, domain-specific challenges.},
  archive      = {J_NCA},
  author       = {Zhang, Xilin and Kong, Haoran and Gao, Shen},
  doi          = {10.1007/s00521-025-11369-1},
  journal      = {Neural Computing and Applications},
  month        = {8},
  number       = {23},
  pages        = {19103-19122},
  shortjournal = {Neural Comput. Appl.},
  title        = {MDA: A multi-agent framework for data analysis task},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Improving the precision of markov-switching GARCH models using denoising methods and auto encoder neural networks. <em>NCA</em>, <em>37</em>(23), 19087-19102. (<a href='https://doi.org/10.1007/s00521-025-11364-6'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The study aims to integrate autoencoder neural networks with denoising techniques, which will advance the field of volatility modeling for financial assets. The data were recreated using an autoencoder neural network while we applied two distinct denoising techniques—wavelet decomposition and variational mode decomposition (VMD)—to the observed financial returns in this study. The study examines the performance of five financial instruments using daily closing prices: the EURUSD, Bitcoin, gold, WTI crude oil, and the SP 500. The models are estimated with both original and denoised returns. The study runs from November 2015 to December 2023. In light of structural changes in financial systems, the results thus support the notion that the application of denoising techniques lowers potential risks while improving model stability and volatility forecast accuracy. This study emphasizes the necessity of integrating neural networks and improved denoising techniques in the domain of risk analysis and financial modeling. Furthermore, improved signal quality has practical implications for financial risk management by improving volatility estimation, facilitating early anomaly detection, and increasing the robustness of forecasting models in dynamically changing market environments.},
  archive      = {J_NCA},
  author       = {Mubarak, Abdulilah},
  doi          = {10.1007/s00521-025-11364-6},
  journal      = {Neural Computing and Applications},
  month        = {8},
  number       = {23},
  pages        = {19087-19102},
  shortjournal = {Neural Comput. Appl.},
  title        = {Improving the precision of markov-switching GARCH models using denoising methods and auto encoder neural networks},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Optimizing student engagement detection using facial and behavioral features. <em>NCA</em>, <em>37</em>(23), 19063-19085. (<a href='https://doi.org/10.1007/s00521-025-11317-z'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, computer vision and machine learning have achieved remarkable advancements across health care, autonomous systems, and robotics sectors. However, the educational domain, particularly the automated detection of student engagement in online and offline learning environments, remains rich in research opportunities. Detecting student engagement is inherently complex and requires sophisticated interpretive capabilities. This paper introduces a novel approach to automatically detect and classify student engagement by integrating facial images with behavioral and facial features, providing a comprehensive solution to enhance engagement recognition. This study begins by extracting behavioral features and correlating them with pre-defined engagement labels to perform engagement classification using machine learning techniques. In parallel, deep learning models are trained and validated on both image and behavioral features, offering a complementary approach. Additionally, the relationship between facial action units (AUs) and engagement labels is analyzed using three distinct metrics: conditional activation probability, relative activation ratio, and the statistical discriminant coefficient (SDC). The study utilizes publicly available datasets (WACV and DAiSEE) to perform extensive evaluations. Experimental results demonstrate that integrating action units significantly enhances model performance, with XGBoost achieving the highest accuracy (82.9%) among traditional models and EfficientNet reaching the best performance (47.2%) in deep learning experiments. These findings highlight the potential of multimodal approaches in improving real-time engagement detection, offering valuable insights into educational technologies and pedagogy.},
  archive      = {J_NCA},
  author       = {Das, Riju and Dev, Soumyabrata},
  doi          = {10.1007/s00521-025-11317-z},
  journal      = {Neural Computing and Applications},
  month        = {8},
  number       = {23},
  pages        = {19063-19085},
  shortjournal = {Neural Comput. Appl.},
  title        = {Optimizing student engagement detection using facial and behavioral features},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Enhancing power grid frequency stability with an optimized TID-MRAC controller and electric vehicle integration under renewable energy penetration. <em>NCA</em>, <em>37</em>(23), 19037-19061. (<a href='https://doi.org/10.1007/s00521-025-11298-z'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This study presents a direct model reference adaptive control (MRAC) system integrated with a tilt-integral-derivative (TID) controller to improve power grid stability during high-renewable energy sources (RESs) penetration and daily load variations. The MRAC incorporates a self-tuning regulator (STR) that dynamically estimates process parameters, enabling real-time adaptation to system changes. Furthermore, the manta ray foraging optimization (MRFO) algorithm is used to adjust the parameters of the TID-MRAC controller. In addition, system nonlinearities, including generation rate constraints (GRC), governor dead bands (GDB), and communication delay time (CDT), are considered within the optimization framework. The effectiveness of the TID-MRAC controller is demonstrated by comparing its performance against other controllers, such as conventional proportional–integral–derivative (PID), PID-MRAC, and fractional-order PID MRAC-MRFO (FO-PID-MRAC-MRFO) controllers. Furthermore, to counteract the challenges posed by high-RESs penetration, plug-in electric vehicles (PEVs) are integrated to assist in load balancing and dynamic power management. The results demonstrate that the synergy between the TID-MRAC controller and PEVs integration provides a robust and adaptive solution for maintaining frequency stability, even under abnormal grid conditions. This innovative approach ensures improved resilience and reliability in modern power grids with high renewable energy integration.},
  archive      = {J_NCA},
  author       = {Ali, Mustafa M. and Elkasem, Ahmed H. A. and Kamel, Salah and Ali, Ahmed S. and Jaber, Gamal T. Abdel- and Sharkawy, Abdel-Nasser and Khamies, Mohamed},
  doi          = {10.1007/s00521-025-11298-z},
  journal      = {Neural Computing and Applications},
  month        = {8},
  number       = {23},
  pages        = {19037-19061},
  shortjournal = {Neural Comput. Appl.},
  title        = {Enhancing power grid frequency stability with an optimized TID-MRAC controller and electric vehicle integration under renewable energy penetration},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Predicting breast cancer treatment response using transfer learning. <em>NCA</em>, <em>37</em>(23), 19009-19035. (<a href='https://doi.org/10.1007/s00521-025-11206-5'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cancer is the world’s leading cause of death. At present, the challenges which medical doctors face in fighting cancer are increasing. Thus, there is a real need for accurate CAD systems not only for detecting cancer, but also for predicting treatment response, to help in cancer diagnosis and treatment later on. In this work, a well-known algorithm which is Convolutional Neural Network (CNN) will be considered. The mechanism/strategy to deal with a patch of images for women breasts that have been screened mammography indicating whether breast cancer exists or not and what is the action taken by the specialist for that specific patient, in order to build a system design based on deep learning approach to predict the treatment response for the patients later on. The methodology is based on Convolutional Neural Network (CNN to classify the manually collected mammograms into two main stages; before treatment (Stage-1) and after treatment (Stage-2), and then extract keywords from the clinical test report for each patient mammogram to detect that it is normal or abnormal. After that, we will have a classified dataset to deal with in a special manner related to the system requirements, to come up with simulation results after doing coding on the classified dataset. The proposed systems excelled in the MobileNet architecture model by applying 10 K cross-validation in the two stages for the dataset of 213 patients with 1705 images and after data augmentation to reach 5100 images. This model’s results came up with 93.7% accuracy for the first stage, and 93.0% accuracy for the second stage. The proposed approach has great benefits for radiologists and physicians as a new mechanism to support and evaluate the predicting response of breast cancer disease treatment, using a simple, accurate and effective technique.},
  archive      = {J_NCA},
  author       = {Otoom, Mwaffaq and Alzubaidi, Mohammad A. and Al-Azzam, Qais H. A.},
  doi          = {10.1007/s00521-025-11206-5},
  journal      = {Neural Computing and Applications},
  month        = {8},
  number       = {23},
  pages        = {19009-19035},
  shortjournal = {Neural Comput. Appl.},
  title        = {Predicting breast cancer treatment response using transfer learning},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). GPI-tree search: Algorithms for decision-time planning with the general policy improvement theorem. <em>NCA</em>, <em>37</em>(23), 18989-19007. (<a href='https://doi.org/10.1007/s00521-025-11304-4'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In Reinforcement Learning, Unsupervised Skill Discovery tackles the learning of several policies for downstream task transfer. Once these skills are learnt, the question of how best to use and combine them remains an open problem. The General Policy Improvement Theorem (GPI) creates a policy stronger than any individual skill by selecting the highest-valued policy, generally evaluated with Successor Features. However, the GPI policy is unable to mix and combine the skills at decision time to formulate stronger plans. In this paper, we propose to adopt a model-based setting in order to make such planning possible, and formally show that a forward search improves on the GPI policy and any shallower searches under some approximation term. We argue for decision-time planning, and design a family of algorithms, GPI-Tree Search Algorithms, to use Monte Carlo Tree Search (MCTS) with GPI. These algorithms foster the skills and Q-value priors of the GPI framework to guide and improve the search, which we back up with visual intuition for the different design choices. Our experiments show that the resulting policies are much stronger than the GPI policy alone, even under approximation; they can also improve beyond the linear constraint of Successor Features.},
  archive      = {J_NCA},
  author       = {Bagot, Louis and D’eer, Lynn and Latré, Steven and De Schepper, Tom and Mets, Kevin},
  doi          = {10.1007/s00521-025-11304-4},
  journal      = {Neural Computing and Applications},
  month        = {8},
  number       = {23},
  pages        = {18989-19007},
  shortjournal = {Neural Comput. Appl.},
  title        = {GPI-tree search: Algorithms for decision-time planning with the general policy improvement theorem},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Human-AI collaboration in real-world complex environment with reinforcement learning. <em>NCA</em>, <em>37</em>(23), 18957-18987. (<a href='https://doi.org/10.1007/s00521-025-11288-1'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent advances in reinforcement learning (RL) and human-in-the-loop (HitL) learning have made human-AI collaboration easier for humans to team with AI agents. Leveraging human expertise and experience with AI in intelligent systems can be efficient and beneficial. Still, it is unclear to what extent human-AI collaboration will be successful, and how such teaming performs compared to humans or AI agents only. In this work, we show that learning from humans is effective and that human-AI collaboration outperforms human-controlled and fully autonomous AI agents in a complex simulation environment. In addition, we have developed a new simulator for critical infrastructure protection, focusing on a scenario where AI-powered drones and human teams collaborate to defend an airport against enemy drone attacks. We develop a user interface to allow humans to assist AI agents effectively. We demonstrated that agents learn faster while learning from policy correction compared to learning from demonstrations. Furthermore, we show that human-AI collaboration requires lower mental and temporal demands, reduces human effort, and yields higher performance than humans controlling all the agents. In conclusion, our results demonstrate that humans can provide helpful advice to the RL agents, allowing them to improve learning in a multi-agent setting.},
  archive      = {J_NCA},
  author       = {Islam, Md Saiful and Das, Srijita and Gottipati, Sai Krishna and Duguay, William and Mars, Clodric and Arabneydi, Jalal and Fagette, Antoine and Guzdial, Matthew and Taylor, Matthew E.},
  doi          = {10.1007/s00521-025-11288-1},
  journal      = {Neural Computing and Applications},
  month        = {8},
  number       = {23},
  pages        = {18957-18987},
  shortjournal = {Neural Comput. Appl.},
  title        = {Human-AI collaboration in real-world complex environment with reinforcement learning},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Communication with factorized policy gradients in multi-agent deep reinforcement learning. <em>NCA</em>, <em>37</em>(23), 18933-18956. (<a href='https://doi.org/10.1007/s00521-025-11272-9'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In multi-agent deep reinforcement learning (MADRL), agents can learn to communicate to broaden their view and understanding of the environment and their teammates. Previous works on communication in MADRL mainly rely on centralized or independent value functions for learning communication, which cannot differentiate how communicating agents individually contribute to the overall learning process. Moreover, continuous environments that incorporate continuous state/action spaces have received limited attention in previous research. In this paper, we propose a novel architecture for communicating agents and apply centralized but factorized value functions to differentiate how each agent contributes to learning during communication, along with gradient backpropagation. Additionally, to address the complexity introduced by communication, we investigate the use of an attention mechanism that aggregates messages, enabling policies to maintain a fixed input length. We then present a new policy gradient method termed communication with factorized policy gradients (CFPG), featuring full backpropagation from factorized value functions to communicating agents’ architecture. We demonstrate that CFPG can enhance performance and accelerate learning in continuous predator–prey scenarios and multi-agent MuJoCo, when compared to other learning communication methods.},
  archive      = {J_NCA},
  author       = {Zhu, Changxi and Dastani, Mehdi and Wang, Shihan},
  doi          = {10.1007/s00521-025-11272-9},
  journal      = {Neural Computing and Applications},
  month        = {8},
  number       = {23},
  pages        = {18933-18956},
  shortjournal = {Neural Comput. Appl.},
  title        = {Communication with factorized policy gradients in multi-agent deep reinforcement learning},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Learning in public goods games: The effects of uncertainty and communication on cooperation. <em>NCA</em>, <em>37</em>(23), 18899-18932. (<a href='https://doi.org/10.1007/s00521-024-10530-6'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Communication is a widely used mechanism to promote cooperation in multi-agent systems. In the field of emergent communication, agents are typically trained in specific environments: cooperative, competitive or mixed-motive. Motivated by the idea that real-world settings are characterized by incomplete information and that humans face daily interactions under a wide spectrum of incentives, we aim to explore the role of emergent communication when simultaneously exploited across all these contexts. In this work, we pursue this line of research by focusing on social dilemmas. To do this, we developed an extended version of the Public Goods Game, which allows us to train independent reinforcement learning agents simultaneously in different scenarios where incentives are (mis)aligned to various extents. Additionally, agents experience uncertainty in terms of the alignment of their incentives with those of others. We equip agents with the ability to learn a communication policy and study the impact of emergent communication in the face of uncertainty among agents. Our findings show that in settings where all agents have the same level of uncertainty, communication can enhance the cooperation of the whole group. However, in cases of asymmetric uncertainty, the agents that do not face uncertainty learn to use communication to deceive and exploit their uncertain peers.},
  archive      = {J_NCA},
  author       = {Orzan, Nicole and Acar, Erman and Grossi, Davide and Rădulescu, Roxana},
  doi          = {10.1007/s00521-024-10530-6},
  journal      = {Neural Computing and Applications},
  month        = {8},
  number       = {23},
  pages        = {18899-18932},
  shortjournal = {Neural Comput. Appl.},
  title        = {Learning in public goods games: The effects of uncertainty and communication on cooperation},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). From fair solutions to compromise solutions in multi-objective deep reinforcement learning. <em>NCA</em>, <em>37</em>(23), 18867-18897. (<a href='https://doi.org/10.1007/s00521-024-10602-7'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we focus on multi-objective reinforcement learning (RL) where the expected vector returns are aggregated with a concave function. For this generic framework, which includes notably fair optimization in the multi-user setting and compromise optimization in the multi-criteria setting, we present several contributions. After a discussion of its theoretical properties (e.g., need to resort to stochastic policies), we prove a general performance bound that justifies learning a policy using discounted rewards, even if a policy optimal for the average reward is desired. We extend several deep RL algorithms for our problem; notably our adaptation of DQN can learn stochastic policies. In addition, to illustrate the generality of our framework, we consider in the multi-user setting a novel extension of fair optimization in deep RL where users have different entitlements. Our experimental results validate our propositions and also demonstrate its superiority to reward engineering in single-objective RL.},
  archive      = {J_NCA},
  author       = {Qian, Junqi and Siddique, Umer and Yu, Guanbao and Weng, Paul},
  doi          = {10.1007/s00521-024-10602-7},
  journal      = {Neural Computing and Applications},
  month        = {8},
  number       = {23},
  pages        = {18867-18897},
  shortjournal = {Neural Comput. Appl.},
  title        = {From fair solutions to compromise solutions in multi-objective deep reinforcement learning},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Using incomplete and incorrect plans to shape reinforcement learning in long-sequence sparse-reward tasks. <em>NCA</em>, <em>37</em>(23), 18851-18866. (<a href='https://doi.org/10.1007/s00521-024-10615-2'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Reinforcement learning (RL) agents naturally struggle with long-sequence sparse-reward tasks due to the lack of reward feedback during exploration and the problem of identifying the necessary action sequences required to reach the goal. Previous works have used abstract symbolic task knowledge models to speed up RL agents in these tasks by either splitting the task into easier to solve sub-tasks or by creating an artificial dense reward function. These approaches are often limited by their requirement of perfect symbolic knowledge models, which cannot be guaranteed when the abstract symbolic models are provided by humans and in real-world tasks. We introduce exponential plan-based reward shaping, which is able to leverage the ability to learn from experience of RL to compensate deficiencies in incomplete and incorrect abstract symbolic plans and use them to solve difficult tasks faster, while guaranteeing convergence to the optimal policy. Our approach is able to work with plans that miss important steps, include unnecessary extra steps, contain steps that refer ambiguously to both important and useless states, or encode an incorrect order of steps. We use action representations designed by human experts to automatically compute plans to capture the high-level task structure. The abstract symbolic subgoals defined by the plan are used to create dense reward feedback, which signals important states to the RL agent that should be achieved and explored to reach the goal. We show the theoretical advantages of our approach for plans with many steps and show its effectiveness empirically on multiple tasks with different kinds of incomplete or incorrect knowledge.},
  archive      = {J_NCA},
  author       = {Müller, Henrik and Berg, Lukas and Kudenko, Daniel},
  doi          = {10.1007/s00521-024-10615-2},
  journal      = {Neural Computing and Applications},
  month        = {8},
  number       = {23},
  pages        = {18851-18866},
  shortjournal = {Neural Comput. Appl.},
  title        = {Using incomplete and incorrect plans to shape reinforcement learning in long-sequence sparse-reward tasks},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Learning to cooperate against ensembles of diverse opponents. <em>NCA</em>, <em>37</em>(23), 18835-18849. (<a href='https://doi.org/10.1007/s00521-024-10511-9'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The emergence of cooperation in decentralized multi-agent systems is challenging; naive implementations of learning algorithms typically fail to converge or converge to equilibria without cooperation. Opponent modeling techniques, combined with reinforcement learning, have been successful in promoting cooperation, but face challenges when other agents are plentiful or anonymous. We envision environments in which agents face a sequence of interactions with different and heterogeneous agents. Inspired by models of evolutionary game theory, we introduce RL agents that forgo explicit modeling of others. Instead, they augment their reward signal by considering how to best respond to others assumed to be rational against their own strategy. This technique not only scales well in environments with many agents, but can also outperform opponent modeling techniques across a range of cooperation games. Agents that use the algorithm we propose can successfully maintain and establish cooperation when playing against an ensemble of diverse agents. This finding is robust across different kinds of games and can also be shown not to disadvantage agents in purely competitive interactions. While cooperation in pairwise settings is foundational, interactions across large groups of diverse agents are likely to be the norm in future applications where cooperation is an emergent property of agent design, rather than a design goal at the system level. The algorithm we propose here is a simple and scalable step in this direction.},
  archive      = {J_NCA},
  author       = {Perera, Isuri and de Nijs, Frits and García, Julian},
  doi          = {10.1007/s00521-024-10511-9},
  journal      = {Neural Computing and Applications},
  month        = {8},
  number       = {23},
  pages        = {18835-18849},
  shortjournal = {Neural Comput. Appl.},
  title        = {Learning to cooperate against ensembles of diverse opponents},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Int-HRL: Towards intention-based hierarchical reinforcement learning. <em>NCA</em>, <em>37</em>(23), 18823-18834. (<a href='https://doi.org/10.1007/s00521-024-10596-2'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {While deep reinforcement learning (RL) agents outperform humans on an increasing number of tasks, training them requires data equivalent to decades of human gameplay. Recent hierarchical RL methods have increased sample efficiency by incorporating information inherent to the structure of the decision problem but at the cost of having to discover or use human-annotated sub-goals that guide the learning process. We show that intentions of human players, i.e. the precursor of goal-oriented decisions, can be robustly predicted from eye gaze even for the long-horizon sparse rewards task of Montezuma’s Revenge–one of the most challenging RL tasks in the Atari2600 game suite. We propose Int-HRL: Hierarchical RL with intention-based sub-goals that are inferred from human eye gaze. Our novel sub-goal extraction pipeline is fully automatic and replaces the need for manual sub-goal annotation by human experts. Our evaluations show that replacing hand-crafted sub-goals with automatically extracted intentions leads to an HRL agent that is significantly more sample efficient than previous methods.},
  archive      = {J_NCA},
  author       = {Penzkofer, Anna and Schaefer, Simon and Strohm, Florian and Bâce, Mihai and Leutenegger, Stefan and Bulling, Andreas},
  doi          = {10.1007/s00521-024-10596-2},
  journal      = {Neural Computing and Applications},
  month        = {8},
  number       = {23},
  pages        = {18823-18834},
  shortjournal = {Neural Comput. Appl.},
  title        = {Int-HRL: Towards intention-based hierarchical reinforcement learning},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Increasing energy efficiency of bitcoin infrastructure with reinforcement learning and one-shot path planning for the lightning network. <em>NCA</em>, <em>37</em>(23), 18811-18821. (<a href='https://doi.org/10.1007/s00521-024-10588-2'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The lightning network (LN) is a technological solution designed to solve the bitcoin blockchain transaction speed problem by introducing off-chain transactions. Since LN is a sparse and highly distributed network with three predominant routing protocols, its native pathfinding algorithms can potentially find multi-hop payment paths similar from the payment sender’s perspective, but the algorithms themselves have different performance, computational cost, energy consumption, and ultimately different CO2 emissions per step in the pathfinding phase. Bitcoin itself generates approximately 61.4 million tons of CO2 eq. per year. Since the LN is built on top of bitcoin, every small change in its energy consumption can have a significant impact on overall pollution. In this paper, we show that the reinforcement learning (RL) approach can reduce these costs and achieve better performance in terms of energy consumption at each pathfinding step. We introduce one-shot path prediction and propose a RL solution for a network agent that learns its neighborhood and uses local knowledge to cleverly solve the pathfinding problem and outperform native pathfinding algorithms.},
  archive      = {J_NCA},
  author       = {Valko, Danila and Kudenko, Daniel},
  doi          = {10.1007/s00521-024-10588-2},
  journal      = {Neural Computing and Applications},
  month        = {8},
  number       = {23},
  pages        = {18811-18821},
  shortjournal = {Neural Comput. Appl.},
  title        = {Increasing energy efficiency of bitcoin infrastructure with reinforcement learning and one-shot path planning for the lightning network},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Discriminative reward co-training. <em>NCA</em>, <em>37</em>(23), 18793-18809. (<a href='https://doi.org/10.1007/s00521-024-10512-8'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose discriminative reward co-training (DIRECT) as an extension to deep reinforcement learning algorithms. Building upon the concept of self-imitation learning (SIL), we introduce an imitation buffer to store beneficial trajectories generated by the policy, determined by their return. A discriminator network is trained concurrently to the policy to distinguish between trajectories generated by the current policy and beneficial trajectories generated by previous policies. The discriminator’s verdict is used to construct a reward signal for optimizing the policy. By interpolating prior experience, DIRECT is able to act as a reward surrogate, steering policy optimization toward more valuable regions of the reward landscape, thus, toward learning an optimal policy. In this article, we formally introduce the additional components, their intended purpose and parameterization, and define a unified training procedure. To reveal insights into the mechanics of the proposed architecture, we provide evaluations of the introduced hyperparameters. Further benchmark evaluations in various discrete and continuous control environments provide evidence that DIRECT is especially beneficial in environments possessing sparse rewards, hard exploration tasks, and shifting circumstances. Our results show that DIRECT outperforms state-of-the-art algorithms in those challenging scenarios by providing a surrogate reward to the policy and direct the optimization toward valuable areas.},
  archive      = {J_NCA},
  author       = {Altmann, Philipp and Ritz, Fabian and Zorn, Maximilian and Kölle, Michael and Phan, Thomy and Gabor, Thomas and Linnhoff-Popien, Claudia},
  doi          = {10.1007/s00521-024-10512-8},
  journal      = {Neural Computing and Applications},
  month        = {8},
  number       = {23},
  pages        = {18793-18809},
  shortjournal = {Neural Comput. Appl.},
  title        = {Discriminative reward co-training},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Agent-based dynamic thresholding for adaptive anomaly detection using reinforcement learning. <em>NCA</em>, <em>37</em>(23), 18775-18791. (<a href='https://doi.org/10.1007/s00521-024-10536-0'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The complexity and scale of IT systems is increasing dramatically, posing many challenges to real-world anomaly detection. Over the years, there have been extensive studies toward deep learning-based methods focusing on feature learning and anomaly scoring, achieving tremendous success in this area. However, little work has been done on the thresholding problem despite it being a critical factor for detecting anomalies effectively. The commonly used static or expert-defined thresholds have shown a lack of adaptability to non-stationary and evolving time series. In this paper, we model thresholding in anomaly detection as a Markov decision process and propose an agent-based dynamic thresholding (ADT) framework based on a deep Q-network. First, an anomaly scorer such as an autoencoder is employed to obtain feature representations and produce anomaly scores for complex input data. Afterward, by analyzing anomaly scores and other useful environmental information, ADT can automatically provide appropriate binary thresholds, thereby achieving self-adaptive anomaly detection. Additionally, we introduce a rigorous mathematical approach to convert the binary thresholds into more fine-grained continuous thresholds that can adapt to different user requirements and practical situations. The properties of ADT are studied through comprehensive experiments on three real-world datasets and compared with baseline methods, hence demonstrating its thresholding capability, data-efficient learning, stability, and robustness, leading to significantly improved detection performance. Our research underscores the transformative role of reinforcement learning (RL) in providing adaptive anomaly detection, achieving remarkable results with minimal labels for training, and even in scenarios where labels are partially observable or contaminated with noise. To the best of our knowledge, we are the first to exhibit the application of RL for optimal thresholding control, for both binary and continuous thresholding scenarios, within the domain of time series anomaly detection.},
  archive      = {J_NCA},
  author       = {Yang, Xue and Howley, Enda and Schukat, Michael},
  doi          = {10.1007/s00521-024-10536-0},
  journal      = {Neural Computing and Applications},
  month        = {8},
  number       = {23},
  pages        = {18775-18791},
  shortjournal = {Neural Comput. Appl.},
  title        = {Agent-based dynamic thresholding for adaptive anomaly detection using reinforcement learning},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Compact deep learning models for leaf disease classification and recognition in precision agriculture. <em>NCA</em>, <em>37</em>(22), 18379-18399. (<a href='https://doi.org/10.1007/s00521-025-11400-5'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Accurate identification of plant leaf diseases is critical for precision agriculture, as it ensures healthy crop yields and reduces losses. Although numerous state-of-the-art deep learning models based on different architectures, such as convolutional neural networks, Transformers, and graph neural networks, have shown outstanding performance in general classification tasks, their effectiveness in specific applications, such as plant disease classification and recognition, has not been extensively explored. Therefore, evaluating these models is essential to identify the most effective approaches for the task of leaf disease classification and recognition, particularly by leveraging the advantages of pretraining on large-scale datasets such as ImageNet. Moreover, lightweight models are needed for deployment in precision agriculture, as they enable real-time processing on edge devices, ensuring timely interventions in the field. This study evaluates multiple fine-tuned deep learning models based on the three aforementioned architectures, achieving accuracy from 89.30 to 98.70% on several public leaf disease datasets. Additionally, we created a new cucumber leaf dataset with four classes, including a healthy leaf category, comprising a total of 8057 samples, and evaluated models’ performance on this dataset as well. To optimize deployment, we applied post-training quantization to the fine-tuned models, observing only a slight decrease in performance of Transformer-based models from 0.49 to 1.62% while achieving $$\tiny {\sim }$$ 4 $$\times $$ reduction in model storage requirements. The findings suggest that these lightweight models, with their balanced trade-off between accuracy and storage, can be adopted for real-time deployment on edge devices in precision agriculture.},
  archive      = {J_NCA},
  author       = {Mahto, Ishwar Chandra and Mathew, Jimson},
  doi          = {10.1007/s00521-025-11400-5},
  journal      = {Neural Computing and Applications},
  month        = {8},
  number       = {22},
  pages        = {18379-18399},
  shortjournal = {Neural Comput. Appl.},
  title        = {Compact deep learning models for leaf disease classification and recognition in precision agriculture},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Deep learning-based automatic modulation classifier. <em>NCA</em>, <em>37</em>(22), 18367-18378. (<a href='https://doi.org/10.1007/s00521-025-11381-5'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In non-cooperative communication systems, recognition of the modulation scheme at the receiver end is an important functionality in wireless communication systems. The research problem of automatic modulation classification is, therefore, gaining increasing significance in the areas of both civil and defense applications. In this work, we present a deep learning enabled automatic modulation classifier for a class of modulation schemes. Our proposal is based on a Convolutional-Long Short-Term Memory Deep Neural Network model architecture particularly focusing on low signal-to-noise ratio communication links. Radio ML 2016.10b dataset generated by GNU radio and recognized as the benchmark dataset is used for training and testing of the proposed modulation classifier. Through extensive experimental results, it is shown that the proposed model architecture offers significantly improved classification accuracy up to 90.85% at 0 dB signal-to-noise ratio of the communication link. Our model offers a competitive solution to automatic modulation classification problem in complex radio environment.},
  archive      = {J_NCA},
  author       = {Singh, Brahmjit and Bajaj, Vishakha and Jindal, Poonam and Prakash, Chandra},
  doi          = {10.1007/s00521-025-11381-5},
  journal      = {Neural Computing and Applications},
  month        = {8},
  number       = {22},
  pages        = {18367-18378},
  shortjournal = {Neural Comput. Appl.},
  title        = {Deep learning-based automatic modulation classifier},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Assessment of the artificial neural networks based on spherical fuzzy information. <em>NCA</em>, <em>37</em>(22), 18347-18365. (<a href='https://doi.org/10.1007/s00521-025-11322-2'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Frank t-norms and t-conorms, which are flexible and general, are crucial for information fusion. Additionally, the Maclaurin symmetric mean operator, an extension of various mean operators, takes into account the link between multi-criteria arguments, particularly in multi-attribute group decision-making. The objective of this paper is to develop multiple aggregation operators for the spherical fuzzy set framework using the Frank t-norms and t-conorms approaches. Additionally, the study aims to apply the newly developed aggregation operators in the multiple attribute decision making process. First, the definition of the Maclaurin symmetric mean utilizing the Frank t-norms and t-conorms in the setting of spherical fuzzy values in order to use the multi-attribute group decision-making method is given. Next, suggested spherical fuzzy Frank weighted Maclaurin symmetric mean and spherical fuzzy Frank Maclaurin symmetric mean aggregation operators. The fundamental characteristics of these aggregation operators are then listed. Subsequently, a proposal is put forward that considers the utilization of the newly developed assortment of aggregation operators. Furthermore, the multi-attribute group decision-making problem is addressed by including newly specified operators, which are then applied in a case study that assesses the effectiveness of artificial neural networks. This study also discusses how these aggregation operators’ behavior can vary depending on how sensitive metrics are interpreted.},
  archive      = {J_NCA},
  author       = {Alharbi, Rehab and Ahmad, Ali and Azeem, Muhammad and Koam, Ali N. A.},
  doi          = {10.1007/s00521-025-11322-2},
  journal      = {Neural Computing and Applications},
  month        = {8},
  number       = {22},
  pages        = {18347-18365},
  shortjournal = {Neural Comput. Appl.},
  title        = {Assessment of the artificial neural networks based on spherical fuzzy information},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Neural computing analysis of rate coefficients in nanofluid flow over a wedge with nanoparticle aggregation effect: A machine learning approach. <em>NCA</em>, <em>37</em>(22), 18327-18346. (<a href='https://doi.org/10.1007/s00521-025-11296-1'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The importance of nanoparticle aggregation extends to numerous technical applications, including microfluidic systems, thermal control, and thermal exchangers’ devices using nanofluids. The focus of the proposed research is to explore the impact of nanoparticles agglomerations with the significance of thermal radiation on titania–ethylene glycol nanofluid flow past a Riga wedge. Moreover, the effects of homogeneous-heterogeneous chemical reactions are also vital aspects of this investigation. Further, the adjusted Krieger–Dougarty and Maxwell–Bruggeman models applied to access aggregation of nanoparticles. The Runge–Kutta scheme combined with the shooting method is operated to analyze the characteristics of the flow field followed by the suitable transformation used for the conversion of dimensional governing equations to their corresponding non-dimensional form. However, the main focus of this work is enhancing rate coefficients through artificial neural network (ANN)-based regression analysis, supported by the Levenberg–Marquardt algorithm. The variation in rate coefficients with different parameters is presented in tabular form, comparing the presence of nanoparticles in aggregated and non-aggregated environments. The main outcomes are the velocity profile decreases with a higher solid volume fraction and increased porosity parameter results in a higher velocity. Also, nanoparticles without aggregation demonstrate a stronger influence than those that undergo aggregation.},
  archive      = {J_NCA},
  author       = {Pattnaik, P. K. and Mishra, S. R. and Sharma, Abhishek and Sharma, Ram Prakash},
  doi          = {10.1007/s00521-025-11296-1},
  journal      = {Neural Computing and Applications},
  month        = {8},
  number       = {22},
  pages        = {18327-18346},
  shortjournal = {Neural Comput. Appl.},
  title        = {Neural computing analysis of rate coefficients in nanofluid flow over a wedge with nanoparticle aggregation effect: A machine learning approach},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Ensemble interpretation: A unified framework for explanation methods. <em>NCA</em>, <em>37</em>(22), 18303-18326. (<a href='https://doi.org/10.1007/s00521-025-11130-8'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As black-box model is increasingly required to achieve model transparency in high-stake applications, it is important to ensure that the explanations are accurate and reliable. However, it has been demonstrated that the explanations generated by diverse interpretation methods are inconsistent, and there is little insight into which method to choose. To address this challenge, a novel framework is proposed to unify the popular post hoc explanation methods (LIME, SHAP, SmoothGrad, Integrated Gradients, Vanilla Gradients, Gradients $$\times$$ Input and MUSE) involving three categories (perturbation-based, gradient-based and rule-based). This framework is based on Unified Local Function Approximation (ULFA), in which all the post hoc explanation methods have the same mathematical expression with different local approximate functions. The unification not only enables us to make concrete inferences about their explanation reliability (i.e., faithfulness, stability and fairness), but also provides a principle to choose suitable methods. Then, an ensemble interpretation method is designed based on voting axioms to achieve the proposed framework and eliminate the inconsistency of the different explanation methods. The generated ensemble explanation holds higher quality in faithfulness, stability and fairness. Empirically experiments are performed to validate the effectiveness of our method by using real-world and synthetic datasets. The explanation results show that our method is general enough to be applicable for different data modalities.},
  archive      = {J_NCA},
  author       = {Min, Chao and Liao, Guoyong and Wen, Guoquan and Li, Yingjun and Guo, Xing},
  doi          = {10.1007/s00521-025-11130-8},
  journal      = {Neural Computing and Applications},
  month        = {8},
  number       = {22},
  pages        = {18303-18326},
  shortjournal = {Neural Comput. Appl.},
  title        = {Ensemble interpretation: A unified framework for explanation methods},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Gender classification using dorsal NIR hand veins imaging. <em>NCA</em>, <em>37</em>(22), 18275-18301. (<a href='https://doi.org/10.1007/s00521-025-11363-7'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Gender classification is soft biometric trait that can be used for identification, surveillance, databases-indexing, social/medical services, forensic anthropology, and e-marketing. Gender classification has been addressed with face, iris, fingerprints, hand veins of fingers/palm, and hand geometry. The hand has several biometric modalities such as dorsal and palmar hand veins, fingers veins, hand geometry, and fingerprints. Near-infrared dorsal hand veins were not addressed for gender classification. They are hidden under skin and very hard to fake unlike face and fingerprints. They can be imaged using cheap cameras. In this paper, we acquired novel multimodal dataset for 200 subjects, for the purpose of gender classification. The acquired near-infrared image modalities include dorsal hand veins in fisted and landed with wrist positions and hand geometry. At least 5 images per each hand were acquired over several sessions. We also acquired 7 facial expressions and side-view color images. Image enhancements, region of interest extraction, texture mapping, and data augmentation have been proposed. Sixty-three individual convolutional neural networks models were proposed, trained, validated, and tested, to classify gender from twenty-one hand veins image modalities, and for three split ratios. Highest obtained testing accuracy for all individual models is 96.97% with fivefold cross-validation mean value of 95.33%. Higher accuracies were found for females and for left hands as 98.73% and 97.35%. Individual model's classifier probability scores were fused, and fusion accuracies are over 99% for some fusion rules. Superior performance accuracies highlight the potential of our proposed gender classification system for biometric applications, particularly where gender classification accuracy is crucial.},
  archive      = {J_NCA},
  author       = {Badawi, Ahmed and Saber, Alaa and Mohamed, Bassam and Shahin, Mohamed and Abdelrahman, Ahmed},
  doi          = {10.1007/s00521-025-11363-7},
  journal      = {Neural Computing and Applications},
  month        = {8},
  number       = {22},
  pages        = {18275-18301},
  shortjournal = {Neural Comput. Appl.},
  title        = {Gender classification using dorsal NIR hand veins imaging},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Toward trustworthy and sustainable clinical decision support by training ensembles of specialized logistic regressors. <em>NCA</em>, <em>37</em>(22), 18233-18274. (<a href='https://doi.org/10.1007/s00521-025-11360-w'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper presents a novel Mixture of Experts (MoE)-based framework designed to enhance clinical decision-making by balancing predictive accuracy, interpretability, and adaptability. Our approach relies on a set of locally specialized logistic regression models, dynamically selecting the most suitable expert for each instance based on local feature patterns. To enforce sparsity in both the gating mechanism and expert models, we employ the Gumbel-softmax relaxation, enabling end-to-end differentiable selection of both the active expert and the most relevant features for each prediction. By integrating this mechanism, our method improves computational efficiency and generalization while maintaining instance-level interpretability. Unlike black-box models that require post hoc explanation techniques, our solution provides transparency by construction, offering direct insights into feature contributions for each decision. We evaluated our approach on multiple real-world healthcare datasets, spanning both standard clinical classification tasks and process-oriented predictive scenarios. Experimental results demonstrated that our MoE-based framework achieves robust and competitive performance while maintaining lower complexity than black-box methods such as XGBoost and Random Forest and improving generalization over simpler interpretable models, including Decision Trees, Linear Trees, and standard Logistic Regressors. Additionally, our analysis of the trade-off between model complexity and predictive performance shows that our method delivers stable and reliable results across diverse datasets and evaluation metrics. These findings underscore the advantages of an interpretable MoE-based approach in clinical AI, supporting transparent and accountable decision-making.},
  archive      = {J_NCA},
  author       = {Cuzzocrea, Alfredo and Folino, Francesco and Pontieri, Luigi and Sabatino, Pietro and Samami, Maryam},
  doi          = {10.1007/s00521-025-11360-w},
  journal      = {Neural Computing and Applications},
  month        = {8},
  number       = {22},
  pages        = {18233-18274},
  shortjournal = {Neural Comput. Appl.},
  title        = {Toward trustworthy and sustainable clinical decision support by training ensembles of specialized logistic regressors},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Cardiovascular disease detection using stacking meta-learner with pipeline machine learning. <em>NCA</em>, <em>37</em>(22), 18219-18232. (<a href='https://doi.org/10.1007/s00521-025-11347-7'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cardiovascular disease is a major global health concern since number of deaths projected by heart disease may reach up to 22.2 million in 2030. However, diagnosis of cardiovascular disease is a time-consuming task by the healthcare practitioners. Traditional methods of diagnosing CVDs are slow and costly and often require various tests and examinations, such as blood tests, electrocardiograph and blood pressure measurements. Hence, there is need to automate the cardiovascular disease detection using machine learning approach. This paper aims to leverage recent advancements in artificial intelligence and data analytics to create a new automated process for early detection of heart disease using machine learning. This research work presents the novel automated process to detect cardiovascular disease by using stacking ensemble method with pipeline machine learning. Proposed stacking ensemble meta-learner (SEML) model is implemented for early detection of heart disease using logistic regression as meta-learner. The method of combining pipeline machine learning with stacking meta-learner improves the efficiency and effectiveness by automatic handling of data transformation, feature scaling, allowing hyperparameter to run all over estimators and crucial preprocessing task. Diverse base classifiers such as logistic regression, SVM, KNN, NB and XGBoost are trained and merged with meta-learner to produce the prediction. The SEML model achieved 91.80% accuracy in predicting cardiovascular disease. The ROC curve for multiple base classifiers shows AUC scores ranging from 0.75 to 0.95, indicating good model performance. F1 score, precision, recall, MCC score and log loss metrics are used to validate the robustness of the model. Experimental findings reveal that the stacking meta-learner with pipeline approach significantly improved the cardiovascular disease prediction, thereby helping medical professionals or any individual at risk for facilitating early interventions, prevention and treatment plan.},
  archive      = {J_NCA},
  author       = {Uplopwar, Arundhati and Vashisth, Rashmi and Kushwaha, Arvinda},
  doi          = {10.1007/s00521-025-11347-7},
  journal      = {Neural Computing and Applications},
  month        = {8},
  number       = {22},
  pages        = {18219-18232},
  shortjournal = {Neural Comput. Appl.},
  title        = {Cardiovascular disease detection using stacking meta-learner with pipeline machine learning},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A new approach for novel object image captioning based on data key multitask conformer with representative mask. <em>NCA</em>, <em>37</em>(22), 18185-18217. (<a href='https://doi.org/10.1007/s00521-025-11341-z'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Image captioning is the detailed portrayal of the content of an image using natural language, which often requires a large amount of paired image-caption data during training. However, these pretrained models have difficulty adapting to new domains with novel objects that rarely or never appear during training. In this paper, we introduce the zero-shot novel object captioning task, which generates descriptions for novel objects without the need for paired image-caption data and extra training sentences. We propose a model called data key multitask learning conformer with representative mask for novel object image captioning (KMMC) with the following components: (1) introduction of a new multitask learning model based on data keys: data key multitask learning (DKMT). Unlike conventional multitask learning models that use decision heads, our model relies on (key, value) pairs to determine the execution of different tasks. This results in a lightweight model with scalability for multiple arbitrary tasks and maximum parameter sharing; (2) proposing the representative mask (RM) method to share context from seen objects for novel objects, generate multi-attention, and combine it with DKMT to create a robust triplet activation. This approach increases the likelihood of novel object labels appearing in the description compared to other placeholder methods. Our model has been tested on the COCO, NoCaps and ImageNet datasets, yielding competitive results with other SOTAs in novel object captioning.},
  archive      = {J_NCA},
  author       = {Van, Trieu Nghiem and Huu, Quynh Nguyen and Quoc, Tao Ngo},
  doi          = {10.1007/s00521-025-11341-z},
  journal      = {Neural Computing and Applications},
  month        = {8},
  number       = {22},
  pages        = {18185-18217},
  shortjournal = {Neural Comput. Appl.},
  title        = {A new approach for novel object image captioning based on data key multitask conformer with representative mask},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Advancing temporal forecasting: A comparative analysis of conventional paradigms and deep learning architectures on publicly accessible datasets. <em>NCA</em>, <em>37</em>(22), 18173-18184. (<a href='https://doi.org/10.1007/s00521-025-11324-0'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this study, we compare the performance of classical time series models (AR, MA, ARMA, ARIMA) and modern deep learning techniques (LSTM, Bi-LSTM, Seq2Seq) alongside a state-of-the-art Transformers model. These models have been individually studied and applied to specific problems, but the lack of a standardized benchmark for comparing their performance across diverse datasets creates a significant gap in the literature. This research aims to benchmark the performance of representative time-series models across different datasets. We evaluate the models using three publicly available datasets and compare their prediction accuracy using metrics i.e. MSE, RMSE, and MAE. The results show that modern deep learning techniques outperform classical models, with the sim- plest architecture, LSTM, achieving the lowest MSE across all datasets (e.g., for the temperature dataset, LSTM has the lowest RMSE at 0.01, compared to the highest RMSE at 0.21 by ARIMA). Although the Transformers model demon- strated superior performance to classical models, it did not outperform modern techniques. This suggests that for the datasets used in this study, complex Trans- formers architectures may not be necessary to achieve optimal results. This study provides some insights into time series studies.},
  archive      = {J_NCA},
  author       = {Gao, Liang and Jafari, Reza and Jafari, Amir H.},
  doi          = {10.1007/s00521-025-11324-0},
  journal      = {Neural Computing and Applications},
  month        = {8},
  number       = {22},
  pages        = {18173-18184},
  shortjournal = {Neural Comput. Appl.},
  title        = {Advancing temporal forecasting: A comparative analysis of conventional paradigms and deep learning architectures on publicly accessible datasets},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Synthetic chest X-ray data generation for tuberculosis infection detection using generative adversarial networks. <em>NCA</em>, <em>37</em>(22), 18151-18171. (<a href='https://doi.org/10.1007/s00521-025-11388-y'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Tuberculosis (TB) is the deadliest disease from a single infectious agent, ranking above malaria, HIV/AIDS and COVID-19. The World Health Organization (WHO) states that treating both active tuberculosis (ATB) and tuberculosis infection (TBI) can reduce tuberculosis mortality to fewer than one death per million by 2050. In this context, the WHO recommends the use of computer-aided detection (CAD) for screening TB as part of a world-wide elimination plan. Recently, CADs have been composed of deep learning models trained with medical images as a tool for classification, segmentation and synthetic image generation. Medical images are scarcer than nature pictures, hence one of the primary gaps in producing more accurate models. Therefore, we propose a framework with three generative adversarial networks (GAN) (i.e., Wasserstein GAN, GAN Pix2Pix, Cycle-GAN) as a synthetic data generate strategy to enlarge TB-related data availability while introducing diversity into the classifier training process of a CAD classifier model. We emphasize that among the synthetic production of chest radiographs (CXR) related to TB, we have created synthetic images from data collected in TBI studies, a novelty to our knowledge.},
  archive      = {J_NCA},
  author       = {Nascimento, Otto Tavares and de Seixas, José Manoel and Trajman, Anete},
  doi          = {10.1007/s00521-025-11388-y},
  journal      = {Neural Computing and Applications},
  month        = {8},
  number       = {22},
  pages        = {18151-18171},
  shortjournal = {Neural Comput. Appl.},
  title        = {Synthetic chest X-ray data generation for tuberculosis infection detection using generative adversarial networks},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Human detection in UAV imagery using deep learning: A review. <em>NCA</em>, <em>37</em>(22), 18109-18150. (<a href='https://doi.org/10.1007/s00521-025-11446-5'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep learning models have enabled real-time human detection in Unmanned Aerial Vehicle (UAV) imagery for various applications, such as autonomous navigation and surveillance. Despite recent advances, detecting small objects—such as humans in UAV imagery—remains challenging. Although research activity has increased over the past few years, no literature review specifically focused on human detection in UAV imagery has been identified. Given the complexity and importance of this task, this study analyzes articles published over the last five years to address this gap. A synthesis of the principal detection methods, the specific challenges of small object detection, and the corresponding evaluation metrics are presented. Unlike previous review articles, this paper identifies and discusses original research papers focused on human detection in UAV imagery, as well as the major UAV datasets containing annotated persons. Although small object detection remains challenging for single-stage models, “You Only Look Once” (YOLO) architectures are predominantly adopted in UAV human detection research (70% of the reviewed original articles employ a YOLO model or one of its variants). The primary bottlenecks identified for accurate human detection in UAV imagery involve the limited availability of large, properly annotated datasets, leading 48% of the studies to rely on bespoke datasets, and the integration of optical and thermal imagery, which is addressed in 15% of the research. Cloud-based solutions and the deployment of lightweight, fast models on mobile devices using embedded boards emerge as the key directions for future work.},
  archive      = {J_NCA},
  author       = {Simões, Débora Paula and Oliveira, Henrique Cândido de and Marsico, Salvatore and Souza, Jefferson Rodrigo de and Barbosa, Luciano Aparecido},
  doi          = {10.1007/s00521-025-11446-5},
  journal      = {Neural Computing and Applications},
  month        = {8},
  number       = {22},
  pages        = {18109-18150},
  shortjournal = {Neural Comput. Appl.},
  title        = {Human detection in UAV imagery using deep learning: A review},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). GUNNEL: Guided mixup augmentation and multi-model fusion for aquatic animal segmentation. <em>NCA</em>, <em>37</em>(22), 18091-18108. (<a href='https://doi.org/10.1007/s00521-025-11417-w'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent years have witnessed great advances in object segmentation research. In addition to generic objects, aquatic animals have attracted research attention. Deep learning-based methods are widely used for aquatic animal segmentation and have achieved promising performance. However, there is a lack of challenging datasets for benchmarking. In this work, we build a new dataset dubbed “Aquatic Animal Species." We also devise a novel GUided mixup augmeNtatioN and multi-modEl fusion for aquatic animaL segmentation (GUNNEL) that leverages the advantages of multiple segmentation models to segment aquatic animals effectively and improves the training performance by synthesizing hard samples. Extensive experiments demonstrated the superiority of our proposed framework over existing state-of-the-art instance segmentation methods.},
  archive      = {J_NCA},
  author       = {Le, Minh-Quan and Le, Trung-Nghia and Nguyen, Tam V. and Echizen, Isao and Tran, Minh-Triet},
  doi          = {10.1007/s00521-025-11417-w},
  journal      = {Neural Computing and Applications},
  month        = {8},
  number       = {22},
  pages        = {18091-18108},
  shortjournal = {Neural Comput. Appl.},
  title        = {GUNNEL: Guided mixup augmentation and multi-model fusion for aquatic animal segmentation},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). OneEncoder: A lightweight framework for efficient multimodal training. <em>NCA</em>, <em>37</em>(22), 18067-18090. (<a href='https://doi.org/10.1007/s00521-025-11435-8'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cross-modal alignment learning integrates information from different modalities like text, image, audio, and video to create unified models. This approach develops shared representations and learns correlations between modalities, enabling applications such as visual question answering and audiovisual content analysis. Current techniques rely on large modality-specific encoders, necessitating fine-tuning or training from scratch on vast aligned datasets (e.g., text-image, text-audio, and image-audio). This approach has several limitations: (i) It is highly costly, as it requires training large encoders on vast datasets, (ii) It is difficult to achieve, since obtaining large, well-aligned paired datasets is difficult, and (iii) It is time-consuming, due to the fact that introducing new modalities necessitates retraining the entire framework to accommodate them. To address these issues, we propose OneEncoder, a lightweight framework that progressively represents and aligns four modalities (image, text, audio, and video). Initially, we train a lightweight Universal Projection (UP) module to align image and text modalities. Then, we freeze the pretrained UP and progressively align future modalities to those already aligned. OneEncoder operates efficiently and cost-effectively, even in scenarios where vast aligned datasets are unavailable, due to its lightweight design. Trained on small paired datasets, it shows strong performance in tasks like classification, querying, and visual question answering, surpassing methods that rely on large datasets and specialized encoders.},
  archive      = {J_NCA},
  author       = {Faye, Bilal and Azzag, Hanane and Lebbah, Mustapha and Bouchaffra, Djamel},
  doi          = {10.1007/s00521-025-11435-8},
  journal      = {Neural Computing and Applications},
  month        = {8},
  number       = {22},
  pages        = {18067-18090},
  shortjournal = {Neural Comput. Appl.},
  title        = {OneEncoder: A lightweight framework for efficient multimodal training},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Deep learning techniques for analyzing peripheral blood smears: A meta-analysis. <em>NCA</em>, <em>37</em>(22), 18039-18065. (<a href='https://doi.org/10.1007/s00521-025-11401-4'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This meta-analysis synthesizes insights from over 400 peer-reviewed studies and critically examines the transformative impact of deep learning models in peripheral blood smear analysis. This review categorizes and evaluates prominent methodologies like convolutional neural networks, object detection frameworks, and segmentation techniques, emphasizing their significance in classification, counting, morphological analysis, disease detection, and cell segmentation. This analysis indicates significant advancements in automated diagnostic tools, especially in identifying disease in blood cells. Some critical gaps remain, including the limited exploration of differential white blood cell counting, the underdevelopment of automated morphological analysis, and the absence of standardized evaluation criteria. A notable lack of large, diverse, and well-annotated datasets also contributes to the generalizability and robustness of deep learning models. To fully realize the potential of deep learning technologies in improving diagnostic accuracy and clinical decision-making in hematology, these findings underscore the need to address these gaps in future research.},
  archive      = {J_NCA},
  author       = {Margret, Issac Neha and Rajakumar, K.},
  doi          = {10.1007/s00521-025-11401-4},
  journal      = {Neural Computing and Applications},
  month        = {8},
  number       = {22},
  pages        = {18039-18065},
  shortjournal = {Neural Comput. Appl.},
  title        = {Deep learning techniques for analyzing peripheral blood smears: A meta-analysis},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A joint learning classification for intent detection and slot filling from classical to deep learning: A review. <em>NCA</em>, <em>37</em>(22), 17993-18037. (<a href='https://doi.org/10.1007/s00521-025-11329-9'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In a dialogue system, the natural language understanding component plays a critical role in enabling effective communication. The two core tasks within this component are intent detection and slot filling. Intent detection identifies the user’s goal, while slot filling extracts relevant information to fulfill that goal. Traditionally, these tasks were approached separately or in a pipeline-like manner. However, recent studies have emphasized the benefits of solving them jointly due to their natural interconnections. This study explores the evolution of joint learning models for intent detection and slot filling from 2008 to 2024, covering both classical and deep learning approaches. It discusses the limitations of classical models, which led to the rise of deep learning techniques, and introduces a new taxonomy for joint learning classifying joint learning architectures. Key benchmark datasets, evaluation metrics, and the challenges faced by joint models are also analyzed. Finally, the review identifies open research questions and proposes directions for future exploration in this field.},
  archive      = {J_NCA},
  author       = {Muhammad, Yusuf Idris and Salim, Naomie and Zainal, Anazida and Suhaili, Sinarwati Mohammad},
  doi          = {10.1007/s00521-025-11329-9},
  journal      = {Neural Computing and Applications},
  month        = {8},
  number       = {22},
  pages        = {17993-18037},
  shortjournal = {Neural Comput. Appl.},
  title        = {A joint learning classification for intent detection and slot filling from classical to deep learning: A review},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). An epsilon constraint-based evolutionary algorithm and multi-objective quality metrics for combined economic emission dispatch problem. <em>NCA</em>, <em>37</em>(22), 17963-17992. (<a href='https://doi.org/10.1007/s00521-025-11330-2'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Solving combined economic emission dispatch (CEED) problem optimizes power generation by balancing cost minimization with emission reduction, addressing economic and environmental goals simultaneously. This trade-off results in a Pareto front, where each non-dominated solution represents an optimal balance between costs and emissions. Decision-makers can select solutions based on their preferences. This paper proposes an Epsilon-based multi-objective genetic algorithm (MOGA) to solve the CEED problem. By integrating evolutionary techniques and Epsilon constraint methods, the proposed method actively explores diverse solutions, avoiding local optima and enhancing the Pareto front. The two-objective CEED problem is reformulated into two single-objective problems, alternately minimizing cost or emissions. The Epsilon constraint algorithm accelerates the search for optimal solutions. Two quality indicators are proposed to evaluate the Pareto fronts. The first indicator measures solution spread; it assesses the diversity of generator settings and the dominated volume. The second indicator evaluates the uniformity of solution distribution, where smaller distances indicate better uniformity. High spread and uniform distribution signify superior Pareto fronts. If diversity is insufficient, the proposed Epsilon-based MOGA continues to refine the front. The performance of the proposed method was tested on IEEE 30-bus and 118-bus systems, showing improved results compared to RNSGA-II, the Epsilon constraint algorithm, and NSGA-II. The proposed method produced more diverse and uniformly distributed non-dominated solutions; it offers grid operators a broader range of options to balance costs and emissions. Additionally, it achieved lower costs and emissions. The computational time for the larger IEEE 118 system is manageable and does not increase exponentially compared to the smaller IEEE 30 system.},
  archive      = {J_NCA},
  author       = {Chan, Kit Yan and Yiu, Ka Fai Cedric and Kim, Dowon},
  doi          = {10.1007/s00521-025-11330-2},
  journal      = {Neural Computing and Applications},
  month        = {8},
  number       = {22},
  pages        = {17963-17992},
  shortjournal = {Neural Comput. Appl.},
  title        = {An epsilon constraint-based evolutionary algorithm and multi-objective quality metrics for combined economic emission dispatch problem},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Exploring the frontiers of image super-resolution: A review of modern techniques and emerging applications. <em>NCA</em>, <em>37</em>(22), 17913-17961. (<a href='https://doi.org/10.1007/s00521-025-11331-1'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Super-resolution (SR) aims to reconstruct high-resolution images from low-resolution inputs, with deep learning advancements driving substantial improvements in SR performance. This paper presents a comprehensive review of single- and multi-image SR techniques, analyzing findings from 12,873 research papers published between 2015 and 2025 in the computer science field. Key insights are derived from fifteen summary tables covering various SR tasks, including natural, medical, video, burst, depth map, and underwater image SR. The analysis highlights several major findings: (1) the integration of specialized modules, such as attention mechanisms, has led to consistent yearly improvements in performance metrics like PSNR and SSIM; (2) domain-specific architectures often outperform general models, particularly in medical and underwater SR applications; (3) while benchmark datasets enable objective comparisons, real-world validation remains limited, reducing the generalizability of current approaches; (4) inconsistent metric reporting across studies hampers reproducibility and fair evaluation; and (5) practical deployment considerations, including computational efficiency and real-time processing, are rarely addressed. Despite significant progress, challenges such as the need for more diverse training datasets, robust validation, and better interpretability persist. This review synthesizes these critical findings, offering an updated perspective on SR advancements, emerging trends, and future research directions.},
  archive      = {J_NCA},
  author       = {Hassan, Esraa and El-Rashidy, Nora and Elbedwehy, Samar and Abd El-Hafeez , Tarek and Saber, Abeer and Shams, Mahmoud Y.},
  doi          = {10.1007/s00521-025-11331-1},
  journal      = {Neural Computing and Applications},
  month        = {8},
  number       = {22},
  pages        = {17913-17961},
  shortjournal = {Neural Comput. Appl.},
  title        = {Exploring the frontiers of image super-resolution: A review of modern techniques and emerging applications},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). MICSA: A multi-strategy integrated chameleon swarm algorithm for community detection with a new objective function. <em>NCA</em>, <em>37</em>(22), 17887-17912. (<a href='https://doi.org/10.1007/s00521-025-11266-7'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Community detection (CD) unveiling the latent structural organization behind the complex networks holds substantial value in various applications. Although metaheuristic algorithms have been introduced to effectively address the CD, most methods are prone to prematurely converge to the local optimum. Furthermore, the common objective function such as modularity may fail to evaluate the diverse characteristic of communities. Therefore, we propose a multi-strategy integrated chameleon swarm algorithm (MICSA) for CD to effectively avoid local optima and provide fast convergence speed. Furthermore, a novel multi-objective function is presented by introducing the metric in directed graph of software module clustering to CD for further evaluating the various characteristic of communities. Additionally, various strategies are combined to generate high-quality initial population. An adaptive multi-granularity searching strategy implements different mutation operators according to converge. And elites prey searching strategy is proposed to utilize the community structure among elite individuals. These strategies are integrated with prey searching stage to escape from local optimum and enhance the exploitation capability. The vector similarity strategy utilized graph embedding is combined with prey capture stage to differentiate complex relationships between nodes. Extensive experiments in both types of networks demonstrate the superiority of MICSA compared with fifteen classical and state-of-the-art algorithms.},
  archive      = {J_NCA},
  author       = {Xie, Wentao and Kang, Yan and Huang, Xin and Yang, Mingjian and Yuan, Hu},
  doi          = {10.1007/s00521-025-11266-7},
  journal      = {Neural Computing and Applications},
  month        = {8},
  number       = {22},
  pages        = {17887-17912},
  shortjournal = {Neural Comput. Appl.},
  title        = {MICSA: A multi-strategy integrated chameleon swarm algorithm for community detection with a new objective function},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Neural network-guided motion blur estimation and patch representation method for face hallucination. <em>NCA</em>, <em>37</em>(22), 17859-17886. (<a href='https://doi.org/10.1007/s00521-025-11275-6'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Face hallucination (or super-resolution) is a technique of generating high-resolution face images from low-resolution (LR) inputs. The current face hallucination algorithms struggle with motion blur, a common issue in captured images due to various factors, including camera defocusing, objects in motion, etc. To address this problem, a new motion blur robust face hallucination algorithm via neural network-guided motion blur estimation and patch representation (NNMEPR) is proposed in this paper. The NNMEPR algorithm first estimates the motion blur kernels i.e., length ( $$\lambda$$ ) and angle ( $$\alpha$$ ) from a motion-blurred LR test faces using a neural network fitting approach with scale conjugate gradient (SCG) learning algorithm. Then, the estimated factor is embedded in the LR dictionary face images to make them compatible with the test images and mitigate the impact of motion blur from the reconstruction process. Moreover, the proposed algorithm employs a neighboring position patch representation in the correlation coefficient calculation process to sustain the sharp edges and texture details in the resulting HR face images. Experiments on standard datasets and locally captured real-world faces demonstrate the superior performance of the proposed NNMEPR algorithm over the existing state-of-the-art techniques.},
  archive      = {J_NCA},
  author       = {Kumar, Banti and Rajput, Shyam Singh},
  doi          = {10.1007/s00521-025-11275-6},
  journal      = {Neural Computing and Applications},
  month        = {8},
  number       = {22},
  pages        = {17859-17886},
  shortjournal = {Neural Comput. Appl.},
  title        = {Neural network-guided motion blur estimation and patch representation method for face hallucination},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). An overview of transformers for video anomaly detection. <em>NCA</em>, <em>37</em>(22), 17825-17857. (<a href='https://doi.org/10.1007/s00521-025-11218-1'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Transformer is a kind of deep neural network that relies on the technique of self-attention and used initially in the field of natural language processing. Scientists use transformer for computer vision (CV) applications because of its good data representation capabilities. Transformer-based models yield similar performance or surpass other network architectures, including convolutional and recurrent neural networks, in a variety of visual benchmarks. In this work, we investigate the methods for video anomaly detection (VAD) using vision transformer models in the recent literature. The main topics we explore comprise vision transformers used in CV applications with a special focus on VAD methods leveraging transformer architecture. We also briefly present anomaly detection methods based on transformers. Additionally, we address the advantages, challenges and current limitations of the transformer architecture as well as potential solutions to address the technical challenges. In the concluding section of this study, we offer avenues for further investigation concerning the use of vision transformers in VAD tasks.},
  archive      = {J_NCA},
  author       = {Dilek, Esma and Dener, Murat},
  doi          = {10.1007/s00521-025-11218-1},
  journal      = {Neural Computing and Applications},
  month        = {8},
  number       = {22},
  pages        = {17825-17857},
  shortjournal = {Neural Comput. Appl.},
  title        = {An overview of transformers for video anomaly detection},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A scoping and bibliometric review of deep learning techniques in breast cancer imaging: Mapping the landscape and future directions. <em>NCA</em>, <em>37</em>(22), 17759-17823. (<a href='https://doi.org/10.1007/s00521-025-11215-4'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, there has been a rise in interest in the application of deep learning (DL) to breast cancer identification. This work intends to deliver a review demonstrating new DL applications for identifying and categorizing breast cancer and offers a bibliometric analysis of advancements in this field. A comprehensive search between January 1, 2016, and February 20, 2023, was done on the Scopus database to perform the bibliometric analysis of DL applications in breast cancer detection. The statistics on publications, journals, authors, nations, and keywords were created using VOSviewer software. Moreover, Medline (through PubMed), Web of Science (WoS), and Scopus were used for the scoping review (from January 1, 2016, to December 24, 2022); all retrieved titles were checked for eligibility requirements by the Preferred Reporting Items for Systematic Reviews and Meta-Analysis (PRISMA) methodology. Since 2016, the frequency of papers being recovered has grown, and in 2022, a significant number of citations were retrieved. This paper gives a general overview of the various DL methodologies and tailored architectures for the detection, categorization, and segmentation of breast cancer. Performance of the approaches is assessed using accuracy, sensitivity, specificity, area under curve (AUC), F1 score, Dice similarity coefficient (DSC), and intersection over union (IoU); results with high accuracy are frequently obtained at the expense of sensitivity.},
  archive      = {J_NCA},
  author       = {Rezayi, Sorayya and Nilashi, Merhbakhsh and Esmaeeli, Erfan and Ramezanghorbani, Nahid and Arji, Goli and Ahmadi, Hossein and Shahmoradi, Leila and Zahmatkeshan, Maryam},
  doi          = {10.1007/s00521-025-11215-4},
  journal      = {Neural Computing and Applications},
  month        = {8},
  number       = {22},
  pages        = {17759-17823},
  shortjournal = {Neural Comput. Appl.},
  title        = {A scoping and bibliometric review of deep learning techniques in breast cancer imaging: Mapping the landscape and future directions},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Navigating intent-based networking: From user descriptions to deployable configurations. <em>NCA</em>, <em>37</em>(22), 17723-17758. (<a href='https://doi.org/10.1007/s00521-025-11193-7'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Network automation development has accompanied network evolution due to its significant role in speeding up and simplifying network operations. Emerging networking and computing paradigms such as information-centric networks, next-generation networks, cloud, and edge computing and recent innovative technologies, such as the Internet of things (IoT), enabled novel network services (such as the Internet of Vehicles (IoV), context-aware applications, virtual reality, and augmented reality) that demand complex configurations and management. Intent-based networking (IBN) is a promising networking paradigm that provides abstract and autonomous network management. IBN promises to simplify configuring networking devices, allowing network engineers and service providers to focus on providing the expected services and continuously verifying that the network operates within the desired status. An IBN process starts by expressing the user requirement in a high-level descriptive format. Then, the IBN system translates these requirements to a low-level deployable format in a process called intent translation. In this work, we formally define the intent translation process and propose a generic intent translation system. Furthermore, we review the research on intent translation published between 2018 and 2022. We analyze and classify the proposed intent translation schemes and discuss the challenges and recent trends in intent translation.},
  archive      = {J_NCA},
  author       = {AlSamarneh, Ala’ A. and Al-Hammouri, Ahmad T. and Al-Jarrah, Omar Y.},
  doi          = {10.1007/s00521-025-11193-7},
  journal      = {Neural Computing and Applications},
  month        = {8},
  number       = {22},
  pages        = {17723-17758},
  shortjournal = {Neural Comput. Appl.},
  title        = {Navigating intent-based networking: From user descriptions to deployable configurations},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Cross-modal heterogeneous graph reasoning network for visual question answering. <em>NCA</em>, <em>37</em>(22), 17701-17721. (<a href='https://doi.org/10.1007/s00521-025-11261-y'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Most current Visual Question Answering (VQA) methods struggle to achieve effective cross-modal interaction between visual and semantic information, resulting in difficulties in accurately combining visual content with contextual semantics for answer prediction. To address this problem, a Cross-modal Heterogeneous Graph Reasoning Network (CHGRN) is proposed for VQA, incorporating a novel Cross-modal Reasoning Module (CRM) to enhance the interactive analysis between images and questions, enabling more profound joint reasoning of visual and semantic features. The CRM improves cross-modal information reasoning by effectively analyzing and understanding the visual semantic information in the target regions of images. Additionally, an answer type prediction module is introduced, employing multi-task learning with answer type annotations to filter out irrelevant semantic information, thereby improving reasoning accuracy. Moreover, the semantic-assisted attention-aligned decoder ensures precise alignment between visual and semantic data. Extensive experiments demonstrate that the proposed CHGRN achieves excellent performance in visual question answering and outperforms most state-of-the-art methods on widely used public datasets.},
  archive      = {J_NCA},
  author       = {Zhang, Jing and Teng, Jiong and Ding, Weichao and Wang, Zhe},
  doi          = {10.1007/s00521-025-11261-y},
  journal      = {Neural Computing and Applications},
  month        = {8},
  number       = {22},
  pages        = {17701-17721},
  shortjournal = {Neural Comput. Appl.},
  title        = {Cross-modal heterogeneous graph reasoning network for visual question answering},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Enhancing unit commitment in power systems with transmission loss analysis. <em>NCA</em>, <em>37</em>(22), 17675-17699. (<a href='https://doi.org/10.1007/s00521-025-11350-y'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the increasing competition in the electricity market and the rising energy demand, unit commitment has become a critical and challenging task in power systems. Unit commitment is essential for effective planning and scheduling in modern power systems, contributing significantly to energy economics by reducing annual costs. This study employs the General Algebraic Modeling System (GAMS) using the Branch and Reduce Optimization Navigator (BARON) solver combined with the Monarch Butterfly Optimization (MBO) algorithm. It considers equality and inequality constraints of various units, system power balance, and losses. The BARON solver in GAMS and the MBO method addresses a range of units and system constraints to find optimal solutions. To evaluate the performance of these methods, simulations are conducted on three different systems: the IEEE 14-bus system with five generators, the IEEE 30-bus system with six generators, and the Uttar Pradesh 75-bus system with fifteen generators. Comparative analyses are performed by comparing the results from the BARON solver in GAMS with other available solvers in GAMS and the MBO approach. The significant percentage improvement of the proposed technique over the MBO method is seen in the higher test systems compared to small-scale test systems. The improvement in operating costs without and with transmission losses is 0.223% and 0.377%, respectively, for the 75-bus Uttar Pradesh utility data relative to the binary MBO approach. These findings indicate that BARON consistently surpasses other solvers and the MBO technique in minimizing total operating costs, ensuring robustness and constraint satisfaction levels. The practical implications of the research findings underscore the relevance and applicability of these methods in unit commitment optimization, providing a convincing and reassuring conclusion to this study.},
  archive      = {J_NCA},
  author       = {Kumar, Vineet and Naresh, R. and Sharma, Sumit and Dev, Ark},
  doi          = {10.1007/s00521-025-11350-y},
  journal      = {Neural Computing and Applications},
  month        = {8},
  number       = {22},
  pages        = {17675-17699},
  shortjournal = {Neural Comput. Appl.},
  title        = {Enhancing unit commitment in power systems with transmission loss analysis},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Cyber resilience in shipboard microgrids: Adaptive hybrid artificial intelligent methods and systematic review. <em>NCA</em>, <em>37</em>(22), 17633-17674. (<a href='https://doi.org/10.1007/s00521-025-11090-z'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cyber resilience is paramount in modernizing maritime transportation; however, cyberattacks pose significant challenges in deploying resilience, remote control, and monitoring technologies. Shipboard microgrids (SMGs) are not immune to cyber threats, given their tightly integrated cyber-physical processes managed through advanced software. With extensive power electronic components, SMGs are highly vulnerable to attacks, compromising their security and stability. False data injection attacks and potential malware, spoofing, and jamming attacks threaten the integrity and safety of SMGs. Robust attack detection and prevention techniques are crucial for enhancing SMG resilience, stability, and adaptive capability and stabilizing them at their optimum operating state. This study addresses cutting-edge cybersecurity issues concerning SMGs, offering an inclusive review of attacks and protective measures across critical maritime systems for attack detection, prevention, and countermeasures for SMGs. To illustrate the cyber resilience in the SMG using artificial intelligence methods, a novel cyberattack detection method integrating the Hilbert–Huang transform (HHT) and deep learning (DL) is proposed and used as a case study. An advanced one-dimensional convolutional neural network (1D-CNN) with the Adam optimizer and HHT for signal feature extraction has been employed along with the construction of multiple-input basis models within the DL framework for automatic intrinsic feature extraction from raw signal fluctuations. Various basis patterns are generated using a 1D-CNN-based autoencoder–decoder system for deep feature extraction, optimizing the ensemble’s performance to detect attack types using weighted 1D-CNN models. The simulation results demonstrate the efficiency and reliability of the proposed method, achieving a 94.75% accuracy rate for attack detection across diverse scenarios in DC SMGs. This underscores the robustness and practical applicability of the proposed method for enhancing SMG operation security. In addition, the potential future trends and challenges in SMG cybersecurity are discussed.},
  archive      = {J_NCA},
  author       = {Ali, Zulfiqar and Su, Chun-Lien and Terriche, Yacine and Rouhani, Seyed Hossein and Hoang, Le Quang Nhat and Sadiq, Muhammad and Tsao, Shao-Hang and Abbas, Syed Zagam and Ahmad, Ejaz and Elsisi, Mahmoud},
  doi          = {10.1007/s00521-025-11090-z},
  journal      = {Neural Computing and Applications},
  month        = {8},
  number       = {22},
  pages        = {17633-17674},
  shortjournal = {Neural Comput. Appl.},
  title        = {Cyber resilience in shipboard microgrids: Adaptive hybrid artificial intelligent methods and systematic review},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A next-generation hybrid energy converter empowered by machine learning: Pioneering sustainable integration of photovoltaic and grid power. <em>NCA</em>, <em>37</em>(22), 17609-17632. (<a href='https://doi.org/10.1007/s00521-025-11058-z'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Hybrid energy systems are increasingly critical in addressing the growing demand for sustainable and efficient power solutions. In this paper, a novel converter for a hybrid energy system with the capability to integrate two power sources of different characteristics, namely AC and DC is proposed. This paper aims to enhance the efficiency of hybrid energy systems that involve multiple power conversions and necessitate multiple power converters. The pivotal aspect of the proposed converter lies in its ability to connect photovoltaic (PV) and grid power sources. Diverging from conventional setups, this converter eliminates the need for a diode rectifier, streamlining the power conversion process and mitigating complexities associated with multiple stage conversions between DC and AC power stages. The proposed converter shows versatility by operating solely on grid power, solar power, or a combination of grid and solar power, and it is able to change its operating mode by adapting dynamically to varying power availability. The proposed converter with proposed flat-topped waveform has 5.69% voltage THD, which is 87.50% less than conventional system voltage waveform. Various regression models, such as trees, Gaussian processes regression (GPR), ensembles of trees, support vector machine, and neural network were trained and tested to forecast the PV power. Among these, the squared exponential GPR model outperforms other regression models, exhibiting the least root mean square error of 0.16745 and mean square error of 0.02841. The paper further analyses the behavior of the proposed converter in water pumping systems used for residential, commercial, and irrigation applications. The operating modes of the converter are determined by machine learning-based power predictions, influencing transitions between grid and solar power as well as the concurrent utilization of both sources. This research provides insights into the transient behaviors during these operational mode changes, contributing to a comprehensive understanding of the converter's performance.},
  archive      = {J_NCA},
  author       = {Agrawal, Nikhil and Agarwal, Anshul and Kanumuri, Tirupathiraju},
  doi          = {10.1007/s00521-025-11058-z},
  journal      = {Neural Computing and Applications},
  month        = {8},
  number       = {22},
  pages        = {17609-17632},
  shortjournal = {Neural Comput. Appl.},
  title        = {A next-generation hybrid energy converter empowered by machine learning: Pioneering sustainable integration of photovoltaic and grid power},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Optimizing the placement of distributed energy storage and improving distribution power system reliability via genetic algorithms and strategic load curtailment. <em>NCA</em>, <em>37</em>(22), 17589-17608. (<a href='https://doi.org/10.1007/s00521-025-11037-4'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As the integration of distributed generation (DG) and smart grid technologies grows, the need for enhanced reliability and efficiency in power systems becomes increasingly paramount. Energy storage systems (ESS) play a crucial role in achieving these objectives, particularly in enabling effective islanding operations during emergencies. This research leverages genetic algorithms to identify optimal combinations of ESS units and strategic load curtailment techniques to mitigate potential contingencies. The results demonstrate that integrating ESS significantly improves network reliability, optimizes infrastructure utilization, and is particularly beneficial in regions with high commercial and industrial load percentages. Moreover, the study underscores the critical role of load shedding in balancing reliability improvements and the sizing of DG units, offering valuable insights for future energy management approaches. By employing binary load curtailment strategies, the research determines the optimal location and size of ESS and DG units within the distribution network. The analysis reveals that ESS integration leads to a reduction in net annual costs while providing valuable insights into network reliability through the assessment of expected energy not supplied. Two case studies comparing various storage technologies with a base case without ESS highlight the cost-effectiveness of enhancing system reliability through distributed storage allocation. The study also emphasizes the significance of load shedding in balancing reliability enhancement and DG sizing, examining the effects of DG placement across different network buses. Finally, this article investigates the impact of diverse load mixes, indicating the need for larger ESS units as the proportion of commercial and industrial loads increases, accompanied by rising costs. Through these comprehensive analyses, the study offers valuable insights into optimizing the placement of distributed storage units and improving the reliability of distribution systems.},
  archive      = {J_NCA},
  author       = {Islam, Anfaj and Rudra, Souman and Kolhe, Mohan Lal},
  doi          = {10.1007/s00521-025-11037-4},
  journal      = {Neural Computing and Applications},
  month        = {8},
  number       = {22},
  pages        = {17589-17608},
  shortjournal = {Neural Comput. Appl.},
  title        = {Optimizing the placement of distributed energy storage and improving distribution power system reliability via genetic algorithms and strategic load curtailment},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Resilient fractional-order fuzzy-based tilt control of renewable-dominated hybrid power system. <em>NCA</em>, <em>37</em>(22), 17571-17587. (<a href='https://doi.org/10.1007/s00521-025-11015-w'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The research conducted for this work has focused on the design and implementation of resilient fractional-order fuzzy integral tilt derivative with filter (FOF-ITDF) controller for renewable-dominated hybrid power system (HPS). The considered HPS employs solar thermal, wind power, diesel engine, aqua electrolyzer, and fuel cell as the distributed generation system. Moreover, flywheel, battery, and ultra-capacitor (UC) are chosen as the energy storage components. The parameters of FOF-ITDF controller are tuned via a hybrid optimization termed as artificial gorilla troops optimizer (GTO) with gradient-based optimizer (GBO). This proposed controller shows enhanced performance over fuzzy proportional–integral–derivative (PID), fuzzy ITDF, and FOF-PID controllers in terms of dynamic as well as steady state. Proposed GTO-GBO: FOF-ITDF controller provides highly robust operation against rate constraint non-linearities, parameter variations, and communication time delays (CTDs). Finally, the performance assessment of the proposed control architecture is also benchmarked over real-power system data via a standard IEEE-14 bus system with its stability evaluation by bode plot. By utilizing the proposed control design, the maximum frequency deviation $$\left(\Delta f\right)$$ in the worst situation, i.e., CTD of 0.3 s, is − 0.024 Hz, which is much less and under the admissible range of IEEE standard.},
  archive      = {J_NCA},
  author       = {Sah, Sneha V. and Prakash, Vivek and Pathak, Pawan Kumar and Yadav, Anil Kumar},
  doi          = {10.1007/s00521-025-11015-w},
  journal      = {Neural Computing and Applications},
  month        = {8},
  number       = {22},
  pages        = {17571-17587},
  shortjournal = {Neural Comput. Appl.},
  title        = {Resilient fractional-order fuzzy-based tilt control of renewable-dominated hybrid power system},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). AI-enabled frequency synchronization control considering FDI attack using metaheuristic algorithm. <em>NCA</em>, <em>37</em>(22), 17541-17570. (<a href='https://doi.org/10.1007/s00521-024-10961-1'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The advancement in centralized power systems has elevated the power grid into a sophisticated smart grid, emblematic of cyber-physical systems (CPS), susceptible to diverse types of false data injection (FDI) and cyber threats. Among these threats, load frequency control (LFC) systems, crucial for regulating power in tie-lines and ensuring frequency synchronization, are particularly vulnerable to FDI attacks. These attacks pose substantial risks to system continuity, stability, and reliability. Modern power systems consist of multi-power sharing hubs and communication systems integrated with dynamic load demand, such as electric transportation (electric bicycles and cars). This complex mechanism requires a stable power sharing mechanism to meet the load demands of residential, commercial, and charging infrastructure, as well as a robust CPS design that can withstand cyber attacks. This paper introduces a novel control algorithm consisting of a grasshopper optimization algorithm (GOA), a proportional derivative filter (PDF), and a proportional integral (PI). We use the algorithm $$\text {GOA}-\text {PDF}+(0.75+\text {PI})$$ to optimize the LFC on multiple load deviations. Afterward, a supervisory control and data acquisition system trains an artificial intelligence/machine learning-based model to detect FDI attacks in a multi-area network of centralized renewable energy power systems. Initially, we trained a Levenberg–Marquardt fast neural network (LMFNN) on data related to renewable centralized power generation, frequency aberrations, tie-line power deviations, electrical vehicle recharging, and active power load deviations in both areas. Finally, to find FDI, we compare the LMFNN’s output control signal with the actual output of the plant to find residuals that show FDI attacks. This gives us a remarkable 0.99 regression coefficient score (R), which lets us tell the difference between systems that are under attack and those that are working normally. The efficacy of the proposed technique is demonstrated through simulation models at Matlab 2023b, considering centralized power generation from solar, wind, and thermal power plants. This approach presents a promising solution for improving the resilience of centralized power grids to FDI attacks while maintaining their operational integrity and security.},
  archive      = {J_NCA},
  author       = {Ahmad, Hasnain and Gulzar, Muhammad Majid and Mustafa, Ghulam and Khan, Abdul Qayyum and Habib, Salman and Ahmed, Ijaz},
  doi          = {10.1007/s00521-024-10961-1},
  journal      = {Neural Computing and Applications},
  month        = {8},
  number       = {22},
  pages        = {17541-17570},
  shortjournal = {Neural Comput. Appl.},
  title        = {AI-enabled frequency synchronization control considering FDI attack using metaheuristic algorithm},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). An improved swarm intelligence for power system economic operations based on optimal power generation to control congestion in transmission channels. <em>NCA</em>, <em>37</em>(22), 17513-17540. (<a href='https://doi.org/10.1007/s00521-024-10952-2'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Power system congestion is a key challenge in the deregulated era. Finding ways to transmit power without congestion is crucial. Achieving optimal generator power output with the application of evolutionary algorithms is a primary method to manage this issue. This paper proposes a novel modified whale optimization algorithm (MWOA) for the optimal power generation to address the power system economic operations for the nonlinear transmission congestion management cost problem. The MWOA has been developed by introducing two correction factors in the exploration and exploitation phases that enhances its performance and coordination in these two phases. The MWOA strikes a good balance between exploring and exploiting WOA phases. Its capabilities are validated using benchmark functions and compared to other heuristic algorithms. The MWOA has been applied to accomplish minimum congestion cost by optimal adjustment in the generator power output while alleviating congestion in the lines. Generator sensitivity factors are computed to identify the most sensitive generators that are participating in the power rescheduling process. The results show that MWOA lowered congestion costs by 6.00%, 4.45%, 3.14%, and 2.30% for the 39-bus system, 7.91%, 4.29%, 2.57%,1.69% for the 30-bus system, and 10.44%, 8.48%, 7.36%, 5.17%, 2.00% for 118-bus system when compared to FEP, GA, DE, and original WOA. Statistical and comparative analysis has shown that with MWOA there has been a considerable reduction in the congestion cost, system losses, amount of power rescheduled, and improved in the system bus voltages when contrasted with other referred optimization algorithms.},
  archive      = {J_NCA},
  author       = {Kumari, Pooja and Paul, Kaushik and Kumar, Niranjan and Sinha, Pampa and Agarwal, Krishna Kant and Vidyarthi, Ankit and Alkhayyat, Ahmed},
  doi          = {10.1007/s00521-024-10952-2},
  journal      = {Neural Computing and Applications},
  month        = {8},
  number       = {22},
  pages        = {17513-17540},
  shortjournal = {Neural Comput. Appl.},
  title        = {An improved swarm intelligence for power system economic operations based on optimal power generation to control congestion in transmission channels},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). BOHDL model: A robust framework for fault detection and classification in ring/radial distribution systems. <em>NCA</em>, <em>37</em>(22), 17493-17512. (<a href='https://doi.org/10.1007/s00521-024-10847-2'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Distribution systems are constantly at risk of failure due to various factors, including lightning strikes, equipment aging, human mistakes, and breakdown of power system components. These occurrences impact the system's dependability, leading to costly repairs, lost productivity, and customer power outages. So, to reduce these issues, this work presents a peculiarity hybrid model (BOHDL) for determining and classifying distribution line faults for different topologies. The model uses fault current signal time series obtained by modeling and simulating an IEEE standard 13 bus test Network with PSCAD. It uses a convolutional neural network for extracting features from the training data automatically and a support vector machine model (having a good generalization ability). For classification, four pretrained convolutional neural network architectures are used in this work, which are AlexNet, GoogleNet, SqueezeNet, and ResNet. To enhance the classification accuracy, two parameters of SVM, i.e., Box Constraint and Kernel Scale, are optimized using the Bayesian approach. The effect of four different Kernel functions, i.e., polynomial, RBF, Gaussian, and Linear, on the SVM algorithm's performance is analyzed. The performance of the evolved model is assessed by calculating classification accuracy, kappa score, precision, sensitivity, and specificity. The simulation results obtained after training and testing validate the efficacy of the evolved model by giving 100% detection accuracy for both ring and radial topologies of power distribution systems. The proposed scheme has achieved a maximum of 99.98% and 96.89% fault classification accuracy for radial and ring topologies, respectively. This paper highlights the innovative aspects of the research, including the development of a novel hybrid model using advanced machine learning techniques for fault detection and classification, and rigorous validation of the model's performance using comprehensive evaluation metrics. These contributions position the proposed work as a significant advancement in the field of distribution system reliability and fault management.},
  archive      = {J_NCA},
  author       = {Tiwari, Garima and Saini, Sanju and Minaxi},
  doi          = {10.1007/s00521-024-10847-2},
  journal      = {Neural Computing and Applications},
  month        = {8},
  number       = {22},
  pages        = {17493-17512},
  shortjournal = {Neural Comput. Appl.},
  title        = {BOHDL model: A robust framework for fault detection and classification in ring/radial distribution systems},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A new intelligent approach for frequency controller of autonomous hybrid power systems. <em>NCA</em>, <em>37</em>(22), 17473-17492. (<a href='https://doi.org/10.1007/s00521-024-10635-y'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {An intelligent approach for load frequency control based on a proportional-integral-derivative (PID) controller, referred to as an intelligent PID (IPID) controller, is proposed to enhance the frequency stability of autonomous hybrid power systems (HPSs) considering the high penetration levels of renewables. The proposed IPID is based on the ultra-local mod4el, i.e., a model-free control, which operates independently of the power system parameters since the controller incorporates a term for unknown disturbances, enhancing its control action. Moreover, a new meta-heuristic optimization algorithm, i.e., Geometric Mean Optimizer (GMO), is used to fine-tune the proposed IPID controller parameters for frequency regulation of the studied HPS. Furthermore, the superior performance of the proposed IPID controller based on the GMO is validated by comparing it with other counterpart controllers used; such as the PID and fractional-order PID (FOPID) controllers under different load/renewables perturbations, system uncertainties, and high renewables penetration. The Matlab simulation results demonstrate that the proposed IPID controller using GMO offers greater stability in reducing response deviations compared to traditional PID and FOPID controller structures. Where the proposed IPID controller demonstrated a 80% improvement in performance compared to the traditional PID controller, and a 57% improvement over the FOPID controller in handling scenarios with random load demand and high renewables penetration. Finally, to combine the accuracy of physical simulation and the adaptability of numerical simulation, the proposed IPID controller based on the GMO is verified and implemented in a real-time environment based on the PLECS RT Box platform.},
  archive      = {J_NCA},
  author       = {Magdy, Gaber and Bakeer, Abualkasim and Bakeer, Mahmoud and Albalawi, Hani and Zaid, Sherif A.},
  doi          = {10.1007/s00521-024-10635-y},
  journal      = {Neural Computing and Applications},
  month        = {8},
  number       = {22},
  pages        = {17473-17492},
  shortjournal = {Neural Comput. Appl.},
  title        = {A new intelligent approach for frequency controller of autonomous hybrid power systems},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A novel MODWT–local pattern transformation feature fusion approach for high-impedance fault detection in medium voltage power distribution networks. <em>NCA</em>, <em>37</em>(22), 17457-17471. (<a href='https://doi.org/10.1007/s00521-024-10863-2'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Due to the arcing nature, the high-impedance fault (HIF), which typically occurs in medium voltage (MV) distribution networks, poses a risk to equipment, personnel, and livestock. Early fault detection can save lives and prevent equipment destruction. Since the fault current in a power system is contained within the normal current range, identifying the occurrence of HIF is difficult. Using maximal overlap discrete wavelet transform (MODWT) and a combination of MODWT–local pattern transformations (local binary pattern, local gradient pattern, local neighbor gradient pattern, and local neighbor descriptive pattern), the paper analyses the current signals from radial and mesh distribution networks and features extracted during HIF and non-HIF (line-to-ground (LG), double line (LL), double line-to-ground (LLG), and triple line-to-ground (LLLG) fault conditions). For the first time in power distribution networks for fault analysis, the suggested algorithm performs the HIF detection in the MV distribution system. To determine the best feature sets from the extracted features, the Kruskal–Wallis test was performed. Bidirectional long short-term memory (Bi-LSTM) was used for the chosen feature sets to classify the data as HIF or non-HIF. MODWT–LGP fusion achieves the highest accuracy for both networks among the four algorithms.},
  archive      = {J_NCA},
  author       = {Varghese, P. Rini and Subathra, M. S. P. and Peter, Geno and Stonier, Albert Alexander and Kuppusamy, Ramya and Teekaraman, Yuvaraja},
  doi          = {10.1007/s00521-024-10863-2},
  journal      = {Neural Computing and Applications},
  month        = {8},
  number       = {22},
  pages        = {17457-17471},
  shortjournal = {Neural Comput. Appl.},
  title        = {A novel MODWT–local pattern transformation feature fusion approach for high-impedance fault detection in medium voltage power distribution networks},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Steady-state linear kalman filter-based FLL control for DSTATCOM with optimized PI gains. <em>NCA</em>, <em>37</em>(22), 17435-17455. (<a href='https://doi.org/10.1007/s00521-024-10525-3'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Steady-state linear Kalman filter-based frequency locked loop control algorithm is introduced to operate distribution static compensator in distribution system which utilizes Kalman filter theory which employs the dynamic state space vectors with its input signal frequency close to its nominal value resulting in low computational burden to determine the noisy parameters with uncertainty. Moreover, steady-state linear Kalman filter provides faster dynamic response which can be observed in tracking of the actual and reference signal in zero-voltage regulation mode within 3 cycles as compared to its counterpart of synchronous reference frame with stability. In reckoning the fine tuning of PI gains for maintaining the value of the AC terminal voltages, two methods, namely crayfish optimization algorithm and particle swarm optimization, have been implemented and their responses are compared for various parameters like peak overshoot, rise time, and fall time. The proposed filter is assessed by MATLAB model and hardware analysis on a digital signal processor in real time which shows the results of the steady-state and dynamic conditions with DC PI settling within two cycles and as per the standards of IEEE 519-2014.},
  archive      = {J_NCA},
  author       = {Pandya, Hardik M. and Arya, Sabha Raj and Giri, Ashutosh K.},
  doi          = {10.1007/s00521-024-10525-3},
  journal      = {Neural Computing and Applications},
  month        = {8},
  number       = {22},
  pages        = {17435-17455},
  shortjournal = {Neural Comput. Appl.},
  title        = {Steady-state linear kalman filter-based FLL control for DSTATCOM with optimized PI gains},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Artificial intelligence-based system frequency response modeling considering contribution of inverter-based resources. <em>NCA</em>, <em>37</em>(22), 17423-17434. (<a href='https://doi.org/10.1007/s00521-024-10487-6'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper proposes an artificial intelligence-based approach to address the contribution of inverter-based resources (IBRs) in the system frequency response (SFR) model. IBRs are assumed to be aggregated within a dynamic virtual power plant (DVPP), a new entity that can be summoned by the system operator to provide ancillary services. Current SFR models in the presence of IBRs are computationally expensive and neglect the synchronization dynamics between conventional synchronous generators (SGs) and IBRs. In conventional SFR models, the interactions between SGs were formulated through a unique synchronization coefficient. However, in modern power systems, the interaction between SGs and IBRs within DVPPs should be represented by a 2 × 2 synchronization coefficients (SCs) matrix. Hence, an attempt is made to deal with the calculation of the SCs matrix using optimization algorithms and a neural network. More precisely, optimization algorithms are employed to calculate the SCs at different prespecified operating points, which are then fed into a neural network for training purposes, enabling online SFR derivation. The proposed approach is characterized by its low computational burden and complexity, which are critical for accommodating different levels of DVPP penetration.},
  archive      = {J_NCA},
  author       = {Feizi, Amir and Golpîra, Hêmin},
  doi          = {10.1007/s00521-024-10487-6},
  journal      = {Neural Computing and Applications},
  month        = {8},
  number       = {22},
  pages        = {17423-17434},
  shortjournal = {Neural Comput. Appl.},
  title        = {Artificial intelligence-based system frequency response modeling considering contribution of inverter-based resources},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Hyper-parameter tuned yearly and seasonal models for aggregated short-term electricity demand prediction. <em>NCA</em>, <em>37</em>(22), 17403-17421. (<a href='https://doi.org/10.1007/s00521-024-10568-6'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article suggests a pooling categorical boosting (CatBoost) regression approach for yearly and seasonal models for aggregated short-term electricity demand prediction (STEDP). The approach was evaluated using full 4 years of aggregated electricity demand data. First, the seasonal behavior of the data is analyzed by visualization plots. The suggested model has been trained with different hyper-parameter settings with Grid Search (GS) optimization technique. The results indicated that the suggested model reduces the variance and bias problems and also overcomes from overfitting problems with less time and more efficiency. The variance problem is checked by changing the length of the training dataset and the bias problem is checked by evaluating different performance parameters. Three trending boosting Machine Learning (ML) models and four different technique-based traditional ML models were developed, evaluated, and compared to the proposed model to prove its superiority. Moreover, further comparison of yearly data with seasonal data of different training lengths indicated that the prediction of aggregated electricity demand could be improved by taking into consideration their demand behavior changes during different seasons. The results show the suggested model defeats the overfitting problem by 9.64% as compared to Random Forest (RF) and 4.35% as compared to Support Vector Regression (SVR).},
  archive      = {J_NCA},
  author       = {Panigrahi, Radharani and Patne, Nita R. and Pemmada, Sumanth and Manchalwar, Ashwini D.},
  doi          = {10.1007/s00521-024-10568-6},
  journal      = {Neural Computing and Applications},
  month        = {8},
  number       = {22},
  pages        = {17403-17421},
  shortjournal = {Neural Comput. Appl.},
  title        = {Hyper-parameter tuned yearly and seasonal models for aggregated short-term electricity demand prediction},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Optimization of environmental impact inclusive cost for sustainable distributed energy resources in distribution system. <em>NCA</em>, <em>37</em>(22), 17389-17401. (<a href='https://doi.org/10.1007/s00521-024-10531-5'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The proliferation of renewable energy (RE) and increasing electrical demand stress the existing generation and transmission systems. Moreover, due to environmental concerns, the proper utilization of small-scale renewable distributed energy resources (DERs) has become essential. DERs can play a pivotal role in evolving grids by reducing emissions and improving energy delivery by providing system services like voltage maintenance. Optimal sizing and sitting of these DERs into the distribution system promise various benefits and a viable solution to overcome the limitations of large transmission and conventional generation systems and provide better voltage maintenance. Besides, utilizing realistic data generation from working plants and varying load patterns allows for better insights into power system operations under growing RE. This work presents a profit-based optimal sizing and sitting of DERs into the distribution system while considering environmental costs. An improved Harmony Search algorithm has been applied to optimize the proposed cost-based objective function, which consists of various costs in generating distributed energy while simultaneously considering the environmental expenses incurred due to the release of different pollutant gasses. Also, the generation data of working solar and wind plants and realistic varying loads across seasons, weekdays, and weekend days have been considered to explore real operating scenarios. Efficacy has been verified from the optimization results that illustrate its capability to allocate DERs in conjunction with reduced costs and emissions, facilitating optimal system operations. The algorithm has been tested for two standard distribution systems focusing on reducing overall generation costs, social costs labelled due to emissions, improvement in voltage profile, and reduction in overall losses of the system.},
  archive      = {J_NCA},
  author       = {Firdous, Aaquib and Tyagi, Arjun and Tyagi, V. V. and Kumar, Bhavnesh},
  doi          = {10.1007/s00521-024-10531-5},
  journal      = {Neural Computing and Applications},
  month        = {8},
  number       = {22},
  pages        = {17389-17401},
  shortjournal = {Neural Comput. Appl.},
  title        = {Optimization of environmental impact inclusive cost for sustainable distributed energy resources in distribution system},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Optimal distributed generation and shunt capacitor bank placement in microgrid distribution planning for enhanced performance. <em>NCA</em>, <em>37</em>(22), 17363-17388. (<a href='https://doi.org/10.1007/s00521-024-10503-9'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The transformation of traditional power distribution networks with the emerging technological revolution of communication technology, semiconductor devices and information technology according to the concept of smart grid and microgrid strategic planning leads to a better system in terms of reliability, cost-effectiveness, robustness and high efficiency. However, among many influential factors, proper distributed generation (DGs) and capacitor placement during the planning stage are vital in bringing efficient power handling capability considering all the system constraints. With this motivation, this paper proposed an enhanced differential evolution-based metaheuristic approach (EDE) for the optimal sitting of DGs and shunt capacitor banks (SCBs) in microgrid radial distribution systems. A single-objective formulation has been taken based on the real power loss minimization meeting the rise of power demand, considering the enhancement of the voltage profile and stability as other priorities. Various equality and inequality system operational constraints are considered in the formulation to make the approach practicable for real-time condition application. In addition to the above, the approach is tested through simulation under different operating conditions related to the power factor variation, load angle variations, types of DGs and loads. Extensive comparative analysis has been done for the IEEE 33-bus and 69-bus, along with other prominent methods suggested by various authors recently. It is observed that the proposed approach exhibits a substantial reduction in active power loss for both the 33-bus and 69-bus systems. The numerical results reflect that the proposed approach effectively handles large and complex optimization problems concerning distribution system planning.},
  archive      = {J_NCA},
  author       = {Malika, Binaya Kumar and Pattanaik, Vivekananda and Sahu, Binod Kumar and Rout, Pravat Kumar and Panda, Subhasis and Bajaj, Mohit},
  doi          = {10.1007/s00521-024-10503-9},
  journal      = {Neural Computing and Applications},
  month        = {8},
  number       = {22},
  pages        = {17363-17388},
  shortjournal = {Neural Comput. Appl.},
  title        = {Optimal distributed generation and shunt capacitor bank placement in microgrid distribution planning for enhanced performance},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). LSTM-based maneuver detection for resident space object catalog maintenance. <em>NCA</em>, <em>37</em>(22), 17341-17362. (<a href='https://doi.org/10.1007/s00521-025-11177-7'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Maintaining a catalog of Resident Space Objects involves a wide range of activities, exploiting measurement processing to estimate and update orbits, in an effort to achieve a comprehensive picture of the Near-Earth environment status in terms of human-made objects. The first step required to update a cataloged orbit is to correctly associate it with new observations. This task is particularly hampered by the growing presence of active satellites that are capable of maneuvering. Nonetheless, a large part of them perform routine maneuvers to preserve their orbit, defining very regular patterns. Given the significant quantity of past orbital and maneuvering data about tracked objects as the main by-product of catalog maintenance, the main focus of this work is to effectively exploit them by training Machine Learning models. The objective is to infer the probability of a target maneuver within a specific time horizon, using a Long Short-Term Memory Recurrent Neural Network, specialized in processing time sequences. Two distinct maneuver detection modules are developed and tested on real LEO object data to, respectively, extend a target’s control history and predict how likely a maneuver will happen in the immediate future.},
  archive      = {J_NCA},
  author       = {Cipollone, Riccardo and Frueh, Carolin and Di Lizia, Pierluigi},
  doi          = {10.1007/s00521-025-11177-7},
  journal      = {Neural Computing and Applications},
  month        = {8},
  number       = {22},
  pages        = {17341-17362},
  shortjournal = {Neural Comput. Appl.},
  title        = {LSTM-based maneuver detection for resident space object catalog maintenance},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Meta-reinforcement learning guidance, navigation, and control for autonomous lunar landing with safe site selection. <em>NCA</em>, <em>37</em>(22), 17311-17340. (<a href='https://doi.org/10.1007/s00521-025-11274-7'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper presents a novel guidance, navigation, and control architecture for planetary landing based on a stabilized seeker guidance algorithm and an autonomous safe landing site selection system. The seeker tracks the designated landing site by adjusting seeker elevation and azimuth angles to center the designated landing site in the sensor field of view. The seeker angles, closing speed, and range to the designated landing site are used to formulate a velocity field, which is mapped, together with attitude, and rotational velocity directly to the commanded thrust for the four thrusters by the guidance and control system to achieve a safe landing at the designated landing site. The guidance and control system is implemented as a policy optimized using meta-reinforcement learning. The designated landing site is selected via semantic segmentation using a convolutional neural network, trained on a hazard map based on the digital elevation model of the landing area and simulated images produced by the onboard camera. We demonstrate that the system is compatible with multiple divert maneuvers during the powered descent phase and is robust to seeker lag, actuator lag and degradation, and that the landing site selection system consistently avoids potential hazard.},
  archive      = {J_NCA},
  author       = {Scorsoglio, Andrea and Gaudet, Brian and Ghilardi, Luca and Furfaro, Roberto},
  doi          = {10.1007/s00521-025-11274-7},
  journal      = {Neural Computing and Applications},
  month        = {8},
  number       = {22},
  pages        = {17311-17340},
  shortjournal = {Neural Comput. Appl.},
  title        = {Meta-reinforcement learning guidance, navigation, and control for autonomous lunar landing with safe site selection},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Neural network-guided optimization of dynamic systems for aerospace parameter estimation. <em>NCA</em>, <em>37</em>(22), 17293-17309. (<a href='https://doi.org/10.1007/s00521-025-11307-1'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Parameter estimation is essential to dynamic system analysis and simulation. Physical measurements need to be explained by a suitably parameterized system of equations. When the gradients describing these parameters are analytically intractable, current methods do not adequately capture temporal dependencies for time-varying parameters. In our work, we present the novel Dynamic Optimizer for Black-Box Systems (DOBBS) algorithm. Swarm-based metaheuristics are combined with generative and surrogate models to efficiently tackle black-box optimization tasks. The surrogate model approximates the gradient, while the generative network constrains the dimensionality of the problem to stabilize gradient descent when estimating profiles of multiple model parameters simultaneously. The “swarm” of profiles explores the problem space to sample diverse high-quality profiles for the surrogate model. This strategy results in maximum information gain from minimal evaluations of an assumed computationally expensive objective function. DOBBS demonstrates significant improvement over popular gradient-free algorithms when applied to a simplified dynamic parameter estimation task. The algorithm was also applied to a 1375-dimensional real-world estimation problem in post-flight analysis. The results show that it significantly outperforms approaches detailed in previous literature in terms of both convergence and accuracy.},
  archive      = {J_NCA},
  author       = {John, Harris V. and Mukundan, Mijaz and Karthikeyan, D. and Samuel, Jishy},
  doi          = {10.1007/s00521-025-11307-1},
  journal      = {Neural Computing and Applications},
  month        = {8},
  number       = {22},
  pages        = {17293-17309},
  shortjournal = {Neural Comput. Appl.},
  title        = {Neural network-guided optimization of dynamic systems for aerospace parameter estimation},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). AMBER: Advanced SegFormer for multi-band image segmentation—an application to hyperspectral imaging. <em>NCA</em>, <em>37</em>(22), 17273-17291. (<a href='https://doi.org/10.1007/s00521-025-11315-1'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep learning has revolutionized the field of hyperspectral image (HSI) analysis, enabling the extraction of complex spectral and spatial features. While convolutional neural networks (CNNs) have been the backbone of HSI classification, their limitations in capturing global contextual features have led to the exploration of Vision Transformers (ViTs). This paper introduces AMBER, an advanced SegFormer specifically designed for multi-band image segmentation. AMBER enhances the original SegFormer by incorporating three-dimensional convolutions, custom kernel sizes and a Funnelizer layer. This architecture enables to process hyperspectral data directly, without requiring spectral dimensionality reduction during preprocessing. Our experiments, conducted on three benchmark datasets (Salinas, Indian Pines and Pavia University) and on a dataset from the PRISMA $$^*$$ satellite, show that AMBER outperforms traditional CNN-based methods in terms of Overall Accuracy, Kappa coefficient, and Average Accuracy on the first three datasets, and achieves state-of-the-art performance on the PRISMA dataset. These findings highlight AMBER’s robustness, adaptability to both airborne and spaceborne data, and its potential as a powerful solution for remote sensing and other domains requiring advanced analysis of high-dimensional data.},
  archive      = {J_NCA},
  author       = {Dosi, Andrea and Brescia, Massimo and Cavuoti, Stefano and D’Aniello, Mariarca and Delli Veneri, Michele and Donadio, Carlo and Ettari, Adriano and Longo, Giuseppe and Rownok, Alvi and Sannino, Luca and Zampella, Maria},
  doi          = {10.1007/s00521-025-11315-1},
  journal      = {Neural Computing and Applications},
  month        = {8},
  number       = {22},
  pages        = {17273-17291},
  shortjournal = {Neural Comput. Appl.},
  title        = {AMBER: Advanced SegFormer for multi-band image segmentation—an application to hyperspectral imaging},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Improving reinforcement learning performance in spacecraft guidance and control through meta-learning: A comparison on planetary landing. <em>NCA</em>, <em>37</em>(22), 17249-17271. (<a href='https://doi.org/10.1007/s00521-024-10520-8'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper investigates the performance and computational complexity of recurrent neural networks (RNNs) trained via meta-reinforcement learning (meta-RL) as onboard spacecraft guidance and control systems. The paper first presents the theoretical background behind meta-RL with RNNs, highlighting the features that make it suitable for real-world spacecraft guidance and control applications. A thorough comparison of meta-RL with a standard RL approach that uses fully connected neural networks is carried out on a benchmark problem related to spacecraft guidance and control, namely a pin-point planetary landing. The focus is on evaluating the optimality of the control policy, the ability to handle constraints, and the robustness of the approach to different kinds and levels of uncertainties, such as unmodeled dynamics, navigation uncertainties, control errors, and engine failures, to highlight the superiority of meta-RL in both nominal and off-nominal operating conditions.},
  archive      = {J_NCA},
  author       = {Federici, Lorenzo and Furfaro, Roberto},
  doi          = {10.1007/s00521-024-10520-8},
  journal      = {Neural Computing and Applications},
  month        = {8},
  number       = {22},
  pages        = {17249-17271},
  shortjournal = {Neural Comput. Appl.},
  title        = {Improving reinforcement learning performance in spacecraft guidance and control through meta-learning: A comparison on planetary landing},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). YOLOv11n-UAV: Improved YOLOv11n model for detecting small UAVs using infrared images on complex backgrounds. <em>NCA</em>, <em>37</em>(22), 17231-17247. (<a href='https://doi.org/10.1007/s00521-025-11305-3'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The unmanned aerial vehicle (UAV) detection system using infrared imaging demonstrates robust performance in both daytime and night time conditions. Although infrared sensor technology and UAV detection algorithms have been extensively studied, the identification of small UAVs in complex scenes remains a significant challenge. In this paper, we propose the YOLOv11n-UAV model, an enhanced version of YOLOv11n, incorporating two specialized detection heads for ultra-small targets and upgrading two feature extraction modules at the input stage. These modifications aim to enhance detection accuracy while maintaining high inference speed. Experimental results indicate that the proposed YOLOv11n-UAV model surpasses state-of-the-art methods in UAV detection using infrared imagery, particularly in complex outdoor environments. This study contributes to UAV surveillance and countermeasure systems, a field that is gaining increasing global attention.},
  archive      = {J_NCA},
  author       = {Nguyen, Phat T. and Nguyen, Long H.},
  doi          = {10.1007/s00521-025-11305-3},
  journal      = {Neural Computing and Applications},
  month        = {8},
  number       = {22},
  pages        = {17231-17247},
  shortjournal = {Neural Comput. Appl.},
  title        = {YOLOv11n-UAV: Improved YOLOv11n model for detecting small UAVs using infrared images on complex backgrounds},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). GAN-UAV-YOLOv10s: Improved YOLOv10s network for detecting small UAV targets in mountainous conditions based on infrared image data. <em>NCA</em>, <em>37</em>(22), 17217-17229. (<a href='https://doi.org/10.1007/s00521-025-11002-1'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the strong development of the field of computer science, deep learning methods have made strides in unmanned aerial vehicle (UAV) detection. However, targeting small infrared UAVs in mountainous conditions is still a challenge for the scientific community, causing many impacts from mountainous conditions such as thermal interference, interference due to complex terrain, and interference from weather. In this paper, we have proposed the UAV-YOLOv10s model based on the YOLOv10s model to detect small infrared UAV targets in mountainous conditions. Additionally, to improve the ability to detect small UAV targets, we proposed using a GAN network to augment data for small UAV targets. The experimental results show that the proposed model gives better results than the four newest models today: YOLOv8s, YOLOv9s, YOLOv10s and YOLOv11s.},
  archive      = {J_NCA},
  author       = {Phat, Nguyen Tien and Giang, Nguyen Long and Duy, Bui Duc},
  doi          = {10.1007/s00521-025-11002-1},
  journal      = {Neural Computing and Applications},
  month        = {8},
  number       = {22},
  pages        = {17217-17229},
  shortjournal = {Neural Comput. Appl.},
  title        = {GAN-UAV-YOLOv10s: Improved YOLOv10s network for detecting small UAV targets in mountainous conditions based on infrared image data},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Deep sky object detection in astronomical imagery using YOLO models: A comparative assessment. <em>NCA</em>, <em>37</em>(22), 17193-17215. (<a href='https://doi.org/10.1007/s00521-025-11223-4'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This study presents a comparative analysis of three YOLO architectures: YOLOv8, YOLOv9, and YOLOv10, focused on their effectiveness in detecting deep sky objects (DSOs). An extended version of the DeepSpaceYoloDataset is used, originally consisting of 4696 images. To enhance data diversity, data augmentation techniques are applied. These include cropping, rotations, blurring, and noise addition. As a result, the dataset expands to 8421 images, better reflecting real astronomical conditions. Each YOLO variant is trained using default parameters for 100 epochs in a standardized environment. The models’ performance is evaluated based on precision, recall, mean average precision (mAP), and computational efficiency. Results show that YOLOv8 performs better than YOLOv9 and YOLOv10. Particularly, YOLOv8 consistently achieves high precision, recall, and mAP scores across various scenarios. Additionally, YOLOv8 models require less training time, even in their most complex versions, while maintaining low latency. However, in detection tests, YOLOv10 models frequently match or even outperform YOLOv8, often achieving confidence scores above 90%. In contrast, YOLOv9 models exhibit lower performance in both quantitative and qualitative evaluations, along with higher training times and latency, making them the least effective option for DSO detection. These findings serve as a reference for selecting suitable models in astronomical research. They also promote the adoption of these technologies to improve the accuracy and efficiency of DSO detection. The augmented dataset used in this study can be accessed at: https://github.com/Leo-Thomas/Augmented-DeepSpaceYolo .},
  archive      = {J_NCA},
  author       = {Ramos, Leo Thomas and Rivas-Echeverría, Francklin},
  doi          = {10.1007/s00521-025-11223-4},
  journal      = {Neural Computing and Applications},
  month        = {8},
  number       = {22},
  pages        = {17193-17215},
  shortjournal = {Neural Comput. Appl.},
  title        = {Deep sky object detection in astronomical imagery using YOLO models: A comparative assessment},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Geometric deep learning for ionospheric TEC modeling using a temporal graph convolutional network. <em>NCA</em>, <em>37</em>(22), 17179-17192. (<a href='https://doi.org/10.1007/s00521-025-11017-8'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This document proposes a spatiotemporal deep learning model for ionospheric total electron content (TEC) modeling using global navigation satellite systems (GNSSs) observables. Data from dual-frequency GNSS receivers are used to compute the daily GNSS TEC timeseries. Usually, these timeseries are computed independently per GNSS permanent station, and the state-of-the art models proposed in literature exploit temporal characteristics of the timeseries and neglect any spatial dependencies and information from different stations. In our approach, we propose a practical solution for parallel processing of TEC timeseries and additional indicators from various adjacent stations to predict future VTEC values. We face the problem in both spatial and temporal dimensions adopting a graph neural network-based approach from the broader family of geometric deep learning. According to our proposed scheme, the different adjacent GNSS stations are structured in a graph and then, we apply the proposed temporal graph convolutional network called ION_TGNN. Our model predicts future vertical TEC (VTEC) values for all stations in a single run with mae error better than 1.0 TECU. Comparisons with state-of-the art models show the superiority of the proposed method in terms of performance but also in terms of computational cost during training and test phases.},
  archive      = {J_NCA},
  author       = {Kaselimi, Maria and Doulamis, Nikolaos and Doulamis, Anastasios and Delikaraoglou, Demitris},
  doi          = {10.1007/s00521-025-11017-8},
  journal      = {Neural Computing and Applications},
  month        = {8},
  number       = {22},
  pages        = {17179-17192},
  shortjournal = {Neural Comput. Appl.},
  title        = {Geometric deep learning for ionospheric TEC modeling using a temporal graph convolutional network},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Object detection on space-based optical images leveraging machine learning techniques. <em>NCA</em>, <em>37</em>(22), 17153-17177. (<a href='https://doi.org/10.1007/s00521-025-11069-w'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The expansion of space exploration has led to a soar in the space debris population, increasing the risks of collisions. Addressing this challenge requires advanced space surveillance technologies. Traditional computer vision approaches are unsuitable for real-time applications due to their significant computational demands. Recent progress has been made in ground-based debris detection, thanks to the integration of CNNs. However, to overcome limitations imposed by the atmosphere and other external disturbances, a space-based framework is appealing for spotting fainter objects. This work presents a novel real-time object detection tool designed for space-based applications using machine learning techniques. The absence of labeled datasets for RSOs detection is one of the primary obstacles to AI training, particularly for space-based observations. To tackle this issue, a synthetic ‘.fits’ image generator named SOIG has been developed using photon mapping techniques. The generator produces two types of images. In one instance, it takes into account the sensor’s pointing, which follows the satellite’s attitude. In the other scenario, the sensor’s pointing is considered fixed. Following its training on synthetic images, the subsequent testing phase is conducted through semisynthetic images, which incorporate noise from an actual space-based image. Results demonstrate exceptional performance (mAP50-95 above 90%) for both fixed pointing and rotated and expanded survey pointing images. In the latter case, the tool utilizes sensor attitude information to enhance debris visibility. On the whole, this research wants to contribute to mitigating space collisions and increasing the understanding of machine learning’s potential in space debris detection.},
  archive      = {J_NCA},
  author       = {Rizzuto, Sebastian Samuele and Cipollone, Riccardo and De Vittori, Andrea and Di Lizia, Pierluigi and Massari, Mauro},
  doi          = {10.1007/s00521-025-11069-w},
  journal      = {Neural Computing and Applications},
  month        = {8},
  number       = {22},
  pages        = {17153-17177},
  shortjournal = {Neural Comput. Appl.},
  title        = {Object detection on space-based optical images leveraging machine learning techniques},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). AI for space: Theories, models and applications. <em>NCA</em>, <em>37</em>(22), 17149-17151. (<a href='https://doi.org/10.1007/s00521-025-11466-1'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_NCA},
  author       = {Ieracitano, Cosimo and Mammone, Nadia and Lanza, Piergiorgio and Gao, Fei and Le Saux, Bertrand and Furfaro, Roberto and Morabito, Francesco Carlo},
  doi          = {10.1007/s00521-025-11466-1},
  journal      = {Neural Computing and Applications},
  month        = {8},
  number       = {22},
  pages        = {17149-17151},
  shortjournal = {Neural Comput. Appl.},
  title        = {AI for space: Theories, models and applications},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multi-model anomaly detection for industrial inspection with dynamic loss weighting and soft-hard features loss. <em>NCA</em>, <em>37</em>(21), 17031-17054. (<a href='https://doi.org/10.1007/s00521-025-11367-3'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Anomaly detection in manufacturing remains a significant challenge, particularly due to the unique characteristics of industrial data and the scarcity of abnormal samples. This study investigates the effectiveness of unsupervised learning approaches, leveraging multi-model frameworks that combine pre-trained models on large-scale datasets with models specifically trained to capture industrial data features. This study proposes two key methodologies to maximize multi-model efficiency. Dynamic loss weighting optimizes the contribution of each model during training, enabling networks with diverse expertise to synergize effectively. Soft-hard feature loss, in particular, focuses on precisely capturing subtle anomaly regions that traditional methods might miss. By emphasizing features with high-error values while appropriately utilizing those with lower error values, the proposed approach enables more detailed anomaly detection compared to existing methods, allowing for the detection of even minor defects through refined anomaly region analysis. Quantitative results on the MVTec and VisA datasets demonstrate that the proposed method achieves remarkable performance improvements, with up to +0.6% (AU-PRO) on the MVTec AD dataset and up to +0.4% (AU-ROC) on the VisA dataset compared to baseline methods. In addition to its superior quantitative performance, the proposed method enables more precise anomaly detection than conventional approaches. Furthermore, the proposed method eliminates the need for experimental tuning, enabling its application to diverse multi-model approaches and ensuring adaptability across various datasets.},
  archive      = {J_NCA},
  author       = {Hendria, Willy Fitra and Kim, Hanbi and Seo, Daeho},
  doi          = {10.1007/s00521-025-11367-3},
  journal      = {Neural Computing and Applications},
  month        = {7},
  number       = {21},
  pages        = {17031-17054},
  shortjournal = {Neural Comput. Appl.},
  title        = {Multi-model anomaly detection for industrial inspection with dynamic loss weighting and soft-hard features loss},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A deep learning-based CNN-GRU prediction model for early fault diagnosis in rolling ball bearings. <em>NCA</em>, <em>37</em>(21), 17015-17030. (<a href='https://doi.org/10.1007/s00521-025-11293-4'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Rolling ball bearings are the essential components of rotating machines that can be affected by various faults, increasing the maintenance costs and downtime of the machine. The paper introduces a novel CNN-GRU prediction model, aimed to detect and diagnose incipient faults in the ball bearings components. The model is learned on nonlinear vibration signals that were recorded from a laboratory experimental setup performed on Machine Fault Simulator (MFS), focusing on defects in the inner race (IR), outer race (OR) and composite faults (CF). By leveraging deep features extracted through neural networks and employing transfer learning with Gated Recurrent Unit (GRU) network, the study enhances classification accuracy and effectively resolves the problem of vanishing gradient problem. The GRU has the advantage of efficiently extracting temporal features which have a long-term dependency in the time-series data, while the CNN captures both local and global spatial features. This combined approach not only enhances fault detection accuracy but also reduces the computational burden on the classifier. Overall, the result of this paper introduces a promising approach for both efficient and accurate fault detection and severity estimation in bearings of induction motors, potentially reducing the need for extensive manpower and sensor usage.},
  archive      = {J_NCA},
  author       = {Kumar, Rajeev and Anand, R. S.},
  doi          = {10.1007/s00521-025-11293-4},
  journal      = {Neural Computing and Applications},
  month        = {7},
  number       = {21},
  pages        = {17015-17030},
  shortjournal = {Neural Comput. Appl.},
  title        = {A deep learning-based CNN-GRU prediction model for early fault diagnosis in rolling ball bearings},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Accelerated black hole optimization algorithm with enhanced FOPID controller for omni-wheel drive mobile robot system. <em>NCA</em>, <em>37</em>(21), 16983-17014. (<a href='https://doi.org/10.1007/s00521-025-11310-6'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Controlling Omni-Wheel Drive Mobile Robot Systems (OWDMRS) presents unique challenges due to their ability to move in multiple directions such as rotation, sideways, and forward/backward motion while minimizing energy consumption and voltage fluctuations. This study introduces a novel framework that enhances motion control and trajectory tracking by integrating an advanced fractional-order proportional–integral–derivative (FOPID) controller with an adaptive neuro-fuzzy inference system (ANFIS). To optimize controller performance, six different optimization algorithms are compared are Accelerated Convergence Black Hole Optimization (ACBHO), Black Hole Optimization (BHO), Aquila Optimizer (AO), Hybrid Firefly Particle Swarm Optimization (HFPSO), Enhanced JAYA (EJAYA), and Sunflower Optimizer (SFO). Among these, the proposed ACBHO algorithm significantly improved trajectory tracking accuracy and control efficiency. The framework effectively manages voltage regulation and enhances motion precision by fine-tuning FOPID and ANFIS parameters. These results demonstrate the potential of ACBHO-based optimization as a robust solution for improving control system performance in advanced mobile robotics applications.},
  archive      = {J_NCA},
  author       = {Basil, Noorulden and Marhoon, Hamzah M. and Sahib, Dheyaaldeen Faez and Mohammed, Abdullah Fadhil and Ridha, Hussein Mohammed and Ma’arif, Alfian},
  doi          = {10.1007/s00521-025-11310-6},
  journal      = {Neural Computing and Applications},
  month        = {7},
  number       = {21},
  pages        = {16983-17014},
  shortjournal = {Neural Comput. Appl.},
  title        = {Accelerated black hole optimization algorithm with enhanced FOPID controller for omni-wheel drive mobile robot system},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). MSAM: A multi-scale attention mechanism for improving industrial defect segmentation. <em>NCA</em>, <em>37</em>(21), 16969-16982. (<a href='https://doi.org/10.1007/s00521-025-11362-8'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a novel multi-scale attention mechanism (MSAM) that specifically addresses the challenges of environmental noise in industrial defect segmentation. The MSAM is composed of two submodules, namely the multi-scale feature filtering (MSFF) submodule and the scale selection (SS) submodule. The MSFF generates spatial attention weights that provide precise spatial location guidance for multi-scale features. This enhances the regions of interest while suppressing irrelevant regions. Additionally, the SS adaptively models the interdependencies between the channels of multi-scale features and assigns different weights to different channels. Experiments on the three industrial datasets demonstrate that the application of the MSAM module can effectively tackle the influence of complex noise in industrial segmentation and significantly improve the accuracy of industrial defect segmentation.},
  archive      = {J_NCA},
  author       = {Han, Menghao and Ji, Zhenyan and Yang, Yanyan and Feng, Qibo and Yin, Shen},
  doi          = {10.1007/s00521-025-11362-8},
  journal      = {Neural Computing and Applications},
  month        = {7},
  number       = {21},
  pages        = {16969-16982},
  shortjournal = {Neural Comput. Appl.},
  title        = {MSAM: A multi-scale attention mechanism for improving industrial defect segmentation},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Deep learning IoT malware analysis: Investigation and understanding. <em>NCA</em>, <em>37</em>(21), 16941-16968. (<a href='https://doi.org/10.1007/s00521-025-11365-5'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This empirical study explores the ability of lightweight convolutional neural networks (CNNs) for malware analysis in Internet of Things (IoT) environments, emphasizing the impact of input dimensionality and size on feature extraction quality. By converting malware binaries into 1D and 2D grayscale images, we systematically evaluate how varying input resolutions (e.g., 64 × 64 for 2D, 4096 for 1D) influence model performance, resource efficiency, and interpretability. Experiments on the Microsoft Malware Dataset (MMD) and IoT Malware Dataset (IMD) reveal that 1D CNN models outperform their 2D counterparts in accuracy, parameter efficiency, and training time, with gated recurrent unit (GRU) classifiers demonstrating superior performance across both dimensions. Post hoc visual interpretability techniques, including saliency maps and activation heatmaps, uncover critical feature patterns driving classification decisions, addressing the "black box" limitations of deep learning in cybersecurity. Our findings challenge the assumption that 2D models inherently excel in image-based tasks and provide actionable insights for designing resource-efficient IoT malware detection systems. By bridging the gap between model complexity and deployment feasibility, this work advances the application of explainable AI to protect IoT ecosystems against evolving cyber threats.},
  archive      = {J_NCA},
  author       = {Abdullah, Muhammed Amin and Yu, Yongbin and Cai, Jingye and Addo, Daniel and Bankas, Edem K. and Gu, Yeong Hyeon and Alqahtani, Ali and Al-antari, Mugahed A.},
  doi          = {10.1007/s00521-025-11365-5},
  journal      = {Neural Computing and Applications},
  month        = {7},
  number       = {21},
  pages        = {16941-16968},
  shortjournal = {Neural Comput. Appl.},
  title        = {Deep learning IoT malware analysis: Investigation and understanding},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Hash-binding: DNA-protein binding prediction using hashing-based embedding. <em>NCA</em>, <em>37</em>(21), 16905-16940. (<a href='https://doi.org/10.1007/s00521-025-11349-5'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A crucial challenge in molecular biology is the prediction of DNA-protein binding interactions, which has applications in the study of gene regulation and genome functionality. In this paper, we present a novel approach involving hashing-based embedding to predict DNA-protein binding interactions with increased accuracy and faster computation speed. Before applying machine learning models for predicting DNA-protein binding, we need to obtain an appropriate numeric representation of the biological sequences. There exist several embedding options for protein sequences but the computation cost allocated with them is high. Therefore, in our approach, we propose hashing-based embedding, which is alignment-free, has faster generation speed, and is computationally inexpensive as compared to baseline embedding methods. Our approach captures the sequence-specific binding preferences of proteins, enabling researchers to learn more about the underlying binding mechanisms. Experiments on diverse DNA-protein interaction datasets demonstrate that the proposed approach not only improves the predictive performance in terms of accuracy but is also much more efficient as compared to the baseline methods. The proposed method holds significant potential in deciphering intricate DNA-protein interactions, ultimately advancing our comprehension of gene regulation mechanisms.},
  archive      = {J_NCA},
  author       = {Ali, Sarwan and Ali, Tamkanat E. and Patterson, Murray},
  doi          = {10.1007/s00521-025-11349-5},
  journal      = {Neural Computing and Applications},
  month        = {7},
  number       = {21},
  pages        = {16905-16940},
  shortjournal = {Neural Comput. Appl.},
  title        = {Hash-binding: DNA-protein binding prediction using hashing-based embedding},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Lightweight and high-precision spectral convolutional neural network model for custom 94-class ASCII character recognition. <em>NCA</em>, <em>37</em>(21), 16883-16904. (<a href='https://doi.org/10.1007/s00521-025-11376-2'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper introduces a lightweight and precise convolutional neural network (CNN) model designed for mobile edge computing devices. This model operates primarily in the spectral domain, making it highly efficient for artificial intelligence tasks. Unlike conventional models, it performs a single domain transformation on the source feature map, while the remainder of the network processes data exclusively in the spectral domain. To further enhance performance, specialized spectral size and depth optimization techniques have been applied, significantly reducing computational complexity. The proposed model was rigorously tested on a newly developed 94-class ASCII character dataset, which includes a diverse range of lowercase and uppercase letters, numbers, symbols, and special characters across various fonts. Detailed comparisons were made between the proposed spectral CNN models and equivalent models operating in the conventional spatial domain. The results demonstrate that the accuracy of the spectral and spatial models is comparable. However, the spectral model outperforms in other critical metrics, such as precision, where it shows an improvement of over 95%. Moreover, the computational workload in the spectral domain is reduced by several orders of magnitude. For instance, similar models, such as VGG7, exhibit a 2.1x faster training time and a 4.4x faster testing time when implemented in the spectral domain. Compared to existing methods in the literature, particularly for the MNIST dataset, the proposed model not only achieves the highest accuracy at 97.5% but also demonstrates up to a 20x reduction in computational workload.},
  archive      = {J_NCA},
  author       = {Alshareef, Ibrahim Y. and Ab Rahman, Ab Al-Hadi and Khan, Nuzhat and Rizvi, Shahriyar Masud and Manzak, Ali and Rusli, Mohd Shahrizal and Mohammed, Mohammed Sultan and Hassan, Mohamed Khalafalla},
  doi          = {10.1007/s00521-025-11376-2},
  journal      = {Neural Computing and Applications},
  month        = {7},
  number       = {21},
  pages        = {16883-16904},
  shortjournal = {Neural Comput. Appl.},
  title        = {Lightweight and high-precision spectral convolutional neural network model for custom 94-class ASCII character recognition},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Outlier detection variational autoencoder. <em>NCA</em>, <em>37</em>(21), 16871-16882. (<a href='https://doi.org/10.1007/s00521-025-11357-5'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Anomaly detection in graph-based data is an emerging field in machine learning with many relevant applications. Although some algorithms have been developed, current models lack consistency on real-world data and often have problems with overfitting. The paper presents a new model to address these challenges. The model is a combination of graph neural network layers in a variational autoencoder framework. The graph convolution layers learn complex relationships between the node attributes, while also taking the graph structure into account. By using the variational autoencoder framework, the model is less likely to have overfitting problems. The model outperformed existing models on four of five real-world datasets with organic outliers, two out of three real-world datasets with synthetic outliers, and was the second best on the other two datasets. The model demonstrates the potential to combine the variational autoencoder architecture with graph convolution layers in graph deep learning tasks on outlier detection.},
  archive      = {J_NCA},
  author       = {Powers, Henry and Edoh, Kossi},
  doi          = {10.1007/s00521-025-11357-5},
  journal      = {Neural Computing and Applications},
  month        = {7},
  number       = {21},
  pages        = {16871-16882},
  shortjournal = {Neural Comput. Appl.},
  title        = {Outlier detection variational autoencoder},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). An efficient epileptic seizure detection framework based on optimized deep residual network. <em>NCA</em>, <em>37</em>(21), 16849-16870. (<a href='https://doi.org/10.1007/s00521-025-11354-8'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Epilepsy is a chronic neurological disorder triggered by an aberrant neural activity of the brain. The precise identification of epileptic seizures is important and challenging for drug-resistant epilepsy. Therefore, the primary objective of this research is to propose an innovative and highly reliable hybrid framework that utilizes the concepts of deep residual network (ResNet), swarm optimization, and variational mode extraction (VME) for the effective detection of seizures. However, in ResNet, an incorrect hyperparameter configuration might result in an overfitted or underfitted training network. Hence, in this work, an optimized deep ResNet model is proposed to find the hyperparameters of the ResNet model optimally without the involvement of human experts by using a swarm intelligence-based Walrus optimizer. Also, the ocular artifacts caused by eye blinks may emerge as seizure peaks in the EEG recordings. So, the removal of eyeblinks plays a significant attention in improving the accuracy of the seizure detection system. Accordingly, the proposed work provides a solution for the precise detection of epileptic seizures by applying regression to the VME technique. The key benefit of this method is that it focuses only on the removal of artifact-contaminated intervals without affecting non-artifact regions. The performance of the proposed framework is evaluated using the publicly available CHB-MIT dataset, and the model achieves 97.64% accuracy, 97.9% precision, 98.01% recall, 97.78% specificity, and 97.74% F1-score. These experimental outcomes reveal the robustness of the proposed model by effectively detecting the seizures.},
  archive      = {J_NCA},
  author       = {Silpa, Bommala and Hota, Malaya Kumar},
  doi          = {10.1007/s00521-025-11354-8},
  journal      = {Neural Computing and Applications},
  month        = {7},
  number       = {21},
  pages        = {16849-16870},
  shortjournal = {Neural Comput. Appl.},
  title        = {An efficient epileptic seizure detection framework based on optimized deep residual network},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Alzheimer's stage progression modeling using graph neural network and MRI biomarkers. <em>NCA</em>, <em>37</em>(21), 16825-16847. (<a href='https://doi.org/10.1007/s00521-025-11353-9'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Stage progression and early detection are the key issues in predicting Alzheimer's disease using MRI. As of now, Alzheimer's existing work generates progression from one stage to multiple stages without the probability of progression. To overcome this problem, the graph network is very feasible for one-to-one stage progression. As for the probability of stage progression concern, this is very important to create feature vectors of biomarkers associated with the stages. The proposed work formulates the graph consisting of stages on nodes and biomarkers on edge. Each stage biomarkers are represented as a feature vector. The feature vector passes through all the stages to the destination stage and gets updated as per biomarkers. Because of many intermediate stages, more than one path is generated while traversing from one stage to another. It is also important to select the optimal path from many paths. The optimal path is selected with the inclusion of max-cut to create optimal path feature vectors. The optimal path feature vectors are multiplied by a weight matrix to produce aggregated results known as logits. After that, the logits are learned, and features are extracted during training by inputting them into a graph neural network (GNN). The proposed model adds a softmax function to GNN, which converts the trained logits into probabilities. These probabilities are scores of stage progression from one stage to another. Finally, quantum approximate optimization is used to tune the parameters and optimize the model solutions. This paper focuses on MRI for Alzheimer's early detection, one-to-one stage progression, and the probability of progression. The model achieved accuracy of 98.10% and F1 score of 97.01% for detecting AD stage. It also reported micro-precision of 99.01% and micro-recall of 99.86%. The model results illustrate the efficacy of the proposed approach.},
  archive      = {J_NCA},
  author       = {Shankar, Venkatesh Gauri and Sisodia, Dilip Singh and Chandrakar, Preeti},
  doi          = {10.1007/s00521-025-11353-9},
  journal      = {Neural Computing and Applications},
  month        = {7},
  number       = {21},
  pages        = {16825-16847},
  shortjournal = {Neural Comput. Appl.},
  title        = {Alzheimer's stage progression modeling using graph neural network and MRI biomarkers},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Hybrid wavelet-SVR, machine learning, and deep learning models for predicting clean energy markets amidst global events. <em>NCA</em>, <em>37</em>(21), 16781-16823. (<a href='https://doi.org/10.1007/s00521-025-11345-9'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This study investigates hybrid machine learning models combined with wavelet transforms for predicting clean energy market dynamics from 01.04.2014 to 02.05.2024. Models such as support vector regression (SVR), artificial neural networks (ANNs), eXtreme Gradient Boosting (XGBoost), gradient boosting machine (GBM), long short-term memory (LSTM), and convolutional neural network (CNN) are compared to forecast the Nasdaq Clean Edge Green Energy Index (NasdaqClean). Discrete wavelet transform (DWT) and continuous wavelet transform (CWT) are used for feature extraction and visualizations, capturing both short-term fluctuations and long-term trends. Shapley additive explanations (SHAP) and permutation feature importance (PFI) assess feature contributions. Analysis across sub-periods, including the Paris Agreement, COVID-19, and the Russia–Ukraine conflict, reveals that different models perform optimally in different periods. Specifically, Wavelet-SVR emerges as the most accurate model in the entire dataset, before the Paris Agreement and Paris Agreement periods, demonstrating strong predictive power by reducing noise and enhancing feature extraction. LSTM performs best during COVID-19, capturing long-term dependencies and volatile market dynamics. Meanwhile, CNN yields the most accurate predictions during the Russia–Ukraine conflict, effectively identifying spatial patterns in the dataset.},
  archive      = {J_NCA},
  author       = {Kayral, İhsan Erdem and Aktaş Bozkurt, Melike and Bejaoui, Azza and Jeribi, Ahmed},
  doi          = {10.1007/s00521-025-11345-9},
  journal      = {Neural Computing and Applications},
  month        = {7},
  number       = {21},
  pages        = {16781-16823},
  shortjournal = {Neural Comput. Appl.},
  title        = {Hybrid wavelet-SVR, machine learning, and deep learning models for predicting clean energy markets amidst global events},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Early detection of fetal health status based on cardiotocography using artificial intelligence. <em>NCA</em>, <em>37</em>(21), 16753-16779. (<a href='https://doi.org/10.1007/s00521-025-11343-x'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Fetal health is a vital aspect of pregnancy, influencing both the mother and her fetus. Frequent observation and prompt response are essential for achieving optimal outcomes. It is important to assess fetal health within the womb, ensuring that any potential issues are addressed rapidly. Prioritizing fetal monitoring is essential for safe and healthy pregnancy, one of such methods is Cardiotocography (CTG). CTG is employed to monitor the uterine contraction patterns and fetal heart rate during pregnancy and labor. The aim of this paper is to use artificial intelligence to enhance the accuracy of fetal health prediction and enhance clinical decision-making. Seven machine learning (ML) algorithms and five deep learning (DL) algorithms are applied. In addition, H2O.ai and Lazy predict platforms were applied for prediction. Ensemble learning was employed to combine the most effective models to construct the Blender model, emulate the traditional ML, DL models, and ML with DL in meta classifiers. The results for ML models showed that meta-model with stacking classifier had the highest accuracy of 98.9%. The results for DL models showed that ANN had the highest accuracy of 97.7%. The analysis of each model’s performance demonstrated that the proposed stacking classifier achieved 98.9% accuracy, 99% precision, 98.6% recall, 99.3% F1-score, and 99.8% area under the ROC curve. This implies that stacking classifier model demonstrates a strong capability in predicting fetal health and it can be integrated with the CTG device for real-time monitoring and medical follow-up by healthcare providers.},
  archive      = {J_NCA},
  author       = {Ahmed, Sara S. and Mahmoud, Nourelhoda M.},
  doi          = {10.1007/s00521-025-11343-x},
  journal      = {Neural Computing and Applications},
  month        = {7},
  number       = {21},
  pages        = {16753-16779},
  shortjournal = {Neural Comput. Appl.},
  title        = {Early detection of fetal health status based on cardiotocography using artificial intelligence},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Application of ensemble learning models in computer-aided diagnosis of skin diseases. <em>NCA</em>, <em>37</em>(21), 16735-16751. (<a href='https://doi.org/10.1007/s00521-025-11336-w'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Computer-aided diagnosis (CADx) systems often involve error-prone and/or manual processes. Recent studies show that various machine learning models have the potential to improve the accuracy of CADx systems. However, existing models suffer from low prediction accuracy. In this work, we present research findings to improve the effectiveness of CADx systems for detecting skin diseases by adopting optimized ensemble machine learning models. The investigation encompasses the exploration of three popular classification methods: linear discriminant analysis (LDA), support vector machine (SVM), and convolutional neural network (CNN); two customized CNN models: LeNet-5 and ResNet; and an ensemble model of CNN with SVM. The ensemble CNN-SVM model is optimized using techniques such as feature aggregation and weight adjustments. Skin lesion images from Kaggle’s Human Against Machine 10000 (HAM10000) are used to train and test all classification models. Through rigorous experiments, the results highlight the compelling efficacy of the ensemble CNN-SVM model, unveiling heightened accuracy of up to 92% (from ResNet accuracy of 88%, CNN accuracy of 85%, SVM accuracy of 83%, LeNet-5 accuracy of 77%, and LDA accuracy of 75%). The models are tested on another dataset from Kaggle’s Melanoma Skin Cancer Dataset of 10000 Images; new results follow a similar trend to those using the HAM10000 dataset. The outcome of this work has profound implications for artificial intelligence (AI) accelerated engineering applications in advancing the effectiveness of skin disease treatment through diagnosis systems.},
  archive      = {J_NCA},
  author       = {Asaduzzaman, Abu and Thompson, Christian C. and Sibai, Fadi N. and Uddin, Md J.},
  doi          = {10.1007/s00521-025-11336-w},
  journal      = {Neural Computing and Applications},
  month        = {7},
  number       = {21},
  pages        = {16735-16751},
  shortjournal = {Neural Comput. Appl.},
  title        = {Application of ensemble learning models in computer-aided diagnosis of skin diseases},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). SLF-ADM: Securing linux frontiers: Advanced persistent threat (APT) detection using machine learning. <em>NCA</em>, <em>37</em>(21), 16715-16734. (<a href='https://doi.org/10.1007/s00521-025-11338-8'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cyber warfare has reached its peak and the most dangerous threat to any country/state/organization in cyber attack space is Advanced Persistent Threat (APT) due to its nature, intention and impact. These attack campaigns are sophisticated and complex enough to get easily detected, specially in the Linux environment on which most of the back-end technologies and super computers are based. There is no proper solution to detect and predict these sophisticated attack campaign. This article presents a novel machine learning approach that utilizes the FastText embedding with support vector machine (SVM) algorithm for rapid and systematic APT attack detection and prediction. The dataset used in the study is developed by simulating several latest APTs and executing payloads in the Linux environment to effectively analyse, detect and predict the campaign. As per the APT life cycle, there are different stages and each stage has different characteristics and paths to observe, which are taken into consideration in the selected dataset. Utilizing the proposed methodology, there are four ML-model which are trained and tested against the accuracy. The maximum predicted accuracy of 96% has been achieved for SVM. The results are compared with other machine learning models on same dataset as well. The experiment shows that SVM has performed significantly better in comparison to the other proposed models.},
  archive      = {J_NCA},
  author       = {Karim, Syed Sohaib and Afzal, Mehreen and Iqbal, Waseem and Abri, Dawood Al and Abbas, Yawar},
  doi          = {10.1007/s00521-025-11338-8},
  journal      = {Neural Computing and Applications},
  month        = {7},
  number       = {21},
  pages        = {16715-16734},
  shortjournal = {Neural Comput. Appl.},
  title        = {SLF-ADM: Securing linux frontiers: Advanced persistent threat (APT) detection using machine learning},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Enhancing gaze estimation accuracy in wearable eye-tracking devices using neural networks. <em>NCA</em>, <em>37</em>(21), 16703-16714. (<a href='https://doi.org/10.1007/s00521-025-11334-y'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Eye-tracking devices are convenient for interpreting human behaviors and intentions, enabling contactless human–computer interaction, such as in medical image interpretation using eye-gaze tracking Recent advances in wearable eye-tracking devices have further allowed wearers to move freely and use them in regular activities. However, gaze estimation from wearable devices tends to be less precise than that from standard stationary eye-tracking devices. This is due to device design constraints and a lack of interpretation of the relationship between the scene and the wearer. In this work, we propose to enhance the accuracy of gaze estimation in wearable eye-tracking devices through a framework that incorporates two neural networks, CorNN and CalNN. The CorNN corrects the bias induced by the distance between the observer and the gaze locations, primarily resulting from the parallax and lens distortion effects. Meanwhile, the CalNN focuses on improving calibration specific to each wearer. To collect precise training data for these networks, we have implemented an automated robotic data collection pipeline. The proposed framework was demonstrated on the Pupil Labs Invisible eye-tracking device and tested on 11 wearers, showing improved average gaze estimation accuracy for all wearers.},
  archive      = {J_NCA},
  author       = {Charton, Jerome and Zhang, Tianpeng and Li, Na and Li, Quanzheng},
  doi          = {10.1007/s00521-025-11334-y},
  journal      = {Neural Computing and Applications},
  month        = {7},
  number       = {21},
  pages        = {16703-16714},
  shortjournal = {Neural Comput. Appl.},
  title        = {Enhancing gaze estimation accuracy in wearable eye-tracking devices using neural networks},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Deep learning-based beat-to-beat arterial blood pressure estimation using distant radar signals. <em>NCA</em>, <em>37</em>(21), 16677-16702. (<a href='https://doi.org/10.1007/s00521-025-11327-x'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Maintaining constant vigilance over arterial blood pressure (ABP) is crucial for diagnosing hypertension and other critical cardiovascular diseases. While traditional cuff-based approaches are non-invasive, they have limitations in providing continuous blood pressure monitoring. In contrast, complex ABP monitoring systems, while accurate, are primarily suitable for clinical settings due to their intrusive nature. This study introduces a groundbreaking method for generating arterial blood pressure (ABP) waveforms using remote radar signals and deep learning (DL) techniques. This approach eliminates the need for invasive procedures, wearable biosensors, and costly equipment typically associated with ABP recording. We introduce MultiResLinkNet, a segmentation model based on a one-dimensional convolutional neural network (1D CNN), specifically designed to synthesize arterial blood pressure (ABP) directly from raw radar waveforms. We trained and evaluated the end-to-end DL framework using a publicly available benchmark radar dataset containing raw radar data and corresponding physiological signals from 30 subjects across various scenarios, including Resting, Valsalva, Apnea, Tilt-up, and Tilt-down. The proposed MultiResLinkNet excelled in ABP segmentation, outperforming state-of-the-art networks in combined and individual scenarios, and produced the best average temporal and spectral correlations as well as the lowest temporal and spectral errors in nearly all scenarios’ data. Furthermore, qualitative evaluation demonstrated a strong resemblance between the synthesized and ground truth ABP waveforms. Our novel approach enables remote monitoring of critical patients continuously, especially those undergoing surgery, by predicting ABP waveforms from non-contact radar signals. This breakthrough offers significant advantages, facilitating continuous ABP monitoring without the need for invasive procedures or cumbersome wearable sensors.},
  archive      = {J_NCA},
  author       = {Chowdhury, Farhana Ahmed and Hosain, Md Kamal and Hossain, Md Shafayet and Chowdhury, Muhammad E. H. and Mahmud, Sakib and Kabir, Muhammad Ashad and Alqahtani, Abdulrahman and Hasan, Anwarul},
  doi          = {10.1007/s00521-025-11327-x},
  journal      = {Neural Computing and Applications},
  month        = {7},
  number       = {21},
  pages        = {16677-16702},
  shortjournal = {Neural Comput. Appl.},
  title        = {Deep learning-based beat-to-beat arterial blood pressure estimation using distant radar signals},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). An enhanced weighted mean of vectors optimizer: Addressing combined heat and power economic dispatch with system losses and valve point loading effect. <em>NCA</em>, <em>37</em>(21), 16643-16675. (<a href='https://doi.org/10.1007/s00521-025-11245-y'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The optimization of combined heat and power economic dispatch (CHPED) presents a critical, complex, nonlinear, and non-convex challenge vital for achieving optimal economic performance in modern power systems. The difficulty of CHPED increases further when factors such as valve-point loading effect (VPLE), prohibited zones, and system losses are considered. This study presents an enhanced weighted mean of vectors optimizer (EINFO) to address the CHPED in small-scale systems, including 4-unit, 7-unit, and 24-unit setups, and accommodates VPLE for large-scale systems. The EINFO improves the global search capability of the conventional INFO method by incorporating three strategies: fitness distance balance (FDB), a chaotic mechanism (CM), and quasi-oppositional based learning (QOBL). Evaluations using the CEC 2022 benchmark functions involve statistical comparisons, convergence analyses, and boxplot assessments against several established methods, including SCA, BDO, AVOA, GTO, MGO, ARO, BWO, FFA, and traditional INFO. The results show that EINFO achieves cost reductions ranging from 0.0000324 to 2.0643% for 4-unit systems, 0.00234–2.2346% for 7-unit systems, and 0.01568–17.5413% for 24-unit systems compared to the best outcomes from other methods.},
  archive      = {J_NCA},
  author       = {Ebeed, Mohamed and Elnaka, Mosaed and Khan, Noor Habib and Jamal, Raheela and Abdel-Rahman, Adel Bedair and Jurado, Francisco and Kamel, Salah and Rihan, Mahmoud},
  doi          = {10.1007/s00521-025-11245-y},
  journal      = {Neural Computing and Applications},
  month        = {7},
  number       = {21},
  pages        = {16643-16675},
  shortjournal = {Neural Comput. Appl.},
  title        = {An enhanced weighted mean of vectors optimizer: Addressing combined heat and power economic dispatch with system losses and valve point loading effect},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Comprehensive dual framework for lesion segmentation and classification in retinal OCT scans. <em>NCA</em>, <em>37</em>(21), 16621-16642. (<a href='https://doi.org/10.1007/s00521-025-11375-3'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Disorders affecting the retina pose a considerable risk to human vision, with an array of factors including aging, diabetes, hypertension, obesity, ocular trauma, and tobacco use exacerbating this issue in contemporary times. OCT is a rapidly developing imaging modality that is capable of identifying early signs of vascular, ocular, and central nervous system abnormalities. This means that to provide a critical chance for early intervention, which could enhance the treatments and survival rates. OCT can diagnose retinal diseases through image classification, but quantifying the lesion area requires image segmentation. To overcome this obstacle, we have developed an innovative deep learning framework that can perform both tasks simultaneously. The suggested framework employs a parallel mask-guided convolutional neural network (PM-CNN) for the classification of OCT B-scans and a grade activation map (GAM) output from the PM-CNN to help a VNet network (GAM VNet) to segment retinal lesions. The guiding mask for the PM-CNN is obtained from the auxiliary segmentation job. The effectiveness of the dual framework was evaluated using a combined dataset that encompassed four publicly accessible datasets along with an additional real-time dataset. This compilation included 11 categories of retinal diseases. The four publicly available datasets provided a robust foundation for the validation of the dual framework, while the real-time dataset enabled the framework’s performance to be assessed on a broader range of retinal disease categories. The segmentation Dice coefficient was 78.33%, while the classification accuracy was 99.10%. The model’s ability to effectively segment retinal fluids and identify retinal lesions on a different dataset was an excellent demonstration of its generalizability.},
  archive      = {J_NCA},
  author       = {Mani, Pavithra and Ramachandran, Neelaveni and Paul, Sweety Jose and Ramesh, Prasanna Venkatesh},
  doi          = {10.1007/s00521-025-11375-3},
  journal      = {Neural Computing and Applications},
  month        = {7},
  number       = {21},
  pages        = {16621-16642},
  shortjournal = {Neural Comput. Appl.},
  title        = {Comprehensive dual framework for lesion segmentation and classification in retinal OCT scans},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). MRFFD: Multimodal recommender based on feature fusion and decoupling. <em>NCA</em>, <em>37</em>(21), 16599-16620. (<a href='https://doi.org/10.1007/s00521-025-11372-6'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multimodal recommender systems utilize users’ interaction history and associated multimodal information to recommend items effectively. Although there have been significant advancements in current research, some approaches do not fully capture the complexity of user behavior when interacting with multimodal information. This complexity arises from two factors: the comprehensive influence of a large amount of image and text information related to the items, and the preference differences users exhibit for various item factors across different modal scenarios. Therefore, we propose a novel method named MRFFD (multimodal recommender based on feature fusion and decoupling). This method employs two separate attention networks to extract and integrate the key visual and textual features of items. We then decouple the multimodal features to identify different factors of the items. Finally, in both local scenarios (single-modal) and global scenarios (multimodal), we compute user preference scores for each factor to achieve precise recommendations. Experimental results on two open datasets have demonstrated the effectiveness and superiority of our proposed model. Specifically, MRFFD demonstrated substantial improvements in Precision@20, Recall@20, and NDCG@20 metrics, achieving increases of 4.29%, 2.53%, and 3.93% on the Toy dataset, and 5.78%, 13.24%, and 1.55% on the Yelp dataset, respectively, compared to the baseline models.},
  archive      = {J_NCA},
  author       = {Ping, Yuchao and Wang, Shuqin and Yang, Ziyi and Dong, Yongquan and Jia, Rui},
  doi          = {10.1007/s00521-025-11372-6},
  journal      = {Neural Computing and Applications},
  month        = {7},
  number       = {21},
  pages        = {16599-16620},
  shortjournal = {Neural Comput. Appl.},
  title        = {MRFFD: Multimodal recommender based on feature fusion and decoupling},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A genetic-based hyper-heuristic optimisation method to solve the constrained multi-row facility layout problem. <em>NCA</em>, <em>37</em>(21), 16575-16598. (<a href='https://doi.org/10.1007/s00521-025-11370-8'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The multi-row facility layout problem is an important component of the facility layout problem. Typically, in this problem, no restrictions are imposed on the location and order of facility placement. However, practical problems in the real world require the addition of various constraints on the facilities. To address these shortcomings in the scientific literature, this paper focuses on the constrained multi-row facility layout problem, incorporating both specified-row positioning and ordering constraints. A mixed-integer programming model is developed to accurately represent the problem with the objective of minimising material handling costs, serving as a foundation for exact optimisation and performance evaluation. Recognising the computational complexity of solving the constrained multi-row facility layout problem, we propose a novel genetic-based hyper-heuristic algorithm with a reward mechanism to efficiently explore the solution space. A problem-specific heuristic rule is developed in the algorithm to generate high-quality initial solutions. The proposed algorithm employs a genetic algorithm on a high-level algorithm, manipulates low-level heuristic operators to act on the problem domain, and designs nine simple and efficient low-level heuristics. An extensive series of experiments on the benchmark instances have been conducted to evaluate the proposed algorithm. The results illustrate that the proposed algorithm outperforms the comparative methods in 13 out of 17 sets of instances in terms of solution results, and 20 sets of instances in terms of solution time, which demonstrates that the proposed algorithm outperforms the other methods in terms of solution quality, stability, and efficiency.},
  archive      = {J_NCA},
  author       = {He, Zongxing and Zhang, Zeqiang and Liu, Junqi and Zhang, Yu and Liu, Silu},
  doi          = {10.1007/s00521-025-11370-8},
  journal      = {Neural Computing and Applications},
  month        = {7},
  number       = {21},
  pages        = {16575-16598},
  shortjournal = {Neural Comput. Appl.},
  title        = {A genetic-based hyper-heuristic optimisation method to solve the constrained multi-row facility layout problem},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). OMER-NPU: On-device multimodal emotion recognition on neural processing unit for low latency and power consumption. <em>NCA</em>, <em>37</em>(21), 16547-16574. (<a href='https://doi.org/10.1007/s00521-025-11368-2'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The escalating integration of artificial intelligence and robotics in daily life underscores the significance of human–robot interaction (HRI). Emotion recognition (ER) within HRI, crucial for meaningful interactions and understanding holistic emotional states, finds applications in diverse domains. Leveraging edge computing, specifically a neural processing unit (NPU), contributes to real-time robotics systems in offline environments, addressing user data security concerns and ensuring prompt responses. This study presents on-device multimodal ER on NPU (OMER-NPU), utilizing heart rate, electroencephalogram, speech, and image modalities with multimodal fusion on NPU. Employing a score-based fusion for a multimodal approach, OMER-NPU optimizes ER models, compresses them, and embeds them in NPU. Comprehensively understanding human emotions, OMER-NPU achieves an accuracy of 99.68%, outperforming unimodal and bimodal configurations. Compared to GPU-based models, OMER-NPU reduces average power consumption by 3.1151 times and latency by 1.4703 times, highlighting NPU’s suitability for low power and rapid responses in HRI, enhancing practicality in social robots. The OMER-NPU facilitates real-time emotion monitoring during daily routines, assisting stress management and holding promise for enhancing individual welfare. This study holds promise for propelling future advancements in HRI by refining the practicality and portability of ER systems.},
  archive      = {J_NCA},
  author       = {Kim, Taein and Moon, Eesun and Kang, Hoyeon and Kim, Hyung Seok},
  doi          = {10.1007/s00521-025-11368-2},
  journal      = {Neural Computing and Applications},
  month        = {7},
  number       = {21},
  pages        = {16547-16574},
  shortjournal = {Neural Comput. Appl.},
  title        = {OMER-NPU: On-device multimodal emotion recognition on neural processing unit for low latency and power consumption},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Identifying overlapping bird species from raw field audio recordings by assembling grouped channel feature attention with multi-scale residual CBAM. <em>NCA</em>, <em>37</em>(21), 16527-16546. (<a href='https://doi.org/10.1007/s00521-025-11361-9'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The automated analysis of bird vocalizations is crucial for applications in ecology, conservation, and vocal behavioural studies. Recent advancements in deep learning have improved classification accuracy, but existing models often struggle with small and imbalanced datasets. This paper presents a novel multi-label bird audio classification framework using Res2Net combined with Convolutional Block Attention Module (CBAM), Spatial Attention (SA), and Grouped Channel Feature Attention (GCA) employing a sequential aggregation (Se) strategy. The proposed model refines predictions by normalizing aggregated sigmoid probabilities and selecting target species based on the highest scores. We evaluate our approach on the xeno-canto database, consisting of 10 bird species. We tested the proposed model on 434 audio recordings with calls of two and three species. The presented framework, which is characterized by fewer parameters compared to other residual networks with attention mechanism, can be directly used for image and audio classification tasks. Experimental results demonstrate that our model achieves an F1-score of 72.20% using Mel-spectrogram features, outperforming state-of-the-art methods. The novelty of the proposed framework lies in the sequential aggregation strategy and Residual CBAM with grouped channel attention mechanism.},
  archive      = {J_NCA},
  author       = {Abdul Kareem, Noumida and Rajan, Rajeev},
  doi          = {10.1007/s00521-025-11361-9},
  journal      = {Neural Computing and Applications},
  month        = {7},
  number       = {21},
  pages        = {16527-16546},
  shortjournal = {Neural Comput. Appl.},
  title        = {Identifying overlapping bird species from raw field audio recordings by assembling grouped channel feature attention with multi-scale residual CBAM},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). An innovative approach harmonizing convolution and self-attention to enhance EEG-based seizure detection. <em>NCA</em>, <em>37</em>(21), 16505-16526. (<a href='https://doi.org/10.1007/s00521-025-11356-6'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Electroencephalography (EEG) is pivotal in diagnosing epilepsy by capturing brain electrical activity. Neurologists visually analyse EEG recordings to detect seizure-related segments. However, manual examination of the extensive data from ambulatory EEGs used for continuous monitoring is time-consuming and can vary with expertise. To address these challenges, we introduce three hybrid architectures for binary seizure classification. The first two architectures combine convolutional neural networks (CNN) with bidirectional gated recurrent unit (Bi-GRU) and bidirectional long short-term memory (Bi-LSTM). These hybrid models were selected based on their ability to capture both spatial features and temporal dependencies, which are crucial for accurate seizure detection. The third model, the ‘convolutional self-attention model’, blends CNN with the Transformer architecture, pioneering transformers in seizure detection. Deviating from single architecture models, our hybrid approach leverages the strengths of two architectures, augmenting the performance of detection models. This methodology strategically employs CNNs to discern spatial patterns, RNNs to identify sequential patterns, and transformers to unveil global relationships within EEG data. Evaluation on the CHB-MIT dataset yielded promising results. CNN-Bi-GRU achieved 98.21% accuracy, 92.63% sensitivity, and 98.31% specificity; CNN-Bi-LSTM achieved 98.88% accuracy, 92.98% sensitivity, and 99% specificity; and the convolutional self-attention model exhibited superior performance with 99.04% accuracy, 96.06% sensitivity, and 99.11% specificity. Our models outperformed existing seizure detection methods on the CHB-MIT dataset, advancing automated seizure detection for better diagnostic accuracy and patient care. The novelty of the study lies in integrating diverse neural network architectures, highlighting the need for personalized models and future research directions.},
  archive      = {J_NCA},
  author       = {Cherian, Resmi and Grace Mary Kanaga, E.},
  doi          = {10.1007/s00521-025-11356-6},
  journal      = {Neural Computing and Applications},
  month        = {7},
  number       = {21},
  pages        = {16505-16526},
  shortjournal = {Neural Comput. Appl.},
  title        = {An innovative approach harmonizing convolution and self-attention to enhance EEG-based seizure detection},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Optimizing energy efficiency through precise occupancy detection: A tailored CNN architecture for smart buildings and beyond. <em>NCA</em>, <em>37</em>(21), 16487-16503. (<a href='https://doi.org/10.1007/s00521-025-11348-6'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Occupancy detection is crucial for various applications, including smart buildings, security systems, and energy management. This paper introduces a novel convolutional neural network (CNN) architecture based on an image encoding approach for accurate occupancy detection. Our network effectively extracts relevant features from occupancy images by leveraging deep learning and image processing techniques, enabling reliable and real-time detection. We employed an image encoding method that converts environmental time-series data into 2D image representations—either grayscale or RGB-like—depending on the input requirements of the CNN model. This transformation captures spatial and temporal characteristics of the data, allowing the network to learn more expressive occupancy-related patterns from raw 1D input. Additionally, we developed a custom CNN architecture optimized for the encoded images, enabling the network to identify key features and understand complex spatial relationships. We evaluated the performance of our CNN through extensive testing on well-known occupancy datasets. The results highlight the superiority of our approach, outperforming existing techniques in accuracy, precision, recall, and F1-score. Our model achieved impressive accuracies of 98.45%, 99.05%, and 97.32% across the three datasets used in this study.},
  archive      = {J_NCA},
  author       = {Sayed, Aya Nabil and Mahmud, Sakib and Bensaali, Faycal and Chowdhury, Muhammad E. H. and Himeur, Yassine},
  doi          = {10.1007/s00521-025-11348-6},
  journal      = {Neural Computing and Applications},
  month        = {7},
  number       = {21},
  pages        = {16487-16503},
  shortjournal = {Neural Comput. Appl.},
  title        = {Optimizing energy efficiency through precise occupancy detection: A tailored CNN architecture for smart buildings and beyond},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Evaluation of LSTM for predicting grip strength using electromyography: A comparison of setups and methods. <em>NCA</em>, <em>37</em>(21), 16461-16485. (<a href='https://doi.org/10.1007/s00521-025-11337-9'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Despite decades of research in prosthetics and myocontrol, using electromyography (EMG) to accurately predict the force a user grasps an object with is still a subject of investigation. Although the problem seems trivial, the optimal EMG setup, able to deliver high prediction accuracy at a minimal economic and computational cost needs to be found. In this work, we compare several EMG setups consisting of one to eight sensors and deep learning methods to find out which combination is most convenient. In particular, we compare long short-term memory (LSTM), together with a stacked autoencoder (LSTM–SAE) and an attention mechanism (LSTMATT). Our experimental results reveal that, while the best performance is attained by LSTM–SAE (coefficient of correlation $$0.9867\pm 0.0087$$ , coefficient of determination $$0.9676\pm 0.0489$$ , normalized root mean square error $$0.048\pm 0.0213$$ ), statistically significant differences can only be found when the number of sensors is drastically reduced, namely to 2 sensors, in which case, anyway, the performance is still close to optimal and even surpasses state-of-the-art methods. Further research will focus on testing the optimal approach and setup online on amputated users using prosthetic hardware in daily living activities.},
  archive      = {J_NCA},
  author       = {Anam, Khairul and Sudrajat, Ahmad and Rizal, Naufal Ainur and Intyanto, Gramandha Wega and Muldayani, Wahyu and Negara, Mohamad Agung Prawira and Sumardi and Bukhori, Saiful and Gitakarma, Made Santo and Castellini, Claudio},
  doi          = {10.1007/s00521-025-11337-9},
  journal      = {Neural Computing and Applications},
  month        = {7},
  number       = {21},
  pages        = {16461-16485},
  shortjournal = {Neural Comput. Appl.},
  title        = {Evaluation of LSTM for predicting grip strength using electromyography: A comparison of setups and methods},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Deep learning-based method for detecting the association between the personnel operating attitudes and the operational targets on offshore platform. <em>NCA</em>, <em>37</em>(21), 16445-16460. (<a href='https://doi.org/10.1007/s00521-025-11373-5'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {S. To address the issue of ineffective monitoring of personnel movement and posture in offshore platform surveillance, we propose a detection method based on the YOLOv7-Pose cascaded with the YOLOv7 model for human pose-operational target association. By using YOLOv7-Pose, the method can be adopted for recognizing human targets and their joint points, postures, and movements in the offshore platform operational scene. The cascaded YOLOv7 model is used to detect possible operational targets from the obtained human posture recognition data set. The association between human posture and operational targets is established then. However, there are practical challenges to its use on offshore platform scenarios such as target variety, density, overlap, obscuration, and small size. Therefore, The YOLOv7 model’s recognition accuracy for operational targets is improved through the introduction of the PConv2D_BN_SiLU module and CBAM attention mechanism, as well as the use of a preferred loss function for optimization. Test experiments on the field dataset verify the feasibility and effectiveness of human posture recognition, operational target recognition, and the association detection method between the two counterparts.},
  archive      = {J_NCA},
  author       = {Sun, Feng and Zhang, Shihai and Qu, Chongnian and Li, Zhenwei},
  doi          = {10.1007/s00521-025-11373-5},
  journal      = {Neural Computing and Applications},
  month        = {7},
  number       = {21},
  pages        = {16445-16460},
  shortjournal = {Neural Comput. Appl.},
  title        = {Deep learning-based method for detecting the association between the personnel operating attitudes and the operational targets on offshore platform},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A transformer approach for predicting transitions in emotional trajectory directly from music audio. <em>NCA</em>, <em>37</em>(21), 16427-16444. (<a href='https://doi.org/10.1007/s00521-025-11333-z'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Music is a key form of artistic communication, closely connected to the perception, evocation, and expression of emotions. Thus, the development of prediction models that estimate perceived emotions in music must consider changes in music audio waves, as this is highly correlated with perceived emotional transitions. Within the music emotion variation detection field, transformer-based models have been gaining attention due to their ability to capture long-range dependencies. This work uses a dimensional approach based on Russell’s Circumplex model of Affect to estimate values in the valence/arousal plane, a curve introduced as the emotional trajectory. Based on the available dynamic annotations of the MediaEval Database of Emotional Analysis of Music, an adapted transformer model architecture is implemented to predict emotional transitions directly from music audio features. Our model achieves state-of-the-art results, with RMSE values of 0.217 and 0.261 for arousal and valence dimensions, respectively, obtaining an approximation of the expected emotional trajectory evoked by a song. Furthermore, we applied a windowed concordance correlation coefficient-based metric to capture trend variations in the estimated trajectory, demonstrating promising results in recognizing emotional transitions.},
  archive      = {J_NCA},
  author       = {Stoller, Pascal A. and Araya, Mauricio},
  doi          = {10.1007/s00521-025-11333-z},
  journal      = {Neural Computing and Applications},
  month        = {7},
  number       = {21},
  pages        = {16427-16444},
  shortjournal = {Neural Comput. Appl.},
  title        = {A transformer approach for predicting transitions in emotional trajectory directly from music audio},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A novel efficient hybrid deep learning framework for ECG-based heartbeat arrhythmia classification. <em>NCA</em>, <em>37</em>(21), 16409-16425. (<a href='https://doi.org/10.1007/s00521-025-11319-x'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, the use of deep learning in ECG analysis has become a major research topic. The main reason for the increasing tendency of researchers to use deep learning is to achieve better results than classical methods based on traditional feature extraction. However, the use of these methods is limited due to the impossibility of extracting automatic features based on the relations of different consecutive segmented input data. The proposed hybrid neural network-based model in this paper, along with automatic features extracted from a deep convolutional neural network (DCNN), uses some helpful ECG-based heartbeat classical features that cannot be learned by DCNN because of segmenting ECG record to heartbeats. These classical features are based on simple necessary time-domain parameters for heartbeat record segmentation without any excessive processing time. Also, this approach allows using independent subnetworks, each one properly designed and trained for its input data type. Experiments conducted on the standard MIT-BIH Arrhythmia dataset show the effectiveness of the proposed scheme, in terms of precision 95.04%, recall 92.28%, F1-score 93.64%, and accuracy 99.13% in four-class state and precision 95.99%, recall 94.09%, F1-score 95.02%, and accuracy 99.33% in two-class state for 70%-30% training–test division of dataset.},
  archive      = {J_NCA},
  author       = {Bahrami, Reza and Fotouhi, Ali M.},
  doi          = {10.1007/s00521-025-11319-x},
  journal      = {Neural Computing and Applications},
  month        = {7},
  number       = {21},
  pages        = {16409-16425},
  shortjournal = {Neural Comput. Appl.},
  title        = {A novel efficient hybrid deep learning framework for ECG-based heartbeat arrhythmia classification},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A CNN-based framework for land use land cover classification of heterogeneous terrain using satellite images. <em>NCA</em>, <em>37</em>(21), 16381-16408. (<a href='https://doi.org/10.1007/s00521-025-11314-2'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {One of the important topics in remote sensing is land use land cover classification. This paper presents a framework that aims to correctly classify land use land cover into seven different landscape characteristics such as agricultural area, agricultural green area, greenery, land, mountain region, settlement, and water body using a convolutional neural network (CNN) model. Thousands of satellite images of different regions in India are obtained from the Google Maps platform using the Maps Static API. These images are pre-processed using image augmentation techniques, and the proposed CNN model is thoroughly trained with this dataset. Several existing deep CNN models such as VGG-16, ResNet-50, and InceptionV3 are also considered. Instead of training these CNN models from scratch, transfer learning is used and these pre-trained networks are fine-tuned by replacing the final layers with additional layers. All the models are trained rigorously using the satellite image dataset and their performances are compared. Results show that the proposed CNN model classifies the test dataset with 89.03% accuracy. Although ResNet-50 provides the most promising results with a classification accuracy of 90.94% on the test dataset, the proposed CNN model has much fewer parameters than ResNet-50, and therefore its training and prediction times are shorter. The model training time and the per-image prediction time of the proposed model are 77.06% and 30.55% less than those of ResNet-50, respectively. Some case studies are also conducted to show the application of the proposed model in classifying the land use land cover of different locations. Additionally, the efficacy of the model is validated by using it for land use land cover classification on the RGB version of the benchmark EuroSAT dataset. The results show that the proposed model can effectively perform land use land cover classification on the EuroSAT dataset with 94.02% accuracy.},
  archive      = {J_NCA},
  author       = {Tarafdar, Anurina and Middya, Asif Iqbal and Banerjee, Sounak and Khatua, Sunirmal and Roy, Sarbani},
  doi          = {10.1007/s00521-025-11314-2},
  journal      = {Neural Computing and Applications},
  month        = {7},
  number       = {21},
  pages        = {16381-16408},
  shortjournal = {Neural Comput. Appl.},
  title        = {A CNN-based framework for land use land cover classification of heterogeneous terrain using satellite images},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Supervised regularized attention-aware clock-triggered recurrent neural network for video summarization. <em>NCA</em>, <em>37</em>(21), 16349-16380. (<a href='https://doi.org/10.1007/s00521-025-11309-z'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Video summarization deals with the generation of a short and meaningful version of the original video. The performance of existing recurrent network-based video summarization methods is limited by the inefficacy of long short-term memory (LSTM) or gated recurrent unit (GRU) variants of recurrent neural network (RNN), in capturing the nonlinear dependencies among relatively long-frame sequences. To address this limitation, a supervised video summarization model is proposed that employs clockwork memory mechanism-based recurrent structure to capture long-term temporal dependencies effectively. The paper also employs regularized self-attention to extract meaningful contextual information among video frames. Further, the video summarization research area lacks an unsupervised objective evaluation mechanism for gauging the performance of automatic summarization models. This study fills that gap by proposing an evaluation mechanism that is free of the subjectivity problem associated with ground-truth annotations. The scope of the presented work is twofold. Firstly, a regularized attention augmented clock-triggered RNN-based summarization (RACR-SUM) model is introduced for the generation of video excerpts. Secondly, a novel unsupervised evaluation mechanism is suggested for the performance evaluation of a summarization model on the basis of various desired aspects of the generated summary. The proposed unified architecture showcases remarkable improvements in performance when compared with various existing studies using both supervised (F-score) and proposed unsupervised evaluation (UVSUMEval) metrics. Detailed experimentation is carried out on four diverse datasets—SumMe, Title-based video Summarization (TvSum), Video Titles in the Wild (VTW), and League Of Legends (LOL)—to study the behavior of the constituent components of the introduced summarization model.},
  archive      = {J_NCA},
  author       = {Gupta, Deeksha and Sharma, Akashdeep},
  doi          = {10.1007/s00521-025-11309-z},
  journal      = {Neural Computing and Applications},
  month        = {7},
  number       = {21},
  pages        = {16349-16380},
  shortjournal = {Neural Comput. Appl.},
  title        = {Supervised regularized attention-aware clock-triggered recurrent neural network for video summarization},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Bayesian math word problem solvers with credibility level: Know what they know and what they do not know. <em>NCA</em>, <em>37</em>(21), 16327-16348. (<a href='https://doi.org/10.1007/s00521-025-11366-4'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Training a model capable of solving math word problems (MWPs) is an interesting research topic in the field of natural language processing. The MWP solver takes a textual math problem as input and generates the solution expression as output. Existing research focuses on how to improve the accuracy of MWP solvers on various datasets. Given a MWP, the existing MWP solvers generate the solution expression, but do not know whether the generated solution expression is correct. Therefore, the existing MWP solvers do not know what they know and what they do not know. In this paper, we propose Bayesian MWP solvers and define a credibility level which is able to judge whether the generated solution expression is correct. We extend two existing MWP solvers with Monte Carlo dropout and Deep Ensembles to obtain Bayesian MWP solvers. With dropout turned on at test time, for each MWP, multiple stochastic forward passes are performed to generate multiple solution expressions. For each solution expression, we calculate the corresponding answer and multiple answers are obtained. We define the credibility level as the proportion of the most frequently occurring answer. We find that if the credibility level is high, the generated solution expression is most likely correct, and vice versa. With credibility level, the Bayesian MWP solvers know what they know and what they do not know. The effectiveness of the proposed Bayesian MWP solvers and credibility level is verified on various datasets.},
  archive      = {J_NCA},
  author       = {Tang, Shengbing and He, Bin and Yu, Xinguo},
  doi          = {10.1007/s00521-025-11366-4},
  journal      = {Neural Computing and Applications},
  month        = {7},
  number       = {21},
  pages        = {16327-16348},
  shortjournal = {Neural Comput. Appl.},
  title        = {Bayesian math word problem solvers with credibility level: Know what they know and what they do not know},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Enhancing computer program anomaly detection: A decreasing-threshold approach. <em>NCA</em>, <em>37</em>(21), 16305-16326. (<a href='https://doi.org/10.1007/s00521-025-11344-w'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The article proposes an enhanced negative selection algorithm to detect anomalies in computer programs. It relies on the adoption of a decreasing activation threshold in the binary template algorithm, which classically features a fixed activation threshold. The algorithm has two steps: first, a minimal set of receptors providing the desired probability of anomaly detection is determined. Second, this set of receptors is adapted to an individual computer program and used to train the intrusion detection system. The final set of receptors is applied to detect anomalies. To verify the proper functioning of the algorithm, practical experiments to detect anomalies using the EMBER dataset and VirusShare repository were carried out. The probabilities of anomaly detection of the experimental results are nearly the same as those calculated analytically.},
  archive      = {J_NCA},
  author       = {Wawryn, Krzysztof and Widulinski, Patryk},
  doi          = {10.1007/s00521-025-11344-w},
  journal      = {Neural Computing and Applications},
  month        = {7},
  number       = {21},
  pages        = {16305-16326},
  shortjournal = {Neural Comput. Appl.},
  title        = {Enhancing computer program anomaly detection: A decreasing-threshold approach},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). The impact of intrinsic rewards on exploration in reinforcement learning. <em>NCA</em>, <em>37</em>(21), 16269-16303. (<a href='https://doi.org/10.1007/s00521-025-11340-0'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {One of the open challenges in Reinforcement Learning (RL) is the hard exploration problem in sparse reward environments. Various types of intrinsic rewards have been proposed to address this challenge by pushing toward diversity. This diversity might be imposed at different levels, favoring the agent to explore different states, policies, or behaviors (State, Policy, and Skill level diversity, respectively). However, the impact of diversity on the agent’s behavior remains unclear. In this work, we aim to fill this gap by studying the effect of different levels of diversity imposed by intrinsic rewards on the exploration patterns of RL agents. We select four intrinsic rewards (State Count, Intrinsic Curiosity Module (ICM), Maximum Entropy, and Diversity is All You Need (DIAYN)), each pushing for a different diversity level. We conduct an empirical study on MiniGrid environments to compare their impact on exploration considering various metrics related to the agent’s exploration, namely: episodic return, observation coverage, agent’s position coverage, policy entropy, and timeframes to reach the sparse reward. The main outcome of the study is that State Count leads to the best exploration performance in the case of low-dimensional observations. However, in the case of RGB observations, the performance of State Count is highly degraded mainly due to representation learning challenges. Conversely, Maximum Entropy is less impacted, resulting in a more robust exploration, despite not always being optimal. Lastly, our empirical study revealed that learning diverse skills with DIAYN, often linked to improved robustness and generalization, does not promote exploration in MiniGrid environments. This is because: (i) Learning the skill space itself can be challenging, and (ii) exploration within the skill space prioritizes differentiating between behaviors rather than achieving uniform state visitation.},
  archive      = {J_NCA},
  author       = {Kayal, Aya and Pignatelli, Eduardo and Toni, Laura},
  doi          = {10.1007/s00521-025-11340-0},
  journal      = {Neural Computing and Applications},
  month        = {7},
  number       = {21},
  pages        = {16269-16303},
  shortjournal = {Neural Comput. Appl.},
  title        = {The impact of intrinsic rewards on exploration in reinforcement learning},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). 90s thai music classification using audio-visual multimodal model. <em>NCA</em>, <em>37</em>(21), 16253-16268. (<a href='https://doi.org/10.1007/s00521-025-11328-w'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep learning, particularly Neural Networks, has demonstrated strong capabilities in analyzing, detecting, and recognizing human activities, with widespread applications across various industries, including entertainment. Leveraging deep learning to predict song names from short video clips featuring dance movements and “E”-monosyllable sounds opens up the possibility of developing automatic trivia games that enhance user experience and offer commercial value. Despite its potential, this area remains underexplored, especially in the context of classifying 90s Thai songs. To address this gap, we curated a custom dataset of 750 video clips from 15 well-known 90s Thai songs, chosen for their distinctive dance styles and continued popularity. In this study, we developed three models: an audio-unimodal model using long short-term memory (LSTM), a visual-unimodal model using convolutional neural network (CNN), and an audio-visual multimodal model that integrates both modalities. Experimental results show that the visual-unimodal model achieved the highest testing accuracy at 91.33%, while the audio-unimodal model performed significantly lower at 47.33%. The multimodal model performed comparably to the visual model, with an accuracy of 91.11%. These findings suggest that visual-based approaches are effective and practical for recognizing dance movements associated with 90s Thai songs. Moreover, the proposed visual model has the potential to generalize to international songs that exhibit distinctive dance characteristics.},
  archive      = {J_NCA},
  author       = {Charntaweekhun, Kanis and Ketchanchai, Duangkamon and Narapan, Jittikan and Wutthijuk, Penprapa and Sirintrawutthiwong, Sasithorn and Siriborvornratanakul, Thitirat},
  doi          = {10.1007/s00521-025-11328-w},
  journal      = {Neural Computing and Applications},
  month        = {7},
  number       = {21},
  pages        = {16253-16268},
  shortjournal = {Neural Comput. Appl.},
  title        = {90s thai music classification using audio-visual multimodal model},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Integrating artificial intelligence for stability assessment in casson hybrid nanofluid flow using LMS-BPNN. <em>NCA</em>, <em>37</em>(21), 16231-16252. (<a href='https://doi.org/10.1007/s00521-025-11320-4'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Artificial intelligence and machine learning revolutionizing the domain of fluid mechanic due to their precise modeling, optimization, and understanding the complex and nonlinearity more efficient. The author uses the AI-based Levenberg–Marquardt Scheme with a Backpropagation Neural Network (LMS-BPNN) to investigate the flow stability of MHD boundary layer flow of Casson Hybrid Nanofluid (CHNF) over a porous shrinking sheet. The partial differential equations (PDEs) that describe Casson hybrid nanofluid are transformed into a system of ordinary differential equations (ODEs) with efficient similarity variables. The initial/reference solution is generated using bvp4c function (an embedded MATLAB function designed to solve systems of ODEs) for various input parameters as demonstrated in scenarios 1–5. There are three options to divide numerical data: $$80\%$$ for training, $$10\%$$ for testing, and an additional $$10\%$$ for validation. The LMS-BPNN is used to obtain the approximate solution for scenarios 1–5. The effectiveness and reliability of the proposed LMS-BPNN are validated through fitness curves based on correlation index ( $$R$$ ), error, and regression analysis. It is noted that velocity and temperature profiles satisfy boundary conditions asymptotically for Senario1-5 with LMS-BPNN. Intelligent algorithms are used to calculate the dual solution for evaluating flow performance results. The perturbation scheme is applied to an unsteady boundary layer problem to obtain the eigenvalues problem. An unsteady solution $$f(\eta , \tau )$$ converges to steady solution $${f}_{o}(\eta )$$ for $$\tau \to \infty$$ when $$\gamma \ge 0$$ . However, an unsteady solution $$f(\eta , \tau )$$ diverges to a steady solution $${f}_{o}(\eta )$$ for $$\tau \to \infty$$ when $$\gamma <0$$ . It is found that the boundary layer thickness for the second (lower branch) solution is higher than the first (upper branch) solution. This investigation is the evidence that the first (upper branch) solution is stable and reliable. The analysis of errors demonstrates the consistency and reliability of the intelligent algorithm.},
  archive      = {J_NCA},
  author       = {Khan, Muhammad Imran and Asgher, Zaheer and Zeeshan, Ahmed and Xu, Huijin},
  doi          = {10.1007/s00521-025-11320-4},
  journal      = {Neural Computing and Applications},
  month        = {7},
  number       = {21},
  pages        = {16231-16252},
  shortjournal = {Neural Comput. Appl.},
  title        = {Integrating artificial intelligence for stability assessment in casson hybrid nanofluid flow using LMS-BPNN},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Data-driven heat pump management: Combining machine learning with anomaly detection for residential hot water systems. <em>NCA</em>, <em>37</em>(21), 16203-16229. (<a href='https://doi.org/10.1007/s00521-025-11318-y'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Heat pumps (HPs) have emerged as a cost-effective and clean technology for sustainable energy systems, but their efficiency in producing hot water remains restricted by conventional threshold-based control methods. Although machine learning (ML) has been successfully implemented for various HP applications, optimization of household hot water demand forecasting remains understudied. This paper addresses this problem by introducing a novel approach that combines predictive ML with anomaly detection to create adaptive hot water production strategies based on household-specific consumption patterns. Our key contributions include: (1) a composite approach combining ML and isolation forest (iForest) to forecast household demand for hot water and steer responsive HP operations; (2) multi-step feature selection with advanced time series analysis to capture complex usage patterns; (3) application and tuning of three ML models: light gradient boosting machine (LightGBM), long short-term memory (LSTM), and bidirectional LSTM with the self-attention mechanism on data from different types of real HP installations; and (4) experimental validation on six real household installations. Our experiments show that the best-performing model LightGBM achieves superior performance, with RMSE improvements of up to 9.37% compared to LSTM variants with $$R^2$$ values between 0.748 $$-$$ 0.983. For anomaly detection, our iForest implementation achieved an F1-score of 0.87 with a false alarm rate of only 5.2%, demonstrating strong generalization capabilities across different household types and consumption patterns, making it suitable for real-world HP deployments.},
  archive      = {J_NCA},
  author       = {Rahal, Manal and Ahmed, Bestoun S. and Renström, Roger and Stener, Robert and Wurtz, Albrecht},
  doi          = {10.1007/s00521-025-11318-y},
  journal      = {Neural Computing and Applications},
  month        = {7},
  number       = {21},
  pages        = {16203-16229},
  shortjournal = {Neural Comput. Appl.},
  title        = {Data-driven heat pump management: Combining machine learning with anomaly detection for residential hot water systems},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). 3DECNN: A novel method for segmentation of the diabetic retinopathy in retinal fundus images using 3D-edge CNN. <em>NCA</em>, <em>37</em>(21), 16187-16201. (<a href='https://doi.org/10.1007/s00521-025-11355-7'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Diabetic retinopathy (DR) is a complication of diabetes that affects the blood vessels in the retina. Over time, high blood sugar levels can damage these vessels, causing them to leak or close off. This can lead to vision loss or blindness if left untreated. Segmentation is a crucial task for improving DR diagnosis, but because the lesions are small, asymmetrical, and blurry in morphological patterns, it is complicated, and automated segmentation is challenging. Majority of multi-step segmentation techniques now in use are based on spatial-based categorization with huge computation time, incorporate errors at various stages, and can result in error aggregation. We introduce a computerized segmentation that uses convolutional neural network (CNN) and vision transformer, capture deeper 3D voxel characteristics, and merge the multi-level metadata of scan images. Two components make-up the suggested 3D edge network (3DECNN): (1) The vision transformer modified with a deep multi-resolution attention mechanism (DMRA) is employed as the foundation system; this is the initial effort to channel the lesions using a 3D CNN. (2) A heavy state space transformer architecture is incorporated further into DMRA that emphasize the regional levels and parallelly suppress the unnecessary additional context information. The contextual information in-between the saliency maps at high levels and the voxels at various scales can be dynamically re-aligned. Using data involving 397 individuals with 3200 images taken from the RFMiD database, we conducted tests using five cross-fold validations. The findings prove that the proposed paradigm is effective, more substantial, robust, and more accurate than previous approaches, with the dice coefficient and the sensitivity score being 0.9176 and 0.9846, respectively.},
  archive      = {J_NCA},
  author       = {Kuruba, Chandrakala and Gopalan, N. P.},
  doi          = {10.1007/s00521-025-11355-7},
  journal      = {Neural Computing and Applications},
  month        = {7},
  number       = {21},
  pages        = {16187-16201},
  shortjournal = {Neural Comput. Appl.},
  title        = {3DECNN: A novel method for segmentation of the diabetic retinopathy in retinal fundus images using 3D-edge CNN},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Nonlinear and reinforcement learning control for motion of hybrid aerial underwater vehicle. <em>NCA</em>, <em>37</em>(21), 16149-16185. (<a href='https://doi.org/10.1007/s00521-024-10592-6'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Hybrid aerial underwater vehicle (HAUV), a new type of vehicle that can operate in the air and underwater, has emerged in recent years and facilitates the joint aerial and underwater missions. Due to the large differences between air and water medium environments, HAUV is a complex system and is challenging to control, especially in the air/water transition. This paper proposes the nonlinear control and deep reinforcement learning control of fixed-wing HAUV (FHAUV). First, the three-dimensional space motion model of FHAUV is developed, and three key issues are involved: disturbance and uncertainty, the air/water transition, control input dead zone and saturation due to the medium differences. Then, for FHAUV and the issues, a nonlinear control including robustness, adaptation and fuzzy logic is proposed, the air/water transition is visualized, and a deep reinforcement learning control of FHAUV is designed by deterministic policy, neural network and temporal difference learning. Finally, the effectiveness of nonlinear control and deep reinforcement learning control of FHAUV is verified in the trajectory tracking control of three key issues. The control results and performance indexes are compared and analyzed. Two methods achieve the goal, the nonlinear control of FHAUV has faster response, and the deep reinforcement learning control of FHAUV has better convergence time and overall error.},
  archive      = {J_NCA},
  author       = {Li, Junping and Zhou, Hexiong and Lu, Di and Zeng, Zheng and Lian, Lian},
  doi          = {10.1007/s00521-024-10592-6},
  journal      = {Neural Computing and Applications},
  month        = {7},
  number       = {21},
  pages        = {16149-16185},
  shortjournal = {Neural Comput. Appl.},
  title        = {Nonlinear and reinforcement learning control for motion of hybrid aerial underwater vehicle},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). PID-fuzzy switching-based strategy to heading control for remote operated vehicle. <em>NCA</em>, <em>37</em>(21), 16131-16147. (<a href='https://doi.org/10.1007/s00521-024-10911-x'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Accurate heading control is the premise for underwater vehicles to complete underwater operations. In order to improve the performance of heading control, this paper proposes a heading control strategy based on PID-fuzzy switching. First, the force analysis of the steering motion was carried out to derive the dynamics model of the steering motion. Then, the whole heading control process is divided into two phases, steering phase and holding phase. Fuzzy control is used in the steering phase, and PID control is used in the holding phase. By setting the automatic switching switch, the control system can switch freely between the two to meet the needs of system adjustment under different errors. Finally, the performance of the established heading control system is verified through motion control simulations and pool experiments, and the proposed control strategy is compared with other methods. The results show that the control strategy can effectively achieve ROV heading control with good steady-state accuracy, and the overall performance has been greatly improved.},
  archive      = {J_NCA},
  author       = {Xie, Baolong and He, Shuping and Cao, Xiang and Wang, Honghai and Stojanovic, Vladimir and Shi, Kaibo},
  doi          = {10.1007/s00521-024-10911-x},
  journal      = {Neural Computing and Applications},
  month        = {7},
  number       = {21},
  pages        = {16131-16147},
  shortjournal = {Neural Comput. Appl.},
  title        = {PID-fuzzy switching-based strategy to heading control for remote operated vehicle},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Dynamic event-triggered adaptive tracking control of UMSVs with internal and external uncertainties. <em>NCA</em>, <em>37</em>(21), 16117-16129. (<a href='https://doi.org/10.1007/s00521-024-10612-5'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This work discusses the adaptive control problem of the underactuated marine surface vehicles (UMSVs) under internal and external uncertainties. To solve the design problem derived from the underactuated feature, the virtual input approach is introduced, and then a level integral cascade form of the UMSVs’ motion model is established. On this basis, under the vectorized backstepping design framework, the uncertain parameters and external disturbances are linearly characterized. Using parameter adaptive technique, adaptive update laws are designed to update the unknown model parameters as well as upper bound of disturbance; meanwhile, to reduce the actuator mechanical wear, a dynamic event-triggered protocol (DETP) is designed in the controller–actuator channel. As a result, a novel DETP-based adaptive control solution is proposed. Compared with the existing work, the main characteristics of the proposed control solution include: (1) the proposed control solution depends only on the state information of the system; (2) the virtual input approach is extended into the model-free scenario; (3) the actuator mechanical wear is inhibited. The theoretical results indicates that the boundedness of all signals in the closed-loop control system of UMSVs can be guaranteed. The effectiveness of the control solution is verified by simulation tests.},
  archive      = {J_NCA},
  author       = {Ma, Yong and Wu, Yixin and Jin, Xinjuan and Gao, Chao and Zhu, Guibing},
  doi          = {10.1007/s00521-024-10612-5},
  journal      = {Neural Computing and Applications},
  month        = {7},
  number       = {21},
  pages        = {16117-16129},
  shortjournal = {Neural Comput. Appl.},
  title        = {Dynamic event-triggered adaptive tracking control of UMSVs with internal and external uncertainties},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Intelligent bounded robust adaptive neural network controller design for fully actuated autonomous underwater vehicles with guaranteed performance using a novel reinforcement learning method. <em>NCA</em>, <em>37</em>(21), 16093-16115. (<a href='https://doi.org/10.1007/s00521-024-10576-6'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, a novel amplitude-limited reinforcement learning controller is proposed for fully actuated autonomous underwater vehicles (AUVs) in the presence of the saturating actuators in six degrees-of-freedom with a guaranteed performance. At first, a second-order error dynamic model is expanded through a nonlinear transformation in terms of constrained posture errors to unconstrained ones. The generalized saturation functions are employed to diminish the actuator saturation risk by bounding the transformed errors. An effective combination of a reinforcement learning strategy, a critic function, actor-critic neural networks and a robust adaptive controller is integrated to secure the robustness of the controller versus nonlinear-in-parameter uncertain terms, the effects of the dynamics that cannot be modeled and exogenous disturbances. A Lyapunov-based stability approach is employed to prove that all time-variant variables in the control system will stay semi-globally uniformly ultimately bounded. The tracking errors will also show a funnel convergence behavior by the designed controller. Eventually, the results of simulation along with a comparative study demonstrate the importance and control objectives of the proposed algorithm and confirm the theoretical contributions.},
  archive      = {J_NCA},
  author       = {Elhaki, Omid and Shojaei, Khoshnam and Sajadian, Seyed Jalal and Moghtaderizadeh, Iman},
  doi          = {10.1007/s00521-024-10576-6},
  journal      = {Neural Computing and Applications},
  month        = {7},
  number       = {21},
  pages        = {16093-16115},
  shortjournal = {Neural Comput. Appl.},
  title        = {Intelligent bounded robust adaptive neural network controller design for fully actuated autonomous underwater vehicles with guaranteed performance using a novel reinforcement learning method},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Fixed-time multilayer neural network-based leader–follower formation control of autonomous surface vessels with limited field-of-view sensors and saturated actuators. <em>NCA</em>, <em>37</em>(21), 16071-16091. (<a href='https://doi.org/10.1007/s00521-024-10575-7'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper proposes a fixed-time multilayer neural network-based formation control problem for the autonomous surface vessels based on the relative distance and orientation angle constraints. The proposed approach only expends the measurements of comparative distance and orientation angle sensors with a limited field-of-view (FOV). The proposed strategy is able to obtain the perfect trajectory tracking performance when the motion ability of all the vessels is restricted into a predefined region owing to the limited FOV limitations. An asymmetric time-varying barrier Lyapunov function is efficiently utilized to cope with the limited FOV constraints. A multilayer neural network is efficiently applied to estimate the model uncertainties and unmodeled dynamics through the online updating of weight matrices. The suggested controller preserves both the comparative distance and orientation angles between consecutive vessels inside the predefined constraints, and the state errors converge to small residual sets around the zero in a fixed time. This feature accelerates the convergence speed and modifies the transient performance of the trajectory tracking control for all the vessels in the formation construction.},
  archive      = {J_NCA},
  author       = {Naderolasli, Amir and Shojaei, Khoshnam and Chatraei, Abbas},
  doi          = {10.1007/s00521-024-10575-7},
  journal      = {Neural Computing and Applications},
  month        = {7},
  number       = {21},
  pages        = {16071-16091},
  shortjournal = {Neural Comput. Appl.},
  title        = {Fixed-time multilayer neural network-based leader–follower formation control of autonomous surface vessels with limited field-of-view sensors and saturated actuators},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A novel approach to multi-USV cooperative search in unknown dynamic marine environment using reinforcement learning. <em>NCA</em>, <em>37</em>(21), 16055-16070. (<a href='https://doi.org/10.1007/s00521-024-10524-4'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Unmanned Surface Vehicles (USVs) are pivotal in diverse marine operations, including search and rescue, environmental monitoring, and maritime security. As their application grows, coordinating multiple USVs for collaborative searching in dynamic and uncharted marine environments presents significant challenges. These challenges stem from the dynamic characteristics of USVs and the interference issues in oceanic communications. Current algorithms often struggle with real-time decision-making and efficient coordination, which are vital for these operations. This paper introduces an innovative strategy for multi-USV collaborative search, leveraging particle swarm optimization (PSO) and reinforcement learning (RL). This approach allows USVs to adapt and respond in real-time, utilizing a dynamic target probability distribution map. Combining PSO with RL, the USVs proficiently maneuver through interference and circumvent obstacles, while ensuring effective teamwork. The proposed method offers a theoretical advancement in the field of multi-USV cooperative search, with potential implications for real-world maritime environments. The proposed method’s effectiveness and efficiency surpass those of conventional approaches, as evidenced by comprehensive simulations in various search scenarios.},
  archive      = {J_NCA},
  author       = {Song, Rui and Gao, Senhui and Li, Yao},
  doi          = {10.1007/s00521-024-10524-4},
  journal      = {Neural Computing and Applications},
  month        = {7},
  number       = {21},
  pages        = {16055-16070},
  shortjournal = {Neural Comput. Appl.},
  title        = {A novel approach to multi-USV cooperative search in unknown dynamic marine environment using reinforcement learning},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Non-singular predefined-time sliding mode control for unmanned surface vehicles based on fuzzy disturbance observer. <em>NCA</em>, <em>37</em>(21), 16039-16053. (<a href='https://doi.org/10.1007/s00521-024-10522-6'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper investigates the non-singular predefined-time sliding mode control problem for unmanned surface vehicles (USVs) based on a fuzzy disturbance observer (FDO). Firstly, a fuzzy disturbance observer is designed to estimate the lumped system uncertainties. Secondly, a non-singular predefined-time sliding surface is proposed, and a corresponding non-singular predefined-time control law is developed in conjunction with the fuzzy disturbance observer. The stability and arrival time of the system are analyzed using Lyapunov stability theory. Finally, simulations verify the effectiveness of the proposed non-singular predefined-time sliding mode control method.},
  archive      = {J_NCA},
  author       = {Chen, Long and Yang, Xu and Wang, Hai and Yan, Bin and Yang, Zhuopeng and Wang, Guangyi},
  doi          = {10.1007/s00521-024-10522-6},
  journal      = {Neural Computing and Applications},
  month        = {7},
  number       = {21},
  pages        = {16039-16053},
  shortjournal = {Neural Comput. Appl.},
  title        = {Non-singular predefined-time sliding mode control for unmanned surface vehicles based on fuzzy disturbance observer},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Reinforcement learning-based distributed cooperative sliding mode control for unmanned surface vehicles. <em>NCA</em>, <em>37</em>(21), 16029-16038. (<a href='https://doi.org/10.1007/s00521-024-10253-8'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The complex marine environment poses a huge challenge to the ocean operations of unmanned surface vehicles (USVs). In this article, we will utilize sliding mode control theory and reinforcement learning (RL) method to consider collaborative control problem for USVs during the ocean operations. Firstly, by using the Kronecker product property, the USVs are further rewritten as an equivalent extended dimensional system, which can be implemented by the distributed control operations. For this extended dimensional system, we formulate the controller with sliding mode gain and reinforcement learning update rate. On this basis, we solve the collaborative control problem for USVs, and then, the corresponding algebraic criteria are also formulated in detail. Subsequently, the reachability is demonstrated for the designed sliding surface. To ends of the paper, an USV numerical example is given to verify the effectiveness of the collaborative control method and algorithm design proposed in this article.},
  archive      = {J_NCA},
  author       = {Zhang, Guangchen and Gao, Han and Yang, Xiaofei and Hu, Jiabao and He, Shuping},
  doi          = {10.1007/s00521-024-10253-8},
  journal      = {Neural Computing and Applications},
  month        = {7},
  number       = {21},
  pages        = {16029-16038},
  shortjournal = {Neural Comput. Appl.},
  title        = {Reinforcement learning-based distributed cooperative sliding mode control for unmanned surface vehicles},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Manoeuvring of underwater snake robot with tail thrust using the actor-critic neural network super-twisting sliding mode control in the uncertain environment and disturbances. <em>NCA</em>, <em>37</em>(21), 16013-16027. (<a href='https://doi.org/10.1007/s00521-023-09113-8'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Snake robots are used for surveillance in unknown underwater environments. The robot comes across the drag force and added mass effect during the motion. The speed of the robot is very low in the underwater environment. Therefore, some thrust force is required to increase the speed. In this work, the snake robot follows a path in uncertain environments with external disturbances. The speed of the snake robot is improved by implementing a thruster at the tail of a snake robot. The equation of motion for the snake robot is derived by considering the effect of the thrust force on each link. The virtual holonomic constraints are formulated to derive the reduced-order dynamical system. The control objective is defined based on reduced-order dynamics using the sliding surface approach. The super-twisting sliding mode control (STSMC) scheme with reinforcement learning (i.e. actor-critic neural network) is proposed for the motion control of the snake robot in the uncertain underwater environment. The dynamic model of the snake robot (equivalent control law) is estimated using the reinforcement learning control approach. The switching control law is designed using the STSMC. The proposed control scheme not only improves the tracking errors but also reduces the chattering effect and control effort. Finally, these results are verified by comparing them to the existing control scheme. Overall, the online dynamic model estimation problem is addressed by designing the controller for the head link of the snake robot.},
  archive      = {J_NCA},
  author       = {Patel, Bhavik M. and Dwivedy, Santosha K.},
  doi          = {10.1007/s00521-023-09113-8},
  journal      = {Neural Computing and Applications},
  month        = {7},
  number       = {21},
  pages        = {16013-16027},
  shortjournal = {Neural Comput. Appl.},
  title        = {Manoeuvring of underwater snake robot with tail thrust using the actor-critic neural network super-twisting sliding mode control in the uncertain environment and disturbances},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Particle swarm optimization-based subsea cable electromagnetic detection by autonomous underwater vehicle. <em>NCA</em>, <em>37</em>(21), 15995-16012. (<a href='https://doi.org/10.1007/s00521-023-09060-4'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Subsea cables are widely used for power and information transmission in offshore infrastructure such as global communication networks and offshore wind farms. The precise positioning of subsea cables is an important prerequisite for maintenance and repair. However, the small diameter and burial depth of 3–5 m below the seabed make locating and detecting the subsea cable technically challenging. This paper investigates the localization and intelligent tracking detection of subsea cables based on autonomous underwater vehicle (AUV) and particle swarm optimization (PSO) algorithm. First, an autonomous electromagnetic localization and tracking topology of subsea cable is designed based on AUV architecture and PSO algorithm. The basic principle of locating the horizontal position and depth of a subsea cable based on horizontal survey line measurement is illustrated. Second, the PSO algorithm is retrofitted with electromagnetic localization. Specifically, the subsea cable localization flow diagram, region of interest (ROI) and fitness function are constructed, respectively. Third, an online swath path planning method for subsea cable detection is designed based on the PSO localization results. Finally, the effectiveness of the PSO submarine cable location algorithm and the online planning method for detection paths is verified through multiple controlled simulation tests.},
  archive      = {J_NCA},
  author       = {Zhang, Jialei and Xiang, Xianbo and Zhang, Qin and Tao, Bo},
  doi          = {10.1007/s00521-023-09060-4},
  journal      = {Neural Computing and Applications},
  month        = {7},
  number       = {21},
  pages        = {15995-16012},
  shortjournal = {Neural Comput. Appl.},
  title        = {Particle swarm optimization-based subsea cable electromagnetic detection by autonomous underwater vehicle},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). PL-net: Towards deep learning-based localization for underwater terrain. <em>NCA</em>, <em>37</em>(21), 15979-15994. (<a href='https://doi.org/10.1007/s00521-023-08931-0'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Underwater terrain matching localization is a method for achieving precise localization by using the features of submarine topography to perform relevant matching operations. Existing underwater terrain matching algorithms suffer from issues such as limited application range, poor real-time performance, and low localization accuracy because of the variable carrier motion state, insignificant undulatory features of underwater terrain, and low underwater terrain data measurement stability. To solve the above problems, we present PL-Net, a novel learning-based underwater terrain matching localization method. First, we extract the terrain’s keypoints to avoid the complexity of large-scale computation and improve the algorithm’s real-time performance. Then, the pretrained deep neural network automatically extracts the high-level terrain features, avoiding the manual setting of feature extraction patterns, reducing the probability of localization failure caused by environmental and equipment factors, and improving the algorithm’s robustness. Finally, we improve the localization accuracy by adding a self-attention mechanism that enables the network to adjust the weights of each component and pay more attention to the regions with significant features. We conducted a series of tests to validate the proposed method. The results indicate that our method significantly outperforms other methods, indicating that our method can achieve accurate and real-time localization in a wide range of underwater environments.},
  archive      = {J_NCA},
  author       = {Peng, Xin and Zhang, Yang and Xu, Zhen and Zhang, Zhiguo and Chen, Lijun and Li, Cong},
  doi          = {10.1007/s00521-023-08931-0},
  journal      = {Neural Computing and Applications},
  month        = {7},
  number       = {21},
  pages        = {15979-15994},
  shortjournal = {Neural Comput. Appl.},
  title        = {PL-net: Towards deep learning-based localization for underwater terrain},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A novel deep reinforcement learning for POMDP-based autonomous ship collision decision-making. <em>NCA</em>, <em>37</em>(21), 15963-15977. (<a href='https://doi.org/10.1007/s00521-023-08908-z'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To address the challenge of partially observable environment states in multi-ship collision avoidance decision-making, a novel collision avoidance decision model is developed based on Partially Observable Markov Decision Processes (POMDP), integrating both static and dynamic obstacles. A new reward mechanism is designed to overcome the problem of sparse rewards, incorporating the international regulations for preventing collisions at sea (COLREGs) into the reward function. The Proximal Policy Optimization (PPO) algorithm is employed to train the model, accompanied by a suitable network structure. Furthermore, image conversion and scaling operations are applied to the network’s architecture to reduce the dimensionality of the state space, thereby enhancing the training speed and fitting performance of the algorithm. Subsequently, a simulation environment is created using the Gym platform, incorporating multiple static and dynamic obstacles, and a series of experiments containing classic encounter scenarios are designed to validate the model. The evaluation metrics, such as the total cumulative reward and training steps, demonstrate that the proposed algorithm is capable of making accurate decisions in an environment with combined static and dynamic obstacles. Through several ablation experiments and analysis of total run-time, it is shown that the proposed reward mechanism effectively addresses the issue of sparse rewards, while the algorithm significantly improves the fitting speed and expedites the training process. The proposed algorithm’s ability to address the problem of partially observable environment states in multi-ship collision avoidance decision-making is confirmed through a comparison with other classical deep reinforcement learning (DRL) algorithms.},
  archive      = {J_NCA},
  author       = {Zhang, Xinyu and Zheng, Kangjie and Wang, Chengbo and Chen, Jihong and Qi, Huaiyuan},
  doi          = {10.1007/s00521-023-08908-z},
  journal      = {Neural Computing and Applications},
  month        = {7},
  number       = {21},
  pages        = {15963-15977},
  shortjournal = {Neural Comput. Appl.},
  title        = {A novel deep reinforcement learning for POMDP-based autonomous ship collision decision-making},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Nonlinear ELM estimator-based path-following control for perturbed unmanned marine systems with prescribed performance. <em>NCA</em>, <em>37</em>(21), 15949-15962. (<a href='https://doi.org/10.1007/s00521-023-08653-3'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper is concerned with the robust path-following control problem for a class of perturbed unmanned marine systems with prescribed performance under the influence of perturbations which are composed by external disturbances and parametric uncertainties. In order to withstand the negative impacts of perturbations, a novel nonlinear extreme learning machine (ELM)-based estimator is presented to estimate the perturbations. On the basis of the estimation results, a robust perturbation rejection control scheme is developed to ensure the following errors of the unmanned marine system reducing into a predefined region by devising a decaying variable. The asymptotic path-following result of the unmanned marine system is concluded by Lyapunov stability theory, as well as the prescribed performance is ensured by employing the decaying variable. Finally, simulations verify the effectiveness of the proposed nonlinear ELM estimator-based perturbation rejection control technique.},
  archive      = {J_NCA},
  author       = {Jin, Xiaozheng and Jiang, Jiahuan and Wang, Hai and Deng, Chao},
  doi          = {10.1007/s00521-023-08653-3},
  journal      = {Neural Computing and Applications},
  month        = {7},
  number       = {21},
  pages        = {15949-15962},
  shortjournal = {Neural Comput. Appl.},
  title        = {Nonlinear ELM estimator-based path-following control for perturbed unmanned marine systems with prescribed performance},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Levenberg–Marquardt neural network analysis of entropy optimization on MHD nanofluid convective flow with nonlinear thermal radiation and Cattaneo–Christov heat and mass fluxes: A comparative study. <em>NCA</em>, <em>37</em>(20), 15761-15791. (<a href='https://doi.org/10.1007/s00521-025-11312-4'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The current study presents a comparative analysis to validate the computing efficiency of artificial neural network (ANN) with Levenberg–Marquardt feed-forward backpropagation algorithm (LMFFBPA) against numerical method (NM) and multiple linear regression (MLR) for analysing the optimization of entropy in MHD nanofluid convective flow over two distinct geometries. The impact of nonlinear radiation, Brownian motion, Cattaneo–Christov heat and mass fluxes, and thermophoretic particle deposition under the convective boundary conditions are taken into consideration. By applying the appropriate transformations, the system of nonlinear PDEs (Partial differential equations) is converted into a set of nonlinear ODEs. The resulting nonlinear system of ODEs (Ordinary differential equations) is solved using NM. The acquired datasets are utilised to train, validate and test LMFFBPA, enabling precise predictions of the skin friction coefficient, Nusselt number and Sherwood number. Examination of mean square error (MSE), error histogram, and regression fitness confirms the proficiency of ANN. These evaluations ensure the accuracy and reliability of the present outcomes. The regression at the training, validation, testing, and all stages tends to be almost 1, indicating optimal ANN performance with high accuracy and minimal error. Based on the comparisons between ANN and MLR, it can be determined that the ANN exhibits superior performance with a high level of accuracy. Artificial neural networks using the Levenberg–Marquardt technique improve simulation speed and precision, hence facilitating improved design and optimization. This research will yield practical real-world solutions in material design and manufacturing, energy generation and conversion, thermal management systems, and other related areas.},
  archive      = {J_NCA},
  author       = {Priya, M. and Bala Anki Reddy, P.},
  doi          = {10.1007/s00521-025-11312-4},
  journal      = {Neural Computing and Applications},
  month        = {7},
  number       = {20},
  pages        = {15761-15791},
  shortjournal = {Neural Comput. Appl.},
  title        = {Levenberg–Marquardt neural network analysis of entropy optimization on MHD nanofluid convective flow with nonlinear thermal radiation and Cattaneo–Christov heat and mass fluxes: A comparative study},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). An imperceptible attack on machine translation system and the countermeasure. <em>NCA</em>, <em>37</em>(20), 15745-15759. (<a href='https://doi.org/10.1007/s00521-025-11308-0'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Currently, neural machine translation plays an important role in cross-language communication and cooperation. Since it is very vulnerable to small perturbations in input text, it must face various injection attacks. Different from image data, text data are discrete, which makes it difficult to create imperceptible perturbation in text. In order to keep the similarity of the original input and the adversarial sample, similar words replacement-based heuristic strategy is often used by attackers to create perturbations in input text to manipulate the results of machine translation. It usually needs high overhead but is unable to achieve optimal attack performance. Imperceptible characters such as invisible characters and homoglyphs have very good concealment and deception to human eyes, using them to design adversarial samples against neural machine translation is a good option. Unfortunately, there are rare researchers paying attention to such attacks against neural machine translation. To defend against them, four types of special characters (zero-width space character (ZWSP), homoglyphs, reordering control characters, deletion control characters) that have the greatest influence on the translation result and the basic principle of character encoding and semantic understanding in the machine translation are systematically analyzed. And inspired by the feature filtering advantages of the LSTM’s gating mechanism in processing sequence data, the LSTM is employed to deal with imperceptible character in machine translation. Based on these, a machine translation model integrated with LSTM and Transformer is constructed. The experimental results show that LSTM can effectively identify and process illegal invisible characters, and the proposed LSTM + Transformer translation model can perform much better than the Transformer translation model in dealing with the imperceptible characters injection.},
  archive      = {J_NCA},
  author       = {Jiang, Jiamin and Yao, Xuanxia and Ouyang, Ben},
  doi          = {10.1007/s00521-025-11308-0},
  journal      = {Neural Computing and Applications},
  month        = {7},
  number       = {20},
  pages        = {15745-15759},
  shortjournal = {Neural Comput. Appl.},
  title        = {An imperceptible attack on machine translation system and the countermeasure},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A comparative study of hybrid neural network with metaheuristic algorithm for breast cancer data classification with TOPSIS MCDM approach. <em>NCA</em>, <em>37</em>(20), 15719-15744. (<a href='https://doi.org/10.1007/s00521-025-11281-8'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Fatal disorders associated with cancer are prevalent in many industrialized and developing nations across the world. It is especially true for women, where the daily incidence of breast cancer rises partly as a result of early diagnosis errors and general ignorance. Early detection and accurate classification of the cancer must occur in order to provide an appropriate initial course of treatment for breast cancer. Artificial neural networks (ANNs) are a powerful process of classifying for medical data and are essential for diagnosing illnesses. The learning algorithm selection has a significant impact on how well ANN training works and a number of approaches have shown potential. To train ANNs for classifying breast cancer data, present work carries out an empirical investigation that focuses on performance of several metaheuristic (MH) algorithms as learning techniques. The experimental in this article makes use of the well-known Wisconsin breast cancer dataset for binary classification (benign or malignant). Eight newly developed parameter-less swarm-based MHs, namely GAO, COA, OOA, WOA, ZOA, POA, NGO, and TDO, have been used as learning algorithms, and their performance is evaluated against the traditional backpropagation (BP) neural network. Performance is evaluated based on several factors, including accuracy, sensitivity, specificity, precision, geometric mean (GM), F-measure, and false-positive rate (FPR), using box plots. Also, binary entropy loss versus iteration has been explored using convergence graph. Finally, the ideal ANN model for accurate classification is found using the TOPSIS method, a multi-attribute decision-making (MADM) technique. According to studies, the northern goshawk optimization (NGO) algorithm performs better than any other algorithm. These results highlight how MHs, particularly the NGO, can improve ANN classification performance for medical data, especially when it comes to breast cancer diagnosis.},
  archive      = {J_NCA},
  author       = {Das, Banya and Roy, Susmita and Debbarma, Naima and Bhattacharya, Paritosh},
  doi          = {10.1007/s00521-025-11281-8},
  journal      = {Neural Computing and Applications},
  month        = {7},
  number       = {20},
  pages        = {15719-15744},
  shortjournal = {Neural Comput. Appl.},
  title        = {A comparative study of hybrid neural network with metaheuristic algorithm for breast cancer data classification with TOPSIS MCDM approach},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). CTAUNet: Improved retinal blood vessel segmentation with collaborative transformer attention U-net. <em>NCA</em>, <em>37</em>(20), 15705-15718. (<a href='https://doi.org/10.1007/s00521-025-11332-0'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Accurate segmentation of retinal blood vessels is essential for the early diagnosis and effective treatment of various ophthalmic diseases. However, the complex structure of blood vessels, variations in vessel width, and the presence of low-contrast regions in retinal images pose significant challenges. To address these challenges, this paper presents a collaborative transformer attention U-Net (CTAUNet) framework for blood vessel segmentation. A U-Net architecture built with convolutional neural network (CNN) is employed by CTAUNet to achieve accurate blood vessel localization, while a swin transformer module is integrated to capture both fine details and broad image context within the retinal image. Furthermore, a collaborative feature fusion mechanism effectively combines multi-scale features from both CNN and transformer branches, improving the segmentation of blood vessels across different scales. Extensive experiments conducted on multiple benchmark datasets (STARE, CHASE DB1, and DRIVE) show improvement over existing methods, achieving F1-scores of 0.872, 0.888, and 0.879 and accuracy of 0.965, 0.989, and 0.981 on DRIVE, STARE, and CHASE DB1 datasets, respectively.},
  archive      = {J_NCA},
  author       = {Punn, Narinder Singh and Kumar, Sunil},
  doi          = {10.1007/s00521-025-11332-0},
  journal      = {Neural Computing and Applications},
  month        = {7},
  number       = {20},
  pages        = {15705-15718},
  shortjournal = {Neural Comput. Appl.},
  title        = {CTAUNet: Improved retinal blood vessel segmentation with collaborative transformer attention U-net},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Exploring the scope of deep neural network to predict print densitometry from scanner data. <em>NCA</em>, <em>37</em>(20), 15681-15704. (<a href='https://doi.org/10.1007/s00521-025-11313-3'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Optical density is an important metric for print process control and quality assurance in print production. Conventionally, handheld densitometers are used for this metrology. These devices are expensive and need skilled manpower for operation which may become difficult for small- and medium-scale printing presses. This, in turn, restricts such presses to meet different print standards and business growth, particularly in respect to the global print market. This work proposes the use of a reflective scanner as a possible alternative to such densitometers since scanners are widely available in every press and much less expensive compared to densitometers. For experiments, test targets were printed on a variety of substrates that are commonly used for print production, resulting in a total 3168 color patches. The features were extracted from these patches to build the deep neural network (DNN)-driven prediction model. The model was optimized using various soft computing techniques. The results of different methods were compared and grid search technique has been found most suitable for predicting optical densities from scanned printed patches. The model was compared against some of the existing DNN models and the proposed method has been found to be a promising competitor in terms of prediction accuracy and other standard performance metrics. Based on the considerable performance and consistency, the proposed method can be considered as a feasible, much less expensive and easy to operate alternative of existing densitometer-based process control and evaluation methods in print production.},
  archive      = {J_NCA},
  author       = {Debnath, Shankhya and Chatterjee, Arpitam},
  doi          = {10.1007/s00521-025-11313-3},
  journal      = {Neural Computing and Applications},
  month        = {7},
  number       = {20},
  pages        = {15681-15704},
  shortjournal = {Neural Comput. Appl.},
  title        = {Exploring the scope of deep neural network to predict print densitometry from scanner data},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Gaussian random fuzzy and nature-inspired neural networks: A novel approach to brent oil price prediction. <em>NCA</em>, <em>37</em>(20), 15661-15679. (<a href='https://doi.org/10.1007/s00521-025-11306-2'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Given the volatile nature of oil prices in the wake of COVID-19 and the Russia-Ukraine war, the need for advanced prediction models is evident. The Autoregressive Integrated Moving Average model estimated through the maximum likelihood method with Marquardt-BFGS optimisation (ARIMA-BFGS) was used to select the relevant predictors for three different models: the Extreme Learning Machine (ELM), the newly introduced Evidential Neural Network for Regression with Gaussian Random Fuzzy numbers (EVNN-FUZZY) and an Artificial Neural Network fine-tuned with Particle Swarm Optimisation (ANN-PSO). Formal unit root tests, Augmented Dickey Fuller (ADF) and Phillips-Perron (PP) are used to test the stationarity of the Brent oil price before estimating ARIMA-BFGS. Evaluation measures such as root-mean-squared error (RMSE), mean absolute error (MAE), mean absolute percentage error (MAPE) and coefficient of determination ( $$R^2$$ ) are used to assess the performance of the models. The study utilises a combination of traditional methods and neural networks to improve the accuracy of the Brent oil price prediction. ANN-PSO improves the predictive precision of ARIMA-BFGS by 65.30% for the training dataset and 88.72% for the testing sample. The incorporation of COVID-19 and the Russia-Ukraine war has improved the performance of EVNN-FUZZY. Governments, investors and producers can all benefit from these outcomes while making financial decisions. The findings of this study can be used by oil-exporting economies to guide their budgets, while oil-importing countries can use them to manage inflation.},
  archive      = {J_NCA},
  author       = {Mati, Sagiru and Ismael, Goran Yousif and Usman, Abduallahi Garba and Samour, Ahmed and Aliyu, Nazifi and Alsakarneh, Raad Abdelhalim Ibrahim and Abba, Sani I.},
  doi          = {10.1007/s00521-025-11306-2},
  journal      = {Neural Computing and Applications},
  month        = {7},
  number       = {20},
  pages        = {15661-15679},
  shortjournal = {Neural Comput. Appl.},
  title        = {Gaussian random fuzzy and nature-inspired neural networks: A novel approach to brent oil price prediction},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). MSAE-DL: Enhancing breast cancer classification through hybrid self-attention integration, feature fusion, and ensemble classification in digital breast tomosynthesis. <em>NCA</em>, <em>37</em>(20), 15635-15659. (<a href='https://doi.org/10.1007/s00521-025-11192-8'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Breast cancer is a significant global health challenge, with highest rates of occurrence and mortality worldwide. Early detection is important for the improvement of patient outcomes and the reduction of the overall burden of the disease. Digital breast tomosynthesis (DBT) scans offer three-dimensional images of the breast tissue and is becoming a valuable tool in the detection of breast abnormalities. However, accurately classifying DBT scans is challenging due to the complexity of the anatomy of the breast and the presence of minor abnormalities. This study introduces the MSAE-DL system for the multi-class classification of DBT scans. The system incorporates a novel multi-head self-attention model with a unique ensemble classification model. Features were extracted from the Mod_AlexNet Self-Attention model and fused with histogram of oriented gradients (HOG) descriptors. Subsequently, feature vectors are reduced using three feature selection models. Finally, a novel ensemble classification model is introduced and fuses class and classifier weights for the final prediction using various classifiers. The system demonstrates optimal performance in classifying DBT scans into normal, benign, and malignant classes, achieving an accuracy of 90.13%, precision of 92.77%, and f1-score of 91.03%. The experimental results underscore the potential of this approach in enhancing DBT classification into three different classes, rather than simply binary classification.},
  archive      = {J_NCA},
  author       = {El-Shazli, Alaa M. Adel and Youssef, Sherin M. and Soliman, Abdel Hamid and Chibelushi, Claude},
  doi          = {10.1007/s00521-025-11192-8},
  journal      = {Neural Computing and Applications},
  month        = {7},
  number       = {20},
  pages        = {15635-15659},
  shortjournal = {Neural Comput. Appl.},
  title        = {MSAE-DL: Enhancing breast cancer classification through hybrid self-attention integration, feature fusion, and ensemble classification in digital breast tomosynthesis},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Dynamic optimization of active power filters in grid-tied photovoltaic systems using partial reinforcement learning. <em>NCA</em>, <em>37</em>(20), 15605-15634. (<a href='https://doi.org/10.1007/s00521-025-11186-6'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As the global energy demand raises, renewable energy sources are considered as the reliable option to meet the growing energy demands. The grid-tied photovoltaic (PV) systems gained more significant scope in last few years because of their eco-friendly nature but limited due to prevalent power quality issues including harmonics distortion, and lower power factors that affect the quality of the power distribution systems. This paper introduces a novel partial reinforcement optimizer-based active power filter algorithm for grid-tied PV systems to enhance power factor and mitigate harmonics distortion. MATLAB/Simulink is used to implement the PRO-based APF algorithm and tested in different scenarios including, nominal operation, harmonic distortion, power factor variation, and rapid voltage fluctuations. The simulation findings exhibit that the PRO-based APF model achieves a total harmonic distortion (THD) of 8.47%, a power factor improvement of 0.95, and a response time of 0.0125 s. This algorithm exhibits better performance in terms of THD reduction, power factor improvement, and response time. Furthermore, by adapting to dynamic changes in different test scenarios the PRO-based APF algorithm depicts improved robustness in Grid-tied PV systems. Therefore, the proposed method by optimizing the control parameters effectively contributes to the improved grid stability of the power distribution systems.},
  archive      = {J_NCA},
  author       = {Dash, Dipak Kumar and Sadhu, Pradip Kumar and Shrivastav, Alok Kumar},
  doi          = {10.1007/s00521-025-11186-6},
  journal      = {Neural Computing and Applications},
  month        = {7},
  number       = {20},
  pages        = {15605-15634},
  shortjournal = {Neural Comput. Appl.},
  title        = {Dynamic optimization of active power filters in grid-tied photovoltaic systems using partial reinforcement learning},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Fourier convolutional decoder: Reconstructing solar flare images via deep learning. <em>NCA</em>, <em>37</em>(20), 15573-15604. (<a href='https://doi.org/10.1007/s00521-025-11283-6'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Reconstructing images from observational data is a complex and time-consuming process, particularly in astronomy, where traditional algorithms like CLEAN require extensive computational resources and expert interpretation to distinguish genuine features from artifacts, especially without ground truth data. To address these challenges, we developed the Fourier convolutional decoder (FCD), a custom-made overcomplete autoencoder trained on simulated data with available ground truth. This enables the network to generate outputs that closely approximate expected ground truth. The model’s versatility was demonstrated on both simulated and observational datasets, with a specific application to data from the spectrometer/telescope for imaging X-rays (STIX) on the solar orbiter. In the simulated environment, FCD’s performance was evaluated using multiple-image reconstruction metrics, demonstrating its ability to produce accurate images with minimal artifacts. For observational data, FCD was compared with benchmark algorithms, focusing on reconstruction metrics related to Fourier components. Our evaluation found that FCD is the fastest imaging method, with runtimes on the order of milliseconds. Its computational cost is comparable to the most efficient reconstruction algorithm and 280 $${\times }$$ faster than the slowest imaging method for single-image reconstruction on a CPU. Additionally, its runtime can be reduced by an order of magnitude for multiple-image reconstruction on a GPU. FCD outperforms or matches state-of-the-art methods on simulated data, achieving a mean MS-SSIM of 0.97, LPIPS of 0.04, PSNR of 35.70 dB, a Dice coefficient of 0.83, and a Hausdorff distance of 5.08. Finally, on experimental STIX observations, FCD remains competitive with top methods despite reduced performance compared to simulated data.},
  archive      = {J_NCA},
  author       = {Selcuk-Simsek, Merve and Massa, Paolo and Xiao, Hualin and Krucker, Säm and Csillaghy, André},
  doi          = {10.1007/s00521-025-11283-6},
  journal      = {Neural Computing and Applications},
  month        = {7},
  number       = {20},
  pages        = {15573-15604},
  shortjournal = {Neural Comput. Appl.},
  title        = {Fourier convolutional decoder: Reconstructing solar flare images via deep learning},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Smart navigation system for emergency vehicles (SNSEV): Utilizing fog and cloud computing technology for real-time traffic management. <em>NCA</em>, <em>37</em>(20), 15547-15571. (<a href='https://doi.org/10.1007/s00521-025-11278-3'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The navigation of emergency vehicles is a critical component of effective emergency response in a smart city. To improve response time, it is necessary to have a navigation system that can predict the shortest path to the destination and adjust in real time to current traffic conditions. This paper proposes a smart navigation system for emergency vehicles (SNSEV), a real-time navigation algorithm for emergency vehicles in smart cities that utilize fog and cloud computing technology for real-time traffic management. IoT devices such as sensors and cameras collect real-time data on traffic conditions and roadblocks, which is then processed and analyzed using fog computing technology. Cloud computing technology is then utilized to provide emergency vehicles with real-time navigation and priority control, reducing response time and ensuring that they reach their destination as quickly as possible. This paper presents the proposed SNSEV algorithm and the system architecture and discusses its potential benefits and challenges. The results show that SNSEV can significantly improve the efficiency and effectiveness of emergency response in a smart city, leading to improved public safety and well-being. According to the findings of several experiments, SNSEV works better than its competitors because it allows for the highest possible throughput, the lowest possible bandwidth usage, and the shortest possible delay.},
  archive      = {J_NCA},
  author       = {Talaat, Fatma M. and Gamel, Samah A.},
  doi          = {10.1007/s00521-025-11278-3},
  journal      = {Neural Computing and Applications},
  month        = {7},
  number       = {20},
  pages        = {15547-15571},
  shortjournal = {Neural Comput. Appl.},
  title        = {Smart navigation system for emergency vehicles (SNSEV): Utilizing fog and cloud computing technology for real-time traffic management},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). TransConv: A lightweight architecture based on transformers and convolutional neural networks for adenocarcinoma and barrett’s esophagus identification. <em>NCA</em>, <em>37</em>(20), 15535-15546. (<a href='https://doi.org/10.1007/s00521-025-11299-y'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Barrett’s esophagus, also known as BE, is commonly associated with repeated exposure to stomach acid. If not treated properly, it may evolve into esophageal adenocarcinoma, aka esophageal cancer. This paper proposes TransConv, a hybrid architecture that benefits from features learned by pre-trained vision transformers (ViTs) and convolutional neural networks (CNNs), followed by a shallow neural network composed of three normalizations, ReLU activations, and fully connected layers, and a SoftMax head to distinguish between BE and esophageal cancer. TransConv is designed to be training-lightweight, and for the ViT and CNN backbone models, weights are kept frozen during training, i.e., the primary goal of TransConv is to learn the weights of the fully connected layer from both backbones only, avoiding the burden of updating their weights but still learning their final descriptions for the lightweight convolutional model. We report promising results with low computational training costs in two datasets, one public and another private. From our achievements, TransConv was able to deliver balanced accuracy results around 85% and 86% for each evaluated dataset, respectively, in a design that required only 50 epochs of model training, a very reduced number compared to state-of-the-art conducted studies in the same domain.},
  archive      = {J_NCA},
  author       = {Souza, Luis A. and Pacheco, André G. C. and Souza, Alberto F. De and Oliveira-Santos, Thiago and Badue, Claudine and Palm, Christoph and Papa, João Paulo},
  doi          = {10.1007/s00521-025-11299-y},
  journal      = {Neural Computing and Applications},
  month        = {7},
  number       = {20},
  pages        = {15535-15546},
  shortjournal = {Neural Comput. Appl.},
  title        = {TransConv: A lightweight architecture based on transformers and convolutional neural networks for adenocarcinoma and barrett’s esophagus identification},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Attention-based transformer model for arabic image captioning. <em>NCA</em>, <em>37</em>(20), 15501-15533. (<a href='https://doi.org/10.1007/s00521-025-11199-1'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Image captioning generates descriptive text from an input image, establishing a connection between the image content and words. Recently, the most successful approaches for automatically creating image captions have been based on transformer learning models. Arabic image captioning has gained importance due to the unique characteristics of the Arabic language. This paper introduces an attention-based transformer model for Arabic image captioning (ARTIC). ARTIC employs a deep learning convolutional neural network (CNN) for feature extraction from the images and a transformer encoder–decoder architecture for generating textual captions. ARTIC utilizes an ensemble learning approach based on a voting mechanism that selects the caption with the highest bilingual evaluation understudy (BLEU) score to produce the captions. To evaluate the effectiveness of the proposed model, the publicly available Flickr8k benchmark dataset was used for Arabic image captioning. Our results show that ARTIC achieved the best scores for BLEU-1, CIDEr, and ROUGE at rates of (0.626), (0.838), and (0.471), respectively. The other metrics, such as BLEU-2 and METEOR, achieved competitive rates of (0.381) and (0.332), respectively. The experiments with the Flickr30k English dataset demonstrated the generalizability of the proposed approach to other languages. These results indicate that the suggested model outperformed other models used for comparison.},
  archive      = {J_NCA},
  author       = {Al Badarneh, Israa and Al Mahmoud, Rana Husni and Hammo, Bassam H. and Al-Kadi, Omar},
  doi          = {10.1007/s00521-025-11199-1},
  journal      = {Neural Computing and Applications},
  month        = {7},
  number       = {20},
  pages        = {15501-15533},
  shortjournal = {Neural Comput. Appl.},
  title        = {Attention-based transformer model for arabic image captioning},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). MMHBC-net: A multi-modal hybrid approach for breast cancer classification. <em>NCA</em>, <em>37</em>(20), 15469-15499. (<a href='https://doi.org/10.1007/s00521-025-11291-6'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Early detection of breast cancer is crucial for improving survival rates. Many computer-aided diagnosis (CAD) models have been proposed in the past for the detection of breast cancer. However, analyzing breast cancer across multiple imaging modalities remains a challenge. Different imaging modalities provide complimentary information on the lesion and help in detecting the abnormalities at an early stage. Our study introduces a MMHBC-Net model that utilizes mammogram, ultrasound, and histopathology images to enhance detection accuracy. Pre-processing and augmentation of the images are performed for better accuracy. For extracting features from mammogram and ultrasound images, a deep learning model that includes four convolutional layers followed by a dense layer. For histopathology, handcrafted techniques based on texture and color feature extraction are employed. Data-level fusion is performed among the image modalities and passed through various classifiers, including RF, BAG, DT, ADA, NB, LGBM, XGB, GBC, kNN SVM, and GCN. Based on the classifier’s performance, the top five classifiers for each modality are selected to construct a stacking ensemble classifier for the final classification. Experimentation involves datasets such as DDSM, MIAS, INbreast for mammograms, BUS and MBU for ultrasounds, and BreakHis for histopathology images. The results reveal that the multi-modal CAD system outperforms uni-modal CAD systems by obtaining test accuracy of 99.96, 88.16, 95.33, and 91.79% for combined, mammogram, ultrasound, and histopathology images. This highlights the effectiveness of the multi-modal approach for breast cancer detection.},
  archive      = {J_NCA},
  author       = {Deb, Dipti and Dash, Ratnakar and Mohapatra, Durga Prasad},
  doi          = {10.1007/s00521-025-11291-6},
  journal      = {Neural Computing and Applications},
  month        = {7},
  number       = {20},
  pages        = {15469-15499},
  shortjournal = {Neural Comput. Appl.},
  title        = {MMHBC-net: A multi-modal hybrid approach for breast cancer classification},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A convolutional neural network method to obtain the dynamic model parameters of a three-linear-axis cartesian robot. <em>NCA</em>, <em>37</em>(20), 15439-15467. (<a href='https://doi.org/10.1007/s00521-025-11297-0'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Parametric identification plays a crucial role in improving the performance, accuracy, robustness and repeatability of Cartesian robots in advanced manufacturing processes applications, assembly and material handling. This paper presents an innovative convolutional neural network (CNN) architecture for the parametric identification of 18 critical dynamic model parameters in a three-linear-axis Cartesian robot. The dynamic model is obtained by the Euler–Lagrange motion equations from an analysis of lumped parameters such as friction, inertia, mass and stiffness of the design of a three-axis linear Cartesian robot. The proposed CNN architecture is trained and validated using experimental data, achieving a parametric identification accuracy of approximately 98%. The identified parameters are further applied to optimize the robot’s dynamic response, demonstrating improved trajectory tracking and reduced energy consumption in manufacturing tasks. This work bridges the gap between theoretical modeling and practical application, providing a robust framework for enhancing robotic system performance in advanced manufacturing environments.},
  archive      = {J_NCA},
  author       = {Guerra-Marín, Michelle and Vargas-Treviño, M. Aurora D. and Vergara-Limon, Sergio and López-Gómez, Jesús and Carreón-Diaz-de-León, Carlos L. and González-Arriaga, Daniel M. and García-López, Margarita C. and Pintor-Michimani, Juan F.},
  doi          = {10.1007/s00521-025-11297-0},
  journal      = {Neural Computing and Applications},
  month        = {7},
  number       = {20},
  pages        = {15439-15467},
  shortjournal = {Neural Comput. Appl.},
  title        = {A convolutional neural network method to obtain the dynamic model parameters of a three-linear-axis cartesian robot},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multilayer perceptron ensembles in a truly sparse training context. <em>NCA</em>, <em>37</em>(20), 15419-15438. (<a href='https://doi.org/10.1007/s00521-025-11294-3'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Ensemble learning for artificial neural networks (ANNs) is an effective method to enhance predictive performance. However, ANNs are computationally and memory intensive, and naively training multiple networks can lead to excessive training times and costs. An effective tool for improving ensemble efficiency is introducing topological sparsity. Even though several implementations of efficient ensembles have been proposed, none of them can provide actual benefits in terms of computational overhead as the sparsity is simulated using binary masks. In this paper, we address this issue by introducing a Truly Sparse Ensemble without binary masks and directly incorporate native sparsity. We also propose two algorithms for initializing new subnetworks within the ensemble, leveraging this native topological sparsity to enhance subnetwork diversity. We demonstrate the performance of the resulting models at high levels of sparsity on several datasets in terms of classification accuracy, floating point operations (FLOPs), and actual running time. The proposed methods outperform all baseline dense and truly sparse models on tabular data, successfully diversify the training trajectory of the subnetworks, and increase the topological distance between subnetworks after re-initialization.},
  archive      = {J_NCA},
  author       = {van der Wal, Peter R. D. and Strisciuglio, Nicola and Azzopardi, George and Mocanu, Decebal Constatin},
  doi          = {10.1007/s00521-025-11294-3},
  journal      = {Neural Computing and Applications},
  month        = {7},
  number       = {20},
  pages        = {15419-15438},
  shortjournal = {Neural Comput. Appl.},
  title        = {Multilayer perceptron ensembles in a truly sparse training context},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Synthetic training data for crack propagation measurement by neural networks. <em>NCA</em>, <em>37</em>(20), 15389-15418. (<a href='https://doi.org/10.1007/s00521-025-11292-5'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Crack initiation and propagation are of central importance in experimental mechanics, structural assessment, and process analysis. However, conventional optical deformation analysis techniques have thus far yielded limited accuracy in capturing these phenomena and are generally not suitable for real-time control applications due to the high necessary computation time. This study investigates the application of a fully convolutional network (FCN), originally developed for biomedical image segmentation, to the task of detecting crack initiation and tracking propagation. Supervised deep learning approaches typically require extensive, pixel-level annotated datasets, which are time-consuming and prone to human error. To address this limitation, a method is introduced for generating synthetic image data using a physics-inspired crack propagation model combined with data augmentation techniques. The FCN is trained and validated entirely on the synthetic dataset. Its performance is then evaluated using real experimental data from a metal shearing test. Results demonstrate that the FCN can accurately identify both crack initiation and propagation. This deep learning-based approach shows strong potential to enhance existing optical deformation analysis methods, particularly in the context of material failure assessment or even in real-time process control systems.},
  archive      = {J_NCA},
  author       = {Hartmann, Christoph and Klauck, Sophia},
  doi          = {10.1007/s00521-025-11292-5},
  journal      = {Neural Computing and Applications},
  month        = {7},
  number       = {20},
  pages        = {15389-15418},
  shortjournal = {Neural Comput. Appl.},
  title        = {Synthetic training data for crack propagation measurement by neural networks},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Optimizing federated learning efficiency: Accelerating model averaging with cluster-based approach using class occurrences - FedCCO. <em>NCA</em>, <em>37</em>(20), 15369-15387. (<a href='https://doi.org/10.1007/s00521-025-11289-0'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the exploration of diverse clustering techniques-specifically KMeans, Hierarchical, and Mean Shift-within the context of federated learning, this study meticulously scrutinizes their influence on collaborative model training. Employing the MNIST dataset alongside a convolutional neural network (CNN), the research delves into the intricate dynamics of each clustering method in shaping the collaborative learning process. The findings reveal a nuanced understanding of how each clustering technique impacts the learning framework, shedding light on distinct variations in cluster performance. Through the adept utilization of a sophisticated weighted averaging approach, the study successfully achieves a harmonious balance in overall model performance, effectively mitigating the disparate effects observed among clusters. Furthermore, the research underscores the pivotal importance of considering computational efficiency in federated learning scenarios. Notably, Mean Shift emerges as a front-runner in this aspect, demonstrating commendable efficiency in model averaging and highlighting its potential as a preferred clustering method in certain contexts. In conclusion, this comprehensive examination underscores the critical significance of judicious clustering method selection in federated learning environments. The insights gleaned from this study not only enhance overall model performance but also propel advancements in computational efficiency, thereby paving the way for more robust and generalized outcomes in collaborative learning scenarios.},
  archive      = {J_NCA},
  author       = {Elahi, A. S. E. and Khanam, S. and Rahman, N. and Rabbi, R. I. and Bhowmik, A. and Karmaker, D.},
  doi          = {10.1007/s00521-025-11289-0},
  journal      = {Neural Computing and Applications},
  month        = {7},
  number       = {20},
  pages        = {15369-15387},
  shortjournal = {Neural Comput. Appl.},
  title        = {Optimizing federated learning efficiency: Accelerating model averaging with cluster-based approach using class occurrences - FedCCO},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Global cultural appropriation discourse on reddit: A topic modeling approach. <em>NCA</em>, <em>37</em>(20), 15345-15368. (<a href='https://doi.org/10.1007/s00521-025-11277-4'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cultural appropriation is a vast and complex domain spanning sociology, psychology, marketing, law, communication, and cultural studies. Based on more than 31,000 Reddit posts, this research aims to use computational text-mining techniques to analyze the major latent themes related to cultural appropriation. The study also visualizes the diachronic variation in sentiments over time. Results show that Reddit online communities focus on various topics related to cultural appropriation, including racism, cultural diversity, self-awareness, religion, and fashion appropriation. Sentiment and emotion analysis results reveal that although trust tops the list of emotions, there seem to be also some negative emotions such as fear, anger, and sadness. Analysis of the cultural appropriation emojis used by the Reddit community reveals that the face with tears of joy emoji is the most frequently used, followed by the rolling on the floor laughing (ROFL), the person shrugging, the loudly crying face, and the female sign ( ). Given the global reach of social media platforms, our research offers a novel perspective on understanding this phenomenon on Reddit as it captures the complex relationships between Reddit users’ comments and specific aspects of cultural appropriation.},
  archive      = {J_NCA},
  author       = {Mostafa, Mohamed M.},
  doi          = {10.1007/s00521-025-11277-4},
  journal      = {Neural Computing and Applications},
  month        = {7},
  number       = {20},
  pages        = {15345-15368},
  shortjournal = {Neural Comput. Appl.},
  title        = {Global cultural appropriation discourse on reddit: A topic modeling approach},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Integrating V2X solutions in intelligent green cities: An AI-driven point exchange system approach. <em>NCA</em>, <em>37</em>(20), 15311-15343. (<a href='https://doi.org/10.1007/s00521-025-11187-5'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With rapid urbanization, smart cities have become essential for enhancing urban management and sustainability by integrating technological, social, and institutional innovations. Among these innovations, vehicle-to-everything (V2X) communications and electric vehicles (EVs) play a critical role in reducing carbon emissions and optimizing urban mobility. To address the existing gaps in holistic V2X integration, this paper presents a novel eco-assistive fog-based traffic management system (EAFTMS) that leverages a four-tier architecture (IoT, fog, cloud, and application) for scalable, real-time traffic optimization. A key innovation of this system is the AI-driven point exchange system (PES), designed to incentivize sustainable behaviors such as reducing unnecessary vehicle usage and promoting green lifestyle choices. Unlike conventional models, the proposed framework incorporates real-time behavioral monitoring, rewards-based sustainability programs, and V2X-enabled dynamic traffic control. Empirical validation demonstrates that EAFTMS outperforms existing models, including support vector regression (SVR), achieving a 30% reduction in latency, a 40% improvement in response time, a 25% increase in traffic flow efficiency, and a 35% reduction in CO2 emissions. These results highlight the framework’s potential to set new standards in intelligent green cities by offering scalable, practical, and environmentally impactful solutions to urban transportation challenges.},
  archive      = {J_NCA},
  author       = {Talaat, Fatma M. and Shaban, Warda M. and ZainEldin, Hanaa and Badawy, Mahmoud and Elhosseini, Mostafa},
  doi          = {10.1007/s00521-025-11187-5},
  journal      = {Neural Computing and Applications},
  month        = {7},
  number       = {20},
  pages        = {15311-15343},
  shortjournal = {Neural Comput. Appl.},
  title        = {Integrating V2X solutions in intelligent green cities: An AI-driven point exchange system approach},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Enhancing schizophrenia diagnosis through deep learning: A resting-state fMRI approach. <em>NCA</em>, <em>37</em>(20), 15277-15309. (<a href='https://doi.org/10.1007/s00521-025-11184-8'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Schizophrenia (SC) is a complex mental disorder with diverse symptoms that make diagnosis challenging. This study aims to enhance diagnostic accuracy for SC using deep learning techniques applied to resting-state functional MRI (rsfMRI) data, capturing both spatial and temporal features of brain activity. We introduced a novel slice-wise classification approach using convolutional neural networks (CNNs) to analyze brain activity from rsfMRI images. Preprocessing included normalization, noise reduction, and contrast enhancement. The study utilized data from 158 subjects, including 83 schizophrenia patients and 75 healthy controls. We combined CNN-extracted features with traditional machine learning models such as support vector machines (SVM), random forest (RF), and gradient boosting (GB) to boost classification performance. Transfer learning using pre-trained models like VGG16, ResNet50, and Xception was applied to leverage advanced feature extraction. Model performance was evaluated based on precision, accuracy, recall, and F1 score. The CNN-based approach achieved significant improvements in classification accuracy, with a peak accuracy of 98.67%. The hybrid models combining CNN features with SVM, RF, and GB achieved accuracies of 97.01%, 98.44%, and 98.84%, respectively. The CNN model alone achieved a precision of 0.9826, recall of 1.0, and F1 score of 0.9912. Pre-trained models also demonstrated high performance, with ResNet50 achieving an accuracy of 98.71%. Brain regions such as the frontal lobe (slices 5 and 9) and temporal lobe (slices 15 and 25) were identified as key areas with significant differences between schizophrenia patients and healthy controls. This study demonstrates the effectiveness of deep learning models, particularly CNNs and hybrid approaches with traditional machine learning models, in enhancing schizophrenia diagnosis using rsfMRI data. Identifying key brain regions provides insights into schizophrenia’s neurobiological underpinnings. Future research should focus on larger datasets and interpretable models to advance diagnosis.},
  archive      = {J_NCA},
  author       = {Raeisi, Zahra and Mehrnia, Mehri and Ahmadi Lashaki, Reza and Abedi Lomer, Fatemeh},
  doi          = {10.1007/s00521-025-11184-8},
  journal      = {Neural Computing and Applications},
  month        = {7},
  number       = {20},
  pages        = {15277-15309},
  shortjournal = {Neural Comput. Appl.},
  title        = {Enhancing schizophrenia diagnosis through deep learning: A resting-state fMRI approach},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Egyptian motorcycle and vehicle license plate recognition. <em>NCA</em>, <em>37</em>(20), 15255-15276. (<a href='https://doi.org/10.1007/s00521-025-11302-6'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {License plate recognition is a pivotal technology in global vehicle surveillance systems, yet the development of effective systems for Egyptian license plates, particularly for motorcycles, has been constrained by the lack of a comprehensive dataset. Several Egyptian license plate recognition systems have been developed using deep learning models, image processing procedures, morphological operations, and other techniques, focusing exclusively on car license plates and not addressing the unique challenges of motorcycle license plate recognition. This paper presents a novel YOLOv8-based recognition system designed to address this significant research gap. The proposed system efficiently detects and tracks license plates in video streams, applies advanced image enhancement techniques, and leverages optical character recognition (OCR) to accurately extract alphanumeric characters. One of the key contributions of this work is the creation and annotation of a new dataset comprising over 1200 images of motorcycle license plates, specifically tailored for the YOLO format. The system demonstrates superior performance, achieving a 100% detection rate and an 80% recognition accuracy, surpassing existing algorithms in the field. The recognized license plates are systematically stored in a .csv file for future analysis, enhancing the system’s utility in real-world applications. This research sets a new benchmark for Egyptian license plate recognition, particularly in the underexplored domain of motorcycle plates, and provides a valuable dataset that can be utilized for further advancements in this area.},
  archive      = {J_NCA},
  author       = {Elattar, Tarteel Abdelfattah and Ahmed, Youssef Abdelrahman and Salem, Mohammed Abdel-Megeed and Afifi, Shereen Moataz},
  doi          = {10.1007/s00521-025-11302-6},
  journal      = {Neural Computing and Applications},
  month        = {7},
  number       = {20},
  pages        = {15255-15276},
  shortjournal = {Neural Comput. Appl.},
  title        = {Egyptian motorcycle and vehicle license plate recognition},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Advancing crop health with YOLOv11 classification of plant diseases. <em>NCA</em>, <em>37</em>(20), 15223-15253. (<a href='https://doi.org/10.1007/s00521-025-11287-2'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This study investigates the potential of deep learning and computer vision for automated plant disease detection and classification. This study presents a novel approach to plant disease detection and classification using a modified YOLOv11 architecture. The methodology incorporates a custom classification head, dynamic validation splitting, and comprehensive evaluation metrics to enhance performance. Leveraging two comprehensive datasets, the "Tomato Leaf Disease Dataset" and the "Plant Disease 2" dataset, we developed and evaluated a robust deep learning model. The "Tomato Leaf Disease Dataset" comprises 23,723 images across 11 classes, encompassing healthy tomato leaves and diseases like bacterial spots and early blight. The "Plant Disease 2" dataset consists of 2,904 images, focusing on diseases in beans, strawberries, and tomatoes, categorized into 12 classes. We meticulously preprocessed and augmented the datasets to enhance model generalization and performance. The deep learning model was rigorously trained and evaluated using key metrics such as accuracy, precision, recall, and F1-score. The model achieved high accuracy (97.88% and 96.90%, respectively) on both datasets. It demonstrated its effectiveness in recognizing a wide range of disease conditions with consistently high precision, recall, and F1 scores across various disease classes. This research underscores the transformative potential of deep learning in plant pathology, offering a rapid and accurate solution for disease identification. The ability to automate disease detection can significantly improve crop management strategies, leading to increased yields and reduced reliance on pesticides. This technology holds promise for enhancing agricultural sustainability and ensuring food security.},
  archive      = {J_NCA},
  author       = {Eliwa, Entesar Hamed I. and Abd El-Hafeez, Tarek},
  doi          = {10.1007/s00521-025-11287-2},
  journal      = {Neural Computing and Applications},
  month        = {7},
  number       = {20},
  pages        = {15223-15253},
  shortjournal = {Neural Comput. Appl.},
  title        = {Advancing crop health with YOLOv11 classification of plant diseases},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Veri-car: Towards open-world vehicle information retrieval. <em>NCA</em>, <em>37</em>(20), 15183-15221. (<a href='https://doi.org/10.1007/s00521-025-11257-8'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Many industrial and service sectors require tools to extract vehicle characteristics from images. This is a complex task not only by the variety of noise, and large number of classes, but also by the constant introduction of new vehicle models to the market. In this paper, we present Veri-Car, an information retrieval integrated approach designed to help with this task. It leverages supervised learning techniques to accurately identify the make, type, model, year, color, and license plate of cars. The approach also addresses the challenge of handling open-world problems, where new car models and variations frequently emerge, by employing a sophisticated combination of pre-trained models and a hierarchical multi-similarity loss. Veri-Car demonstrates robust performance, achieving high precision and accuracy in classifying both seen and unseen data. Additionally, it integrates an ensemble license plate detection and an OCR model to extract license plate numbers with impressive accuracy.},
  archive      = {J_NCA},
  author       = {Munoz, Andrés and Thomas, Nancy and Vapsi, Annita and Borrajo, Daniel},
  doi          = {10.1007/s00521-025-11257-8},
  journal      = {Neural Computing and Applications},
  month        = {7},
  number       = {20},
  pages        = {15183-15221},
  shortjournal = {Neural Comput. Appl.},
  title        = {Veri-car: Towards open-world vehicle information retrieval},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). BoostingCNNSOS hybrid model for house price forecasting based on SOS optimization. <em>NCA</em>, <em>37</em>(20), 15153-15181. (<a href='https://doi.org/10.1007/s00521-025-11198-2'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Housing price forecasting is a critical aspect of societal functioning as it has a profound impact on individuals, societal stability, and government management. Accurate predictions enable informed decision-making during house purchases. This study introduces a novel hybrid model called BoostingCNNSOS, which combines the strengths of convolutional neural network (CNN) and boosting algorithms to deliver highly accurate housing price forecasts with minimal errors. To further enhance the model’s performance, an optimized meta-heuristic method is employed for precise parameter adjustments. The analysis utilizes a comprehensive dataset of 998,299 sale transactions of residential units in Tehran from 2014 to 2021. This dataset includes 11 crucial features that are essential for accurate forecasting. Before the analysis, a series of preprocessing steps are undertaken to ensure the integrity and reliability of the data. The BoostingCNNSOS model’s effectiveness is evaluated by comparing it with established models such as XGBoost, CatBoost, LightGBM, and LSTM in terms of accuracy and error rates. Various evaluation criteria, including MSE, RMSE, MAE, ME, and R2, are employed. The consistent comparisons across all evaluation criteria demonstrate the stability and superior performance of the BoostingCNNSOS model. In Fold-1, the model achieves exceptional performance, with MSE, RMSE, MAE, ME, and R2 values of 0.0000025, 0.00158, 0.00046, 0.1364, and 0.88, respectively. These results highlight the accuracy and predictive capabilities of the model in housing price forecasting. The BoostingCNNSOS model continues to demonstrate superiority in Fold-2 to Fold-4. In contrast, the LSTM model shows weaker performance, while the XGBoost model showcases relatively good performance compared to other models. The findings of this research hold significant value for various stakeholders, including builders, planners, policymakers, researchers, and home buyers.},
  archive      = {J_NCA},
  author       = {Farahzadi, Mehdi and Farnoosh, Rahman and Behzadi, Mohammad Hassan},
  doi          = {10.1007/s00521-025-11198-2},
  journal      = {Neural Computing and Applications},
  month        = {7},
  number       = {20},
  pages        = {15153-15181},
  shortjournal = {Neural Comput. Appl.},
  title        = {BoostingCNNSOS hybrid model for house price forecasting based on SOS optimization},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). LTCR: Long-text chinese rumor detection dataset. <em>NCA</em>, <em>37</em>(20), 15133-15152. (<a href='https://doi.org/10.1007/s00521-025-11260-z'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {False information can spread quickly on social media, negatively influencing the citizens’ behaviors and responses to social events. To better detect all of the fake news, especially long texts which are harder to find completely, a Long-Text Chinese Rumor detection dataset named LTCR is proposed. The LTCR dataset provides a valuable resource for accurately detecting misinformation, especially in the context of complex fake news related to COVID-19. The dataset consists of 1729 and 561 pieces of real and fake news, respectively. The average lengths of real and fake news are approximately 230 and 152 characters. We also propose DoubleCheck, Salience-aware Fake News Detection Model, which achieves the highest accuracy (95.85%), fake news recall (90.91%) and F-score (90.60%) on the dataset ( https://github.com/Enderfga/DoubleCheck. ).},
  archive      = {J_NCA},
  author       = {Ma, Ziyang and Liu, Mengsha and Fang, Guian and Shen, Ying},
  doi          = {10.1007/s00521-025-11260-z},
  journal      = {Neural Computing and Applications},
  month        = {7},
  number       = {20},
  pages        = {15133-15152},
  shortjournal = {Neural Comput. Appl.},
  title        = {LTCR: Long-text chinese rumor detection dataset},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Self-supervised rotation-aware guided attentive distilled transformer network for crowd face detection. <em>NCA</em>, <em>37</em>(20), 15105-15131. (<a href='https://doi.org/10.1007/s00521-025-11214-5'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Accurate crowd face detection is vital in surveillance, security, and crowd management due to challenges like varying densities, occlusions, and scale variations. Resizing issues arise when traditional techniques fail to handle diverse face sizes and densities, alongside patterns or objects resembling faces, hindering accuracy. To address this, we propose the Self-Supervised Rotation-aware Guided Attentive Distilled Transformer Network (SSRGADTN) for crowd face detection. This method combines self-supervised learning and a rotation-aware distilled transformer network, effectively utilizing unlabeled data to learn robust representations and handle diverse crowd scenes. The workflow starts with self-supervised training on the FWL dataset, followed by integration with the Guided Attentive Detector Network (GADN) trained on the WIDERFACE dataset to enhance face detection, forming the SSRGADN (Self-Supervised Rotation-aware Guided Attentive Detector Network) model. Knowledge distillation further refines the model using a transformer network as the student model trained on the CrowdHuman dataset, resulting in the SSRGADTN model, which outperforms other state-of-the-art models in crowd face detection accuracy.},
  archive      = {J_NCA},
  author       = {Jayanthan, K. S and Domnic, S},
  doi          = {10.1007/s00521-025-11214-5},
  journal      = {Neural Computing and Applications},
  month        = {7},
  number       = {20},
  pages        = {15105-15131},
  shortjournal = {Neural Comput. Appl.},
  title        = {Self-supervised rotation-aware guided attentive distilled transformer network for crowd face detection},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Dynamic model selection for enhanced pivot-based neural machine translation. <em>NCA</em>, <em>37</em>(20), 15095-15104. (<a href='https://doi.org/10.1007/s00521-025-11213-6'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Pivot-based machine translation (PBMT) provides a viable solution for language pairs with limited or no parallel data. However, traditional PBMT systems often rely on fixed pivot languages and models, limiting their flexibility and performance. To address these limitations, we propose two novel frameworks that harness the growing availability of open-source multilingual machine translation (MMT) models. Our first framework, fixed pivot and dynamic translation model (FP-DTM), employs a fixed pivot language while dynamically selecting the most appropriate translation model for the target language from a pool of pre-trained MMT models. The second framework, dynamic pivot and translation model (DP-DTM), extends this flexibility by dynamically selecting both the pivot language and the translation model based on the specific language pair and available MMT models. Through extensive experiments, we demonstrate the effectiveness of both frameworks in improving translation quality, especially for low-resource language pairs. Our approach offers a promising direction for future research in PBMT, enabling more robust and adaptable translation systems.},
  archive      = {J_NCA},
  author       = {Narzary, Sanjib and Nandi, Sukumar and Som, Bidisha},
  doi          = {10.1007/s00521-025-11213-6},
  journal      = {Neural Computing and Applications},
  month        = {7},
  number       = {20},
  pages        = {15095-15104},
  shortjournal = {Neural Comput. Appl.},
  title        = {Dynamic model selection for enhanced pivot-based neural machine translation},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). ELinear: Ensemble linear model for stock prediction. <em>NCA</em>, <em>37</em>(20), 15079-15093. (<a href='https://doi.org/10.1007/s00521-025-11280-9'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This study proposes a novel ensemble network model called ELinear, which is based on the Linear model. This model utilizes mutual learning algorithms and channel-independent techniques to effectively reduce the error rates of Linear models in stock price prediction. We conducted comprehensive comparisons between ELinear and several state-of-the-art (SOTA) forecasting models using stock price data from companies in mainland China. The results show that ELinear achieved the lowest average prediction errors, with improvements ranging from 1.8 to 9.7% over SOTA models. Additionally, by comparing with basic models, we found that the Linear model has significant advantages in stock price prediction compared to mainstream LSTM models and newer Transformer models. Finally, based on the predictions made by ELinear, we performed validity analysis and profit assessment. The results indicate that the validity of the ELinear model’s predictions can be as high as 90%, and applying it to stock trading could achieve an average increase in returns of 9%.},
  archive      = {J_NCA},
  author       = {Guo, Yi and Guo, HongHong and Gu, DeFeng},
  doi          = {10.1007/s00521-025-11280-9},
  journal      = {Neural Computing and Applications},
  month        = {7},
  number       = {20},
  pages        = {15079-15093},
  shortjournal = {Neural Comput. Appl.},
  title        = {ELinear: Ensemble linear model for stock prediction},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A machining center selection study using neural networks. <em>NCA</em>, <em>37</em>(20), 15041-15078. (<a href='https://doi.org/10.1007/s00521-025-11224-3'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Machining centers (MCs) are crucial high-technology machine tools that are widely used in the manufacturing industry. Since their high investment necessities, the selection of the appropriate MC for a company is an important task so that the company’s competitive advantages can be continued in the future. In the literature, there are many studies for MC selection using multi-criteria decision-making (MCDM) models and their fuzzy versions. This type of selection model must use selection criteria weighting procedures that are mainly based on subjective preferences. On the other hand, some attempts are provided in the literature to set objective decision-making models, such as the design of an experiment (DoE)-based Technique for Order Preferences by Similarity to the Ideal Solution (TOPSIS) model. In this study, we improved the DoE-based TOPSIS approach using neural network-based solutions and compared their capability of the MC selection studies against the classical MCDM and DoE-TOPSIS models. The proposed model can handle the newly generated scenarios using the Neural Networks. The feedforward neural network (FFNN)-based model utilizes machine learning techniques to overcome some of the challenges by modeling the complex relationships among various decision criteria without relying on human judgment for weighting. As a form of artificial neural network, FFNNs learned from large datasets and captured non-linear patterns, making them well-suited for the MC selection problem. Their capability to process high-dimensional inputs with numerous variables and interactions aligns effectively with the complexities of the MC selection problem. Therefore, the proposed FFNN model is very competitive against the DoE-TOPSIS and classical TOPSIS models.},
  archive      = {J_NCA},
  author       = {Ic, Yusuf Tansel and Yıldız, Burak and Kececi, Barış and Sert, Mustafa},
  doi          = {10.1007/s00521-025-11224-3},
  journal      = {Neural Computing and Applications},
  month        = {7},
  number       = {20},
  pages        = {15041-15078},
  shortjournal = {Neural Comput. Appl.},
  title        = {A machining center selection study using neural networks},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Graph self-supervised long-tail item augmentation for recommendation. <em>NCA</em>, <em>37</em>(20), 15017-15039. (<a href='https://doi.org/10.1007/s00521-025-11279-2'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Self-supervised learning empowers models to learn from unlabeled data, making it a popular technique for addressing the challenge of insufficient labeled data. In this paper, we combine the issue of cold items lacking supervision signals due to the long-tail distribution in recommendation systems, which can lead to model degradation, and propose graph self-supervised long-tail item augmentation for recommendation (SLIA). We study the proxy task of distinguishing between popular items and cold items and employ various graph structure perturbation schemes on the user–item interaction graph (U–I graph) for contrastive learning. To be more specific, we propose a proportional edge dropping strategy for popular items. For cold items, while retaining their sparse interaction data, we introduce random perturbations to create a new enhanced U–I graph. Through the fine-grained construction of contrastive views, this scheme ensures a global perturbation of the original views and prevents the loss of information for cold items in the tail of the power-law distribution. Additionally, we propose a contrastive loss with a punishment mechanism to explicitly enhance the invariance of node representations. Through theoretical analysis, we find that SLIA is not only effective in mining hard negative samples but also reduces the occurrence of outliers in the representation distribution space. Experimental results on four benchmark datasets demonstrate that SLIA achieves competitive results in both supervised and self-supervised recommendation algorithms.},
  archive      = {J_NCA},
  author       = {Wen, Xilin and Yang, Xu-Hua and Long, Hai-Xia},
  doi          = {10.1007/s00521-025-11279-2},
  journal      = {Neural Computing and Applications},
  month        = {7},
  number       = {20},
  pages        = {15017-15039},
  shortjournal = {Neural Comput. Appl.},
  title        = {Graph self-supervised long-tail item augmentation for recommendation},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Stochastic DNN-based models meet hidden markov models: A challenge on natural gas prices at the henry hub. <em>NCA</em>, <em>37</em>(20), 14997-15016. (<a href='https://doi.org/10.1007/s00521-025-11286-3'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a new methodology for building stochastic models of commodity prices based on the probabilistic predictions of a suitably designed and trained deep neural network (DNN). Our method directly generates probabilistic paths, capturing the main features of market price dynamics such as mean-reversion and extreme movements without requiring complex parameter estimation techniques. To reproduce the main stylized facts of the commodity price dynamics, a suitable DNN architecture and an innovative training procedure are proposed. Our findings show that our DNN-based approach is highly effective in capturing mean-reversion effects, extreme price movements, and other nonlinearities of the dynamics. A comparison is proposed on natural gas market prices observed at the Henry Hub between the DNN-based model and a suitable regime-switching hidden Markov model. Comparative experiments show that the DNN-based model provides similar or superior performance in terms of statistical accuracy and predictive reliability.},
  archive      = {J_NCA},
  author       = {Mari, Carlo and Mari, Emiliano},
  doi          = {10.1007/s00521-025-11286-3},
  journal      = {Neural Computing and Applications},
  month        = {7},
  number       = {20},
  pages        = {14997-15016},
  shortjournal = {Neural Comput. Appl.},
  title        = {Stochastic DNN-based models meet hidden markov models: A challenge on natural gas prices at the henry hub},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). EEG-based motor imagery recognition via novel explainable ensemble learning architecture. <em>NCA</em>, <em>37</em>(20), 14971-14995. (<a href='https://doi.org/10.1007/s00521-025-11300-8'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Brain–computer interfaces (BCIs) are interactive machines using implicit neurophysiological signals, with applications ranging from medical rehabilitation to smart prostheses and entertainment. In this context, the need for high recognition performance demands increasingly complex machine learning (ML) architectures. Generally, the more complex the architecture, the less transparent its reasoning. This leads to difficulties in motivating their outputs and validating their internal model. Moreover, explainability is explicitly required by recent regulations on personal data processing, which advise against black box modeling. Here, a novel ensemble learning model is proposed aiming to effectively balance recognition performances and explainability. The proposed architecture employs different multilayer perceptrons, each one specialized to distinguish a single pair of classes and to provide counterfactual explanations and the minimal feature changes resulting in a classification shift. Subsequently, their outcomes are weighted to minimize the contribution of the non-competent classifiers and combined to address a multiclass classification problem. Results were gathered from two publicly available datasets on multiclass electroencephalography-based motor imagery and demonstrate that the proposed architecture overcomes state-of-the-art recognition performance while providing information on the most discriminant brain areas and power bands. For the sake of reproducibility, the implementation of the proposed approach is made publicly available.},
  archive      = {J_NCA},
  author       = {Alfeo, Antonio L. and Catrambone, Vincenzo and Cimino, Mario G. C. A. and Valenza, Gaetano},
  doi          = {10.1007/s00521-025-11300-8},
  journal      = {Neural Computing and Applications},
  month        = {7},
  number       = {20},
  pages        = {14971-14995},
  shortjournal = {Neural Comput. Appl.},
  title        = {EEG-based motor imagery recognition via novel explainable ensemble learning architecture},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Evaluation and analysis of visual methods for CNN explainability: A novel approach and experimental study. <em>NCA</em>, <em>37</em>(20), 14935-14970. (<a href='https://doi.org/10.1007/s00521-025-11282-7'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The rapid development and widespread adoption of deep neural networks (DNNs) are attributed to their remarkable performance and practical efficacy across numerous applications. Nevertheless, the sophistication of the inference process often comes at the expense of explainability, becoming an increasingly critical concern. Explainable AI endeavors to shed light on the inner workings of neural networks, thereby enhancing trust in their predictive capabilities and facilitating more reliable and informed decision-making. Visual explanations for the behavior of DNNs contribute to understanding their functioning. However, selecting the right visualization method for a particular task and evaluating its performance remains a challenge. This paper offers a comprehensive analysis of post hoc methods for generating visual explanations, concentrating on class activation mapping (CAM) techniques. It introduces two novel approaches: high-resolution relevance propagation CAM (HiResRP-CAM) and EigenLayer-CAM. These methods are evaluated both qualitatively and quantitatively across various experimental setups within the context of convolutional neural networks (CNNs) for image classification tasks. The evaluation encompasses diverse network architectures, datasets, and metrics, including certain proposed enhancements. The paper discusses experimental results, including instances of model failure due to adversarial noise, and explores factors to deepen understanding of model behavior, characteristics of explainability methods and current strategies for assessing explanation quality. In this way, we provide a perspective on the challenges and limitations inherent in interpreting CNNs and on the relevant aspects for choosing a specific method.},
  archive      = {J_NCA},
  author       = {Dugăeșescu, Andrei and Florea, Adina Magda},
  doi          = {10.1007/s00521-025-11282-7},
  journal      = {Neural Computing and Applications},
  month        = {7},
  number       = {20},
  pages        = {14935-14970},
  shortjournal = {Neural Comput. Appl.},
  title        = {Evaluation and analysis of visual methods for CNN explainability: A novel approach and experimental study},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). GY-YOLO: Ghost separable YOLO for pedestrian detection. <em>NCA</em>, <em>37</em>(20), 14907-14933. (<a href='https://doi.org/10.1007/s00521-025-11207-4'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, there has been impressive development in human detection. The main challenge in pedestrian detection is the training data. To assess detectors in crowd scenarios more effectively, a novel dataset in this study called the HEP dataset (Hybrid Egyptian Pedestrian dataset) is introduced. The HEP dataset is extensive, has comprehensive annotations, and is highly diverse. The dataset images are collected by two different means. Most of the images are collected from different mobile cameras for people crossing the street in high crowded streets in Egypt, and the rest of the images are collected from the web. That is why the dataset is called hybrid. The collected dataset is more suitable for pedestrian detection as the whole images focus on pedestrian scenarios for people outdoors crossing the street. This outperforms the previous benchmarks such as CrowdHuman and WiderPerson which collect data from the web and surveillance cameras with lots of images for indoor people. GS-YOLO also is proposed to address the real-time performance and the occlusion in the crowd scenes issues. GS-YOLO is a novel pedestrian detection model that utilizes efficient Ghost and depth separable convolution modules. GS-YOLO replaces all the convolution layers in the backbone and the head of the original YOLOv8 with Ghost and depth separable modules, respectively. A deformable to-features module is proposed to enrich features for the different feature pyramid networks. GS-YOLO is trained and tested over the collected dataset and other benchmarks like CrowdHuman and WiderPerson datasets. GS-YOLO achieves competitive results over the state-of-the-art models such as YOLOv5 and YOLOv8. GS-YOLO achieves 92.8% mAp on the HEP dataset, while YOLOv5 achieves 90.3% mAp and YOLOv8 achieves 91.1% mAp.},
  archive      = {J_NCA},
  author       = {Elhenidy, Ali M. and Labib, Labib M. and Haikal, Amira Y. and Saafan, Mahmoud M.},
  doi          = {10.1007/s00521-025-11207-4},
  journal      = {Neural Computing and Applications},
  month        = {7},
  number       = {20},
  pages        = {14907-14933},
  shortjournal = {Neural Comput. Appl.},
  title        = {GY-YOLO: Ghost separable YOLO for pedestrian detection},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Gated attention unit and mask attention network for traffic flow forecasting. <em>NCA</em>, <em>37</em>(20), 14889-14905. (<a href='https://doi.org/10.1007/s00521-025-11378-0'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Short-term traffic prediction plays a pivotal role in urban traffic management. However, the complexity and nonlinearity of traffic flow pose challenges to traditional methods. Fortunately, the research on short-term traffic prediction using deep learning neural networks has seen rapid growth. In this study, I propose a deep learning framework called GAMAN, which combines masked attention and Gate Attention Unit (GAU). Unlike traditional convolutional kernels that can only capture features from nearby time points, GAU can be divided into two parallel modules. The Gated Linear Units (GLU) module utilizes one-dimensional convolution to extract relationships between closely spaced time points. This compact convolutional kernel aids in parameter efficiency and enhances training speed. On the other hand, the primary function of the GAU module is to establish relationships between distant time points. Consequently, GAU comprehensively and intricately constructs temporal feature relationships at different scales without the need for deep layers as in traditional convolutions or sacrificing parallelism as in RNNs. Masked attention is employed to handle the aggregation of spatial information, introducing actual spatial information from adjacency matrices on top of the existing global attention mechanism. Extensive experiments on two real-world public transportation datasets demonstrate that GAMAN exhibits high performance and competitive computational efficiency. Furthermore, the attention mechanism in both the temporal and spatial modules enhances interpretability. The project page is https://github.com/hybridIce/GAMAN.git .},
  archive      = {J_NCA},
  author       = {Leng, Sen},
  doi          = {10.1007/s00521-025-11378-0},
  journal      = {Neural Computing and Applications},
  month        = {7},
  number       = {20},
  pages        = {14889-14905},
  shortjournal = {Neural Comput. Appl.},
  title        = {Gated attention unit and mask attention network for traffic flow forecasting},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Short-term multi-step-ahead sector-based traffic flow prediction based on the attention-enhanced graph convolutional LSTM network (AGC-LSTM). <em>NCA</em>, <em>37</em>(20), 14869-14888. (<a href='https://doi.org/10.1007/s00521-024-09827-3'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Accurate sector-based air traffic flow predictions are essential for ensuring the safety and efficiency of the air traffic management (ATM) system. However, due to the inherent spatial and temporal dependencies of air traffic flow, it is still a challenging problem. To solve this problem, some methods are proposed considering the relationship between sectors, while the complicated spatiotemporal dynamics and interdependencies between traffic flow of route segments related to the sector are not taken into account. To address this challenge, the attention-enhanced graph convolutional long short-term memory network (AGC-LSTM) model is applied to improve the short-term sector-based traffic flow prediction, in which spatial structures of route segments related to the sector are considered for the first time. Specifically, the graph convolutional networks (GCN)-LSTM network model was employed to capture spatiotemporal dependencies of the flight data, and the attention mechanism is designed to concentrate on the informative features from key nodes at each layer of the AGC-LSTM model. The proposed model is evaluated through a case study of the typical enroute sector in the central–southern region of China. The prediction results show that MAE reduces by 14.4% compared to the best performing GCN-LSTM model among the other five models. Furthermore, the study involves comparative analyses to assess the influence of route segment range, input and output sequence lengths, and time granularities on prediction performance. This study helps air traffic managers predict flight situations more accurately and avoid implementing overly conservative or excessively aggressive flow management measures for the sectors.},
  archive      = {J_NCA},
  author       = {Zhang, Ying and Xu, Shimin and Zhang, Linghui and Jiang, Weiwei and Alam, Sameer and Xue, Dabin},
  doi          = {10.1007/s00521-024-09827-3},
  journal      = {Neural Computing and Applications},
  month        = {7},
  number       = {20},
  pages        = {14869-14888},
  shortjournal = {Neural Comput. Appl.},
  title        = {Short-term multi-step-ahead sector-based traffic flow prediction based on the attention-enhanced graph convolutional LSTM network (AGC-LSTM)},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A framework for object tracking in videos with complex backgrounds and obstructions. <em>NCA</em>, <em>37</em>(20), 14849-14868. (<a href='https://doi.org/10.1007/s00521-024-10853-4'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The research of object tracking in videos utilizes computer vision and machine learning techniques to identify and track objects in the consecutive image frames of videos. The popular algorithms used in the research are YOLO, DeepSORT, StrongSORT, and ByteTrack. YOLO has gained recognition for its hardware efficiency and impressive performance in object detection tasks. It efficiently detects objects in the consecutive image frames of videos. On the other hand, DeepSORT, StrongSORT, and ByteTrack leverage the prediction capabilities to estimate the positions of objects in the next image frames. These algorithms play a significant role to enhance the capabilities of object tracking systems. Recently, many researchers have proposed their methods to enhance the accuracy in object tracking tasks. However, the division of videos into the consecutive image frames for tracking objects can pose challenges, especially when the videos have complex backgrounds and many obstructions in the image frames. In fact, the complex backgrounds with many obstructions in image frames decrease the accuracy of tracking objects in applications. In this study, we present TraObs, a framework for object Tracking in complex backgrounds with Obstructions in the image frames of videos. In TraObs, two novel mechanisms CFM and M-controller are proposed for detectors and trackers to enhance the accuracy of object tracking performance. CFM addresses the tracked objects which are missing in some image frames of videos, while M-controller makes sure the tracked objects with the correct object ids in the system. Some experiments with four testing datasets and benchmark datasets are designed and used respectively to analyze the performance of TraObs. Furthermore, the HOTA and MOTA metrics are used to evaluate our proposed methods. The experimental results also prove the effectiveness of CFM and M-controller in our framework TraObs.},
  archive      = {J_NCA},
  author       = {Chang, Tsui-Ping and Chen, Tzer-Long and Hsiao, Tsung-Chih},
  doi          = {10.1007/s00521-024-10853-4},
  journal      = {Neural Computing and Applications},
  month        = {7},
  number       = {20},
  pages        = {14849-14868},
  shortjournal = {Neural Comput. Appl.},
  title        = {A framework for object tracking in videos with complex backgrounds and obstructions},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Intercity round-trip multi-region demand prediction based on multi-task fusion recurrent graph attention network. <em>NCA</em>, <em>37</em>(20), 14829-14848. (<a href='https://doi.org/10.1007/s00521-024-10672-7'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Existing multi-region prediction models are mainly confined to single cities and fail to fully capture the complex spatial-temporal correlations between cities. This study formally describes the problem of predicting multi-region round-trip demand between two cities and proposes a multi-task fusion recurrent graph attention network (MT-FRGAN). Firstly, this model utilizes the term frequency-inverse document frequency (TF-IDF) algorithm to measure the differences between regions and performs feature selection. Secondly, we employ a multi-head graph attention network (MGAT) to learn heterogeneous spatial correlations between adjacent regions within cities and related regions between cities, and combine it with the recurrent graph attention network (RGAN) to capture intercity multi-region temporal correlations. Lastly, we conducted multi-modal feature fusion and devised a multi-task learning mechanism to forecast demand across various types of regions. Experimental results on a dataset of round trip between Anxi County and Xiamen City demonstrate that the MT-FRGAN outperforms current single-city multi-region prediction methods by approximately 13%. Furthermore, respective ablation experiments validated the effectiveness of different components proposed in the method.},
  archive      = {J_NCA},
  author       = {Dai, Zezhong and Wang, Cheng and Hu, Die and Chen, Jianwei and Fu, Shunkai},
  doi          = {10.1007/s00521-024-10672-7},
  journal      = {Neural Computing and Applications},
  month        = {7},
  number       = {20},
  pages        = {14829-14848},
  shortjournal = {Neural Comput. Appl.},
  title        = {Intercity round-trip multi-region demand prediction based on multi-task fusion recurrent graph attention network},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Dear: Vehicle mobility prediction using diffusion-expanded attention network based on IoV trajectory data. <em>NCA</em>, <em>37</em>(20), 14813-14827. (<a href='https://doi.org/10.1007/s00521-024-10641-0'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, the rapid development of vehicle networking technology makes the acquisition of Internet of vehicles (IoV) trajectory data easier and more extensive. Mobility prediction based on these IoV trajectory data has become one of the research hotspots in the field of transportation. However, due to the randomness and nonlinear characteristics of IoV trajectory data, vehicle mobility prediction is a challenging task. In addition to traditional statistical methods, neural networks are promising methods for predicting vehicle mobility. Therefore, we design a deep neural network model and introduce the attention mechanism to achieve accurate prediction of vehicle mobility by learning and modeling the timing characteristics of IoV trajectory data. Diffusion-Expanded Attention Recurrent Neural Network (DEAR), a position prediction model for preference perception, is proposed. The introduction of diffusion model can better consider the dynamics of vehicle movement, adapt to the ever-changing traffic environment, and improve the accuracy of trajectory prediction. The experimental results show that our method can accurately predict the moving behavior of vehicles such as position, speed, and acceleration, which provides important support for real-time traffic flow optimization and driving safety, and has a wide application prospect.},
  archive      = {J_NCA},
  author       = {Yang, Jiali and Yang, Kehua and Zeng, Fanzi and Cheng, Qixuan and Xiao, Zhu and Jiang, Hongbo},
  doi          = {10.1007/s00521-024-10641-0},
  journal      = {Neural Computing and Applications},
  month        = {7},
  number       = {20},
  pages        = {14813-14827},
  shortjournal = {Neural Comput. Appl.},
  title        = {Dear: Vehicle mobility prediction using diffusion-expanded attention network based on IoV trajectory data},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). NWSTAN: A lightweight dynamic spatial–temporal attention network for traffic prediction. <em>NCA</em>, <em>37</em>(20), 14797-14812. (<a href='https://doi.org/10.1007/s00521-024-10638-9'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Advanced deep learning technology has promoted the development of high-accuracy traffic prediction algorithms. However, existing algorithms struggle to balance model size and predictive accuracy, lack the ability to model the spatial correlation effectively, and require many parameters, resulting in a significantly larger model size. To overcome these challenges, this paper introduces a new solution, the lightweight Nodewalk Spatial–Temporal Attention Network (NWSTAN). NWSTAN uses an improved lightweight nodewalk algorithm based on Node2vec to model the correlations between sensors on different roads dynamically. Additionally, it employs a spatiotemporal feature extraction module that leverages the attention mechanism to extract relevant features from the traffic flow data. Experiments conducted on two real-world datasets show that NWSTAN reduced the mean absolute error (MAE) by 3-5% compared to the baseline model while significantly reducing the number of parameters (by at least 27%). This makes NWSTAN easier to integrate into existing edge computing devices, broadening its potential for real-world applications.},
  archive      = {J_NCA},
  author       = {Sun, Jingru and Zhang, Yao and Qiu, Ziyu and Cheng, Qixuan and Xiao, Zhu},
  doi          = {10.1007/s00521-024-10638-9},
  journal      = {Neural Computing and Applications},
  month        = {7},
  number       = {20},
  pages        = {14797-14812},
  shortjournal = {Neural Comput. Appl.},
  title        = {NWSTAN: A lightweight dynamic spatial–temporal attention network for traffic prediction},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). STADGCN: Spatial–temporal adaptive dynamic graph convolutional network for traffic flow prediction. <em>NCA</em>, <em>37</em>(20), 14783-14796. (<a href='https://doi.org/10.1007/s00521-024-10606-3'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Traffic prediction plays a crucial role in intelligent transportation systems. However, the complexity of traffic data, stemming from a mix of local and global spatial–temporal correlations, poses significant challenges for accurate traffic flow prediction. Most recurrent neural networks or convolutional neural networks struggle to effectively capture temporal dependencies, especially long-term ones. Additionally, existing graph neural networks typically use static graph structures to capture spatial dependencies, often overlooking the dynamic patterns inherent in traffic networks. In this paper, we propose a novel spatial–temporal adaptive dynamic graph convolutional network for traffic flow prediction. This network incorporates a Gated Temporal Convolution Network (Gated TCN) that uses dilated causal convolutions at different granular levels to capture temporal dependencies in traffic flow. Additionally, we designed a spatial static–dynamic graph learning layer, which integrates static adaptive graph learning, dynamic graph learning, and a spatial gated fusion module to synchronously capture dynamic spatial–temporal features from historical traffic data. Experiments conducted on two real-world traffic datasets demonstrate that the proposed model effectively extracts dynamic spatial–temporal features and significantly outperforms popular baseline methods in prediction accuracy.},
  archive      = {J_NCA},
  author       = {Shi, Ying and Cui, Wentian and Wang, Ruiqin and Lou, Jungang and Shen, Qing},
  doi          = {10.1007/s00521-024-10606-3},
  journal      = {Neural Computing and Applications},
  month        = {7},
  number       = {20},
  pages        = {14783-14796},
  shortjournal = {Neural Comput. Appl.},
  title        = {STADGCN: Spatial–temporal adaptive dynamic graph convolutional network for traffic flow prediction},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Trust-aware task offloading for cost-effective UAV-based edge computing based on reinforcement learning. <em>NCA</em>, <em>37</em>(20), 14765-14782. (<a href='https://doi.org/10.1007/s00521-024-10593-5'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the development of drone technology, unmanned aerial vehicle (UAV)-based edge computing has emerged as a promising computing paradigm with broad application prospects. However, the limited endurance of UAVs makes UAV-based edge computing more sensitive to the energy consumption and time delay. In addition, open network environment will lead to various potential adversarial attacks targeting information security, such as location privacy leakage, black hole attacks, etc. To combat these challenges, this paper proposes a trust-aware task offloading framework for cost-effective UAV-based edge computing. Firstly, a trust framework to evaluate the level of information security of the devices is proposed. Reinforcement learning algorithms are employed to optimize the trust values. Secondly, a multi-user edge computing model and cost function are established, transforming the multi-objective optimization problem into a single-objective nonlinear programming problem. Thirdly, a cost optimization model is developed to assess the cost-effectiveness of IoT devices in the UAV edge computing system with the constraints of trust values. Additionally, to address convergence issues and local optima challenges, the Monte Carlo Q-learning algorithm is exercised to ascertain the optimal cost. Finally, the simulation results show that the average cost of this scheme is at least 25.22% lower than other schemes in terms of the number of IoT devices, 6.68% lower than other schemes in terms of the CPU frequency of UAV edge computing server, and 18.21% lower than other schemes in terms of the average size of tasks.},
  archive      = {J_NCA},
  author       = {Liu, Jianhua and Xie, Peng and Lin, Kemeng and Tu, Xiaoguang and Fan, Rong},
  doi          = {10.1007/s00521-024-10593-5},
  journal      = {Neural Computing and Applications},
  month        = {7},
  number       = {20},
  pages        = {14765-14782},
  shortjournal = {Neural Comput. Appl.},
  title        = {Trust-aware task offloading for cost-effective UAV-based edge computing based on reinforcement learning},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). PSTCGCN: Principal spatio-temporal causal graph convolutional network for traffic flow prediction. <em>NCA</em>, <em>37</em>(20), 14751-14764. (<a href='https://doi.org/10.1007/s00521-024-10591-7'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Traffic prediction is indispensable for constructing transportation networks in smart cities. Due to the complex spatio-temporal correlations of traffic data, this task presents challenges. Recent studies mainly use graph neural networks to simulate complex spatio-temporal relationships through fixed or adaptive graphs. While fixed graphs may not adapt to data drift caused by changes in road network structures, adaptive graphs overlook critical information of the original roads. To address this challenge, we propose a principal spatio-temporal causal graph convolutional network (PSTCGCN) to accurately predict traffic flow. In response to the data drift problem, we introduce a data-driven semi-principal generated graph embedding (SPGGE) that first extracts the principal features of the original roads to model the spatio-temporal sequence distribution and then remodels the data after drift through adaptive transformation. Traffic flow data, while showcasing fundamental spatial relationships, also exhibit temporal dynamics. We propose an effective temporal causal convolution component comprising SPGGE, graph convolution, both local and global causal learning models to jointly learn short-term and long-term spatio-temporal correlations. PSTCGCN was evaluated using two actual highway datasets, PEMS03 and PEMS07, achieving a notable improvement of 6.12% in RMSE on PEMS03 compared to STGATRGN. Our code is available at https://github.com/OvOYu/PSTCGCN .},
  archive      = {J_NCA},
  author       = {Yang, Shiyu and Wu, Qunyong and Li, Ziwei and Wang, Keyue},
  doi          = {10.1007/s00521-024-10591-7},
  journal      = {Neural Computing and Applications},
  month        = {7},
  number       = {20},
  pages        = {14751-14764},
  shortjournal = {Neural Comput. Appl.},
  title        = {PSTCGCN: Principal spatio-temporal causal graph convolutional network for traffic flow prediction},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Spatiotemporal attention based multi-graph convolutional network for passenger congestion delay short-term prediction. <em>NCA</em>, <em>37</em>(20), 14733-14749. (<a href='https://doi.org/10.1007/s00521-024-10562-y'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The development of bus rapid transit (BRT) is currently in a rising phase, and at the same time, the demand for resident travel is continuously increasing. During peak periods, a large volume of commuter traffic converges at stations in a short time. Due to the limited carrying capacity of vehicles, some passengers are forced to remain on platforms waiting for the next or even more buses. This delays the overall travel time and causes crowding. Precisely predicting the passenger congestion in advance has become an urgent problem to solve. Addressing the issue that existing studies do not adequately consider spatial correlations, temporal correlations, and external factors, which leads to low prediction accuracy, a spatiotemporal attention multi-graph convolutional network (STA-MGCN) is proposed for short-term forecasting of passenger congestion delays. By establishing a spatiotemporal attention mechanism, the model adjusts the input data to effectively capture the dynamic spatiotemporal correlations in traffic data. It encodes spatial and semantic correlations and models them as multi-location graphs, which are then fused through a multi-graph convolutional network. A bidirectional recurrent layer module considers the backward and forward states of congestion delay time series, ultimately fusing multi-source data to generate the final prediction results. Taking the Xiamen BRT network and Xiamen subway network passenger congestion delay data as examples, experimental results show that the proposed STA-MGCN method has higher prediction accuracy compared to baseline models.},
  archive      = {J_NCA},
  author       = {Wang, Cheng and Fang, Yipeng and Li, Xinyi and Su, Mingxian},
  doi          = {10.1007/s00521-024-10562-y},
  journal      = {Neural Computing and Applications},
  month        = {7},
  number       = {20},
  pages        = {14733-14749},
  shortjournal = {Neural Comput. Appl.},
  title        = {Spatiotemporal attention based multi-graph convolutional network for passenger congestion delay short-term prediction},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Improved adaptive graph local spatial–temporal multi-head self-attention network: A deep learning framework for flight delay prediction. <em>NCA</em>, <em>37</em>(20), 14723-14732. (<a href='https://doi.org/10.1007/s00521-024-10559-7'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Flight delay prediction is a challenging problem due to the sophisticated spatial–temporal correlations between airports and difficult-to-fit delay patterns. This paper proposed an improved adaptive graph local spatial–temporal multi-head self-attention network to predict flight arrival punctuality. First, an adaptive graph can learn about changing airport spatial–temporal correlations and capture their hidden spatial relationships. After that, local spatial–temporal multi-head self-attention was used to capture the flight delay’s spatial–temporal feature, and the attention receptive field was set to the spatially neighboring nodes to reduce the time complexity. Finally, dilated inception convolution was used to merge more features in input data to improve long-term forecasting performance. A case study of the flight arrival punctuality of 50 airports in the USA from 2009 to 2019 validated this model’s feasibility, which outperformed other models in predicting long-term flight delays.},
  archive      = {J_NCA},
  author       = {Wei, Ming and Xu, Ziqing and Gao, Ruifeng},
  doi          = {10.1007/s00521-024-10559-7},
  journal      = {Neural Computing and Applications},
  month        = {7},
  number       = {20},
  pages        = {14723-14732},
  shortjournal = {Neural Comput. Appl.},
  title        = {Improved adaptive graph local spatial–temporal multi-head self-attention network: A deep learning framework for flight delay prediction},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Research on highway traffic flow prediction based on a hybrid model of ARIMA–GWO–LSTM. <em>NCA</em>, <em>37</em>(20), 14703-14722. (<a href='https://doi.org/10.1007/s00521-024-10550-2'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As an important part of the transportation industry, highway transportation, compared with other modes of transportation, has the advantages of high flexibility, wide adaptability and wide coverage. Traditional time series-based traffic forecasting is difficult to solve the problem of nonlinear and non-stationary characteristics. To solve these problems, a highway traffic forecasting method based on ARIMA–GWO–LSTM combination model is proposed. Firstly, the preliminary prediction of the UK M25 highway traffic using the Autoregressive Integrated Moving Average Model (ARIMA) model obtains linear prediction results. Then, a highway traffic prediction model based on the Long Short-Term Memory Network (LSTM) is established, and automatic optimization of the LSTM hyperparameters is achieved through the introduction of the Gray Wolf Optimizer (GWO) into the traditional LSTM model, and the residuals are corrected using the LSTM model to obtain the nonlinear prediction results; the model finally combines the linear and nonlinear prediction results to obtain the predicted value of the traffic flow of the UK highway. Finally, with the help of the prediction error indexes, such as MAE, RMES, MAPE, and R2, the hybrid model is compared and analyzed with the LSTM, ARIMA, SVR, ARIMA-SVR, and ARIMA-LSTM models. The results show that the ARIMA–GWO–LSTM model reduces the MAE by 77.66%, the RMSE by 75.57%, the MAPE by 75.06%, and the R2 is closer to 1 than the single model. Verifying that the hybrid model cannot only extract long-term dependencies of data, but also effectively capture linear and nonlinear features of data, thus achieving more accurate prediction.},
  archive      = {J_NCA},
  author       = {Ma, Changxi and Gu, Keyan and Zhao, Yongpeng and Wang, Tao},
  doi          = {10.1007/s00521-024-10550-2},
  journal      = {Neural Computing and Applications},
  month        = {7},
  number       = {20},
  pages        = {14703-14722},
  shortjournal = {Neural Comput. Appl.},
  title        = {Research on highway traffic flow prediction based on a hybrid model of ARIMA–GWO–LSTM},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). ASTNAT: An attention-based spatial–temporal non-autoregressive transformer network for vehicle trajectory prediction. <em>NCA</em>, <em>37</em>(20), 14687-14702. (<a href='https://doi.org/10.1007/s00521-024-10548-w'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Accurate vehicle trajectory prediction is a fundamental prerequisite for downstream tasks like safety analysis and trajectory planning. Despite extensive research efforts for trajectory prediction, challenges still persist in fusing driving intentions and deeply exploring interaction features among vehicles in complex traffic environments. In this paper, an Attention-based Spatial–Temporal Non-Autoregressive Transformer (ASTNAT) network to capture complex features in historical information for vehicle trajectory prediction is proposed. To begin, we present the Historical Trajectory Time Encoder (HTTE) module, designed to capture the dependencies of different timestamps of vehicles. Secondly, interaction features between the target vehicle and surrounding vehicles at timestamp $$t$$ are captured by the Spatial Social Interaction (SSI) module, while the Spatial–Temporal Social Dependence (STSD) module captures interaction features across different timestamps. Thirdly, the Driving Intention Feature Fusion (DIFF) module is used to capture driving intention features. Finally, in the Decoder Output module, future trajectories are generated all at once in a non-autoregressive manner, avoiding the impact of error accumulation in single timestamp iterative prediction output. Experimental results on the NGSIM and HighD datasets indicate that compared to state-of-the-art models, the proposed model exhibits better performance. The Root Mean Square Error (RMSE) of the prediction trajectory at 5 s time horizon is 3.23 m on NGSIM dataset and 1.31 m on the HighD dataset. Furthermore, ablation experiments are conducted to evaluate the performance of each module.},
  archive      = {J_NCA},
  author       = {Zhang, Xingrong and Lyu, Hao and Cheng, Rongjun},
  doi          = {10.1007/s00521-024-10548-w},
  journal      = {Neural Computing and Applications},
  month        = {7},
  number       = {20},
  pages        = {14687-14702},
  shortjournal = {Neural Comput. Appl.},
  title        = {ASTNAT: An attention-based spatial–temporal non-autoregressive transformer network for vehicle trajectory prediction},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Deep learning model for efficient traffic forecasting in intelligent transportation systems. <em>NCA</em>, <em>37</em>(20), 14673-14686. (<a href='https://doi.org/10.1007/s00521-024-10537-z'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, a novel intelligent transportation framework called graph convolutional neural network for distributed machine learning (GCN-DML) is presented. Its primary application is traffic forecasting in large-scale, dynamically-evolving traffic scenarios. One unique method in traffic forecasting, called GCN-DML, is that it allows user data to be processed locally on edge devices, removing the requirement to send all data to CS (CS). Designed specifically for edge computing, this distributed machine learning framework makes it easier to train neural networks on a variety of edge devices. In addition to protecting user privacy, this method reduces the difficulties related to centralised neural network training, minimises communication delays, lowers data transfer quantities, and improves overall data processing efficiency. GCN-LSTM, the composite graph convolutional networks used in this framework, combine edge-enhanced attention mechanisms with the feature transmission capabilities of graph convolutional neural networks to effectively handle the challenging task of traffic forecasting. Even as the number of cars in the network rises, they perform exceptionally well at quickly extracting and sending interaction information between vehicles, while preserving high prediction accuracy and low time complexity. The versatility of GCN-DML is extended to a range of traffic forecasting application scenarios, whereby edge devices can adjust the kind and scale of the neural network according to available compute and storage resources, all without sacrificing performance. The usefulness of GCN-DML in traffic forecasting is demonstrated by experimental evaluations on the NGSIM public dataset, where it outperforms other models with better computation time and prediction performance. It obtains noteworthy F1 scores of 0.9473, 0.9557, and 0.9391, respectively, for recall, precision, and precision. Even when there is an increase in the number of cars, the accuracy remains high, reaching 91.21%, even with a prediction length of only 1 s. At the same time, the time complexity remains low, under 0.1 s. These findings clearly demonstrate the effectiveness and potency of GCN-DML as a traffic forecasting tool for intelligent transportation systems.},
  archive      = {J_NCA},
  author       = {Khan, Shakir and Alghayadh, Faisal Yousef and Ahanger, Tariq Ahamed and Soni, Mukesh and Viriyasitavat, Wattana and Berdieva, Uguloy and Byeon, Haewon},
  doi          = {10.1007/s00521-024-10537-z},
  journal      = {Neural Computing and Applications},
  month        = {7},
  number       = {20},
  pages        = {14673-14686},
  shortjournal = {Neural Comput. Appl.},
  title        = {Deep learning model for efficient traffic forecasting in intelligent transportation systems},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). TF-MVGNN: An accurate traffic forecasting framework based on spatial–temporal graph neural network through exploiting multiple-view graph construction and learning. <em>NCA</em>, <em>37</em>(20), 14657-14671. (<a href='https://doi.org/10.1007/s00521-024-10508-4'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In complex and volatile transportation traffic situations, accurately forecasting traffic flow can greatly facilitate to manage and plan real-time traffic. However, due to complex road map and traffic conditions, forecasting temporal traffic is a highly challenging task. In literature, most existing works rely on various spatial–temporal models solely built on the topological structure of road networks, to capture both spatial and temporal dependencies of traffic flow at multiple sensing sites. However, besides the road topological relationship, the dynamics of real-world traffic is indispensably affected by the complex but unknown inter-dependency of multiple sites. Based on the above consideration, this paper proposed a novel traffic forecast framework based on multiple-view graph construction and learning, and spatiotemporal graph neural network (STGNN). Specifically, our work’s contributions are following. First, considering transfer entropy (TE)-based relationship graph can characterize the causal influence from one sensing sites to another, the traditional undirected topological graph is transformed into a directed graph through multiplying with the constructed TE relationship graph, which can capture both the topological relationship, and latent casual influence from one site to another, and greatly facilitate to accurately forecast traffic. Meanwhile, a learnable graph matrix is trained to complement the TE-enhanced causality graph. Second, the proposed STGNN-based forecasting framework smoothly integrates recurrent neural network and GNN that works on directional and learnable graphs, so-called DL-GNN. In detail, the aggregation process in DL-GNN is formulated as three parts: aggregation on indegree and outdegree directions of the causality graph, and on learnable graph to fully extract the sites’ representations. The comprehensive experiments on real traffic data demonstrate that our proposal outperforms other state-of-the-art STGNN-based traffic forecasting schemes.},
  archive      = {J_NCA},
  author       = {Cheng, Haoyuan and Wang, Yufeng and Ma, Jianhua and Jin, Qun},
  doi          = {10.1007/s00521-024-10508-4},
  journal      = {Neural Computing and Applications},
  month        = {7},
  number       = {20},
  pages        = {14657-14671},
  shortjournal = {Neural Comput. Appl.},
  title        = {TF-MVGNN: An accurate traffic forecasting framework based on spatial–temporal graph neural network through exploiting multiple-view graph construction and learning},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). EEMD-MST-resnet: A hybrid deep learning approach for predicting passenger flow in urban transportation hubs. <em>NCA</em>, <em>37</em>(20), 14635-14655. (<a href='https://doi.org/10.1007/s00521-024-10494-7'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The urban public transportation hubs (UPTHs) serve as central and multimodal interchange points that provide convenient travel for city residents. Accurate prediction of the passenger flow at these hubs is crucial for balancing the travel demand with the service capacity of the hubs, which is of utmost importance for transportation planning, operations, and management. In this paper, we introduce EEMD-MST-Resnet, an integrated deep learning prediction model, incorporating ensemble empirical modal decomposition (EEMD) and multi-frequency deep spatiotemporal residual network (MST-Resnet). Firstly, we apply the EEMD algorithm to decompose passenger flow time series data into several intrinsic mode components. Subsequently, the components are evaluated using sample entropy, and the evaluated results are clustered using density-based spatial clustering of applications with noise to obtain a reconstruction plan for the components. The reconstruction process categorizes them into high-frequency, medium-frequency, and low-frequency components. Furthermore, we analyze and incorporate external factors influencing passenger travel to enhance prediction accuracy. These factors, along with the reconstructed components, are utilized in the corresponding MST-Resnet model for training and prediction. The final predictions are obtained by combining results from each component. Our experimental results reveal that our combined deep learning prediction method outperforms benchmark algorithms, including CNN, LSTM, BiLSTM, and ST-Resnet, as evident from the superior performance in RMSE, MAE, and sMAPE metrics. Remarkably, we achieve a significant improvement of over 8.225% in sMAPE. This research significantly advances accurate passenger flow prediction in UPTHs, underscoring the effectiveness of our EEMD-MST-Resnet model. The findings of this study hold substantial implications for enhancing transportation planning, operation, and management in urban settings.},
  archive      = {J_NCA},
  author       = {Chen, Junxi and Wei, Zhenlin},
  doi          = {10.1007/s00521-024-10494-7},
  journal      = {Neural Computing and Applications},
  month        = {7},
  number       = {20},
  pages        = {14635-14655},
  shortjournal = {Neural Comput. Appl.},
  title        = {EEMD-MST-resnet: A hybrid deep learning approach for predicting passenger flow in urban transportation hubs},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Conducting patch contrastive learning with mixture of experts on mixed datasets for medical image segmentation. <em>NCA</em>, <em>37</em>(19), 14189-14216. (<a href='https://doi.org/10.1007/s00521-025-11234-1'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Medical image segmentation is critical for accurate diagnosis, treatment planning, and surgical navigation. In recent years, large multitask segmentation models have often struggled due to the limited size of datasets and significant variability in target structures, image resolutions, and annotation standards. These variations can introduce task competitions during multitask model training, which hinder effective feature learning. To address these challenges, we propose PatchMoE, a unified framework designed to compensate for resolution discrepancies across datasets and feature conflicts arising in mixed-dataset training. PatchMoE is the first to introduce patch-based contrastive learning into medical image segmentation tasks, which divides images into equal-sized patches represented in 3D coordinate space. This novel approach ensures that mixed datasets with varying resolutions can be trained in a unified manner, preserving spatial relationships and enhancing contextual understanding. PatchMoE also incorporates a mixture of experts (MoE) mechanism into the decoder, which dynamically selects dataset-specific expert combinations. This design mitigates parameter conflicts through network sparsification, effectively resolving optimization conflicts in multitask datasets. The effectiveness of the proposed method was demonstrated in four independent segmentation tasks: retinal vessel (DRIVE), near-infrared blurred vessel (HVNIR), abdominal multiorgan (Synapse), and polyp segmentation (Kvasir-SEG). We compared performance using multiple metrics, including Dice score, Intersection over Union (IoU), and Hausdorff distance (HD). Compared with the state-of-the-art (SOTA) GCASCADE model, PatchMoE achieved an improvement of 3.04% in the mean Dice score across all tasks. The proposed method also achieved an average Dice score improvement of 0.88% compared to four independently trained SOTA models for each individual task. In summary, PatchMoE combines patch-based contrastive learning with dataset-informed expert gating to provide promising solutions for dataset conflicts in large transformer-based medical segmentation models.},
  archive      = {J_NCA},
  author       = {Wang, Jiazhe and Yoshie, Osamu and Ieiri, Yuya},
  doi          = {10.1007/s00521-025-11234-1},
  journal      = {Neural Computing and Applications},
  month        = {7},
  number       = {19},
  pages        = {14189-14216},
  shortjournal = {Neural Comput. Appl.},
  title        = {Conducting patch contrastive learning with mixture of experts on mixed datasets for medical image segmentation},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Curiosity-driven dual-policy action selection in temporal difference learning for model predictive control. <em>NCA</em>, <em>37</em>(19), 14171-14187. (<a href='https://doi.org/10.1007/s00521-025-11252-z'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper introduces a new reinforcement learning framework that integrates three key methods to optimize action selection into the Temporal Difference Model Predictive Control (TD-MPC) (as reported by Hansen In: international conference on machine learning, 2022), a leading model-based reinforcement learning algorithm, and improve its learning performance in complex environments. First, dual-policy action selection combines the strengths of both model-based and model-free reinforcement learning, enabling more effective and flexible decision-making. Second, policy annealing is introduced to dynamically balance the contributions of these dual policies, preventing excessive exploitation and ensuring robust learning across varying conditions. Third, curiosity-driven exploration is employed to strike an optimal balance between exploration and exploitation, encouraging the agent to explore less familiar states and enhancing overall learning effectiveness. Ultimately, we propose a new scheme, called TD-MPC++, which incorporates the above three key methods into TD-MPC, a leading model-based reinforcement learning algorithm. Through extensive empirical evaluation in the varied environments of the DeepMind Control Suite, we demonstrate that TD-MPC++ achieves not only marked improvements in both sample efficiency and overall training performance when compared to its predecessor, TD-MPC, but also outperforms other state-of-the-art algorithms, including Dreamer V3 and Soft Actor-Critic. Notably, TD-MPC++ achieves superior performance compared to TD-MPC without introducing any additional learning tasks to the models or increasing time complexity.},
  archive      = {J_NCA},
  author       = {Ji, Chang-Hun and Choi, Yo-Han and Han, Youn-Hee},
  doi          = {10.1007/s00521-025-11252-z},
  journal      = {Neural Computing and Applications},
  month        = {7},
  number       = {19},
  pages        = {14171-14187},
  shortjournal = {Neural Comput. Appl.},
  title        = {Curiosity-driven dual-policy action selection in temporal difference learning for model predictive control},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Knowledge contrast-enhanced continuous prompt tuning for few-shot learning. <em>NCA</em>, <em>37</em>(19), 14151-14169. (<a href='https://doi.org/10.1007/s00521-025-11251-0'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Prompt tuning for pre-trained language models (PLMs) has shown its effectiveness and superiority in few-shot learning. However, its success heavily depends on prompt engineering which designs different prompts for specific tasks. Continuous prompt templates can avoid this compared to discrete prompt templates, but how to obtain an optimal continuous prompt template is a question worth studying. In order to overcome this challenge, we focus on eliciting knowledge from PLMs and propose a novel Knowledge Contrast-Enhanced Continuous Prompt Tuning method (KCPT) based on contrastive learning. This method aims to obtain more meaningful and distinguishable prompt embeddings by introducing a contrastive learning strategy, enabling the model to better capture subtle differences between different prompts. Extensive experiments on 10 datasets demonstrate that our KCPT improves accuracy by 1–3% and stability by 26–40% on average compared to SOTA prompt tuning methods in low-resource settings.},
  archive      = {J_NCA},
  author       = {Li, Fei and Huang, Youzhi and Wang, Yanyan and Chen, Zhengyi and Xu, Yin and Li, Xiangyang},
  doi          = {10.1007/s00521-025-11251-0},
  journal      = {Neural Computing and Applications},
  month        = {7},
  number       = {19},
  pages        = {14151-14169},
  shortjournal = {Neural Comput. Appl.},
  title        = {Knowledge contrast-enhanced continuous prompt tuning for few-shot learning},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). GPRN: GAN-based prototype refinement network for few-shot learning. <em>NCA</em>, <em>37</em>(19), 14127-14150. (<a href='https://doi.org/10.1007/s00521-025-11235-0'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Few-shot learning (FSL) aims to achieve accurate recognition of unseen classes with extremely limited samples for each class. In recent years, numerous methods adopt prototype-based strategies to address the FSL challenges, leading to significant advancements in the field. However, this class of methods may suffer from prototype bias issues due to the presence of outlier samples in support set and the scarcity of samples. To address these issues, we propose a GAN-based prototype refinement network (GPRN) for few-shot learning. Specifically, to solve the outlier samples problem, we use a pre-correction module to selectively enhance the discriminability of sample features. Moreover, we employ a novel Conditional Wasserstein Generative Adversarial Network in the FSL framework to generate additional sample features for each class to solve the sample scarcity problem. And, by considering the relationship between generated sample features and class prototypes, different weight coefficients are assigned to each generated feature to reduce the interference from noisy features within the generated features. Finally, a prototype refinement module is used to fuse the few-shot prototypes and fake prototypes to obtain the final rectified prototypes. We show that the rectified prototypes are closer to the true class centers, and when these prototypes are used in subsequent classification measurements, the classification results are significantly improved. Experimental results on four benchmark datasets, mini-ImageNet, tiered-ImageNet, CIFAR-FS, and CUB-200–2011, show that our method achieves 5-way 1-shot accuracy of 53.42%, 55.74%, 62.40%, and 59.21%, and 5-way 5-shot accuracy of 69.42%, 72.75%, 78.58%, and 73.42%, respectively, demonstrating the validity of GPRN.},
  archive      = {J_NCA},
  author       = {Luo, Liufei and Yang, Zhao and Chen, Zhiyong and Cao, Renlong and Liu, Yusen},
  doi          = {10.1007/s00521-025-11235-0},
  journal      = {Neural Computing and Applications},
  month        = {7},
  number       = {19},
  pages        = {14127-14150},
  shortjournal = {Neural Comput. Appl.},
  title        = {GPRN: GAN-based prototype refinement network for few-shot learning},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Efficient and accurate brain tumor detection and classification using advanced hybrid filtering and self-attention generative adversarial networks. <em>NCA</em>, <em>37</em>(19), 14097-14125. (<a href='https://doi.org/10.1007/s00521-025-11216-3'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Brain tumor detection and classification is a critical task in medical imaging aimed at improving diagnostic accuracy and treatment outcomes. This study presents an advanced methodology for brain tumor detection and classification using MRI and X-ray images, leveraging the ADKF-SSECT-SAGAN-NGOA model. The proposed method begins with preprocessing with anisotropic diffusion and Kuwahara filtering (ADKF), which is a hybrid filtering technique that combines anisotropic diffusion and Kuwahara filtering to reduce noise while maintaining edges and improving the local contrast. The proposed algorithm performs feature extraction using the aynchro squeezing extract chirplet transform (SSECT), followed by segmentation with SegNet. The proposed model integrates self-attention mechanisms within generative adversarial networks (GANs) to enhance image resolution and classification accuracy. Additionally, the Northern Goshawk optimization algorithm (NGOA) was employed to optimize the model parameters effectively. The proposed algorithm demonstrates the model's superior performance, achieving an average accuracy of 99.45% for the magnetic resonance imaging (MRI) dataset (a), 99.71% for the MRI dataset (b), and 99.1% for X-ray images. The model also shows remarkable efficiency, with significantly reduced computation times compared to existing methods. This combination of high accuracy and efficiency underscores the potential of the proposed model as a reliable tool for medical image analysis and clinical diagnostics.},
  archive      = {J_NCA},
  author       = {Rajakumari, R. and Selvapandian, A.},
  doi          = {10.1007/s00521-025-11216-3},
  journal      = {Neural Computing and Applications},
  month        = {7},
  number       = {19},
  pages        = {14097-14125},
  shortjournal = {Neural Comput. Appl.},
  title        = {Efficient and accurate brain tumor detection and classification using advanced hybrid filtering and self-attention generative adversarial networks},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Optimal sizing of hybrid PV/biomass/hydro-pumped storage unit systems using an enhanced manta ray foraging optimizer: A benchmark and comparative study. <em>NCA</em>, <em>37</em>(19), 14067-14096. (<a href='https://doi.org/10.1007/s00521-025-11210-9'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {While the Manta Ray Foraging Optimization (MRFO) methodology has shown promising results in tackling complex optimization problems, it has certain limitations, including restricted exploitation potential and a decline in population diversity, which can hinder its effectiveness in some applications. To address these shortcomings, this research introduces the Enhanced Manta Ray Foraging Optimization (EMRFO) algorithm, which incorporates high- and low-velocity ratios to improve the balance between exploitation and exploration. The EMRFO algorithm undergoes thorough testing against seven benchmark functions and is compared with several established optimization methods, including MRFO, marine predators algorithm (MPA), artificial rabbits optimization (ARO), grey wolf optimizer (GWO), dung beetle optimizer (DBO), and pelican optimization algorithm (POA). Simulation results indicate that EMRFO consistently outperforms its competitors, achieving superior solutions and faster convergence rates across most benchmark functions. Furthermore, the algorithm exhibits significant resilience and adaptability to different optimization parameters, making it suitable for a wide range of real-world applications, especially in the context of microgrid systems. These findings demonstrate that EMRFO is a highly effective optimization method, offering substantial improvements over other algorithms. In addition to algorithmic advancements, this study presents an isolated hybrid power system designed to ensure reliable and efficient energy supply. The system includes a photovoltaic (PV) unit, a biomass system, and a hydro-pumped storage unit (HPSU), with optimal sizing achieved through various optimization techniques, including EMRFO and other well-established algorithms such as MRFO, zebra optimization algorithm (ZOA), whale optimization algorithm (WOA), and hunger games search (HGS). Although all algorithms performed well, EMRFO consistently delivered the best results, achieving the lowest objective function and optimal sizing for the hybrid power system.},
  archive      = {J_NCA},
  author       = {Abd El-Sattar , Hoda and Kamel, Salah and Hassan, Moahmed H. and Jurado, Francisco},
  doi          = {10.1007/s00521-025-11210-9},
  journal      = {Neural Computing and Applications},
  month        = {7},
  number       = {19},
  pages        = {14067-14096},
  shortjournal = {Neural Comput. Appl.},
  title        = {Optimal sizing of hybrid PV/biomass/hydro-pumped storage unit systems using an enhanced manta ray foraging optimizer: A benchmark and comparative study},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). ROPREM: A prototype-free retrival method for automatic check-out. <em>NCA</em>, <em>37</em>(19), 14049-14066. (<a href='https://doi.org/10.1007/s00521-025-11259-6'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, automatic check-out (ACO) gains increasing interest and has been widely used in daily life. However, current works mainly rely on both counter and product prototype images in the training phase, and it is hard to maintain the performance in an incremental setting. To deal with this problem, in this paper, we propose a robust prototype-f ree r etrieval method (ROPREM) for ACO, which is a cascaded framework composed of a product detector module and a product retrieval module. Specifically, we use the product detector module to locate the products and then deliver the results to the subsequent product retrieval module for counting. The product detector module is trained without product class information, which can avoid the model overfitting for the known classes and improve the performance for the incremental setting. Additionally, we treat the check-out process as a retrieval process rather than a classification process. The retrieval result is considered as the product class by calculating the feature similarity between the query and gallery templates. The quantities of each category are treated as the final counts. To our best knowledge, this is the first attempt to treat ACO as a retrieval task. Extensive experiments are conducted to validate the proposed ROPREM, and the results show that ROPREM achieves the best performance in comparison with several state-of-the-art methods on the public retail product checkout (RPC) dataset.},
  archive      = {J_NCA},
  author       = {Huangfu, Huijie and Yang, Ziyuan and Ran, Maosong and Zhang, Weihua and Lu, Jingfeng and Zhang, Yi},
  doi          = {10.1007/s00521-025-11259-6},
  journal      = {Neural Computing and Applications},
  month        = {7},
  number       = {19},
  pages        = {14049-14066},
  shortjournal = {Neural Comput. Appl.},
  title        = {ROPREM: A prototype-free retrival method for automatic check-out},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). DF-CFAR: Deep feature constant false alarm ratio detector based on feature game for sea-surface small target with robustness to sea states. <em>NCA</em>, <em>37</em>(19), 14029-14048. (<a href='https://doi.org/10.1007/s00521-025-11246-x'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The classical constant false alarm rate (CFAR) detector is optimal for target detection in Gaussian white noise but struggles with unknown, time-varying sea states. Data-driven target detection methods are highly sensitive to clutter distribution, leading to poor detection performance and high false alarm rates ( $$P_{fa}$$ ) under unknown sea states. This paper proposes a deep feature constant false alarm ratio (DF-CFAR) detector based on a feature game model. The input to the feature extraction network is fragmented fast time-dimensional data processed by coherent integration, incorporating a feature game mechanism. This mechanism effectively mines target echo features that are uncorrelated with the background sea clutter. By transforming the target detection problem under varying sea state conditions into a regression prediction problem, we achieve constant false alarm detection through a statistical probability threshold. Simulation and real data experiments demonstrate that, compared to commonly used algorithms, the proposed method not only achieves higher detection probability and lower false alarm rates but also exhibits excellent constant false alarm characteristics and stronger robustness to unseen sea states. Results from publicly available X-band radar data processing show that the proposed algorithm can eliminate the influence of different sea clutters and improve the detection performance of sea-surface small targets by an equivalent signal-to-clutter ratio (SCR) of about 3.1 dB.},
  archive      = {J_NCA},
  author       = {Xiang, Houhong and Zhu, Liangliang and Chen, Yufeng and Wang, Fengyu and Zeng, Xiaolu},
  doi          = {10.1007/s00521-025-11246-x},
  journal      = {Neural Computing and Applications},
  month        = {7},
  number       = {19},
  pages        = {14029-14048},
  shortjournal = {Neural Comput. Appl.},
  title        = {DF-CFAR: Deep feature constant false alarm ratio detector based on feature game for sea-surface small target with robustness to sea states},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Stochastic gradient sampling for enhancing neural networks training. <em>NCA</em>, <em>37</em>(19), 14005-14028. (<a href='https://doi.org/10.1007/s00521-025-11242-1'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we introduce StochGradAdam, a novel optimizer designed as an extension of the Adam algorithm, incorporating stochastic gradient sampling techniques to improve computational efficiency while maintaining robust performance. StochGradAdam optimizes by selectively sampling a subset of gradients during training, reducing the computational cost while preserving the advantages of adaptive learning rates and bias corrections found in Adam. Our experimental results, applied to image classification and segmentation tasks, demonstrate that StochGradAdam can achieve comparable or superior performance to Adam, even when using fewer gradient updates per iteration. By focusing on key gradient updates, StochGradAdam offers stable convergence and enhanced exploration of the loss landscape, while mitigating the impact of noisy gradients. The results suggest that this approach is particularly effective for large-scale models and datasets, providing a promising alternative to traditional optimization techniques for deep learning applications.},
  archive      = {J_NCA},
  author       = {Yun, Juyoung},
  doi          = {10.1007/s00521-025-11242-1},
  journal      = {Neural Computing and Applications},
  month        = {7},
  number       = {19},
  pages        = {14005-14028},
  shortjournal = {Neural Comput. Appl.},
  title        = {Stochastic gradient sampling for enhancing neural networks training},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). DSTEN-CSLR: Dual spatial–temporal enhancement network for continuous sign language recognition. <em>NCA</em>, <em>37</em>(19), 13981-14004. (<a href='https://doi.org/10.1007/s00521-025-11240-3'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Continuous sign language recognition (CSLR) aims to transcribe sign language videos into target gloss sequences. This task presents certain challenges as it requires learning the spatial and temporal dimensions of sign language frame sequences. The architecture of the backbone can be categorized into three main components: the visual module, the contextual module, and the alignment module. However, the spike phenomenon of the connectionist temporal classification (CTC) algorithm, which is commonly used in the alignment module, causes the features extracted by the network to be not discriminative enough, leading to inaccurate recognition results. To address this, we propose a dual spatial–temporal enhancement network (DSTEN) that aims at enhancing the network training from both spatial and temporal perspectives. The first enhancement focuses on improving the visual module, which often suffers from insufficient training. Since sign language mainly conveys information through the signer’s face and hands, this paper proposes to insert a spatial-channel attention (SCA) module assisted by heatmaps of keypoints into the visual module, i.e., dense keypoints-assisted spatial enhancement (DKSE), which aims to capture more fine-grained discriminative features. The second enhancement is designed to optimize the sequence learning capability of the contextual module, as existing contextual modeling networks commonly used for most sequence tasks are not entirely suitable for CSLR. Considering the characteristic of local semantic consistency in sign language, we propose a multi-scale temporal enhancement (MSTE) network based on 1D convolutional neural network (1D-CNN) to capture long- and short-term dependencies. It demonstrates excellent contextual modeling capability and effectively captures information at different time scales through an adaptive multi-scale mechanism while providing feedback to the visual module. Extensive experiments demonstrate the effectiveness of the proposed network by competitive performance on the PHOENIX-2014, PHOENIX-2014T, CSL and CSL-Daily datasets.},
  archive      = {J_NCA},
  author       = {Yu, Ming and Gan, Aobo and Xue, Cuihong and Yan, Gang},
  doi          = {10.1007/s00521-025-11240-3},
  journal      = {Neural Computing and Applications},
  month        = {7},
  number       = {19},
  pages        = {13981-14004},
  shortjournal = {Neural Comput. Appl.},
  title        = {DSTEN-CSLR: Dual spatial–temporal enhancement network for continuous sign language recognition},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Deep learning for predicting essential proteins using topological features and gene ontology. <em>NCA</em>, <em>37</em>(19), 13965-13980. (<a href='https://doi.org/10.1007/s00521-025-11217-2'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Identifying essential proteins is vital for understanding cellular functions, discovering new drug targets, and linking genes to human diseases. Traditional methods like gene deletions and RNA interference are often slow and resource-intensive. To address this, computational approaches analyze sequence-based features and topology-based features from protein–protein interaction (PPI) networks. However, these methods face challenges due to imbalances in PPI networks, leading to less accurate predictions. This study aims to improve the identification of essential proteins by combining gene ontology (GO) annotations for yeast proteins with topological information from PPI data and applying deep learning techniques. In the feature extraction process, the PPI network is converted into a graph, and node embeddings are generated using the Node2Vec algorithm, which captures structural and relational properties within the network. Additionally, 695 GO features were collected from the UniProt database and merged with the node embeddings to form a comprehensive feature set. This integrated approach, termed Node Embedding + GO features, significantly enhances prediction accuracy while addressing data imbalance using SMOTE. The proposed method achieved an accuracy of 94.20%, providing a robust and scalable framework for future research in essential protein identification. The study of essential proteins is crucial for understanding cellular functions, identifying potential drug targets, and advancing disease research. It has significant practical applications in drug development.},
  archive      = {J_NCA},
  author       = {Islam, Md. Shahidul and Islam, Md. Rafiqul and Mistry, Durjoy},
  doi          = {10.1007/s00521-025-11217-2},
  journal      = {Neural Computing and Applications},
  month        = {7},
  number       = {19},
  pages        = {13965-13980},
  shortjournal = {Neural Comput. Appl.},
  title        = {Deep learning for predicting essential proteins using topological features and gene ontology},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). GoogleNet’s semantic hierarchical feature fusion for the classification of lung cancer CT images. <em>NCA</em>, <em>37</em>(19), 13943-13963. (<a href='https://doi.org/10.1007/s00521-025-11185-7'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Lung cancer arises and progresses due to genetic alterations in deoxyribonucleic acid, resulting in aberrant cell proliferation and tumor formation. Early-stage lung cancer often presents without discernible symptoms, necessitating diagnosis through medical imaging. Computed tomography (CT) serves as a primary modality for detecting lung cancer. However, the complex structure of the human lung poses significant challenges for radiologists and clinicians in identifying malignant regions, thus driving the exploration of intelligent diagnostic systems to aid in lung cancer detection. In this approach, feature maps of lung cancer images are integrated across multiple levels of the GoogleNet, creating a semantic hierarchy of lung cancer images. Subsequently, handcrafted texture and brightness features are hierarchically extracted from the semantic image hierarchy and fused together. Following the selection of salient features from lung cancer images through their Shapley values, a k-NN classifier is employed to classify test samples of lung cancer images. The proposed methodology is evaluated for classifying lung cancer into three categories—normal, benign, and malignant—using the “IQ-OTH/NCCD-Lung” and “Kaggle CT scan images” benchmark datasets. The segregation of class categories in the evaluation dataset is visualized using the t-distributed stochastic neighbor embedding statistical technique. Evaluation findings confirm the effectiveness of the proposed approach in classifying lung cancer CT images compared to state-of-the-art methods.},
  archive      = {J_NCA},
  author       = {Taheri, Fatemeh and Rahbar, Kambiz},
  doi          = {10.1007/s00521-025-11185-7},
  journal      = {Neural Computing and Applications},
  month        = {7},
  number       = {19},
  pages        = {13943-13963},
  shortjournal = {Neural Comput. Appl.},
  title        = {GoogleNet’s semantic hierarchical feature fusion for the classification of lung cancer CT images},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Convolutional neural network models with low spatial variability hamper the transfer learning process. <em>NCA</em>, <em>37</em>(19), 13927-13942. (<a href='https://doi.org/10.1007/s00521-025-11267-6'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Using pre-trained convolutional neural networks (CNNs) architectures have proven effective in high-resolution remote sensing, even when homogeneous and few data samples are used. However, it is still uncertain how well models trained with limited spatial information can transfer the learning process from one domain to a new domain (transductive transfer learning). This paper evaluates transductive transfer learning in CNN regression models using RGB-based data captured by unoccupied aerial vehicles on five sites, using the prediction of Pinus radiata canopy coverage as a case study. We trained five models, one per site, analyzing their internal performance using fine-tuning and feature extraction training approaches. Then, we evaluated their transfer learning ability to new unseeing sites. We found that the trained models perform accurately within their domain, as previous research demonstrates ( $$R^2$$ > 0.90 using fine-tuning). However, we depicted varying performances during transfer learning, with $$R^2$$ ranging from − 0.68 to 0.63 for feature extraction and from − 4.42 to 0.77 for fine-tuning. Our results show that these poor performances are independent of the training approach (fine-tuning or feature extraction), the number of observations, or the complexity of the model. In contrast, the success during transfer learning is closely linked to the similarity between the source and target domains, which is often unknown when predicting new data. These results depict the importance of carefully planning the future use of such models for their sustainability and generalization over time.},
  archive      = {J_NCA},
  author       = {Bravo-Diaz, Alejandra and Moreno, Sebastián and Lopatin, Javier},
  doi          = {10.1007/s00521-025-11267-6},
  journal      = {Neural Computing and Applications},
  month        = {7},
  number       = {19},
  pages        = {13927-13942},
  shortjournal = {Neural Comput. Appl.},
  title        = {Convolutional neural network models with low spatial variability hamper the transfer learning process},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Facial AU detection based on a guided attention inference network with embedded regional segmentation branch. <em>NCA</em>, <em>37</em>(19), 13901-13925. (<a href='https://doi.org/10.1007/s00521-025-11254-x'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Facial action units (AUs) serve as a precise descriptor of facial expressions, revealing an individual’s psychological and mental state. Due to the fact that each AU is confined to a specific facial region, AU feature extraction usually necessitates the integration of landmark detection tools and prior knowledge regarding to locations of different AUs to partition the face, which leads to time-consuming and laborsome pre-processing procedure. To tackle this issue, a weakly supervised guided attention inference network is proposed for AU detection. The network encompasses two modules with shared parameters: a classification and ROI segmentation module (CRSM) and an attention mining module (AMM). The CRSM autonomously identifies regions of interest for target AUs and generates class activation attention maps. The AMM utilizes these maps to exclude facial regions of target AUs so that a weak constraint that minimizes AU prediction scores for target AUs can be imposed on network training, which thereby ensures that the network’s attention maps encompass all most discriminant regions contributing to AU classification decisions. Experimental results on the BP4D and DISFA datasets demonstrate that, even in the absence of landmark detection and pre-facial region partitioning, the proposed model sustains excellent detection performance during testing. Furthermore, the generated AU attention map can accurately indicate the spatial locations of AU occurrences, which makes the AU detection results explainable.},
  archive      = {J_NCA},
  author       = {Li, Kui and Liang, Chaolei and Zou, Wei and Hu, Danfeng and Wang, JiaJun},
  doi          = {10.1007/s00521-025-11254-x},
  journal      = {Neural Computing and Applications},
  month        = {7},
  number       = {19},
  pages        = {13901-13925},
  shortjournal = {Neural Comput. Appl.},
  title        = {Facial AU detection based on a guided attention inference network with embedded regional segmentation branch},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A hybrid approach for alzheimer’s disease diagnosis: Image segmentation and deep learning classification. <em>NCA</em>, <em>37</em>(19), 13881-13899. (<a href='https://doi.org/10.1007/s00521-025-11249-8'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Alzheimer’s disease is a severe neurodegenerative disease that leads to cognitive decline and memory loss, and hence, the early diagnosis is essential for proper management and care. This study investigates the usage of the improved fuzzy C-means (FCM) algorithm for segmentation of MRI scan and five deep learning models, namely Densenet121, Inception-v3, VGG-19, Xception, and Resnet-50, for image classification. Enhanced fuzzy C-means algorithm helps segment MRI data, while the selected deep learning architectures classify these images for accurate classification. Rigorous processes, including five-fold cross-validation, early stopping, and optimization techniques, were employed to ensure model performance. The results indicated that combining the enhanced fuzzy C-means algorithm with deep learning architectures significantly improved MRI classification accuracy, where Densenet121 and Resnet-50 exhibited the best results. This approach holds promise for enhancing the diagnosis and treatment of Alzheimer’s disease because it increases detection accuracy and reliability, allowing for better patient care.},
  archive      = {J_NCA},
  author       = {Bhimavarapu, Usharani},
  doi          = {10.1007/s00521-025-11249-8},
  journal      = {Neural Computing and Applications},
  month        = {7},
  number       = {19},
  pages        = {13881-13899},
  shortjournal = {Neural Comput. Appl.},
  title        = {A hybrid approach for alzheimer’s disease diagnosis: Image segmentation and deep learning classification},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). AQRG: Adaptive quantization reconstruction granularity for post-training quantization. <em>NCA</em>, <em>37</em>(19), 13863-13879. (<a href='https://doi.org/10.1007/s00521-025-11268-5'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deploying models on resource-constrained edge devices remains always a critical challenge for the application of neural network. Quantization is one of the most popular methods to compress the model for meeting the performance limitations. As only a small amount of calibration data is required, post-training quantization (PTQ) is more suitable for protecting privacy than quantization-aware training(QAT). However, PTQ often causes substantial accuracy degradation when it goes below 4-bit, and previous PTQ works primarily focused on single reconstruction quantization granularity, either all layer-wise or all block-wise. Nevertheless, it is proved in our exploratory experiments that these schemes are sub-optimal. In this paper, we explore the relation of Hessian matrix trace and the inter-layer dependency which takes key role in the choice of quantization reconstruction granularity. Based on the discovery, we propose a novel hybrid reconstruction granularity quantization scheme AQRG, which adaptively adjusts quantization granularity guided by the Hessian matrix trace. In image classification and object detection, AQRG achieves better accuracy and robustness for calibration data size on several typical convolutional neural networks. In particular, our 4-bit weight 2-bit activation (W4A2) scheme in ResNet-18 achieved 65.06% accuracy on the ImageNet dataset.},
  archive      = {J_NCA},
  author       = {Zhang, Wenbo and Wang, Tianshuo and Fu, Guohang and Bao, Zhenshan},
  doi          = {10.1007/s00521-025-11268-5},
  journal      = {Neural Computing and Applications},
  month        = {7},
  number       = {19},
  pages        = {13863-13879},
  shortjournal = {Neural Comput. Appl.},
  title        = {AQRG: Adaptive quantization reconstruction granularity for post-training quantization},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). FaLa: Feature-augmented location-aware transformer network for graph representation learning. <em>NCA</em>, <em>37</em>(19), 13835-13862. (<a href='https://doi.org/10.1007/s00521-025-11264-9'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Data augmentation is a technique that improves the ability of neural networks to make accurate predictions by increasing the size of the training dataset. However, it is still uncertain how to properly use data augmentation on graph data in order to boost the performance of graph neural networks (GNNs). While the majority of current graph regularizers primarily concentrate on modifying the topological structures of graphs by adding or deleting nodes and edges, we propose a technique to augment node and edge features in order to improve efficiency. More precisely, we use cosine similarity to perform cross-functions on the existing features. This allows us to generate new graph features, such as node and edge attributes, as well as new graph structures. We then merge these new features together to provide specific pairs of inputs for graph transformer networks. We next propose feature-augmented location-aware transformer (FaLa) networks that generalize well across several graph learning problems by combining the positional and structural encoding of nodes. Additionally, in order to avoid collecting duplicate data from other features, we apply a disparity restriction during training on these latent node embeddings. Experimental findings across eleven real-world datasets demonstrate that our methodology enhances node classification accuracy by a significant margin of 2.8% to 22.7%, link prediction by 1.6% to 8.5%, and graph classification by 7.0% to 39.8% compared to the state-of-the-art graph convolutional network (GCN) and graph transformer (GT) models. This clearly shows the importance of augmenting graph features and location encoding for efficient graph representation learning.},
  archive      = {J_NCA},
  author       = {Morshed, Md Golam and Sultana, Tangina and Lee, Young-Koo},
  doi          = {10.1007/s00521-025-11264-9},
  journal      = {Neural Computing and Applications},
  month        = {7},
  number       = {19},
  pages        = {13835-13862},
  shortjournal = {Neural Comput. Appl.},
  title        = {FaLa: Feature-augmented location-aware transformer network for graph representation learning},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). RedTops: Real-time energy-aware dynamic task offloading via federated mountain gazelle optimisation in SDN-enhanced edge computing. <em>NCA</em>, <em>37</em>(19), 13795-13833. (<a href='https://doi.org/10.1007/s00521-025-11248-9'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Offloading computational tasks is vital for real-time applications on mobile devices with limited resources. Mobile edge computing (MEC) is deemed a solution that puts computational resources closer to users. Nevertheless, there are many associated concerns during the offloading procedure (i.e., privacy, delay, and high energy consumption). Federated learning (FL) has been considered a solution to address MEC’s data privacy issues; however, it comes with its own resource consumption issues. To address these issues, this paper proposes a distributed learning paradigm inspired by FL. We propose an optimisation technique for offloading computational tasks that aims to reduce both total delay and energy consumption by using the mountain gazelle optimisation algorithm, which shows it can reduce both delay and energy consumption in dynamic situations. Additionally, an improved variant known as the improved mountain gazelle optimiser is integrated into a distributed SDN controller architecture to create an offloading policy model for optimal edge node selection. We also present a new SDN-enabled edge computing architecture that achieves the best task distribution through task offloading using federated mountain gazelle optimisation (RedTops). Energy usage, delay, and bandwidth are considered by RedTops, which successfully addresses high training costs, dependability issues, and privacy concerns in MEC. Based on the outcomes of five extensive simulations, RedTops is more energy-efficient and faster at completing tasks than four state-of-the-art offloading methods (DDLO, DROO, DRL without TL and SDN, and DTRL).},
  archive      = {J_NCA},
  author       = {Al Aghbari, Zaher and Khedr, Ahmed M. and Ahmed, Naveed and Girija, Shini and Baker, Thar},
  doi          = {10.1007/s00521-025-11248-9},
  journal      = {Neural Computing and Applications},
  month        = {7},
  number       = {19},
  pages        = {13795-13833},
  shortjournal = {Neural Comput. Appl.},
  title        = {RedTops: Real-time energy-aware dynamic task offloading via federated mountain gazelle optimisation in SDN-enhanced edge computing},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Analyzing word embeddings and their impact on semantic similarity: Through extreme simulated conditions to real dataset characteristics. <em>NCA</em>, <em>37</em>(19), 13765-13793. (<a href='https://doi.org/10.1007/s00521-025-11231-4'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This study investigates the behavior and characteristics of word embeddings through a series of experiments, focusing on models such as Word2Vec, FastText, and BERT. First, we analyze word embeddings under extreme simulated conditions by creating a data corpus without any interrelation of words and compare the resulting embeddings with randomly initialized vectors. This experiment reveals the tendency of word embedding models to create apparent word relationships even in the absence of real contextual connections. Second, we examine the evolution of word similarity with respect to increasing frequency of co-occurrence, demonstrating how embedding similarity changes as word co-occurrence frequency increases and identifying potential saturation effects. Third, we employ machine learning models-including random forest and XGBoost—to predict word similarity based on various statistical and lexical characteristics derived from real-world data. We identify influential factors such as co-occurrence frequency, context correlation, word frequency, and human-judged similarity scores, which significantly contribute to the prediction of word similarity. The prediction of semantic similarity was formulated as both regression and classification tasks, achieving high accuracy, with the random forest regression model yielding a Pearson correlation of 0.84 (p !' 5e-145). Interpretability methods such as feature importance, partial dependence plots (PDP), and local interpretable model-agnostic explanations (LIME) were used to reveal important relationships and provide insights into the factors influencing embedding similarity. Our findings contribute to a deeper understanding of how word embeddings capture semantic similarity and the factors that influence this process, offering valuable implications for the development of more transparent natural language processing models.},
  archive      = {J_NCA},
  author       = {Dvořáčková, Lucie},
  doi          = {10.1007/s00521-025-11231-4},
  journal      = {Neural Computing and Applications},
  month        = {7},
  number       = {19},
  pages        = {13765-13793},
  shortjournal = {Neural Comput. Appl.},
  title        = {Analyzing word embeddings and their impact on semantic similarity: Through extreme simulated conditions to real dataset characteristics},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). OmniNet: An expandable causal inference network for multiple modalities. <em>NCA</em>, <em>37</em>(19), 13749-13763. (<a href='https://doi.org/10.1007/s00521-025-11202-9'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {When making a causal prediction, it is important to provide a detailed description of the pre-treatment situation to minimise the risk of overlooking a hidden yet significant variable. This often requires utilising various data formats to capture all relevant details. However, only a few causal models are able to handle complex data formats. An adaptable and expandable causal model capable of processing multiple data formats could address this limitation by incorporating new data formats and their processing units into the existing model and adjusting accordingly. This would improve the model’s estimation and enhance the accuracy of causal inference results. In this paper, we introduce and test an adaptable causal architecture to demonstrate its efficacy in handling both simple and complex data. The new model, named OmniNet, features a modular design that enables easy expansion and integration of different covariate types with minimal adjustments. Like an omnivorous animal feeding on both meat and plants, OmniNet can take different types of parameters once set. Our goal is to show that OmniNet performs on par with existing models when dealing with simple data while significantly improving accuracy when additional details are included. In the experiments, OmniNet had proved itself to be expandable and able to improve its performance by taking new covariates into concern. Such ability may allow it to work on more causal problems in a wider field.},
  archive      = {J_NCA},
  author       = {Zhou, Yinuo and Leontidis, Georgios and Zhong, Mingjun},
  doi          = {10.1007/s00521-025-11202-9},
  journal      = {Neural Computing and Applications},
  month        = {7},
  number       = {19},
  pages        = {13749-13763},
  shortjournal = {Neural Comput. Appl.},
  title        = {OmniNet: An expandable causal inference network for multiple modalities},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Noise-tolerant fixed-time leader-follower consensus controller design for multi-agent systems via fuzzy-neural-network. <em>NCA</em>, <em>37</em>(19), 13725-13747. (<a href='https://doi.org/10.1007/s00521-025-11241-2'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The leader-follower consensus control problem in multi-agent systems (MASs) is critical and has received significant attention. However, the simultaneous achievement of fixed-time stability and robustness is often challenging in MASs due to their inherent complexity and uncertainty. This paper developed a controller based on the proposed noise-tolerant fixed-time fuzzy neural network (NF-FNN) model to realize the leader-follower consensus of MASs. Specifically, the introduction of an integral error term makes the NF-FNN model have powerful noise tolerance, and a fuzzy gain parameter generated by the Takagi-Sugeno fuzzy logic system makes the NF-FNN model have fuzzy adaptiveness. In addition, a new partition-sign-by-power activation function is developed to ensure fixed-time stability of the NF-FNN model. Theoretical analysis and comparative simulations confirm the superb swift stability and excellent noise tolerance of controllers based on the NF-FNN model for achieving the leader-follower consensus of MASs, as compared with existing methods.},
  archive      = {J_NCA},
  author       = {Dai, Jianhua and Tan, Ping and Xiao, Lin and Wang, Zidong and He, Yongjun and Zuo, Qiuyue},
  doi          = {10.1007/s00521-025-11241-2},
  journal      = {Neural Computing and Applications},
  month        = {7},
  number       = {19},
  pages        = {13725-13747},
  shortjournal = {Neural Comput. Appl.},
  title        = {Noise-tolerant fixed-time leader-follower consensus controller design for multi-agent systems via fuzzy-neural-network},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A novel meta-heuristic optimization algorithm inspired by water uptake and transport in plants. <em>NCA</em>, <em>37</em>(19), 13643-13724. (<a href='https://doi.org/10.1007/s00521-025-11228-z'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper presents a novel nature-inspired optimization algorithm, the so-called water uptake and transport in plants (WUTP) algorithm, which can be used to solve numerical optimization problems. The basic thoughts and principles underlying the WUTP algorithm are inspired by modeling the process of water movement within plant membranes and how water flows from soil to plant roots and then to the plant leaves. These observations are mathematically designed to focus attention on exploration and exploitation of water flow process within plants in a particular search space. More importantly, WUTP can search the entire search area accurately with reasonable convergence speed. The performance of the proposed WUTP is entirely examined on a set of 23 well-known benchmark functions with varied levels of complexity. Further, the robustness and reliability of the WUTP algorithm are comprehensively evaluated on a collection of 29 well-known functions from the CEC-2017 benchmark for a diversity of dimensions. This test includes unimodal, multimodal, hybrid, and composition test functions with various levels of complexity. To show the reliability and appropriateness of WUTP in real-world problems, it is also applied to solve benchmark problems for nine engineering designs. A comprehensive study of the results of computational, qualitative, quantitative, and statistical analysis is presented to illustrate the efficacy and stability degrees of the proposed algorithm. The stability of the WUTP algorithm was tested in both exploration and exploitation, and the performance of WUTP was verified using several evaluation measures. The effectiveness of the proposed algorithm was further compared with several highly well-thought-of optimization methods based on the generated solutions and convergence rates. The obtained results demonstrate that WUTP generates better solutions, in terms of global optimality, solution accuracy, and reliability, in the majority of test problems compared to other promising optimization algorithms.},
  archive      = {J_NCA},
  author       = {Braik, Malik and Al-Hiary, Heba},
  doi          = {10.1007/s00521-025-11228-z},
  journal      = {Neural Computing and Applications},
  month        = {7},
  number       = {19},
  pages        = {13643-13724},
  shortjournal = {Neural Comput. Appl.},
  title        = {A novel meta-heuristic optimization algorithm inspired by water uptake and transport in plants},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Improving local interpretable classifier explanations exploiting self-generated semantic features. <em>NCA</em>, <em>37</em>(19), 13617-13641. (<a href='https://doi.org/10.1007/s00521-025-11227-0'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Explaining predictions of classifiers is a fundamental problem in eXplainable Artificial Intelligence (XAI). LIME (for Local Interpretable Model-agnostic Explanations) is a popular XAI technique able to explain any classifier by providing an interpretable model which approximates the black box locally to the instance under consideration. In order to build interpretable local models, LIME requires the user to explicitly define a space of interpretable components, also called artefacts, associated with the input instance. To reconstruct local black-box behaviour, the instance neighbourhood is explored by generating instance neighbours as random subsets of the provided artefacts. In this work, we note that the above-depicted strategy has a limitation given by the fact that the local explanation is limited to be expressed only in terms of object artefacts. To overcome this limitation, in this work we propose $$\mathcal {S}{\text {-LIME}}$$ , a variant of the basic LIME method exploiting unsupervised learning to replace object artefacts with self-generated semantic features in neighbourhood generation. This characteristic enables our approach to sample instance neighbours in a more semantic-driven fashion and greatly reduces the bias associated with explanations. We demonstrate the applicability and effectiveness of our proposal in the text classification domain. We also present a further extension for textual data in which word groups are used to obtain richer explanations. Comparison with the baseline highlights the superior quality of the explanations obtained by adopting our strategy.},
  archive      = {J_NCA},
  author       = {Angiulli, Fabrizio and Fassetti, Fabio and Nisticò, Simona},
  doi          = {10.1007/s00521-025-11227-0},
  journal      = {Neural Computing and Applications},
  month        = {7},
  number       = {19},
  pages        = {13617-13641},
  shortjournal = {Neural Comput. Appl.},
  title        = {Improving local interpretable classifier explanations exploiting self-generated semantic features},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Optimal deep neural network architecture design with improved generalization for data-driven cooling load estimation problem. <em>NCA</em>, <em>37</em>(19), 13597-13616. (<a href='https://doi.org/10.1007/s00521-025-11212-7'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Decision of the model complexity is a significant challenge in contemporary data-driven modeling applications. Designing neural architecture involves the process of determining the optimal model complexity for deep neural networks (DNNs) models in order to uncover relationships in real-world data patterns. Consequently, optimizing DNN architectures is crucial for enhancing the practical approximation performance of DNN models to real-world data. This study implements a data-driven neuroevolution scheme for the optimal neural architecture search (NAS) and demonstrates a data-driven engineering application for cooling load estimation. The proposed neuroevolution scheme aims at evolving to the best generalizing DNN model that well suits the modeling complexity requirements of the dataset. To this end, the objective function for the neural architecture optimization process is simplified to the mean square error of the test dataset, which enables to reduce the risk of insufficient generalization during NAS. By employing this objective function for evolution field optimization (EFO), the proposed neuroevolution process can automatically achieve the optimal model complexity, preventing overfitting and underfitting cases, and thereby attaining almost the best generalization for the dataset. For this purpose, this approach combines parametric learning with the backpropagation algorithm and structural learning with EFO-based neural architecture search to address data-driven, optimal complexity DNN model generation problems. Effectiveness of the method is demonstrated in the cooling load estimation problem of residential buildings, and performances of the optimal DNN models with four objective functions are analyzed. The design of objective function for the best generalizing model is also elaborated.},
  archive      = {J_NCA},
  author       = {Alagoz, Baris Baykant and Keles, Cemal and Ates, Abdullah and Özdemir, Edanur and Alkhulaifi, Nasser},
  doi          = {10.1007/s00521-025-11212-7},
  journal      = {Neural Computing and Applications},
  month        = {7},
  number       = {19},
  pages        = {13597-13616},
  shortjournal = {Neural Comput. Appl.},
  title        = {Optimal deep neural network architecture design with improved generalization for data-driven cooling load estimation problem},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Classifying EEG data from spinal cord injured patients using manifold learning methods for brain-computer interface-based rehabilitation. <em>NCA</em>, <em>37</em>(19), 13573-13596. (<a href='https://doi.org/10.1007/s00521-025-11201-w'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Spinal cord injuries (SCI) occur when the spinal cord is damaged due to any trauma. Treatment of this condition typically involves a long and challenging rehabilitation process. Brain-computer interface (BCI) controlled rehabilitation systems show promise for people with SCI, as they offer repetitive and controlled treatment in the home environment without requiring a specialist. There is a growing demand for electroencephalography (EEG)–based BCI rehabilitation systems, particularly for patients with SCI. In this study, EEG signals from ten SCI patients were analyzed. At the same time, they imagined performing rehabilitation movements for five different hand and arm actions (pronation, supination, palmar grasp, lateral grasp, and hand opening). The study tested manifold learning algorithms, feature extraction, and classification methods in EEG analysis to improve usability in real-time applications. Manifold learning algorithms were used to represent complex and high-dimensional data in a lower-dimensional space, allowing for better representation and separation of the temporal and spatial characteristics of brain activity. The spectral embedding algorithm was used in this study, which, to the best of our knowledge, is the first time this algorithm has been applied to the data of SCI patients. Additionally, we conducted a comparative analysis of commonly encountered methods in the literature, including multi-dimensional scaling (MDS), isometric feature mapping (ISOMAP), local linear embedding (LLE), and t-distributed stochastic neighbor embedding (t-SNE). Machine learning algorithms, such as k-nearest neighbor (k-NN), support vector machine (SVM), and Naive Bayes methods, were used to obtain classification results for both multi-class and binary classes. The combination of pronation and hand open was found to yield the best performance for binary classification of the movements. The study determined that the ISOMAP manifold learning method with the k-NN algorithm is the optimal method for processing times of both the train and the test. The method also demonstrated a high accuracy value of 0.967 and a short time of 0.088 units in multiple classifications, which is promising. The study utilized the spectral embedding method for the first time and achieved an accuracy rate of 0.649 in multi-class classification and 0.933 ± 0.049 in binary-class classification.},
  archive      = {J_NCA},
  author       = {Sayilgan, Ebru},
  doi          = {10.1007/s00521-025-11201-w},
  journal      = {Neural Computing and Applications},
  month        = {7},
  number       = {19},
  pages        = {13573-13596},
  shortjournal = {Neural Comput. Appl.},
  title        = {Classifying EEG data from spinal cord injured patients using manifold learning methods for brain-computer interface-based rehabilitation},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Toward precision cardiology: A transformer-based system for adaptive prediction of heart disease. <em>NCA</em>, <em>37</em>(19), 13547-13571. (<a href='https://doi.org/10.1007/s00521-025-11172-y'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Accurate and timely diagnosis of heart disease remains a critical challenge in modern healthcare. Traditional machine learning approaches, such as support vector machines (SVM) and Random Forests, have shown some success in predicting heart disease risk. However, these methods often struggle to capture complex, nonlinear relationships within the data, potentially leading to suboptimal diagnostic accuracy. To address these limitations, this study proposes a novel approach utilizing a deep learning architecture based on Transformer networks. The Transformer’s attention mechanism allows the model to dynamically weigh the importance of different patient features, capturing intricate interactions between various risk factors. Experimental results demonstrate the superior performance of the proposed Transformer-based model compared to traditional machine learning algorithms. The model achieved significantly higher accuracy (99%), F1-score, and AUC metrics on a comprehensive dataset of patient records. This improvement suggests that the Transformer architecture can effectively leverage the inherent complexity of the data to enhance the accuracy and reliability of heart disease diagnosis.},
  archive      = {J_NCA},
  author       = {Talaat, Fatma M. and Aly, Wesam F.},
  doi          = {10.1007/s00521-025-11172-y},
  journal      = {Neural Computing and Applications},
  month        = {7},
  number       = {19},
  pages        = {13547-13571},
  shortjournal = {Neural Comput. Appl.},
  title        = {Toward precision cardiology: A transformer-based system for adaptive prediction of heart disease},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Adaptive multi-player poker policy learning based on opponent style modeling. <em>NCA</em>, <em>37</em>(19), 13525-13546. (<a href='https://doi.org/10.1007/s00521-025-11262-x'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-player games, such as multi-player poker, are a critical area of research and application in game theory. Currently, there is no theoretical optimal policy for multi-player game problems, including multi-player poker, and there is a lack of policy learning methods capable of adaptively making decisions against diverse opponents. To address this gap, this paper proposes an adaptive multi-player poker policy (AMP3) learning method based on opponent style modeling (OSM) for multi-player Texas Hold’Em poker games. First, we construct a style library and a gaming dataset for poker, designing style features based on traditional and statistical indicators specific to Texas Hold’Em. Next, we propose an OSM algorithm leveraging deep learning to predict style features from opponents’ historical data. Third, a novel reinforcement learning (RL) algorithm, based on the Actor-Critic framework, is introduced to learn adaptive policies by utilizing the opponents’ style features. To the best of our knowledge, this paper is the first to integrate OSM into RL to develop the AMP3 learning algorithm. Experimental results show that the proposed method exhibits strong adaptability in adjusting its policy style when facing players with varying strategies in six-player Texas Hold’Em poker.},
  archive      = {J_NCA},
  author       = {Shi, Daming and Guo, Xudong and Liu, Yi and Fan, Wenhui},
  doi          = {10.1007/s00521-025-11262-x},
  journal      = {Neural Computing and Applications},
  month        = {7},
  number       = {19},
  pages        = {13525-13546},
  shortjournal = {Neural Comput. Appl.},
  title        = {Adaptive multi-player poker policy learning based on opponent style modeling},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A compact and flexible FPGA accelerator for regular and octave convolutional neural networks. <em>NCA</em>, <em>37</em>(19), 13497-13524. (<a href='https://doi.org/10.1007/s00521-025-11209-2'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Edge artificial intelligence (AI) is powerful in harnessing AI to provide real-time, scalable, and secure solutions directly at the edge. However, developing edge AI devices poses several challenges, including power consumption constraints, limited storage and processing capabilities. Recently, octave convolution was proposed to remove spatial redundancy from input feature maps, thus reducing the convolutional neural networks (CNN) computation and memory cost. Although octave convolution is a promising solution, not many existing hardware architectures support octave convolution, which can be deployed on a field-programmable-gate-array (FPGA). In this work, we present the first compact and flexible CNN accelerator on an FPGA, which supports normal, pointwise and depthwise convolutions for state-of-the-art octave convolution, as well as the existing regular convolution. To ensure efficient accelerator utilization, a novel adaptive scheduling scheme is presented to schedule the number of computation tasks based on varying feature map dimensions across CNN layers. A novel integration technique is presented to execute the varying computational patterns in octave convolution due to multiple branching in a single computation unit. An efficient memory scheme and execution scheduling for octave convolution are also proposed to overlap the long data fetch time with the on-going octave computation for better overall computation performance. Compared to other state-of-the-art work, our proposed accelerator achieved $$1.98\times$$ higher computation density for MobileNetV2 implementation on XC7ZU9EG, as well as about 83.9% lesser power for ResNet-50 implementation on VU9P. Besides octave convolution, the proposed accelerator can also support regular convolutions which are already widely used, allowing the flexibility to satisfy different types of applications. This allows a more efficient and lightweight CNN to be deployed on edge devices with a faster response time and lower power consumption.},
  archive      = {J_NCA},
  author       = {See, Jin-Chuan and Ng, Hui-Fuang and Tan, Hung-Khoon and Chang, Jing-Jing and Lee, Wai-Kong},
  doi          = {10.1007/s00521-025-11209-2},
  journal      = {Neural Computing and Applications},
  month        = {7},
  number       = {19},
  pages        = {13497-13524},
  shortjournal = {Neural Comput. Appl.},
  title        = {A compact and flexible FPGA accelerator for regular and octave convolutional neural networks},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). HiSkew: A novel histogram skewness classification applied to face anti-spoofing. <em>NCA</em>, <em>37</em>(19), 13469-13496. (<a href='https://doi.org/10.1007/s00521-025-11205-6'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Face anti-spoofing (FAS) plays a vital role in securing the face recognition systems based on video replay. Recent works have revealed the usefulness of 3D convolutional neural networks (3D-CNNs) to classify videos between bonafide and attacks. These 3D-CNN are characterized to obtain a high accuracy at expense of a high computational cost. In this context, this work presents a new strategy for visualizing and classifying video data using 3D-CNN applied to FAS challenge. It describes an original visualization method, named feature map cube visualization, based on using histograms of the inner-layer network output (feature map). This visualization method has served as the motivation for developing a novel FAS classification method, called histogram skewness classification (HiSkew), which classifies FAS videos based on skewness histograms derived from spatiotemporal feature maps. The accuracy and computational cost of HiSkew are compared with some representative spatiotemporal convolutional models of the state of the art. This experimentation reveals that HiSkew is able to obtain an accuracy similar to the state of the art with a much lower computational cost.},
  archive      = {J_NCA},
  author       = {da Silva, Vitor Luiz and Giné, Francesc and Valls, Magda},
  doi          = {10.1007/s00521-025-11205-6},
  journal      = {Neural Computing and Applications},
  month        = {7},
  number       = {19},
  pages        = {13469-13496},
  shortjournal = {Neural Comput. Appl.},
  title        = {HiSkew: A novel histogram skewness classification applied to face anti-spoofing},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Diagnosis of unbalanced bearing fault samples based on cycle generative adversarial networks. <em>NCA</em>, <em>37</em>(19), 13447-13467. (<a href='https://doi.org/10.1007/s00521-025-11265-8'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The robust feature extraction capability of deep learning (DL) has rendered it a popular choice for fault diagnosis in various industries. However, the accuracy of fault diagnosis models is often compromised due to an imbalance between normal and faulty data during the production process. To address this issue, this study proposes an unbalanced bearing fault sample diagnostic method based on a cycle generative adversarial network, using vibration fault diagnosis of rotating machinery rolling bearings as an illustrative example for validation purposes. The method utilizes continuous wavelet transform (CWT) to convert the original vibration signal into time–frequency maps, which are then utilized as input for the cycle generative adversarial network. To address issues related to unstable training and timely convergence of the model, spectral normalization and weight decay techniques are introduced, thereby enhancing the performance of the improved cycle generative adversarial network in generating additional fault samples. Finally, the Swin Transformer model is employed for fault diagnosis and compared with other methods such as Random Forest (RF), Sparse Autoencoder (SAE), Support Vector Machine (SVM), and Convolutional Neural Network (CNN). The diagnostic experiment encompasses sample expansion, imbalanced proportion between normal and faulty samples, as well as scenarios involving limited sample sizes. Experimental results demonstrate that the proposed method can generate higher quality synthetic samples when training data is scarce; moreover, utilizing the Swin Transformer for diagnosis yields superior accuracy compared to alternative approaches such as RF, SAE, SVM, or CNN models—thus showcasing significant potential in addressing imbalanced data within fault diagnosis.},
  archive      = {J_NCA},
  author       = {Huang, Rihao and Ma, Liangyu and Hu, Jingchen and Duan, Xiaochong and Gao, Haitian and Ma, Jin},
  doi          = {10.1007/s00521-025-11265-8},
  journal      = {Neural Computing and Applications},
  month        = {7},
  number       = {19},
  pages        = {13447-13467},
  shortjournal = {Neural Comput. Appl.},
  title        = {Diagnosis of unbalanced bearing fault samples based on cycle generative adversarial networks},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Adaptive dynamic programming based MPPT control for doubly-fed induction generator-wind turbine. <em>NCA</em>, <em>37</em>(19), 13429-13446. (<a href='https://doi.org/10.1007/s00521-025-11247-w'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This study presents an online adaptive dynamic programming-based controller for the variable-speed wind turbine system. The controller’s primary objective is to enable the wind turbine to effectively track the maximum power point under unknown and varying conditions. The control system consists of two main components: the adaptive optimal control component and the high-order disturbance observer-based adaptive control component. The adaptive optimal control component leverages an online adaptive dynamic programming technique, a reinforcement learning algorithm, to achieve optimal performance for the nonlinear system. Meanwhile, the high-order disturbance observer accurately estimates system uncertainties and external disturbances. These estimated disturbances are integrated into the adaptive control module to compensate for the system’s disturbances effectively. As a result, the system maintains optimal performance despite the presence of unknown disturbances. Additionally, the convergence of adaptive rules and the stability of the entire system are rigorously ensured through Lyapunov theory. Ultimately, comparative simulations are conducted in order to validate the merits of the proposed control method in comparison to existing works.},
  archive      = {J_NCA},
  author       = {Pham, Quang Dai and Nguyen, Hoang Anh and Vu, Nga Thi-Thuy},
  doi          = {10.1007/s00521-025-11247-w},
  journal      = {Neural Computing and Applications},
  month        = {7},
  number       = {19},
  pages        = {13429-13446},
  shortjournal = {Neural Comput. Appl.},
  title        = {Adaptive dynamic programming based MPPT control for doubly-fed induction generator-wind turbine},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). NiNformer: A network in network transformer with token mixing generated gating function. <em>NCA</em>, <em>37</em>(19), 13411-13428. (<a href='https://doi.org/10.1007/s00521-025-11226-1'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The attention mechanism is the primary component of the transformer architecture; it has led to significant advancements in deep learning spanning many domains and covering multiple tasks. In computer vision, the attention mechanism was first incorporated in the vision transformer (ViT), and then its usage has expanded into many tasks in the vision domain, such as classification, segmentation, object detection, and image generation. While the attention mechanism is very expressive and capable, it comes with the disadvantage of being computationally expensive and requiring datasets of considerable size for effective optimization. To address these shortcomings, many designs have been proposed in the literature to reduce the computational burden and alleviate the data size requirements. Examples of such attempts in the vision domain are the MLP-Mixer, the Conv-Mixer, the Perceiver-IO, and many more attempts with different sets of advantages and disadvantages. This paper introduces a new computational block as an alternative to the standard ViT block. The newly proposed block reduces the computational requirements by replacing the normal attention layers with a network in network structure, therefore enhancing the static approach of the MLP-Mixer with a dynamic learning of element-wise gating function generated by a token-mixing process. Extensive experimentation shows that the proposed design provides better performance than the baseline architectures on multiple datasets applied in the image classification task of the vision domain.},
  archive      = {J_NCA},
  author       = {Abdullah, Abdullah Nazhat and Aydin, Tarkan},
  doi          = {10.1007/s00521-025-11226-1},
  journal      = {Neural Computing and Applications},
  month        = {7},
  number       = {19},
  pages        = {13411-13428},
  shortjournal = {Neural Comput. Appl.},
  title        = {NiNformer: A network in network transformer with token mixing generated gating function},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Kernel-free quadratic least squares twin support vector clustering. <em>NCA</em>, <em>37</em>(19), 13391-13410. (<a href='https://doi.org/10.1007/s00521-024-10683-4'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As a support vector-based clustering approach, the twin support vector clustering (TWSVC) model has achieved great success. Equipped with a kernel function, the kernel-based TWSVC model is able to handle nonlinear clustering tasks. However, it takes much extra effort selecting a proper kernel function and tuning the hyper-parameters. In this research, a novel kernel-free nonlinear quadratic least squares TWSVC model is proposed. Without utilizing any kernel functions, it captures the clusters by directly producing quadratic surfaces. By employing the idea of TWSVC, each cluster is characterized individually, which brings a better generality of the model. In addition, the proposed model adopts the least squares constraints so that the clustering results can be efficiently obtained by solving a series of linear systems of equations. Computational experiments on artificial and public benchmark datasets are conducted to validate the clustering performance of the proposed model. Moreover, the proposed model is applied to the steel clustering.},
  archive      = {J_NCA},
  author       = {Gao, Zheming and Fu, Haojie and Huang, Min and Luo, Jian},
  doi          = {10.1007/s00521-024-10683-4},
  journal      = {Neural Computing and Applications},
  month        = {7},
  number       = {19},
  pages        = {13391-13410},
  shortjournal = {Neural Comput. Appl.},
  title        = {Kernel-free quadratic least squares twin support vector clustering},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Valley-loss multiple birth support vector machine for multi-class classification. <em>NCA</em>, <em>37</em>(19), 13371-13389. (<a href='https://doi.org/10.1007/s00521-025-11237-y'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In daily life, real data are often affected by various noises, including feature noise and labeling noise. The valley loss function inherits the merits of pinball loss function and ramp loss function, which is not only robust to feature noise and label noise, but also has good sparsity. The multiple birth support vector machine (MBSVM) has a faster training speed in solving multi-classification problems, which has an obvious advantage over other multi-classification algorithms. However, due to the use of hinge loss function in MBSVM, it is sensitive to outliers and unstable to re-sampling. To tackle these problems, we propose a novel multiple birth support vector machine with a valley loss function (VMBSVM). We further theoretically analyze the noise insensitivity and sparsity of VMBSVM and discuss the computational complexity of VMBSVM. Since the valley loss function is difficult to optimize due to its non-convex property, we employ a concave-convex procedure (CCCP) to solve VMBSVM. A host of experiments are conducted to verify the effectiveness of our proposed VMBSVM.},
  archive      = {J_NCA},
  author       = {Li, Xue and Zhang, Jiaqi and Yang, Hu},
  doi          = {10.1007/s00521-025-11237-y},
  journal      = {Neural Computing and Applications},
  month        = {7},
  number       = {19},
  pages        = {13371-13389},
  shortjournal = {Neural Comput. Appl.},
  title        = {Valley-loss multiple birth support vector machine for multi-class classification},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Trajectory tracking of a SCARA robot using intelligent active force control. <em>NCA</em>, <em>37</em>(19), 13345-13370. (<a href='https://doi.org/10.1007/s00521-025-11200-x'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Trajectory tracking with disturbance rejection is a challenging problem in robotics, particularly in applications involving selective compliance articulated robot arms (SCARA). In this paper, we address the trajectory tracking problem with the presence of disturbances in applying SCARA, by designing controllers with active force control (AFC)-based control methods. AFC has shown potential in disturbance rejection, and its per efficiency of the designed controllers, we integrated different machine learning techniques into the AFC controller, including iterative learning (IL), adaptive neuro-fuzzy inference system (ANFIS) and reinforcement learning (RL). Two case studies were conducted and compared with two different benchmark controllers to validate intelligent AFC-based controllers: a port-controlled Hamiltonian (PCH) control and a hybrid proportional-integral-derivative (PID) control. The results demonstrate that the AFC-based controllers consistently outperform the benchmark methods. Specifically, in Case 1, the AFC-RL controller achieves a 99.99% improvement in root mean square error for joint 1 compared to the hybrid PID control. In Case 2, the AFC-RL controller outperforms the AFC-IL controller in trajectory tracking accuracy by 98.71%. Also, disturbance rejection ability was tested on the AFC-based controllers with various types of disturbances. Among the three AFC-based controllers, AFC-RL shows the best performance. The findings highlight the potential of integrating machine learning into AFC for more accurate and efficient robotic control.},
  archive      = {J_NCA},
  author       = {Huang, Hanyi and Arogbonlo, Adetokunbo and Yu, Samson and Kwek, Lee Chung and Lim, Chee Peng},
  doi          = {10.1007/s00521-025-11200-x},
  journal      = {Neural Computing and Applications},
  month        = {7},
  number       = {19},
  pages        = {13345-13370},
  shortjournal = {Neural Comput. Appl.},
  title        = {Trajectory tracking of a SCARA robot using intelligent active force control},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A hybrid ensemble framework for integrating environmental parameters for improved crop yield prediction and sustainable agricultural decision-making in iraq. <em>NCA</em>, <em>37</em>(19), 13321-13343. (<a href='https://doi.org/10.1007/s00521-025-11221-6'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This study introduces the Hybrid Recurrent Ensemble Depth (HyRED) framework to enhance crop yield prediction in Iraq by integrating diverse environmental and agricultural parameters. The analysis utilises the Valued Agricultural Grounds (VAG) dataset, a novel resource developed exclusively for this study, comprising data from 1034 agricultural plots in Wasit Governorate, Iraq. This unique dataset combines soil properties, agricultural practices, meteorological data, geographic information, and remote sensing indicators such as vegetation health and biomass indices. By using this comprehensive dataset, HyRED employs an advanced hierarchical ensemble structure that integrates memory-based and pattern-recognition modules to capture both short-term temporal patterns and long-term dependencies. This approach achieves high predictive accuracy (MSE = 0.432) while reducing overfitting and enhancing generalisability. By using this novel dataset, remote sensing indicators, and advanced predictive tools, this study offers actionable recommendations for improving agricultural productivity and food security in Iraq, setting a new benchmark for sustainable agriculture research. Notable insights include the identification of optimal fertiliser rates: 60 kg / IQ Dunum for diammonium phosphate (DAP) and 100 kg / IQ Dunum for nitrogen, which increased productivity by up to 25% compared to conventional practices. Positive correlations with nitrogen (0.8985) and DAP (0.7853) underscore their importance, while negative correlations, such as soil electrical conductivity (− 0.9220) and extreme temperatures (− 0.5967), highlight environmental challenges. Integrating remote sensing indicators revealed critical growth stages and stress patterns, providing actionable insights to optimise crop management. This study sets a benchmark for sustainable agriculture by combining innovative predictive tools with a novel dataset to improve productivity and food security in Iraq.},
  archive      = {J_NCA},
  author       = {Albaaji, Ghassan Faisal and Chandra S S, Vinod},
  doi          = {10.1007/s00521-025-11221-6},
  journal      = {Neural Computing and Applications},
  month        = {7},
  number       = {19},
  pages        = {13321-13343},
  shortjournal = {Neural Comput. Appl.},
  title        = {A hybrid ensemble framework for integrating environmental parameters for improved crop yield prediction and sustainable agricultural decision-making in iraq},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A digital pen-based writing state recognition algorithm for student performance assessment. <em>NCA</em>, <em>37</em>(19), 13309-13319. (<a href='https://doi.org/10.1007/s00521-024-09955-w'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Technology-enhanced learning is an irresistible trend in intelligent education. However, most digital pen-based studies focus on handwriting character recognition, writing behavior research are extremely scarce. In this work, we prototype an embedded digital pen aimed at classifying students’ writing behaviors. Utilizing state recognition, feature extraction and optimized k-means modeling method, we present a WSR (Writing State Recognition) algorithm. WSR can classify writing and short-writing indexes. One hundred and eighteen juniors participated in the algorithm validation. Experiment results show that writing behaviors are strongly correlated with the test scores. Our proposed WSR algorithm can help teachers grasp students’ writing status, assess performance and acquaint learning emotions. The digital pen-based assistant application can shed light on personalized teaching and also has great prospects in the future education.},
  archive      = {J_NCA},
  author       = {Han, Laiquan and Pan, Bo and Chen, Ying and Tang, Jianhua},
  doi          = {10.1007/s00521-024-09955-w},
  journal      = {Neural Computing and Applications},
  month        = {7},
  number       = {19},
  pages        = {13309-13319},
  shortjournal = {Neural Comput. Appl.},
  title        = {A digital pen-based writing state recognition algorithm for student performance assessment},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multiple-choice question generation and difficulty calculations based on semantic similarity. <em>NCA</em>, <em>37</em>(19), 13295-13307. (<a href='https://doi.org/10.1007/s00521-024-10671-8'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multiple-choice questions (MCQs) are very common in students’ exams. Constructing good test questions is the main goal of automatic MCQ generation. To solve the problem of generating multiple-choice questions of different levels of difficulty for the same question, the contribution of this paper is to propose a method to automatically generate multiple-choice questions of different difficulty levels more accurately. To better determine the difficulty of multiple-choice questions, we quantified the difficulty of test questions and used the similarity between the distractors of multiple-choice questions and the correct answers as the influencing factor of the difficulty of test questions. We also improved upon the classical similarity algorithm. Topological weights are added to the algorithm, and good results are obtained. The system can also control the difficulty level through the different levels of answers divided by the database.},
  archive      = {J_NCA},
  author       = {Zhu, Junjie and Liu, Dongfeng and Chen, Silun},
  doi          = {10.1007/s00521-024-10671-8},
  journal      = {Neural Computing and Applications},
  month        = {7},
  number       = {19},
  pages        = {13295-13307},
  shortjournal = {Neural Comput. Appl.},
  title        = {Multiple-choice question generation and difficulty calculations based on semantic similarity},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). DualRW: A dual fusion network for rating quicksketch works. <em>NCA</em>, <em>37</em>(19), 13283-13294. (<a href='https://doi.org/10.1007/s00521-024-10623-2'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Quicksketch rating is a crucial component of the China Fine Arts College Entrance Examination and an important measure for evaluating the quality of students’ quicksketch works. With the advancements in artificial intelligence, intelligent rating of quicksketches has emerged as a promising and challenging task. However, very few studies have applied computer technology to the rating of quicksketch works. Most existing methods focus on the recognition of fine art works using hand-crafted or deep features extracted. In this paper, we propose a novel architecture named Dual Fusion Network for the rating of quicksketch works, which utilizes both position information and channel information. We collect 2442 quicksketches from the Guangdong Art Joint Examination and construct a dataset named SCNU-QuickSketch. The proposed architecture comprises a multi-scale feature extraction module, position attention subnetwork, and channel interaction subnetwork. The multi-scale feature extraction module extracts various feature maps from the convolutional neural network. This process effectively aggregates global information for the position attention and channel interaction subnetworks. The position attention subnetwork strengthens the interaction of different position information of the feature map, and the channel interaction subnetwork enhances the representation ability of specific semantics to achieve more detailed details of quicksketch works. The experimental results show that the Dual Fusion Network achieves 79.9% accuracy on the SCNU-QuickSketch dataset, which is 5.8% and 3.5% higher than the ResNet50 and Swin-B, respectively, and visualization shows the effectiveness of our proposed subnetworks. This study provides insights into the application of computer technology in the evaluation process.},
  archive      = {J_NCA},
  author       = {Liang, Jun and Kuang, Xiaoyang and Su, Hai},
  doi          = {10.1007/s00521-024-10623-2},
  journal      = {Neural Computing and Applications},
  month        = {7},
  number       = {19},
  pages        = {13283-13294},
  shortjournal = {Neural Comput. Appl.},
  title        = {DualRW: A dual fusion network for rating quicksketch works},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A video course enhancement technique utilizing generated talking heads. <em>NCA</em>, <em>37</em>(19), 13267-13282. (<a href='https://doi.org/10.1007/s00521-024-10608-1'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the field of intelligent education, course videos integrate an instructor’s image, voice, and instructional content. These videos play a vital role in the teaching and learning process, but their production requires a substantial investment of time and effort from instructors. To reduce the stress experienced by instructors during lesson preparation, we propose a course video generation solution that is specifically designed to efficiently produce high-quality educational videos. Unlike traditional methods, our solution requires only the instructor to record a video of the course screen and provide an image of themselves, such as a photo or short video. Our method can be divided into two modules: the talking head generation module and the fusion module for generated video and screen-recorded video. In regard to the crucial talking head generation module, existing methods have certain limitations in terms of clarity and naturalness. Consequently, there is a need for further improvement in this area. To address these problems, we propose a transformer-based network to generate talking head videos with the instructor’s image and then combine screen-recorded video and talking head video to obtain the final course video. We performed separate comparison experiments and ablation experiments on the talking head videos obtained using our proposed method. In the comparison experiments, we compared different methods, all of which yielded better results. In the ablation experiments, we presented the methods we used as well as some optimization modules and compared them in terms of objective metrics. Moreover, we also conducted survey experiments with instructors and students to demonstrate the effectiveness of the generated videos using our framework. In addition, we have developed an application through a series of processes using our framework. The application has been deployed on the smart education platform of our university, which has received good feedback.},
  archive      = {J_NCA},
  author       = {Lu, Zixiang and Tian, Bujia and Gao, Ping and Miao, Qiguang and Xie, Kun and Liu, Ruyi and Quan, Yining},
  doi          = {10.1007/s00521-024-10608-1},
  journal      = {Neural Computing and Applications},
  month        = {7},
  number       = {19},
  pages        = {13267-13282},
  shortjournal = {Neural Comput. Appl.},
  title        = {A video course enhancement technique utilizing generated talking heads},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). An unsupervised medical image registration network for intelligent medical education. <em>NCA</em>, <em>37</em>(19), 13251-13266. (<a href='https://doi.org/10.1007/s00521-024-10585-5'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Intelligent education, which applies artificial intelligence technologies to the education field, has attracted increasing attention in recent years. Intelligent medical education is an effective learning way to assist medical practitioners in mastering complex medical knowledge through artificial intelligence technologies like big data analysis and machine learning. With the development of medical imaging technology, the types of medical images are becoming abundant, such as computed tomography image and magnetic resonance imaging image. Medical experts with long clinical experience can analyze the patient’s condition from different types of medical images to achieve more accurate diagnosis. Nevertheless, cultivating an expert who can accurately analyze different types of medical images is time-consuming and laborious. Therefore, it is necessary to utilize the intelligent medical education way to design medical image processing method that can guide medical practitioners in simultaneously accurately analyzing different types of medical images. The medical image registration, which can automatically align different types of medical images and guide medical practitioners in diagnosing, has become popular. However, existing methods lack labeled training data and ignore interactive information between registered images. To solve these problems, we propose a cross-view interactive network (CVIN). First, CVIN utilizes alternate stages with information interaction modules to extract data features, which can use unlabeled medical data to train the model. Second, we introduce an InfoMax module to learn the mutual information between registered images. The experiment results show that CVIN is promising and can use to guide the medical practitioners to conduct accurate disease diagnosis.},
  archive      = {J_NCA},
  author       = {Shi, Ke and Mu, Jie and Xu, Jian and Zhang, Jing and Yan, Tiantian and Wang, Wei and Zhang, Hua and Ren, Wenqi},
  doi          = {10.1007/s00521-024-10585-5},
  journal      = {Neural Computing and Applications},
  month        = {7},
  number       = {19},
  pages        = {13251-13266},
  shortjournal = {Neural Comput. Appl.},
  title        = {An unsupervised medical image registration network for intelligent medical education},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Research on the impact of pointing gestures based on computer vision technology on classroom concentration. <em>NCA</em>, <em>37</em>(19), 13237-13249. (<a href='https://doi.org/10.1007/s00521-024-10529-z'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Classroom concentration is an essential manifestation of learners’ engagement in the classroom, and it is a critical factor in adjusting learning states and optimizing teaching processes. A thorough exploration of the factors that affect classroom concentration and their effects is of great significance for enhancing it. This study proposes a classroom concentration recognition model by integrating the two dimensions of emotional and behavioral concentration using computer vision technology. The model’s effectiveness in recognizing classroom concentration is verified through a comparative experiment with EEG equipment. Based on this model, this study further investigates the impact of teachers’ pointing gestures on learners’ classroom concentration. The results of ANOVA show that when teachers use pointing gestures, they can improve learners’ concentration in class in a short time, but this positive effect has boundary effects. Our research results can help improve teachers’ teaching behaviors and enhance learners’ learning effects to some extent.},
  archive      = {J_NCA},
  author       = {Shi, Jianyang and Chen, Zhangze and Zhu, Jia and Zhou, Jian and Wang, Qing and Ma, Xiaodong},
  doi          = {10.1007/s00521-024-10529-z},
  journal      = {Neural Computing and Applications},
  month        = {7},
  number       = {19},
  pages        = {13237-13249},
  shortjournal = {Neural Comput. Appl.},
  title        = {Research on the impact of pointing gestures based on computer vision technology on classroom concentration},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Hammer: Multi-level coordination of reinforcement learning agents via learned messaging. <em>NCA</em>, <em>37</em>(19), 13221-13236. (<a href='https://doi.org/10.1007/s00521-023-09096-6'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cooperative multi-agent reinforcement learning (MARL) has achieved significant results, most notably by leveraging the representation-learning abilities of deep neural networks. However, large centralized approaches quickly become infeasible as the number of agents scale, and fully decentralized approaches can miss important opportunities for information sharing and coordination. Furthermore, not all agents are equal—in some cases, individual agents may not even have the ability to send communication to other agents or explicitly model other agents. This paper considers the case where there is a single, powerful, central agent that can observe the entire observation space, and there are multiple, low-powered local agents that can only receive local observations and are not able to communicate with each other. The central agent’s job is to learn what message needs to be sent to different local agents based on the global observations, not by centrally solving the entire problem and sending action commands, but by determining what additional information an individual agent should receive so that it can make a better decision. In this work, we present our MARL algorithm hammer, describe where it would be most applicable, and implement it in the cooperative navigation and multi-agent walker domains. Empirical results show that (1) learned communication does indeed improve system performance, (2) results generalize to heterogeneous local agents, and (3) results generalize to different reward structures.},
  archive      = {J_NCA},
  author       = {Gupta, Nikunj and Srinivasaraghavan, G. and Mohalik, Swarup and Kumar, Nishant and Taylor, Matthew E.},
  doi          = {10.1007/s00521-023-09096-6},
  journal      = {Neural Computing and Applications},
  month        = {7},
  number       = {19},
  pages        = {13221-13236},
  shortjournal = {Neural Comput. Appl.},
  title        = {Hammer: Multi-level coordination of reinforcement learning agents via learned messaging},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A maintenance planning framework using online and offline deep reinforcement learning. <em>NCA</em>, <em>37</em>(19), 13209-13220. (<a href='https://doi.org/10.1007/s00521-023-08560-7'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cost-effective asset management is an area of interest across several industries. Specifically, this paper develops a deep reinforcement learning (DRL) solution to automatically determine an optimal rehabilitation policy for continuously deteriorating water pipes. We approach the problem of rehabilitation planning in an online and offline DRL setting. In online DRL, the agent interacts with a simulated environment of multiple pipes with distinct lengths, materials, and failure rate characteristics. We train the agent using deep Q-learning (DQN) to learn an optimal policy with minimal average costs and reduced failure probability. In offline learning, the agent uses static data, e.g., DQN replay data, to learn an optimal policy via a conservative Q-learning algorithm without further interactions with the environment. We demonstrate that DRL-based policies improve over standard preventive, corrective, and greedy planning alternatives. Additionally, learning from the fixed DQN replay dataset in an offline setting further improves the performance. The results warrant that the existing deterioration profiles of water pipes consisting of large and diverse states and action trajectories provide a valuable avenue to learn rehabilitation policies in the offline setting, which can be further fine-tuned using the simulator.},
  archive      = {J_NCA},
  author       = {Bukhsh, Zaharah A. and Molegraaf, Hajo and Jansen, Nils},
  doi          = {10.1007/s00521-023-08560-7},
  journal      = {Neural Computing and Applications},
  month        = {7},
  number       = {19},
  pages        = {13209-13220},
  shortjournal = {Neural Comput. Appl.},
  title        = {A maintenance planning framework using online and offline deep reinforcement learning},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Graph learning-based generation of abstractions for reinforcement learning. <em>NCA</em>, <em>37</em>(19), 13187-13207. (<a href='https://doi.org/10.1007/s00521-023-08211-x'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The application of reinforcement learning (RL) algorithms is often hindered by the combinatorial explosion of the state space. Previous works have leveraged abstractions which condense large state spaces to find tractable solutions. However, they assumed that the abstractions are provided by a domain expert. In this work, we propose a new approach to automatically construct abstract Markov decision processes (AMDPs) for potential-based reward shaping to improve the sample efficiency of RL algorithms. Our approach to constructing abstract states is inspired by graph representation learning methods, it effectively encodes the topological and reward structure of the ground-level MDP. We perform large-scale quantitative experiments on a range of navigation and gathering tasks under both stationary and stochastic settings. Our approach shows improvements of up to 8.5 times in sample efficiency and up to 3 times in run time over the baseline approach. Besides, with our qualitative analyses of the generated AMDPs, we are able to visually demonstrate the capability of our approach to preserve the topological and reward structure of the ground-level MDP.},
  archive      = {J_NCA},
  author       = {Xue, Yuan and Kudenko, Daniel and Khosla, Megha},
  doi          = {10.1007/s00521-023-08211-x},
  journal      = {Neural Computing and Applications},
  month        = {7},
  number       = {19},
  pages        = {13187-13207},
  shortjournal = {Neural Comput. Appl.},
  title        = {Graph learning-based generation of abstractions for reinforcement learning},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Difference rewards policy gradients. <em>NCA</em>, <em>37</em>(19), 13163-13186. (<a href='https://doi.org/10.1007/s00521-022-07960-5'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Policy gradient methods have become one of the most popular classes of algorithms for multi-agent reinforcement learning. A key challenge, however, that is not addressed by many of these methods is multi-agent credit assignment: assessing an agent’s contribution to the overall performance, which is crucial for learning good policies. We propose a novel algorithm called Dr.Reinforce that explicitly tackles this by combining difference rewards with policy gradients to allow for learning decentralized policies when the reward function is known. By differencing the reward function directly, Dr.Reinforce avoids difficulties associated with learning the Q-function as done by counterfactual multi-agent policy gradients (COMA), a state-of-the-art difference rewards method. For applications where the reward function is unknown, we show the effectiveness of a version of Dr.Reinforce that learns an additional reward network that is used to estimate the difference rewards.},
  archive      = {J_NCA},
  author       = {Castellini, Jacopo and Devlin, Sam and Oliehoek, Frans A. and Savani, Rahul},
  doi          = {10.1007/s00521-022-07960-5},
  journal      = {Neural Computing and Applications},
  month        = {7},
  number       = {19},
  pages        = {13163-13186},
  shortjournal = {Neural Comput. Appl.},
  title        = {Difference rewards policy gradients},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Influence-aware memory architectures for deep reinforcement learning in POMDPs. <em>NCA</em>, <em>37</em>(19), 13145-13161. (<a href='https://doi.org/10.1007/s00521-022-07691-7'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Due to its perceptual limitations, an agent may have too little information about the environment to act optimally. In such cases, it is important to keep track of the action-observation history to uncover hidden state information. Recent deep reinforcement learning methods use recurrent neural networks (RNN) to memorize past observations. However, these models are expensive to train and have convergence difficulties, especially when dealing with high dimensional data. In this paper, we propose influence-aware memory, a theoretically inspired memory architecture that alleviates the training difficulties by restricting the input of the recurrent layers to those variables that influence the hidden state information. Moreover, as opposed to standard RNNs, in which every piece of information used for estimating Q values is inevitably fed back into the network for the next prediction, our model allows information to flow without being necessarily stored in the RNN’s internal memory. Results indicate that, by letting the recurrent layers focus on a small fraction of the observation variables while processing the rest of the information with a feedforward neural network, we can outperform standard recurrent architectures both in training speed and policy performance. This approach also reduces runtime and obtains better scores than methods that stack multiple observations to remove partial observability.},
  archive      = {J_NCA},
  author       = {Suau, Miguel and He, Jinke and Congeduti, Elena and Starre, Rolf A. N. and Czechowski, Aleksander and Oliehoek, Frans A.},
  doi          = {10.1007/s00521-022-07691-7},
  journal      = {Neural Computing and Applications},
  month        = {7},
  number       = {19},
  pages        = {13145-13161},
  shortjournal = {Neural Comput. Appl.},
  title        = {Influence-aware memory architectures for deep reinforcement learning in POMDPs},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Preference communication in multi-objective normal-form games. <em>NCA</em>, <em>37</em>(19), 13119-13144. (<a href='https://doi.org/10.1007/s00521-022-07533-6'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider preference communication in two-player multi-objective normal-form games. In such games, the payoffs resulting from joint actions are vector-valued. Taking a utility-based approach, we assume there exists a utility function for each player which maps vectors to scalar utilities and consider agents that aim to maximise the utility of expected payoff vectors. As agents typically do not know their opponent’s utility function or strategy, they must learn policies to interact with each other. Inspired by Stackelberg games, we introduce four novel preference communication protocols to aid agents in arriving at adequate solutions. Each protocol describes a specific approach for one agent to communicate preferences over their actions and how another agent responds. Additionally, to study when communication emerges, we introduce a communication protocol where agents must learn when to communicate. These protocols are subsequently evaluated on a set of five benchmark games against baseline agents that do not communicate. We find that preference communication can alter the learning process and lead to the emergence of cyclic policies which had not been previously observed in this setting. We further observe that the resulting policies can heavily depend on the characteristics of the game that is played. Lastly, we find that communication naturally emerges in both cooperative and self-interested settings.},
  archive      = {J_NCA},
  author       = {Röpke, Willem and Roijers, Diederik M. and Nowé, Ann and Rădulescu, Roxana},
  doi          = {10.1007/s00521-022-07533-6},
  journal      = {Neural Computing and Applications},
  month        = {7},
  number       = {19},
  pages        = {13119-13144},
  shortjournal = {Neural Comput. Appl.},
  title        = {Preference communication in multi-objective normal-form games},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A framework for flexibly guiding learning agents. <em>NCA</em>, <em>37</em>(19), 13101-13117. (<a href='https://doi.org/10.1007/s00521-022-07396-x'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Reinforcement Learning (RL) enables artificial agents to learn through direct interaction with the environment. However, it usually does not scale up well to large problems due to its sampling inefficiency. Reward Shaping is a well-established approach that allows for more efficient learning by incorporating domain knowledge in RL agents via supplementary rewards. In this work, we propose a novel methodology that automatically generates reward shaping functions from user-provided Linear Temporal Logic on finite traces ( $${\text {LTL}}_{\rm f}$$ ) formulas. $${\text {LTL}}_{\rm f}$$ in our work serves as a rich language that allows the user to communicate domain knowledge to the learning agent. In both single and multi-agent settings, we demonstrate that our approach performs at least as well as the baseline approach while providing essential advantages in terms of flexibility and ease of use. We elaborate on some of these advantages empirically by demonstrating that our approach can handle domain knowledge with different levels of accuracy, and provides the user with the flexibility to express aspects of uncertainty in the provided advice.},
  archive      = {J_NCA},
  author       = {Elbarbari, Mahmoud and Delgrange, Florent and Vervlimmeren, Ivo and Efthymiadis, Kyriakos and Vanderborght, Bram and Nowé, Ann},
  doi          = {10.1007/s00521-022-07396-x},
  journal      = {Neural Computing and Applications},
  month        = {7},
  number       = {19},
  pages        = {13101-13117},
  shortjournal = {Neural Comput. Appl.},
  title        = {A framework for flexibly guiding learning agents},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Expected scalarised returns dominance: A new solution concept for multi-objective decision making. <em>NCA</em>, <em>37</em>(19), 13079-13099. (<a href='https://doi.org/10.1007/s00521-022-07334-x'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In many real-world scenarios, the utility of a user is derived from a single execution of a policy. In this case, to apply multi-objective reinforcement learning, the expected utility of the returns must be optimised. Various scenarios exist where a user’s preferences over objectives (also known as the utility function) are unknown or difficult to specify. In such scenarios, a set of optimal policies must be learned. However, settings where the expected utility must be maximised have been largely overlooked by the multi-objective reinforcement learning community and, as a consequence, a set of optimal solutions has yet to be defined. In this work, we propose first-order stochastic dominance as a criterion to build solution sets to maximise expected utility. We also define a new dominance criterion, known as expected scalarised returns (ESR) dominance, that extends first-order stochastic dominance to allow a set of optimal policies to be learned in practice. Additionally, we define a new solution concept called the ESR set, which is a set of policies that are ESR dominant. Finally, we present a new multi-objective tabular distributional reinforcement learning (MOTDRL) algorithm to learn the ESR set in multi-objective multi-armed bandit settings.},
  archive      = {J_NCA},
  author       = {Hayes, Conor F. and Verstraeten, Timothy and Roijers, Diederik M. and Howley, Enda and Mannion, Patrick},
  doi          = {10.1007/s00521-022-07334-x},
  journal      = {Neural Computing and Applications},
  month        = {7},
  number       = {19},
  pages        = {13079-13099},
  shortjournal = {Neural Comput. Appl.},
  title        = {Expected scalarised returns dominance: A new solution concept for multi-objective decision making},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multi-objective optimization and predictive analytics of strength and embodied impacts of CDW-based geopolymers using various machine learning approaches. <em>NCA</em>, <em>37</em>(18), 12791-12823. (<a href='https://doi.org/10.1007/s00521-025-11191-9'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Geopolymers generally exhibit heterogeneous composition, variability and complex interactions, posing significant challenges in predicting their properties. This study focuses on the multi-objective optimization (MOO) and predictive analytics of geopolymer mortars (GPMs) incorporating construction and demolition wastes (CDWs) as recycled precursors and aggregates. The objectives are to predict and optimize the compressive strength (CS) while minimizing the embodied energy (E-energy) and CO2 (E-CO2) emissions. To achieve these goals, various interpretable ensemble machine learning (ML) models, including linear regressors, advanced tree-based methods, K-nearest neighbors and neural network-based multi-layer perceptron regressors, are employed. The standalone extreme gradient boosting model achieved the best performance, attaining the highest predictive accuracy (R2 = 0.939, 0.937 and 0.843 for CS, E-energy and E-CO2, respectively) with minimal error indices. Furthermore, a MOO framework utilizing advanced sorting genetic algorithms was employed to generate a Pareto front of optimal solutions for maximized CS and minimized E-energy and E-CO2. SHapley Additive exPlanations and sensitivity analyses identified SiO2/Al2O3, curing temperature and recycled concrete aggregate as the three most influential parameters contributing to the output prediction of ML models. Experimental validation confirmed a high-level predictive accuracy for the proposed multi-objective optimization framework and ML models, demonstrating the possibility of reaching strengths varying from 30 to 58 MPa, E-CO2 from 90 to 150 kg CO2/m3 and E-energy from 300 to 500 MJ/m3 for GPMs prepared with CDW-based binders and aggregates.},
  archive      = {J_NCA},
  author       = {Mahmoodi, Obaid and Siad, Hocine and Lachemi, Mohamed and Şahmaran, Mustafa},
  doi          = {10.1007/s00521-025-11191-9},
  journal      = {Neural Computing and Applications},
  month        = {6},
  number       = {18},
  pages        = {12791-12823},
  shortjournal = {Neural Comput. Appl.},
  title        = {Multi-objective optimization and predictive analytics of strength and embodied impacts of CDW-based geopolymers using various machine learning approaches},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). SGGAA: A robust adversarial generation based on synthesizing point cloud and mesh space. <em>NCA</em>, <em>37</em>(18), 12767-12789. (<a href='https://doi.org/10.1007/s00521-025-11182-w'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {3D vision recognition offers a significantly more robust tool for achieving machine cognition compared to traditional 2D vision techniques. However, similar to the vulnerabilities present in 2D vision, many 3D vision recognition applications, including point cloud classification models, are susceptible to degradation when confronted with imperceptible outliers. Outliers have a considerable impact on the performance and accuracy of models, compromising their overall effectiveness. Existing 3D point cloud attack and defense strategies neglect outliers and overlook the crucial impact of 3D printability in real-world scenarios. Neglecting this aspect hinders the consideration of potential manipulation or alteration of 3D objects in the physical realm, posing significant implications for the security and robustness of 3D vision systems. This manuscript presents a novel and effective targeted attack framework called synthesizing geometry and geodesic attack (SGGAA). It offers an alternative adversarial training algorithm that bridges the gap between the digital and physical domains. SGGAA introduces a novel generalized distance metric, facilitating the design of powerful and simple techniques for generating adversarial samples and improving the training process. It focuses on the regularizer loss object in synthesizing attack modes for mesh and point cloud spaces, utilizing Hausdorff distance, geometry distance, and geodesic distance losses. Comprehensive evaluations, including hyperparameter tuning, qualitative analysis, and quantitative experiments, were conducted to assess SGGAA’s performance. The results demonstrated its superiority, with high attack success rates, excellent geometric similarity scores, and outperformance of state-of-the-art attack strategies across various 3D defense techniques.},
  archive      = {J_NCA},
  author       = {Hu, Ruihan and Yang, Rui},
  doi          = {10.1007/s00521-025-11182-w},
  journal      = {Neural Computing and Applications},
  month        = {6},
  number       = {18},
  pages        = {12767-12789},
  shortjournal = {Neural Comput. Appl.},
  title        = {SGGAA: A robust adversarial generation based on synthesizing point cloud and mesh space},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). SmartGrid AI: Enhancing home energy management with blockchain and deep learning for renewable integration. <em>NCA</em>, <em>37</em>(18), 12735-12766. (<a href='https://doi.org/10.1007/s00521-025-11181-x'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {SmartGrid AI revolutionizes home energy management by combining blockchain technology, deep learning, and vehicle-to-home (V2H) technology. It uses neural network-based Q-learning algorithms refined by advanced data preprocessing techniques such as normalization, standardization, and missing value imputation. These preprocessing steps ensure high-quality, accurate data for appliance planning and efficient energy storage management. V2H technology enables efficient energy transfer between electric vehicles and homes, optimizing energy consumption and reducing waste. Blockchain technology is used to secure and verify transactions, ensuring that all data exchanges are transparent and reliable. Real-time data from photovoltaic systems is integrated to improve the accuracy of energy management decisions. Using data from a Tunisian weather database, SmartGrid AI enables a significant 23% reduction in monthly electricity costs compared to traditional methods such as integer linear programming. The system’s advanced capabilities are demonstrated by its robust AI models, comprehensive performance metrics, efficient simulation models, and seamless integration of renewable energy sources. SmartGrid AI provides a cutting-edge solution for efficient and cost-effective home energy management.},
  archive      = {J_NCA},
  author       = {Ben Slama, Sami Abdullah and Dahmani, Nadia},
  doi          = {10.1007/s00521-025-11181-x},
  journal      = {Neural Computing and Applications},
  month        = {6},
  number       = {18},
  pages        = {12735-12766},
  shortjournal = {Neural Comput. Appl.},
  title        = {SmartGrid AI: Enhancing home energy management with blockchain and deep learning for renewable integration},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Designing a skilled soccer team for RoboCup: Exploring skill-set-primitives through reinforcement learning. <em>NCA</em>, <em>37</em>(18), 12699-12734. (<a href='https://doi.org/10.1007/s00521-025-11151-3'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The RoboCup 3D soccer simulation league serves as a competitive platform for showcasing innovation in autonomous humanoid robot agents through simulated soccer matches. Our team, FC Portugal, developed a new codebase from scratch in Python after RoboCup 2021. The team’s performance relies on a set of skills centered around novel unifying primitives and a custom, symmetry-extended version of the proximal policy optimization algorithm. Our methods have been thoroughly tested in official RoboCup matches, where FC Portugal has won the last two main competitions, in 2022 and 2023. This paper presents our training framework, as well as a timeline of skills developed using our skill-set-primitives, which considerably improve the sample efficiency and stability of skills, and motivate seamless transitions. We start with a significantly fast Sprint-Kick developed in 2021 and progress to the most recent skill set, including a multi-purpose omnidirectional walk, a dribble with unprecedented ball control, a solid kick, and a push skill. The push addresses low-level collision scenarios and high-level strategies to increase ball possession. We address the resource-intensive nature of this task through an innovative multi-agent learning approach. Finally, we release the team’s codebase to the RoboCup community, providing other teams with a robust and modern foundation upon which they can build new features.},
  archive      = {J_NCA},
  author       = {Abreu, Miguel and Reis, Luís Paulo and Lau, Nuno},
  doi          = {10.1007/s00521-025-11151-3},
  journal      = {Neural Computing and Applications},
  month        = {6},
  number       = {18},
  pages        = {12699-12734},
  shortjournal = {Neural Comput. Appl.},
  title        = {Designing a skilled soccer team for RoboCup: Exploring skill-set-primitives through reinforcement learning},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). MFMAM-YOLO: A method for detecting pole-like obstacles in complex environments. <em>NCA</em>, <em>37</em>(18), 12673-12698. (<a href='https://doi.org/10.1007/s00521-025-11136-2'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In real-world traffic, there are various uncertainties and complexities in road and weather conditions. To solve the problem that the feature information of pole-like obstacles in complex environments is easily lost, resulting in low detection accuracy and low real-time performance, a multi-scale hybrid attention mechanism detection algorithm is proposed in this paper. First, the optimal transport function Monge–Kantorovich (MK) is incorporated not only to solve the problem of overlapping multiple prediction frames with optimal matching but also the MK function can be regularized to prevent model over-fitting; then, the features at different scales are up-sampled separately according to the optimized efficient multi-scale feature pyramid. Finally, the extraction of multi-scale feature space channel information is enhanced in complex environments based on the hybrid attention mechanism, which suppresses the irrelevant complex environment background information and focuses the feature information of pole-like obstacles. Meanwhile, this paper conducts real road test experiments in a variety of complex environments. The experimental results show that the detection precision, recall, and average precision of the method are 94.7, 93.1, and 97.4%, respectively, and the detection frame rate is 400 f·s−1. This research method can detect pole-like obstacles in a complex road environment in real time and accurately, which further promotes innovation and progress in the field of automatic driving.},
  archive      = {J_NCA},
  author       = {Cai, Lei and Wang, Hao and Zhou, Congling and Wang, Yongqiang and Liu, Boyu},
  doi          = {10.1007/s00521-025-11136-2},
  journal      = {Neural Computing and Applications},
  month        = {6},
  number       = {18},
  pages        = {12673-12698},
  shortjournal = {Neural Comput. Appl.},
  title        = {MFMAM-YOLO: A method for detecting pole-like obstacles in complex environments},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). An adversarial network based on anomaly domain decomposition and transformation for industrial PCBA defect inspection. <em>NCA</em>, <em>37</em>(18), 12653-12671. (<a href='https://doi.org/10.1007/s00521-025-11124-6'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the industrial production of printed circuit board assembly (PCBA) products, accurate defect detection is crucial. Automated optical inspection has been the mainstream method using for detection in industry. However, traditional methods still face numerous challenges in terms of effectiveness and accuracy. In this study, we propose a novel adversarial domain transformation and abnormality decomposition network (ADTAN) for defect detection in PCBA products. Based on the assumption that normal background features and abnormal defect features can be decomposed, and that all background features from normal and abnormal images share a common latent space, our proposed ADTAN effectively detects defects in PCBA products. Through training, ADTAN not only learns to accurately reconstruct the background of defect regions, but also achieves precise defect segmentation in an end-to-end manner. We conducted a series of experiments on mainstream defect datasets and on-site collected PCBA samples. The results demonstrate that our method achieves an AUROC score of 97.8% for mainstream public datasets, and various defects were accurately detected for industrial PCBA products.},
  archive      = {J_NCA},
  author       = {Pan, Qianfeng and Liu, Teng and Hou, Yue and Chang, Jingyun and Yang, Hua},
  doi          = {10.1007/s00521-025-11124-6},
  journal      = {Neural Computing and Applications},
  month        = {6},
  number       = {18},
  pages        = {12653-12671},
  shortjournal = {Neural Comput. Appl.},
  title        = {An adversarial network based on anomaly domain decomposition and transformation for industrial PCBA defect inspection},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Automatic test-time adaptation for heterogeneous contexts in meta-learning. <em>NCA</em>, <em>37</em>(18), 12631-12652. (<a href='https://doi.org/10.1007/s00521-025-11095-8'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Meta-learning (ML) methods fail to adapt to heterogeneous contexts of unknown tasks during test-time, tend to have meta-overfitting on task-shared structures, and therefore obtain poor generalization performance. In this paper, we propose an algorithm of automatic test-time adaptation for heterogeneous contexts (ATTA-HC) in ML to mitigate meta-overfitting, facilitate minimal adaptation of heterogeneous context, and enhance interpretability. The ATTA-HC algorithm is general, data-agnostic, model-agnostic, and loss-agnostic. Specifically, ATTA-HC automatically achieves this by automatically dividing model parameters after meta-training into two distinct sets based on the quantile of parameters: task-specific parameters describing the heterogeneous contexts of tasks, which are fine-tuned for each individual task, and task-shared parameters, which are meta-trained and encode shared structures across tasks. During test-time, only the task-specific parameters require updating, resulting in a compact task representation. ATTA-HC does not need crafted structures, extra parameters, computation cost, and input manipulation. Experiments including regression, classification, and reinforcement learning show the generalization and superiority of ATTA-HC. Furthermore, our experiments shed light on potential flaws in existing benchmarks, revealing that the level of test-time adaptation can be minimal and automatic in certain scenarios.},
  archive      = {J_NCA},
  author       = {Liang, Yunsheng and Chen, Kai},
  doi          = {10.1007/s00521-025-11095-8},
  journal      = {Neural Computing and Applications},
  month        = {6},
  number       = {18},
  pages        = {12631-12652},
  shortjournal = {Neural Comput. Appl.},
  title        = {Automatic test-time adaptation for heterogeneous contexts in meta-learning},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A new CNN-based watermarking method for color medical images in a fusion domain. <em>NCA</em>, <em>37</em>(18), 12611-12629. (<a href='https://doi.org/10.1007/s00521-025-11056-1'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The transmission of medical images via medical agencies raises security concerns, necessitating increased security measures to ensure integrity and security. However, many watermarking algorithms overlook equipoise; the relation between robustness, invisibility, and payload capacity results in a less satisfactory performance. To bridge this gap, we propose a new blind watermarking method for securing medical images based on a convolutional neural network in a fusion domain. First, we transmute the host color image using a phase-only transform (PHOT) to detect the surface pattern. We feed the detected surface pattern into a pre-trained VGG19 model to impeccably extract the stable feature vector. We encrypt the extracted image features using a hybrid Chirikov map to enhance their robustness and reliability. The synergy between the pre-trained VGG19 model and PHOT in an integrated domain effectively captures additional native and localized image features. A hybrid Fibonacci Q-matrix method scrambles the binary watermark to enhance security. Integrating double encryption into the proposed method markedly enhances its resistance against diverse countermeasures. Experimental outcomes indicate that the proposed scheme performs efficiently in robustness and invisibility. The obtained PSNR value was up to 60.15 dB, which is optimal for human perception. The embedded watermark can be retrieved without any warping. The retrieved watermark seems to be authentic, exhibiting ideal BER and NC values. In almost all attack circumstances, the BER values approached zero while the NC values got closer. The proposed method demonstrates notable enhancements in robustness and invisibility compared to prior methods.},
  archive      = {J_NCA},
  author       = {Hosny, Khalid M. and Abdel-Aziz, Mostafa M. and Lashin, Nabil A. and Hamza, Hanaa M.},
  doi          = {10.1007/s00521-025-11056-1},
  journal      = {Neural Computing and Applications},
  month        = {6},
  number       = {18},
  pages        = {12611-12629},
  shortjournal = {Neural Comput. Appl.},
  title        = {A new CNN-based watermarking method for color medical images in a fusion domain},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Decomposition-based hybrid methods employing statistical, machine learning, and deep learning models for crude oil price forecasting. <em>NCA</em>, <em>37</em>(18), 12565-12610. (<a href='https://doi.org/10.1007/s00521-025-11178-6'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Crude oil prices (COP) profoundly influence global economic stability, with fluctuations reverberating across various sectors. Accurate forecasting of COP is indispensable for governments, policymakers, and stakeholders to make well-informed decisions and effectively mitigate risks. The decomposition-based hybrid models have been showing promising COP forecasting accuracy than other time series forecasting methods. Despite this fact, no systematic study has been conducted to evaluate the true potential of different decomposition-based hybrid methods employing different forecasting models to forecast the COP. Therefore, a hybrid modeling framework is developed by combining efficient decomposition techniques, namely empirical mode decomposition (EMD), ensemble EMD (EEMD), complete EEMD with adaptive noise (CEEMDAN), and variational mode decomposition (VMD) with seven statistical models, fourteen machine learning (ML) models, and six deep learning (DL) models. Further, a systematic study is conducted on the resulting decomposition-based hybrid models to find the best hybrid model for COP forecasting. Three distinct train-test data splits are employed to ensure a reliable evaluation of the models using four performance metrics. Extensive statistical analysis is conducted to identify the optimal combination of the decomposition technique and forecasting model for precise COP prediction. The results demonstrate that the proposed decomposition-based hybrid model employing VMD and Huber Regression is statistically the best method among all alternatives to forecast monthly COP. The proposed hybrid method VMD-Huber Regression improves the root mean square error (RMSE) by 21% than CEEMDAN-ARIMA, 58.31% than EEMD-Theta, 13.18% than EMD-Random Walk, and 49.44% than VMD-TBATS hybrid methods in 60–40 Train-Test split ratio.},
  archive      = {J_NCA},
  author       = {Purohit, Sourav Kumar and Panigrahi, Sibarama},
  doi          = {10.1007/s00521-025-11178-6},
  journal      = {Neural Computing and Applications},
  month        = {6},
  number       = {18},
  pages        = {12565-12610},
  shortjournal = {Neural Comput. Appl.},
  title        = {Decomposition-based hybrid methods employing statistical, machine learning, and deep learning models for crude oil price forecasting},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). SRF2T-ID: An implementation of ensemble learning-based IDS with wireless of things secure communication for smart residency environment. <em>NCA</em>, <em>37</em>(18), 12525-12564. (<a href='https://doi.org/10.1007/s00521-025-11154-0'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The widespread adoption of Internet of Things (IoT) devices in smart applications has led to the advancement of modern smart residencies. These smart residency applications heavily rely on communication and information technologies to improve operational efficiency, sustainability, and automation of home services. However, the heterogeneous nature of IoT devices introduces significant security challenges to the network, especially in the face of persistent cyber-attacks. Additionally, the proliferation of data-centric services offered by such applications also demands system privacy. This paper addresses these challenges in a distributed smart residency system by introducing a combination of federated learning-driven Intrusion Detection System with Wireless of Things-based secure communication network. Specifically, a novel two-tier security framework SRF2T-ID - a Smart Residency Federated Learning-based 2-Tier intrusion detection model is proposed. This model incorporates Advanced Encryption Standard encryption, packet division, clock synchronization, and randomization of transmission channels and receiver pipes in the inner layer corresponding to each smart home and further deploys a novel binary-multiclass classification approach in the outer layer including different nature-inspired metaheuristic-based feature selection algorithms, that secures the overall network. two different datasets has been used namely, Canadian Institute for Cybersecurity Intrusion Detection System 2017 and Canadian Institute for Cybersecurity IoT Dataset 2023. The experimental result reveals that using the Grey wolf optimization-based features, the ensemble learning framework achieves a detection accuracy of 99.81% and 99.90%, respectively. Extensive performance evaluation with respect to packet transmission success rate, packet loss, latency, throughput, accuracy, precision, recall and F1-score state the high success rate of the proposed model. Finally, in order to demonstrate the effectiveness and practical feasibility of the proposed work, a real-time test-bed model is designed and implemented in both layers.},
  archive      = {J_NCA},
  author       = {Barman, Prokash and Chowdhury, Ratul and Chakraborty, Tamal and Goswami, Arpan and Ghosh, Sayak and Saha, Banani},
  doi          = {10.1007/s00521-025-11154-0},
  journal      = {Neural Computing and Applications},
  month        = {6},
  number       = {18},
  pages        = {12525-12564},
  shortjournal = {Neural Comput. Appl.},
  title        = {SRF2T-ID: An implementation of ensemble learning-based IDS with wireless of things secure communication for smart residency environment},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Cost-effective economic dispatch in large-scale power systems using enhanced manta ray foraging optimization. <em>NCA</em>, <em>37</em>(18), 12487-12524. (<a href='https://doi.org/10.1007/s00521-025-11086-9'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Economic dispatch (ED) is a critical optimization problem in power systems, challenged by real-world constraints such as prohibited operating zones (POZ), valve-point loading effects (VPL), and multi-fuel options (MFO). POZ are regions where generators cannot operate due to mechanical limitations; VPL introduces cost fluctuations caused by turbine valve operations; and MFO allows generators to switch between multiple fuel types, adding complexity to cost functions. To address these challenges, this study proposes four enhanced Manta Ray Foraging Optimization (MRFO) variants: opposition-based MRFO (OMRFO), quasi-oppositional MRFO (QMRFO), opposition-based generation jumping MRFO (JOMRFO), and quasi-oppositional generation jumping MRFO (JQMRFO). These variants leverage opposition-based learning (OBL), quasi-oppositional learning (QOBL), and a generation jumping mechanism to balance exploration and exploitation, overcoming limitations of standard MRFO, such as slow convergence and local optima entrapment. OBL and QOBL diversify the search by generating opposite or quasi-opposite solutions, expanding the search space, and avoiding stagnation. The jumping mechanism introduces probabilistic "jumps" to explore non-adjacent regions, enhancing exploration further. Exploitation is refined by retaining and improving the most promising solutions. The algorithms are tested on standard benchmark systems widely used in power systems literature, including the 10, 15, 40, 140, and 160-unit systems, ensuring comparability and reproducibility. Results show that JOMRFO outperforms MRFO and other state-of-the-art methods, achieving significant annual cost savings: $3,730,344 on a 10-unit system, $54,641,376 on a 40-unit system, and $955,501,418 on a 140-unit system. These findings highlight the effectiveness of the proposed variants in improving optimization efficiency and reducing operational costs.},
  archive      = {J_NCA},
  author       = {Spea, S. R.},
  doi          = {10.1007/s00521-025-11086-9},
  journal      = {Neural Computing and Applications},
  month        = {6},
  number       = {18},
  pages        = {12487-12524},
  shortjournal = {Neural Comput. Appl.},
  title        = {Cost-effective economic dispatch in large-scale power systems using enhanced manta ray foraging optimization},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Future trends in the design of memetic algorithms: The case of the linear ordering problem. <em>NCA</em>, <em>37</em>(18), 12471-12485. (<a href='https://doi.org/10.1007/s00521-025-11171-z'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The way heuristic optimizers are designed has evolved over the decades, as computing power has increased. Such has been the case for the linear ordering problem (LOP), a field in which trajectory-based strategies led the way during the 1990 s, but which have now been surpassed by memetic schemes. This paper focuses on understanding how the design of LOP optimizers will change in the future, as computing power continues to increase, yielding two main contributions. On the one hand, a metaheuristic was designed that is capable of effectively exploiting a large amount of computational resources, specifically computing power equivalent to what a recent core can output during runs lasting over four months. Our analyses show that as the power of the computational resources increases, it will be necessary to boost the capacities of the intensification methods applied in the memetic algorithms to keep the population from stagnating. And, on the other hand, the best-known results for today’s most challenging set of instances (xLOLIB2) were significantly outperformed. New bounds were established in this benchmark, which provides a new reference frame for future research.},
  archive      = {J_NCA},
  author       = {Pérez Lugo, Lázaro Jesús and Segura, Carlos and Miranda, Gara},
  doi          = {10.1007/s00521-025-11171-z},
  journal      = {Neural Computing and Applications},
  month        = {6},
  number       = {18},
  pages        = {12471-12485},
  shortjournal = {Neural Comput. Appl.},
  title        = {Future trends in the design of memetic algorithms: The case of the linear ordering problem},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). LaGR-SEQ: Language-guided reinforcement learning with sample-efficient querying. <em>NCA</em>, <em>37</em>(18), 12447-12470. (<a href='https://doi.org/10.1007/s00521-025-11156-y'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Large language models (LLMs) have recently demonstrated their impressive ability to provide context-aware responses via text. This ability could potentially be used to predict plausible solutions in sequential decision making tasks pertaining to pattern completion. For example, by observing a partial stack of cubes, LLMs can predict the correct sequence in which the remaining cubes should be stacked by extrapolating the observed patterns (e.g., cube sizes, colors or other attributes) in the partial stack. In this work, we introduce LaGR (language-guided reinforcement learning), which uses this predictive ability of LLMs to propose solutions to tasks that have been partially completed by a primary reinforcement learning (RL) agent, in order to subsequently guide the latter’s training. However, as RL training is generally not sample-efficient, deploying this approach would inherently imply that the LLM be repeatedly queried for solutions; a process that can be expensive and infeasible. To address this issue, we introduce SEQ (sample-efficient querying), where we simultaneously train a secondary RL agent to decide when the LLM should be queried for solutions. Specifically, we use the quality of the solutions emanating from the LLM as the reward to train this agent. We show that our proposed framework LaGR-SEQ enables more efficient primary RL training, while simultaneously minimizing the number of queries to the LLM. We demonstrate our approach on a series of tasks and highlight the advantages of our approach, along with its limitations and potential future research directions.},
  archive      = {J_NCA},
  author       = {George, Thommen Karimpanal and Semage, Buddhika Laknath and Rana, Santu and Le, Hung and Tran, Truyen and Gupta, Sunil and Venkatesh, Svetha},
  doi          = {10.1007/s00521-025-11156-y},
  journal      = {Neural Computing and Applications},
  month        = {6},
  number       = {18},
  pages        = {12447-12470},
  shortjournal = {Neural Comput. Appl.},
  title        = {LaGR-SEQ: Language-guided reinforcement learning with sample-efficient querying},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Improved thin cap fibroatheroma detection system using virtual histology-intravascular ultrasound image based on the empirical study. <em>NCA</em>, <em>37</em>(18), 12407-12445. (<a href='https://doi.org/10.1007/s00521-025-11102-y'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Detecting a life-threatening atherosclerotic plaque, called Thin Cap Fibroatheroma (TCFA) or vulnerable plaque in Virtual Histology-Intravascular Ultrasound (VH-IVUS) images is a challenging task. To improve the reliability of detecting TCFA early, a new segmentation method, namely the Plaque Burden Assessment by Local Search (PBALS) algorithm, has been proposed using VH-IVUS images, and geometric features have been extracted. Additionally, a hybrid feature extraction method uses discrete cosine transform (DCT) and discrete wavelet transform (DWT) algorithms proposed to extract the texture feature. These features have been used by a set of proposed ensemble classifications to detect the non-TCFA from TCFA plaques with expected reliability and robustness. 566 in-vivo IVUS images and their matching VH-IVUS images gathered from 10 patients were used in the experiment. Based on the results, the combination of VH-IVUS and IVUS features performs better than standalone VH-IVUS features. Moreover, our proposed methods outperformed most other state-of-the-art methods regarding accuracy, specificity, and sensitivity. Some of our proposed ensemble methods showed promising results even with a few features. However, more VH-IVUS and IVUS data are needed to use the proposed methods as an aid to cardiologists in identifying the types of plaques and assessing their vulnerability.},
  archive      = {J_NCA},
  author       = {Rezaei, Zahra and Abaei, Golnoush},
  doi          = {10.1007/s00521-025-11102-y},
  journal      = {Neural Computing and Applications},
  month        = {6},
  number       = {18},
  pages        = {12407-12445},
  shortjournal = {Neural Comput. Appl.},
  title        = {Improved thin cap fibroatheroma detection system using virtual histology-intravascular ultrasound image based on the empirical study},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Tuberculosis detection using deep and hybrid learning techniques using X-ray images. <em>NCA</em>, <em>37</em>(18), 12373-12406. (<a href='https://doi.org/10.1007/s00521-025-11173-x'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Tuberculosis (TB) remains as a significant threat, particularly in many developing countries, where its means of transmission occurs through direct contact or airborne particles. Despite being among the top ten most common causes of mortality worldwide, detection at an early stage is critical for improving the patient outcomes. This study presents an integrated method that uses artificial intelligence (AI) techniques to aid in the timely detection of TB, especially during its preliminary stages, using the images of chest X-ray. A comprehensive database containing 3500 TB-infected and 3500 normal chest X-ray images was carefully selected from diverse public sources. Apart from these results, the study evaluated how well several classification models trained on varied pre-processing techniques performed, including Histogram Equalization (HE), Contrast Limited Adaptive Histogram Equalization (CLAHE), Gamma correction and Discrete Wavelet Transforms (DWT), and its combinations. Models such as AlexNet, EfficientNetB0, MobileNetV2, ResNet50, and Xception were assessed, with performance measures such as Precision, Recall, Specificity, F1-score, Kappa Score, and Accuracy in detecting TB using X-ray images. We conducted 30 experiments, notably, eleven experiments yielded 100% accuracy, which raised concerns regarding potential overfitting. To address this, cross-validation was applied specifically to these eleven cases, ensuring the robustness of the results. The findings demonstrate, with cross-validation confirming the reliability of the high accuracy achieved. The peak classification accuracy, as measured by the F1-score, also achieved the perfection score. The results showed the consistent high performance of EfficientNetB0, ResNet50, and MobileNetV2 across different pre-processing methods, with techniques like CLAHE and Gamma Correction significantly enhancing model performance by enhancing image quality and feature representation. The findings demonstrate, with cross-validation confirming the reliability of the high accuracy achieved.},
  archive      = {J_NCA},
  author       = {Shilpa, N. and Ayeesha Banu, W.},
  doi          = {10.1007/s00521-025-11173-x},
  journal      = {Neural Computing and Applications},
  month        = {6},
  number       = {18},
  pages        = {12373-12406},
  shortjournal = {Neural Comput. Appl.},
  title        = {Tuberculosis detection using deep and hybrid learning techniques using X-ray images},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Mastitis diagnosis with machine learning algorithms. <em>NCA</em>, <em>37</em>(18), 12351-12372. (<a href='https://doi.org/10.1007/s00521-025-11176-8'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Artificial intelligence is technologically intelligent computer software that can derive mathematical conclusions from what it has learned and help us make decisions. Machine learning, a sub-branch of artificial intelligence, is widely used in the medical field today. Studies in the literature show that machine learning methods provide quite successful results in diagnosing diseases. Mastitis disease affects many components of milk obtained from cows. In this context, as well as the number of somatic cells used in the detection of mastitis, analyzing changes in other components of milk enables a more accurate diagnosis of the disease. In this study, milk samples were taken from 118 different cows in dairy farms operating in Bucak district of Burdur province of Türkiye. The protein, fat, pH, lactose, viscosity, and color values of the milk samples were analyzed in a laboratory environment. The analysis results were used in the training and testing of machine learning algorithms, and mastitis disease was tried to be diagnosed with the results obtained from these algorithms. Considering the inputs of the research and the results obtained, appropriate algorithms were selected among machine learning algorithms. These algorithms are GaussianNB, Decision Tree, Support Vector Machine, K-Nearest Neighbor, Random Forest, Logistic Regression, XGBoost and LightGBM algorithms. As a result of the study, the performances of the algorithms for the diagnosis of mastitis were compared and the highest accuracy rate was achieved with the Decision Tree algorithm (89%). Additionally, this study showed that mastitis disease can be diagnosed with a 89% accuracy rate if the protein, fat, pH, lactose, viscosity, and color values in cow’s milk are analyzed as a whole. Therefore, mastitis in cows can be diagnosed by considering the number of somatic cells; it can also be diagnosed by holistic examination of protein, fat, pH, lactose, viscosity, and color values in milk. This study provides original information for the diagnosis of mastitis by holistically evaluating the relationship between the components of milk affected by mastitis using machine learning algorithms.},
  archive      = {J_NCA},
  author       = {Kalkan, Adnan and Tepeli, Mehmet and Göde, Aslı},
  doi          = {10.1007/s00521-025-11176-8},
  journal      = {Neural Computing and Applications},
  month        = {6},
  number       = {18},
  pages        = {12351-12372},
  shortjournal = {Neural Comput. Appl.},
  title        = {Mastitis diagnosis with machine learning algorithms},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). An enhanced hybrid single-image-based structure-guided ℓ0-norm and radiance–reflectance optimization. <em>NCA</em>, <em>37</em>(18), 12329-12349. (<a href='https://doi.org/10.1007/s00521-025-11168-8'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The reduced visibility during the winter season in an outdoor setting can be attributed primarily to the presence of haze or fog. Despite adjusting the lens of an optical sensor system for various purposes, such as automated driver assistance, remote sensing, and visual surveillance, the visual quality remains compromised. Owing to the overcast and murky atmosphere, it is difficult to remove these haziness emissions from a single image. To address this problem, we present a novel optimization-based dehazing algorithm that combines radiance and reflectance components with an additional refinement via a structure-guided $${\ell}$$ 0-norm filter. We first estimate the poor reflectance map and optimize our transmission maps by using an estimated diffuse map. In addition, we estimate the removal of dehazing artifacts via a structure-guided $${\ell}$$ 0 transmission map. In terms of qualitative and quantitative measures, compared with simulated pairs of images, the experimental results show that the proposed method is superior to the state-of-the-art algorithms. Moreover, the results of real-world enhancements confirm that the proposed method can provide high-quality images with no undesirable effects. In addition, textures may be removed, and edges can be preserved by the guided $${\ell}$$ 0-norm filter, while the general image enhancement algorithms are being applied. The test results indicate that the proposed algorithm effectively generates enhanced images with greater visibility. The proposed approach has demonstrated superior performance metrics, including a 25.17% improvement in the peak signal-to-noise ratio (PSNR) and a 6.51% enhancement in the structural similarity index (SSI), surpassing the average performance of alternative methods.},
  archive      = {J_NCA},
  author       = {WangNo, Nitit and Thaiklang, Saowaluk},
  doi          = {10.1007/s00521-025-11168-8},
  journal      = {Neural Computing and Applications},
  month        = {6},
  number       = {18},
  pages        = {12329-12349},
  shortjournal = {Neural Comput. Appl.},
  title        = {An enhanced hybrid single-image-based structure-guided ℓ0-norm and radiance–reflectance optimization},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Embedded DNN-based nonlinear MPC with guaranteed constraint satisfaction. <em>NCA</em>, <em>37</em>(18), 12305-12328. (<a href='https://doi.org/10.1007/s00521-025-11063-2'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The real-time implementation of nonlinear model predictive control (NMPC) is challenging due to the computational complexity of solving online optimization problems within a stipulated sample time. A popular solution to this issue involves approximating NMPC control laws using supervised learning with deep neural networks (DNNs). The resulting DNN controller offers faster execution and has a smaller memory footprint than the standard NMPC methods. However, guaranteeing constraint satisfaction remains crucial in this imitation learning framework. To address this challenge, one approach is to incorporate constraint information during the neural network training phase. This study implements the penalty and logarithmic barrier methods to enrich the neural network’s loss function with insights regarding constraints. Using these methods, the neural network can handle constraints, ensuring adherence to them during control decisions. To validate the proposed approach’s efficacy, it is evaluated using a 2-degree-of-freedom (DOF) nonlinear helicopter model through deployment on STM32 Nucleo-144 development board using hardware-in-the-loop (HIL) co-simulation. The performance of the proposed controller is compared to that of the standard NMPC. The comparative analysis demonstrates that the proposed DNN algorithm not only reduces memory consumption and controller execution time but also effectively handles the system constraints, delivering performance comparable to standard NMPC.},
  archive      = {J_NCA},
  author       = {Mohanty, Nirlipta Ranjan and Ubare, Promod and Patne, Vaishali and Ingole, Deepak and Sonawane, Dayaram},
  doi          = {10.1007/s00521-025-11063-2},
  journal      = {Neural Computing and Applications},
  month        = {6},
  number       = {18},
  pages        = {12305-12328},
  shortjournal = {Neural Comput. Appl.},
  title        = {Embedded DNN-based nonlinear MPC with guaranteed constraint satisfaction},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Ip-GCN: A hybrid deep learning approach to predict influence in complex networks. <em>NCA</em>, <em>37</em>(18), 12281-12303. (<a href='https://doi.org/10.1007/s00521-025-11163-z'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, various methods have emerged for predictive modeling across multiple domains to forecast the influence of nodes in complex networks. The existing machine learning based methods predict influential nodes in complex networks by using feature selection techniques for the nodes. However, these methods suffer from limitations in feature engineering. In contrast, Graph Convolutional Networks (GCNs) present a breakthrough in deep learning, offering an effective approach for analysing graph-structured data. This research work proposes the Influential node Prediction based on Graph Convolutional Network (Ip-GCN) method for complex networks, which integrates GCN and Long Short-Term Memory (LSTM) techniques. Initially, the proposed method preprocesses datasets by removing noisy nodes and constructing the adjacency matrix of the graph. The addition of the identity and adjacency matrices accounts for the self-contribution of individual nodes in the graph, and the degree matrix is derived from the updated adjacency matrix. Furthermore, the feature matrix is computed from the adjacency matrix to capture the connections of nodes and the hidden structure of the graph. The scaling of the adjacency matrix is performed using the inverse of the degree matrix, followed by normalization with the feature matrix. The resultant normalized matrix is provided as input to the GCN layer for embeddings. The LSTM layer is then applied to the embedded matrix to predict the influence of nodes. The comparative performance of Ip-GCN is evaluated against existing methods, including deep learning, machine learning and centrality-based methods. The Ip-GCN model significantly enhances performance across all five real-world datasets. It improves the F1 score by 9%, 29%, 14%, 9%, and 18% on Hamsterster Friendship, Human Protein (Vidal), CA-GrQc, CA-HepTh, and CA-CondMat respectively. In addition to that, it enhances accuracy by 5%, 1%, 0.4%, and 4% on Human Protein (Vidal), CA-GrQc, CA-HepTh, and CA-CondMat respectively. The predicted influential nodes by the Ip-GCN method are validated using the Susceptible Infected Recovered (SIR) simulation model .},
  archive      = {J_NCA},
  author       = {Patel, Asmita and Singh, Naveen Kumar and Singh, Buddha},
  doi          = {10.1007/s00521-025-11163-z},
  journal      = {Neural Computing and Applications},
  month        = {6},
  number       = {18},
  pages        = {12281-12303},
  shortjournal = {Neural Comput. Appl.},
  title        = {Ip-GCN: A hybrid deep learning approach to predict influence in complex networks},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A genetic engineering algorithm for the generalized quadratic assignment problem. <em>NCA</em>, <em>37</em>(18), 12253-12279. (<a href='https://doi.org/10.1007/s00521-025-11155-z'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The generalized quadratic assignment problem (GQAP) poses a significant challenge in optimization, known for its NP-hard complexity and wide-ranging applications in supply chain and manufacturing contexts. While various metaheuristic algorithms have addressed the GQAP, this paper introduces a novel modification of the genetic algorithm (GA), namely the genetic engineering algorithm (GEA). A traditional GA initializes the population of solutions randomly and refines these solutions iteratively through crossover and mutation operators. However, its reliance on randomness often leads to premature convergence, susceptibility to local optima, and suboptimal solutions at termination of the algorithmic run. To address these drawbacks, our research presents a modified GA framework inspired by genetic engineering principles. This framework introduces novel search mechanisms mimicking gene mining processes, targeting population enhancement through insertion, purification, and genetic material exchange from elite solutions. The GEA adds three search scenarios to the traditional GA framework to exhibit desired characteristics from chromosomes, enhancing solution quality and convergence speed. This GEA demonstrates a superior performance across various search scenarios against state-of-the-art algorithms and exact solutions for the benchmark instances of the GQAP. Additionally, this study conducts comprehensive sensitivity analyses on the search scenarios of the GEA. By employing gene mining strategies, the proposed GEA offers promising solutions for effectively addressing the NP-hard optimization challenges of the GQAP on large-scale datasets.},
  archive      = {J_NCA},
  author       = {Sohrabi, Majid and Fathollahi-Fard, Amir M. and Gromov, Vasilii A. and Dulebenets, Maxim A.},
  doi          = {10.1007/s00521-025-11155-z},
  journal      = {Neural Computing and Applications},
  month        = {6},
  number       = {18},
  pages        = {12253-12279},
  shortjournal = {Neural Comput. Appl.},
  title        = {A genetic engineering algorithm for the generalized quadratic assignment problem},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A novel coarse-grained knowledge graph embedding framework for platform risk identification from relational data. <em>NCA</em>, <em>37</em>(18), 12231-12251. (<a href='https://doi.org/10.1007/s00521-025-11127-3'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Platform risk identification from relational data aims to determine whether the target platforms are risky or not. However, in such a scenario, the relation data involve massive types of relations and numerous numerical-aware relations composed of attributes and infinite continuous values are challenges in this field. It is paramount to capture semantic information of relations and precise control power, drastically affecting the power of learning and the performance of identifying risky platforms. Recent financial risk studies often employed graph neural networks (GNNs) to model relational data, which fall short of capturing the intricacies of such complex relationships resulting in incorrect identification of platforms. To solve this problem, we propose a novel coarse-grained knowledge graph embedding (CKGE) framework that integrates coarse-grained knowledge graph construction and adopts five well-known knowledge graph embedding methods and ensemble learning. The extensive experimental results based on a real-world peer-to-peer lending platform dataset demonstrate that the CKGE framework achieves state of the art over GNNs for platform risk identification. Our study highlights the importance of utilizing the relational data of multi-type and multi-numerical relations for revealing platform risk situations and provides an alternative solution for identifying platform risk.},
  archive      = {J_NCA},
  author       = {Zhang, Qi and Wang, Shicheng and Wang, Lihong and Sheng, Jiawei and Guo, Shu and Li, Chen and He, Min and Zhang, Renqiang},
  doi          = {10.1007/s00521-025-11127-3},
  journal      = {Neural Computing and Applications},
  month        = {6},
  number       = {18},
  pages        = {12231-12251},
  shortjournal = {Neural Comput. Appl.},
  title        = {A novel coarse-grained knowledge graph embedding framework for platform risk identification from relational data},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Training multilayer perceptron using combined and superior chaotic biogeography-based optimizer. <em>NCA</em>, <em>37</em>(18), 12179-12229. (<a href='https://doi.org/10.1007/s00521-025-11116-6'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {An artificial neural network (ANN) depicts that the information processing mechanism imitating the human brain neuronal system has been successfully applied in many fields like classification, clustering, control, and prediction to name a few. Multilayer perceptron neural networks (MLP NNs) are the ones training of whose neuronal architecture is considered to be the most important and challenging step in the development of neural networks. This difficulty in training is mainly due to a large number of candidate solutions and the different search spaces because of differing datasets involved in the learning stage. Gradient descent and recursive methods have long been used as training measures, which often suffer from limitations of being stagnant in local optimal solutions, improper classification of features, slow rate of convergence and sensitive dependence on initializing variables. To overcome these drawbacks, hybrid approaches (metaheuristic algorithms) using chaos have been used. Inspired from the previously obtained successful results, we have used chaos-based biogeography-based optimizer (BBO) in superior orbit and combined two chaotic maps to train MLP NNs. The proposed methods make use of greater discoverability of search spaces for different datasets and benchmark functions to find global optimal solutions in comparison with the competing algorithms quite significantly.},
  archive      = {J_NCA},
  author       = {Kumar, Deepak},
  doi          = {10.1007/s00521-025-11116-6},
  journal      = {Neural Computing and Applications},
  month        = {6},
  number       = {18},
  pages        = {12179-12229},
  shortjournal = {Neural Comput. Appl.},
  title        = {Training multilayer perceptron using combined and superior chaotic biogeography-based optimizer},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Enhancing PID control robustness in CSTRs: A hybrid approach to tuning under external disturbances with GA, PSO, and machine learning. <em>NCA</em>, <em>37</em>(18), 12153-12177. (<a href='https://doi.org/10.1007/s00521-025-11170-0'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The optimization of PID controllers for continuous stirred tank reactors (CSTRs) is critical for ensuring stable and efficient chemical processing under varying operational conditions and external disturbances. This study presents a novel approach that integrates advanced tuning techniques, including genetic algorithms (GA), particle swarm optimization (PSO), and their hybrid combinations with a machine learning (ML) surrogate model, to improve PID controller performance in disturbed environments. A unique evolutionary framework is employed, where populations of both controllers and plants are co-evolved to handle the most challenging plant models. An adversarial testing approach is utilized to evaluate the best-tuned controller against the four most difficult CSTR plants, with disturbances such as feed concentration and temperature fluctuations. The results demonstrate that both GA and PSO, when enhanced with the ML surrogate model, effectively tune PID controllers to manage disturbances, with the GA-tuned controller achieving faster convergence and the PSO-tuned controller showing greater robustness. Additionally, the hybrid ML surrogate model significantly improved control performance and disturbance rejection. The findings highlight the ability of the ML_GA and ML_PSO controllers to maintain stability and accuracy across a range of challenging conditions, providing a robust solution for optimizing control systems in nonlinear dynamic environments. This research contributes to the field of process control, showcasing the potential of combining evolutionary algorithms with machine learning surrogate models for adaptive, resilient control strategies in complex chemical systems.},
  archive      = {J_NCA},
  author       = {Ajlouni, Naim and Osyavas, Adem and Ajlouni, Firas and Almassri, Abdelrahman and Takaoglu, Faruk and Takaoglu, Mustafa},
  doi          = {10.1007/s00521-025-11170-0},
  journal      = {Neural Computing and Applications},
  month        = {6},
  number       = {18},
  pages        = {12153-12177},
  shortjournal = {Neural Comput. Appl.},
  title        = {Enhancing PID control robustness in CSTRs: A hybrid approach to tuning under external disturbances with GA, PSO, and machine learning},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Fine-tuned and explainable machine learning models for temperature-dependent rheological behavior prediction of magnetorheological materials. <em>NCA</em>, <em>37</em>(18), 12123-12152. (<a href='https://doi.org/10.1007/s00521-025-11169-7'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Machine learning (ML) models have shown great potential in predicting how rheological properties of magnetorheological (MR) materials change with temperature, outperforming traditional models. However, several issues have been raised, which are the determination of arbitrary hyperparameters and the black-box nature behavior. Therefore, this study introduces a systematic framework for analyzing the temperature effect on the rheological behavior of MR materials utilizing ML, the metaheuristic algorithm (MA), and explainable artificial intelligence (XAI). In the model development stage, the proposed approach enables ML hyperparameter auto-tuning using MAs. For the post-processing, two activities are conducted, which are observing the input–output interaction using Shapley Additive exPlanations (SHAP) as XAI and MA-based-characterization of rheological behavior caused by the temperature change. This approach was tested on eight datasets from different types of materials, including MR fluids, greases, and elastomers, to ensure wide applicability. Among eight MAs, stochastic assigned inertia weight particle swarm optimization (PSO) and shrike optimization algorithm (SHOA) deliver superior optimization performance. From twelve ML configurations and models, deep neural networks (DNNs) achieve the most consistent highest accuracy, extreme learning machines (ELMs) are the fastest to train, and single-layer artificial neural networks (ANNs) provide simpler structures. SHAP analysis identifies the magnetic field as the primary influence on rheological behavior, followed by temperature. Further, PSO-optimized biplastic Bingham modeling reveals that increasing temperature decreases yield stress and plastic viscosity at low shear rates, with minimal impact on viscosity at high shear rates. The results showed that integrating ML, MAs, and XAI allows for automated model tuning, provides clear and interpretable insights, and produces highly accurate predictions across various MR materials.},
  archive      = {J_NCA},
  author       = {Bahiuddin, Irfan and Imaduddin, Fitrian and Saharuddin, Kasma Diana and Mazlan, Saiful Amri},
  doi          = {10.1007/s00521-025-11169-7},
  journal      = {Neural Computing and Applications},
  month        = {6},
  number       = {18},
  pages        = {12123-12152},
  shortjournal = {Neural Comput. Appl.},
  title        = {Fine-tuned and explainable machine learning models for temperature-dependent rheological behavior prediction of magnetorheological materials},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Detection of translucent flesh disorder and automatic grading of mangosteens in multi-view images. <em>NCA</em>, <em>37</em>(18), 12103-12121. (<a href='https://doi.org/10.1007/s00521-025-11165-x'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, convolutional neural network (CNN)-based solutions are developed for grading assessment and flesh disorder detection of mangosteens in images. The grading is set to three classes of three quality levels based on the local market, where the data were collected. In addition, three flesh disorders/status are focused in this work, including translucent flesh disorder, gamboge, and rotten. Three types of solutions are attempted in this paper. The first solution relies on the well-known CNN architectures with the transfer learning and data augmentation. The second solution is developed based on the detection model, i.e., YOLOv8. The third solution is to design a new architecture by taking into account of human expert knowledge that is used for the manual grading and detection. Multiple views of each mangosteen must be considered simultaneously for the disorder detection. Four side views should be considered together, before looking at the top and bottom views. This is a very difficult task even for the human experts. The proposed solutions are trained and evaluated on the self-collected dataset of 206 mangosteens captured under six views (i.e., top view, bottom view, and four side views). The proposed solutions could achieve the perfect accuracy of 100% for the grading and up to 78% AUC for the disorder detection.},
  archive      = {J_NCA},
  author       = {Kusakunniran, Worapan and Imaromkul, Thanandon and Aukkapinyo, Kittinun and Thongkanchorn, Kittikhun and Somsong, Pimpinan and Tiyayon, Pimsiri},
  doi          = {10.1007/s00521-025-11165-x},
  journal      = {Neural Computing and Applications},
  month        = {6},
  number       = {18},
  pages        = {12103-12121},
  shortjournal = {Neural Comput. Appl.},
  title        = {Detection of translucent flesh disorder and automatic grading of mangosteens in multi-view images},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multi-task learning-based temporal pattern matching network for guitar tablature transcription. <em>NCA</em>, <em>37</em>(18), 12083-12102. (<a href='https://doi.org/10.1007/s00521-025-11148-y'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Guitar tablature transcription poses unique challenges in automatic music transcription, as it requires capturing both pitch and string usage on a multi-string instrument with various expressive techniques. While guitar tablature is widely used by guitarists in the music field, neural architecture modeling for this task remains underexplored, particularly in accurately mapping pitches to their respective strings. In this work, we propose a multi-task learning-based temporal pattern-matching network (TPMNet) that effectively captures temporal information from guitar recordings, improving the alignment of predicted results. The key contribution of this work is the advancement of neural network architecture, leading to notable improvements in prediction performance for guitar tablature transcription. Additionally, we explored the optimal pooling layer selection method tailored to different tasks, addressing a long-confusing problem in the field. TPMNet’s efficacy was validated through experiments on the GuitarSet dataset, and its generalizability was confirmed via cross-evaluation with the EGDB dataset.},
  archive      = {J_NCA},
  author       = {Kim, Taehyeon and Kim, Man-Je and Ahn, Chang Wook},
  doi          = {10.1007/s00521-025-11148-y},
  journal      = {Neural Computing and Applications},
  month        = {6},
  number       = {18},
  pages        = {12083-12102},
  shortjournal = {Neural Comput. Appl.},
  title        = {Multi-task learning-based temporal pattern matching network for guitar tablature transcription},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Integrating finite element analysis and machine learning for non-invasive tumor detection: A piezoelectric tactile sensor-based vibration absorber approach. <em>NCA</em>, <em>37</em>(18), 12059-12081. (<a href='https://doi.org/10.1007/s00521-025-11126-4'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Timely cancer detection is crucial for reducing mortality rates associated with delayed diagnosis and treatment. This study presents a novel, non-invasive tumor detection technique that integrates finite element analysis (FEA) with machine learning (ML) models. A piezoelectric sensor, developed using vibration absorber phenomena, was designed to detect changes in tissue stiffness caused by tumors. The sensor's interaction with soft tissue of varying Young’s modulus (9 to 185 kPa) was simulated to mimic the properties of cancerous tissue. Modal and harmonic analyses using ANSYS software, including indentation trials, were conducted to study the sensor’s response to stiffness variations, creating a dataset based on tumor sizes (5, 10, 12, 15, 17, 20, and 25 mm in diameter) at different vertical and horizontal distances from the tissue surface to the tumor center, simulating various growth stages and depths. The collected data were used to train two machine learning models: a recurrent neural network (RNN) for precise tumor localization and sizing and a feedforward neural network (FNN) to estimate tissue stiffness through sensor absorber frequencies. The proposed methodology demonstrated promising results, with a minimum error of 0.04 mm in tumor size estimation and 0.0319 kPa in stiffness detection. This approach offers potential improvements in early tumor detection by providing accurate and noninvasive diagnostics, particularly in resource-constrained environments.},
  archive      = {J_NCA},
  author       = {Hashem, Radwa and El-Hussieny, Haitham and Umezu, Shinjiro and El-Bab, Ahmed M. R. Fath},
  doi          = {10.1007/s00521-025-11126-4},
  journal      = {Neural Computing and Applications},
  month        = {6},
  number       = {18},
  pages        = {12059-12081},
  shortjournal = {Neural Comput. Appl.},
  title        = {Integrating finite element analysis and machine learning for non-invasive tumor detection: A piezoelectric tactile sensor-based vibration absorber approach},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Propagation-aware Q-coverage and Q-connectivity network design in relay-aided IoT sensor systems using heuristic and genetic algorithms. <em>NCA</em>, <em>37</em>(18), 12031-12058. (<a href='https://doi.org/10.1007/s00521-025-11108-6'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Wireless sensor networks play a crucial role in Internet of Things systems, whose development and prevalence have given rise to numerous challenges. One of the most notable problems regarding wireless sensor networks is the sensor deployment problem, where the objective is to minimize the number of nodes used and to satisfy the constraints of coverage and connectivity. In reality, sensors may be prone to errors and failures; therefore, simple coverage and simple connectivity cannot guarantee fault tolerance for a system. This leads to the concepts of Q-coverage and Q-connectivity, which indicates that each target needs q covering sensors and q node-disjoint paths to the base station. To minimize the number of nodes while still ensuring Q-coverage and Q-connectivity, we propose a heuristic algorithm and an algorithm based on the genetic algorithm. Besides, we construct various experiment scenarios and a realistic dataset to evaluate the efficiency of our proposed methods. The experiment results show that our proposed algorithms significantly outperform existing methods in both solution quality and running time.},
  archive      = {J_NCA},
  author       = {Thang, Nguyen Xuan and Hanh, Nguyen Thi and Son, Nguyen Van and Tan, Nguyen Phuc and Hung, To Quang and Chien, Trinh Van and Binh, Huynh Thi Thanh},
  doi          = {10.1007/s00521-025-11108-6},
  journal      = {Neural Computing and Applications},
  month        = {6},
  number       = {18},
  pages        = {12031-12058},
  shortjournal = {Neural Comput. Appl.},
  title        = {Propagation-aware Q-coverage and Q-connectivity network design in relay-aided IoT sensor systems using heuristic and genetic algorithms},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Mitigating exposure bias in large language model distillation: An imitation learning approach. <em>NCA</em>, <em>37</em>(18), 12013-12029. (<a href='https://doi.org/10.1007/s00521-025-11162-0'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Knowledge distillation is recognized as a valuable model compression strategy that alleviates the computational burden of large language models while preserving performance. This strategy involves training a smaller model utilizing both real data and predictions from a more cumbersome model. Traditional distillation methods, however, are often compromised by exposure bias, which results from reliance on next-step prediction training loss. This bias emerges when models are tested in free-running mode, differing from their training regime and leading to a progressive drift in input distributions between testing and training phases. An analogous issue, known as ‘distributional shift’, has been effectively addressed in imitation learning through various methodologies. Therefore, this paper specifically tailors an imitation learning-based solution to a traditional knowledge distillation framework which inherently considers both real data and the teacher’s predictions as dual sources of expert demonstrations. The effectiveness of this approach is demonstrated over five different test datasets, where it outperforms traditional benchmarks across all evaluation metrics. Specifically, it achieves superior results in perplexity, multi-token generation, and G-Eval score, indicating improvements in both predictive accuracy and alignment with human judgment in text quality. These results underscore the potential of this approach to effectively address exposure bias in large language model distillation.},
  archive      = {J_NCA},
  author       = {Pozzi, Andrea and Incremona, Alessandro and Tessera, Daniele and Toti, Daniele},
  doi          = {10.1007/s00521-025-11162-0},
  journal      = {Neural Computing and Applications},
  month        = {6},
  number       = {18},
  pages        = {12013-12029},
  shortjournal = {Neural Comput. Appl.},
  title        = {Mitigating exposure bias in large language model distillation: An imitation learning approach},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). TopView: Vectorising road users in a bird’s eye view from uncalibrated street-level imagery with deep learning. <em>NCA</em>, <em>37</em>(18), 11991-12011. (<a href='https://doi.org/10.1007/s00521-025-11152-2'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Generating a bird’s eye view of road users is beneficial for a variety of applications, including navigation, detecting agent conflicts, and measuring space occupancy, as well as the ability to utilise the metric system to measure distances between different objects. In this research, we introduce a simple approach for estimating a bird’s eye view from images without prior knowledge of a given camera’s intrinsic and extrinsic parameters. The model is based on the orthogonal projection of objects from various fields of view to a bird’s eye view by learning the vanishing point of a given scene. Additionally, we utilised the learned vanishing point alongside the trajectory line to transform the 2D bounding boxes of road users into 3D bounding information. The introduced framework has been applied to several applications to generate a live Map from camera feeds and to analyse social distancing violations at the city scale. The introduced framework shows a high validation in geolocating road users in various uncalibrated cameras. It also paves the way for new adaptations in urban modelling techniques and simulating the built environment accurately, which could benefit agent-based modelling by relying on deep learning and computer vision.},
  archive      = {J_NCA},
  author       = {Ibrahim, Mohamed R.},
  doi          = {10.1007/s00521-025-11152-2},
  journal      = {Neural Computing and Applications},
  month        = {6},
  number       = {18},
  pages        = {11991-12011},
  shortjournal = {Neural Comput. Appl.},
  title        = {TopView: Vectorising road users in a bird’s eye view from uncalibrated street-level imagery with deep learning},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Stance classification model with knowledge-aware multi-feature attention network. <em>NCA</em>, <em>37</em>(18), 11965-11990. (<a href='https://doi.org/10.1007/s00521-025-11147-z'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Stance classification aims to identify the stance conveyed in tweets toward a specific target. Recent works have been devoted to leveraging target word embedding to incorporate target information into the stance classification model. However, it is difficult to capture implicit target information solely through target word embedding. In addition, stance knowledge is often ignored in previous work. To address these issues, this paper proposes a novel stance classification model with knowledge-aware multi-feature attention network (SC-KMAN). Firstly, we introduce richer target information into SC-KMAN through the target information extractor T-BERT designed in this paper. Meanwhile, we introduce a sentiment feature extractor S-BERT by transfer learning. Then, we propose a knowledge-based multi-feature attention network (KMAN) to introduce stance knowledge into the stance classification model. KMAN utilizes the stance, sentiment, and target features of texts as inputs. Subsequently, it integrates stance knowledge to accurately infer the stance of texts. The experimental results on two Twitter datasets and one Chinese stance classification dataset demonstrate that SC-KMAN achieves relatively considerable performance.},
  archive      = {J_NCA},
  author       = {Meng, Chao and Fang, Binxing and Zhang, Hongli and Yang, Yuchen and Yin, Gongzhu and Lu, Kun},
  doi          = {10.1007/s00521-025-11147-z},
  journal      = {Neural Computing and Applications},
  month        = {6},
  number       = {18},
  pages        = {11965-11990},
  shortjournal = {Neural Comput. Appl.},
  title        = {Stance classification model with knowledge-aware multi-feature attention network},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A study on self-supervised sketch-based image retrieval on unpaired data (S3BIR). <em>NCA</em>, <em>37</em>(18), 11945-11963. (<a href='https://doi.org/10.1007/s00521-025-11142-4'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Sketch-based Image Retrieval (SBIR) is a prevalent task in computer vision, where a model should produce a bimodal sketch-photo feature space. Training such a model requires sketch-photo pairs to adjust a bimodal neural network. However, accessing paired data is impractical in real scenarios, such as in the case of eCommerce search engines. To address this problem, we can leverage self-supervised learning strategies to learn the sketch-photo space, which has yet to be explored. Therefore, this work presents a study of the performance of diverse self-supervised methodologies adapted to the SBIR domain. The term self-supervised means the model cannot access sketch-photo pairs, relaxing the training to see only pseudo-sketches generated during training. So far, our study is the first to explore diverse self-supervised mechanisms for SBIR (S3BIR). Our results show the outstanding performance of contrastive-based models like SimCLR and CLIP adapted to SBIR under a self-supervised regimen. S3BIR-CLIP is the best model in terms of effectiveness, achieving a mAP of 54.03% in Flickr15K, 45.38% in eCommerce, and 13.80% in QD. In the eCommerce dataset, we increase performance by around 20% w.r.t. previously published results. However, regarding resource consumption, S3BIR-SimCLR is the most competitive.},
  archive      = {J_NCA},
  author       = {Campos, Waldo and Saavedra, Jose M. and Stears, Christopher},
  doi          = {10.1007/s00521-025-11142-4},
  journal      = {Neural Computing and Applications},
  month        = {6},
  number       = {18},
  pages        = {11945-11963},
  shortjournal = {Neural Comput. Appl.},
  title        = {A study on self-supervised sketch-based image retrieval on unpaired data (S3BIR)},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). $$\hbox {C}^2$$ DFL: Cross-view cross-layer discriminative feature learning for fine-grained 3D shape classification. <em>NCA</em>, <em>37</em>(18), 11923-11944. (<a href='https://doi.org/10.1007/s00521-025-11115-7'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Fine-grained 3D shape classification poses challenges in effectively capturing and integrating discriminative features residing in subtle local regions. Previous methods typically extract features independently from individual views of 3D shapes, with a focus on various strategies for fusing these extracted view features. However, this approach neglects interview correlations and potential redundancies among different views. In this study, we introduce $$\hbox {C}^2$$ DFL, which consists of two primary modules: cross-view discriminative feature extraction (CV-DFE) and cross-layer discriminative feature fusion (CL-DFF). CV-DFE integrates discriminative features by merging inputs from multiple views, mitigating limitations associated with isolated feature extraction. CL-DFF dynamically selects key tokens using a transformer model to interactively fuse discriminative features from various levels. Extensive experiments conducted on three categories of the FG3D dataset demonstrate the exceptional efficacy of $$\hbox {C}^2$$ DFL in capturing and integrating discriminative features of 3D shapes. The proposed method achieves state-of-the-art accuracy in fine-grained 3D shape classification (FGSC).},
  archive      = {J_NCA},
  author       = {Jiang, Jinzhe and Bai, Jing and Ma, Xiangyu},
  doi          = {10.1007/s00521-025-11115-7},
  journal      = {Neural Computing and Applications},
  month        = {6},
  number       = {18},
  pages        = {11923-11944},
  shortjournal = {Neural Comput. Appl.},
  title        = {$$\hbox {C}^2$$ DFL: Cross-view cross-layer discriminative feature learning for fine-grained 3D shape classification},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Improved KW entropy: A complexity measurement technique for time series and its application in feature extraction of quay crane gearbox. <em>NCA</em>, <em>37</em>(18), 11909-11922. (<a href='https://doi.org/10.1007/s00521-025-11159-9'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The quay crane operates in a severe and specific working environment. The gearbox, being a pivotal transmission component within the hoisting mechanism, plays a crucial role in ensuring the reliable and smooth operation of the entire system. Its health status is of utmost importance. To derive degradation characteristics from intricate vibration monitoring signals, we introduce an enhanced KW symbol entropy feature extraction approach. This method ensures uniformity in symbol standardization by adopting the root mean square (RMS) of the signal under normal conditions as the benchmark, integrating a symbol coefficient to establish a uniform symbol scale. Additionally, we incorporate a variable number of symbols to enlarge the symbol set, thereby enhancing the capacity for information representation. Subsequently, leveraging information entropy theory, we compute the complexity of both the symbol sequence itself and its distribution, yielding two distinctive features: improved symbol sequence entropy (IKSE) and improved symbol distribution entropy (IKDE). We have analyzed the logistic chaotic sequence and the operational lifespan signal of the hoisting gearbox independently. The results demonstrate that the introduced features possess remarkable capabilities in elucidating the complexities embedded within nonlinear time series, thereby facilitating a precise depiction of the entire progression of performance degradation in the lifting gearbox. Notably, this technique manifests a remarkable degree of stability, with its performance remaining largely unaffected by variations in its parameters.},
  archive      = {J_NCA},
  author       = {Gu, Chunxia and Bi, Juan and Wang, Bing},
  doi          = {10.1007/s00521-025-11159-9},
  journal      = {Neural Computing and Applications},
  month        = {6},
  number       = {18},
  pages        = {11909-11922},
  shortjournal = {Neural Comput. Appl.},
  title        = {Improved KW entropy: A complexity measurement technique for time series and its application in feature extraction of quay crane gearbox},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Guiding evolutionary algorithms with large language models to learn fuzzy cognitive maps. <em>NCA</em>, <em>37</em>(18), 11891-11908. (<a href='https://doi.org/10.1007/s00521-025-11157-x'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Fuzzy Cognitive Maps (FCMs) are interpretable simulation models that represent causal relationships between concepts as a weighted digraph with labeled nodes. They serve to examine a system’s structure (e.g., what concepts are critical to spreading an intervention’s effects?) and long-term behavior (e.g., if we increase fruit availability, how will its consumption change?). When modelers build FCMs by leveraging participants’ knowledge, the resulting participant-built FCMs can be analyzed and interpreted since participants report perceived causality. However, engaging enough knowledgeable participants to construct an FCM can be challenging. Alternatively, machine learning algorithms derive FCMs from data by selecting relationships to maximize a metric such as accuracy, which can produce overly dense FCMs where relationships may not represent valid causal mechanisms—this hinders the critically important interpretability of FCMs. In this paper, we address the need for expert knowledge and the validity of causal mechanisms in FCMs. Specifically, we propose using Large Language Models (LLMs) to guide algorithms in building FCMs where valid pairs of concepts are connected in the right causal direction and with the correct causal type (increase/decrease). Our approach combines LLMs with CMA-ES, a ubiquitous, state-of-the-art evolutionary algorithm. Using three real-world case studies and several LLMs, we show our method successfully (i) learns sparse FCMs that fit the data well and (ii) represents valid causal relationships. Moreover, the learned FCMs are sparser than the corresponding participant-built ones, demonstrating our method may help simplify existing FCMs and selectively includes causal relationships, which is essential to build trustworthy and interpretable FCMs.},
  archive      = {J_NCA},
  author       = {Schuerkamp, Ryan and Giabbanelli, Philippe J.},
  doi          = {10.1007/s00521-025-11157-x},
  journal      = {Neural Computing and Applications},
  month        = {6},
  number       = {18},
  pages        = {11891-11908},
  shortjournal = {Neural Comput. Appl.},
  title        = {Guiding evolutionary algorithms with large language models to learn fuzzy cognitive maps},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Bi-layer energy management framework based on neural networks for multi-microgrid systems. <em>NCA</em>, <em>37</em>(18), 11863-11889. (<a href='https://doi.org/10.1007/s00521-025-11120-w'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper presents a bi-layer framework developed using a new class of discrete-time recurrent neural networks for optimal energy management in a multi-microgrid (MMG) system. The first layer involves a neural network-based energy management system (NN-EMS) designed to perform multi-objective energy management in each microgrid within the MMG system. The optimal output solutions obtained by the NN-EMS serve as references for dispatchable distributed energy resources and the amount of power traded with the grid in each respective microgrid, aiming to minimize operating and environmental costs simultaneously. In the second layer, a central energy management system (CEMS) is introduced as a supervisory entity. The optimal output solutions obtained by the CEMS are employed as references for scheduling internal power trading between microgrids with surplus and deficit power, aiming to minimize costs associated with power trading between the MMG system and the main grid, as well as power losses during transactions. A case study under two different scenarios is conducted on a grid-connected MMG system using real-world data and the real-time simulator Opal-RT® OP5600 to verify the effectiveness of the proposed solution in comparison with the well-known non-dominated sorting genetic algorithm II (NSGA-II) technique. The results demonstrate that the proposed methodology improved overall MMG system costs by $$30.41\%$$ in Scenario I and $$29.67\%$$ in Scenario II, compared to the results obtained with NSGA-II.},
  archive      = {J_NCA},
  author       = {Conchas, Robin F. and Loukianov, Alexander G. and Sanchez, Edgar N. and Coronado-Mendoza, Alberto and Alanis, Alma Y.},
  doi          = {10.1007/s00521-025-11120-w},
  journal      = {Neural Computing and Applications},
  month        = {6},
  number       = {18},
  pages        = {11863-11889},
  shortjournal = {Neural Comput. Appl.},
  title        = {Bi-layer energy management framework based on neural networks for multi-microgrid systems},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). KPQDG: A key sentence prompt-tuning for automatic question-distractor pairs generation. <em>NCA</em>, <em>37</em>(18), 11843-11861. (<a href='https://doi.org/10.1007/s00521-025-11149-x'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Automatic question generation(QG) and distractor generation(DG) for reading comprehension are widely studied in the field of NLP, with application scenarios both for computer models and human education. However, most previous work treats these two tasks as separate endeavors, resulting in narrower single-objective usage scenarios. Additionally, they often rely on traditional encoder-decoder frameworks, which pose challenges for maintaining long sequence quality. To achieve the coherent generation of question-distractor pairs and enhance the quality of results to make them suitable for human examinations, we propose a novel method that utilizes key sentence information to assist in prompt-tuning the pre-trained model. Our method treats QG and DG as dual tasks and performs joint-learning to ensure semantic alignment in the generated results. Specifically, we design a graph attention encoder and a dual gated attention layer to enhance the encoding and reasoning abilities based on the source passage. Experiments show that the proposed methods align well with the characteristics of our target. Experimental results demonstrate that our model is superior in both automatic and human evaluations, with QG and DG achieving $$\hbox {Bleu}_4$$ scores of 22.58 and 9.78, respectively. Further studies also indicate the quality and difficulty level of the generated contents are applicable for real-world educational contexts.},
  archive      = {J_NCA},
  author       = {Yin, Zhongwei and Li, Li},
  doi          = {10.1007/s00521-025-11149-x},
  journal      = {Neural Computing and Applications},
  month        = {6},
  number       = {18},
  pages        = {11843-11861},
  shortjournal = {Neural Comput. Appl.},
  title        = {KPQDG: A key sentence prompt-tuning for automatic question-distractor pairs generation},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Heterogeneous graph multi-level semantics extraction for node classification. <em>NCA</em>, <em>37</em>(18), 11821-11841. (<a href='https://doi.org/10.1007/s00521-025-11133-5'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Heterogeneous graphs, with their diverse nodes and edge types, offer a detailed and accurate representation of real-world semantic connections. Recently, many excellent graph models have been proposed for heterogeneous graph nodes representation and have achieved eye-catching performance, but they often fail to fully identify and extract the semantic information in heterogeneous graphs for downstream tasks, especially node classification. This paper focuses on capturing rich semantics within heterogeneous graphs for node representation learning and node classification. We present an algorithm to extract multi-level semantics on heterogeneous graphs. For node semantics, our algorithm can capture node-to-node interactions through double-node semantics fusion. For meta-path semantics, we propose to search for nodes at different hops according to given meta-paths to generate meta-path semantics representation. And finally for class semantics, we propose a novel multi-round projection and classification module to resolve the prevalent semantic deviation problem in class label representation. To verify and study the performance of our proposed model, we conduct experiments from three aspects including general node classification performance, parameter sensitivity analysis and ablation study. And the results evidence that our comprehensive extraction and representation of three proposed levels of semantic information on heterogeneous graphs can build better node representations and further improve node classification performance.},
  archive      = {J_NCA},
  author       = {Hao, Haochang and Huang, Jun and Rao, Shuzhen},
  doi          = {10.1007/s00521-025-11133-5},
  journal      = {Neural Computing and Applications},
  month        = {6},
  number       = {18},
  pages        = {11821-11841},
  shortjournal = {Neural Comput. Appl.},
  title        = {Heterogeneous graph multi-level semantics extraction for node classification},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Average impulsive estimation-based exponential synchronization of chaotic neural networks with time-varying delayed impulses. <em>NCA</em>, <em>37</em>(18), 11799-11820. (<a href='https://doi.org/10.1007/s00521-025-11114-8'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, the issue of exponential synchronization in chaotic neural networks with system time-varying delay is investigated by employing delayed impulsive control. In order to tackle the challenge posed by the flexible delay existing in impulsive controller, the concept of average impulsive delay (AID) is adopted, which considers time delay in impulse from a holistic perspective. Then, some Lyapunov-based relaxed synchronization conditions are established by applying average impulsive interval and AID methods, where such delay is no longer constrained by some strong conditions. Meanwhile, considering different functionalities of impulsive signal, the convergence rates are precisely calculated, which indicates that the system delay may either promote or impede network synchronization. In addition, the results also clearly show that the time delay in impulse plays either a positive or a negative role in the synchronization of chaotic neural networks. Moreover, allow for the controller involving synchronizing and desynchronizing delayed impulses, the notion of average impulsive estimation serves to address time-varying impulsive effects. Ultimately, three examples are simulated to exemplify the rightness of the theoretical results.},
  archive      = {J_NCA},
  author       = {Geng, Ziqing and Tang, Ze and Ding, Dong},
  doi          = {10.1007/s00521-025-11114-8},
  journal      = {Neural Computing and Applications},
  month        = {6},
  number       = {18},
  pages        = {11799-11820},
  shortjournal = {Neural Comput. Appl.},
  title        = {Average impulsive estimation-based exponential synchronization of chaotic neural networks with time-varying delayed impulses},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Exploratory data analysis, time series analysis, crime type prediction, and trend forecasting in crime data using machine learning, deep learning, and statistical methods. <em>NCA</em>, <em>37</em>(18), 11773-11798. (<a href='https://doi.org/10.1007/s00521-025-11094-9'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Criminal activities are a critical obstacle to socioeconomic development and must be controlled. However, human surveillance-based control methods are prone to error, raise legal concerns, and necessitate the development of more robust alternatives. This study aims to contribute to the development of strategies for reducing and preventing crime by ensuring the optimal allocation of police resources to locations at the right time. To achieve this goal, crime datasets from three of the most metropolitan cities in the USA—San Francisco, Chicago, and Philadelphia—were subjected to comprehensive preprocessing and exploratory data analysis. The analysis identified the most reliable and dangerous months, days, and hours in terms of the frequency of criminal incidents, the most common types of crimes, and the police districts with the highest crime rates. Crime-type prediction models were developed using machine learning algorithms, including XGBoost, CatBoost, random forest (RF), decision tree (DT), multilayer perceptron (MLP), K-nearest neighbors (KNN), Gaussian Naive Bayes (GNB), and logistic regression (LR). Additionally, time series analyses were conducted in 10, 22, and 22 different police districts for the three datasets, respectively, using deep learning models such as long short-term memory (LSTM) and bidirectional long short-term memory (BLSTM) and statistical methods such as Holt–Winters exponential smoothing (HWES), Prophet, and seasonal autoregressive integrated moving average (SARIMA). The primary objective was to accurately predict future high-crime hot spots. Furthermore, crime trend forecasts for the next 5 years were made using the best models, based on the lowest root-mean-squared error (RMSE) values obtained through statistical methods. By combining traditional machine learning methods, deep learning approaches, and statistical techniques, this study analyzed criminal incidents from various perspectives, including crime-type prediction, regional crime prediction, trend forecasting, and exploratory data analysis. The results obtained are expected to contribute to the development of proactive policing strategies.},
  archive      = {J_NCA},
  author       = {İlgün, Esen Gül and Dener, Murat},
  doi          = {10.1007/s00521-025-11094-9},
  journal      = {Neural Computing and Applications},
  month        = {6},
  number       = {18},
  pages        = {11773-11798},
  shortjournal = {Neural Comput. Appl.},
  title        = {Exploratory data analysis, time series analysis, crime type prediction, and trend forecasting in crime data using machine learning, deep learning, and statistical methods},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Kernelshap-nas: A shapley additive explanatory approach for characterizing operation influences. <em>NCA</em>, <em>37</em>(18), 11755-11771. (<a href='https://doi.org/10.1007/s00521-025-11071-2'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Neural architecture search is a rapidly growing field that has shown promising results in various applications. Existing gradient-based NAS approaches achieved high-quality results by diminishing the expense of computation. However, the updated parameters do not accurately reflect the importance of different operations. Recently, a Shapley-value-based method Shapley-NAS has been introduced to rise above the drawbacks. Shapley value calculation has been known as a NP-complete problem, which Shapley-NAS adapted and surpassed the practical constraint by Monte-Carlo sampling. However, this method raised concerns whether the sampling method is fair or not since the number of samples is too small compared to the total number of permutations in the sample space, which may lead to unstable performance. To address the problem of interpreting the model’s predictions, KernelSHAP, a well-known framework on explainable AI, precisely approximate Shapley values of each features in a dataset. KernelSHAP can potentially be utilized for approximating Shapley values in other tasks. By leveraging the power of KernelSHAP on approximating the Shapley value, we propose KernelSHAP-NAS, a more insightful approach to exploring contribution of each operator in the supernet. The proposed algorithm outperforms other existing approaches’ results in DARTS search space with the CIFAR-10 dataset. The results also show that KernelSHAP-NAS outputs better Pearson correlation and accuracy compared to other existing architecture parameter search methods on NAS-Bench-201.},
  archive      = {J_NCA},
  author       = {Tran, Thanh Hai and Nguyen, Dac Tam and Ngo, Minh Duc and Doan, Long and Luong, Ngoc Hoang and Binh, Huynh Thi Thanh},
  doi          = {10.1007/s00521-025-11071-2},
  journal      = {Neural Computing and Applications},
  month        = {6},
  number       = {18},
  pages        = {11755-11771},
  shortjournal = {Neural Comput. Appl.},
  title        = {Kernelshap-nas: A shapley additive explanatory approach for characterizing operation influences},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Modeling dislocation dynamics data using semantic web technologies. <em>NCA</em>, <em>37</em>(18), 11737-11753. (<a href='https://doi.org/10.1007/s00521-024-10674-5'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The research in Materials Science and Engineering focuses on the design, synthesis, properties, and performance of materials. An important class of materials that is widely investigated are crystalline materials, including metals and semiconductors. Crystalline material typically contains a specific type of defect called “dislocation”. This defect significantly affects various material properties, including bending strength, fracture toughness, and ductility. Researchers have devoted a significant effort in recent years to understanding dislocation behaviour through experimental characterization techniques and simulations, e.g., dislocation dynamics simulations. This paper presents how data from dislocation dynamics simulations can be modelled using semantic web technologies through annotating data with ontologies. We extend the dislocation ontology by adding missing concepts and aligning it with two other domain-related ontologies (i.e., the Elementary Multi-perspective Material Ontology and the Materials Design Ontology), allowing for efficiently representing the dislocation simulation data. Moreover, we present a real-world use case for representing the discrete dislocation dynamics data as a knowledge graph (DisLocKG) which can depict the relationship between them. We also developed a SPARQL endpoint that brings extensive flexibility for querying DisLocKG.},
  archive      = {J_NCA},
  author       = {Ihsan, Ahmad Zainul and Fathalla, Said and Sandfeld, Stefan},
  doi          = {10.1007/s00521-024-10674-5},
  journal      = {Neural Computing and Applications},
  month        = {6},
  number       = {18},
  pages        = {11737-11753},
  shortjournal = {Neural Comput. Appl.},
  title        = {Modeling dislocation dynamics data using semantic web technologies},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Enhancing quality inspection in automotive manufacturing through deep learning and transfer learning. <em>NCA</em>, <em>37</em>(18), 11711-11736. (<a href='https://doi.org/10.1007/s00521-024-10534-2'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Ensuring quality control and accurate defect detection are paramount in the automotive manufacturing industry. Nevertheless, manual and automated optical inspection approaches have encountered limitations in efficiently addressing this area. To overcome these challenges, automotive companies are actively investigating the utilization of deep learning models to automate and optimize quality control processes. This paper focuses on the development of image classification models with the specific objective of accurately categorizing industrial components as acceptable or defective. The targeted manufacturing processes include terminal crimps, gluing the cap of coils, soldering pins and inductors. Our approach leverages transfer learning to mitigate the limited data, utilizing ten state-of-the-art convolutional neural network models (CNNs) previously developed for the ImageNet Large-Scale Visual Recognition Challenge (ILSVRC): DenseNet121, DenseNet201, InceptionResNetV2, InceptionV3, MobileNetV2, ResNet50V2, ResNet101V2, ResNet152V2, VGG19, and Xception. The last layer of these models replaced to align with our dataset's classes, and various strategies such as freezing layers, fine-tuning, adding other layers, and retraining are employed. Finally, we evaluated our performances models using metrics such as accuracy, precision, recall, and ROC–AUC. The results demonstrate the efficiency of transfer learning in quality inspection classification, with DenseNet121 and VGG19 achieving accuracy and precision rates exceeding 99%.},
  archive      = {J_NCA},
  author       = {El Wahabi, Abdelhamid and Mesquiny, Ayoub and El Ahmadi, Oumaima and Baraka, Ibrahim Hadj and Hamdoune, Salaheddine and Boudhir, Anouar Abdelhakim},
  doi          = {10.1007/s00521-024-10534-2},
  journal      = {Neural Computing and Applications},
  month        = {6},
  number       = {18},
  pages        = {11711-11736},
  shortjournal = {Neural Comput. Appl.},
  title        = {Enhancing quality inspection in automotive manufacturing through deep learning and transfer learning},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Correction: Enhancing real-time fire detection: An effective multi-attention network and a fire benchmark. <em>NCA</em>, <em>37</em>(18), 11709. (<a href='https://doi.org/10.1007/s00521-023-09413-z'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_NCA},
  author       = {Khan, Taimoor and Khan, Zulfiqar Ahmad and Choi, Chang},
  doi          = {10.1007/s00521-023-09413-z},
  journal      = {Neural Computing and Applications},
  month        = {6},
  number       = {18},
  pages        = {11709},
  shortjournal = {Neural Comput. Appl.},
  title        = {Correction: Enhancing real-time fire detection: An effective multi-attention network and a fire benchmark},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Enhancing real-time fire detection: An effective multi-attention network and a fire benchmark. <em>NCA</em>, <em>37</em>(18), 11693-11707. (<a href='https://doi.org/10.1007/s00521-023-09298-y'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Over the past decades, fire has been considered one of the most serious natural disasters because of its devastating nature, rapid spread, and high impact on the ecology, economy, environment, and life preservation. Therefore, early fire detection has immense significance in computer vision. However, existing methods suffer from high false prediction rates and slow inference times, which limit their real-time applicability. To bridge these gaps, this study introduces a multi-attention fire network (MAFire-Net) that integrates a modified ConvNeXtTiny (ConvNeXt-T) architecture with channel attention (CA) and spatial attention (SA) modules. These attention modules are integrated after each block of the ConvNeXt-T architecture where the CA module is responsible for capturing dominant channels within the features, leading to highly emphasized feature maps. The SA module enhances the spatial details, enabling the model to distinguish between fire and non-fire scenarios more accurately. Additionally, fine-tuning strategies are applied to streamline the ConvNeXt-T architecture, resulting in an optimized model tailored for real-world fire detection. Furthermore, a comprehensive large-scale fire dataset is developed that encompasses diverse, imbalanced, and challenging fire/nonfire images (both indoors and outdoors). Extensive experiments were conducted to validate the superior generalization capability of the MAFire-Net compared with several baseline architectures using four benchmarks (Yar, BowFire, FD, and DFAN). The experimental results demonstrated that the proposed MAFire-Net outperforms state-of-the-art (SOTA) techniques, demonstrating higher accuracy and faster inference times, which make it an ideal choice for real-time deployment over edge devices.},
  archive      = {J_NCA},
  author       = {Khan, Taimoor and Khan, Zulfiqar Ahmad and Choi, Chang},
  doi          = {10.1007/s00521-023-09298-y},
  journal      = {Neural Computing and Applications},
  month        = {6},
  number       = {18},
  pages        = {11693-11707},
  shortjournal = {Neural Comput. Appl.},
  title        = {Enhancing real-time fire detection: An effective multi-attention network and a fire benchmark},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Quality–diversity optimization of decision trees for interpretable reinforcement learning. <em>NCA</em>, <em>37</em>(18), 11681-11692. (<a href='https://doi.org/10.1007/s00521-023-09124-5'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the current Artificial Intelligence (AI) landscape, addressing explainability and interpretability in Machine Learning (ML) is of critical importance. In fact, the vast majority of works on AI focus on Deep Neural Networks (DNNs), which are not interpretable, as they are extremely hard to inspect and understand for humans. This is a crucial disadvantage of these methods, which hinders their trustability in high-stakes scenarios. On the other hand, interpretable models are considerably easier to inspect, which allows humans to test them exhaustively, and thus trust them. While the fields of eXplainable Artificial Intelligence (XAI) and Interpretable Artificial Intelligence (IAI) are progressing in supervised settings, the field of Interpretable Reinforcement Learning (IRL) is falling behind. Several approaches leveraging Decision Trees (DTs) for IRL have been proposed in recent years. However, all of them use goal-directed optimization methods, which may have limited exploration capabilities. In this work, we extend a previous study on the applicability of Quality–Diversity (QD) algorithms to the optimization of DTs for IRL. We test the methods on two well-known Reinforcement Learning (RL) benchmark tasks from OpenAI Gym, comparing their results in terms of score and “illumination” patterns. We show that using QD algorithms is an effective way to explore the search space of IRL models. Moreover, we find that, in the context of DTs for IRL, QD approaches based on MAP-Elites (ME) and its variant Covariance Matrix Adaptation MAP-Elites (CMA-ME) can significantly improve convergence speed over the goal-directed approaches.},
  archive      = {J_NCA},
  author       = {Ferigo, Andrea and Custode, Leonardo Lucio and Iacca, Giovanni},
  doi          = {10.1007/s00521-023-09124-5},
  journal      = {Neural Computing and Applications},
  month        = {6},
  number       = {18},
  pages        = {11681-11692},
  shortjournal = {Neural Comput. Appl.},
  title        = {Quality–diversity optimization of decision trees for interpretable reinforcement learning},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Surrogate-assisted hyper-parameter search for portfolio optimisation: Multi-period considerations. <em>NCA</em>, <em>37</em>(18), 11663-11680. (<a href='https://doi.org/10.1007/s00521-023-09176-7'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Portfolio management is a multi-period multi-objective optimisation problem subject to various constraints. However, portfolio management is treated as a single-period problem partly due to the computationally burdensome hyper-parameter search procedure needed to construct a multi-period Pareto frontier. This study presents the Pareto driven surrogate (ParDen-Sur) modelling framework to efficiently perform the required hyper-parameter search. ParDen-Sur extends previous surrogate frameworks by including a reservoir sampling-based look-ahead mechanism for offspring generation in evolutionary algorithms (EAs) alongside the traditional acceptance sampling scheme. We evaluate this framework against, and in conjunction with, several seminal multi-objective (MO) EAs on two datasets for both the single- and multi-period use cases. When considering hypervolume ParDen-Sur improves marginally (0.8%) over the state-of-the-art (SOTA)-NSGA-II. However, for generational distance plus and inverted generational distance plus, these improvements over the SOTA are 19.4% and 66.5%, respectively. When considering the average number of evaluations and generations to reach a 99% success rate, ParDen-Sur is shown to be 1.84× and 2.02× more effective than the SOTA. This improvement is statistically significant for the Pareto frontiers, across multiple EAs, for both datasets and use cases.},
  archive      = {J_NCA},
  author       = {van Zyl, Terence L. and Woolway, Matthew and Paskaramoorthy, Andrew},
  doi          = {10.1007/s00521-023-09176-7},
  journal      = {Neural Computing and Applications},
  month        = {6},
  number       = {18},
  pages        = {11663-11680},
  shortjournal = {Neural Comput. Appl.},
  title        = {Surrogate-assisted hyper-parameter search for portfolio optimisation: Multi-period considerations},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Hybrid GA-PSO method with local search and image clustering for automatic IFS image reconstruction of fractal colored images. <em>NCA</em>, <em>37</em>(18), 11635-11661. (<a href='https://doi.org/10.1007/s00521-023-08954-7'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Over the past few decades, the application of iterated function systems (IFS) in reconstructing fractal images has been a challenging research area. Numerous methods have been proposed to address this issue. However, they generally focus on binary or grayscale images, neglecting the color component of the process. Consequently, they are unsuitable for reconstructing colored images. In a previous paper presented at the ISCMI 2021 conference, the authors introduced a novel approach that utilizes the cuckoo search algorithm and k-means clustering for IFS fractal reconstruction of colored images. Building upon that work, this paper introduces an enhanced and extended method by combining genetic algorithms (GAs) and particle swarm optimization (PSO) with local search and image clustering. In this approach, GA and PSO are mutually coupled to automatically determine the color of the contractive functions and the IFS parameters, respectively. The output of each method serves as the input for the other in an iterative manner. Main contributions of this method are: (1) it computes automatically the optimal number and IFS code of the contractive functions; (2) the color of the contractive functions is determined automatically through an optimization process using GA; (3) a local refinement step is performed to further enhance the final solution. Overall, this new method yields highly accurate results in reconstructing the geometry and color of input fractal images, without requiring any additional information about the target beyond the bitmap image.},
  archive      = {J_NCA},
  author       = {Gálvez, Akemi and Fister, Iztok and Deb, Suash and Iglesias, Andrés},
  doi          = {10.1007/s00521-023-08954-7},
  journal      = {Neural Computing and Applications},
  month        = {6},
  number       = {18},
  pages        = {11635-11661},
  shortjournal = {Neural Comput. Appl.},
  title        = {Hybrid GA-PSO method with local search and image clustering for automatic IFS image reconstruction of fractal colored images},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Topic-guided abstractive multimodal summarization with multimodal output. <em>NCA</em>, <em>37</em>(18), 11619-11634. (<a href='https://doi.org/10.1007/s00521-023-08821-5'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Summarization is a technique that produces condensed text from large text documents by using different deep-learning techniques. Over the past few years, abstractive summarization has drawn much attention because of the capability of generating human-like sentences with the help of machines. However, it must improve repetition, redundancy and lexical problems while generating sentences. Previous studies show that incorporating images with text modality in the abstractive summary may reduce redundancy, but the concentration still needs to lay on the semantics of the sentences. This paper considers adding a topic to a multimodal summary to address semantics and linguistics problems. This stress the need to develop a multimodal summarization system with the topic. Multimodal summarization uses two or more modalities to extract the essential features to increase user satisfaction in generating an abstractive summary. However, the paper’s primary aim is to explore the generation of user preference summaries of a particular topic by proposing a Hybrid Image Text Topic (HITT) to guide the extracted essential information from text and image modalities with the help of topic that addresses semantics and linguistic problems to generate a topic-guided abstractive multimodal summary. Furthermore, a caption-summary order space technique has been introduced in this proposed work to retrieve the relevant image for the generated summary. Finally, the MSMO dataset compares and validates the results with rouge and image precision scores. Besides, we also calculated the model’s loss using sparse categorical cross entropy and showed significant improvement over other state-of-the-art techniques.},
  archive      = {J_NCA},
  author       = {Rafi, Shaik and Das, Ranjita},
  doi          = {10.1007/s00521-023-08821-5},
  journal      = {Neural Computing and Applications},
  month        = {6},
  number       = {18},
  pages        = {11619-11634},
  shortjournal = {Neural Comput. Appl.},
  title        = {Topic-guided abstractive multimodal summarization with multimodal output},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Massive open online course recommendation system based on a reinforcement learning algorithm. <em>NCA</em>, <em>37</em>(18), 11607-11618. (<a href='https://doi.org/10.1007/s00521-023-08686-8'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Massive open online courses (MOOCs) are open online courses designed on the basis of the teaching progress. Videos and learning exercises are used as learning materials in these courses, which are open to numerous users. However, determining the prerequisite knowledge and learning progress of learners is difficult. On the basis of learners’ online learning trajectory, we designed a set of practice questions for a recommendation system for MOOCs, provided suitable practice questions to students through the LINE chatbot (a type of social media software), and used mobile devices to encourage participation in MOOCs. Reinforcement learning, which involves reward function design and iterative solution improvement, was used to set task goals, including those related to course learning and practice question difficulty. The proposed system encouraged certain learning behaviors among students. Students who used the system exhibited an exercise completion rate of 89.97%, which was higher than that of students who did not use the system (47.23%). The system also increased the students’ overall learning effectiveness. Students who used and did not use the proposed system exhibited average midterm scores of 64.73 and 58.21, respectively. We also collected 227 online questionnaires from students. The results of the questionnaires indicated that 90% of the students were satisfied with the system and hoped to continue using it.},
  archive      = {J_NCA},
  author       = {Tzeng, Jian-Wei and Huang, Nen-Fu and Chuang, An-Chi and Huang, Ting-Wei and Chang, Hong-Yi},
  doi          = {10.1007/s00521-023-08686-8},
  journal      = {Neural Computing and Applications},
  month        = {6},
  number       = {18},
  pages        = {11607-11618},
  shortjournal = {Neural Comput. Appl.},
  title        = {Massive open online course recommendation system based on a reinforcement learning algorithm},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Enhancing single-lead electrocardiogram arrhythmia detection with empirical mode decomposition. <em>NCA</em>, <em>37</em>(17), 11583-11605. (<a href='https://doi.org/10.1007/s00521-025-11089-6'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cardiac arrhythmias pose a significant challenge to health care, requiring accurate and reliable detection methods to enable early diagnosis and treatment. However, traditional ECG beat classification methods often lack robustness and fail to generalize effectively to diverse clinical scenarios, particularly when using single-lead-I recordings, which are less studied compared to lead II. To address this gap, this paper proposes a novel method that combines deep learning and empirical mode decomposition (EMD) for arrhythmia detection using single-lead-I ECG recordings obtained with electrodes on each hand. The hybrid model comprises 149 layers of one-dimensional convolution, max pooling, and residual blocks. By leveraging EMD, it classifies ECG beats into normal beats, premature ventricular contractions, and atrial premature beats with high sensitivity (99.86%, 99.72%, and 99.84%, respectively) using the first four intrinsic mode functions (IMFs). The proposed method is applicable in clinical and out-of-hospital monitoring scenarios, and an accompanying web application provides real-time analysis and diagnostic results. This approach addresses a critical gap in lead-I ECG analysis, enhancing diagnostic precision and expanding the scope of ECG applications in health care .},
  archive      = {J_NCA},
  author       = {Issa, Mohamed F. and Yousry, Ahmed and Tuboly, Gergely and Wang, Zeyu and Zoltan, Juhasz and Selim, Mazen M. and AbuEl-Atta, Ahmed H.},
  doi          = {10.1007/s00521-025-11089-6},
  journal      = {Neural Computing and Applications},
  month        = {6},
  number       = {17},
  pages        = {11583-11605},
  shortjournal = {Neural Comput. Appl.},
  title        = {Enhancing single-lead electrocardiogram arrhythmia detection with empirical mode decomposition},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Detecting and classifying breast masses via YOLO-based deep learning. <em>NCA</em>, <em>37</em>(17), 11555-11582. (<a href='https://doi.org/10.1007/s00521-025-11153-1'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Breast cancer has a high incidence and mortality rate in the female population. Mammography is the most reliable method for early and accurate diagnosis of breast cancer. Automated detection and classification of breast masses on mammograms is a challenging task and is essential to assist radiologists in accurately diagnosing breast masses. The aim of this study is to develop a Computer-Aided Diagnosis (CAD) system based on You Look Only Once (YOLO) for identifying breast masses and classifying them as benign or malignant. We propose a YOLOv5-CAD framework that uses a transfer learning approach. Two datasets, CBIS-DDSM and VinDr-Mammo, are utilized for training from scratch. The model weights and parameters are subsequently transferred and fine-tuned onto the smaller INBreast dataset. Furthermore, an analysis is conducted to assess the impact of various data augmentation techniques during the training phase on enhancing model performance. The proposed framework demonstrates encouraging fivefold cross-validation evaluation results. To conclude, transfer learning from CBIS-DDSM achieves 0.843 mAP, precision of 0.855, recall of 0.774, while transfer learning from VinDr- Mammo reaches 0.84 mAP, precision of 0.829, recall of 0.787. Furthermore, the performance of the two fine-tuned models was tested on both the MIAS dataset and the private dataset from Başkent University Ankara Hospital. Such promising performance could be useful for the CAD frameworks being developed to support radiologists as a second opinion reader for the detection and classification of breast masses.},
  archive      = {J_NCA},
  author       = {Karaca Aydemir, Büşra Kübra and Telatar, Ziya and Güney, Selda and Dengiz, Berna},
  doi          = {10.1007/s00521-025-11153-1},
  journal      = {Neural Computing and Applications},
  month        = {6},
  number       = {17},
  pages        = {11555-11582},
  shortjournal = {Neural Comput. Appl.},
  title        = {Detecting and classifying breast masses via YOLO-based deep learning},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Brain tumor diagnosis using modified DenseNet121 architecture with adaptive learning rate and callback mechanism. <em>NCA</em>, <em>37</em>(17), 11527-11553. (<a href='https://doi.org/10.1007/s00521-025-11150-4'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Brain tumors pose a major global health challenge, necessitating early and precise diagnosis for successful treatment. This research tackles the problem of identifying various brain tumor types using MRI images. Existing approaches, which depend on traditional and basic deep learning techniques, struggle with issues such as lower accuracy, poor generalization, and difficulties in managing MRI image variations. These methods also fail to provide real-time solutions and are impractical for transfer learning. Our proposed model leverages advanced deep learning with the DenseNet121 architecture, enhanced by adaptive learning rates and callbacks for improved generalization and efficiency. Adaptive learning rates dynamically adjust the training pace in response to changes in tumor patterns, overcoming the drawbacks of fixed learning rates. Callbacks, including model checkpointing and early stopping, optimize training by saving the best configurations and preventing overfitting, addressing problems like lower accuracy and poor generalization. The model achieves a training accuracy of 98.81% and a test accuracy of 98.09%, outperforming current methods. It demonstrates balanced precision, recall, and F1-score across tumor types, validating its effectiveness in clinical neuro-oncology applications.},
  archive      = {J_NCA},
  author       = {Venkatachalam, Chandrasekar and Shah, Priyanka and Renukadevi, P. and John, Sincy and Venkatachalam, Shanmugavalli},
  doi          = {10.1007/s00521-025-11150-4},
  journal      = {Neural Computing and Applications},
  month        = {6},
  number       = {17},
  pages        = {11527-11553},
  shortjournal = {Neural Comput. Appl.},
  title        = {Brain tumor diagnosis using modified DenseNet121 architecture with adaptive learning rate and callback mechanism},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A new fuzzy cross-belief entropy method for trajectory prediction. <em>NCA</em>, <em>37</em>(17), 11501-11526. (<a href='https://doi.org/10.1007/s00521-025-11141-5'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Pedestrian trajectory prediction is crucial in many fields such as autonomous driving and intelligent robot navigation. Currently, many prediction methods based on generative adversarial networks (GAN) typically model the uncertainty of the discriminator network’s output in the form of probabilities and then optimize the network using cross-entropy. However, this approach rigidly limits the network’s output to only two possibilities, yes or no, and fails to adequately express the uncertainty of the network’s output. This may result in poor adaptability of the network to unknown situations, limiting the improvement of prediction accuracy, especially in capturing predictions for complex scenarios. Evidence theory exhibits strong applicability in complex uncertain environments, able to clearly distinguish between ‘uncertainty’ and ‘ignorance.’ This distinction enables effective handling of uncertain information, which is crucial for practical applications. In evidence theory, belief entropy captures the uncertainty of information by measuring the degree of belief in propositions. Belief entropy remains usable even when prior knowledge is insufficient, demonstrating its flexibility. Inspired by this, we propose a trajectory prediction method that integrates the processing of uncertain information. This method aims to enhance the flexibility of the network, improve its ability to handle uncertain information, and thus increase prediction accuracy. Firstly, by incorporating the evidence theory, the uncertainty of the output from the discriminator network is modeled. Specifically, this is achieved by modeling the soft output state in combination with evidence theory to better express the uncertainty within the network, thus making the network more intelligent. The approach transforms the original probability output of the network into the belief output of evidence theory. Additionally, as belief entropy effectively deals with uncertain information, a new loss called fuzzy cross-belief entropy is proposed by combining belief entropy and cross-entropy based on the soft output state to optimize the network. Finally, the network’s ability to process trajectory information is improved, resulting in more reasonable trajectories. Experiments on the public datasets ETH/UCY show that compared with the baseline method, the average displacement error has been reduced by 22.22 $$\%$$ , and the final displacement error has been reduced by 28.69 $$\%$$ . The proposed method effectively improves the prediction accuracy of trajectories on some subsets, particularly for complex situations. In most cases, such as pedestrians in pairs, avoiding, turning sharply, stopping, and other trajectories can be effectively predicted.},
  archive      = {J_NCA},
  author       = {Yang, Tian and Wang, Gang and Lai, Jian and Wang, Yang},
  doi          = {10.1007/s00521-025-11141-5},
  journal      = {Neural Computing and Applications},
  month        = {6},
  number       = {17},
  pages        = {11501-11526},
  shortjournal = {Neural Comput. Appl.},
  title        = {A new fuzzy cross-belief entropy method for trajectory prediction},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Diversity-based niche genetic algorithm for bi-objective mixed fleet vehicle routing problem with time window. <em>NCA</em>, <em>37</em>(17), 11479-11499. (<a href='https://doi.org/10.1007/s00521-025-11132-6'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the gradual introduction of electric vehicles, logistics systems currently rely primarily on mixed fleets of electric and conventional vehicles to carry out distribution tasks. Given the differences in pollution emissions between electric and traditional vehicles, as well as the additional consideration of charging routes for electric vehicles, a reasonable formation of the two types of vehicles will effectively reduce carbon emissions during distribution and enhance the economic benefits of the logistics system. This article establishes a bi-objective model aimed at minimizing carbon emissions and travel costs while maximizing customer satisfaction. A diversity-based niche genetic algorithm is proposed to better explore distribution schemes using different kinds of mixed fleets. Firstly, a partial variable-length coding method is introduced to facilitate chromosome generation in the model. Secondly, a diversity evaluation based on entropy is proposed to search for various distribution schemes. Finally, niche elitism and niche tournament selection are proposed, and the crossover and mutation operators are correspondingly improved for the algorithm. The experiments suggest that the bi-objective model is well-established, demonstrating the anticipated conflict between the two objectives. Additionally, the diversity-based niche genetic algorithm is shown to be effective in achieving a wider variety of distribution schemes.},
  archive      = {J_NCA},
  author       = {Du, Shengli and Li, Shilong and Han, Honggui and Qiao, Junfei},
  doi          = {10.1007/s00521-025-11132-6},
  journal      = {Neural Computing and Applications},
  month        = {6},
  number       = {17},
  pages        = {11479-11499},
  shortjournal = {Neural Comput. Appl.},
  title        = {Diversity-based niche genetic algorithm for bi-objective mixed fleet vehicle routing problem with time window},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Enhancing the efficiency of lung cancer screening: Predictive models utilizing deep learning from CT scans. <em>NCA</em>, <em>37</em>(17), 11459-11477. (<a href='https://doi.org/10.1007/s00521-025-11084-x'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Lung cancer is the most lethal form of cancer. This paper introduces a novel framework to discern and classify pulmonary disorders such as pneumonia, tuberculosis, and lung cancer by analyzing conventional X-ray and CT scan images called lung cancer risk prediction (LCRP) model. LCRP has four modules, namely data collection and preprocessing, data augmentation module, image segmentation module, and prediction module. Actually, LCRP employs three deep learning models; sequential model, functional model, and transfer model on publicly available training datasets. Convolutional neural networks (CNNs) have emerged as a highly effective field in machine learning, particularly for image datasets in the field of biomedical applications. The primary goal is to validate these models by comparing their performance with other models in order to determine their effectiveness in addressing challenging datasets. Our research has revealed a noteworthy enhancement in the efficiency of binary and multi-class classification using mask R-CNN image segmentation. During the model training process, a combination of Adam and stochastic gradient descent dual optimizers has been used to improve performance. LCRP have outperformed current pre-trained models by minimizing training parameters, computational costs, and overhead. It introduces 98.5% accuracy, 88.7% specificity, 89% sensitivity, 89.2% precision, and 89.09% F-measure.},
  archive      = {J_NCA},
  author       = {Tawfeek, Medhat A. and Alrashdi, Ibrahim and Alruwaili, Madallah and Shaban, Warda M. and Talaat, Fatma M.},
  doi          = {10.1007/s00521-025-11084-x},
  journal      = {Neural Computing and Applications},
  month        = {6},
  number       = {17},
  pages        = {11459-11477},
  shortjournal = {Neural Comput. Appl.},
  title        = {Enhancing the efficiency of lung cancer screening: Predictive models utilizing deep learning from CT scans},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Comprehensive analysis of various imputation and forecasting models for predicting PM2.5 pollutant in delhi. <em>NCA</em>, <em>37</em>(17), 11441-11458. (<a href='https://doi.org/10.1007/s00521-025-11047-2'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Understanding and predicting air quality is pivotal for public health and environmental management, especially in urban areas like Delhi. This study utilizes a comprehensive dataset from the Central Pollution Control Board, detailing air pollutant concentrations at the ITO (Income Tax Office), Delhi station, from 2017 to 2023. Our focus on pollutants such as PM2.5, PM10, NO2, NH3, SO2, ozone, and CO highlights the challenges posed by missing data in environmental studies. Through the application of several imputation models, including KNN, linear regression, forward fill + backward fill, Fourier + KNN, linear interpolation, and statistical methods, we identified combination of forward fill and backward fill as the most effective method for addressing data gaps, specifically for ozone measurements. Building on this imputed dataset, we employ various forecasting models to predict PM2.5 levels, a critical pollutant. Our exploration includes time series and deep learning approaches, such as LSTM, Bi-LSTM, CNN-LSTM, GRU, Deep AR, WaveNet, TCN, ARIMA, SARIMA, and Prophet. The performance of these models is evaluated using daily data, with Bi-LSTM and LSTM with attention emerging as the top performers due to their accuracy in predicting PM2.5 concentrations. The implications of our findings extend beyond academic interest, offering practical insights for environmental policy and health advisories in Delhi. By demonstrating the efficacy of specific imputation and forecasting techniques, our research contributes to the broader effort of improving air quality monitoring and prediction. The success of Bi-LSTM and LSTM with attention models, in particular, suggests a promising avenue for future studies aiming to enhance the precision of environmental forecasts.},
  archive      = {J_NCA},
  author       = {Karnati, Hemanth and Soma, Anuraag and Alam, Adnan and Kalaavathi, B},
  doi          = {10.1007/s00521-025-11047-2},
  journal      = {Neural Computing and Applications},
  month        = {6},
  number       = {17},
  pages        = {11441-11458},
  shortjournal = {Neural Comput. Appl.},
  title        = {Comprehensive analysis of various imputation and forecasting models for predicting PM2.5 pollutant in delhi},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). EPDNet: A fast and accurate express package detection network on CPU. <em>NCA</em>, <em>37</em>(17), 11421-11440. (<a href='https://doi.org/10.1007/s00521-025-11140-6'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Express package detection is one of the most fundamental and crucial tasks in the smart logistics warehousing system. Realizing fast and accurate detection for varying scale, cluttered, and rotated objects under limited edge computing capabilities make this task challenging. This study proposes a lightweight rotation detection network to achieve fast and accurate express package detection on CPU, namely, EPDNet. Specifically, the improved CSPDarkNet53 is designed through factorized convolution and a low-cost attention module to maximize detection accuracy and speed. A simple and efficient multi-scale spatial feature extraction network is devised to improve the detection performance for multi-scale objects while maintaining efficiency. An efficient head is devised to alleviate spatial misalignment issues by decoupling the multiple object function. The circular smooth label is adopted to achieve rotated object detection. Meanwhile, multiple data augmentation techniques are implemented to improve the generalization of the network. Our detection network achieves 81.0 $$\%$$ mAP@0.5:95 (90.7 $$\%$$ mAP@0.5) at 46.3 FPS on Intel Core i9-10920X CPU for the express package detection task. This work provides an efficient method for detecting express packages in complex scenarios that can be a reference for implementing rotation detection methods in practical industrial applications.},
  archive      = {J_NCA},
  author       = {Yin, Hao and Li, Gun and Zhang, Guiping and Tang, Bin and Wang, Zihao and He, Wenhua and Chen, Lu},
  doi          = {10.1007/s00521-025-11140-6},
  journal      = {Neural Computing and Applications},
  month        = {6},
  number       = {17},
  pages        = {11421-11440},
  shortjournal = {Neural Comput. Appl.},
  title        = {EPDNet: A fast and accurate express package detection network on CPU},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multi-scale feature fusion for facial expression recognition. <em>NCA</em>, <em>37</em>(17), 11399-11420. (<a href='https://doi.org/10.1007/s00521-025-11139-z'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As an important computer vision task that can be used in many areas, facial expression recognition (FER) has been widely studied which much progress has been obtained especially when deep learning (DL) approaches have been introduced in FER. However, existing methods are restricted to extracting localized information or utilizing attention mechanism to focus on the region most relevant to the emotion, possibly resulting in errors in facial expression interpretation. Moreover, the inconsistency of image sizes in the wild FER datasets further increases the difficulty of the FER task. To address these issues, we propose a multi-scale feature fusion (MSFF) for FER. Specifically, our method first extracts features from facial images using multi-scale feature extraction to obtain rich multi-dimensional information. Meanwhile, extensive feature extraction at different scales is achieved by expanding the receptive field of the model, which provides the basis for more detailed analysis of facial expressions. Eventually, features at different scales are fused to enhance their synergistic effect. Subsequently, the adaptive extraction of comprehensive and valuable information from the fused features is performed, optimizing the performance of expression recognition. Extensive experimental results demonstrate that the proposed method outperforms the state-of-the-art methods on in-the-wild datasets. It is promising for FER in realistic and complex scenarios.},
  archive      = {J_NCA},
  author       = {Guo, Jiatao and Peng, Junjie and Huang, Yansong and Chen, Gan and Cai, Zesu and Tan, Shuhua},
  doi          = {10.1007/s00521-025-11139-z},
  journal      = {Neural Computing and Applications},
  month        = {6},
  number       = {17},
  pages        = {11399-11420},
  shortjournal = {Neural Comput. Appl.},
  title        = {Multi-scale feature fusion for facial expression recognition},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Cycle generative adversarial transformer network for MRI brain tumor segmentation. <em>NCA</em>, <em>37</em>(17), 11379-11398. (<a href='https://doi.org/10.1007/s00521-025-11137-1'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, adversarial learning-based U-Net models have achieved encouraging performance in MRI brain tumor segmentation. However, existing works still have limitations in capturing global dependencies and inter-channel semantic information of brain tumor images. To address these issues, this work proposes a novel cycle generative adversarial Transformer network, i.e., CycTransU-Net, that simultaneously integrates adversarial learning and Transformer U-Net models to improve brain tumor segmentation performance. More specifically, CycTransU-Net adopts part of CycleGAN as the adversarial learning framework, and it constructs an improved Transformer U-Net as a generator to obtain brain tumor segmentation results while utilizing PatchGAN as a discriminator to evaluate the segmentation results. In the generator, a dynamic token sparsification Transformer module is embedded into the U-Net. This module not only removes irrelevant background information in brain tumor images via dynamic sparsification but also establishes long-range dependencies to capture comprehensive global information, leading to improved tumor segmentation accuracy. Additionally, we have combined tied block convolution that facilitates the exchange of semantic information across channels, to optimize the generation of brain tumor images. In summary, our network architecture differs from previous approaches by introducing Transformer modules and bound block convolution in the CycleGAN architecture. CycTransU-Net not only facilitates the exchange of feature information across interactive channels, but also empowers the network to capture a wealth of global feature. We extensively evaluate CycTransU-Net on the public BraTS 2019–2021 and MSD datasets. Through online evaluation for BraTS 2019–2020 validation sets, it achieves dice values of 90.5%/90.6%, 85.2%/85.1%, and 77.6%/77.8% for whole tumor, core tumor, and enhancing tumor segmentation. The results demonstrate the effectiveness and competitiveness of CycTransU-Net compared with state-of-the-art works.},
  archive      = {J_NCA},
  author       = {Zhang, Muqing and Sun, Qiule and Han, Yutong and Liu, Bin and Wang, Jun and Zhang, Mingli and Toussaint, Paule-J. and Zhang, Jianxin and Evans, Alan C.},
  doi          = {10.1007/s00521-025-11137-1},
  journal      = {Neural Computing and Applications},
  month        = {6},
  number       = {17},
  pages        = {11379-11398},
  shortjournal = {Neural Comput. Appl.},
  title        = {Cycle generative adversarial transformer network for MRI brain tumor segmentation},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Augmented reality interaction: A comprehensive review of gesture and speech integration techniques. <em>NCA</em>, <em>37</em>(17), 11347-11377. (<a href='https://doi.org/10.1007/s00521-025-11190-w'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the ever-evolving landscape of Augmented Reality (AR), gesture and speech interaction technologies have emerged as pivotal components, reshaping experiences across diverse domains, from art to healthcare and education. Existing reviews may talk extensively about various types of interactions in augmented reality, but this paper fills a gap in this targeted area by discussing research that adopted both gesture and speech interactions. This paper employs the PRISMA methodology to curate and analyze a selection of cutting-edge research articles, offering a systematic and comprehensive review of 16 AR-based gesture, speech, and multimodal interaction technologies published between 2019 and 2023. Among them, “gesture + speech” accounted for 75%, while “gesture + speech + gaze” and “gesture + speech + head movement” models accounted for 12.5%. Highlighting the primary findings and contributions of this review, this article uncovers the prevailing trends in interaction technology implementation within AR environments. The review explores not only the methodologies but also the practical applications across a spectrum of AR scenarios. This comprehensive overview serves to contextualize the significance of these interaction technologies in enhancing user experiences and opens up new avenues for future research and development. Furthermore, this article underscores the real-world implications of these findings, shedding light on the potential for broader integration of gesture and speech in AR applications. As we look ahead, this paper provides insights into potential areas for further exploration in this dynamic field. By delving into the past, illuminating the present, and paving the way for the future, this review underscores the transformative power of gesture and speech interaction technologies in the realm of Augmented Reality.},
  archive      = {J_NCA},
  author       = {Bai, Jin and Sunar, Mohd Shahrizal and Mohd Suaib, Norhaida},
  doi          = {10.1007/s00521-025-11190-w},
  journal      = {Neural Computing and Applications},
  month        = {6},
  number       = {17},
  pages        = {11347-11377},
  shortjournal = {Neural Comput. Appl.},
  title        = {Augmented reality interaction: A comprehensive review of gesture and speech integration techniques},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Polyp segmentation with colonoscopic images: A study. <em>NCA</em>, <em>37</em>(17), 11311-11346. (<a href='https://doi.org/10.1007/s00521-025-11144-2'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Colon cancer is a type of cancer caused by polyps that become malignant within the colon or rectum. Dealing with colon cancer effectively requires the diagnosis of the cancer at an early stage, which is of vital importance. Computer-aided diagnostic systems are being developed to ensure the accurate and rapid diagnosis of cancer in its early stages. In this paper, deep learning approaches that segment polyps from colorectal polyp images have been reviewed, and a detailed analysis is presented. Firstly, deep learning approaches for polyp segmentation from colorectal polyp images were categorized into three categories. The categories include conventional convolutional neural networks, attention-based models, and transformer architectures. A detailed analysis of the approaches grouped under each category has been conducted. This review provides a summary of current deep learning architectures used in colorectal polyp images, detailing which dataset methods were utilized, preferred performance metrics, challenges encountered, and the hardware and software infrastructure. It is hoped that this study will be beneficial for researchers who wish to use deep learning techniques to segment colorectal polyp images in diagnosing colon cancer.},
  archive      = {J_NCA},
  author       = {Akgöl, Yaren and Toptaş, Buket and Toptaş, Murat},
  doi          = {10.1007/s00521-025-11144-2},
  journal      = {Neural Computing and Applications},
  month        = {6},
  number       = {17},
  pages        = {11311-11346},
  shortjournal = {Neural Comput. Appl.},
  title        = {Polyp segmentation with colonoscopic images: A study},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Deep learning for recognition and detection of plant diseases and pests. <em>NCA</em>, <em>37</em>(17), 11265-11310. (<a href='https://doi.org/10.1007/s00521-025-11125-5'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Plants are the primary source of food, and a secure food supply chain is essential for societal stability. Plant diseases and pests caused by seasons, environment, and pathogens can severely impact the yield of economic and food crops. Currently, farmers cannot obtain timely and effective information on the growth and distribution of diseases and pests during farmland management, leading to reduced crop yields and frequent pesticide use. Therefore, non-destructive, timely, and accurate methods for identifying diseases and pests are essential for promoting precise field management and increasing crop yields. The widespread application of artificial intelligence, particularly deep learning (DL), offers new opportunities for this approach. This paper reviews relevant literature published since 2010. It outlines the background of DL, commonly used disease and pest datasets, performance evaluation metrics, and data augmentation methods for both regular and small targets. It reviews the application status, challenges, and future research directions of DL in asymptomatic detection, visual detection and recognition, and small object detection of plant diseases and pests. The key advancements of DL in plant disease and pest recognition and detection will help further explore and expand its application in agriculture.},
  archive      = {J_NCA},
  author       = {Yue, Xiang and Qi, Kai and Na, Xinyi and Liu, Yanhua and Yang, Fuhao and Wang, Wei},
  doi          = {10.1007/s00521-025-11125-5},
  journal      = {Neural Computing and Applications},
  month        = {6},
  number       = {17},
  pages        = {11265-11310},
  shortjournal = {Neural Comput. Appl.},
  title        = {Deep learning for recognition and detection of plant diseases and pests},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A survey of deep learning methods on cell instance segmentation. <em>NCA</em>, <em>37</em>(17), 11195-11264. (<a href='https://doi.org/10.1007/s00521-025-11119-3'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cell segmentation is a key topic in medical image analysis with a wide range of applications in the study of diagnosis and prognosis of pathology and cytology. Along with the recent development of generative adversarial networks and transformers, there has been a substantial amount of work aimed at developing cell segmentation approaches using deep learning (DL) models. Inspired by this transition, in this survey, we provide a comprehensive review of the current situation and future technology development in cell instance segmentation by systematically reviewing 198 research papers, covering a broad spectrum of models for instance-level cell segmentation from 2020 to 2024, including convolutional networks, encoder–decoder architectures, recurrent networks, transformers and generative adversarial models. We have examined the loss functions, training strategies, evaluation methods, widely used datasets and quantitative performance of individual methods. A comprehensive summary of the selected seminal works on DL-based cell segmentation with microscopic images is further provided to investigate the effectiveness of methods. We have also performed a comparative analysis on two challenging cell instance segmentation datasets with technical challenges, including unclear cell boundaries, clustered or overlapping cells, variations in cell appearance and sparse or missing annotations, utilizing 18 state-of-the-art DL approaches in cell instance segmentation. Finally, we described the strengths and challenges of the cell instance segmentation models with discussions on future research directions in this area.},
  archive      = {J_NCA},
  author       = {Wang, Ching-Wei and Lee, Wei-Tang and Su, Ting-Sheng},
  doi          = {10.1007/s00521-025-11119-3},
  journal      = {Neural Computing and Applications},
  month        = {6},
  number       = {17},
  pages        = {11195-11264},
  shortjournal = {Neural Comput. Appl.},
  title        = {A survey of deep learning methods on cell instance segmentation},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). CEDT2M: Text-driven human motion generation via cross-modal mixture of encoder-decoder. <em>NCA</em>, <em>37</em>(17), 11175-11193. (<a href='https://doi.org/10.1007/s00521-025-11112-w'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Generating natural 3D human motion from given text is a crucial research hotspot in the field of computer vision in recent years. However, due to the complexity of human motion and the challenge of learning cross-modal relationships between text and motion, accurately generating corresponding continuous motion from complex text remains a formidable problem, which requires a deep understanding of linguistic nuances and human motion analysis. To address these challenges, we propose CEDT2M, a framework for text to motion generation based on cross-modal mixture of encoder-decoder. Specifically, our CEDT2M introduces a multitask learning approach to jointly train unsupervised motion-text alignment and motion generation without relying on labeled data. This approach enables the model to learn multiple related tasks simultaneously and capture the internal relationship between these tasks. In addition, our CEDT2M combines a visual transformer encoder for encoding skeleton-based motion sequences with a BERT-like text encoder, enabling efficient processing of both modalities. Extensive experiments demonstrate that CEDT2M can achieve competitive zero-shot retrieval performance on benchmark datasets like KIT and HumanML3D, while also outperforming the state-of-the-art results. The code will be available at: https://github.com/lplp926120/CEDT2M.},
  archive      = {J_NCA},
  author       = {Wang, XiangYang and Li, Peng and Wang, Rui},
  doi          = {10.1007/s00521-025-11112-w},
  journal      = {Neural Computing and Applications},
  month        = {6},
  number       = {17},
  pages        = {11175-11193},
  shortjournal = {Neural Comput. Appl.},
  title        = {CEDT2M: Text-driven human motion generation via cross-modal mixture of encoder-decoder},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Deep learning for iris recognition: A review. <em>NCA</em>, <em>37</em>(17), 11125-11173. (<a href='https://doi.org/10.1007/s00521-025-11109-5'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Iris recognition is a secure biometric technology known for its stability and privacy. With no two irises being identical and little change throughout a person’s lifetime, iris recognition is considered more reliable and less susceptible to external factors than other biometric recognition methods. Unlike traditional machine learning-based iris recognition methods, deep learning technology does not rely on feature engineering and boasts excellent performance. This paper collects 131 relevant papers to summarize the development of iris recognition based on deep learning. We introduce the background of iris recognition and the motivation and contribution of this survey. Then, we present the common datasets widely used in iris recognition. After that, we summarize the key tasks involved in the process of iris recognition based on deep learning technology, including identification, segmentation, presentation attack detection, and localization. Finally, we discuss the challenges and potential development of iris recognition. This review provides a comprehensive sight of the research of iris recognition based on deep learning.},
  archive      = {J_NCA},
  author       = {Yin, Yimin and He, Siliang and Zhang, Renye and Chang, Hongli and Zhang, Jinghua},
  doi          = {10.1007/s00521-025-11109-5},
  journal      = {Neural Computing and Applications},
  month        = {6},
  number       = {17},
  pages        = {11125-11173},
  shortjournal = {Neural Comput. Appl.},
  title        = {Deep learning for iris recognition: A review},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Advances and applications in inverse reinforcement learning: A comprehensive review. <em>NCA</em>, <em>37</em>(17), 11071-11123. (<a href='https://doi.org/10.1007/s00521-025-11100-0'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Reinforcement learning, characterized by trial-and-error learning and delayed rewards, is central to decision-making processes. Its core component, the reward function, is traditionally handcrafted, but designing these functions is often challenging or impossible in real-world scenarios. Inverse reinforcement learning (IRL) addresses this issue by extracting reward functions from expert demonstrations, facilitating optimal policy derivation and offering a deeper understanding of expert behavior. This comprehensive review focuses on three key aspects: the diverse methodologies employed in IRL, its wide-ranging applications across fields such as robotics, autonomous vehicles, and human intent analysis, and the importance of curated datasets in advancing IRL research. A structured analysis of IRL techniques is provided, applications are categorized by domain, and the role of benchmark datasets in evaluating performance and guiding future developments is emphasized. The unique value of IRL in bridging the gap between human and artificial learning is highlighted, demonstrating its potential to unlock advancements in machine learning, decision making, and explainable AI. By summarizing the current state of IRL research and advocating for future directions, this review serves as a valuable resource for researchers and practitioners seeking to explore and advance the field.},
  archive      = {J_NCA},
  author       = {Deshpande, Saurabh and Walambe, Rahee and Kotecha, Ketan and Selvachandran, Ganeshsree and Abraham, Ajith},
  doi          = {10.1007/s00521-025-11100-0},
  journal      = {Neural Computing and Applications},
  month        = {6},
  number       = {17},
  pages        = {11071-11123},
  shortjournal = {Neural Comput. Appl.},
  title        = {Advances and applications in inverse reinforcement learning: A comprehensive review},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). The paradigm of digital health: AI applications and transformative trends. <em>NCA</em>, <em>37</em>(17), 11039-11070. (<a href='https://doi.org/10.1007/s00521-025-11081-0'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Artificial intelligence is transforming healthcare by enhancing diagnostic accuracy with AI-driven imaging and personalized treatment plans. This review focuses on various applications of AI across diverse healthcare domains, from diagnostics and imaging to surgical and procedural care, patient monitoring, pharmaceuticals, rehabilitation, healthcare operations, and public health, examining factors influencing its adoption like technological readiness and regulatory support. Real-time intraoperative guidance, drug discovery, integration of telehealth, and the creation of personalized care plans are some examples where AI-driven improvements can significantly influence practice and will shape the future of traditional healthcare delivery. These drives with machine learning and deep learning AI technologies have resulted in genomics, vaccine development, and public health surveillance innovations. Based on an extensive literature review of articles from Scopus, PubMed, and Google Scholar, AI is perceived to have a major impact on reshaping the face of healthcare delivery. There has been tremendous progress by AI in early detection of diseases, sophisticated robotic surgeries, and personalized medicine, but its successful integration comes with significant challenges, ensuring data security, safeguarding against security breaches, and algorithmic bias and transparency. Achieving the full potential of AI in health requires the joint effort of technology developers, healthcare professionals, and policymakers to challenge and overcome the challenges with safe, effective, and ethical implementation of AI. With investment in AI research, development, and training, healthcare systems can now create that power to benefit patients, increase efficiency, and spur innovation.},
  archive      = {J_NCA},
  author       = {Rashid, Zubia and Ahmed, Hania and Nadeem, Neha and Zafar, Syeda Bushra and Yousaf, Muhammad Zubair},
  doi          = {10.1007/s00521-025-11081-0},
  journal      = {Neural Computing and Applications},
  month        = {6},
  number       = {17},
  pages        = {11039-11070},
  shortjournal = {Neural Comput. Appl.},
  title        = {The paradigm of digital health: AI applications and transformative trends},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Time series analysis in compressor-based machines: A survey. <em>NCA</em>, <em>37</em>(17), 11001-11038. (<a href='https://doi.org/10.1007/s00521-025-11065-0'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In both industrial and residential contexts, compressor-based machines, such as refrigerators, heat, ventilation, and air conditioning systems, heat pumps, and chillers, are essential to fulfil production and consumers’ needs. The diffusion of sensors and internet of things connectivity support the development of monitoring systems that can detect and predict faults, identify behavioural shifts, and forecast the operational status of machines and their components. The focus of this paper is to survey the recent research on such tasks as fault detection (FD), fault prediction (FP), forecasting, and change point detection (CPD) applied to multivariate time series characterizing the operations of compressor-based machines. These tasks play a critical role in improving the efficiency and longevity of machines by minimizing downtime and maintenance costs and improving energy efficiency. Specifically, FD detects and diagnoses faults, FP predicts such occurrences, forecasting anticipates the future value of characteristic variables of machines, and CPD identifies significant variations in the behaviour of the appliances, such as a change in the working regime. We identify and classify the approaches to the tasks mentioned above, compare the algorithms employed, highlight the gaps in the current state of the art, and discuss the most promising future research directions in the field.},
  archive      = {J_NCA},
  author       = {Forbicini, Francesca and Pinciroli Vago, Nicolò Oreste and Fraternali, Piero},
  doi          = {10.1007/s00521-025-11065-0},
  journal      = {Neural Computing and Applications},
  month        = {6},
  number       = {17},
  pages        = {11001-11038},
  shortjournal = {Neural Comput. Appl.},
  title        = {Time series analysis in compressor-based machines: A survey},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). GraphXAI: A survey of graph neural networks (GNNs) for explainable AI (XAI). <em>NCA</em>, <em>37</em>(17), 10949-11000. (<a href='https://doi.org/10.1007/s00521-025-11054-3'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Graphs find wide applications in numerous domains, ranging from simulating physical systems to learning molecular fingerprints, predicting protein interfaces, diagnosing diseases, etc. These applications encompass simulations in non-Euclidean space, in which a graph serves as an ideal representation, and are also an indispensable means of illustrating the connections and interdependencies among its various constituents. Graph neural networks (GNNs) are specific types of neural networks that are specifically built to handle data possessing a graph structure. They are highly effective at capturing intricate relationships among different entities. Nonetheless, their “black-box” characteristics pose difficulties in transparency, trust, and interpretability, especially in critical sectors like heath care, banking, and autonomous systems. Explainable artificial intelligence (XAI) has emerged to clarify these obscure decision-making processes, thus enhancing trust and accountability in AI systems. This survey paper delves into the intricate interplay between GNNs and XAI, including an exhaustive taxonomy of the various explainability methods designed for graph-structured data. It classifies the existing explainability methods into post hoc and self-interpretable models. The paper analyzes their practical applications in diversified fields, highlighting the significance of transparent GNNs in essential sectors such as fraud detection, drug development, and network security. The survey also delineates evaluation parameters for assessing explainability along with addressing persistent issues in scalability and fairness. The paper concludes by addressing prospective advancements in the subject, including the creation of innovative XAI methodologies tailored for GNN architectures, integration with federated learning, and utilization of these models in interdisciplinary fields. This study bridges the gap between GNNs and XAI, providing an essential resource for researchers and practitioners aiming to enhance the interpretability and efficacy of graph-based AI systems.},
  archive      = {J_NCA},
  author       = {Nandan, Mauparna and Mitra, Soma and De, Debashis},
  doi          = {10.1007/s00521-025-11054-3},
  journal      = {Neural Computing and Applications},
  month        = {6},
  number       = {17},
  pages        = {10949-11000},
  shortjournal = {Neural Comput. Appl.},
  title        = {GraphXAI: A survey of graph neural networks (GNNs) for explainable AI (XAI)},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). In-depth survey: Deep learning in recommender systems—exploring prediction and ranking models, datasets, feature analysis, and emerging trends. <em>NCA</em>, <em>37</em>(17), 10875-10947. (<a href='https://doi.org/10.1007/s00521-024-10866-z'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Due to the exponential growth of online information, users are often welcomed with a huge range of products and services along with descriptions, reviews, and comments. Although this information available to users is valuable, at the same time, massive data sources confuse them to retrieve desired content, which is known as information overload. Recommender systems are examined as effective tools that play a vital role in filtering information and ultimately addressing the information overload problem. Various online platforms use recommendation systems to provide users with more relevant and personalized content. With the remarkable success of deep learning in the field of artificial intelligence, it procures much attention in the recommendation research area in recent years. The exiting literature on recommender system research commonly distinguishes between two main directions: rating prediction and top-N ranking. In this survey paper, we examine deep learning methodologies in the context of both rating prediction and top-N ranking recommendation approaches. Additionally, we investigate pre- and post-modeling critiques of recommender systems and provide insights into exiting benchmark datasets, feature learning analysis, and evaluation measuring techniques. In the end, we highlight the new generation recommender system trend with respective future research directions.},
  archive      = {J_NCA},
  author       = {Gheewala, Shivangi and Xu, Shuxiang and Yeom, Soonja},
  doi          = {10.1007/s00521-024-10866-z},
  journal      = {Neural Computing and Applications},
  month        = {6},
  number       = {17},
  pages        = {10875-10947},
  shortjournal = {Neural Comput. Appl.},
  title        = {In-depth survey: Deep learning in recommender systems—exploring prediction and ranking models, datasets, feature analysis, and emerging trends},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Optimizing task scheduling in cloud environments: A hybrid golden search whale optimization algorithm approach. <em>NCA</em>, <em>37</em>(17), 10851-10873. (<a href='https://doi.org/10.1007/s00521-025-11113-9'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Managing fluctuating workloads and optimizing resource utilization in cloud environments pose significant challenges, particularly in fields requiring real-time data processing, such as healthcare. This paper introduces a novel hybrid metaheuristic technique, the Golden Search Whale Optimization Algorithm (GSWOA), specifically designed for scheduling independent dynamic biomedical data. GSWOA merges the strengths of Golden Search Optimization (GSO) and Whale Optimization Algorithm (WOA), optimizing numerical function optimization and achieving a balance between exploration and exploitation. The algorithm’s effectiveness was assessed using MATLAB by applying standard benchmark functions and further evaluated on a real-world biomedical dataset within the CloudSim environment. The performance evaluations demonstrate that GSWOA significantly outperforms existing metaheuristic and traditional scheduling techniques, achieving a 42.71% increase in resource utilization and a 14.17% reduction in makespan compared to conventional methods. These results highlight GSWOA’s potential to enhance scheduling efficiency substantially in cloud computing infrastructures, suggesting it is a powerful tool for complex task allocations. Future research will explore the scalability of GSWOA and its applicability across other data-intensive sectors.},
  archive      = {J_NCA},
  author       = {Acharya, Biswaranjan and Panda, Sucheta and Das, Satyabrata and Majhi, Santosh Kumar and Gerogiannis, Vassilis C. and Kanavos, Andreas},
  doi          = {10.1007/s00521-025-11113-9},
  journal      = {Neural Computing and Applications},
  month        = {6},
  number       = {17},
  pages        = {10851-10873},
  shortjournal = {Neural Comput. Appl.},
  title        = {Optimizing task scheduling in cloud environments: A hybrid golden search whale optimization algorithm approach},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A novel human action recognition using grad-CAM visualization with gated recurrent units. <em>NCA</em>, <em>37</em>(17), 10835-10850. (<a href='https://doi.org/10.1007/s00521-025-10978-0'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Human action recognition is a vital aspect of computer vision, with applications ranging from security systems to interactive technology. Our study presents a comprehensive methodology that employs multiple feature extraction and optimization techniques to enhance the accuracy and efficiency of human action identification. The video input was divided into four distinct elements: RGB images, optical flow information, spatial saliency maps, and temporal saliency maps. Each component was analyzed independently using advanced computer vision algorithms. The process involves utilizing various algorithms and techniques to extract meaningful information from the visual data. The Farneback algorithm was employed to examine the optical flow, whereas Canny edge detection was used to assess spatial prominence. Additionally, frame comparison helps to identify motion-based prominence. These processed elements provide a comprehensive representation of both spatial and temporal information. The extracted data were then input into distinct pretrained deep learning models. Specifically, Inception V3 was used for RGB frames and optical flow analysis, ResNetV2 processed spatial saliency maps, and DenseNet-121 handled motion saliency maps. The input data are processed separately by these networks, each of which extracts specific features that are suited to their respective modalities. This feature extraction process ensures the comprehensive capture of both static and dynamic elements in video data. Subsequently, sequence modeling and classification were performed using a gated recurrent unit (GRU) that incorporated an attention mechanism. This mechanism dynamically highlights the most significant temporal segments, improving the capacity of the model to comprehend intricate human actions within video sequences. To enhance the efficiency of the model, we implemented the Grasshopper optimization algorithm to optimize the feature selection and classification stages, thus maximizing the use of extracted features. We evaluated our approach using two standard datasets, UCF101 and HMDB51. The method demonstrated its efficacy in identifying various human actions, achieving 98.35% accuracy on UCF101 and 83.45% accuracy on HMDB51. The Grad-CAM visualization technique reveals key areas the model focuses on for action recognition. This study underscores the effectiveness of integrating multimodal feature extraction, deep learning, and optimization for precise and interpretable human action recognition. The proposed method excels across diverse complex datasets, offering a practical solution for real-world applications like automated surveillance, human–computer interfaces, and activity monitoring platforms.},
  archive      = {J_NCA},
  author       = {Jayamohan, M. and Yuvaraj, S.},
  doi          = {10.1007/s00521-025-10978-0},
  journal      = {Neural Computing and Applications},
  month        = {6},
  number       = {17},
  pages        = {10835-10850},
  shortjournal = {Neural Comput. Appl.},
  title        = {A novel human action recognition using grad-CAM visualization with gated recurrent units},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Artificial neural network to characterize spatially varying quantity through random field approach. <em>NCA</em>, <em>37</em>(17), 10823-10833. (<a href='https://doi.org/10.1007/s00521-024-10955-z'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Random field theory is commonly employed to characterize spatially varying quantities by decomposing them into deterministic and random components. The unknown deterministic part is approximated using algebraic polynomials, but this becomes challenging with sparse and noisy data. This study introduces a framework based on deep neural networks (DNN) for robustly characterizing spatially varying quantities. The DNN, coupled with an efficient regularization technique, determines the deterministic part, while the Karhunen–Lo $$\grave{e}$$ ve expansion generates random field samples. The efficacy of this technique is demonstrated using two numerical examples: spatially varying log-normal data and undrained shear strength of the soil. The findings reveal that the DNN-based framework provides robust characterization of the spatially varying quantities with good correlation, even with sparse or noisy data, outperforming conventional regression-based detrending methods. Thus, this paper highlights the application of this framework across a diverse range of engineering domains.},
  archive      = {J_NCA},
  author       = {Kumar, Pratyush},
  doi          = {10.1007/s00521-024-10955-z},
  journal      = {Neural Computing and Applications},
  month        = {6},
  number       = {17},
  pages        = {10823-10833},
  shortjournal = {Neural Comput. Appl.},
  title        = {Artificial neural network to characterize spatially varying quantity through random field approach},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Exploring distribution-based approaches for out-of-distribution detection in deep learning models. <em>NCA</em>, <em>37</em>(17), 10807-10822. (<a href='https://doi.org/10.1007/s00521-024-10912-w'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Detecting unknown samples is a crucial task for deep learning applications, especially when considering open-set problems such as autonomous driving or disease classification. To improve DL models’ robustness in identifying unseen classes, out-of-distribution (OOD) methods are utilized to distinguish between in-distribution (ID) and OOD samples using distinct patterns from the model’s output space. While utilizing the output space for OOD detection is common due to its practicality and effectiveness, relying solely on model scores may lead to issues such as overconfidence. In this study, we propose leveraging the logit distribution, with a focus on the Dirichlet Gaussian Mixture Models, to identify OOD samples. These approaches consider not only the score associated with the highest class but also those assigned to other classes. We evaluate different distributional assumptions and analyze the advantages and disadvantages of using Gaussian-based models for logit distribution in OOD detection. Based on the experiments of this work, the Dirichlet Gaussian Mixture approach obtained up to 27.5% improvement if compared to the best baseline strategy using the output space in the AUROC metric and 13.6% for the FPR95 metric.},
  archive      = {J_NCA},
  author       = {Carvalho, Thiago and Vellasco, Marley and Amaral, José Franco},
  doi          = {10.1007/s00521-024-10912-w},
  journal      = {Neural Computing and Applications},
  month        = {6},
  number       = {17},
  pages        = {10807-10822},
  shortjournal = {Neural Comput. Appl.},
  title        = {Exploring distribution-based approaches for out-of-distribution detection in deep learning models},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Zero-day android botnet detection using neural networks. <em>NCA</em>, <em>37</em>(17), 10795-10805. (<a href='https://doi.org/10.1007/s00521-024-10818-7'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Android devices have evolved to offer a diverse array of services, spanning applications related to banking, business, health, and entertainment. The widespread adoption of Android devices, coupled with the open-source architecture of the Android operating system, has rendered them a prime target for malicious actors. Among the most perilous threats are Android botnets, which enable malicious actors, often referred to as botmasters, to exert remote control for the execution of destructive attacks. Android botnets have huge potential to be an emerging threat to mobile device security. In this paper, we focus on detecting evolving Android botnets and introduce a new dataset of 3458 apps, represented by 455 permission-based features. We propose an improved multilayer perceptron neural network for zero-day botnet detection. Our methodology, in this way, achieves an accuracy of 98.5%, thus outperforming traditional classifiers. It has a lot of functionality and is based on the neural network approach, making it able to identify slight botnet behaviours in order to improve Android security.},
  archive      = {J_NCA},
  author       = {Seraj, Saeed and Pimenidis, Elias and Trovati, Marcello and Polatidis, Nikolaos},
  doi          = {10.1007/s00521-024-10818-7},
  journal      = {Neural Computing and Applications},
  month        = {6},
  number       = {17},
  pages        = {10795-10805},
  shortjournal = {Neural Comput. Appl.},
  title        = {Zero-day android botnet detection using neural networks},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Forecasting stock market volatility using social media sentiment analysis. <em>NCA</em>, <em>37</em>(17), 10771-10794. (<a href='https://doi.org/10.1007/s00521-024-10807-w'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the era where social media significantly influences public sentiment, platforms such as Twitter have become vital in predicting stock market trends. This paper presents a cutting-edge predictive model that integrates historical stock market data, Twitter sentiment analysis, and an extensive array of tweet-related features. Utilizing advanced regression techniques and deep neural networks, our model forecasts the daily closing prices of the U.S. stock market indices with notable accuracy and demonstrates a strong link between market values, sentiment scores, and social media activities. Our analysis particularly emphasizes the importance of tweet diffusion and the influence of prominent Twitter users in refining prediction accuracy. Contrary to conventional wisdom, we discovered that incorporating a wide range of tweet-derived features significantly improves the model’s performance without leading to sparsity challenges. This study not only questions established paradigms but also underscores the potential of social media analytics in financial market forecasting, with substantial implications for investors, market analysts, and policy makers.},
  archive      = {J_NCA},
  author       = {Saravanos, Christina and Kanavos, Andreas},
  doi          = {10.1007/s00521-024-10807-w},
  journal      = {Neural Computing and Applications},
  month        = {6},
  number       = {17},
  pages        = {10771-10794},
  shortjournal = {Neural Comput. Appl.},
  title        = {Forecasting stock market volatility using social media sentiment analysis},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). CM-MLP: Hybrid convmixer-deep MLP architecture for enhanced identification of corn and apple leaf diseases. <em>NCA</em>, <em>37</em>(17), 10757-10769. (<a href='https://doi.org/10.1007/s00521-024-10774-2'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This study aims to identify diseases impacting the agricultural sector, specifically focusing on corn and apple leaves. We propose a novel hybrid ConvMixer and Deep MLP architecture called CM-MLP that combines the strengths of ConvMixer with various Deep Multi-layer Perceptron (MLP), like MLP-Mixer, gMLP, and FNet, to tackle the classification challenges in this context. Notably, the final block of the Deep MLP can accommodate both MLP-Mixer and gMLP, as well as other designs. The ConvMixer architecture employs depthwise and pointwise convolution techniques to enrich the input image features, which are subsequently processed by the Deep MLP block to classify corn and apple leaf diseases comprehensively. In this study, we implement gradient centralization (GC) to enhance training performance. The results reveal that training with GC leads to a robust CM-MLP model, which achieves an impressive average accuracy exceeding 99.00 and proves effective in classifying diseases in corn and apple leaves.},
  archive      = {J_NCA},
  author       = {Li, Li-Hua and Tanone, Radius},
  doi          = {10.1007/s00521-024-10774-2},
  journal      = {Neural Computing and Applications},
  month        = {6},
  number       = {17},
  pages        = {10757-10769},
  shortjournal = {Neural Comput. Appl.},
  title        = {CM-MLP: Hybrid convmixer-deep MLP architecture for enhanced identification of corn and apple leaf diseases},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A deep residual sequence autoencoder for future state estimation and aiding prognostics and diagnostics in machines: A case study of mechanical rolling elements. <em>NCA</em>, <em>37</em>(17), 10737-10756. (<a href='https://doi.org/10.1007/s00521-024-10756-4'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Prognostics and health management (PHM) enables the prediction of faults by condition monitoring and estimation of the future state of engineering systems. By using a predefined end of life in conjunction with the time to start predicting (TSP), remaining useful life (RUL) is estimated. Due to the nature of recurrent neural networks (RNNs) and their variants, they have been used extensively in timeseries-related problems, including PHM. However, the existing impediments known in RNNs that cause nonconvergence during training have created setbacks in their application in fields with data characterized by long dependent series. To overcome these setbacks, models are built shallow, notwithstanding the fact that the depth of a model significantly contributes to the increase in results accuracy. Keeping these problems in mind, this study proposes a deep residual autoencoder comprising LSTM neurons and applies it in PHM to predict the future state of mechanical rolling elements. The study also proposed an objective and scientific technique for detecting TSP contrary to available studies that subjectively perform this task. The model proposed was trained and validated with bearing vibration data available in public repositories. To achieve the desired results, data were normalized using minmax normalization and transformed to positive values, it was then converted into a shape compatible to the model proposed. The model was fed with the latest window after TSP was detected to predict the future state, aiding the prediction of RUL. Finally, the model was evaluated with cumulative relative accuracy as the main evaluation metric together with root mean squared error and mean absolute error; they all demonstrated that the proposed model exhibits reliable results in PHM.},
  archive      = {J_NCA},
  author       = {Ramadhan, Bwambale Rashid and Cahit, Perkgoz},
  doi          = {10.1007/s00521-024-10756-4},
  journal      = {Neural Computing and Applications},
  month        = {6},
  number       = {17},
  pages        = {10737-10756},
  shortjournal = {Neural Comput. Appl.},
  title        = {A deep residual sequence autoencoder for future state estimation and aiding prognostics and diagnostics in machines: A case study of mechanical rolling elements},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Researching the detection of continuous gravitational waves based on signal processing and ensemble learning. <em>NCA</em>, <em>37</em>(17), 10723-10736. (<a href='https://doi.org/10.1007/s00521-024-10744-8'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The detection of Gravitational Waves has introduced a new era for physics, astronomy, and astrophysics, unveiling new universe mysteries. Unfortunately, research studies focused on detection of Binary bursting Gravitorial Waves (B-GWs), which are produced by the rotation of binary compact objects such as black holes. Quite recently researchers attempted to detect and analyze another type of gravitational waves named Continuous Gravitational Waves (C-GWs). In contrast to the complex burst nature of B-GWs, C-GWs have an elegant and significantly simpler form while they are able to provide higher quality data for the exploration of the universe. In more detail, the direct detection of C-GWs can considerably improve our understanding of the universe by allowing researchers to examine the complexion and existence of the most extreme stars and cosmic wonders, something that was impossible up until now. However, C-GWs are much less weaker signals than B-GWs, which makes their identification a significantly challenging task. In this work, it is proposed a new framework combining signal processing techniques within ensemble learning for the effective detection of C-GWs. The key idea is to remove noise for creating a robust representation of the input data using short-time Fourier and power spectrum transformations as well as statistical tools for feeding convolutional neural network models. The individual prediction of the developed models are combined using an ensemble strategy based on a “super-learner” philosophy. In addition, the proposed framework is enforced with a data augmentation methodology for improving the prediction performance. This research work utilizes data provided by the Laser Interferometer Gravitational-Wave Observatory (LIGO). Extensive analysis confirms that the proposed framework significantly outperforms existing methods in all tested configurations, demonstrating its effectiveness.},
  archive      = {J_NCA},
  author       = {Pintelas, Emmanuel and Livieris, Ioannis E. and Pintelas, Panagiotis},
  doi          = {10.1007/s00521-024-10744-8},
  journal      = {Neural Computing and Applications},
  month        = {6},
  number       = {17},
  pages        = {10723-10736},
  shortjournal = {Neural Comput. Appl.},
  title        = {Researching the detection of continuous gravitational waves based on signal processing and ensemble learning},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). WaveFLSTM: Wavelet-based fuzzy LSTM model for forecasting complex time series data. <em>NCA</em>, <em>37</em>(17), 10707-10721. (<a href='https://doi.org/10.1007/s00521-024-10622-3'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The forecasting of time series continues to be a prominent area of interest among researchers exploring advanced learning techniques. In recent times, deep recurrent neural networks, particularly long short-term memory (LSTM) models, have demonstrated exceptional forecasting capabilities compared to other neural network architectures. To tackle the fuzzy datasets, fuzzy LSTM (FLSTM) model was developed by incorporating the advantage of the intuitionistic fuzzy logic (IFL). Most of the time series data generated from different fields including agriculture are not only fuzzy, but also exhibit nonlinear and non-stationary characteristics. The present study proposed wavelet-based fuzzy LSTM (WaveFLSTM) model, a novel approach for forecasting complex time series data, specifically addressing the challenges posed by fuzzy, nonlinear, and non-stationary characteristics of time series. The proposed WaveFLSTM has advantage of denoising through maximal overlap discrete wavelet transform (MODWT) and integrating the advantage of fuzzy logic by means of IFL. The fuzzy relations with LSTM networks are applied to each of the denoised series by using membership and non-membership values through intuitionistic fuzzy c-means technique. The prediction accuracy of proposed WaveFLSTM model is compared with that of LSTM, FLSTM and wavelet LSTM (WaveLSTM) models using monthly wholesale price data of different pulse crops from various markets in India. The percentage gain in accuracy of the proposed model, as compared to LSTM, WaveLSTM, and FLSTM, is found out to be 29%, 20%, and 14% respectively. Besides, the usual accuracy measures, the model confidence sets and technique for order preference by similarity to ideal solution algorithm have also been used. The findings demonstrated the effectiveness of the proposed WaveFLSTM model in improving forecasting accuracy of complex time series data.},
  archive      = {J_NCA},
  author       = {Sarkar, Anita and Yeasin, Md and Paul, Ranjit Kumar and Paul, A. K. and Singh, Ankit Kumar},
  doi          = {10.1007/s00521-024-10622-3},
  journal      = {Neural Computing and Applications},
  month        = {6},
  number       = {17},
  pages        = {10707-10721},
  shortjournal = {Neural Comput. Appl.},
  title        = {WaveFLSTM: Wavelet-based fuzzy LSTM model for forecasting complex time series data},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Memetic algorithm-based optimization of hybrid forecasting systems for multivariate time series. <em>NCA</em>, <em>37</em>(17), 10689-10706. (<a href='https://doi.org/10.1007/s00521-024-10618-z'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent decades, wind speed’s growing use in electricity generation has posed challenges due to its intermittent and fluctuating nature, hindering its reliable incorporation into the grid. To address this, accurate forecasting of wind energy production is essential. This paper presents the Memetic Hybrid Forecasting System (MHFS), a hybrid system that combines linear statistical and deep learning (DL) models employing a novel memetic algorithm (MA). The MHFS is divided into two phases: (i) linear modeling of the wind speed and (ii) generation of the final forecast using a DL model optimized by the MA. The MA searches for the best combination of lags of the time series, residual series, and exogenous variables with the linear forecast aiming to enhance the accuracy. Two versions of the MHFS are evaluated by combining Seasonal Autoregressive Integrated Moving Average (SARIMA) and SARIMA with eXogenous factors (SARIMAX) with long short-term memory (LSTM) neural network in three wind speed datasets. Results consistently demonstrate superior performance of MHFS over single and hybrid models (linear and nonlinear combinations) in the literature, assessed by mean squared error (MSE) and mean absolute percentage error (MAPE). The MHFS version using SARIMA and LSTM attained an overall MAPE superior to single models, linear and nonlinear combinations of 21.02%, 21.48%, and 18.76%, respectively. The version that employs SARIMAX and LSTM reached an improvement regarding MAPE of 21.38%, 13.35%, and 8.68%, respectively. The proposal presents an innovative combination of statistical and DL models using an MA that refines feature selection, improving forecasting accuracy.},
  archive      = {J_NCA},
  author       = {Padilha, Guilherme Afonso Galindo and Jung, Jason J. and de Mattos Neto, Paulo S. G.},
  doi          = {10.1007/s00521-024-10618-z},
  journal      = {Neural Computing and Applications},
  month        = {6},
  number       = {17},
  pages        = {10689-10706},
  shortjournal = {Neural Comput. Appl.},
  title        = {Memetic algorithm-based optimization of hybrid forecasting systems for multivariate time series},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Residual connections improve click-through rate and conversion rate prediction performance. <em>NCA</em>, <em>37</em>(17), 10675-10688. (<a href='https://doi.org/10.1007/s00521-024-10617-0'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The prediction of click-through rate (CTR) and conversion rate (CVR) are crucial tasks in online advertising and recommendation systems. As the learning models become more complex with increasing depth, it has become increasingly challenging to predict CTR and CVR accurately. This paper addresses the challenges associated with the increasing depth in CTR and CVR prediction models by introducing the integration of residual connections into the models. The experiments we conducted involve using five different CTR or CVR prediction models together with residual connections on benchmark datasets from both Avazu and Criteo, and the company dataset. The results demonstrate that residual connections can effectively improve CTR and CVR prediction models, with an increase in AUC by $$1.4\%$$ , a decrease in loss by $$3.5\%$$ , and an increase in F1 by $$20.2\%$$ . The results also show that we can safely increase the depth and the size of the network without a need to optimize or a decrease in the performance.},
  archive      = {J_NCA},
  author       = {Biçici, Ergun},
  doi          = {10.1007/s00521-024-10617-0},
  journal      = {Neural Computing and Applications},
  month        = {6},
  number       = {17},
  pages        = {10675-10688},
  shortjournal = {Neural Comput. Appl.},
  title        = {Residual connections improve click-through rate and conversion rate prediction performance},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Efficient deterministic renewable energy forecasting guided by multiple-location weather data. <em>NCA</em>, <em>37</em>(17), 10647-10674. (<a href='https://doi.org/10.1007/s00521-024-10607-2'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Electricity generated from renewable energy sources has been established as an efficient remedy for both energy shortages and the environmental pollution stemming from conventional energy production methods. At current stage, hydropower remains the primary contributor to electricity generation among renewable energy sources. Nonetheless, solar and wind power are also recognized as dominant and exceptionally promising renewable energy sources. The accurate forecasting of the energy generation of those sources facilitates their integration into electric grids, by minimizing the negative impact of uncertainty regarding their management and operation. This paper proposes a novel methodology for deterministic wind and solar energy generation forecasting for multiple generation sites, utilizing multi-location weather forecasts. The method employs a U-shaped temporal convolutional auto-encoder (UTCAE) architecture for temporal processing of weather-related and energy-related time-series across each site. The multi-sized kernels convolutional spatiotemporal attention (MKST-Attention), inspired by the multi-head scaled dot product attention mechanism, is also proposed aiming to efficiently transfer temporal patterns from weather data to energy data, without a priori knowledge of the locations of the power stations and the locations of provided weather data. The conducted experimental evaluation on a day-ahead solar and wind energy forecasting scenario on five datasets demonstrated that the proposed method achieves top results, outperforming all competitive time-series forecasting state-of-the-art methods. In particular, in the AEMO-H dataset, encompassing hourly wind energy generation data alongside weather data from 22 power stations, the method attained the best mean absolute error (MAE), root mean square error (RMSE) and coefficient of determination (R2) scores at each station. Additionally, it recorded the highest MAE of 0.098, the highest RMSE of 0.138 and the highest R2 score of 0.791, averaged across all energy stations.},
  archive      = {J_NCA},
  author       = {Symeonidis, Charalampos and Nikolaidis, Nikos},
  doi          = {10.1007/s00521-024-10607-2},
  journal      = {Neural Computing and Applications},
  month        = {6},
  number       = {17},
  pages        = {10647-10674},
  shortjournal = {Neural Comput. Appl.},
  title        = {Efficient deterministic renewable energy forecasting guided by multiple-location weather data},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). An explainable approach for prediction of remaining useful life in turbofan condition monitoring. <em>NCA</em>, <em>37</em>(17), 10621-10645. (<a href='https://doi.org/10.1007/s00521-024-10605-4'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The goal of this research is to present an approach that predicts the remaining useful life (RUL) estimation of a turbofan engine system within specific time frames of aircraft operation. In the first phase, due to the high number of features, a group decision-making approach for feature selection is proposed and performed based on various supervised and unsupervised methods. In the second phase, among various machine learning and deep learning approaches, bidirectional long short-term memory (BiLSTM) and multilayer perceptron (MLP) that provided more appropriate results are selected. Unlike most previous studies that focused on determining the status of the equipment (whether it is healthy or faulty), the main objective of this research is to predict RUL. Finally, in the third phase, thorough analysis using explainable AI (XAI) was conducted concerning the importance of features and an investigation into features that led to an increase in errors in some intervals. The results show that the presented approach has been able to predict the RUL well, although it is biased in some time intervals for all turbofans, and the features related to this bias prediction are determined by using XAI. They can be further investigated by the maintenance department.},
  archive      = {J_NCA},
  author       = {Mansourvar, Zahra and Jahangoshai Rezaee, Mustafa and Eshkevari, Milad},
  doi          = {10.1007/s00521-024-10605-4},
  journal      = {Neural Computing and Applications},
  month        = {6},
  number       = {17},
  pages        = {10621-10645},
  shortjournal = {Neural Comput. Appl.},
  title        = {An explainable approach for prediction of remaining useful life in turbofan condition monitoring},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A fuzzy zeroing neural network and its application on dynamic hill cipher. <em>NCA</em>, <em>37</em>(17), 10605-10619. (<a href='https://doi.org/10.1007/s00521-024-10599-z'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cryptography is the core of information security, and the Hill cipher is one of the most important methods for cryptography. For the purpose of the improvement in the security of traditional Hill cipher (THC) with time-invariant key, a new dynamic Hill cipher (DHC) method with time-varying key is proposed in this paper, which replaced the time-invariant matrix key of the THC with a time-varying matrix key for encryption. In order to ensure the efficiency and accuracy of the proposed DHC, the zeroing neural network (ZNN) is adopted to solve the time-varying key inversion (TVKI) matrix of the DHC decryption. Generally, conventional zeroing neural network (C-ZNN) models cannot effectively deal with the noises, and their convergence and robustness cannot be guaranteed in noisy environments. For the purpose of ensuring the robustness of the ZNN model for solving the TVKI matrix of the DHC decryption, a fuzzy activation function is designed, and a new fuzzy ZNN (FZNN) is constructed to solve the TVKI matrix of the DHC decryption. Comparative simulation results of the FZNN model with other C-ZNN models for solving the TVKI matrix of the DHC demonstrate that the FZNN model converges to the theoretical solution of the TVKI matrix within 0.8s in noisy environment, while other C-ZNN models take more than 0.8s or fail due to noises, which further validates the superior performances of the FZNN model. In addition, the encryption and decryption experiments on strings and RGB images are provided to prove the security of the proposed DHC, and comparative simulation results show that DHC has high feasibility and security in information encryption.},
  archive      = {J_NCA},
  author       = {Jin, Jie and Lei, Xiaoyang and Chen, Chaoyang and Lu, Ming and Wu, Lianghong and Li, Zhijing},
  doi          = {10.1007/s00521-024-10599-z},
  journal      = {Neural Computing and Applications},
  month        = {6},
  number       = {17},
  pages        = {10605-10619},
  shortjournal = {Neural Comput. Appl.},
  title        = {A fuzzy zeroing neural network and its application on dynamic hill cipher},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Deep learning approaches and data augmentation for melanoma detection. <em>NCA</em>, <em>37</em>(17), 10591-10604. (<a href='https://doi.org/10.1007/s00521-024-10590-8'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Skin cancer is one of the most common and dangerous forms of cancer. Diagnosis in the preliminary stages plays a significant role in determining the probability of getting cured, particularly for cancers associated with a higher risk such as melanoma. Traditional examination procedures are surgical treatments that cause significant soreness and scarring. Consequently, there is a rising need for an automatic, painless and accurate skin cancer detection. Machine learning (ML) and deep learning (DL) techniques have shown a promising results in prediction and classification tasks. This study has two primary contributions: Firstly, we developed four deep learning models for skin cancer detection using Convolutional Neural Networks (CNNs) including: ResNetv2, VGG16, EfficientNet-B5 and EfficientNet-B7. Secondly, we highlight the importance of handling data imbalance for supervised classification of medical images, by applying image data augmentation with Generative Adversarial Network (GANs). We compare the obtained results with the state of art which shows the efficiency of the CNN for skin cancer detection. Furthermore, we demonstrate the performance of an ML model, namely Support Vector Machine (SVM), against the CNN model results. The proposed classification system was evaluated using the International Skin Imaging Collaboration (ISIC) Archive which is a large public skin lesion dataset. The results show that the four CNN models outperform the SVM in accuracy, precision, recall and F1 score. The F1 scores for the CNN classifiers were 84.22%, 80.42%, 80.01% and 80.98% for EfficientNet-B7, EfficientNet-B5, ResNet50V2 and VGG16, respectively. On the other hand, the F1 score for the SVM classifier was 69.56%. Data augmentation increased the F1 scores performance to 86.08%, 83.79%, 84.16%, 82.46% and 73.52% for EfficientNet-B7, EfficientNet-B5, ResNet50V2 and VGG16, SVM, respectively. Overall, the results show that EfficientNet-B7 outperformed the other proposed CNN models as well as the SVM for skin cancer detection.},
  archive      = {J_NCA},
  author       = {Alzamel, Mai and Iliopoulos, Costas and Lim, Zara},
  doi          = {10.1007/s00521-024-10590-8},
  journal      = {Neural Computing and Applications},
  month        = {6},
  number       = {17},
  pages        = {10591-10604},
  shortjournal = {Neural Comput. Appl.},
  title        = {Deep learning approaches and data augmentation for melanoma detection},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). DACL+: Domain-adapted contrastive learning for enhanced low-resource language representations in document clustering tasks. <em>NCA</em>, <em>37</em>(17), 10577-10590. (<a href='https://doi.org/10.1007/s00521-024-10589-1'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Low-resource languages in natural language processing present unique challenges, marked by limited linguistic resources and sparse data. These challenges extend to document clustering tasks, where the need for meaningful and semantically rich representations is crucial. Along with the emergence of transformer-based language models (LM), the need for vast amounts of training data has also increased significantly. To this end, we introduce a domain-adapted contrastive learning approach for low-resource Greek document clustering. We introduce manually annotated datasets, essential for LM pre-training and clustering tasks, and extend the investigations by combining Greek BERT and Longformer models. We explore the efficacy of various domain adaptation pre-training objectives and of further pre-training the LMs using contrastive learning with diverse loss functions on datasets generated from a classification corpus. By maximizing the similarity between positive examples and minimizing the similarity between negative examples, our proposed approach learns meaningful representations that capture the underlying structure of the documents. We demonstrate that our proposed approach significantly improves the accuracy of clustering tasks, with an average improvement of up to 50% compared to the base LM, leading to enhanced performance in unsupervised learning tasks. Furthermore, we show how combining language models optimized for different sequence lengths improves performance and compare this approach against an unsupervised graph-based summarization method. Our findings underscore the importance of effective document representations in enhancing the accuracy of clustering tasks in low-resource language settings.},
  archive      = {J_NCA},
  author       = {Zaikis, Dimitrios and Vlahavas, Ioannis},
  doi          = {10.1007/s00521-024-10589-1},
  journal      = {Neural Computing and Applications},
  month        = {6},
  number       = {17},
  pages        = {10577-10590},
  shortjournal = {Neural Comput. Appl.},
  title        = {DACL+: Domain-adapted contrastive learning for enhanced low-resource language representations in document clustering tasks},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). DAMNet: Lightweight dual attention mixed network for efficient image deraining. <em>NCA</em>, <em>37</em>(17), 10557-10575. (<a href='https://doi.org/10.1007/s00521-024-10569-5'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A long-standing problem in computer vision (CV) is image deraining. Current deraining networks frequently fail to achieve a good balance between low system complexity and great image quality, particularly when considering spatial features and contextual feature map information. We propose the Dual Attention Mixed Network (DAMNet) as a solution, which achieves a balance between these conflicting objectives. Encoder-Decoder, Dual Attention Network (DANet), and Channel Attention Sub-Module Network (CASMNet) are the three sub-networks which jointly make up DAMNet. It has two hierarchical levels. The Encoder-Decoder and DANet operate at the first level, while CASMNet operates at the second hierarchical level. An enhanced U-Net serves as the base for the Encoder-Decoder architecture, which effectively learns contextual feature map information. The main objective of CASMNet is to preserve local information. In order to reduce network complexity, a window multi-head self-attention network is used in place of the Middle Block (MB) and multiplication is applied in place of the nonlinear activation function in the U-Net encoder-decoder. DAMNet can gradually retrieve contextual information and spatial details from degraded images through this approach. Our network performs better than many current networks on the Rain100H dataset, reaching at least 0.55 dB greater PSNR, although only using 16.3 million parameters. In comparison with state-of-the-art (SOTA) networks, extensive testing on synthetic, real-world, and raindrop datasets show that DAMNet yields promising results. Moreover, the implementation of DAMNet on a low-end edge device such as the Jetson Nano demonstrates its effectiveness in practical world environment.},
  archive      = {J_NCA},
  author       = {Thatikonda, Ragini and Cheruku, Ramalingaswamy and Kodali, Prakash},
  doi          = {10.1007/s00521-024-10569-5},
  journal      = {Neural Computing and Applications},
  month        = {6},
  number       = {17},
  pages        = {10557-10575},
  shortjournal = {Neural Comput. Appl.},
  title        = {DAMNet: Lightweight dual attention mixed network for efficient image deraining},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A parameter-free nearest neighbor algorithm with reduced prediction time and improved performance through injected randomness. <em>NCA</em>, <em>37</em>(17), 10531-10556. (<a href='https://doi.org/10.1007/s00521-024-10565-9'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {K-nearest neighbor is considered in top machine learning algorithms because of its effectiveness in pattern classification and simple implementation. However, usage of KNN is limited due to its larger prediction time than model-based machine learning algorithms, its sensitivity to the existing outliers in the training dataset, and tuning parameter neighborhood size ( $$k$$ ). Therefore, this research article proposes a new variant of the KNN to reduce the training and prediction time with improved performance. The prediction time of the KNN is reduced by making a binary search tree (BST) using the divide-and-conquer strategy, and prediction performance is improved using ensembling by injecting randomness such as bootstrap aggregation, random subspace, and random node splitting. The proposed KNN variant is parameter-free and, hence, not sensitive to the hyperparameter neighborhood size. Finally, three experiments have been performed based on 26 selected datasets to show the prediction time and prediction power superiority of the proposed KNN over random forest and six selected KNN variants. Results prove that the proposed KNN variant gives better prediction results with reduced prediction and training time.},
  archive      = {J_NCA},
  author       = {Singh, Manpreet and Chhabra, Jitender Kumar},
  doi          = {10.1007/s00521-024-10565-9},
  journal      = {Neural Computing and Applications},
  month        = {6},
  number       = {17},
  pages        = {10531-10556},
  shortjournal = {Neural Comput. Appl.},
  title        = {A parameter-free nearest neighbor algorithm with reduced prediction time and improved performance through injected randomness},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Novel wavelet-LSTM approach for time series prediction. <em>NCA</em>, <em>37</em>(17), 10521-10530. (<a href='https://doi.org/10.1007/s00521-024-10561-z'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Time series prediction often faces challenges due to hidden patterns and noise within the data. This paper presented a novel algorithm that combines wavelet decomposition with long short-term memory (LSTM) networks, providing a distinct method for handling these challenges. The study considered the monthly rainfall data (mm) of India from January 1901 to December 2021. The series under consideration was denoised using maximum overlap discrete wavelet transform (MODWT), followed by long short-term memory (LSTM) modeling on each denoised series. The hyperband search algorithm was employed to identify the optimal hyperparameter combination for each LSTM model, aiding in further fine-tuning the model. The algorithm ended with the implementing of the inverse wavelet transform on the final predictions. In order to evaluate the efficacy of the proposed approach, it was benchmarked against the other established models such as LSTM, recurrent neural network (RNN), and artificial neural network (ANN). The result showed that the proposed model (MODWT-LSTM) significantly outperformed the other benchmark models like LSTM, RNN, and ANN in terms of forecast accuracy. Specifically, in terms of root-mean-square error (RMSE), the proposed algorithm witnessed a gain in prediction accuracy to the tune of 18.5%, 32.8%, and 36.47% than that of LSTM, RNN, and ANN model, respectively. The superiority of the proposed model is further confirmed by use of Diebold–Mariano (DM) test, establishing the hierarchy of model effectiveness as MODWT-LSTM > LSTM > RNN = ANN in terms of predictive performance.},
  archive      = {J_NCA},
  author       = {Tamilselvi, C. and Paul, Ranjit Kumar and Yeasin, Md and Paul, A. K.},
  doi          = {10.1007/s00521-024-10561-z},
  journal      = {Neural Computing and Applications},
  month        = {6},
  number       = {17},
  pages        = {10521-10530},
  shortjournal = {Neural Comput. Appl.},
  title        = {Novel wavelet-LSTM approach for time series prediction},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Assessment and deployment of a LSTM-based virtual sensor in an industrial process control loop. <em>NCA</em>, <em>37</em>(17), 10507-10519. (<a href='https://doi.org/10.1007/s00521-024-10560-0'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Measurement of certain variables within the industrial sector remains a challenge due to the prohibitive costs of sensors, the intricate installation processes, or the continuous nature of production demands. Moreover, if a backup sensor is required in case the main sensor fails, the installation and maintenance difficulties are further increased. A possibility to address this issue is the indirect estimation of the desired variable by leveraging other correlated measures within the operational process. Data-driven techniques are well-suited for this aim, given their capacity to model potentially complex industrial processes. This paper proposes the implementation of a virtual flow sensor for its integration in the control loop of an industrial process. More specifically, four different data-driven methods have been tested to obtain the virtual sensor: multiple linear regression (MLR), multilayer perceptron (MLP), long-short term memory (LSTM) and deep long-short term memory (DeepLSTM). MAE, RMSE and $$R^2$$ have been chosen as evaluation metrics for model selection and testing. Furthermore, the robustness of the virtual flow sensor is not only evaluated under ideal operating conditions, but it is also tested under adverse conditions with various noise levels added to the measured signals. Additionally, the performance of the flow control loop using the real and virtual sensors is also evaluated in both ideal and adverse conditions. IAE, ITAE, and IAVU indices are used to assess the control performance. The results prove the robustness of the LSTM-based virtual flow sensor and the effectiveness of the control loop using it, avoiding the modification of the controller and interrupting the process when the real flow sensor fails.},
  archive      = {J_NCA},
  author       = {González-Herbón, Raúl and González-Mateos, Guzmán and Rodríguez-Ossorio, José R. and Prada, Miguel A. and Morán, Antonio and Alonso, Serafín and Fuertes, Juan J. and Domínguez, Manuel},
  doi          = {10.1007/s00521-024-10560-0},
  journal      = {Neural Computing and Applications},
  month        = {6},
  number       = {17},
  pages        = {10507-10519},
  shortjournal = {Neural Comput. Appl.},
  title        = {Assessment and deployment of a LSTM-based virtual sensor in an industrial process control loop},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Conditioned fully convolutional denoising autoencoder for multi-target NILM. <em>NCA</em>, <em>37</em>(17), 10491-10505. (<a href='https://doi.org/10.1007/s00521-024-10552-0'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Energy management requires reliable tools to support decisions aimed at optimising consumption. Advances in data-driven models provide techniques like Non-Intrusive Load Monitoring (NILM), which estimates the energy demand of appliances from total consumption. Common single-target NILM approaches perform energy disaggregation by using separate learned models for each device. However, the use of single-target systems in real scenarios is computationally expensive and can obscure the interpretation of the resulting feedback. This study assesses a conditioned deep neural network built upon a Fully Convolutional Denoising AutoEncoder (FCNdAE) as multi-target NILM model. The network performs multiple disaggregations using a conditioning input that allows the specification of the target appliance. Experiments compare this approach with several single-target and multi-target models using public residential data from households and non-residential data from a hospital facility. Results show that the multi-target FCNdAE model enhances the disaggregation accuracy compared to previous models, particularly in non-residential data, and improves computational efficiency by reducing the number of trainable weights below 2 million and inference time below 0.25 s for several sequence lengths. Furthermore, the conditioning input helps the user to interpret the model and gain insight into its internal behaviour when predicting the energy demand of different appliances.},
  archive      = {J_NCA},
  author       = {García, Diego and Pérez, Daniel and Papapetrou, Panagiotis and Díaz, Ignacio and Cuadrado, Abel A. and Enguita, José M. and Domínguez, Manuel},
  doi          = {10.1007/s00521-024-10552-0},
  journal      = {Neural Computing and Applications},
  month        = {6},
  number       = {17},
  pages        = {10491-10505},
  shortjournal = {Neural Comput. Appl.},
  title        = {Conditioned fully convolutional denoising autoencoder for multi-target NILM},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Feature analysis and ensemble-based fault detection techniques for nonlinear systems. <em>NCA</em>, <em>37</em>(17), 10465-10489. (<a href='https://doi.org/10.1007/s00521-024-10551-1'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Machine learning approaches play a crucial role in nonlinear system modeling across diverse domains, finding applications in system monitoring, anomaly/fault detection, control, and various other areas. With technological advancements, today such systems might include hundreds or thousands of sensors that generate large amounts of multivariate data streams. This inevitably results in increased model complexity. In response, feature selection techniques are widely employed as a means to reduce complexity, avoid the curse of high dimensionality, decrease training and inference times, and eliminate redundant features. This paper introduces a sensitivity-inspired feature analysis technique for regression tasks. Leveraging the energy distance on the model prediction errors, this approach performs both feature ranking and selection. Additionally, this paper introduces an ensemble-based unsupervised fault detection methodology that incorporates homogeneous units, specifically long short-term memory (LSTM) predictors and cumulative sum-based detectors. The proposed predictors utilize a variant of the teacher forcing (TF) algorithm during both the training and inference phases. Additionally, predictors are used to model the normal behavior of the system, whereas detectors are used to identify deviations from normality. The detector decisions are aggregated using a majority voting scheme. The validity of the proposed approach is illustrated on the two representative datasets, where numerous experiments are performed for feature selection and fault detection evaluation. Experimental assessment reveals promising results, even compared to well-established techniques. Nevertheless, the results also demonstrate the need to perform additional experiments with datasets originating from both simulators and real systems. Further possible refinements of the detection ensemble include the addition of heterogeneous units and other decision fusion techniques.},
  archive      = {J_NCA},
  author       = {Bolboacă, Roland and Haller, Piroska and Genge, Bela},
  doi          = {10.1007/s00521-024-10551-1},
  journal      = {Neural Computing and Applications},
  month        = {6},
  number       = {17},
  pages        = {10465-10489},
  shortjournal = {Neural Comput. Appl.},
  title        = {Feature analysis and ensemble-based fault detection techniques for nonlinear systems},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Dual bi-LSTM-GRU based stance detection in tweets ordered classes. <em>NCA</em>, <em>37</em>(17), 10439-10463. (<a href='https://doi.org/10.1007/s00521-024-10549-9'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {There has been a tremendous increase in social media text-based opinions and reviews as a result of the quick development of social media. It emphasizes those users on platforms like social media often express their viewpoints, favor, or against for other views or ideas. The existing stance detection methods can determine users’ stances based on the text through various classes and ordered classification. This implies a possible vulnerability in the current stance detection methods, which primarily focus on binary type (favor or against), an enormous range of stances for specific sequences in which they were expressed. To tackle these challenges, we proposed a model that can effectively handle stance detection using ordered classification (SDOC). The SDOC model accurately identified the sequence and arrangement of different positions within the given text. So our proposed model, SDOC, is based on Dual BiLSTM-GRU models for word embedding techniques (word2vec and glove 100, 200, and 300 dimensions) to different epoch sizes. The proposed model performs better accuracy, precision, recall, and F1-Score for specific epoch values (5,10,15).},
  archive      = {J_NCA},
  author       = {Poonam, Km and Ramakrishnudu, Tene},
  doi          = {10.1007/s00521-024-10549-9},
  journal      = {Neural Computing and Applications},
  month        = {6},
  number       = {17},
  pages        = {10439-10463},
  shortjournal = {Neural Comput. Appl.},
  title        = {Dual bi-LSTM-GRU based stance detection in tweets ordered classes},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Stacked semi-supervised autoencoder-regularized RVFLNs for reliable prediction of molten iron quality in blast furnace. <em>NCA</em>, <em>37</em>(17), 10425-10438. (<a href='https://doi.org/10.1007/s00521-024-10539-x'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper proposes a novel stacked semi-supervised autoencoder-regularized random vector functional-link networks (RVFLNs) for reliable prediction of molten iron quality (MIQ) in blast furnace (BF) ironmaking. First, in order to accurately describe the importance of different process variables on the multivariate MIQ indices, an attention mechanism based on feedforward compensation is introduced. The mechanism is embedded at the front end of the stacked semi-supervised autoencoder deep network. Secondly, to extract the deep feature information which is closely related to the prediction target from the process data, a deep network structure composed of multiple semi-supervised autoencoder models is introduced. The output of the last hidden layer is then used as the input of the subsequent prediction model. At the same time, two regularization terms L1 and L2 are incorporated into RVFLNs to sparse network output weights and improve the robustness of modeling, aiming to solve the multicollinearity and overfitting problems of the basic RVFLNs. Experiments using the standard dataset and actual industrial data of BF demonstrate that the proposed method has good performance.},
  archive      = {J_NCA},
  author       = {Zhou, Ping and Zhao, Peng and Ou, Zihui and Chai, Tianyou},
  doi          = {10.1007/s00521-024-10539-x},
  journal      = {Neural Computing and Applications},
  month        = {6},
  number       = {17},
  pages        = {10425-10438},
  shortjournal = {Neural Comput. Appl.},
  title        = {Stacked semi-supervised autoencoder-regularized RVFLNs for reliable prediction of molten iron quality in blast furnace},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). JDC-GCN: Joint diversity and consistency graph convolutional network. <em>NCA</em>, <em>37</em>(16), 10407-10423. (<a href='https://doi.org/10.1007/s00521-024-10897-6'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In real-world scenarios, multi-view data comprises heterogeneous features, with each feature corresponding to a specific view. The objective of multi-view semi-supervised classification is to enhance classification performance by leveraging the inherent complementary and consistent information present within diverse views. Nevertheless, many existing frameworks primarily focus on assigning suitable weights to different views while neglecting the importance of consistent information. In this paper, a multi-view semi-supervised classification framework called joint diversity and consistency graph convolutional network (JDC-GCN) is proposed. Firstly, the structure of graph convolutional network is introduced to the multi-view semi-supervised classification, capable of propagating the label information over the topological structure of multi-view data. Secondly, the proposed JDC-GCN captures the complementary and consistent information from multiple views through two indispensable sub-modules, Diversity-GCN and Consistency-GCN, respectively. Finally, the attention mechanism is leveraged to dynamically adjust the weights of various views, allowing us to measure the significance of heterogeneous features and the consistent graph without introducing additional parameters. Comprehensive experiments on eight multi-view datasets are conducted to validate the effectiveness of the JDC-GCN algorithm. The results show that the proposed method exhibits superior classification performance compared to other state-of-the-art methods.},
  archive      = {J_NCA},
  author       = {Lin, Renjie and Yao, Jie and Wang, Shiping and Guo, Wenzhong},
  doi          = {10.1007/s00521-024-10897-6},
  journal      = {Neural Computing and Applications},
  month        = {6},
  number       = {16},
  pages        = {10407-10423},
  shortjournal = {Neural Comput. Appl.},
  title        = {JDC-GCN: Joint diversity and consistency graph convolutional network},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Improving three-term conjugate gradient methods for training artificial neural networks in accurate heart disease prediction. <em>NCA</em>, <em>37</em>(16), 10381-10405. (<a href='https://doi.org/10.1007/s00521-025-11121-9'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This study presents a new optimization technique, the three-term conjugate gradient method, which enhances the efficiency of solving unconstrained optimization problems and its application in training artificial neural networks to predict heart diseases. It incorporates a gradient condition and a modified gradient difference vector. The method is validated through its global convergence and sufficient condition, ensuring both mathematical accuracy and computational stability. Compared to traditional conjugate gradient methods and three-term conjugate gradient methods, the proposed method demonstrates superior performance, characterized by fewer iterations, reduced function evaluations, and lower computational time. The method is also used to teach artificial neural networks how to predict heart disease, with an 88.15% success rate and precision, recall, and F1-Score values of 0.87, 0.86, and 0.87, respectively. These results confirm the method’s effectiveness in enhancing the predictive power of artificial neural networks making it a valuable tool for clinical applications. This study underscores the potential of the new method to improve decision-support systems in healthcare, aiding early detection and better patient outcomes. Future research may focus on refining the method and integrating it with other machine learning techniques to create advanced real-time predictive systems that further advance intelligent healthcare solutions.},
  archive      = {J_NCA},
  author       = {Ibrahim, Alaa Luqman and Fathi, Bayda Ghanim and Abdulrazzaq, Maiwan Bahjat},
  doi          = {10.1007/s00521-025-11121-9},
  journal      = {Neural Computing and Applications},
  month        = {6},
  number       = {16},
  pages        = {10381-10405},
  shortjournal = {Neural Comput. Appl.},
  title        = {Improving three-term conjugate gradient methods for training artificial neural networks in accurate heart disease prediction},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). GPU-accelerated homomorphic encryption computing: Empowering federated learning in IoV. <em>NCA</em>, <em>37</em>(16), 10351-10380. (<a href='https://doi.org/10.1007/s00521-025-11099-4'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Internet of Vehicles (IoV) has gained prominence due to advancements in intelligent connected technologies, generating vast amounts of personal data through various sensors and communication devices. However, traditional data transfer methods compromise user privacy, highlighting the need for efficient real-time data processing. This paper introduces a novel fully homomorphic encryption (FHE) model optimized for IoV Federated Learning (FL), addressing slow processing speeds inherent in existing FHE techniques. By leveraging GPU acceleration, the proposed framework facilitates local data training, reducing communication overhead and enhancing the speed of homomorphic operations. The optimization focuses on critical computations, including homomorphic multiplication, number theoretic transform (NTT), Chinese Remainder Theorem (CRT), and kernel fusion, using parallel processing strategies. Experimental evaluations reveal that the GPU-accelerated FHE framework improves execution efficiency dramatically: The CRT computation is enhanced by 103.6%, while homomorphic multiplication operations achieve an overall efficiency boost of 98.49%. Notably, for the MNIST dataset, the average execution time for homomorphic multiplication is reduced to 31.6 ms, compared to 2312.3 ms on the CPU. Similarly, for the CIFAR-10 dataset, the execution time drops to 67.1 ms from 3700.9 ms on the CPU. Additionally, the efficiency of the number theoretic transform is improved by 143.6%, demonstrating significant gains in performance. In terms of model accuracy, the proposed system achieves over 90% accuracy on the MNIST dataset and shows substantial improvement on the CIFAR-10 dataset, particularly with a rapid increase in accuracy noted in the latter. These results confirm the framework's capability to meet the low-latency demands of IoV applications while ensuring data privacy.},
  archive      = {J_NCA},
  author       = {Khan, Sangeen and Qiming, Huang},
  doi          = {10.1007/s00521-025-11099-4},
  journal      = {Neural Computing and Applications},
  month        = {6},
  number       = {16},
  pages        = {10351-10380},
  shortjournal = {Neural Comput. Appl.},
  title        = {GPU-accelerated homomorphic encryption computing: Empowering federated learning in IoV},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Bank credit risk prediction using machine learning model. <em>NCA</em>, <em>37</em>(16), 10333-10350. (<a href='https://doi.org/10.1007/s00521-025-11044-5'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Credit risk is one of the most prevalent risks in the banking sector. For credit risk management, digital transformation brings greater clarity to the risk profiles. Typically, datasets include various features, and many of them may be irrelevant. Removing relevant features and keeping irrelevant ones in the dataset may be harmful and lead to ruining the decision-making process. In this paper, we investigate the interesting decision variables that can be used to create a credit risk prediction model. This model will help the bank staff evaluate the loan applications of customers and classify the loan applicants as “defaulter” and “non-defaulter” customers. In addition, the loan applicants can decide on the loan acceptance status online without physical visits to the bank. In this work, we build a machine learning model by using a multilayer perceptron (MLP) algorithm based on the Australian and Taiwan banking dataset. Besides, we deploy the SMOTE technique to balance the data and overcome the suboptimal results. For statistical calculations, we use the R computing environment. The results show that demographic, financial, economic, and ‘behavioral indicators have significant effects on credit risk and the efficiency of the proposed model, with an average accuracy of 89.543% for the Australian dataset and 88.345% for the Taiwan dataset.},
  archive      = {J_NCA},
  author       = {Gasmi, Ines and Neji, Sana and Mansouri, Nesrine and Soui, Makram},
  doi          = {10.1007/s00521-025-11044-5},
  journal      = {Neural Computing and Applications},
  month        = {6},
  number       = {16},
  pages        = {10333-10350},
  shortjournal = {Neural Comput. Appl.},
  title        = {Bank credit risk prediction using machine learning model},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). An aspect-based sentiment analysis model for arabic game reviews based on hybrid transformers models. <em>NCA</em>, <em>37</em>(16), 10309-10331. (<a href='https://doi.org/10.1007/s00521-025-11032-9'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Aspect-based sentiment analysis (ABSA) is a natural language processing (NLP) technique to determine the various sentiments of a customer in a single comment regarding different aspects. The increasing online data content generated by interested customers and reviewers motivated researchers and data scientists to conduct ABSA. ABSA has become increasingly popular in recent years due to its versatility in e-commerce, social media, and customer feedback analysis. However, ABSA faces several significant challenges, including determining the aspects and their sentiment polarities (positive, negative, or neutral) in a given text. Moreover, ABSA faces particular challenges in non-English languages such as Arabic due to the lack of resources and mature models. Typically, ABSA tackles one or more of the ABSA research tasks: (T1) aspect term extraction, (T2) aspect term polarity, (T3) aspect category identification, and (T4) aspect category polarity. To identify the aspects and their corresponding sentiment polarities in a given text, accurate and efficient NLP techniques are required. Despite growing interest in Arabic ABSA, the lack of annotated datasets and pre-trained models has hindered its development. In this research, we have collected a dataset of Arabic game reviews and annotated them using three annotators, and then we trained an ABSA deep learning model based on the BERT pre-trained model combined with zero-shot learning (ZSL) to tackle all the four aforementioned tasks. Our best performing model achieved a high accuracy on all four tasks with an accuracy of 91.61% on T1, 90.99% on T2, 79.08% on T3, and 88.17% on T4. Finally, we compared our model’s accuracy with the state-of-the-art Arabic-based ABSA models on different datasets.},
  archive      = {J_NCA},
  author       = {Hammad, Mahmoud and AbuEnnab, Noor and Al-Refai, Mohammed},
  doi          = {10.1007/s00521-025-11032-9},
  journal      = {Neural Computing and Applications},
  month        = {6},
  number       = {16},
  pages        = {10309-10331},
  shortjournal = {Neural Comput. Appl.},
  title        = {An aspect-based sentiment analysis model for arabic game reviews based on hybrid transformers models},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Design of electric powertrains to achieve NVH performance using autoencoders and a physical meaningful latent space. <em>NCA</em>, <em>37</em>(16), 10287-10308. (<a href='https://doi.org/10.1007/s00521-025-11117-5'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {One of the fundamental differences in the perception of electric (e-) vehicles is how their radiated noise is perceived with respect to classic internal combustion engines. Even though e-vehicles are usually quieter, the tonal content of the radiated noise can be more annoying. This paper proposes a novel approach that starts from the assumed radiated noise spectrum profile as input to a neural network that can return powertrain design parameters that would lead to generation of that specific noise profile. The proposed network acts as an autoencoder where the latent space is forced to have a physical meaning. As diverse combinations of powertrain parameters can result in similar noise profiles, a variational autoencoder is used to learn a structured latent representation, ensuring continuity and smooth transitions between possible solutions. The network predictions are validated against results of a three-dimensional CAE e-powertrain model. Overall, the mean absolute error is around 5 dBA for this feasibility study, which aims to demonstrate the concept. This work takes an inverse approach to the optimisation problem by starting from the user-perceived noise to predict the parameters required to achieve that. Although this study focuses solely on gear teeth microgeometry changes and bearing preloads, additional powertrain parameters could be incorporated as needed.},
  archive      = {J_NCA},
  author       = {Ricardo Souza, Marcos and Offner, Guenter and Soltoggio, Andrea and Mohammadpour, Mahdi and Theodossiades, Stephanos},
  doi          = {10.1007/s00521-025-11117-5},
  journal      = {Neural Computing and Applications},
  month        = {6},
  number       = {16},
  pages        = {10287-10308},
  shortjournal = {Neural Comput. Appl.},
  title        = {Design of electric powertrains to achieve NVH performance using autoencoders and a physical meaningful latent space},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multi-route multi-objective TSP: Mathematical model and its solution by reference point and aspiration level-based MOQO jaya algorithm. <em>NCA</em>, <em>37</em>(16), 10243-10285. (<a href='https://doi.org/10.1007/s00521-025-11029-4'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, Multi-Route Multi-Objective Traveling Salesman Problem (MRMOTSP) including the consequences of carbon emission under uncertain environment is developed and solved using RPAL-based MOQO Jaya algorithm. Expected and optimistic model is considered in this paper. The uncertain MRMOTSP is solved for 10 and 50 city nodes of Surat city. Proposed algorithm is devoid of metaphorical elements and does not necessitate any algorithm-specific parameters. The solutions are compared with the solutions obtained by established optimization techniques like hybrid GA (HGA), optimization tool CPLEX, fuzzy programming technique (FPT), Rao algorithms, and AL-based MOQO Jaya to check the efficiency of the proposed algorithm. The developed algorithm outperforms other methods by yielding superior solutions and offering a greater number of alternative solutions to the decision-maker, all within a shorter runtime when compared to alternative methods.},
  archive      = {J_NCA},
  author       = {Bajaj, Aaishwarya and Dhodiya, Jayesh},
  doi          = {10.1007/s00521-025-11029-4},
  journal      = {Neural Computing and Applications},
  month        = {6},
  number       = {16},
  pages        = {10243-10285},
  shortjournal = {Neural Comput. Appl.},
  title        = {Multi-route multi-objective TSP: Mathematical model and its solution by reference point and aspiration level-based MOQO jaya algorithm},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Impression evaluation of product images using deep neural network. <em>NCA</em>, <em>37</em>(16), 10215-10242. (<a href='https://doi.org/10.1007/s00521-025-11129-1'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Understanding products and customers is a critical challenge for efficient business operations. While various machine learning-based analytical methods have been proposed, most rely on objective metrics such as evaluation scores or tags. However, estimating subjective evaluation scores is also an essential aspect of understanding customers, yet research in this area remains limited. Moreover, it is well-known that directly evaluating the subjective scores of targets is challenging. Consequently, traditional methods have used pairwise comparisons between targets to estimate true evaluation scores. However, as the number of targets increases, the required number of pairwise comparisons grows exponentially, making it difficult to estimate subjective evaluations for a large number of targets using conventional methods. To address this issue, this study proposes a scalable model for subjective evaluation score estimation by completing pairwise comparison data using a deep learning model trained on a limited number of annotations. Specifically, the deep learning model is trained on pairwise comparison results from a subset of evaluation target combinations annotated by humans, and the model’s predictions are used to complete the pairwise comparison matrix. The effectiveness and practical applicability of the proposed method are demonstrated through applications to multiple real-world datasets.},
  archive      = {J_NCA},
  author       = {Yamagiwa, Ayako and Goto, Masayuki},
  doi          = {10.1007/s00521-025-11129-1},
  journal      = {Neural Computing and Applications},
  month        = {6},
  number       = {16},
  pages        = {10215-10242},
  shortjournal = {Neural Comput. Appl.},
  title        = {Impression evaluation of product images using deep neural network},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Using join learning of multi-level features for automated essay scoring. <em>NCA</em>, <em>37</em>(16), 10197-10214. (<a href='https://doi.org/10.1007/s00521-025-11131-7'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, the approach of extracting features of essays at different levels for joint learning and scoring using hybrid models has achieved excellent results. However, there are still some issues that need to be improved: (1) Splitting the essays into sentences of the same length when extracting sentence-level features may result in the truncation of sentence-level semantics; (2) multi-level representations of essays obtained through pre-trained language models (PLMs) are not sufficiently comprehensive and in depth to provide limited improvement in model performance; and (3) in extracting features from multi-level essay representations, finer-grained local features in sentences and dependencies among features are ignored. To address these issues, we propose an improved method for joint learning of multi-level features based on PLMs. Specifically, we first split the essays according to different semantic scales through a text splitter to maximize the retention of semantic and structural features at sentence level; secondly, we integrate the essay representations derived from the PLM and mine more comprehensive and deeper multi-level features from it; and finally, we aggregate sentence-level features and construct dependencies among them by a feature extraction fusion method. The value of average quadratic weighted kappa (QWK) on the Kaggle ASAP competition dataset is 0.809, which validates the effectiveness of our proposed method in the automated essay scoring (AES) task.},
  archive      = {J_NCA},
  author       = {Li, Shi and Gao, Yan},
  doi          = {10.1007/s00521-025-11131-7},
  journal      = {Neural Computing and Applications},
  month        = {6},
  number       = {16},
  pages        = {10197-10214},
  shortjournal = {Neural Comput. Appl.},
  title        = {Using join learning of multi-level features for automated essay scoring},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Melanoma lesion localization using UNet and explainable AI. <em>NCA</em>, <em>37</em>(16), 10175-10196. (<a href='https://doi.org/10.1007/s00521-025-11080-1'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Melanoma is a lethal type of skin cancer that has become very common due to its high metastatic rate. Therefore, accurate and timely diagnosis plays a vital role in a patient’s effective treatment and recovery. Melanoma dermoscopic images provide a detailed analysis of pigmented lesions. Traditional manual segmentation by dermatologists has limitations such as inter-observer variability, time consumption, and human error. The deep learning (DL) techniques enhance diagnosis by automating lesion detection and segmentation. In this work, a DL framework for the localization of melanoma lesions using dermoscopic images is presented. The proposed framework utilizes an encoder-decoder architecture inspired by the UNet model. The encoder-decoder architecture enables effective feature extraction and spatial information preservation. The encoder part efficiently captures hierarchical features from the input data. At the same time, the decoder part reconstructs the spatial details, leading to accurate segmentation results. Therefore, the proposed framework takes advantage of the capability of the encoder-decoder architecture and employs it in the depth of 3. Extensive experiments are conducted to determine the optimal set of hyperparameters and architecture. The performance of the proposed framework is assessed on unseen samples via a cross-database validation scenario. The proposed modified UNet framework achieves notable accuracy, with a Jaccard Index and BF Score of 0.95, 0.92, and 0.73, respectively. Subsequently, our proposed framework’s outcomes are visually analyzed using Explainable Artificial Intelligence (XAI) algorithms. It showcases the proposed framework’s ability to accurately segment lesions even in the presence of various artifacts such as hair, clinical swatches, markers, and variations in intensity and size. The performance of the proposed framework is compared with the existing works. The efficacy and robustness of the proposed framework are evident from the results on benchmark datasets.},
  archive      = {J_NCA},
  author       = {Kibriya, Hareem and Siddiqa, Ayesha and Khan, Wazir Zada},
  doi          = {10.1007/s00521-025-11080-1},
  journal      = {Neural Computing and Applications},
  month        = {6},
  number       = {16},
  pages        = {10175-10196},
  shortjournal = {Neural Comput. Appl.},
  title        = {Melanoma lesion localization using UNet and explainable AI},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Quantum-based deep learning method for recognition of facial expressions. <em>NCA</em>, <em>37</em>(16), 10163-10173. (<a href='https://doi.org/10.1007/s00521-024-10968-8'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Emotion recognition from facial expressions is a fundamental challenge in affective computing due to variability, context, cultural differences, and subtlety of expressions. Traditional approaches have predominantly relied on classical machine learning techniques and neural networks to decipher emotional cues from facial images. In this work, a framework for emotion recognition using facial images that leverage the power of Quantum Convolutional Neural Network (QCNN) is presented. Drawing inspiration from the principles of quantum computing, the proposed Quantum-inspired Convolutional Neural Network (QiCNN) approach revolutionizes how emotional features are captured, processed, and analyzed. The proposed model encodes intricate emotional nuances within quantum states by harnessing quantum-inspired operations, enabling enhanced feature extraction and fusion. Moreover, the performance of the proposed model was evaluated on three emotion datasets, i.e., the CK+, FER2013, and AffectNet, showcasing its potential advantages for reliability and robustness. Also, a comparative evaluation of the proposed model with the conventional Convolutional Neural Network (CNN) technique in terms of accuracy and execution time is done along with the analysis of the proposed model performance with other recent contemporary methods.},
  archive      = {J_NCA},
  author       = {Golchha, Roopa and Sahu, Mridu and Bhateja, Vikrant},
  doi          = {10.1007/s00521-024-10968-8},
  journal      = {Neural Computing and Applications},
  month        = {6},
  number       = {16},
  pages        = {10163-10173},
  shortjournal = {Neural Comput. Appl.},
  title        = {Quantum-based deep learning method for recognition of facial expressions},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Zero-shot image segmentation for scene objects based on the l0 gradient minimization and adaptive superpixel method. <em>NCA</em>, <em>37</em>(16), 10141-10161. (<a href='https://doi.org/10.1007/s00521-025-11122-8'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper explores using convolutional neural networks (CNNs) for unsupervised image segmentation. The method enhances pixel labeling accuracy through superpixel and propagates back strategies. Leveraging CNN’s feature extraction capabilities, pixels are assigned labels without training data or prior knowledge. In an unsupervised context, a single image is the network’s input, and parameters are updated via gradient descent. A preprocessing module applies image smoothing before network input to improve segmentation performance. The convolutional kernels alternate between $$3\times 3$$ and $$1\times 1$$ sizes, and the activation functions, including ReLU and Batch Normalization, are reordered. Additionally, the Gray-Level Co-occurrence Matrix is integrated, and an adaptive superpixel method is introduced. Experimental results show significant improvements in Precision, Recall, and F1-score across corresponding datasets and superior performance in statistical indices like ARE, DH, AMI, and FMI.},
  archive      = {J_NCA},
  author       = {Yan, Hailong and Huang, Junjian and Zheng, Mao and Tang, Yijie},
  doi          = {10.1007/s00521-025-11122-8},
  journal      = {Neural Computing and Applications},
  month        = {6},
  number       = {16},
  pages        = {10141-10161},
  shortjournal = {Neural Comput. Appl.},
  title        = {Zero-shot image segmentation for scene objects based on the l0 gradient minimization and adaptive superpixel method},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Integrating convolutional and recurrent neural networks for lung and colon disease detection. <em>NCA</em>, <em>37</em>(16), 10119-10140. (<a href='https://doi.org/10.1007/s00521-025-11143-3'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Research on automated techniques for analysing medical imaging for lung and colon disease is one of the primary areas of focus. In order to improve patient outcomes and survival rates, lung and colon illnesses must be accurately and promptly identified. This study proposes an innovative deep learning approach that seamlessly integrates CNNs and RNNs to accurately detect lung and colon diseases from medical imaging data. The CNN component excels in extracting discriminative visual features from CT scans, while the RNN component effectively models the inherent temporal dependencies within the data. Proposed integrated CNN-RNN models consistently outperformed existing methods, setting new benchmarks for lung and colon disease detection tasks through rigorous evaluation on multiple publicly available datasets, such as the LUNA16 dataset for lung nodule detection, the ILD-Chest dataset for interstitial lung disease classification, and the CVC-Clinic DB and CVC-Video land datasets for polyp detection in colonoscopy videos. The proposed methodology establishes a framework for diagnosing lung and colon diseases. In the present study, extensive experiments were conducted by integrating various CNN architectures, such as DenseNet201, EfficientNetB3, InceptionV3, InceptionResNetV2, MobileNetV3Large, and Xception, with GRU and LSTM components for sequence modelling. The results demonstrated exceptional performance, with the EfficientNetB3, MobileNetV3Large, and Xception architectures combined with GRU and LSTM achieving an impressive accuracy of 99.96%.},
  archive      = {J_NCA},
  author       = {Singh, Anirudh and Kumar, Satyam and Gangrade, Jayesh and Singh, Yadvendra Pratap and Dey, Tapan Kumar},
  doi          = {10.1007/s00521-025-11143-3},
  journal      = {Neural Computing and Applications},
  month        = {6},
  number       = {16},
  pages        = {10119-10140},
  shortjournal = {Neural Comput. Appl.},
  title        = {Integrating convolutional and recurrent neural networks for lung and colon disease detection},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A novel commonsense reasoning method based on heterogeneous knowledge fusion and adversarial training. <em>NCA</em>, <em>37</em>(16), 10101-10118. (<a href='https://doi.org/10.1007/s00521-025-11128-2'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Commonsense question answering (CQA) task is an effective way to measure the ability of language models to understand commonsense knowledge. The previous research focus on combining pre-trained language models (PLMs) with knowledge graphs (KG) for knowledge reasoning. However, these methods still have some limitations, such as the lacking of entity background knowledge in KG, how to fuse heterogeneous information effectively and poor model robustness. To solve the above problems, we propose a novel commonsense reasoning method based on heterogeneous knowledge fusion and adversarial training (HKF-AT). A multi-source knowledge retrieval method is adopted to retrieve entity-related texts in a large corpus and extract the knowledge path from question entity to option entity in KG. Further, multi-source information fusion strategy makes the model utilize and learn rich external knowledge information directly. Specially, the new adversarial samples are generated for both the retrieved texts and triples, and adversarial training is carried out. The experimental results on public benchmark dataset indicate that the proposed method effectively improves the performance and robustness of the model compared with other excellent models.},
  archive      = {J_NCA},
  author       = {Du, Yongping and Zhang, Qi and Yan, Jingya and Hou, Ying and Han, Honggui},
  doi          = {10.1007/s00521-025-11128-2},
  journal      = {Neural Computing and Applications},
  month        = {6},
  number       = {16},
  pages        = {10101-10118},
  shortjournal = {Neural Comput. Appl.},
  title        = {A novel commonsense reasoning method based on heterogeneous knowledge fusion and adversarial training},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Adaptive friction modeling in feeding systems based on dual neural networks. <em>NCA</em>, <em>37</em>(16), 10079-10100. (<a href='https://doi.org/10.1007/s00521-025-11098-5'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Friction is one of the primary factors affecting the tracking performance of a machine tool feed system. Establishing an accurate friction model is the key to achieving friction compensation. Although neural networks trained on friction data can make accurate predictions, they cannot adapt to friction characteristic changes that occur over time because their model parameters are fixed. In this work, a sample acquisition scheme with changing friction characteristics is first designed. Then, an adaptive modeling method using dual neural networks is proposed. The method constructs two networks with the same structure, Net-P and Net-T, which carry out the prediction and training tasks, respectively. While Net-P performs prediction in real-time, Net-T can continuously update its parameters with the latest friction data. The parameters of Net-T are then copied to Net-P to adapt to the friction characteristic changes. In addition, a classifier-based sample pool is proposed to enable the model to learn the latest friction characteristics, while preventing overfitting. The experimental results show that the proposed method can successfully adapt to variation in the friction characteristics, with a root mean square error (RMSE) of 0.034 Nm and a maximum absolute error (MAE) of 0.248 Nm. Compared with those of the three comparison models, the RMSEs of the proposed model are 13.7%, 70.8%, and 73.9% smaller, and the MAEs are 40.1%, 45.2%, and 47.2% smaller. Moreover, the prediction results of the proposed model are smoother than those of the compared adaptive models and less likely to cause system vibration.},
  archive      = {J_NCA},
  author       = {Huang, Dehai and Yang, Jianzhong and Zhou, Huicheng and Xu, Guangda},
  doi          = {10.1007/s00521-025-11098-5},
  journal      = {Neural Computing and Applications},
  month        = {6},
  number       = {16},
  pages        = {10079-10100},
  shortjournal = {Neural Comput. Appl.},
  title        = {Adaptive friction modeling in feeding systems based on dual neural networks},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). An effective medical image fusion method utilizing moth-flame optimization and coupled neural p systems. <em>NCA</em>, <em>37</em>(16), 10037-10077. (<a href='https://doi.org/10.1007/s00521-025-11072-1'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Medical image fusion is crucial for various applications, yet existing fusion models often encounter issues such as low quality, information loss, and insufficient contrast. This paper presents a novel approach to medical image fusion that directly addresses these challenges by enhancing brightness and contrast, improving overall image quality, and preserving essential structural features. Our method begins by decomposing input images into a base component and two detail components using a three-component decomposition (TCD) method with a truncated Huber filter (THF). The detail components are subsequently fused using an integrated approach combining the structure tensor saliency detection operator (STSDO), distance-weighted regional energy (DWRE), fast guided filter (FGF), and coupled neural P system (CNPS). The base components are then fused through an adaptive method guided by the moth-flame optimization (MFO) algorithm. To evaluate our approach, we used a dataset of 120 pairs of medical images and assessed performance with metrics such as $$Q_{MLI}$$ , $$Q_{CI}$$ , $$Q_{AG}$$ , $$Q^{AB/F}$$ , $$Q_{P}$$ , $$Q_{W}$$ , $$Q_{E}$$ , and $$Q_{CB}$$ . Experimental results demonstrate that the proposed model significantly enhances brightness and contrast and more effectively preserves information compared to recent methods.},
  archive      = {J_NCA},
  author       = {Dinh, Phu-Hung and Le, Thi-Hong-Ha and Giang, Nguyen Long},
  doi          = {10.1007/s00521-025-11072-1},
  journal      = {Neural Computing and Applications},
  month        = {6},
  number       = {16},
  pages        = {10037-10077},
  shortjournal = {Neural Comput. Appl.},
  title        = {An effective medical image fusion method utilizing moth-flame optimization and coupled neural p systems},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Fusion of drones tracking using different LSTM approaches and a CMA-EA knowledge base approach. <em>NCA</em>, <em>37</em>(16), 9991-10036. (<a href='https://doi.org/10.1007/s00521-025-11060-5'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper introduces new data fusion techniques to enhance the accuracy and adaptability of drone tracking systems in complex and dynamic scenarios. The paper investigates the application of three approaches that rely on long short-term memory (LSTM) in addition to the covariance matrix adaptation evolution strategy (CMA-ES) algorithm approach. All are used to predict the optimal fusion of tracks for drone tracking data tasks. In particular, and in the first approach, the LSTM capability in capturing the mixing parameter temporal patterns is used to predict future values. In the second approach, which is the dynamic fusing approach, the LSTM learns to return fused tracks by learning how to apply fusion by itself. Furthermore, in the third approach, the fusion is achieved by incorporating the minimization of the drone tracking innovation values. The fourth approach, the CMA-ES, is used offline to build a knowledge base that is used in online inference for the fusion mixing parameter values. In all methods, the extended Kalman filter method is used for estimating the tracks. The paper comprehensively analyzes the proposed methods across multiple simulation scenarios and compares their performance against baseline approaches. The results show that the proposed methods significantly improve the accuracy and robustness of drone tracking systems in a Stone Soup simulated environment. Overall, this paper contributes to the advancement of drone tracking systems by proposing effective methods for finding the optimum mixing parameters in data fusion. Comparisons are made between the different proposed techniques based on standard metric techniques such as OSPA (optimal subpattern assignment) and the SIAP (single integrated air picture). Each method showed strengths and weaknesses; however, the LSTM second and third approaches showed advantages over the other techniques.},
  archive      = {J_NCA},
  author       = {Zitar, Raed Abu and Fares, Samar and El Fallah Seghrouchni, Amal and Barbaresco, Frederic},
  doi          = {10.1007/s00521-025-11060-5},
  journal      = {Neural Computing and Applications},
  month        = {6},
  number       = {16},
  pages        = {9991-10036},
  shortjournal = {Neural Comput. Appl.},
  title        = {Fusion of drones tracking using different LSTM approaches and a CMA-EA knowledge base approach},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). 3D AIR-UNet: Attention–inception–residual-based U-net for brain tumor segmentation from multimodal MRI. <em>NCA</em>, <em>37</em>(16), 9969-9990. (<a href='https://doi.org/10.1007/s00521-025-11105-9'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Brain tumors are ranked highly among the leading causes of cancer-related fatalities. Precise segmentation and quantitative assessment of brain tumors are crucial for effective diagnosis and treatment planning. However, manual segmentation is often laborious, challenging, and prone to errors, necessitating the creation of a fully automated brain tumor segmentation approach. This article introduces “3D AIR-UNet,” an end-to-end architecture aiming to automate the segmentation of brain tumors from MRI data. The presented model employs an encoder–decoder architecture, with carefully constructed inception–residual units replacing the usual convolution layers used in UNet. The inception–residual block combines the advantages of inception modules and residual connections to provide a powerful feature extraction mechanism. It captures extensive multi-scale information by combining different filter sizes. This block’s design is effective at handling complex 3D data patterns, making it a vital component of sophisticated neural network architecture. Moreover, an attention mechanism further boosts the capability of the model to differentiate between tumor and non-tumor regions, leading to improved localization and contextual understanding. Additionally, skip connections are employed between the encoder and decoder at each level to speed up the training process. The proposed 3D AIR-UNet architecture demonstrated encouraging outcomes, attaining dice scores of 0.9218 for the whole tumor, 0.9019 for the tumor core, and 0.8788 for the enhancing tumor when evaluated on the BraTS 2020 dataset. Comparative analysis with contemporary methods suggests that 3D AIR-UNet notably enhances the segmentation accuracy of brain tumor subregions.},
  archive      = {J_NCA},
  author       = {Sharma, Vani and Kumar, Mohit and Yadav, Arun Kumar},
  doi          = {10.1007/s00521-025-11105-9},
  journal      = {Neural Computing and Applications},
  month        = {6},
  number       = {16},
  pages        = {9969-9990},
  shortjournal = {Neural Comput. Appl.},
  title        = {3D AIR-UNet: Attention–inception–residual-based U-net for brain tumor segmentation from multimodal MRI},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Evaluating BERT-based language models for detecting misinformation. <em>NCA</em>, <em>37</em>(16), 9937-9968. (<a href='https://doi.org/10.1007/s00521-025-11101-z'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Online misinformation poses a significant challenge due to its rapid spread and limited supervision. To address this issue, automated rumour detection techniques are essential for countering the negative impact of false information. Previous research primarily focussed on extracting text features, which proved time-consuming and less effective. In this study, we contribute substantially to two domains: rumour detection on Twitter and the evaluation of text embeddings. We thoroughly analyse rumour detection models and compare the quality of text embeddings generated by various fine-tuned BERT-based models. Our findings indicate that our proposed models outperform existing techniques. Notably, when we test these models on combined datasets, we observe significant performance improvements with larger training and testing data sizes. We conclude that carefully considering the dataset, data splitting, and classification techniques is crucial for evaluating solution performance. Additionally, we find that differences in the quality of text embeddings between RoBERTa, BERT, and DistilBERT are insignificant. This challenges existing assumptions and highlights the need for future research to explore these nuances further.},
  archive      = {J_NCA},
  author       = {Anggrainingsih, Rini and Hassan, Ghulam Mubashar and Datta, Amitava},
  doi          = {10.1007/s00521-025-11101-z},
  journal      = {Neural Computing and Applications},
  month        = {6},
  number       = {16},
  pages        = {9937-9968},
  shortjournal = {Neural Comput. Appl.},
  title        = {Evaluating BERT-based language models for detecting misinformation},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A deep learning method combined with clustering to support the diagnosis of atopic dermatitis and contact dermatitis in vietnam. <em>NCA</em>, <em>37</em>(16), 9911-9935. (<a href='https://doi.org/10.1007/s00521-025-11111-x'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Skin diseases are common across all ages and are significant sources of infection. Diagnosing skin diseases involves a number of tests. The diagnostic process is laborious, time-consuming, and requires extensive understanding, especially for skin diseases with similar symptoms. Atopic dermatitis and contact dermatitis are two skin diseases with similar symptoms that are often misdiagnosed. These are also two common skin diseases in Vietnam. The disease images used in this study were provided by the Central Hospital of Dermatology. The images were taken using common techniques such as smartphones or cameras and labeled by specialists. To improve the diagnosis of these two inflammatory skin diseases, author proposes a technique that utilizes deep learning features extracted by the VGG-16 model. The K-means clustering technique is then applied to the feature set. Next, a deep convolutional neural network model is trained on each cluster. Finally, author uses a technique that combines the above models to produce the final result. The proposed technique achieves accuracy, recall, and F1-score metrics of 73%, 73%, and 71%, respectively.},
  archive      = {J_NCA},
  author       = {Vu, Van-Hieu},
  doi          = {10.1007/s00521-025-11111-x},
  journal      = {Neural Computing and Applications},
  month        = {6},
  number       = {16},
  pages        = {9911-9935},
  shortjournal = {Neural Comput. Appl.},
  title        = {A deep learning method combined with clustering to support the diagnosis of atopic dermatitis and contact dermatitis in vietnam},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Crop weed separation through image-level segmentation: An ensemble of modified U-net and encoder–decoder. <em>NCA</em>, <em>37</em>(16), 9887-9910. (<a href='https://doi.org/10.1007/s00521-025-11123-7'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Crop weed segmentation is one of the most challenging tasks in the field of computer vision. This is because, unlike other object detection or segmentation tasks, crop and weed are similar in terms of spectral features, shape, dimensions, etc. For precision agriculture to flourish in terms of smart spraying of crops, efficient systems to distinguish between crop and weed are the need of the hour, which if precise, will take a huge step toward solving the issue of food scarcity. To tackle this issue, we propose new ensemble architecture of two models—a U-Net with a modified backbone and an encoder–decoder. These networks learn to distinguish between soil and crop and soil and weed, respectively, whose ensemble gives state-of-the-art results on pixel-wise annotations of combined crop and weed images. Moreover, it also learns that the model captures un-annotated features since each component of the architecture learnt either crop or weed features to high precision. Finally, the proposed architecture is compared with the U-Net and SegNet, which are popular segmentation networks, and consistently achieved better results.},
  archive      = {J_NCA},
  author       = {Ganapathy, Sannasi and Srinivasan, Srinitish},
  doi          = {10.1007/s00521-025-11123-7},
  journal      = {Neural Computing and Applications},
  month        = {6},
  number       = {16},
  pages        = {9887-9910},
  shortjournal = {Neural Comput. Appl.},
  title        = {Crop weed separation through image-level segmentation: An ensemble of modified U-net and encoder–decoder},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Adaptive hybrid enhanced flower pollination-based simulated annealing scheme for task scheduling in edge-cloud continuum. <em>NCA</em>, <em>37</em>(16), 9861-9886. (<a href='https://doi.org/10.1007/s00521-025-11104-w'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the rapidly advancing edge-cloud continuum computing, efficient task scheduling is crucial for optimizing the performance of large-scale and latency-sensitive applications. However, existing metaheuristic techniques often struggle with slow convergence, high computational complexity, and an imbalance between local and global searches, which limit their scalability and real-time adaptability. To address these challenges, this study aims to develop an efficient task scheduling algorithm, the Adaptive Hybrid Enhanced Flower Pollination-Based Simulated Annealing (AHEFPA-SA) algorithm. The AHEFPA-SA algorithm integrates simulated annealing with adaptive mechanisms, employing a chaotic cycle map to improve initial solutions and adaptive inertia weight to control levy flight distance, thereby reducing computational complexity and accelerating convergence. Implemented within the EdgeCloudSim simulator, the algorithm is evaluated against benchmark algorithms across key performance metrics including makespan, execution cost, and resource utilization, the proposed algorithm demonstrates significant improvements. The results show up to 22% reduction in makespan on edge servers and 20% on cloud servers, along with a 16% reduction in execution cost and a 13% improvement in resource utilization. These findings highlight the adaptability and real-time efficiency of the AHEFPA-SA algorithm, making it well-suited for dynamic, resource-constrained edge-cloud environments.},
  archive      = {J_NCA},
  author       = {Dankolo, Nasiru Muhammad and Radzi, Nor Haizan Mohamed and Mustaffa, Noorfa Haszlinna and Gabi, Danlami},
  doi          = {10.1007/s00521-025-11104-w},
  journal      = {Neural Computing and Applications},
  month        = {6},
  number       = {16},
  pages        = {9861-9886},
  shortjournal = {Neural Comput. Appl.},
  title        = {Adaptive hybrid enhanced flower pollination-based simulated annealing scheme for task scheduling in edge-cloud continuum},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Monocular vision navigation system for UAV autonomous mission: A real-time window-based obstacle avoidance approach. <em>NCA</em>, <em>37</em>(16), 9843-9860. (<a href='https://doi.org/10.1007/s00521-025-11093-w'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The need for autonomous unmanned aerial vehicles (UAVs) is rapidly increasing for various industrial applications today. However, the realization of UAV autonomy requires a careful mix of Artificial Intelligence (AI) and computer vision (CV) techniques to be implemented to address end-user challenges and satisfy field requirements. This study aims to develop an intelligent system for UAV to achieve autonomy with standard low-cost hardware components. We have infused state-of-the-art AI and CV technologies into our developed system architecture to improve autonomous navigational capabilities of UAVs. The proposed architecture has been implemented and tested on the DJI Tello UAV equipped only with a monocular camera on-board and an inertial measurement unit (IMU). We focused on solving multiple practical technical problems that have arisen during this study due to the hardware limitations of the UAV. The study investigated methods for representing obstacles within an environmental map, which are crucial for effective avoidance and path planning, such as their edges, width, and shape, to ensure accurate detection and navigation. The experiments were carried out in an environment that included dynamic obstacles that were not considered in the map. The experimental results showed outstanding performance in terms of successful autonomous navigation for a wide range of light intensity. These results show that the proposed approach has a high potential to achieve autonomous UAV operation in the field with low hardware and energy requirements.},
  archive      = {J_NCA},
  author       = {Javaid, Abdulrahman and Alnaser, Mustafa and Baroudi, Uthman and Alfaraj, Amjad},
  doi          = {10.1007/s00521-025-11093-w},
  journal      = {Neural Computing and Applications},
  month        = {6},
  number       = {16},
  pages        = {9843-9860},
  shortjournal = {Neural Comput. Appl.},
  title        = {Monocular vision navigation system for UAV autonomous mission: A real-time window-based obstacle avoidance approach},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Decision-making algorithm based on the energy of interval-valued hesitant fuzzy soft sets. <em>NCA</em>, <em>37</em>(16), 9821-9841. (<a href='https://doi.org/10.1007/s00521-025-11107-7'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Interval-valued hesitant fuzzy soft sets, a powerful tool for data modeling, have been the focus of numerous studies. In addition to their various applications, examining the properties and characteristics of these structures from a theoretical mathematical perspective enables more efficient utilization in real-world problems. The primary objective of this research is to define a numerical characteristic of an interval-valued hesitant fuzzy soft set, analogous to the energy of a graph and the energy of a fuzzy soft set. The motivation for introducing this numerical characteristic lies in its potential to develop a decision-making algorithm that outperforms existing approaches addressing the same problem. The energy of a graph, as well as the energy of a fuzzy soft set, is defined as a norm function obtained by summing the singular values of the corresponding representation matrix. In optimization problems, this sum of singular values is known as the nuclear norm, which serves as the basis for introducing the notion of energy in interval-valued hesitant fuzzy soft sets. The results of the proposed algorithm are compared in this paper with those of various existing algorithms designed to solve the same problem.},
  archive      = {J_NCA},
  author       = {Stojanović, Nenad and Laković, Maja and Djurović, Ljubica},
  doi          = {10.1007/s00521-025-11107-7},
  journal      = {Neural Computing and Applications},
  month        = {6},
  number       = {16},
  pages        = {9821-9841},
  shortjournal = {Neural Comput. Appl.},
  title        = {Decision-making algorithm based on the energy of interval-valued hesitant fuzzy soft sets},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). BERT-based keyword extraction model for the turkish language. <em>NCA</em>, <em>37</em>(16), 9807-9819. (<a href='https://doi.org/10.1007/s00521-025-11103-x'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Keyword extraction is crucial in natural language processing (NLP) tasks, aiding in information retrieval, document summarization, and content categorization. While many studies have discussed keyword extraction for different languages, the Turkish language presents unique challenges due to its rich morphology, complex syntax, and agglutinative nature. This paper proposes a keyword extraction model for Turkish based on the deep learning model of bidirectional encoder representation transformers (BERT) and NLP. The proposed model has been trained using a novel Turkish dataset specifically collected for this task. The dataset was fetched from over 128,000 theses published in the National Thesis Center of Türkiye. 90% of the dataset used for training the model, and 10% of the dataset used for testing. Our experimental results indicate that the proposed model outperforms similar existing methods highlighting a significant advancement in Turkish text keyword extraction. The performance of the proposed model achieved values of 97.77% F1-score, 97.84% precision, and 97.71% recall.},
  archive      = {J_NCA},
  author       = {Babayigit, Bilal and Sattuf, Hamza},
  doi          = {10.1007/s00521-025-11103-x},
  journal      = {Neural Computing and Applications},
  month        = {6},
  number       = {16},
  pages        = {9807-9819},
  shortjournal = {Neural Comput. Appl.},
  title        = {BERT-based keyword extraction model for the turkish language},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Self-CephaloNet: A two-stage novel framework using operational neural network for cephalometric analysis. <em>NCA</em>, <em>37</em>(16), 9777-9805. (<a href='https://doi.org/10.1007/s00521-025-11097-6'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cephalometric analysis is essential for the diagnosis and treatment planning of orthodontics. In lateral cephalograms, however, the manual detection of anatomical landmarks is a time-consuming procedure. Deep learning solutions hold the potential to address the time constraints associated with certain tasks; however, concerns regarding their performances have been observed. To address this critical issue, we propose an end-to-end cascaded deep learning framework (Self-CephaloNet) for the task, which demonstrates benchmark performance over the ISBI 2015 dataset in predicting 19 cephalometric landmarks. Due to their adaptive nodal capabilities, Self-ONN (self-operational neural networks) demonstrates superior learning performance for complex feature spaces over conventional convolutional neural networks. To leverage this attribute, we introduce a novel self-bottleneck in the HRNetV2 (high-resolution network) backbone, which has exhibited benchmark performance on our landmark detection task. Our first-stage result surpasses previous studies, showcasing the efficacy of our singular end-to-end deep learning model, which achieves a remarkable 70.95% success rate in detecting cephalometric landmarks within a 2-mm range for the Test1 and Test2 datasets which are part of ISBI 2015 dataset. Moreover, the second stage significantly improves overall performance, yielding an impressive 82.25% average success rate for the datasets above within the same 2-mm distance. Furthermore, external validation has been conducted using the PKU cephalogram dataset. Our model demonstrates a commendable success rate of 75.95% within the 2-mm range.},
  archive      = {J_NCA},
  author       = {Sumon, Md. Shaheenur Islam and Islam, Khandaker Reajul and Hossain, Sakib Abrar and Rafique, Tanzila and Ghosh, Ranjit and Hassan, Gazi Shamim and Podder, Kanchon Kanti and Barhom, Noha and Tamimi, Faleh and Chowdhury, Muhammad E. H.},
  doi          = {10.1007/s00521-025-11097-6},
  journal      = {Neural Computing and Applications},
  month        = {6},
  number       = {16},
  pages        = {9777-9805},
  shortjournal = {Neural Comput. Appl.},
  title        = {Self-CephaloNet: A two-stage novel framework using operational neural network for cephalometric analysis},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A novel inference system for detecting cardiac arrhythmia using deep learning framework. <em>NCA</em>, <em>37</em>(16), 9759-9775. (<a href='https://doi.org/10.1007/s00521-025-11092-x'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Bidirectional Long short-term memory (LSTM) units have recently emerged as a boon in the analysis of time-series data. LSTM are a type of Recurrent neural network. In this study, a new DeepBiLSTMnet architecture is proposed that facilitates detection of cardiac arrhythmia. Using this detected arrhythmia beats an inference engine is designed which predicts the severity of illness. The methodology begins with collection of ECG data from MIT-BIH database. The ECG data are then pre-processed. Then, the proposed DeepBiLSTMnet architecture is designed using a wavelet sequence layer and Bi-LSTM layer followed by classification layer with SoftMax activation. Bi-LSTM layer is designed by sequentially connecting 200 hidden Bi-LSTM units. The model is then trained with different network training parameter configurations. Investigating the obtained results by the process of training, the model with system's best training accuracy is selected and tested with test data. To update the weights and offsets, our model is tested with different optimizers. To develop a prototype NVIDIA Jetson Nano Developer Kit is used. Our proposed model gave a high recognition performance with a model accuracy of 99.59%. Other than overall model accuracy, various performance metrics like Precision, Specificity, Recall, F-score and Class accuracy for each class are also calculated. Furthermore, the Inference engine is designed using the obtained classification results to predicted the severity of illness and validated with real-time data.},
  archive      = {J_NCA},
  author       = {Mohebbanaaz and Sai, Y. Padma and Kumari, L. V. Rajani},
  doi          = {10.1007/s00521-025-11092-x},
  journal      = {Neural Computing and Applications},
  month        = {6},
  number       = {16},
  pages        = {9759-9775},
  shortjournal = {Neural Comput. Appl.},
  title        = {A novel inference system for detecting cardiac arrhythmia using deep learning framework},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A critical performance assessment of the machine learning algorithms for slope stability analysis. <em>NCA</em>, <em>37</em>(16), 9735-9757. (<a href='https://doi.org/10.1007/s00521-024-10471-0'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The stability of natural rock and soil slopes, mine tailing dumps, mine slopes, and embankments is one of the most challenging problems in geotechnical engineering. Due to variability in soil properties and natural conditions, advanced machine learning (ML) methods may be a better alternative to mechanistic-based methods for such problems. Identification of important parameters and patterns of the training data plays a decisive role in the accuracy and complexity of the ML predictive model. In this article, an extensive database of natural slopes and artificial slopes due to anthropogenic activities are considered and analyzed using several ensemble ML algorithms including hybrid models. Two evolutionary multi-objective optimization methods, the non-dominated sorting genetic algorithm (NSGA-II) and the strength Pareto evolutionary algorithm (SPEA-II), are coupled with learning algorithms: extreme learning machine (ELM) and artificial neural network (ANN). The key dominating parameters for slope failure, soil unit weight (γ), soil cohesion (c), internal friction angle (φ), slope angle (β), slope height (H), and pore pressure ratio (ru) are considered. The role of parametric variations in the database, performances of ML models, and identification of important input parameters are discussed. A ranking-based method is followed to rank different ML models. The proposed ensemble and hybrid models are found to be effective in developing the best classification model with the optimum number of input parameters for a small and biased dataset.},
  archive      = {J_NCA},
  author       = {Tiwari, Satyam and Das, Sarat Kumar and Mohanty, Madhumita},
  doi          = {10.1007/s00521-024-10471-0},
  journal      = {Neural Computing and Applications},
  month        = {6},
  number       = {16},
  pages        = {9735-9757},
  shortjournal = {Neural Comput. Appl.},
  title        = {A critical performance assessment of the machine learning algorithms for slope stability analysis},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Optimizing the performance of six-phase induction motor-powered electric vehicles with fuzzy-PID and DTC. <em>NCA</em>, <em>37</em>(16), 9721-9734. (<a href='https://doi.org/10.1007/s00521-024-10455-0'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Automotive manufacturers are working on electric vehicles in response to regulations aimed at lowering emissions and increasing fuel efficiency. The purpose of this article is to build and simulate an electric vehicle (EV) application that drives an electric car with a modified six-phase induction motor (MPPIM). The effects of a voltage vector modulation on stator flux and torque changes are illustrated through an investigation of the fundamentals of direct torque control (DTC) for induction machines. Rewinding the stator to function as an MPPIM enhances torque pulsation and dependability of the three-phase asynchronous motor. The proposed DTC scheme is designed to enhance dynamic response and efficiency, making it suitable for the demanding requirements of EV propulsion systems. It is advised to use a fuzzy control strategy to modify the PID controller’s settings to maintain the EV speed at the desired reference speed. The EVs are driven by an FPID that optimizes a DTC linked with a space vector modulation using MATLAB/SIMULINK software to control the speed of the Electric Vehicle. According to simulation results, the suggested control strategy successfully lowers voltage and current total harmonic distortion, enhances the system’s ability to detect changes in the reference speed, and lowers errors in electric flux and Electric Vehicle speed. Overall, the results show that DTC control of six-phase induction motors offers an effective and practical solution for contemporary electric vehicles, opening the door for developments in electric drive technology and helping to create more environmentally friendly transportation networks.},
  archive      = {J_NCA},
  author       = {Abdelwanis, Mohamed I.},
  doi          = {10.1007/s00521-024-10455-0},
  journal      = {Neural Computing and Applications},
  month        = {6},
  number       = {16},
  pages        = {9721-9734},
  shortjournal = {Neural Comput. Appl.},
  title        = {Optimizing the performance of six-phase induction motor-powered electric vehicles with fuzzy-PID and DTC},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). CoNet: A lightweight color classification architecture using residual connection and MBConv. <em>NCA</em>, <em>37</em>(16), 9705-9720. (<a href='https://doi.org/10.1007/s00521-025-11106-8'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Color classification is a critical task across various industries. Deep learning color classification methods are accurate but have high computational complexity, which limits their use in real-world applications. We presented a pipeline employing a lightweight classification architecture to tackle this issue effectively. The tailored architecture was developed based on the EfficientNet-B0 model, integrating innovative residual connections to improve the extraction of color features. This approach resulted in an exceptionally lightweight model with only 1.90 million parameters and a Giga Floating Point Operations per Second (GFLOPs) count of 0.03 while still delivering outstanding at 94.96% and 78.06% for public vehicle and avocado datasets. We also introduced a new benchmark dataset specifically for Vietnamese motorbike color classification and achieved an accuracy of 91.36%. Our code can be accessed at https://github.com/taipt03/MoCo/tree/main .},
  archive      = {J_NCA},
  author       = {Bui, Tien Dung and Pham, Tuan Tai and Dang, Tuan Linh},
  doi          = {10.1007/s00521-025-11106-8},
  journal      = {Neural Computing and Applications},
  month        = {6},
  number       = {16},
  pages        = {9705-9720},
  shortjournal = {Neural Comput. Appl.},
  title        = {CoNet: A lightweight color classification architecture using residual connection and MBConv},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). MCI-GAN: A novel GAN with identity blocks inspired by menstrual cycle behavior for missing pixel imputation. <em>NCA</em>, <em>37</em>(16), 9669-9703. (<a href='https://doi.org/10.1007/s00521-025-11059-y'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper presents MCI-GAN, a novel menstrual cycle imputation (MCI) and generative adversarial network (GAN) framework designed to address the challenge of missing pixel imputation in medical images. Inspired by the intelligent behavior of the endometrial lining during the menstrual cycle, our method introduces four key innovations. First, we propose a novel metaheuristic algorithm that assigns weights to surround pixels based on menstrual cycle behavior, ensuring that the imputed pixels maintain structural integrity and coherence with their neighbors, thus preserving overall image quality. Second, to enhance the learning capability of the GAN, identity blocks are integrated into the network architecture, improving the network’s ability to capture complex spatial relationships and leading to more accurate and consistent imputation of missing pixels. Third, we introduce an adaptive loss function that dynamically adjusts the penalty for pixel discrepancies based on local image context, allowing the model to focus on areas where accurate imputation is most critical and thereby enhancing overall image fidelity. Fourth, the framework incorporates a multi-scale feature extraction mechanism, enabling the GAN to process and combine information at various levels of detail, ensuring that both fine-grained textures and larger structural patterns are accurately captured during the imputation process. The efficacy of MCI-GAN is demonstrated across three diverse medical imaging datasets: mammograms, magnetic resonance imaging (MRI) scans, and skin lesion images. Our results show that the proposed method significantly outperforms existing approaches in terms of accuracy and structural coherence, offering a robust solution for missing pixel imputation in medical imaging.},
  archive      = {J_NCA},
  author       = {Marie, Hanaa Salem and Elbaz, Mostafa},
  doi          = {10.1007/s00521-025-11059-y},
  journal      = {Neural Computing and Applications},
  month        = {6},
  number       = {16},
  pages        = {9669-9703},
  shortjournal = {Neural Comput. Appl.},
  title        = {MCI-GAN: A novel GAN with identity blocks inspired by menstrual cycle behavior for missing pixel imputation},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Stepwise monogamous pairing genetic algorithm method applied to a multi-depot vehicle routing problem with time windows. <em>NCA</em>, <em>37</em>(16), 9639-9668. (<a href='https://doi.org/10.1007/s00521-025-11048-1'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Evolutionary computation (EC) is a powerful tool for global optimization across various domains, including healthcare logistics. However, the prevalence of local optima in complex problems often hinders the ability of the populations to find optimal solutions, leading to premature convergence. More daunting is that complex optimization problems motivated from real-world challenges continue to pose huge hurdles for EC, with many remaining unresolved. To fill this gap, we propose a new framework named SW-MopGA (Stepwise Monogamous Pairing Genetic Algorithm), which hinges on two innovations. First, a controller for diversity maintenance is introduced into MopGA to generate perturbed opposite solution via opposition-learning when necessary. In doing so, the algorithm is able to maintain population diversity effectively, which alleviates the risk of premature convergence. Second, SW-MopGA proposes a two-step framework: Initially, the population undergoes evolution on a simplified version of the originally complex problem. Solutions derived from the simplified problem serve as building blocks (stepping stones) upon which more intricate blocks are constructed for the final problem—a form of incremental evolution. Numerical analysis on two real-world cases in the logistics healthcare sector demonstrates the effectiveness of SW-MopGA. The problems are formulated as a multi-depot vehicle routing problem with time windows (MDVRPTW). Overall, the combined effect of the two innovations resulted in up to 18% and 8% savings in terms of total travel time and distance, and vehicles used, respectively on an off-peak day scenario; while approximately 17% and 7% savings for the same criteria on a peak-day scenario, compared to the baseline MopGA. Additionally, SW-MopGA improved the best-known solution of three instances in the MDVRPTW benchmark dataset. Finally, algorithmic insights are provided. In general, our initial study shows encouraging outcomes for incremental evolution and its potential for other real-world complex optimization models.},
  archive      = {J_NCA},
  author       = {Lim, Ting Yee and Ng, Xin Ju and Tan, Choo Jun and Lim, Chee Peng},
  doi          = {10.1007/s00521-025-11048-1},
  journal      = {Neural Computing and Applications},
  month        = {6},
  number       = {16},
  pages        = {9639-9668},
  shortjournal = {Neural Comput. Appl.},
  title        = {Stepwise monogamous pairing genetic algorithm method applied to a multi-depot vehicle routing problem with time windows},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Spatiotemporal pattern detection, generation, and computation with circuits. <em>NCA</em>, <em>37</em>(16), 9621-9637. (<a href='https://doi.org/10.1007/s00521-025-11046-3'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Implementations of neurons, delays, and synapse circuits are presented with simulations. These neural elements are used to create two small spiking neural networks, the Rate-Window and Order-Biased clusters, which are capable of detecting simple two-spike spatiotemporal patterns. A simple pattern detecting network (SPDN) is created by combining the Rate-Window and Order-Biased clusters, where clusters are small spiking neural networks, and its simple pattern detection ability is demonstrated in simulation. The SPDN is used to implement a complex pattern detecting network (CPDN) and its complex pattern detection ability is demonstrated in simulation. Methods for generating arbitrary spatiotemporal patterns are presented. The CPDN and spatiotemporal pattern generation methods are then used to implement a novel spatiotemporal computing paradigm based on detecting and responding to spatiotemporal symbols. A simulation of a spatiotemporal half adder is presented to demonstrate the computing paradigm.},
  archive      = {J_NCA},
  author       = {Ivans, Robert C. and Cantley, Kurtis D.},
  doi          = {10.1007/s00521-025-11046-3},
  journal      = {Neural Computing and Applications},
  month        = {6},
  number       = {16},
  pages        = {9621-9637},
  shortjournal = {Neural Comput. Appl.},
  title        = {Spatiotemporal pattern detection, generation, and computation with circuits},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Modeling on magnetohydrodynamic stokes flow using machine learning and curve fitting. <em>NCA</em>, <em>37</em>(16), 9603-9619. (<a href='https://doi.org/10.1007/s00521-025-11088-7'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this study, neural network (NN) and curve fitting modeling of fluid flow characteristics of the magnetohydrodynamic (MHD) Stokes flow in a lid-driven cavity are utilized. Firstly, the MHD Stokes flow equations are numerically solved by the method of approximate particular solution for the variations of Hartmann number $$M\in [1,120]$$ and the inclination angle $$a \in [0, \pi ]$$ . The essential data for modeling are extracted from the numerical results. The inputs are M and a, and the outputs are the infinity norm of stream function $$\psi$$ , v velocity component, vorticity $$\omega$$ and the minimum value of u velocity. In modeling of these outputs, the distinct curve fitting functions are examined. NN is employed for different layer numbers and data partitions. It is obtained that the increase in the number of the hidden layers gives less error and locally weighted quadratic regression fit captures the best behavior in curve fitting. The usage of modeling allows us to be independent from the repeated numerical calculations. The capability of trilayer NN for modeling $$\psi ,u,v,\omega$$ in the entire region is also shown.},
  archive      = {J_NCA},
  author       = {Gurbuz-Caldag, Merve and Pekmen, Bengisen},
  doi          = {10.1007/s00521-025-11088-7},
  journal      = {Neural Computing and Applications},
  month        = {6},
  number       = {16},
  pages        = {9603-9619},
  shortjournal = {Neural Comput. Appl.},
  title        = {Modeling on magnetohydrodynamic stokes flow using machine learning and curve fitting},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Risk assessment of young driver behavior using an extended decision-making approach based on FMEA in uncertain environments. <em>NCA</em>, <em>37</em>(16), 9565-9602. (<a href='https://doi.org/10.1007/s00521-025-11087-8'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Drivers’ behavior is one of the most important factors affecting road transportation safety. In particular, studying this issue in relation to young people aged 25 and below becomes more sensitive because they are not experienced and there are some age-related elements. Moreover, a significant percentage of beginner drivers fall into this age category, which can lead to risky behavior. Overconfidence, indiscipline, careless driving, or speeding tendencies may contribute greatly to their vulnerability to hazards on the roads. Hence, there is a need for further research to establish the constraints and possible risks involved with young drivers to improve road safety. Hence, this study aims to analyze the potential hazards associated with youth driving behavior in order to facilitate the development of relevant remedies through a thorough understanding of their behavior for safe transportation on the roads. To achieve this goal, a multi-criteria decision-making approach has been used. The proposed approach uses measurement of options and ranking based on the compromise solution method in an intuitive fuzzy environment to evaluate and rank risks. In addition, through consultation with experts and experienced technicians, a selection of 17 potential hazards were identified from existing risk factors. These risks are classified into three groups: working on the phone, distractions, and non-compliance. The present stud shows that risky driving and driving in reverse represent the highest level of risk, while speeding represents the lowest level of risk among young drivers.},
  archive      = {J_NCA},
  author       = {Jafarzadeh Ghoushchi, Saeid and Shaffiee Haghshenas, Sami and Vahabzadeh, Sahand and Shaffiee Haghshenas, Sina and Astarita, Vittorio and Guido, Giuseppe},
  doi          = {10.1007/s00521-025-11087-8},
  journal      = {Neural Computing and Applications},
  month        = {6},
  number       = {16},
  pages        = {9565-9602},
  shortjournal = {Neural Comput. Appl.},
  title        = {Risk assessment of young driver behavior using an extended decision-making approach based on FMEA in uncertain environments},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Convolutional variational autoencoder for northeast US coastal wind and flood hazard data augmentation. <em>NCA</em>, <em>37</em>(16), 9537-9564. (<a href='https://doi.org/10.1007/s00521-025-11085-w'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The coasts of the Northeastern United States experience wind and flood damage as a result of extratropical cyclones (such as Nor’easters). However, recorded data is limited for hazard analysis and resilience evaluation. This paper describes a method that can efficiently augment the time series of extratropical cyclone severity measures (wind speed and flood elevation) by leveraging convolutional variational autoencoders, a deep learning technique. As a generative model, the proposed convolutional variational autoencoder learns the probability distribution of latent features (here, low-dimensional underlying characteristics of the time series) as a multivariate normal distribution. The augmented severity measure time series are then obtained in two steps: (1) randomly sampling data from the learned multivariate normal distribution and (2) using the randomly sampled data as the input to the decoder of the trained convolutional variational autoencoder to generate severity measures. A case study is conducted to evaluate the quality of the augmented wind speed and flood elevation time series of the extratropical cyclones recorded in Boston Harbor (adjacent to the city of Boston, Massachusetts). This study also demonstrates that the proposed method outperforms other established time series data augmentation methods (i.e., Dynamic Time Warping and Conditional Generative Adversarial Network). An application for hazard frequency analysis—modeling the joint probability of wind speed and flood elevation using a Gumbel copula—is also presented. The results demonstrate that the augmented data can significantly reduce the uncertainty of the estimated copula parameter while obtaining a goodness-of-fit metric value (negative log-likelihood) similar to that of the original, non-augmented data. The proposed convolutional variational autoencoder can be used to augment any time series data. The user-friendly codes are published.},
  archive      = {J_NCA},
  author       = {Jia, Yiming and Sasani, Mehrdad},
  doi          = {10.1007/s00521-025-11085-w},
  journal      = {Neural Computing and Applications},
  month        = {6},
  number       = {16},
  pages        = {9537-9564},
  shortjournal = {Neural Comput. Appl.},
  title        = {Convolutional variational autoencoder for northeast US coastal wind and flood hazard data augmentation},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A systematic review of low-rank and local low-rank matrix approximation in big data medical imaging. <em>NCA</em>, <em>37</em>(16), 9481-9536. (<a href='https://doi.org/10.1007/s00521-025-11055-2'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The large volume and complexity of medical imaging datasets pose significant challenges for storage, transmission, and processing. To address these issues, low-rank matrix approximation (LORMA) and its derivative, local LORMA (LLORMA), have shown promising potential. This paper presents a comprehensive literature review of the application of LORMA and LLORMA across various imaging modalities and examines the challenges and limitations of existing methods. Notably, since 2015, there has been a significant shift toward a preference for LLORMA in the medical imaging field, demonstrating its effectiveness in capturing complex structures in medical data compared to LORMA. Given the limitations of shallow similarity methods in LLORMA, we propose incorporating advanced semantic image segmentation to improve the accuracy of similarity measurement. We further explain how this approach can be utilized to identify similar patches and assess its feasibility in medical imaging applications. We observe that LORMA and LLORMA have primarily been applied to unstructured medical data, and we suggest extending their use to other types of medical data, including structured and semi-structured formats. This paper also explores how LORMA and LLORMA can be adapted for regular data with missing entries, considering the impact of inaccuracies in predicting these missing values and their consequences. In addition, we examine the effect of patch size and suggest using random search (RS) to identify the optimal patch size. To further enhance feasibility, we propose a hybrid approach combining Bayesian optimization and RS, which could improve the application of LORMA and LLORMA in medical imaging.},
  archive      = {J_NCA},
  author       = {Hamlomo, Sisipho and Atemkeng, Marcellin and Brima, Yusuf and Nunhokee, Chuneeta and Baxter, Jeremy},
  doi          = {10.1007/s00521-025-11055-2},
  journal      = {Neural Computing and Applications},
  month        = {6},
  number       = {16},
  pages        = {9481-9536},
  shortjournal = {Neural Comput. Appl.},
  title        = {A systematic review of low-rank and local low-rank matrix approximation in big data medical imaging},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Policy-based optimization for drag reduction via spanwise wall oscillations. <em>NCA</em>, <em>37</em>(16), 9451-9479. (<a href='https://doi.org/10.1007/s00521-025-11067-y'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This study introduces a novel computational framework that synergistically integrates policy-based optimization (PBO) and implicit large eddy simulations (LES) to optimize oscillating spanwise wall motions for drag reduction in turbulent channel flows. Drag reduction remains a critical challenge in fluid dynamics, particularly in the context of improving energy efficiency in various engineering systems. Previous investigations aiming to determine optimal spanwise wall oscillation (SWO) parameters have predominantly relied on computationally expensive methods, such as parametric grid search using direct numerical simulations. Consequently, these studies have been limited in their exploration of the parameter space due to the associated high computational costs. The primary contribution of the present study lies in presenting a significant advancement through a more efficient and cost-effective exploration of the SWO parameter space, without compromising the precision of the simulations. By leveraging machine learning techniques, specifically PBO, the proposed framework enables rapid convergence to optimal control strategies while minimizing the number of simulations required. This approach marks a departure from the conventional a priori parameter selection based on grid search methods. Beyond this, the study also demonstrates the effectiveness of PBO for parametric optimization in turbulent flow control applications and corroborates the previously identified optimal drag reduction parameters while providing deeper insights into their precise optimal ranges and sensitivities. The optimization process is conducted in two stages: initially focusing solely on maximizing drag reduction, followed by the incorporation of control costs to optimize the net energy balance of the system. The optimal oscillation parameters obtained for drag reduction closely align with those reported in the literature, with the optimal period $$T^+_{\text {opti}}$$ falling within the range [110, 125] and the optimal amplitude $$W^+_{\text {opti}}$$ being the highest within the investigated range. Regarding net energy balance, the present study offers additional precision compared to previous works, identifying the optimum at $$T^+_{\text {opti}} \in [125, 155]$$ and $$W^+_{\text {opti}} \in [3.5, 5]$$ . These results reveal that higher amplitudes fail to offset control costs, while the optimal period slightly increases when accounting for the required energy, consistent with the expected decrease in operational costs. The proposed PBO-LES framework establishes a new paradigm for leveraging machine learning to identify optimal control strategies for real-world applications, demonstrating its potential to revolutionize the design of efficient flow control techniques and contribute to the development of sustainable, energy-efficient technologies across various industrial sectors.},
  archive      = {J_NCA},
  author       = {Guérin, Lou and Cordier, Laurent and Flageul, Cédric and Grieu, Stéphane and Agostini, Lionel},
  doi          = {10.1007/s00521-025-11067-y},
  journal      = {Neural Computing and Applications},
  month        = {6},
  number       = {16},
  pages        = {9451-9479},
  shortjournal = {Neural Comput. Appl.},
  title        = {Policy-based optimization for drag reduction via spanwise wall oscillations},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Neural networking-based decisions on Powell–Eyring fluid flow with identical stretched porous and free stream conditions. <em>NCA</em>, <em>37</em>(16), 9433-9450. (<a href='https://doi.org/10.1007/s00521-025-11062-3'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The present article contains a neural networking analysis of Powell–Eyring (PE) fluid flow directed toward a flat surface with suction and injection processes. Heat and mass transfer traits are well-thought-out simultaneously with free stream effect. Flow is formulated in terms of partial differential equations, and the theory of Lie symmetry groups is used to construct the particular set of transformations for reduction of order. The reduced equations are transformed into a system of first-order ordinary differential equations as an initial value problem. The shooting methodology is adopted to report the thermal flow field by evaluating the Nusselt number and temperature. The 4 × 90 matrix is constructed for the Prandtl number, PE fluid parameter, suction parameter and temperature power law index, while the 1 × 90 matrix is developed for the Nusselt number. The 75% (67), 10% (9), and 15% (14) are slotted for training, validation, and testing, respectively, and in hidden layer 15 neurons are used. Levenberg–Marquardt backpropagation is used to train the neural networking model. Regression and mean square analysis are carried out to measure the accuracy of the constructed neural network for Nusselt number. Following the predicted values, the Nusselt number admits inciting values toward the temperature power law index, Prandtl number, and suction parameter while contradictory is the case for the PE fluid parameter.},
  archive      = {J_NCA},
  author       = {Rehman, Khalil Ur and Shatanawi, Wasfi and Yian, Lok Yian},
  doi          = {10.1007/s00521-025-11062-3},
  journal      = {Neural Computing and Applications},
  month        = {6},
  number       = {16},
  pages        = {9433-9450},
  shortjournal = {Neural Comput. Appl.},
  title        = {Neural networking-based decisions on Powell–Eyring fluid flow with identical stretched porous and free stream conditions},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Improving graph collaborative filtering with network motifs. <em>NCA</em>, <em>37</em>(16), 9413-9432. (<a href='https://doi.org/10.1007/s00521-025-11079-8'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep learning on graphs, specifically graph convolutional networks (GCNs), has exhibited exceptional efficacy in the domain of recommender systems. Most GCNs have a message-passing architecture that enables nodes to aggregate information from neighbours iteratively through multiple layers. This enables GCNs to learn from higher-order information, but the model does not allow for direct captions of the local structural patterns. Our rationale is to investigate the effectiveness of capturing such local patterns for graph-based collaborative filtering to enhance model’s learning ability per layer. This technique combines lower-order and higher-order interactions during layer-wise propagation. In this paper, we propose MotifGCN to aggregate both lower-order and higher-order information in each graph convolution layer. Specifically, we develop dedicated algorithms of generating motif adjacency matrices. The matrices are then used for motif-enhanced neighbourhood aggregation in each layer. As this paper focuses on recommender systems, MotifGCN is built on the basis of bipartite graphs. Our experiments on four real-world datasets show that MotifGCN has a superior performance compared to various state-of-the-art methods.},
  archive      = {J_NCA},
  author       = {Zhang, Yuqi and Yu, Jian and Liu, Zhizhong and Wang, Guiling and Nguyen, Minh and Sheng, Quan Z. and Wang, Nancy},
  doi          = {10.1007/s00521-025-11079-8},
  journal      = {Neural Computing and Applications},
  month        = {6},
  number       = {16},
  pages        = {9413-9432},
  shortjournal = {Neural Comput. Appl.},
  title        = {Improving graph collaborative filtering with network motifs},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Evaluating the impact of music tempo on drivers and their performance using an artificial intelligence model: A multi-source data approach. <em>NCA</em>, <em>37</em>(16), 9401-9412. (<a href='https://doi.org/10.1007/s00521-025-11077-w'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Traffic accidents are a major global health and economic concern. As such, research into understanding driving behaviors becomes essential to minimize the associated risks. Among various factors that can influence driving behaviors, listening to music while driving is a complex task that needs investigation. Music can enhance arousal and manage stress, which could potentially improve one’s driving performance. Listening to music, however, also competes for cognitive resources, increasing one’s mental workload and potentially degrading the driving ability. This study investigates the impact of listening to music with different tempos on drivers and their performance using an Artificial Intelligence (AI) approach. A total of 26 participants are subjected to three driving scenarios in a unique simulated experiment, while utilizing a motion platform. The conditions are driving while listening to slow-tempo music, and fast-tempo music, and with no music. A dataset is created by collecting data through Tobii eye-tracking glasses, Equivital sensor belts, and a software tool. This dataset is preprocessed and used to train a convolutional neural network-long short-term memory (CNN-LSTM) model. This model’s performance is optimized through hyperparameter tuning and Chi-squared feature selection, in order to maximize accuracy and minimize computation time. The model performance is compared with those from a densely layered deep learning model and several classical machine learning models. The devised CNN-LSTM model outperforms other machine learning models, achieving an average accuracy rate of 99.29% with minimal variance across multiple evaluations, demonstrating its effectiveness and consistency in classifying drivers’ behaviors under varying auditory conditions.},
  archive      = {J_NCA},
  author       = {Shajari, Arian and Asadi, Houshyar and Alsanwy, Shehab and Nahavandi, Saeid and Lim, Chee Peng},
  doi          = {10.1007/s00521-025-11077-w},
  journal      = {Neural Computing and Applications},
  month        = {6},
  number       = {16},
  pages        = {9401-9412},
  shortjournal = {Neural Comput. Appl.},
  title        = {Evaluating the impact of music tempo on drivers and their performance using an artificial intelligence model: A multi-source data approach},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). The firefighter algorithm for optimization problems. <em>NCA</em>, <em>37</em>(16), 9345-9400. (<a href='https://doi.org/10.1007/s00521-025-11074-z'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper presents the Firefighter Optimization (FFO) algorithm as a new metaheuristic for optimization problems that stems inspiration from the collaborative strategies often deployed by firefighters in firefighting activities. Such strategies include adaptive response to changing conditions, coordination among multiple firefighters (i.e., agents) to converge on a common goal, balancing exploration and exploitation by maintaining diversity within the search space and adapting its parameters to navigate complex landscapes. To evaluate the performance of FFO, extensive experiments were conducted, wherein the FFO was examined against 13 commonly used optimization algorithms, namely, the Ant Colony Optimization (ACO), Bat Algorithm (BA), Biogeography-Based Optimization (BBO), Flower Pollination Algorithm (FPA), Genetic Algorithm (GA), Grey Wolf Optimizer (GWO), Harmony Search (HS), Particle Swarm Optimization (PSO), Simulated Annealing (SA), Tabu Search (TS), and Whale Optimization Algorithm (WOA), and across 24 benchmark functions, as well as 10 standard functions and 4 real engineering problems from the CEC 2020 suite. The results demonstrate that FFO achieves comparative performance and, in some scenarios, outperforms commonly adopted optimization algorithms in terms of the obtained fitness, time taken for exaction, and research space covered per unit of time. More specifically, FFO ranked first in the Distance per Unit Time metric and maintained a top 5 performance in higher dimensions (i.e., 20D and 50D).},
  archive      = {J_NCA},
  author       = {Naser, M. Z. and Naser, Ahmad Z.},
  doi          = {10.1007/s00521-025-11074-z},
  journal      = {Neural Computing and Applications},
  month        = {6},
  number       = {16},
  pages        = {9345-9400},
  shortjournal = {Neural Comput. Appl.},
  title        = {The firefighter algorithm for optimization problems},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Correction: Exploring biologically inspired mechanisms of adversarial robustness. <em>NCA</em>, <em>37</em>(16), 9343-9344. (<a href='https://doi.org/10.1007/s00521-025-11175-9'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_NCA},
  author       = {Holzhausen, Konstantin and Merlid, Mia and Torvik, Håkon Olav and Malthe-Sørenssen, Anders and Lepperød, Mikkel Elle},
  doi          = {10.1007/s00521-025-11175-9},
  journal      = {Neural Computing and Applications},
  month        = {6},
  number       = {16},
  pages        = {9343-9344},
  shortjournal = {Neural Comput. Appl.},
  title        = {Correction: Exploring biologically inspired mechanisms of adversarial robustness},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Exploring biologically inspired mechanisms of adversarial robustness. <em>NCA</em>, <em>37</em>(16), 9331-9342. (<a href='https://doi.org/10.1007/s00521-025-11019-6'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Backpropagation-optimized artificial neural networks, while precise, lack robustness, leading to unforeseen behaviors that affect their safety. Biological neural systems do solve some of these issues already. Unlike artificial models, biological neurons adjust connectivity based on neighboring cell activity. Understanding the biological mechanisms of robustness can pave the way toward building trustworthy and safe systems. Robustness in neural representations is hypothesized to correlate with the smoothness of the encoding manifold. Recent work suggests power law covariance spectra, which were observed studying the primary visual cortex of mice, to be indicative of a balanced trade-off between accuracy and robustness in representations. Here, we show that unsupervised local learning models with winner takes all dynamics learn such power law representations, providing upcoming studies a mechanistic model with that characteristic. Our research aims to understand the interplay between geometry, spectral properties, robustness, and expressivity in neural representations. Hence, we study the link between representation smoothness and spectrum by using weight, Jacobian and spectral regularization while assessing performance and adversarial robustness. Our work serves as a foundation for future research into the mechanisms underlying power law spectra and optimally smooth encodings in both biological and artificial systems. The insights gained may elucidate the mechanisms that realize robust neural networks in mammalian brains and inform the development of more stable and reliable artificial systems.},
  archive      = {J_NCA},
  author       = {Holzhausen, Konstantin and Merlid, Mia and Torvik, Håkon Olav and Malthe-Sørenssen, Anders and Lepperød, Mikkel Elle},
  doi          = {10.1007/s00521-025-11019-6},
  journal      = {Neural Computing and Applications},
  month        = {6},
  number       = {16},
  pages        = {9331-9342},
  shortjournal = {Neural Comput. Appl.},
  title        = {Exploring biologically inspired mechanisms of adversarial robustness},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Deep learning and vision transformers-based framework for breast cancer and subtype identification. <em>NCA</em>, <em>37</em>(16), 9311-9330. (<a href='https://doi.org/10.1007/s00521-025-10984-2'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Breast cancer, marked by uncontrolled cell growth in breast tissue, is the most common cancer among women and a second-leading cause of cancer-related deaths. Among its types, ductal and lobular carcinomas are the most prevalent, with invasive ductal carcinoma accounting for about 70–80% of cases and invasive lobular carcinoma for about 10–15%. Accurate identification is crucial for effective treatment but can be time-consuming and prone to interobserver variability. AI can rapidly analyze pathological images, providing precise, cost-effective identification, thus reducing the pathologists’ workload. This study utilizes a deep learning framework for advanced, automatic breast cancer detection and subtype identification. The framework comprises three key components: detecting cancerous patches, identifying cancer subtypes (ductal and lobular carcinoma), and predicting patient-level outcomes from whole slide images (WSI). The validation process includes visualization using Score-CAM to highlight cancer-affected areas prominently. Datasets include 111 WSIs (85 malignant from the Warwick HER2 dataset and 26 benign from pathologists). For subtype detection, there are 57 ductal and 8 lobular carcinoma cases. A total of 28,428 annotated patches were reviewed by two expert pathologists. Four pre-trained models—DenseNet-201, MobileNetV2, an ensemble of these two, and a Vision Transformer-based model—were fine-tuned and tested on the patches. Patient-level results were predicted using a majority voting technique based on the percentage of each patch type in the WSI. The Vision Transformer-based model outperformed other models in patch classification, achieving an accuracy of 96.74% for cancerous patch detection and 89.78% for cancer subtype classification. For WSI-based cancer classification, the majority voting method attained an F1-score of 99.06 and 96.13% for WSI-based cancer subtype classification. The proposed deep learning-based framework for advanced breast cancer detection and subtype identification yielded promising results. This advanced framework shows great promise in medical practice, offering an economical, efficient solution for generating accurate, clinically relevant results and enhancing diagnostic accuracy in hospitals, research centers, and pathology laboratories. Nonetheless, further studies are needed to validate its effectiveness across various environments and larger datasets.},
  archive      = {J_NCA},
  author       = {Jahan, Ishrat and Chowdhury, Muhammad E. H. and Vranic, Semir and Al Saady, Rafif Mahmood and Kabir, Saidul and Pranto, Zahid Hasan and Mim, Sabiha Jahan and Nobi, Sadia Farhana},
  doi          = {10.1007/s00521-025-10984-2},
  journal      = {Neural Computing and Applications},
  month        = {6},
  number       = {16},
  pages        = {9311-9330},
  shortjournal = {Neural Comput. Appl.},
  title        = {Deep learning and vision transformers-based framework for breast cancer and subtype identification},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Correction: Automated explainable and interpretable framework for anomaly detection and human activity recognition in smart homes. <em>NCA</em>, <em>37</em>(16), 9309-9310. (<a href='https://doi.org/10.1007/s00521-025-11188-4'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_NCA},
  author       = {Kim, Tae Hoon and Ojo, Stephen and Krichen, Moez and Alamro, Meznah A. and Mihoub, Alaeddine and Sampedro, Gabriel Avelino},
  doi          = {10.1007/s00521-025-11188-4},
  journal      = {Neural Computing and Applications},
  month        = {6},
  number       = {16},
  pages        = {9309-9310},
  shortjournal = {Neural Comput. Appl.},
  title        = {Correction: Automated explainable and interpretable framework for anomaly detection and human activity recognition in smart homes},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Automated explainable and interpretable framework for anomaly detection and human activity recognition in smart homes. <em>NCA</em>, <em>37</em>(16), 9295-9308. (<a href='https://doi.org/10.1007/s00521-025-10991-3'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper presents a comprehensive framework for activity recognition and anomaly detection in smart home environments, targeting applications in convenience, efficiency, responsiveness, and healthcare. The proposed framework incorporates explainable artificial intelligence (XAI) to interpret feature impacts on learning models and optimize feature selection for activity recognition and anomaly detection. The study examines the recognition of two activity sets: Activities Set I, comprising 8 activities, and Activities Set II, comprising 11 activities. Various machine learning models (random forest (RF), decision tree (DT), and XGBoost (XGB)) are employed, along with an ensemble voting classifier (EVC) with these machine learning. EVC achieved 91% accuracy for Activities Set I and 90–91% accuracy for Activities Set II. The model demonstrated strong precision, recall, and F1-score across most classes. For anomaly detection, the Isolation Forest and H2O Isolation Forest Estimator algorithms are utilized to uncover irregularities in daily routines. The Isolation Forest identified anomalies in activities such as chores and desk-related tasks, while the H2O Isolation Forest Estimator detected no anomalies in either activity set. This study underscores the potential of the proposed framework to enhance smart home functionality and highlights avenues for future research and system improvements.},
  archive      = {J_NCA},
  author       = {Kim, Tae Hoon and Ojo, Stephen and Krichen, Moez and Alamro, Meznah A. and Mihoub, Alaeddine and Sampedro, Gabriel Avelino},
  doi          = {10.1007/s00521-025-10991-3},
  journal      = {Neural Computing and Applications},
  month        = {6},
  number       = {16},
  pages        = {9295-9308},
  shortjournal = {Neural Comput. Appl.},
  title        = {Automated explainable and interpretable framework for anomaly detection and human activity recognition in smart homes},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A weather station selection method based on the simulated annealing algorithm for electric load forecasting. <em>NCA</em>, <em>37</em>(16), 9275-9294. (<a href='https://doi.org/10.1007/s00521-025-10988-y'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Temperature is a key factor in modeling electricity demand, making the selection of optimal weather stations essential for accurate predictions. However, current methods for selecting weather stations often rely on heuristic approaches that explore only a limited subset of potential combinations, potentially missing better solutions. In this paper, we propose an innovative approach that integrates the Simulated Annealing (SA) algorithm with local search techniques to improve forecast accuracy and reduce implementation time. Our method demonstrates superior performance in both quality and efficiency compared to existing approaches, as validated across three datasets, including data from a major distribution company in Iran and the Global Energy Forecasting Competitions of 2012 and 2014. Our results show that incorporating local search techniques reduces the Root Mean Square Error (RMSE) and Mean Absolute Percentage Error (MAPE) by 3.48% and 2.82%, respectively. Furthermore, the average implementation time of our SA algorithm is 36.52% lower than that of the existing metaheuristic algorithm.},
  archive      = {J_NCA},
  author       = {Salmabadi, Narjes and Salari, Majid and Shadman, Alireza},
  doi          = {10.1007/s00521-025-10988-y},
  journal      = {Neural Computing and Applications},
  month        = {6},
  number       = {16},
  pages        = {9275-9294},
  shortjournal = {Neural Comput. Appl.},
  title        = {A weather station selection method based on the simulated annealing algorithm for electric load forecasting},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Correction: An optimized deep focused U-net model for image segmentation. <em>NCA</em>, <em>37</em>(16), 9273-9274. (<a href='https://doi.org/10.1007/s00521-025-11158-w'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_NCA},
  author       = {Khan, Haroon Haider and Khan, Majid Iqbal},
  doi          = {10.1007/s00521-025-11158-w},
  journal      = {Neural Computing and Applications},
  month        = {6},
  number       = {16},
  pages        = {9273-9274},
  shortjournal = {Neural Comput. Appl.},
  title        = {Correction: An optimized deep focused U-net model for image segmentation},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). An optimized deep focused U-net model for image segmentation. <em>NCA</em>, <em>37</em>(16), 9245-9271. (<a href='https://doi.org/10.1007/s00521-024-10417-6'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Neural network-based segmentation methods are an important advancement in medical image analysis. Issues with class imbalance pose a significant challenge in medical segmentation, with lesions often occupying a considerably smaller volume (hard objects) relative to the background (easy objects). Medical image segmentation tasks require the model to pay attention on specific parts of the image rather than the entire image. Models based on CNN and its variants like U-Net have shown promising results in this regard. However, they often suffer from unstable gradient during the training process. We propose an optimized deep focused U-Net (DF U-Net) model along with a novel learnable optimized focal loss (LOF) function. The DF U-Net is integrated with a novel block that comprises of five subblocks: efficient channel attention, squeeze and excitation, stochastic depth, residual block and dense block, collectively known as (ESSRD) in the decoder layers which helps to address the unstable gradient and overfitting issues. The proposed LOF loss optimizes itself during the training of the model and converges to optimal values within few epochs. We evaluated DF U-Net using four medical imaging data sets: DRIVE, BUS2017, CVC-Clinic and Kvasir-SEG. During the training process, DF U-Net significantly reduced the convergence time by sharply minimizing the loss to 0.001% and achieved an accuracy of 99.5%. The experimental results show that DF U-Net helps to mitigate the unstable gradient issue while focusing on both easy and hard objects equally.},
  archive      = {J_NCA},
  author       = {Khan, Haroon Haider and Khan, Majid Iqbal},
  doi          = {10.1007/s00521-024-10417-6},
  journal      = {Neural Computing and Applications},
  month        = {6},
  number       = {16},
  pages        = {9245-9271},
  shortjournal = {Neural Comput. Appl.},
  title        = {An optimized deep focused U-net model for image segmentation},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Enhancing anomaly detection in WBANs using hybrid deep learning and optimization algorithms. <em>NCA</em>, <em>37</em>(15), 9223-9243. (<a href='https://doi.org/10.1007/s00521-025-11061-4'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Wireless Body Area Network (WBAN)-based healthcare surveillance systems have expanded rapidly due to recent advances in wireless sensor networks. A small sensor affixed to the human body is capable of measuring a range of psychological parameters. Noise, patient perspiration, and hardware misalignment are common causes of WBAN sensor failures. It is challenging for medical professionals to determine whether the information gathered by these sensors is inaccurate or impacted by malevolent attacks. The classification of physiological parameters as normal or abnormal can result from inaccurate data. This work presents a novel approach utilizing hybrid machine learning algorithms to analyse anomaly identification for wireless devices. The primary objective of this research is to identify anomalous values in the dataset using a hybrid Deep Convolutional Neural Network—Grasshopper Optimization Algorithm (DCNN-GOA). The predicted value of each anomalous record can be used to determine if the identified form is anomalous. The proposed DCNN-GOA algorithm is evaluated regarding accuracy, sensitivity, F1 score, and specificity. The experimental outcomes of the proposed hybrid DCNN-GOA algorithm outperform other existing algorithms in terms of precision (99.8%), F-score (97.9%), recall (99.9%), and accuracy (98.8%). The proposed DCNN-GOA protocol outperforms existing ANN, RNN, CNN-GRU, Conv GRU, Conv LSTM, and CNN-LSTM classifiers regarding accuracy, with 17.4%, 7.83%, 5.12%, 3.09%, and 1.85%, respectively.},
  archive      = {J_NCA},
  author       = {Thamaraimanalan, T. and Ramalingam, S.},
  doi          = {10.1007/s00521-025-11061-4},
  journal      = {Neural Computing and Applications},
  month        = {5},
  number       = {15},
  pages        = {9223-9243},
  shortjournal = {Neural Comput. Appl.},
  title        = {Enhancing anomaly detection in WBANs using hybrid deep learning and optimization algorithms},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Prediction of recurrent venous thromboembolism using a spatiotemporal phenomenological model and artificial neural network. <em>NCA</em>, <em>37</em>(15), 9193-9221. (<a href='https://doi.org/10.1007/s00521-025-11070-3'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Venous thromboembolism (VTE) is a major global cause of death, with recurrence risk rising after inadequate treatment. Existing VTE recurrence prediction models often overlook the phenomenological aspects of blood clot formation. This study aimed to predict Recurrent VTE (RVTE) by combining an artificial neural network (ANN) with a phenomenological clot formation model. A spatiotemporal model of clot formation was proposed and simulated using hematological data from 235 patients. Principal component analysis (PCA) and partial least squares (PLS) regression were applied to reduce the number of significant variables, and multiple ANN structures were trained for RVTE predictions. The dataset was initially divided into training and validation subsets to avoid data leakage. No data from the validation subset was used to build the model, ensuring the model's integrity. The validation conducted in this study corroborates the model's good generalizability. The spatiotemporal model effectively differentiated patients based on clot formation characteristics. The application of PCA and PLS reduced the original variables by over 99.99%. The best-performing model achieved, for unseen observations, an area under curve of 0.9689, accuracy of 0.9149, sensitivity of 1.0000, specificity of 0.8919, and F1-score of 0.8333, aligning with established literature values. This methodology outperformed nine other machine learning models commonly used for binary classification. A significant advantage of this approach is its reliance on only six clinical variables for prediction. This study demonstrates that mathematical methods can reveal insights into the spatiotemporal evolution of blood clots, presenting a promising avenue for RVTE prediction. The model could serve as a clinical decision-support system for RVTE treatment.},
  archive      = {J_NCA},
  author       = {Al Bannoud, Mohamad and Martins, Tiago Dias and de Lima Montalvão, Silmara Aparecida and Annichino-Bizzacchi, Joyce Maria and Filho, Rubens Maciel and Maciel, Maria Regina Wolf},
  doi          = {10.1007/s00521-025-11070-3},
  journal      = {Neural Computing and Applications},
  month        = {5},
  number       = {15},
  pages        = {9193-9221},
  shortjournal = {Neural Comput. Appl.},
  title        = {Prediction of recurrent venous thromboembolism using a spatiotemporal phenomenological model and artificial neural network},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Pedestrian mask-wearing detection based on YOLOv5 and DeepSORT. <em>NCA</em>, <em>37</em>(15), 9169-9192. (<a href='https://doi.org/10.1007/s00521-025-11076-x'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To address the challenge of accurately detecting whether people are wearing masks in video sequences, particularly in cases of occlusion or missed detection, we propose DTPM-YOLO (Detection and Tracking of Pedestrian Mask-wearing based on You Only Look Once). DTPM-YOLO introduces a cross-stage partial Bottleneck with 2 convolutions feature (CB2CF) into YOLOv5s to reduce the complexity of feature learning, and incorporates a convolutional block attention module (CBAM) to enhance detection capability, especially for small objects. Additionally, the inner-frame relationship module (IFRM) employs the Hungarian algorithm to establish associations among objects within an image, accurately identifying individuals who are wearing masks and those who are not. To address inconsistencies between the historical prediction direction of tracked targets and the direction of newly detected velocities, we integrate a directional difference factor into the association cost of Deep Simple Online and Real-time Tracking (DeepSORT). Our results demonstrate that DTPM-YOLO achieves a detection speed of 65 FPS, with an mAP@0.5 of 72.39%, which is 8.44% higher than the original YOLOv5. Compared to DeepSORT, our approach improved Multiple Object Tracking Accuracy (MOTA) by 18.0% and Multiple Object Tracking Precision (MOTP) by 3.40% on the MOT16 dataset, effectively enabling real-time mask-wearing detection and tracking.},
  archive      = {J_NCA},
  author       = {Wang, Shuai and Shibghatullah, Abdul Samad and Keoy, Kay Hooi and Iqbal, Javid},
  doi          = {10.1007/s00521-025-11076-x},
  journal      = {Neural Computing and Applications},
  month        = {5},
  number       = {15},
  pages        = {9169-9192},
  shortjournal = {Neural Comput. Appl.},
  title        = {Pedestrian mask-wearing detection based on YOLOv5 and DeepSORT},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Infinite impulse response based reliable fractional order digital differentiator identification using an evolutionary optimization approach. <em>NCA</em>, <em>37</em>(15), 9129-9167. (<a href='https://doi.org/10.1007/s00521-025-11073-0'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This research work presents the realization of a decisive, substantial, and wideband infinite impulse response-type fractional order digital differentiator (FODD) adopting a new nature-influenced meta-heuristic optimization algorithm, namely quasi-chaotic opposition-based mayfly algorithm (QCOMA). QCOMA substantially behaves better than some famous techniques like real-coded genetic algorithm (RGA), particle swarm optimization (PSO), differential evolution (DE), flower pollination algorithm (FPA), hybrid flower pollination algorithm (HFPA), and mayfly algorithm (MA) in terms of various magnitude error behaviors, interpretation aspect precision, convergence nature, and computational time needed to determine the optimal explanation. In this research proposal, the authors intensively analyze the convergence behavior and magnitude error performances of the various order FODD’s by RGA, PSO, DE, FPA, HFPA, MA, and QCOMA. MATLAB simulation outcomes also strongly justify the magnitude response efficiency and stability of the offered various order FODD’s over some currently published research articles.},
  archive      = {J_NCA},
  author       = {Dey, Souvik and Roy, Provas Kumar and Sarkar, Angsuman},
  doi          = {10.1007/s00521-025-11073-0},
  journal      = {Neural Computing and Applications},
  month        = {5},
  number       = {15},
  pages        = {9129-9167},
  shortjournal = {Neural Comput. Appl.},
  title        = {Infinite impulse response based reliable fractional order digital differentiator identification using an evolutionary optimization approach},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Application of spiking neural networks and traditional artificial neural networks for solar radiation forecasting in photovoltaic systems in arab countries. <em>NCA</em>, <em>37</em>(15), 9095-9127. (<a href='https://doi.org/10.1007/s00521-025-11066-z'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Incorporating photovoltaic (PV) systems into power grids is increasingly critical, especially in sun-rich Arab countries. This study introduces a novel approach using spiking neural networks (SNNs) for forecasting solar radiation. We utilize the Lava framework to develop and compare different neural architectures in both univariate and multivariate scenarios. Our findings reveal that SNNs achieve performance comparable to traditional artificial neural networks (ANNs) in modeling and forecasting. Specifically, the error rates of SNNs, when using a convolutional neural network (CNN) architecture on multivariate time series data, are similar to those of their ANN counterparts. However, SNNs present a significant advantage in power efficiency, being approximately 9 times more efficient when estimated theoretically on both the Loihi neuromorphic chip and traditional GPUs using the direct training approach with the SLAYER algorithm. Furthermore, the efficiency improvement is about 7 to 8 times on both Loihi and GPUs using the conversion training approach with a bootstrap algorithm. This research contributes to optimizing PV operations in Arab regions by introducing SNNs as a power-efficient alternative to traditional ANNs for solar radiation forecasting. It aligns with global sustainability goals by offering a robust model for efficiently predicting solar energy outputs while minimizing the computational energy footprint. Although the datasets in this study are drawn from sources in the Arab region, the methodologies and findings are applicable to other areas due to their universal relevance. The practical implications of our findings support the expansion of sustainable energy infrastructures, underscoring the strategic importance of innovative forecasting models in enhancing the reliability and efficiency of renewable energy sources.},
  archive      = {J_NCA},
  author       = {Ayasi, Bahgat and Vázquez, Iago X. and Saleh, Mohammed and Garcia-Vico, Angel M. and Carmona, Cristóbal J.},
  doi          = {10.1007/s00521-025-11066-z},
  journal      = {Neural Computing and Applications},
  month        = {5},
  number       = {15},
  pages        = {9095-9127},
  shortjournal = {Neural Comput. Appl.},
  title        = {Application of spiking neural networks and traditional artificial neural networks for solar radiation forecasting in photovoltaic systems in arab countries},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A new R&D-based algorithm for optimization of large-scale problems. <em>NCA</em>, <em>37</em>(15), 9063-9094. (<a href='https://doi.org/10.1007/s00521-025-11057-0'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Optimization has become a necessary part of any activities in our life. Thus, bunch of optimization algorithms have been introduced by researchers during the past decades. However, large-scale optimization problems are still challenging. In this research, inspired by research and development procedure, a new metaheuristic algorithm called Research and Development (R&D)-Based Algorithm (RDBA) has been proposed for optimization of large-scale problems. The mechanism of searching for the best result is based on four activities of “Learning,” doing a “Teamwork,” participating in the “Conference,” and “Self-study.” The conducted method is tested on 13 well-known benchmarks with dimensions ranging from 30 to 1000, and the results are compared with the previous studies working in this area. The simulations demonstrate that RDBA is much effective than 12 powerful algorithms in solving high-dimensional complicated functions regarding solution precision, stability, and convergence rate.},
  archive      = {J_NCA},
  author       = {Hajimiri, Hossein and Bagheri, Amir},
  doi          = {10.1007/s00521-025-11057-0},
  journal      = {Neural Computing and Applications},
  month        = {5},
  number       = {15},
  pages        = {9063-9094},
  shortjournal = {Neural Comput. Appl.},
  title        = {A new R&D-based algorithm for optimization of large-scale problems},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). From data to diagnosis: Evaluation of machine learning models in predicting kidney stones. <em>NCA</em>, <em>37</em>(15), 9049-9062. (<a href='https://doi.org/10.1007/s00521-025-11049-0'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Kidney stones are one of the diseases affecting a large part of the world’s population, and it is getting worse and worse with dietary changes, obesity, medical conditions etc. This disease causes high morbidity and has a high cost for patients’ health, as it can cause severe complications such as hydronephrosis, reduced kidney function and ultimately kidney failure. The aim of this research is to detect kidney stones using Machine Learning (ML) algorithms. For which, the performance of Logistic Regression (LR), K-nearest neighbors (KNN), Decision tree (DT), Support vector machines (SVM), Gaussian naive bayes (GNB) and Random forests (RF) models were evaluated. The results showed that the LR model obtained the best accuracy rate with 0.78 in detecting kidney stones, significantly outperforming the other models, such as KNN, DT, SVM, GNB and RF, which to some extent also achieved high accuracy rates of 0.61, 0.66, 0.68, 0.70 and 0.76, respectively. The result obtained by the LR model provides us with a more accessible approach to develop solutions in environments, where computational resources are limited. Also, the correlation of variables (gravity, pH, osmolality) highlights their relevance in kidney stone formation. This approach may help motivate future research on variable integration.},
  archive      = {J_NCA},
  author       = {Iparraguirre-Villanueva, Orlando and Paucar-Palomino, George and Paulino-Moreno, Cleoge},
  doi          = {10.1007/s00521-025-11049-0},
  journal      = {Neural Computing and Applications},
  month        = {5},
  number       = {15},
  pages        = {9049-9062},
  shortjournal = {Neural Comput. Appl.},
  title        = {From data to diagnosis: Evaluation of machine learning models in predicting kidney stones},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Enhanced early detection of ovarian cancer through deep learning and fuzzy rough sets. <em>NCA</em>, <em>37</em>(15), 9025-9047. (<a href='https://doi.org/10.1007/s00521-025-11051-6'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This study introduces an innovative approach for the early diagnosis of ovarian cancer by integrating fuzzy rough sets with deep learning techniques. Our methodology preprocesses medical data using fuzzy rough sets to address uncertainty and imprecision, thereby enhancing its suitability for deep learning models. Subsequently, we utilized advanced convolutional neural networks (CNNs) to achieve efficient and accurate diagnosis. The experimental findings reveal that our fuzzy rough convolutional neural network (FRCNN) model attains a perfect classification accuracy of 100%, along with a sensitivity of 1.00 and a specificity of 1.00. This performance markedly outperforms conventional diagnostic approaches and individual machine learning methods. These findings highlight the robustness and effectiveness of our approach in managing the complexities of medical data, resulting in superior diagnostic performance. Despite challenges such as the limited availability of large, annotated datasets and the inherent complexity of medical data, our approach shows great promise for personalized medicine. By equipping clinicians with a powerful tool for early ovarian cancer detection, this method has the potential to improve patient outcomes through earlier and more accurate diagnoses.},
  archive      = {J_NCA},
  author       = {Eldakhly, Nabil M.},
  doi          = {10.1007/s00521-025-11051-6},
  journal      = {Neural Computing and Applications},
  month        = {5},
  number       = {15},
  pages        = {9025-9047},
  shortjournal = {Neural Comput. Appl.},
  title        = {Enhanced early detection of ovarian cancer through deep learning and fuzzy rough sets},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). QLIF: Mitigating the memory and computation overhead to implement spiking convolutional neural networks. <em>NCA</em>, <em>37</em>(15), 9011-9024. (<a href='https://doi.org/10.1007/s00521-025-11045-4'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Spiking neural networks (SNNs) have many advantages in achieving lower power neuromorphic computation. In this paper, we propose new methods that can further mitigate the memory spaces and computation resources that are needed for spiking convolutional neural networks (SCNNs) implementation. Traditional leaky-integrate-and-fire (LIF) mechanism, which is widely adopted in SNN, demands large memory spaces to store the membrane potentiation values for integration, through experiments, we find out that for many layers, the potentiation values can be quickly leaked (qLIF) and no extra memory spaces are needed to store the values from past events. With the adoption of qLIF mechanism, convolution between the input matrix and the kernel matrix can be simplified into a look-up-table (LUT) operation (called ConvLUT in this paper), without the need of computation resources to perform the dot-product. Through experiments on MNIST, CIFAR-10, CIFAR-100 and DVS128-Gesture datasets with SCNNs, we have verified that both qLIF mechanism and ConvLUT method can greatly reduce the memory spaces as well as computation resources in SCNN, the computation latency can also be improved.},
  archive      = {J_NCA},
  author       = {Li, Silong and Wang, Ningning and Kang, Xinyu and Yu, Chunlin and Ye, Terry Tao},
  doi          = {10.1007/s00521-025-11045-4},
  journal      = {Neural Computing and Applications},
  month        = {5},
  number       = {15},
  pages        = {9011-9024},
  shortjournal = {Neural Comput. Appl.},
  title        = {QLIF: Mitigating the memory and computation overhead to implement spiking convolutional neural networks},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). DSR-CM: Decoupled long-term sequential recommendation model leveraging competitive mechanism. <em>NCA</em>, <em>37</em>(15), 8987-9009. (<a href='https://doi.org/10.1007/s00521-025-11033-8'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Existing long-term sequential recommendations ignore the capture of real-time user preferences, which leads to poor recommendation accuracy. To solve this problem, this work proposes a decoupled long-term sequential recommendation model leveraging competitive mechanism (DSR-CM). By introducing a multi-head flow-attention mechanism, the user preference information flow competes spontaneously under the constraints of the competition mechanism, which preserves the learning ability of traditional dot-product attention at the level of linear computational complexity, and captures the dynamic preference relations in the sequence of user behaviours more efficiently. Meanwhile, using decoupled computational position encoding more accurately captures the sequential relationship between user behaviours and models the user preference trends. Extensive experimental studies are carried out on three real-world datasets, where DSR-CM outperforms the existing state-of-the-art methods in terms of both effectiveness and efficiency, it accurately captures user’s dynamically changing preferences and latest trends, and explores new ways to enhance the adaptability and accuracy of the recommender system. The implementation code is available online at https://github.com/cyxg7/DSR-CM .},
  archive      = {J_NCA},
  author       = {Cui, Shaoguo and Li, Xingyu},
  doi          = {10.1007/s00521-025-11033-8},
  journal      = {Neural Computing and Applications},
  month        = {5},
  number       = {15},
  pages        = {8987-9009},
  shortjournal = {Neural Comput. Appl.},
  title        = {DSR-CM: Decoupled long-term sequential recommendation model leveraging competitive mechanism},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Developing block-based physics-informed multi-layered neural network model for simulating the inelastic response of base-isolated structures. <em>NCA</em>, <em>37</em>(15), 8963-8986. (<a href='https://doi.org/10.1007/s00521-025-11040-9'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The advancement of base isolation systems over recent years has been significant, enhancing the performance of structures under seismic conditions. A particularly effective system is the multi-stage friction pendulum, which offers a variety of effective pendula for energy dissipation. However, conducting nonlinear analyses of these structures using finite element analysis is computationally expensive and time-consuming due to the multiple sources of nonlinearity involved. This limitation poses a significant challenge for developing large-scale systems for post-earthquake rapid assessment. Accordingly, this research aims to address this challenge by developing a block-based physics-informed neural network (PINN) model as an alternative to finite element models for rapidly estimating the inelastic response of base-isolated structures. By embedding the governing physics into the neural network, the PINN model mitigates the data dependency issues associated with traditional artificial intelligence techniques and provides physically consistent predictions. Additionally, incorporating long short-term memory networks enhances the model's predictive capabilities. The proposed technique operates in similar to general finite element models where it infers results specific to the structures it was trained on. This capability is crucial for applications requiring rapid post-earthquake assessment, making it suitable for integration into smart city infrastructure where fast earthquake damage detection systems are needed. The study demonstrates the effectiveness of the PINN model, showing superior performance compared to traditional data-driven models and partially informed PINNs, thereby offering a viable solution for overcoming the limitations of finite element analysis in rapid seismic response estimation.},
  archive      = {J_NCA},
  author       = {Habib, Ahed and Yildirim, Umut},
  doi          = {10.1007/s00521-025-11040-9},
  journal      = {Neural Computing and Applications},
  month        = {5},
  number       = {15},
  pages        = {8963-8986},
  shortjournal = {Neural Comput. Appl.},
  title        = {Developing block-based physics-informed multi-layered neural network model for simulating the inelastic response of base-isolated structures},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Artificial neural network for the evaluation of electric propulsion system in unmanned aerial vehicles. <em>NCA</em>, <em>37</em>(15), 8945-8961. (<a href='https://doi.org/10.1007/s00521-025-11043-6'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the domain of unmanned aerial vehicles (UAVs), evaluating electric propulsion systems is pivotal for enhancing performance and efficiency. This study employs a scaled conjugate gradient (SCG) algorithm to train an artificial neural network (ANN) for the propulsion system evaluation, offering a cutting-edge alternative to traditional experimental methods. The ANN architecture consists of an input layer, a single hidden layer, and an output layer. By varying the number of neurons in the hidden layer from 1 to 100, the optimal configuration with 2 neurons was identified, achieving high predictive accuracy. The model was trained using experimental datasets, predicting thrust force with an overall R2 value exceeding 0.99 across training, validation, and testing phases, and a low overall prediction error of 1.27%. These results demonstrate the ANN’s capability to generalize from training data, making it a valuable tool for UAV designers. Integrating ANN-based evaluations accelerates decision-making processes and optimizes UAV performance, marking a significant advancement in UAV technology.},
  archive      = {J_NCA},
  author       = {Goli, Srikanth and Kurtuluş, Dilek Funda and Waqar, Muhammad and Imran, Imil Hamda and Alhems, Luai M. and Kouser, Taiba and Memon, Azhar M.},
  doi          = {10.1007/s00521-025-11043-6},
  journal      = {Neural Computing and Applications},
  month        = {5},
  number       = {15},
  pages        = {8945-8961},
  shortjournal = {Neural Comput. Appl.},
  title        = {Artificial neural network for the evaluation of electric propulsion system in unmanned aerial vehicles},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Optimizing auditory brainstem response detection through NSGA-II guided feature selection. <em>NCA</em>, <em>37</em>(15), 8925-8943. (<a href='https://doi.org/10.1007/s00521-025-11036-5'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Hearing-impaired people undergo auditory brainstem response (ABR) testing to assess their peripheral auditory nerve system. Audiologists apply diagnostic labels to ABR data using reference-based indicators such as peak latency, waveform shape, amplitude, and others. ABR test scoring errors may invalidate auditory nerve system integrity results. Machine learning, especially deep learning, may reduce human error in ABR analysis. This work offers a complete methodology to handle ABR analysis problems. First, extract ABR test images from PDF reports to create a dataset. The next step uses Elastic Distortion approach to create high-quality ABR images while conserving data. ResNet-50 deep learning extracts ABR image characteristics in the third phase. The fourth step finds the most relevant features using a trainable multi-objective non-dominated sorting genetic algorithm II optimizer. Finally, ABR images are classified using machine learning algorithms. The proposed model achieved significant performance metrics: an accuracy of 99.46%, precision of 99.33%, specificity of 99.57%, sensitivity of 99.57%, F1-score of 99.43%, and a mean squared error of 0.0054 when using a support vector machine classifier with both augmented and original images. These findings demonstrate the effectiveness of trainable feature selection and high-quality ABR images in improving ABR analysis accuracy.},
  archive      = {J_NCA},
  author       = {Majidpour, Jafar and Hassanzadeh, Hiwa and Khezri, Edris and Arabi, Hossein},
  doi          = {10.1007/s00521-025-11036-5},
  journal      = {Neural Computing and Applications},
  month        = {5},
  number       = {15},
  pages        = {8925-8943},
  shortjournal = {Neural Comput. Appl.},
  title        = {Optimizing auditory brainstem response detection through NSGA-II guided feature selection},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Enhanced grid stability and prolonging life span in renewable energy power converters using an advanced sugeno-type AI-based neuro-fuzzy control. <em>NCA</em>, <em>37</em>(15), 8895-8923. (<a href='https://doi.org/10.1007/s00521-025-11034-7'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A significant challenge lies in renewable energy sources incapacity to generate high voltages and their limited life spans when subjected to high-ripple conditions. This study introduces an innovative Sugeno-type neuro-fuzzy controller for an interleaved power converter configuration aimed at mitigating the input current ripples associated with these renewable energy sources, directly addressing the longevity concern controlled by an advanced neuro-fuzzy controller. The proposed converter employs a switched capacitor (SC) cell to amplify the generated voltage within the boost converter framework. Key attributes of the proposed converter include high voltage gain, enhanced efficiency and the utilization of short-duty ratio time intervals to minimize conduction power losses at elevated voltages. Furthermore, through interleaved configuration, the current ripple from the source is diminished while the SC cell concurrently amplifies the voltage gain. A Sugeno-type neuro-fuzzy control method, based on artificial intelligence, is employed for the proposed converter to drive the switches and produce an accurate output voltage. Since the converter is primarily built on a fuzzy controller, the proposed method is mathematically simple and easy to implement. The main contribution of the proposed control approach lies in the sampling of both the input and reference, as well as the output voltages, and the generation of precise duty cycles based on the sampled reference output voltage. Due to its capability of generating high voltages, the proposed converter and control system are suitable for use in DC grids and vehicle-to-grid applications.},
  archive      = {J_NCA},
  author       = {Özden, Mustafa and Ertekin, Davut and Baltacı, Kübra},
  doi          = {10.1007/s00521-025-11034-7},
  journal      = {Neural Computing and Applications},
  month        = {5},
  number       = {15},
  pages        = {8895-8923},
  shortjournal = {Neural Comput. Appl.},
  title        = {Enhanced grid stability and prolonging life span in renewable energy power converters using an advanced sugeno-type AI-based neuro-fuzzy control},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). An artificial intelligence approach for predicting water-filled porosity and water saturation for carbonate reservoirs using conventional well logs. <em>NCA</em>, <em>37</em>(15), 8869-8894. (<a href='https://doi.org/10.1007/s00521-025-11041-8'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Water saturation is the most crucial parameter obtained by petrophysical analysis to estimate the potential reserve in oil and gas reservoirs. This parameter is usually obtained by either industry-known water saturation correlations, such as Archie, which carries inherently a wide room of uncertainties, or by the laboratory measurement through the Dean-Stark method. Dielectric logging tool is designed specifically to obtain accurate water saturation measurements that are independent of lithology, environmental effects, and water salinity. However, running the tool will add some complexity to the logging job and impose an extra cost. Therefore, this work aims to utilize artificial intelligence (AI) to obtain fast and reliable estimations for the dielectric water saturation in carbonate rocks utilizing conventional logs. The workflow of developing an AI model to obtain the dielectric water saturation from the conventional well logs went through two stages. Firstly, a new model was developed to predict the dielectric water-filled porosity (PWXO) from the conventional logs. Then, the dielectric’s invaded zone water saturation (SXO) was predicted based on the conventional logs and the PWXO. The conventional logs involved in this study are bulk density (RHOZ), photoelectric lithology factor (PEF), gamma-ray (GR), micro-resistivity (RXOZ), induction resistivities (AF10, AF20, AF30, AF60, and AF90), spontaneous potential (SP), and neutron porosity (NPHI). On the other hand, the actual values of the water-filled porosity (PWXO) and invaded zone water saturation (SXO) were obtained from the dielectric logs. Three AI techniques were used which are artificial neural networks (ANN), adaptive neuro-fuzzy inference system (ANFIS), and support vector machine (SVM). In addition, several evaluation indices were utilized to assess the prediction performance of the developed models, such as normalized root-mean-square error (NRMSE) and Pearson coefficient of correlation (CC). Also, the model inputs and structure were adjusted to achieve the best prediction performance. The water saturation (SXO) can be predicted from the conventional logs with an NRMSE of 0.04 and CC of 0.62; however, including the dielectric porosity (PWXO) as an input led to improve the prediction performance, CC is 0.98 and NRMSE is 0.003. Overall, this work will add a significant contribution to reservoir characterization by providing fast and reliable estimations of the water saturation using the conventional log without the need to run the dielectric tool. Furthermore, it can be used to obtain accurate values of cementation factors, tortuosity factors, and clay minerals’ cation exchange capacity (CEC) values which can be used later as the constants required for the widely known correlations to calculate the reservoir's water saturation.},
  archive      = {J_NCA},
  author       = {Oshaish, Ali and Abdulraheem, Abdulazeez and Hassan, Amjed and El-Husseiny, Ammar and Mahmoud, Mohamed},
  doi          = {10.1007/s00521-025-11041-8},
  journal      = {Neural Computing and Applications},
  month        = {5},
  number       = {15},
  pages        = {8869-8894},
  shortjournal = {Neural Comput. Appl.},
  title        = {An artificial intelligence approach for predicting water-filled porosity and water saturation for carbonate reservoirs using conventional well logs},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Neural network-based surrogate model in postprocessing of topology optimized structures. <em>NCA</em>, <em>37</em>(15), 8845-8867. (<a href='https://doi.org/10.1007/s00521-025-11039-2'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper proposes a general method of creating an accurate neural network-based surrogate model for postprocessing a topologically optimized structure. When topology optimization results are converted into computer-aided design (CAD) files with smooth boundaries for manufacturability, finite element method (FEM) based stresses often do not agree with the topology optimized results due to changes of surface and mesh density. The conversion between topology optimization derived results and CAD files often requires postprocessing, an additional fine tuning of the geometry parameters to reconcile the change of the stress values. In this work, a feedforward, deep artificial neural network (DANN) is presented with varying architecture parameters that are found for each stress output of interest. This network is trained with the data based on a combination of Design of Experiments (DoE) models that have the geometry dimensions as inputs and stress readings under various loads as the outputs. A DANN-based surrogate model is constructed to enable fine tuning of all relevant stress performance metrics. This method of constructing an artificial network-based surrogate model minimizes the number of FEM computations required to generate an optimized, post-processed design. We present a case study of postprocessing a wind tunnel balance, a measurement device that yields the six force and moment components of a test aircraft. It needs to be designed considering multiple stress measures under combinations of the six loading conditions. Excellent performance of a neural network is presented in this paper in terms of accurate prediction of the highly nonlinear stresses under combinations of the six loads. Von Mises stress predictions are within 10% and axial force sensor stress predictions are within 2% for the final post-processed topology. The results support its usefulness for postprocessing of topology optimized structures.},
  archive      = {J_NCA},
  author       = {Persia, Jude Thaddeus and Sung, Myung Kyun and Lee, Soobum and Burns, Devin E.},
  doi          = {10.1007/s00521-025-11039-2},
  journal      = {Neural Computing and Applications},
  month        = {5},
  number       = {15},
  pages        = {8845-8867},
  shortjournal = {Neural Comput. Appl.},
  title        = {Neural network-based surrogate model in postprocessing of topology optimized structures},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Advanced machine learning techniques for predicting power generation and fault detection in solar photovoltaic systems. <em>NCA</em>, <em>37</em>(15), 8825-8844. (<a href='https://doi.org/10.1007/s00521-025-11035-6'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This study investigated the application of advanced Machine Learning techniques to predict power generation and detect abnormalities in solar Photovoltaic systems. The study conducted a comprehensive assessment of various sophisticated models, including Random Trees, Random Forest, eXtreme Gradient Boosting, Linear Regression, Gradient Boosting (GB), and Categorical Boosting (CatBoost), utilizing a substantial dataset of 97,333 sets. The analysis focused on two fundamental objectives: power prediction and fault identification, both of which are crucial for enhancing the effectiveness and dependability of PV systems. CatBoost and GB models exhibited exceptional performance in power prediction, with the maximum R-squared value of 0.994. Demonstrating a strong correlation with the data and the ability to account for a substantial amount of the variation in power generation. These models outperformed others by providing more accurate and reliable projections, which are crucial for effective solar energy management and planning. CatBoost demonstrated superior performance compared to other approaches in the flaw detection test, attaining the highest performance metrics. The model achieved an accuracy of 0.999743, precision of 0.997171, recall of 0.999291, and an F1 score of 0.998230. The measures illustrated CatBoost’s exceptional ability to precisely identify problems with little errors, confirming it as the most successful model among those evaluated. The exceptional precision and dependability of the CatBoost model in identifying faults highlighted its capacity for continuously monitoring and maintaining solar systems in real-time, consequently augmenting operational efficiency. The study emphasized the significance of choosing suitable models to achieve the highest level of accuracy in predicting and detecting faults, thereby enabling the development of more sustainable and efficient solar energy systems. Subsequent research should prioritize the validation of these models using varied datasets, integration of up-to-date meteorological data, and creation of defect detection methods in real-time to enhance system efficiency.},
  archive      = {J_NCA},
  author       = {Abdelsattar, Montaser and AbdelMoety, Ahmed and Emad-Eldeen, Ahmed},
  doi          = {10.1007/s00521-025-11035-6},
  journal      = {Neural Computing and Applications},
  month        = {5},
  number       = {15},
  pages        = {8825-8844},
  shortjournal = {Neural Comput. Appl.},
  title        = {Advanced machine learning techniques for predicting power generation and fault detection in solar photovoltaic systems},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). DeepLeaf: An optimized deep learning approach for automated recognition of grapevine leaf diseases. <em>NCA</em>, <em>37</em>(15), 8799-8823. (<a href='https://doi.org/10.1007/s00521-025-11038-3'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Plant diseases can cause severe losses in agricultural production, impacting food security and safety. Early detection of plant diseases is crucial to minimize crop damage and ensure agricultural sustainability. Manual monitoring is often impractical due to the complexity and time involved, making automated disease recognition essential. This study presents a new Plant Disease Detection Algorithm (PDDA) called DeepLeaf focused on identifying four common grapevine diseases: leaf blight, black rot, stable, and black measles. The PDDA integrates three key modules: an Image Preprocessing Module, a Feature Extraction Module, and an Optimized Convolutional Neural Network (OCNN)-based Classification Module. The OCNN forms the core of the classification system, with its hyperparameters fine-tuned using fuzzy optimization to enhance performance. Preprocessing techniques are applied to analyze diseased leaves, and a logistic regression algorithm is used to downsample the features for better analysis. The CNN is trained on images from the Plant Village dataset, allowing it to detect and classify grapevine leaf diseases accurately. The proposed model's efficiency in the automated diagnosis of grapevine diseases is demonstrated by its remarkable 99.7% accuracy rate. This high accuracy indicates that the PDDA may help with more effective and scalable plant disease monitoring, which will ultimately allow better agricultural practices.},
  archive      = {J_NCA},
  author       = {Talaat, Fatma M. and Shams, Mahmoud Y. and Gamel, Samah A. and ZainEldin, Hanaa},
  doi          = {10.1007/s00521-025-11038-3},
  journal      = {Neural Computing and Applications},
  month        = {5},
  number       = {15},
  pages        = {8799-8823},
  shortjournal = {Neural Comput. Appl.},
  title        = {DeepLeaf: An optimized deep learning approach for automated recognition of grapevine leaf diseases},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Evidential uncertainty quantification with multiple deep learning architectures for spatiotemporal drought forecasting. <em>NCA</em>, <em>37</em>(15), 8773-8797. (<a href='https://doi.org/10.1007/s00521-025-11026-7'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep learning approaches are increasingly being applied to forecasting challenges, but many current methods rely on advanced neural networks that provide only point estimates, without adequately addressing the epistemic and aleatoric uncertainties associated with predictions. Accurately capturing and quantifying both types of uncertainty is essential for assessing the confidence level of model outputs, especially in decision-sensitive contexts. Traditional techniques such as quantile regression, Bayesian neural networks, and Monte Carlo dropout tend to be either computationally intensive or prone to inaccuracies. In this paper, we propose an evidential deep learning (EDL)-based approach for drought forecasting that integrates uncertainty quantification. This method characterizes both aleatoric and epistemic uncertainties, offering a more comprehensive understanding of predictive reliability in relation to drought events. By leveraging the Dirichlet distribution, the model interprets uncertainties as evidence values associated with the neural network outputs. The proposed EDL approach is evaluated across three deep learning architectures-CNN, LSTM, and GAN-applied to datasets from the Horn of Africa and Southwestern Europe. Experimental results show that the EDL method outperforms traditional probabilistic and deterministic models, emphasizing the critical role of uncertainty quantification in enhancing forecasting accuracy.},
  archive      = {J_NCA},
  author       = {Ferchichi, Ahlem and Chihaoui, Mejda and Toujani, Radhia and Ferchichi, Aya and Hendaoui, Fatma},
  doi          = {10.1007/s00521-025-11026-7},
  journal      = {Neural Computing and Applications},
  month        = {5},
  number       = {15},
  pages        = {8773-8797},
  shortjournal = {Neural Comput. Appl.},
  title        = {Evidential uncertainty quantification with multiple deep learning architectures for spatiotemporal drought forecasting},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A deep learning approach for recognizing and solving handwritten mathematical equations. <em>NCA</em>, <em>37</em>(14), 8759-8772. (<a href='https://doi.org/10.1007/s00521-025-11025-8'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recognizing handwritten equations is considerably more challenging than identifying printed ones, primarily due to variations in writing styles. Even though there have been recent improvements in handwriting recognition for things like equation solvers and automation, it is still hard for machines to understand a handwritten mathematical expression (HME). This paper presents a deep learning model for recognizing and solving handwritten equations, as well as its graphical user interface (GUI). A manually constructed large dataset comprising approximately 66k images of digits, mathematical symbols, and characters is utilized, with a fine-tuned InceptionV3 model employed for image classification and segmentation. The system is designed to extract handwritten equations from an image, perform character recognition, and generate the roots of the polynomial equation up to degree four. According to the findings, the trained model achieves remarkable accuracy in recognizing and solving handwritten equations, with over 94% accuracy on the training set. By eliminating the possibility of human error, automatic handwritten equation solvers provide more accurate results than manual methods.},
  archive      = {J_NCA},
  author       = {Yadav, Pavinder and Shantilal, Sankhala Bhavik and Kumar, Vipin and Sihag, Parveen and Sharma, Pawan Kumar and Rana, Pankaj},
  doi          = {10.1007/s00521-025-11025-8},
  journal      = {Neural Computing and Applications},
  month        = {5},
  number       = {14},
  pages        = {8759-8772},
  shortjournal = {Neural Comput. Appl.},
  title        = {A deep learning approach for recognizing and solving handwritten mathematical equations},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Enhanced brain tumor detection and classification using a deep image recognition generative adversarial network (DIR-GAN): A comparative study on MRI, X-ray, and FigShare datasets. <em>NCA</em>, <em>37</em>(14), 8731-8758. (<a href='https://doi.org/10.1007/s00521-025-11031-w'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Given its significant threat to human health, early and precise diagnosis of brain cancer is crucial for improving patient prognosis and treatment efficacy. Although magnetic resonance imaging (MRI) is essential for detecting brain tumors, current deep learning techniques face challenges in managing noise interference and accurately defining tumor boundaries. This study introduces a novel hybrid framework called deep image recognition generative adversarial network (DIR-GAN), which aims to enhance the accuracy and robustness of brain tumor detection and classification in MRI scans. The DIR-GAN approach was designed to address the limitations of the existing methods in processing and analyzing brain imaging data. The proposed methodology integrates cutting-edge techniques in multiple stages. Noise reduction is achieved through adaptive median-bilateral filtering (AMBF), which effectively removes noise while preserving fine structural detail. To achieve precise tumor segmentation, the deep superpixel-based attention segmentation (DSAS) method combines superpixel generation with a hierarchical attention mechanism to achieve highly accurate tumor region delineation. Feature extraction is optimized using a hybrid approach based on the gray-level co-occurrence matrix (GLCM) and chaotic dragonfly algorithm (CDA), leveraging chaotic maps to enhance convergence speed and exploratory efficiency. These optimized features are fed into the DIR-GAN, which employs attention-enhanced multi-scale feature extraction and adversarial training to synthesize high-quality MRI images and strengthen the classification performance. Hybrid residual connections in the generator and discriminator further improved feature learning and classification reliability. The proposed framework was implemented in Python and evaluated on three datasets: Share, MRI, and X-ray. The DIR-GAN demonstrated state-of-the-art performance, achieving an accuracy of 98.90% on the FigShare dataset, 98.60% on the MRI dataset, and 97.93% on the X-ray dataset. This hybrid framework offers a robust and interpretable solution for the early diagnosis and classification of brain tumors, setting a new benchmark for clinical applications and significantly improving patient outcomes.},
  archive      = {J_NCA},
  author       = {Karpakam, S. and Kumareshan, N.},
  doi          = {10.1007/s00521-025-11031-w},
  journal      = {Neural Computing and Applications},
  month        = {5},
  number       = {14},
  pages        = {8731-8758},
  shortjournal = {Neural Comput. Appl.},
  title        = {Enhanced brain tumor detection and classification using a deep image recognition generative adversarial network (DIR-GAN): A comparative study on MRI, X-ray, and FigShare datasets},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Generating prediction clustering graph network analysis data from masked time series with GANs. <em>NCA</em>, <em>37</em>(14), 8699-8729. (<a href='https://doi.org/10.1007/s00521-025-11028-5'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Research in social community mining from social media faces challenges due to the constantly evolving nature of online networks. Deep learning methods often struggle with noisy data caused by sudden changes and fail to capture essential temporal information for community evolution. Additionally, data security concerns are frequently overlooked. This paper presents a novel probabilistic generative adversarial network (GAN) model designed for dynamic social community detection, addressing challenges posed by the evolving nature of networks and noisy data. Our approach integrates a hybrid probabilistic convolutional neural network (PCNN) and gated recurrent unit (GRU) architecture, capturing temporal community dynamics while preserving data confidentiality through deep generative masking techniques. The model generates synthetic time series data that effectively mimics real data while safeguarding sensitive information and improving forecasting accuracy. Experimental results demonstrate the model’s superior performance in terms of RMSE, MAPE, CE, CRPS*, and ARR, outperforming traditional deep learning models and providing valuable insights for decision-making in spatiotemporal community detection.},
  archive      = {J_NCA},
  author       = {Toujani, Radhia and Hendaoui, Fatma and Ferchichi, Ahlem and Chihaoui, Mejda and Ferchichi, Aya and Bakari, Wided},
  doi          = {10.1007/s00521-025-11028-5},
  journal      = {Neural Computing and Applications},
  month        = {5},
  number       = {14},
  pages        = {8699-8729},
  shortjournal = {Neural Comput. Appl.},
  title        = {Generating prediction clustering graph network analysis data from masked time series with GANs},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). ANFISmark: ANFIS-based secure watermarking approach for telemedicine applications. <em>NCA</em>, <em>37</em>(14), 8677-8698. (<a href='https://doi.org/10.1007/s00521-025-11030-x'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The global expansion of multimedia data over the internet has led to a new revolution. Multimedia data, especially medical records, are to be dealt with utmost security for telemedicine applications. This paper presents a novel optimized watermarking technique that ensures the integrity of medical records that bear vital information. This presented technique utilizes the key features of the maximally stable extremal region (MSER) and adaptive neuro-fuzzy inference system (ANFIS) for watermarking. MSER algorithm aids in computing the region of interest (ROI), which is being further embedded with a generated hybrid watermark. To ensure better medical data privacy, a hybrid watermark is generated with two medical images. ANFIS optimization is used to maintain the trade-off among various attributes of watermarking. ANFIS optimization uses different salient features of the images to compute the optimal gain factor. Experimental results analysis highlights the effectiveness of the proposed technique in terms of performance, such as robustness, imperceptibility, etc. Moreover, the proposed method offers better results than the pertaining techniques.},
  archive      = {J_NCA},
  author       = {Awasthi, Divyanshu and Khare, Priyank and Srivastava, Vinay Kumar},
  doi          = {10.1007/s00521-025-11030-x},
  journal      = {Neural Computing and Applications},
  month        = {5},
  number       = {14},
  pages        = {8677-8698},
  shortjournal = {Neural Comput. Appl.},
  title        = {ANFISmark: ANFIS-based secure watermarking approach for telemedicine applications},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). AI-driven churn prediction in subscription services: Addressing economic metrics, data transparency, and customer interdependence. <em>NCA</em>, <em>37</em>(14), 8651-8676. (<a href='https://doi.org/10.1007/s00521-025-11027-6'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This study aims to improve customer churn prediction by integrating machine learning algorithms and evaluating their performance using criteria like accuracy, profit, and Customer Lifetime Value (CLV). Using a Telecom Customer Churn Prediction dataset, the study analyzes the efficiency of various models such as XGBoost, Linear Regression, Decision Tree Regressor, Lasso, Random Forest, and Gradient Boosting Regressor. This study introduces an Adaptive Profit-Centric Churn Prediction Engine (APCPE), an innovative churn prediction system that evaluates profitability and adapts to changing customer behavior. Each model’s accuracy, profit, and CLV are assessed, revealing that the proposed APCPE is the most accurate model, with a 97.01% accuracy rate, a profit of $606.3125, and a CLV of $1212.625. Convergence curves, including R2 convergence curves, show the model’s performance and convergence rates for each algorithm. Convergence curves, including R2 convergence curves, show the model’s performance and convergence rates for each algorithm. The study digs deeper into interdependence modeling, which involves finding and measuring consumer interactions or dependencies such as social network linkages and product co-occurrence. The study adds insights into modeling customer behavior dependencies and their impact on churn prediction, providing a holistic approach to improving corporate tactics for customer retention.},
  archive      = {J_NCA},
  author       = {Talaat, Fatma M. and Aljadani, Abdussalam},
  doi          = {10.1007/s00521-025-11027-6},
  journal      = {Neural Computing and Applications},
  month        = {5},
  number       = {14},
  pages        = {8651-8676},
  shortjournal = {Neural Comput. Appl.},
  title        = {AI-driven churn prediction in subscription services: Addressing economic metrics, data transparency, and customer interdependence},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Classification of saudi coffee beans using a mobile application leveraging squeeze vision transformer technology. <em>NCA</em>, <em>37</em>(14), 8629-8649. (<a href='https://doi.org/10.1007/s00521-025-11024-9'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Identifying coffee bean varieties and roasting levels is crucial for promoting Saudi coffee culture. Varieties like Harari, Khawlani, Nabari, Laqamti, and Bariah can be distinguished by their unique color, shape, and morphological traits. Similarly, roasting levels, from "green" to "dark roast," are identified by color. Traditional visual and mechanical inspection methods have been inadequate for accurately distinguishing these varieties and roast levels. To address this, we present a deep learning model that predicts coffee type and roast level from images. We curated a "Saudi Coffee Type" dataset with 4 roast levels and 5 Saudi coffee bean types. Our model, based on the SqueezeNet architecture with Vision Transformer (ViT) enhancements, effectively interprets this dataset. It uses a multi-head attention mechanism, achieving 85.9% accuracy, an F1 score of 0.871, and an F2 score of 0.872. These metrics demonstrate the model's capability to predict coffee type and roasting level, understanding bean differences. Comparative analyses show our model excels in classifying Saudi coffee beans by morphological traits and accurately categorizing roasting levels through color analysis. Future research will aim to generalize this approach for broader applications, potentially extending it to other global coffee varieties. Enhancements will include expanding the dataset, refining the model architecture, and collaborating with coffee experts. Our model represents a significant advancement in coffee classification, leveraging machine learning to support Saudi coffee culture preservation and promotion.},
  archive      = {J_NCA},
  author       = {Alhasson, Haifa F. and Alharbi, Shuaa S.},
  doi          = {10.1007/s00521-025-11024-9},
  journal      = {Neural Computing and Applications},
  month        = {5},
  number       = {14},
  pages        = {8629-8649},
  shortjournal = {Neural Comput. Appl.},
  title        = {Classification of saudi coffee beans using a mobile application leveraging squeeze vision transformer technology},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). ADNeuroNet: A neuroevolution-based neural network algorithm for the diagnosis of neurodegenerative diseases. <em>NCA</em>, <em>37</em>(14), 8593-8627. (<a href='https://doi.org/10.1007/s00521-025-11021-y'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Neurodegenerative disorders such as dementia and Alzheimer’s disease (AD) have adversely devastated the health and well-being of the older community. Given that early detection might help prevent or delay cognitive disorders associated with AD, developing an effective diagnostic technique is deemed appropriate to control the disease. Despite advances in clinical diagnostic standards and treatment techniques, the global prevalence of cognitive disturbance and behavioral problems remains steep. Besides the above challenge, the scarcity of AD-related open-source raw data prompted us to tailor ADNeuroNet, a neuroevolution-based neural network (NN) designed for predicting cognitively normal, mild cognitive impairment, and AD instances. To construct a robust NN algorithm that can predict the disease with high accuracy and improved efficiency, we worked on the combined cognitive and demographic clinical data to create a comprehensive model. Primarily, the Alzheimer’s Disease Neuroimaging Initiative (ADNI) repository was used to develop this program. We employed three distinct datasets and developed a predictive model that achieved the maximum performance accuracy of 93.42%. Utilizing only baseline details, the as-developed model successfully diagnosed and predicted neurodegenerative disorders and is likely to emerge as an effective clinical tool in future endeavors.},
  archive      = {J_NCA},
  author       = {Khan, Afreen and Zubair, Swaleha and Ali, Irfan},
  doi          = {10.1007/s00521-025-11021-y},
  journal      = {Neural Computing and Applications},
  month        = {5},
  number       = {14},
  pages        = {8593-8627},
  shortjournal = {Neural Comput. Appl.},
  title        = {ADNeuroNet: A neuroevolution-based neural network algorithm for the diagnosis of neurodegenerative diseases},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Intelligent wearable vision systems for the visually impaired in saudi arabia. <em>NCA</em>, <em>37</em>(14), 8571-8591. (<a href='https://doi.org/10.1007/s00521-025-10987-z'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Navigating the world with visual impairments presents unique challenges, often limiting independence and safety. This research introduces SafeStride, a novel algorithm designed to empower visually impaired individuals through real-time obstacle detection and navigation assistance. SafeStride functions through five interconnected phases: sensor data acquisition, data preprocessing, obstacle detection, path planning, and feedback and guidance. A combination of ultrasonic sensors, cameras, and inertial measurement units (IMUs) feed comprehensive environmental data into the system, enabling accurate obstacle detection and safe path planning. Evaluating SafeStride's performance involved the MNIST dataset for image recognition and a dedicated indoor object detection dataset. Additionally, comparisons were made against three deep learning models—convolutional neural network (CNN), long short-term memory (LSTM), and gated recurrent unit (GRU). The results showcased SafeStride's high accuracy and effectiveness in both obstacle detection and classification. Beyond its impressive performance, SafeStride represents a significant leap forward in navigation aids for the visually impaired. By offering a comprehensive solution that enhances safety and independence in diverse environments, it holds immense potential to improve the lives of countless individuals. Future efforts will focus on further optimizing the algorithm and testing its real-world capabilities, paving the way for a future where safe and independent navigation becomes a reality for all. All three deep learning models (CNN, LSTM, GRU) excelled in the experiment, achieving high accuracy above 0.98, strong recall and F1-scores near 0.99, and exceptional AUC-ROC scores exceeding 0.9998.},
  archive      = {J_NCA},
  author       = {Talaat, Fatma M. and El-Shafai, Walid and Soliman, Naglaa F. and Algarni, Abeer D. and El-Samie, Fathi E. Abd},
  doi          = {10.1007/s00521-025-10987-z},
  journal      = {Neural Computing and Applications},
  month        = {5},
  number       = {14},
  pages        = {8571-8591},
  shortjournal = {Neural Comput. Appl.},
  title        = {Intelligent wearable vision systems for the visually impaired in saudi arabia},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). VireNet-SSD: Object detection model for resource-constrained applications based on self-organized operational neural networks. <em>NCA</em>, <em>37</em>(14), 8547-8569. (<a href='https://doi.org/10.1007/s00521-025-10986-0'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Discovering deep learning-based computer vision solutions for use with constrained devices is exceptionally hard, and the trade-offs are often too undermining. Deep learning models are enormous, which makes it challenging to deploy them on constrained platforms. The convolutional neural network is the fundamental framework for majority of the models that are currently in use. However, operational neural networks have recently shown to be a better option to the convolutional equivalents on a variety of tasks due to their heterogeneous nature and greater resemblance to the functioning of biological neurons. The question of whether heterogeneous models could function on constrained devices and be deployed in real time remains a major concern. To address this problem, an object detection model architecture based on a single-shot multi-box detector with self-organized operational neural networks as its backbone was developed, which can perform efficiently on constrained devices such as Raspberry Pi. The resultant backbone architecture was named as VireNet. In contrast to homogeneous conventional deep learning networks that use convolutions, heterogeneous networks were chosen to develop VireNet, which provides a more productive and effective solution. Furthermore, an in-depth explanation of the design space has been provided to aid any future research that is associated with this architectural search. This new approach might mark the very beginning of the use of heterogeneity to address issues on devices with constrained resources.},
  archive      = {J_NCA},
  author       = {Kamath, Vidya and Renuka, A.},
  doi          = {10.1007/s00521-025-10986-0},
  journal      = {Neural Computing and Applications},
  month        = {5},
  number       = {14},
  pages        = {8547-8569},
  shortjournal = {Neural Comput. Appl.},
  title        = {VireNet-SSD: Object detection model for resource-constrained applications based on self-organized operational neural networks},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Advancing alzheimer’s detection: Integrative approaches in MRI analysis with traditional and deep learning models. <em>NCA</em>, <em>37</em>(14), 8527-8546. (<a href='https://doi.org/10.1007/s00521-025-10993-1'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Alzheimer’s dementia (AD) poses a significant global health challenge, characterized by progressive cognitive decline, memory impairment, and behavioral changes. The critical need for early detection to enable timely intervention and personalized care is emphasized by the current lack of effective treatments. This study aims to develop precise diagnostic models for AD by employing machine learning and a customized deep-convolutional neural network (cDCNN) with three convolution layers, utilizing Magnetic Resonance Imaging (MRI) data. Methods involve analyzing two distinct datasets—Alzheimer’s Disease Neuroimaging Initiative (ADNI) and Kaggle—to explore diverse cohorts and imaging features associated with AD pathology. To address class imbalance, the Synthetic Minority Over-sampling Technique (SMOTE) is applied across both datasets. A range of traditional machine learning methods, including support vector machine, k-nearest neighbor, random forest, decision trees, and XGBoost classifier, are evaluated alongside the cDCNN model, which leverages key MRI biomarkers of AD for both datasets. Results show the cDCNN model achieved a specific accuracy of 87% on the ADNI dataset, despite challenges in converting ADNI’s Digital Imaging and Communications in Medicine (DICOM) files to JPEG, impacting image clarity. Conclusions suggest that this research provides critical diagnostic tools for clinicians, offering insights into AD pathology and contributing to the alleviation of AD’s societal impact.},
  archive      = {J_NCA},
  author       = {Vanaja, T. and Shanmugavadivel, Kogilavani and Subramanian, Malliga and Kanimozhiselvi, C. S.},
  doi          = {10.1007/s00521-025-10993-1},
  journal      = {Neural Computing and Applications},
  month        = {5},
  number       = {14},
  pages        = {8527-8546},
  shortjournal = {Neural Comput. Appl.},
  title        = {Advancing alzheimer’s detection: Integrative approaches in MRI analysis with traditional and deep learning models},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Electrical submersible pump fault diagnosis based on 2D transformation of vibration signals and transfer learning of image classification networks. <em>NCA</em>, <em>37</em>(14), 8509-8525. (<a href='https://doi.org/10.1007/s00521-025-10998-w'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Diagnosing faults in electrical submersible pumps using intelligent methods is a challenging task, especially when deep learning techniques are used to extract features directly from vibration signals instead of relying on predefined human features. A key limitation of this approach is the lack of foundational models for machine fault diagnosis using vibration data, unlike the abundance of pre-trained networks available for image classification. To address this, we propose a method that applies various 2D transformations to time domain signals, combines them into RGB images, and leverages these images to fine-tune existing image classification networks. Our results demonstrate that this approach outperforms the state-of-the-art previous deep learning method based on metric learning applied to this task and is comparable to the solution using human-defined features.},
  archive      = {J_NCA},
  author       = {da Silva, Luciano Henrique Peixoto and Rodrigues, Alexandre and Varejão, Flavio and Ribeiro, Marcos Pellegrini and Oliveira-Santos, Thiago},
  doi          = {10.1007/s00521-025-10998-w},
  journal      = {Neural Computing and Applications},
  month        = {5},
  number       = {14},
  pages        = {8509-8525},
  shortjournal = {Neural Comput. Appl.},
  title        = {Electrical submersible pump fault diagnosis based on 2D transformation of vibration signals and transfer learning of image classification networks},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Optimizing medical image analysis through MViTX on multiple datasets with explainable AI. <em>NCA</em>, <em>37</em>(14), 8479-8507. (<a href='https://doi.org/10.1007/s00521-025-11014-x'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cancer remains a leading cause of mortality worldwide, with early detection and accurate diagnosis critical to improving patient outcomes. While computer-aided diagnosis systems powered by deep learning have shown considerable promise, their widespread clinical adoption faces significant challenges in maintaining consistent performance across diverse imaging modalities and datasets. This research addresses the critical challenge of developing robust, generalizable deep learning models by proposing a comprehensive framework utilizing seven diverse medical imaging datasets encompassing fundus photography, histopathology, endoscopy, and MRI, covering diseases such as ocular toxoplasmosis, endometrial cancer, colorectal cancer, gastrointestinal disease, breast cancer, brain tumor, and tympanic membrane conditions. Our methodology combines customized data augmentation strategies (photometric, geometric, and elastic transformations) with an optimized vision transformer with external attention (MViTX) architecture. The MViTX model demonstrated exceptional performance with test accuracies ranging from 94.1 to 99.1% across all datasets, achieving superior metrics in accuracy, precision, recall, F1-score, and AUC compared to state-of-the-art CNNs. The model's effectiveness was further validated through ablation studies and explainable AI techniques, while its practical utility was demonstrated through deployment as a user-friendly web application. Our research establishes the effectiveness of combining tailored data augmentation with attention-based transformer architectures for medical image analysis, representing a significant step toward enhancing healthcare professionals' diagnostic capabilities and ultimately improving patient care outcomes.},
  archive      = {J_NCA},
  author       = {Sheakh, Md. Alif and Tahosin, Mst. Sazia and Alam, Mohammad Jahangir and Begum, Mahbuba},
  doi          = {10.1007/s00521-025-11014-x},
  journal      = {Neural Computing and Applications},
  month        = {5},
  number       = {14},
  pages        = {8479-8507},
  shortjournal = {Neural Comput. Appl.},
  title        = {Optimizing medical image analysis through MViTX on multiple datasets with explainable AI},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Long-term tracking of recovery of built infrastructure after wildfires with deep network topologies. <em>NCA</em>, <em>37</em>(14), 8465-8477. (<a href='https://doi.org/10.1007/s00521-025-11003-0'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Climate change has led to more frequent and intense droughts and temperature extremes in the western United States, resulting in increasingly severe and more frequent wildfires. The concurrent expansion of residential construction into wildland areas has raised the likelihood of ignitions and the potential scale of damage to infrastructure, homes, and businesses. Structure protection in the wildland-urban interface is challenged by these increased ignitions, altered fuel structures in human-dominated areas, and mixed acceptance of prescribed burning and other prefire mitigation to minimize wildfire damage. Being able to track the multiyear building recovery of entire wildfire-impacted communities is crucial to analyze post-fire resilience in recovering communities and can support local decision-makers in determining what conditions slow down or accelerate the repair and rebuilding efforts. Here, we document post-wildfire building recovery using post-fire damage data and corresponding nanosatellite imagery to assess the monthly recovery rate of buildings over 3 years following the 2017 Tubbs Fire and 2018 Camp Fire in California, and the 2020 Echo Mountain Complex and 2020 Almeda Drive Fire in Oregon. The approach facilitates tracking of the building recovery timeline across a community using 3 m freely available multispectral imagery. We trained the deep residual network model with a binary classification layer to detect changes in the reflected spectra related to repairing or rebuilding in four study areas damaged by wildfires. The residual network using small property-sized fragments of imagery outperformed a U-Net segmentation network and achieved a validation accuracy of 94.0% and test accuracy of 92.6% for the classification of undamaged and damaged or destroyed buildings. We found different recovery timelines across study communities in recovery with distinct temporal patterns, with the Echo Mountain Complex recovering most rapidly, primarily due to the smaller number of structures initially lost and the use of a large proportion of prefabricated mobile structures in the rebuilding effort. In contrast, communities in the Camp Fire are rebuilding infrastructure much more slowly, partly due to a shift toward fire-hardened infrastructure and increased insurance regulations on building types. Our machine learning approach can be applied to determine the pace and the extent of recovery with high accuracy using an established deep network approach and freely available satellite data providing sufficient spatial and temporal resolution.},
  archive      = {J_NCA},
  author       = {Schmidt, Andres and Ellsworth, Lisa and Tilt, Jenna and Thiel, Amanda and Hiner, Nancy},
  doi          = {10.1007/s00521-025-11003-0},
  journal      = {Neural Computing and Applications},
  month        = {5},
  number       = {14},
  pages        = {8465-8477},
  shortjournal = {Neural Comput. Appl.},
  title        = {Long-term tracking of recovery of built infrastructure after wildfires with deep network topologies},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Deep learning-based method for detection of copy-move forgery in videos. <em>NCA</em>, <em>37</em>(14), 8451-8464. (<a href='https://doi.org/10.1007/s00521-025-11009-8'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Video forgery is one of the most serious problems affecting the credibility and reliability of video content. Therefore, detecting video forgery presents a major challenge for researchers due to the diversity of forgery types, the modernity of the programs used in forgery operations, and the abundance of information and content present in videos. The seriousness of this issue arises from the widespread use of videos in vital fields that require very high accuracy with no room for doubt or error, such as courtrooms, journalism, and others. Copy-move forgery is one of the most common and dangerous types of video forgery because of the difficulty in identifying it with the naked eye and the great diversity of forgery techniques involved in this particular type. One of the biggest challenges researchers have faced in the past is the complexity of the steps required to detect forgery in videos, leading to significant computational complexity and time consumption. The proposed method also aims to achieve better results than previous methods while reducing computational operations. Ultimately, forgery is detected with great efficiency. Compared to previous methods used for detecting copy-move forgery in the Rewind dataset, the proposed method achieves the highest F1, reaching 0.86, with a significant difference of 0.13 compared to the best result of the previous methods.},
  archive      = {J_NCA},
  author       = {Elbarougy, Reda and Abdelfatah, Osama and Behery, G. M. and El-Badry, Noha M.},
  doi          = {10.1007/s00521-025-11009-8},
  journal      = {Neural Computing and Applications},
  month        = {5},
  number       = {14},
  pages        = {8451-8464},
  shortjournal = {Neural Comput. Appl.},
  title        = {Deep learning-based method for detection of copy-move forgery in videos},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Machine learning to predict gut microbiomes of agricultural pests. <em>NCA</em>, <em>37</em>(14), 8435-8449. (<a href='https://doi.org/10.1007/s00521-025-10999-9'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {While current efforts to control agricultural insect pests largely focus on the widespread use of insecticides, predicting microbiome composition can provide important data for creating more efficient and long-lasting pest control methods by analysing the pest’s food-digesting capacity and resistance to bacteria or viruses. We aim to develop a machine learning model to predict the microbiome composition in agricultural pests and investigate the dynamics of these microbiome compositions using metagenomic samples taken from fruit flies. In this paper, we propose three machine learning-based biological models. Firstly, we propose an intrafamilial model that predicts the relative abundance of bacterial families within themselves using their past generations. Next, we propose two interfamilial models following quantitative and qualitative approaches. The quantitative model predicts the number of bacterial families in a given sample based on the presence of other families in that sample. The qualitative model predicts the relative abundance using binary information of all bacterial families. All three models were tested against least angle regression, random forest, elastic-net, and Lasso. The third approach exhibits promising results by applying a random forest with the lowest mean coefficient of variance of 1.25. The overall results of this study highlight how complex these dynamic systems are and demonstrate that more computationally efficient methods can characterise them quickly. The results of this study are intended to be used as a tool to identify vital taxological families, genera and species of the potential microbiome for better pest control.},
  archive      = {J_NCA},
  author       = {Jobayer, Md and Taylor, Alexander and Hasan, Md Rakibul and Ahmed, Khandaker Asif and Hossain, Md Zakir},
  doi          = {10.1007/s00521-025-10999-9},
  journal      = {Neural Computing and Applications},
  month        = {5},
  number       = {14},
  pages        = {8435-8449},
  shortjournal = {Neural Comput. Appl.},
  title        = {Machine learning to predict gut microbiomes of agricultural pests},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Dual-method for semantic and instance brain tumor segmentation based on UNet and mask R-CNN using MRI. <em>NCA</em>, <em>37</em>(14), 8415-8433. (<a href='https://doi.org/10.1007/s00521-025-11013-y'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Brain tumor detection is a challenge because of the fuzzy growth. The irregular boundaries of tumors lead to inaccurate segmentation of brain tumors. Therefore, to overcome such challenges, a generic segmentation method is proposed named as Javeria Amin segmentation method (JASM) which consists of two phases. The U-network segmentation model is proposed using selected hyperparameters 16 batch-size, 1e-4 learning rate, and 200 epochs for core tumor segmentation. In the second phase, the framework is designed based on the regional proposal network (RPN) model used as the backbone of the pre-trained ResNet-50 model for segmenting core and edema regions. The selected hyperparameters are used to train the framework. Both models are trained on different views of axial, coronal, and sagittal, which help to detect severe brain tumors more accurately. This work is evaluated on AJDBS-2023, Figshare Brain Tumor, BRTAS-2020, and BRTAS-2021 publicly available segmentation datasets. The proposed method provides an Average Dice Score of 0.92 ± 0.015 on AJDBS-2023, 0.92 ± 0.005 on Figshare Brain Tumor, 0.91 ± 0.09 on BRTAS-2020, 0.91 ± 0.004 on BRTAS-2021 datasets. The results prove that the segmentation method can detect the small core tumor region accurately. This method is also partially tested on real patient data in the Hospital under the supervision of expert radiologists that authenticate the contribution of this work based on good performance.},
  archive      = {J_NCA},
  author       = {Amin, Javaria and Gul, Nadia and Sharif, Muhammad},
  doi          = {10.1007/s00521-025-11013-y},
  journal      = {Neural Computing and Applications},
  month        = {5},
  number       = {14},
  pages        = {8415-8433},
  shortjournal = {Neural Comput. Appl.},
  title        = {Dual-method for semantic and instance brain tumor segmentation based on UNet and mask R-CNN using MRI},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multi-domain feature-enhanced attribute updater for generalized zero-shot learning. <em>NCA</em>, <em>37</em>(14), 8397-8414. (<a href='https://doi.org/10.1007/s00521-025-11005-y'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The goal of generalized zero-shot learning (GZSL) is to transfer knowledge from seen classes to unseen classes. However, a significant challenge is the single-category attributes are often inadequate to capture the intra-class variability of diverse image samples. During training, the model tends to cluster samples of seen categories based solely on semantic descriptions, overlooking subtle yet important visual differences. To address these challenges, this paper introduces an instance-level attribute updating method, termed as multi-domain feature-enhanced attribute updater (MDAU), which establishes a one-to-one alignment between visual and semantic by obtaining multiple instance-level attributes that closely correspond to the natural intra-class distribution of the image, in contrast to the many-to-one alignment typically used in traditional GZSL approaches. Specifically, we initially compute feature weights for each individual image, and then, the visual information is incorporated as conditions to facilitate the personalized updating of class attributes. We evaluate our model on three benchmark datasets, the results show that our model can achieve the state-of-the-art performance and detailed analysis also illustrates the effectiveness of each proposed module. Code is available at https://github.com/S-Silvia/MDAU .},
  archive      = {J_NCA},
  author       = {Shi, Yuyan and Jiang, Chenyi and Song, Feifan and Ye, Qiaolin and Long, Yang and Zhang, Haofeng},
  doi          = {10.1007/s00521-025-11005-y},
  journal      = {Neural Computing and Applications},
  month        = {5},
  number       = {14},
  pages        = {8397-8414},
  shortjournal = {Neural Comput. Appl.},
  title        = {Multi-domain feature-enhanced attribute updater for generalized zero-shot learning},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). New comparative approach to multi-level thresholding: Chaotically initialized adaptive meta-heuristic optimization methods. <em>NCA</em>, <em>37</em>(14), 8371-8396. (<a href='https://doi.org/10.1007/s00521-025-11016-9'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {One method aimed at enhancing the performance of meta-heuristic optimization techniques is the incorporation of chaotic systems. Instead of irregular distributions in the search space, chaotic distributions are employed in the initial population of optimization algorithms to improve the efficiency of the search process. This approach enables search agents distributed in a chaotic manner to effectively explore the search space. The initial populations of both the well-established PSO algorithm and the enhanced WSO algorithm, which incorporates advanced search techniques, are distributed in the search space according to the characteristics of Logistic, Chebyshev, Circle, Sine, and Piecewise chaotic maps in this study. The original PSO and WSO algorithms, as well as the resulting chaotically initialized PSO and chaotically initialized WSO algorithms, were tested using 23 benchmark functions. Subsequently, the Otsu method was integrated into the tested optimization algorithms to obtain multi-level thresholding values. These algorithms were applied to five different test images with a manually determined number of thresholds. The results obtained were presented in the study and evaluated using statistical tests.},
  archive      = {J_NCA},
  author       = {Serbet, Fatmanur and Kaya, Turgay},
  doi          = {10.1007/s00521-025-11016-9},
  journal      = {Neural Computing and Applications},
  month        = {5},
  number       = {14},
  pages        = {8371-8396},
  shortjournal = {Neural Comput. Appl.},
  title        = {New comparative approach to multi-level thresholding: Chaotically initialized adaptive meta-heuristic optimization methods},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Advancing ASL kidney image registration: A tailored pipeline with VoxelMorph. <em>NCA</em>, <em>37</em>(14), 8347-8369. (<a href='https://doi.org/10.1007/s00521-025-11000-3'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In clinical renal assessment, image registration plays a pivotal role, as patient movement during data acquisition can significantly impede image post-processing and the accurate estimation of hemodynamic parameters. This study introduces a deep learning-based image registration framework specifically for arterial spin labeling (ASL) imaging. ASL is a magnetic resonance imaging technique that modifies the longitudinal magnetization of blood perfusing the kidney using a series of radiofrequency pulses combined with slice-selective gradients. After tagging the arterial blood, label images are captured following a delay, allowing the tagged blood bolus to enter the renal tissue, while control images are acquired without tagging the arterial spins. Given that perfusion maps are generated at the pixel level by subtracting control images from label images and considering the relatively small signal intensity difference, precise alignment of these images is crucial to minimize motion artefacts and prevent significant errors in perfusion calculations. Moreover, due to the extended ASL acquisition times and the anatomical location of the kidneys, renal images are often susceptible to pulsation, peristalsis, and breathing motion. These motion-induced noises and other instabilities can adversely affect ASL imaging outcomes, making image registration essential. However, research on renal MRI registration, particularly with respect to learning-based techniques, remains limited, with even less focus on renal ASL. Our study proposes a learning-based image registration approach that builds upon VoxelMorph and introduces groupwise inference as a key enhancement. The dataset includes 2448 images of transplanted kidneys (TK) and 2456 images of healthy kidneys (HK). We compared the automatic image registration results with the widely recognized optimization method Elastix. The model’s performance was evaluated using the mean structural similarity index (MSSIM), normalized correlation coefficient (NCC), temporal signal-to-noise ratio (TSNR) of the samples, and the mean cortical signal (CSIM) in perfusion-weighted images, thereby extending the evaluation beyond traditional similarity-based metrics. Our method achieved superior image registration performance, with peak NCC (0.987 ± 0.006) and MSSIM (0.869 ± 0.048) values in the kidney region, significantly surpassing Elastix and the unregistered series (p < 0.05) on TK and HK datasets. Regularization analysis showed that higher λ values (1, 2) produced smoother deformation fields, while moderate λ values (0.5, 0.9) balanced smoothness and detail, maintaining low non-positive Jacobian percentages (<1%) comparable to Elastix. Additionally, our method improved CSIM by 14.3% (2.304 ± 1.167) and TSNR by 13.1% (3.888 ± 2.170) in TK, and achieved up to 13.2% (CSIM) and 29.8% (TSNR) enhancements in HK, demonstrating robustness and improved signal quality across datasets and acquisition techniques.},
  archive      = {J_NCA},
  author       = {Oyarzun-Domeño, Anne and Cia, Izaskun and Echeverria-Chasco, Rebeca and Fernández-Seara, María A. and Martin-Moreno, Paloma L. and Garcia-Fernandez, Nuria and Bastarrika, Gorka and Navallas, Javier and Villanueva, Arantxa},
  doi          = {10.1007/s00521-025-11000-3},
  journal      = {Neural Computing and Applications},
  month        = {5},
  number       = {14},
  pages        = {8347-8369},
  shortjournal = {Neural Comput. Appl.},
  title        = {Advancing ASL kidney image registration: A tailored pipeline with VoxelMorph},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Intelligent recognition of steel plate surface defect based on deep convolutional GAN. <em>NCA</em>, <em>37</em>(14), 8331-8345. (<a href='https://doi.org/10.1007/s00521-025-11007-w'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The production of hot rolled strip steel plates is affected by various uncertainties, resulting in numerous defects on the steel plate surface, such as scratches, cracks, and inclusions, which significantly impact the surface quality and performance of the steel plates. Considering the high rates of missed and false detections by traditional manual visual inspections, along with the limited accuracy of existing defect recognition algorithms due to insufficient defect sample data, a novel intelligent identification method is proposed for the recognition of steel plate surface defects based on deep convolutional generative adversarial network (DCGAN). Aiming at the problem of insufficient samples of actual industrial surface defect images, the proposed method generates steel plate surface image data with a similar distribution to real data based on DCGAN, which will expand the original sample data. Moreover, batch normalization is used to process the data, and an appropriate activation function is selected to better combine the original GAN and the deep convolutional neural network (DCNN) model in structure, thereby effectively improving the quality of the generated image data. Finally, the extended dataset is used to train the DCNN, and the recognition accuracy of the trained model on the test set is 96.16%. To the high error rate of manual inspection and the difficulty of creating and labeling datasets, the experimental results on actual industrial data demonstrate that the proposed method has good engineering application for the recognition of steel plate surface defects.},
  archive      = {J_NCA},
  author       = {Jiang, Benyi and Zhou, Ping and Sun, Xiaoyang and Chai, Tianyou},
  doi          = {10.1007/s00521-025-11007-w},
  journal      = {Neural Computing and Applications},
  month        = {5},
  number       = {14},
  pages        = {8331-8345},
  shortjournal = {Neural Comput. Appl.},
  title        = {Intelligent recognition of steel plate surface defect based on deep convolutional GAN},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Spatial attention-based hybrid VGG-SVM and VGG-RF frameworks for improved cotton leaf disease detection. <em>NCA</em>, <em>37</em>(14), 8309-8329. (<a href='https://doi.org/10.1007/s00521-025-11012-z'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The agriculture industry's production and food quality have been impacted by plant leaf diseases in recent years. Hence, it is vital to have a system that can automatically identify and diagnose diseases at an initial stage for enhancing the quality of agricultural output and also for stopping the overall plant extinction. Most state-of-the-art methods do not provide appropriate accuracy of identification when the similarity among different diseases is higher and the input leaf images have complex background. In this work, a transfer learning-based automated crop disease recognition system is proposed to handle this problem. The cotton leaf disease detection framework being proposed here includes two key stages: (1) the feature extraction stage, where the convolutional neural network (CNN) model from the visual geometry group (VGG-16) is employed to obtain the deep features from cotton leaf images with complex background. (2) Cotton leaf disease detection stage: Here, the extracted optimal features are input to the machine learning classifiers, namely random forest (RF) and nonlinear support vector machine (SVM), for the initial classification of the diseases. For providing an optimal cotton crop disease detection performance, Kaggle’s four-class cotton disease dataset with complex background is chosen in this work. For the final decision making and analysis, two hybrid models named spatial attention-based hybrid VGG-RF and hybrid VGG-SVM have been proposed in this work for enhancing the decision accuracy. These hybrid models offered an average accuracy of approximately 98.29% and 99.31%, respectively, which is higher than the accuracy provided by the related work. This proved that the proposed hybrid model, namely spatial attention-based VGG-SVM and VGG-RF models, with spatial attention outperformed the state-of-the-art classifiers.},
  archive      = {J_NCA},
  author       = {Pandiyaraju, V. and Anusha, B. and Senthil Kumar, A. M. and Jaspin, K. and Venkatraman, Shravan and Kannan, A.},
  doi          = {10.1007/s00521-025-11012-z},
  journal      = {Neural Computing and Applications},
  month        = {5},
  number       = {14},
  pages        = {8309-8329},
  shortjournal = {Neural Comput. Appl.},
  title        = {Spatial attention-based hybrid VGG-SVM and VGG-RF frameworks for improved cotton leaf disease detection},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Small sample learning based on probability-informed neural networks for SAR image segmentation. <em>NCA</em>, <em>37</em>(14), 8285-8308. (<a href='https://doi.org/10.1007/s00521-025-10997-x'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The paper introduces a probability-informed methodology for the segmentation of synthetic aperture radar (SAR) images in the case of small sample learning. It assumes that the amount of training data is limited to several hundred or thousand elements, which prevents the effective training of state-of-the-art neural network (NN) models. This is a typical problem for real SAR images whose characteristics depend significantly on the sensors used to produce them and cannot always be repeated within open available datasets. To solve this problem, we propose NN models called Probability-Informed Neural Networks (PrINNs). As part of our approach, we introduce the use of probability models as a source of additional features for data. Specifically, the training dataset is enriched by modeling the pixel brightness using a finite normal mixture. We prove that such an extension can reduce errors in the learning process theoretically. The resulting enriched dataset is segmented using attention-based convolutional NNs or visual transformers. Then, post-processing is implemented based on another probability model—quadtree, which is a special case of random Markov fields. As we have theoretically demonstrated, this part of PrINNs is analogous to the graph-convolutional NNs with fixed weights. Using open SAR images obtained by different radars (namely, Sentinel-1, Capella, ESAR and HRSID) with various types of underlying surfaces, the possibility of improving segmentation quality based on PrINNs is demonstrated. We tested various combinations of methods from the PrINNs architecture, and in all cases, the PrINN approach we proposed was superior to any other combination of these methods. From the point of view of the achieved accuracy metrics, the mean $$F_1$$ score increased up to $$19.24\%$$ , and the median $$F_1$$ score was improved up to $$9.57\%$$ . Some further architectural improvements to PrINNs are also discussed in the paper.},
  archive      = {J_NCA},
  author       = {Dostovalova, Anastasia and Gorshenin, Andrey},
  doi          = {10.1007/s00521-025-10997-x},
  journal      = {Neural Computing and Applications},
  month        = {5},
  number       = {14},
  pages        = {8285-8308},
  shortjournal = {Neural Comput. Appl.},
  title        = {Small sample learning based on probability-informed neural networks for SAR image segmentation},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). FedLRes: Enhancing lung cancer detection using federated learning with convolution neural network (ResNet50). <em>NCA</em>, <em>37</em>(14), 8273-8284. (<a href='https://doi.org/10.1007/s00521-025-11006-x'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Lung cancer remains a leading global cause of mortality, necessitating efficient early detection. Lung cancer image analysis plays a pivotal role, yet current manual segmentation by oncologists is laborious. Our innovative (FedLRes) approach proposes a comprehensive system for automated diagnoses of lung cancer using federated learning with ResNet50. The lung cancer dataset from the Iraq-Oncology Teaching Hospital/National Centre for Cancer Diseases was gathered by the IQ-OTH/NCCD. The dataset consists of 1097 images, split into training (822 images) and validation (275 images) sets. The training set includes 312 normal, 420 malignant, and 90 benign cases. The training set's data are further enhanced by using data augmentation techniques, which are then applied to normalise images differences before being fed into the federated learning with ResNet50 architecture. This approach combines deep learning models trained on different datasets allowing for improvising accuracy and generalisation. The federated learning approach enables the use of distributed data while ensuring data privacy and security. The proposed approach compared with different state of art algorithms. Through rigorous experimentation, our system showcases remarkable advancements a classification accuracy of 99.40%. This innovative approach, utilising 3D input CT scan data, offers a potent and precise tool for early detection and effective treatment strategies against the scourge of lung cancer.},
  archive      = {J_NCA},
  author       = {Usharani, C. and Selvapandian, A.},
  doi          = {10.1007/s00521-025-11006-x},
  journal      = {Neural Computing and Applications},
  month        = {5},
  number       = {14},
  pages        = {8273-8284},
  shortjournal = {Neural Comput. Appl.},
  title        = {FedLRes: Enhancing lung cancer detection using federated learning with convolution neural network (ResNet50)},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A sturdy CNN-based model for ambience recognition of acoustic communication. <em>NCA</em>, <em>37</em>(14), 8261-8271. (<a href='https://doi.org/10.1007/s00521-025-10996-y'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This research presents a robust convolutional neural network (CNN)-based technique for identifying the ambience of acoustic communication. The study is motivated by the need for accurate categorization of acoustic environments, which has applications in various fields such as security, entertainment, and health monitoring. The purpose of this research is to develop a model that can classify acoustic communications into specific emotional categories with high accuracy. Initially, preprocessing steps are applied to the acoustic communication data, including lowercase conversion, tokenization, padding sequence, and word embedding. These steps produce a feature matrix that serves as the input for a multi-level CNN. The proposed model is designed so that each CNN layer reduces the input matrix to half its size, with the output of one layer being combined with the next through average pooling. This process, referred to as a global pass, is repeated across multiple blocks of the same architecture. The concatenated global outputs and the output from the previous CNN layer are then passed into two fully connected layers with 512 and 8 neurons, respectively. The final output from the second fully connected layer (FC(8)) provides the probabilities for the communication to fall into one of the eight predefined categories: tensed, peaceful, chaotic, joyful, sorrowful, excited, religious, and unhealthy. The proposed model demonstrates superior performance with a 95.90% accuracy rate, outperforming state-of-the-art machine learning techniques. The comparative analysis shows highly encouraging results, suggesting the effectiveness of the model in practical applications. This study contributes to the advancement of acoustic communication analysis by providing a reliable and efficient classification method.},
  archive      = {J_NCA},
  author       = {Rathor, Sandeep},
  doi          = {10.1007/s00521-025-10996-y},
  journal      = {Neural Computing and Applications},
  month        = {5},
  number       = {14},
  pages        = {8261-8271},
  shortjournal = {Neural Comput. Appl.},
  title        = {A sturdy CNN-based model for ambience recognition of acoustic communication},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Sufficient learning: Mining denser high-quality pixel-level labels for edge detection. <em>NCA</em>, <em>37</em>(14), 8245-8260. (<a href='https://doi.org/10.1007/s00521-024-10661-w'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The success of deep neural networks can largely be attributed to large-scale datasets with accurate annotations. In many practical applications, labels are annotated by multiple annotators, resulting in ambiguous labels. To mitigate the impact of such labels, existing deep edge detectors tend only to exploit sparsely reliable samples with high-confident labels by simple voting mechanism. However, lacking sufficient pixel-level learnable samples would severely degrade detection performance. To combat this issue, this work designs a sufficient learning framework for edge detection (SLED) to sufficiently explore reliable samples for model learning, which consists of three stages: sample selection, label refinement, and retraining. Firstly, we propose a sample selection scheme founded on history prediction, which aims to identify more reliable samples and locate unreliable regions containing potentially reliable samples. Secondly, we design an efficient label refinement strategy based on a graph convolutional network (GCN) to propagate reliable label information to unreliable nodes. This strategy can generate "new training targets" with denser, reliable labels for all images. Finally, we retrain the network with these new targets, obtaining a more robust model. Experimental results on three widely used benchmarks show that the proposed detector outperforms all compared SOTA methods. Our code will be released at https://github.com/wenya1994/SLED .},
  archive      = {J_NCA},
  author       = {Chen, Xiaodiao and Yang, Wenya and Wu, Wen and Tao, Xiuting and Mao, Xiaoyang},
  doi          = {10.1007/s00521-024-10661-w},
  journal      = {Neural Computing and Applications},
  month        = {5},
  number       = {14},
  pages        = {8245-8260},
  shortjournal = {Neural Comput. Appl.},
  title        = {Sufficient learning: Mining denser high-quality pixel-level labels for edge detection},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Design and analysis of a dual-output multidirectional 3D printing system through intelligent robots. <em>NCA</em>, <em>37</em>(13), 8233-8244. (<a href='https://doi.org/10.1007/s00521-022-07936-5'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, with the development of 3D printing technology and the continuous innovation of printing materials and equipment, the moulding speed and accuracy of printed products have also been improved continuously, and the printed products have been widely used in the fields of automobiles, aviation, medical treatment, and education. As an emerging manufacturing process, 3D printing technology has built a bridge between design and product. Driven by the digital model, 3D printing technology is evaluated by kinematic analysis, dexterity analysis, and the influence analysis of the length of the connecting rod on the volume of the working space. The 3D printing technology does not need to consider the complex processing technology in traditional mechanical manufacturing and can quickly form parts of any complex shape, shortening the product development cycle. When forming parts, there is almost no waste and save materials. The conductive wire material used is nanosilver paste with a solid content of 70 wt% and a layering speed of 8 mm/s, and the oblique needle has an inner diameter of 400 μm, a feed pressure of 70 psi, and a line speed of 12 mm/s, which reduces the production cost. This research is of great significance for improving the level of the manufacturing industry.},
  archive      = {J_NCA},
  author       = {Huo, Jiaofei and Ren, Wei},
  doi          = {10.1007/s00521-022-07936-5},
  journal      = {Neural Computing and Applications},
  month        = {5},
  number       = {13},
  pages        = {8233-8244},
  shortjournal = {Neural Comput. Appl.},
  title        = {Design and analysis of a dual-output multidirectional 3D printing system through intelligent robots},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A three-tiered semi supervised MTL mechanism and its application in dating apps. <em>NCA</em>, <em>37</em>(13), 8217-8231. (<a href='https://doi.org/10.1007/s00521-022-08081-9'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A thorough understanding of the purpose of dating applications is crucial for service providers in order to optimize the design and user experience of the application. Despite the fact that many APPs prompt users to provide their usage purpose, many do not reveal this attribute. In this study, a three-module framework with semi-supervised and multitask learning mechanisms is proposed (T-SSMTL). Using the T-SSMTL mechanism, the purpose of the dating APP usage can be automatically inferred from the publicly available heterogeneous data of the user. The heterogeneous feature extraction module employs a number of techniques to extract semantic representations, maximizing the use of heterogeneous dating APP data. The multi-task module extracts task-specific knowledge for learning and solves the classification problem involving multiple labels. To alleviate the problem of label insufficiency, the semi-supervised module utilizes a large quantity of unlabeled data generated by users who do not report their usage purpose. A large-scale dataset containing 34,364 active dating APP users with their self-reported usage purpose, portrait image, profile, and posts was collected to evaluate the T-SSMTL framework. In the context of this dataset, simulation experiments have confirmed the efficacy of all three modules of the T-SSMTL framework, demonstrating its substantial theoretical significance as well as its excellent application value.},
  archive      = {J_NCA},
  author       = {Ma, Junyi and Wang, Yasha and Wang, Xuanliang and Wang, Jiangtao and Zhao, Junfeng},
  doi          = {10.1007/s00521-022-08081-9},
  journal      = {Neural Computing and Applications},
  month        = {5},
  number       = {13},
  pages        = {8217-8231},
  shortjournal = {Neural Comput. Appl.},
  title        = {A three-tiered semi supervised MTL mechanism and its application in dating apps},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Posture positioning estimation for players based on attention mechanism and hierarchical context. <em>NCA</em>, <em>37</em>(13), 8205-8216. (<a href='https://doi.org/10.1007/s00521-022-07800-6'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As a basic method to study human motion, pose estimation has become a research hotspot in the field of computer vision. Its main task is to detect the coordinate positions of human joints and key parts in the image, so as to obtain partial or all limb information of the human body, so as to judge the behavior. For the issue of complicated background, non-rigid changes in posture and low efficiency of posture estimation model in athletes’ posture positioning estimation methods, this work proposes a new posture positioning strategy. Based on the attention mechanism, this work designs a large receptive field hourglass attention network and a large receptive field residual module to improve the traditional residual module. The large receptive field residual module expands the effective receptive field area, which can enable the model to more effectively use image’s multi-scale information, and improves the accuracy and robustness of attitude estimation. Second, an efficient human pose estimation model framework is proposed based on the context learning. The key point range area and the specific local area are, respectively, input into the model, and layer-by-layer prediction is performed, which realizes that the model can be efficiently trained and deployed while retaining strong generalization capabilities. The large receptive field hourglass attention network is used as the stage backbone network of the hierarchical context network to achieve a balance between model accuracy and efficiency. Finally, comprehensive and systematic experiments are carried out to verify the superiority and feasibility of the method designed in this work.},
  archive      = {J_NCA},
  author       = {Zhang, Lulu and Qi, Tianyi and Tao, Linrong},
  doi          = {10.1007/s00521-022-07800-6},
  journal      = {Neural Computing and Applications},
  month        = {5},
  number       = {13},
  pages        = {8205-8216},
  shortjournal = {Neural Comput. Appl.},
  title        = {Posture positioning estimation for players based on attention mechanism and hierarchical context},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Application of medical clinic system using improved neural network-based image segmentation technique. <em>NCA</em>, <em>37</em>(13), 8193-8203. (<a href='https://doi.org/10.1007/s00521-022-07913-y'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the design of medical clinical systems, the segmentation and recognition of medical images are important factors affecting the usability effect of the system. Further research and analysis are needed to improve the segmentation effect of medical images by improving neural network technology. In this paper, a new medical image segmentation technique is designed by improving neural networks and introducing an attention mechanism. After the medical image is convoluted, the image feature information enters the channel attention mechanism module. Each feature channel has a weight. The larger the weight, the greater the correlation between the feature and the channel. The smaller the weight, the smaller the correlation between the feature and the channel. This paper analyses the system design objectives, analyses the requirements of the medical clinical system, and designs the software architecture composed of the client layer, the data layer, and the application. For example, the network architecture is mainly composed of the client and the server. The functional modules mainly include six modules: user management module, department use module, equipment maintenance module, budget management module, asset management module, and medical equipment regulations module. Lastly, the choice of hardware and the design of the database for the medical clinical system are explained in detail.},
  archive      = {J_NCA},
  author       = {Wu, Fenglang and Liu, Xinran and Wang, Yudan and Li, Xiaoliang and Zhou, Ming},
  doi          = {10.1007/s00521-022-07913-y},
  journal      = {Neural Computing and Applications},
  month        = {5},
  number       = {13},
  pages        = {8193-8203},
  shortjournal = {Neural Comput. Appl.},
  title        = {Application of medical clinic system using improved neural network-based image segmentation technique},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Performance optimization of computing task scheduling based on the hadoop big data platform. <em>NCA</em>, <em>37</em>(13), 8181-8192. (<a href='https://doi.org/10.1007/s00521-022-08114-3'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Hadoop, a distributed computing framework that can efficiently process large-scale datasets, has been used by an increasing number of organizations as the basic computing framework to build cloud computing platforms. Improving its execution efficiency is a hot research direction in the industry, and the scheduling problem is a key factor affecting the execution efficiency of Hadoop. It is very important to identify its shortcomings and improve them. This paper examines and analyses the optimization of computing task scheduling performance based on the Hadoop big data platform. This paper first analyses Hadoop big data processing. Hadoop has high scalability. Computing nodes can be added at any time, and they can participate in cluster work through simple configuration. The paper discusses the improvement in the Hadoop resource scheduling algorithm. The task scheduling algorithm in the Hadoop-based data task localization proposed in this paper is compared with the default algorithm used in the Hadoop task scheduling algorithm. The former shows better local data in all four jobs, there are more data localization tasks, and the expected goal is achieved. The effectiveness of the algorithm is verified, and the performance is improved by 30%.},
  archive      = {J_NCA},
  author       = {Li, Yang and Hei, Xinhong},
  doi          = {10.1007/s00521-022-08114-3},
  journal      = {Neural Computing and Applications},
  month        = {5},
  number       = {13},
  pages        = {8181-8192},
  shortjournal = {Neural Comput. Appl.},
  title        = {Performance optimization of computing task scheduling based on the hadoop big data platform},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). The dual-credit policy model, a production strategy decision-making algorithm and application to chinese automakers. <em>NCA</em>, <em>37</em>(13), 8165-8179. (<a href='https://doi.org/10.1007/s00521-022-07703-6'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The implementation of the dual-credit policy has imposed environmental and economic pressures on Chinese automakers. Based on the principle of single environmental risk classification and an automatic identification algorithm, this study proposes a decision-making algorithm for automakers’ production strategies under the dual-credit policy. Considering the competitive relationship between fuel vehicles and new energy vehicles, three production strategies of automakers are constructed: strategy B (producing both fuel vehicles and new energy vehicles), Strategy I (producing only fuel vehicles), and Strategy N (producing only new energy vehicles). The results obtained show that automakers’ production strategy choices follow both low and high thresholds. When automakers produce high fuel consumption-type vehicles, the government increases its target for the new energy vehicle production ratio of fuel vehicles, and the corporate average fuel consumption standard will promote an increase in both thresholds and promote the transition of Strategy I to Strategy B and of Strategy B to Strategy N. When automakers produce low fuel consumption-type vehicles, the government increases its target for the new energy vehicle production ratio of fuel vehicles, which can also promote the transition of fuel vehicle automakers to the production of new energy vehicles. However, the government’s increase in the corporate average fuel consumption standard has no effect on the production strategy. Additionally, the implementation of the dual-credit policy cannot improve the fuel economy improvement level of fuel vehicles, but it can restrain the market share of fuel vehicles. Controlling reasonable efforts to improve the mileage range of automakers can improve the profit of automakers producing new energy vehicles.},
  archive      = {J_NCA},
  author       = {Zhu, Xiaodong and Ding, Lian and Zhu, Huiting and Guo, Yajie},
  doi          = {10.1007/s00521-022-07703-6},
  journal      = {Neural Computing and Applications},
  month        = {5},
  number       = {13},
  pages        = {8165-8179},
  shortjournal = {Neural Comput. Appl.},
  title        = {The dual-credit policy model, a production strategy decision-making algorithm and application to chinese automakers},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Fully connected network samples transfer and multi-classifier fusion for motor imagery recognition. <em>NCA</em>, <em>37</em>(13), 8153-8164. (<a href='https://doi.org/10.1007/s00521-022-07748-7'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the field of motor imagery (MI) recognition, there are two problems, which are poor generalization and low recognition performance. A method of MI recognition method based on fully connected network (FCN) samples transfer and multi-classifier fusion is proposed in this study. The distribution similarity-based source domain selection method seeks source domains with marginal distributions that are similar to the target domain. Then, an FCN-based samples transfer method is proposed for transferring suitable samples from similar source domains to the target domain with similar conditional distributions. Additionally, a method for MI recognition is proposed that is based on the fusion of multiple classifiers. The classifiers are trained on labeled samples from the target domain as well as samples transferred from similar source domains. The new sample in the target domain can be identified using the weight fusion of the results of these classifiers. To evaluate the proposed method’s effectiveness, four types of MI from the brain–computer interface competition IV dataset 2A were used to evaluate the recognition ability, and the results confirmed excellent recognition and generalization performance compared to commonly used methods nowadays.},
  archive      = {J_NCA},
  author       = {Cheng, Sihui and Gao, Chang},
  doi          = {10.1007/s00521-022-07748-7},
  journal      = {Neural Computing and Applications},
  month        = {5},
  number       = {13},
  pages        = {8153-8164},
  shortjournal = {Neural Comput. Appl.},
  title        = {Fully connected network samples transfer and multi-classifier fusion for motor imagery recognition},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Design and simulation of precision marketing recommendation system based on the NSSVD++ algorithm. <em>NCA</em>, <em>37</em>(13), 8139-8152. (<a href='https://doi.org/10.1007/s00521-023-08302-9'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article is dedicated to discussing the design and improvement of precision marketing push algorithm under the deep learning target detection system. Scientific progress has made the network form a huge database, and each software platform can accurately locate the user’s market by analyzing the data, determine their interests, and carry out marketing pushes. Based on deep learning algorithms and models, simulation training can be performed on user rating data. It clusters users according to their preferences and performs data filtering to control the impact of data sparseness or differences between users. It uses the relevant similarity of users and uses neighborhood collaborative filtering to make accurate judgments and push. This article is based on the SVD++ (Singular Value Decomposition, SVD++) algorithm and optimized to achieve higher push accuracy. The previous explained the SDD (Single Shot MultiBox Detector, SDD) algorithm, Pearson correlation similarity, neighborhood-based collaborative filtering model, neural network model, and Rayleigh channel system to explain the application of the deep learning target detection system in precision marketing, then verify the feasibility of the improved SVD++ algorithm through experiments. In the experiment, through the comparative analysis of UserCF (Collaborative Filtering, UserCF), Slope one, SVD++, OrdRec, Pure, AllRank, JSVD++, MSSVD++ (Medical Society for the Study of Venereal Diseases, MSSVD++), NSSVD++ (Neighborhood Sampling Singular Value Decomposition) algorithms, the test focuses on JSVD++, MSSVD++, NSSVD++ algorithms. And it is concluded that the NSSVD++ algorithm, that is, the neighborhood sampling method, is the most effective and has the best marketing recommendation effect. Among them, the 1-Call algorithm is 41.62% higher than the SVD++ algorithm, and more than 16.88% higher than other benchmark algorithms, the COV (Covariance, COV) algorithm is improved by more than 12.97%, the CIL algorithm is improved by more than 16.12%, and the improvement of NSSVD++ is at least 71.37% based on the SVD++ algorithm. The experimental results show that the improved recommendation algorithm has better Top-N recommendation accuracy. Although there is indeed negative case information in the missing data, the direction of this experiment is the right direction. The results of the experiment have certain guiding significance for precision marketing push, and can achieve rapid development in this field.},
  archive      = {J_NCA},
  author       = {Liu, Yishu and Zhang, Wenjian},
  doi          = {10.1007/s00521-023-08302-9},
  journal      = {Neural Computing and Applications},
  month        = {5},
  number       = {13},
  pages        = {8139-8152},
  shortjournal = {Neural Comput. Appl.},
  title        = {Design and simulation of precision marketing recommendation system based on the NSSVD++ algorithm},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Collection and application of building visualisation information based on IoT sensors. <em>NCA</em>, <em>37</em>(13), 8127-8137. (<a href='https://doi.org/10.1007/s00521-022-07912-z'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The visualisation of historical buildings can simulate, reproduce and publicise their vulnerability characteristics and make the protection of ancient buildings rise to the height of digitisation and informationisation, which has important practical significance. The database construction of historical buildings can realise the statistics and query of their basic structural information, environmental parameters, human influence and other data to evaluate their safety in real time. The Internet of Things (IoT) sensor technology can use various sorts of sensors and their networks to accurately transmit historical buildings’ data information so as to obtain the real-time status of historical buildings and monitor their specific objects. This paper expounds the research status and significance of the visualisation and database construction of historical buildings by summarising and analysing the previous research works; explains the development background, current status and future challenges of IoT technology; introduces the methods and principles of architectural data extracting and reading text by main memory mapping; and conducts access interface selection for visualisation. The study results show that the visualisation of historical buildings based on IoT sensors can directly display parameter information and describe the ideal relationship between an ancient building database and its physical environment. It can also use relevant software or programs to simplify and register original point cloud and image data, which is convenient for real-time monitoring and protection of historical buildings. The complete components of an IoTs-based historical building database include targets, sensors, comparators, actuators and feedback; it enhances the manageability of large tables at a physical level, making large tables flexible and scalable. At the same time, it improves their maintenance, backup, restore and query performance, correspondingly reducing the management burden with higher availability. The study results of this paper provide a reference for further research on the visualisation and database construction of historical buildings based on IoT sensors.},
  archive      = {J_NCA},
  author       = {Liu, Ping and Li, Yang},
  doi          = {10.1007/s00521-022-07912-z},
  journal      = {Neural Computing and Applications},
  month        = {5},
  number       = {13},
  pages        = {8127-8137},
  shortjournal = {Neural Comput. Appl.},
  title        = {Collection and application of building visualisation information based on IoT sensors},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Compound encryption of multiple images by utilizing a novel chaos and nonlinear transform. <em>NCA</em>, <em>37</em>(13), 8113-8126. (<a href='https://doi.org/10.1007/s00521-022-07849-3'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The purpose of this paper is to propose a multiple image compound encryption based on nonlinear transforms and a novel chaos algorithm. XOR operation, sequence rearrangement, dot operation, and plaintext-related methods are all supported by our scheme. Multiple images are first unified into one image using the multilayered embedded technique. A diffusion-scrambling-nonlinear transform is then used to encrypt the compound image. Multiple images are combined and encrypted into one image; a new diffusion pattern is employed; and multiple nonlinear transforms are compounded to increase the complexity of the algorithm. Multiple nonlinear transforms are employed to increase the complexity of the cryptosystem, which is formed using a novel chaos theory. In order to verify the effectiveness and feasibility of the algorithm, simulations are developed and some performance indicators are calculated in order to evaluate its security and robustness. In comparison with some of the existing image encryption schemes, the proposed algorithm demonstrates some advantages. The first feature is that image compound encryption has greater time efficiency than single image encryption. As a second feature of the algorithm, the key space is extremely large. Furthermore, the new algorithm is more secure than some known algorithms in terms of overall security. Therefore, the proposed algorithm has the potential to be a useful tool in the field of image processing.},
  archive      = {J_NCA},
  author       = {Tao, Limin and Liang, Xikun and Hu, Bin and Han, Lidong},
  doi          = {10.1007/s00521-022-07849-3},
  journal      = {Neural Computing and Applications},
  month        = {5},
  number       = {13},
  pages        = {8113-8126},
  shortjournal = {Neural Comput. Appl.},
  title        = {Compound encryption of multiple images by utilizing a novel chaos and nonlinear transform},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). An event-based data processing system using kafka container cluster on kubernetes environment. <em>NCA</em>, <em>37</em>(13), 8095-8112. (<a href='https://doi.org/10.1007/s00521-023-08326-1'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Smart manufacturing has become a big trend of a new industrial revolution in the manufacturing industry. The advancement of the Internet of Things has made production more efficient and effective through the automated collecting data system and Big Data technology. Dealing with a large amount of real-time production data will be a significant issue for intelligent manufacturing. This paper uses Apache Kafka’s high-performance, low-latency data stream processing platform to process data collection and store it in the Big Data System. Kafka was deployed through Kubernetes, where it has improved on the architecture’s scalability and applies this architecture to the aerospace manufacturing autoclave. These data are then used to analyze the autoclave equipment anomaly. Testing performed on the Kafka Producer Throughput demonstrates that in the event that all other parameters remain unchanged, the real throughput will increase along with the increase in the throughput limit that is being used. For instance, when the throughput limit is 1.2 million, the maximum throughput of this experiment is reached at 1.13 million transactions per second, while the transfer rate is 552.88 megabytes per second (MB/s). The value of the fetch size parameter is set to 10,48,576 by default (1 M). It takes half a time and a quarter of a time down, and it takes up to 2.5 times the value that was preset before you can witness the change in the parameters that affect the performance. The performance achieves its peak of 1.43 million data transferred per second at a speed of 347.93 megabytes per second, and the performance after that has a tendency to remain consistent.},
  archive      = {J_NCA},
  author       = {Liu, Jung-Chun and Hsu, Ching-Hsien and Zhang, Jia-Hao and Kristiani, Endah and Yang, Chao-Tung},
  doi          = {10.1007/s00521-023-08326-1},
  journal      = {Neural Computing and Applications},
  month        = {5},
  number       = {13},
  pages        = {8095-8112},
  shortjournal = {Neural Comput. Appl.},
  title        = {An event-based data processing system using kafka container cluster on kubernetes environment},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Electromagnetic magnetic compatibility optimization in mobile communication based on machine learning. <em>NCA</em>, <em>37</em>(13), 8083-8093. (<a href='https://doi.org/10.1007/s00521-022-07801-5'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the continuous growth of the number of Internet users, the number of intelligent electronic mobile terminals has also increased rapidly, which has brought huge communication data traffic. At the same time, problems such as network jam and delay need to be solved urgently. Therefore, how to achieve high-speed data transmission mode without delay, ensure communication quality, improve user satisfaction, and minimize communication energy consumption has become a hotspot in the field of mobile communication. This paper takes the optimal configuration of electromagnetic compatibility in mobile characteristics as the research goal, combines machine learning algorithm with mobile communication technology, and makes a correct prediction of further communication behavior by training historical communication data. The support-vector machine algorithm is used to predict the location of data and communication access points in the communication network, and the system cache performance and energy efficiency are evaluated with the help of Markov decision-making process, in order to improve the adaptive management ability of the communication system and improve the system construction of mobile communication network. At the same time, the main problems in the integration of Electromagnetic Magnetic Compatibility (EMC) measurement system, such as time and quality, are analyzed, and the hardware optimization configuration scheme is designed based on the existing measurement standards. When processing high-frequency signals, the connector is regarded as an important part of the electromagnetic transmission line. Through the analysis of electrolyte loss, the transmission characteristics of cables with different lengths are obtained. Finally, the relevant strategies of EMC optimization in mobile communication system are analyzed from the perspectives of connector performance and supplier selection, to provide some reference for realizing more flexible and efficient information sharing services.},
  archive      = {J_NCA},
  author       = {Li, Bo and Nie, Ruirui},
  doi          = {10.1007/s00521-022-07801-5},
  journal      = {Neural Computing and Applications},
  month        = {5},
  number       = {13},
  pages        = {8083-8093},
  shortjournal = {Neural Comput. Appl.},
  title        = {Electromagnetic magnetic compatibility optimization in mobile communication based on machine learning},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Machine learning-based image recognition for rural architectural planning and design. <em>NCA</em>, <em>37</em>(13), 8073-8082. (<a href='https://doi.org/10.1007/s00521-022-07799-w'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the rapid development of China's economy and the accelerated process of urbanization, China's rural construction has also accelerated, and the construction of beautiful villages has become a hot spot. This paper aims at image recognition of buildings based on machine learning and other technologies and puts forward the application and development strategy of rural architectural design. Firstly, this paper determines the image recognition process of rural buildings, including image processing and recognition. The recognition method consists of image feature extraction and image classification of architectural drawings. The extraction of image contour features adopts HOG algorithm. The trained SVM classifier is used to classify the image features, and finally the building recognition can be completed. In order to improve the accuracy of recognition, this paper proposes a new recognition and authentication model Recog-Net, which first extracts the image features of rural buildings and then completes the building image authentication through algorithm classification. After comparing multi-angle analysis with single-angle recognition, this paper improves the building image recognition method in significant area. The results show that the accuracy of building recognition has been significantly improved. We discussed the residential layout of rural buildings from three aspects: function, economy, and space environment, and puts forward the influencing factors and the best range index. Finally, for the new rural architectural design, this paper puts forward some development strategies, such as paying attention to the overall regional planning and development, designing reasonable street space, building diversified rural space functions and so on, to help the rural architectural design and planning.},
  archive      = {J_NCA},
  author       = {Wang, Yansong and Hu, Xian},
  doi          = {10.1007/s00521-022-07799-w},
  journal      = {Neural Computing and Applications},
  month        = {5},
  number       = {13},
  pages        = {8073-8082},
  shortjournal = {Neural Comput. Appl.},
  title        = {Machine learning-based image recognition for rural architectural planning and design},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Cartoon art style rendering algorithm based on deep learning. <em>NCA</em>, <em>37</em>(13), 8061-8072. (<a href='https://doi.org/10.1007/s00521-022-07850-w'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To improve cartoon art style rendering systems, increase their adaptability to different front-end rendering requirements, and shorten the waiting time of simultaneous multiuser online renderings, the author proposes an improved algorithm based on deep learning. In this paper, the advantages and disadvantages of the existing neural network rendering model of deep learning machines are comprehensively analyzed, and the design of the rendering algorithm is improved on this basis. The algorithm training link adopts the TensorFlow framework. In the generation link, the algorithm first reads the cartoon picture and carries out the corresponding convolution operation through a convolutional neural network in forward transmission mode. This effect can be achieved by setting the stripe displacement distance to 1 and the convolution kernel size to 3 × 3 and filling 1 layer of 0 at the edge. The analysis results show that the background of the whole website follows the operation law of the asynchronous process pool, which can better improve the rendering response speed. After performing runs and tests, it is found that the website can basically achieve high-efficiency interactions, the application of the new algorithm greatly shortens the development cycle, and the coupling degree between the modules of the rendering webpages is low, which can guarantee the smooth operation and maintenance of the system.},
  archive      = {J_NCA},
  author       = {Shi, Yujie and Wang, Baoqing},
  doi          = {10.1007/s00521-022-07850-w},
  journal      = {Neural Computing and Applications},
  month        = {5},
  number       = {13},
  pages        = {8061-8072},
  shortjournal = {Neural Comput. Appl.},
  title        = {Cartoon art style rendering algorithm based on deep learning},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Intelligent water resources management platform for precision irrigation agriculture based on internet of things. <em>NCA</em>, <em>37</em>(13), 8051-8060. (<a href='https://doi.org/10.1007/s00521-022-07902-1'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {When designing an agricultural intelligent water resources management platform, plant growth information, temperature and humidity, and other environmental information have an important impact on the effect of precision irrigation. How to build a more accurate agricultural irrigation control system and design an intelligent water resources management platform need to be further studied. This paper introduces the definition and characteristics of Internet of Things technology, explains the principle of the PID control algorithm in detail, and uses the method based on digital image processing to analyse crop growth information. Before image analysis, the image needs to be preprocessed, and the soil temperature and humidity measurement control algorithm model is constructed. By using fuzzy control logic, the precision irrigation strategy is designed according to the collected temperature, humidity, and plant water shortage. This paper analyses the requirements of the platform, designs functional modules such as data sensing modules, communication transmission modules, central processing modules, and mechanical control modules, and introduces the hardware system and software system of the platform. Finally, the response time of temperature and humidity sensors under different acquisition modes is tested and analysed. Through a multi-hop network experiment, the relationship between success rate and distance, communication distance, and RSSI value is tested, which verifies the stability and reliability of the communication network of the whole system.},
  archive      = {J_NCA},
  author       = {Haiyan, Zheng and Yanhui, Cheng},
  doi          = {10.1007/s00521-022-07902-1},
  journal      = {Neural Computing and Applications},
  month        = {5},
  number       = {13},
  pages        = {8051-8060},
  shortjournal = {Neural Comput. Appl.},
  title        = {Intelligent water resources management platform for precision irrigation agriculture based on internet of things},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A dual industrial chain model and optimal equilibrium condition analysis based on information computation. <em>NCA</em>, <em>37</em>(13), 8033-8050. (<a href='https://doi.org/10.1007/s00521-022-07749-6'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the information technology era, the upstream and downstream of the supply and demand industrial chain constitute an “Internet + ” supply chain scenario of logistics, capital flow and information flow with information computing capabilities such as lending, payment and settlement. The high-quality growth of the economy requires the optimization of the industrial chain structure, while the factor income distribution reflects the profit level of every link in the chain. This paper establishes a dual industrial chain model composed of capital-intensive and labour-intensive industrial chains. Additionally, it discusses and explores the optimal balance condition of the industrial chain in a closed economy and an open economy. The results show that industrial chain sustainability depends on the relative size between the chain’s establishment cost and profit—the capital stock (establishment cost) of the industrial chain has a positive (negative) effect on a high (low)-efficiency department. In a closed economy, the capital income contribution completely depends on the relative price of capital goods, and the positive or negative relationship between the two depends on the elasticity of substitution of final products in the two industrial chains. In an open economy, foreign trade gives rise to labour reallocation among departments and promotes the adoption of comparatively advanced departments in the industrial chain model.},
  archive      = {J_NCA},
  author       = {Bai, Jiangtao and Zheng, Aibing and Gai, Linlin},
  doi          = {10.1007/s00521-022-07749-6},
  journal      = {Neural Computing and Applications},
  month        = {5},
  number       = {13},
  pages        = {8033-8050},
  shortjournal = {Neural Comput. Appl.},
  title        = {A dual industrial chain model and optimal equilibrium condition analysis based on information computation},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Image captioning with residual swin transformer and actor-critic. <em>NCA</em>, <em>37</em>(13), 8019-8031. (<a href='https://doi.org/10.1007/s00521-022-07848-4'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Image captioning is one essential work in the multi-modal area, which employs computer vision and natural language processing technology together to describe image content. Most current methods employ the encoder–decoder framework to achieve satisfactory results. Recently, transformers have been extensively utilized in image captioning tasks and earned satisfactory results. Nevertheless, transformers pay more attention to the global features in images and divide images into fixed sizes and process them separately. Those models can not capture the relationship between the internal components in image, such as the relationships between objects, regions and object attributes. In this article, we introduce a novel Residual Swin Transformer and Actor-Critic (RSTAC) in the image captioning task. RSTAC consists of two modules: Residual Swin Transformer and Actor-Critic modules. In particular, we employ a residual network to preserve the vanilla feature in the first place and several residual Swin Transformer blocks with convolution operations to obtain the mid-level features, which are also composed of a residual network and several Swin Transformer blocks and convolution operations. It aims to explore the internal correlation between image content and obtain multi-view features in the image. Then, a policy network is utilized to explore possible words and forecast the future generation word. A value network is employed to calculate the reward of generated sentences whose goal is directly optimizing non-differentiable quality metrics and enhancing the performance of generated sentences. Experiments reveal that our model surpasses other competitive models and performs better on the MSCOCO and Flickr30k datasets.},
  archive      = {J_NCA},
  author       = {Zhou, Zhibo and Yang, Yang and Li, Zhoujun and Zhang, Xiaoming and Huang, Feiran},
  doi          = {10.1007/s00521-022-07848-4},
  journal      = {Neural Computing and Applications},
  month        = {5},
  number       = {13},
  pages        = {8019-8031},
  shortjournal = {Neural Comput. Appl.},
  title        = {Image captioning with residual swin transformer and actor-critic},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Theoretical analysis framework of a shared economy supervision and management model using mobile information system data. <em>NCA</em>, <em>37</em>(13), 8007-8017. (<a href='https://doi.org/10.1007/s00521-022-07831-z'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Through the use of Internet of Things technology, users and suppliers can be freely matched, and their idle resources can be traded in the display world to improve product utilization. A sharing economy means that organizations or individuals with idle resources transfer the right to use these resources to others for rewards; thus, sharers create value by sharing their idle resources with others. However, the development time of the sharing economy is short, and the entire development model has shortcomings. This paper aims to study innovation research on the sharing economy supervision mode based on mobile information systems. It links the mobile information system with the sharing economy model and regulates the sharing economy development model. A mobile information system is a model that reflects the internal structure of an information system and the connections between its parts. It generally refers to the structural model of an information system; thus, it is usually represented by a structural diagram. This paper proposes a theoretical analysis framework based on mobile information systems, which provides a theoretical basis for the exploration of the sharing economy. Starting from the intersection of supervision, this paper first discusses the rationality of the implementation of internet sharing economy supervision and points out the problems that exist in the development of the sharing economy. The test results of this paper show that only 18% of people think that the price of online car-hailing is high, and only 7.7% of people think that the price of online car-hailing is very high; these results indicate that the current price of shared transportation needs to be increased.},
  archive      = {J_NCA},
  author       = {Wang, Song},
  doi          = {10.1007/s00521-022-07831-z},
  journal      = {Neural Computing and Applications},
  month        = {5},
  number       = {13},
  pages        = {8007-8017},
  shortjournal = {Neural Comput. Appl.},
  title        = {Theoretical analysis framework of a shared economy supervision and management model using mobile information system data},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Dysarthric speech recognition: An investigation on using depthwise separable convolutions and residual connections. <em>NCA</em>, <em>37</em>(12), 7991-8005. (<a href='https://doi.org/10.1007/s00521-024-10870-3'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Dysarthria is a neurological condition resulting from impairments affecting muscle control involved in speech articulation, leading to reduced intelligibility or unintelligible speech, which affects communication abilities. Although Automatic Speech Recognition (ASR) technologies hold the potential to improve the lives of people with dysarthria significantly, ASR systems designed for normal speech have shown limited effectiveness when presented with impaired speech. Consequently, researchers have focused on developing ASR systems specifically tailored for dysarthria. However, progress in this area has been gradual due to the scarcity of dysarthric speech for training and the increased variability of speech among dysarthric individuals, necessitating a larger dataset of dysarthric utterances. One potential solution to enhance the robustness of dysarthric ASR is to deepen the architecture of the acoustic model, which maps the speech signal to words or phonetic units. However, deeper architectures require more training data and pose challenges in dealing with issues such as the vanishing gradient problem and representational bottlenecks in deep learning models. In this study, we expanded on our previous findings and investigated the applications of Depthwise Separable Convolution neurons and the inclusion of Residual Connections to propose a deep dysarthric acoustic model, tackling both vanishing gradients and representational bottleneck issues in dysarthric ASR. Multiple speaker-adaptive dysarthric ASRs were trained and evaluated for 15 UA-Speech dysarthric subjects, then benchmarked against the state-of-the-art and our previous dysarthric ASRs. Our proposed architectures have delivered up to 22.58% word recognition rate (WRR) improvements over the reference models. We observed an average of 10.81% better WRRs over the base traditional dysarthric ASR for all speakers. Likewise, the proposed acoustic model outperformed the state-of-the-art Transformer-based dysarthric ASR reference model for all subjects with mild dysarthria, and up to 14.26% better WRR for the moderate dysarthric subjects was obtained. Our findings indicate the importance of architecture optimization to not only deal with vanishing gradient and representational bottleneck but also maintain the depth of the acoustic model to ensure sufficient model capacity is available to learn intraspeaker variability caused by dysarthria.},
  archive      = {J_NCA},
  author       = {Shahamiri, Seyed Reza and Mandal, Krishnendu and Sarkar, Sudeshna},
  doi          = {10.1007/s00521-024-10870-3},
  journal      = {Neural Computing and Applications},
  month        = {4},
  number       = {12},
  pages        = {7991-8005},
  shortjournal = {Neural Comput. Appl.},
  title        = {Dysarthric speech recognition: An investigation on using depthwise separable convolutions and residual connections},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Enhancing the adversarial robustness in medical image classification: Exploring adversarial machine learning with vision transformers-based models. <em>NCA</em>, <em>37</em>(12), 7971-7989. (<a href='https://doi.org/10.1007/s00521-024-10516-4'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Despite their remarkable achievements in computer-aided medical image-related tasks, including detection, segmentation, and classification, deep learning techniques remain vulnerable to imperceptible adversarial attacks, which could lead to potential misdiagnosis in clinical applications. Therefore, adversarial attacks and their defense in deep medical diagnosis systems have been witnessed remarkable progress in recent years. Although the importance of transformers in various medical applications has grown immensely, a critical concern for the reliability and security of their susceptibility to adversarial attacks has not yet been sufficiently investigated. Furthermore, many studies in the field of ViT-based adversarial machine learning mainly focus on pure ViT architecture. To this end, this paper provides a comprehensive evaluation, comparison, and analysis of state-of-the-art ViT-based models such as ViT, DeiT, Swin transformer, and PVTv2 for their robustness against FGSM and PGD adversarial machine learning attacks and investigates the impact of the k-step PGD adversarial training defense mechanism in the domain of various medical imaging tasks. The findings indicate that ViT-based models are vulnerable to adversarial attacks even at a small perturbation degree. Furthermore, the significant drop in accuracy from around 90.0% underlines the vulnerability of these models to adversarial attacks and highlights the urgent need for robust defenses. We also conclude that the ViT-based models show significant robustness against adversarial attacks using adversarial training, i.e., the defense strategy can achieve improved classification accuracy, close to the clean image accuracy. By analyzing quantitative results, we believe that this study aims to fill the gap in research on the robustness of ViT-based models to adversarial machine learning attacks in medical image analysis, highlighting future research directions.},
  archive      = {J_NCA},
  author       = {Kanca Gulsoy, Elif and Ayas, Selen and Baykal Kablan, Elif and Ekinci, Murat},
  doi          = {10.1007/s00521-024-10516-4},
  journal      = {Neural Computing and Applications},
  month        = {4},
  number       = {12},
  pages        = {7971-7989},
  shortjournal = {Neural Comput. Appl.},
  title        = {Enhancing the adversarial robustness in medical image classification: Exploring adversarial machine learning with vision transformers-based models},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Developing an imperialist competitive algorithm based on two improvement strategies in a hierarchical capacitated health network. <em>NCA</em>, <em>37</em>(12), 7947-7970. (<a href='https://doi.org/10.1007/s00521-024-10513-7'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The present paper on the location of clinic (C), hospital (H) and medical center (MC) in the Golestan province of Iran is motivated by its present condition coming from limited distribution and ease of access for related Cs, Hs, and MCs. Design of a median hierarchical location-allocation model for the needed healthcare facilities, from Cs to Hs and MCs, is a vital and valuable activity from the emergency viewpoints of both patients and the government. This model has been formulated as a mixed-integer linear mathematical framework for finding the optimal location of these capacitated healthcare facilities, the allocation of patients to these Cs, Hs, or MCs and also for the referrals of the patients' needs to them while minimizing the total demand-weighted travel distance. This problem is in the category of an NP-hard problem. An efficient and robust imperialist competitive algorithm based on two initialization and local mechanisms is also presented to improve the computational time and accuracy of simulation results. Comparative performance of the developed method with some well-known metaheuristics has been surveyed using a real case study for the healthcare network for different problems with a change in the model parameters' values. The novel method is reliable and valid according to accuracy and execution time. The sensitivity analysis results concerning the maximum number of locations (i.e., Cs, Hs and MCs). Furthermore, the percent of the referred demand determines the significance and practical observations related to the combination of the Cs, Hs, and MCs to be established. Our new model is illustrated to be gainful as it offers a robust build plan to designers for making location decisions for developing the Golestan healthcare network.},
  archive      = {J_NCA},
  author       = {Khanduzi, Raheleh and Sadati, Mir Ehsan Hesam},
  doi          = {10.1007/s00521-024-10513-7},
  journal      = {Neural Computing and Applications},
  month        = {4},
  number       = {12},
  pages        = {7947-7970},
  shortjournal = {Neural Comput. Appl.},
  title        = {Developing an imperialist competitive algorithm based on two improvement strategies in a hierarchical capacitated health network},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). The using effect of fuzzy analytic hierarchy process in project engineering risk management. <em>NCA</em>, <em>37</em>(12), 7935-7945. (<a href='https://doi.org/10.1007/s00521-023-09046-2'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This work aims to explore the effectiveness of the fuzzy analytic hierarchy process (FAHP) in project engineering risk management and comprehensively investigate the application of genetic algorithm (GA) and neuro-fuzzy system in this field. Experimental research methods are employed, and three different types of projects, namely construction engineering, information technology projects, and manufacturing projects, are selected for risk evaluation. In the research process, an evaluation index system is established by identifying and analyzing the risk factors of each project, and a FAHP model is constructed. To more accurately assess the mutual influences and weights of the factors, fuzzy mathematics, and fuzzy logic methods are applied to fuzzify the parameters during the risk factor stratification and model construction stages. Besides, the GA and neuro-fuzzy system are applied to the model to further construct a decision support system. The research results indicate that the proposed model has an error rate of less than 10%, demonstrating high reliability and accuracy. Furthermore, the use of FAHP can improve the accuracy of risk management control. Compared to the traditional simple hierarchy analysis method, the proposed method improves accuracy by 9.6% and precision by 8.5%. This work provides a new and effective approach for project engineering risk evaluation, which can assist project managers in more accurately evaluating and managing risks, thereby enhancing the efficiency and quality of project management. This work has practical value in improving the efficiency and quality of project management.},
  archive      = {J_NCA},
  author       = {Dong, Tao and Li, Haiyan and Zhang, Ziqiong},
  doi          = {10.1007/s00521-023-09046-2},
  journal      = {Neural Computing and Applications},
  month        = {4},
  number       = {12},
  pages        = {7935-7945},
  shortjournal = {Neural Comput. Appl.},
  title        = {The using effect of fuzzy analytic hierarchy process in project engineering risk management},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A maneuvering target tracking based on fastIMM-extended viterbi algorithm. <em>NCA</em>, <em>37</em>(12), 7925-7934. (<a href='https://doi.org/10.1007/s00521-023-09039-1'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A fastIMM-extended Viterbi (fastIMM-EV) algorithm-based maneuvering target tracking method is proposed for the real-time tracking of ground maneuvering targets by a ballistic acoustic array, which firstly adopts the extended Viterbi interactive multi-model (IMM-EV) algorithm to select the best model from a given model set to match the maneuvering target motion pattern; secondly, the α–β filter and α–β–γ filter are used to replace the 2D or 3D Kalman filter in the traditional IMM algorithm, respectively, to form the fastIMM-EV algorithm, which nearly improves the algorithm efficiency, and at the same time, for the switching problem of different fastIMM-EV modules, a target maneuver recognition parameter is defined as the switching factor of the fastIMM-EV module, so that fastIMM-EV to switch the module when the target maneuver occurs; finally, the MATLAB simulation test results verify the practicality and high efficiency of the algorithm in this paper compared with different IMM target tracking methods.},
  archive      = {J_NCA},
  author       = {Di, Yi and Li, Ruiheng and Tian, Hao and Guo, Jia and Shi, Binghua and Wang, Zheng and Yan, Ke and Liu, Yueheng},
  doi          = {10.1007/s00521-023-09039-1},
  journal      = {Neural Computing and Applications},
  month        = {4},
  number       = {12},
  pages        = {7925-7934},
  shortjournal = {Neural Comput. Appl.},
  title        = {A maneuvering target tracking based on fastIMM-extended viterbi algorithm},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). An empirical evaluation of extreme learning machine uncertainty quantification for automated breast cancer detection. <em>NCA</em>, <em>37</em>(12), 7909-7924. (<a href='https://doi.org/10.1007/s00521-023-08992-1'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Early detection and diagnosis are the key factors in decreasing the breast cancer mortality rate in medical image analysis. A randomized learning technique called extreme learning machine (ELM) plays a vital role in learning the single hidden layer feed-forward network with fast learning speed and good generalization. The input weight and bias are randomly generated and fixed during the ELM training phase, and subsequently, the analytical procedure determines the output weight. The extreme learning machine’s learning ability is based on three uncertainty factors: the number of hidden nodes, an input weight initialization, and the type of activation function in the hidden layer. Various breast classification works have experimented with extreme learning machine techniques and did not investigate the following factors. This paper evaluates the extreme learning machine model’s performance with different configurations on the standard ultra-sound breast cancer dataset, BUSI. The proposed extreme learning machine configuration model experimented on original and filtered ultra-sound images. A fivefold stratified cross-validation scheme is applied here to enhance the model’s generalization performance. The proposed computer-aided diagnosis (CAD) model provides 100% accuracy with the best extreme learning machine configurations. Then, we compare the classification results of the proposed model with typical variants of extreme learning machines like Hybrid ELM (HELM), online-sequential ELM (OS-ELM), Weighted ELM, and complex ELM (CELM). The experimental results demonstrate that the proposed extreme learning machine model is superior to existing models, offering good generalization without any feature extraction or reduction method.},
  archive      = {J_NCA},
  author       = {Muduli, Debendra and Kumar, Rakesh Ranjan and Pradhan, Jitesh and Kumar, Abhinav},
  doi          = {10.1007/s00521-023-08992-1},
  journal      = {Neural Computing and Applications},
  month        = {4},
  number       = {12},
  pages        = {7909-7924},
  shortjournal = {Neural Comput. Appl.},
  title        = {An empirical evaluation of extreme learning machine uncertainty quantification for automated breast cancer detection},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Project-based learning model based on intelligent computing of the internet of things: Characteristics, hidden worries, and beyond. <em>NCA</em>, <em>37</em>(12), 7897-7908. (<a href='https://doi.org/10.1007/s00521-023-08990-3'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Project-based learning is a relatively novel learning mode in the modern education industry, which helps to improve students’ various abilities. However, in the specific practical process, project-based learning also has many shortcomings that need further improvement. This article integrated intelligent Internet of Things (IoT) technology, analyzed the characteristics of project-based learning, and analyzes the problems it faces in detail. From this, it also explored a model of project-based learning space to study students’ learning behavior. This article also combined the comprehensive weighted fusion algorithm to further explore the evaluation of project-based learning, and carried out relevant experimental analysis. The experimental results showed that in terms of autonomous learning ability, the average test result of this algorithm was 82.30%, while the average test result of traditional algorithms was 76.94%; In terms of teamwork ability, the average test result of this algorithm was 87.25%, the average test result of the traditional algorithm was 78.64%; In terms of skill application ability, the average test result of this algorithm was 78.37%; while, the average test result of traditional algorithms was 72.68%. In summary, this algorithm can effectively evaluate students’ abilities in various aspects.},
  archive      = {J_NCA},
  author       = {Hu, Liangliang},
  doi          = {10.1007/s00521-023-08990-3},
  journal      = {Neural Computing and Applications},
  month        = {4},
  number       = {12},
  pages        = {7897-7908},
  shortjournal = {Neural Comput. Appl.},
  title        = {Project-based learning model based on intelligent computing of the internet of things: Characteristics, hidden worries, and beyond},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Classifying distinct emotions from parents of ASD child using EEG source data by combining Bernoulli–Laplace prior and graph neural networks. <em>NCA</em>, <em>37</em>(12), 7877-7895. (<a href='https://doi.org/10.1007/s00521-023-09171-y'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Emotion recognition using biological brain signals needs to be reliable to attain effective signal processing and feature extraction techniques. The impact of emotions in interpretations, conversations, and decision-making, has made automatic emotion recognition and examination of a significant feature in the field of psychiatric disease treatment and cure. The problem arises from the limited spatial resolution of EEG recorders. Predetermined quantities of electroencephalography (EEG) channels are used by existing algorithms, which combine several methods to extract significant data. The major intention of this study was to focus on enhancing the efficiency of recognizing emotions using signals from the brain through an experimental, adaptive selective channel selection approach that recognizes that brain function shows distinctive behaviors that vary from one individual to another individual and from one state of emotions to another. We apply a Bernoulli–Laplace-based Bayesian model to map each emotion from the scalp senses to brain sources to resolve this issue of emotion mapping. The standard low-resolution electromagnetic tomography (sLORETA) technique is employed to instantiate the source signals. We employed a progressive graph convolutional neural network (PG-CNN) to identify the sources of the suggested localization model and the emotional EEG as the main graph nodes. In this study, the proposed framework uses a PG-CNN adjacency matrix to express the connectivity between the EEG source signals and the matrix. Research on an EEG dataset of parents of an ASD (autism spectrum disorder) child has been utilized to investigate the ways of parenting of the child's mother and father. We engage with identifying the personality of parental behaviors when regulating the child and supervising his or her daily activities. These recorded datasets incorporated by the proposed method identify five emotions from brain source modeling, which significantly improves the accuracy of emotion recognition in comparison with the existing algorithms. The results show a 1% to 2% increase in classification accuracy in absolute terms. Furthermore, an experiment indicates the proposed method performs better than similar methods. We also discovered that the suggested approach performs admirably when using conventional classification techniques.},
  archive      = {J_NCA},
  author       = {ArulDass, Stephen Dass and Jayagopal, Prabhu},
  doi          = {10.1007/s00521-023-09171-y},
  journal      = {Neural Computing and Applications},
  month        = {4},
  number       = {12},
  pages        = {7877-7895},
  shortjournal = {Neural Comput. Appl.},
  title        = {Classifying distinct emotions from parents of ASD child using EEG source data by combining Bernoulli–Laplace prior and graph neural networks},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Formulation of twin graph labelings. <em>NCA</em>, <em>37</em>(12), 7857-7875. (<a href='https://doi.org/10.1007/s00521-024-10116-2'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, two new graph labeling techniques are introduced, namely difference amicable labeling and sum amicable labeling, which together are named the twin graph labelings. Here, the formulation of labelings and verification of a few simple, finite, and undirected graphs including cubic graphs, quartic graphs, and multi-regular graphs are verified to be twin graphs. Two new mathematical board games and two coding techniques are introduced and presented as applications.},
  archive      = {J_NCA},
  author       = {Ignatius, Fredrick and Kaspar, S.},
  doi          = {10.1007/s00521-024-10116-2},
  journal      = {Neural Computing and Applications},
  month        = {4},
  number       = {12},
  pages        = {7857-7875},
  shortjournal = {Neural Comput. Appl.},
  title        = {Formulation of twin graph labelings},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Empirical asset pricing based on network big data mining and privacy protection. <em>NCA</em>, <em>37</em>(12), 7841-7855. (<a href='https://doi.org/10.1007/s00521-024-10110-8'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {For the pricing model, a solid economic foundation is the key node to improve the pricing model. This is not only true for stocks and other profitable assets, but also for other assets such as creditor’s rights. This study is mainly based on the empirical asset pricing model, constructs a stochastic equilibrium model, and uses the generalized matrix method to analyze the asset pricing model. The estimation results show that these parameters are significant at the 5% or even 1% significance level, and the estimated values of the parameters meet the economic expectations, which can be tested by over-identification of tool variables. The experimental results prove that the empirical asset pricing model in this paper can effectively improve the effect of the single feedforward neural network model, and emphasize the necessity of feature learning, especially nonlinear unsupervised feature learning, in the application of machine learning in the field of empirical finance, which enriches the relevant research in the cross field of machine learning and empirical finance and has potential practical value.},
  archive      = {J_NCA},
  author       = {Xu, Xiaoxiang},
  doi          = {10.1007/s00521-024-10110-8},
  journal      = {Neural Computing and Applications},
  month        = {4},
  number       = {12},
  pages        = {7841-7855},
  shortjournal = {Neural Comput. Appl.},
  title        = {Empirical asset pricing based on network big data mining and privacy protection},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A novel algorithm of joint frequency–power domain anti-jamming based on PER-DQN. <em>NCA</em>, <em>37</em>(12), 7823-7840. (<a href='https://doi.org/10.1007/s00521-023-09136-1'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In order to improve the reliability of wireless communication systems under a malicious jamming environment, a novel anti-jamming algorithm based on PER-DQN which can reach the optimized anti-jamming decisions in the join frequency–power domain is proposed in this paper. In the proposed algorithm, the anti-jamming problem of a wireless communication system is modeled as a Markov decision process and the proposed algorithm engrafts DQN scheme with a prioritized replay strategy to improve its learning efficiency. Simulation results show that the proposed algorithm can overcome the shortcomings of existing schemes and improve the performance of previous anti-jamming algorithms, such as Q-learning and DQN anti-jamming algorithms, in terms of signal transmission gain and algorithm convergence speed.},
  archive      = {J_NCA},
  author       = {Wan, Boyu and Niu, Yingtao and Chen, Changxing and Zhou, Zhanyang and Xiang, Peng},
  doi          = {10.1007/s00521-023-09136-1},
  journal      = {Neural Computing and Applications},
  month        = {4},
  number       = {12},
  pages        = {7823-7840},
  shortjournal = {Neural Comput. Appl.},
  title        = {A novel algorithm of joint frequency–power domain anti-jamming based on PER-DQN},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Research on artificial intelligence-based computer-assisted anesthesia intelligent monitoring and diagnostic methods in health care. <em>NCA</em>, <em>37</em>(12), 7813-7822. (<a href='https://doi.org/10.1007/s00521-023-08998-9'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the field of health care, anesthesia is a crucial therapeutic measure, but it also carries certain risks. Insufficient or excessive anesthesia can lead to significant consequences for patients, such as intraoperative awareness and impaired spontaneous breathing. Therefore, monitoring the depth of anesthesia is one of the vital life-supporting measures during clinical surgery. Currently, commonly used clinical indicators such as blood pressure, heart rate, and respiratory rate are used to estimate the depth of anesthesia in patients. However, due to variations in patients' physical conditions and anesthesia medications, these indicators exhibit significant differences in their performance such that there is not reliable that analyzing these clinical indicators alone. Therefore, considering that electroencephalogram (EEG) reflects a high degree of brain activity, this paper proposes an intelligent detection for anesthesia based on the transformer framework and EEG signals. First, the original single-channel EEG is preprocessed to extract spectral and differential entropy features. Subsequently, the two types of features are fused and sent to the transformer encoder network to complete the anesthesia depth prediction. Finally, the validation of the proposed algorithm was completed on the sevoflurane anesthesia dataset from Waikato Hospital in Hamilton, New Zealand, and a high prediction probability of 85.32% was achieved.},
  archive      = {J_NCA},
  author       = {Huang, Xiqiang and Liu, Jin and Yang, Yinqi and Yuan, Binglin and Gjoni, Gazmir and Jianxing, Wang},
  doi          = {10.1007/s00521-023-08998-9},
  journal      = {Neural Computing and Applications},
  month        = {4},
  number       = {12},
  pages        = {7813-7822},
  shortjournal = {Neural Comput. Appl.},
  title        = {Research on artificial intelligence-based computer-assisted anesthesia intelligent monitoring and diagnostic methods in health care},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). The innovation dynamic mechanism of platform enterprise business model based on deep learning. <em>NCA</em>, <em>37</em>(12), 7797-7811. (<a href='https://doi.org/10.1007/s00521-024-10242-x'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the continuous emergence and rapid development of high and new technologies such as big data, cloud computing, artificial intelligence, mobile Internet, and the Internet of Things, the platform economy has developed rapidly and has become the current mainstream business model. This paper first analyzes the external driving factors that promote the rapid development of platform-based business models, then combines the existing research results of scholars to analyze the components of platform-based business models and uses deep learning methods. The research carried out model construction, drew causal relationship diagrams and flow diagrams, selected typical and representative platform-based enterprises for research, collected relevant data, and verified that the model's effectiveness reached 98%. On this basis, the model was compounded. Simulation and sensitivity analysis explores the critical factor driving platform-type enterprises to carry out business model innovation: the service quality coefficient of platform-type enterprises.},
  archive      = {J_NCA},
  author       = {Kang, Yanjun and Liu, Guoquan},
  doi          = {10.1007/s00521-024-10242-x},
  journal      = {Neural Computing and Applications},
  month        = {4},
  number       = {12},
  pages        = {7797-7811},
  shortjournal = {Neural Comput. Appl.},
  title        = {The innovation dynamic mechanism of platform enterprise business model based on deep learning},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Bias analysis of AI models for undergraduate student admissions. <em>NCA</em>, <em>37</em>(12), 7785-7795. (<a href='https://doi.org/10.1007/s00521-024-10762-6'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Bias detection and mitigation is an active area of research in machine learning. This work extends previous research done by the authors Van Busum and Fang (Proceedings of the 38th ACM/SIGAPP Symposium on Applied Computing, 2023) to provide a rigorous and more complete analysis of the bias found in AI predictive models. Admissions data spanning six years was used to create an AI model to determine whether a given student would be directly admitted into the School of Science under various scenarios at a large urban research university. During this time, submission of standardized test scores as part of a student’s application became optional which led to interesting questions about the impact of standardized test scores on admission decisions. We developed and analyzed AI models to understand which variables are important in admissions decisions, and how the decision to exclude test scores affects the demographics of the students who are admitted. We then evaluated the predictive models to detect and analyze biases these models may carry with respect to three variables chosen to represent sensitive populations: gender, race, and whether a student was the first in his/her family to attend college. We also extended our analysis to show that the biases detected were persistent. Finally, we included several fairness metrics in our analysis and discussed the uses and limitations of these metrics.},
  archive      = {J_NCA},
  author       = {Van Busum, Kelly and Fang, Shiaofen},
  doi          = {10.1007/s00521-024-10762-6},
  journal      = {Neural Computing and Applications},
  month        = {4},
  number       = {12},
  pages        = {7785-7795},
  shortjournal = {Neural Comput. Appl.},
  title        = {Bias analysis of AI models for undergraduate student admissions},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A novel feature selection method based on adaptive search particle swarm optimization. <em>NCA</em>, <em>37</em>(12), 7767-7783. (<a href='https://doi.org/10.1007/s00521-024-10611-6'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As an effective method for data dimensionality reduction, feature selection could improve the classification accuracy and reduce the computational cost when dealing with high-dimensional data. Feature selection is essentially a complex optimization search problem. Among many optimization algorithms, particle swarm optimization (PSO) has been widely used due to its good global search ability and easy to implement. However, most feature selection methods based on PSO ignore the correlations between the features of data, which may lead to more redundant features, and the feature selection methods are prone to fall into local minima. This study proposes an improved feature selection method based on adaptive search particle swarm optimization (AS-BPSO-FS). On one hand, AS-BPSO-FS is designed to consider the feature correlation according to the feature correlation information. The particle position adaptive update strategy selects features based on the correlation coefficient between features, ensuring that features with higher correlation are more likely to be culled, so as to obtain a subset of features with less redundancy. On the other hand, AS-BPSO-FS identifies particles trapped in local minima by calculating the update time of individual and global optimal positions, and uses an adaptive particle neighborhood search strategy to help particles escape from local minima. The AS-BPSO-FS has been tested on 10 UCI data and compared with some state-of-the-art feature selection methods. The results verify that the proposed method could obtain feature subsets with better classification performance and less redundancy.},
  archive      = {J_NCA},
  author       = {Han, Fei and Wang, Yi-Huai and Li, Fan-Yu},
  doi          = {10.1007/s00521-024-10611-6},
  journal      = {Neural Computing and Applications},
  month        = {4},
  number       = {12},
  pages        = {7767-7783},
  shortjournal = {Neural Comput. Appl.},
  title        = {A novel feature selection method based on adaptive search particle swarm optimization},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Graph-level heterogeneous information network embeddings for cardholder transaction analysis. <em>NCA</em>, <em>37</em>(12), 7751-7765. (<a href='https://doi.org/10.1007/s00521-024-10586-4'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Graph-related applications, including classification, regression, and clustering, have seen significant advancements with the development of graph neural networks (GNNs). However, a gap remains in effectively using these models for heterogeneous graphs, as current methods primarily focus on homogeneous graphs, often overlooking potentially valuable semantic information. To address this issue, our work introduces a novel approach, G-HIN2VEC (Graph-level heterogeneous information network to vector), specifically designed to generate heterogeneous graph representations. This method uniquely leverages a single graph to learn its own embeddings without relying on a graph dataset by sharing model parameters across the dataset. Inspired by recent developments in unsupervised learning in natural language processing, G-HIN2VEC employs a negative sampling technique to learn graph-level embedding matrices from a variety of metapaths. This approach has been applied to real-world credit card data, facilitating the analysis of cardholder transactions through three downstream applications: graph-level regression and classification tasks, including age and income prediction and gender classification. G-HIN2VEC outperforms traditional methods, demonstrating improvements in gender classification accuracy by 2.45% and income prediction R-squared (R2) by 7.19%. Furthermore, for age prediction, we achieved an increase of 6.55% in the mean absolute error (MAE) compared to DiffPool, a strong baseline.},
  archive      = {J_NCA},
  author       = {Damoun, Farouk and Seba, Hamida and Hilger, Jean and State, Radu},
  doi          = {10.1007/s00521-024-10586-4},
  journal      = {Neural Computing and Applications},
  month        = {4},
  number       = {12},
  pages        = {7751-7765},
  shortjournal = {Neural Comput. Appl.},
  title        = {Graph-level heterogeneous information network embeddings for cardholder transaction analysis},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Evolutionary extreme learning machine based on an improved MOPSO algorithm. <em>NCA</em>, <em>37</em>(12), 7733-7750. (<a href='https://doi.org/10.1007/s00521-024-10578-4'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Extreme learning machine (ELM), as a single hidden layer feedforward neural network (SLFN), has attracted extensive attention because of its fast learning speed and high accuracy. However, the random selection of input weights and hidden biases is the main reason that deteriorates the generalization performance and stability of ELM network. In this study, an improved ELM (IMOPSO-ELM) is proposed to enhance the generalization performance and convergence stability of the SLFN by using a multi-objective particle swarm optimization (MOPSO) to determine the input parameters including input weights and hidden biases of the SLFN. Firstly, different from the traditional improved ELM based on single-objective evolutionary algorithm, the proposed algorithm used MOPSO to optimize the input weights and hidden biases of SLFN by considering the two objectives including accuracy on the validation set and the 2-norm of the SLFN output weights. Secondly, in order to improve the diversity and convergence of the solution set obtained by MOPSO, an improved MOPSO (IMOPSO) is proposed. The improved MOPSO uses a new optimal global particle selection strategy, by randomly dividing the population into several subpopulations, each subpopulation uses different particle information in the external archive to guide the subpopulation update, and uses the external archive set as the platform to share the information between sub-swarms. Finally, the experiment on the four regression problems and four classification problems verifies the effectiveness of the approach in improving ELM generalization performance and performance stability.},
  archive      = {J_NCA},
  author       = {Ling, Qinghua and Tan, Kaimin and Wang, Yuyan and Li, Zexu and Liu, Wenkai},
  doi          = {10.1007/s00521-024-10578-4},
  journal      = {Neural Computing and Applications},
  month        = {4},
  number       = {12},
  pages        = {7733-7750},
  shortjournal = {Neural Comput. Appl.},
  title        = {Evolutionary extreme learning machine based on an improved MOPSO algorithm},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Knowledge distillation vulnerability of DeiT through CNN adversarial attack. <em>NCA</em>, <em>37</em>(12), 7721-7731. (<a href='https://doi.org/10.1007/s00521-023-09297-z'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the field of computer vision, active research is conducted to improve model performance. The successful application of transformer models in computer vision has led to the development of new models that incorporate this structure. However, the security vulnerabilities of these new models against adversarial attacks have not yet been thoroughly examined. This study investigated the adversarial attack vulnerabilities of DeiT, a model that combines CNN and transformer models through knowledge distillation techniques. We propose that even with only the teacher model (CNN model) information, a fatal attack on DeiT is possible, defining this attack scenario as a partial-white-box environment. In addition, owing to the integration of both CNN’s local information and the transformer’s global information, DeiT is more susceptible to attacks in a black-box environment than other models. The experimental results demonstrate that when adversarial examples (AEs) generated by the teacher model are inserted into DeiT, Fast Gradient Sign Method (FGSM) causes a 46.49% decrease in accuracy, Projected Gradient Descent (PGD) results in a 65.59% decrease. Furthermore, in a black-box environment, AEs generated by ViT and ResNet-50 have detrimental effects on DeiT. Notably, both the CNN and transformer models induced fatal FGSM attacks on DeiT, resulting in vulnerabilities of 70.49% and 53.59%, respectively. These findings demonstrate the additional vulnerability of DeiT to black-box attacks. Moreover, it highlights that DeiT poses a greater risk in practical applications compared to other models. Based on these vulnerabilities, we hope knowledge distillation research with enhanced adversarial robustness will be actively conducted.},
  archive      = {J_NCA},
  author       = {Hong, Inpyo and Choi, Chang},
  doi          = {10.1007/s00521-023-09297-z},
  journal      = {Neural Computing and Applications},
  month        = {4},
  number       = {12},
  pages        = {7721-7731},
  shortjournal = {Neural Comput. Appl.},
  title        = {Knowledge distillation vulnerability of DeiT through CNN adversarial attack},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Improving paraphrase generation using supervised neural-based statistical machine translation framework. <em>NCA</em>, <em>37</em>(11), 7705-7719. (<a href='https://doi.org/10.1007/s00521-023-08830-4'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In phrase generation (PG), a sentence in the natural language is changed into a new one with a different syntactic structure but having the same semantic meaning. The present sequence-to-sequence strategy aims to recall the words and structures from the training dataset rather than learning the words' semantics. As a result, the resulting statements are frequently grammatically accurate but incorrect linguistically. The neural machine translation approach suffers to handle unusual words, domain mismatch, and unfamiliar words, but it takes context well. This work presents a novel model for creating paraphrases that use neural-based statistical machine translation (NSMT). Our approach creates potential paraphrases for any source input, calculates the level of semantic similarity between text segments of any length, and encodes paraphrases in a continuous space. To evaluate the suggested model, Quora Question Pair and Microsoft Common Objects in Context benchmark datasets are used. We demonstrate that the proposed technique achieves cutting-edge performance on both datasets using automatic and human assessments. Experimental findings across tasks and datasets demonstrate that the suggested NSMT-based PG outperforms those achieved with traditional phrase-based techniques. We also show that the proposed technique may be used automatically for the development of paraphrases for a variety of languages.},
  archive      = {J_NCA},
  author       = {Razaq, Abdur and Shah, Babar and Khan, Gohar and Alfandi, Omar and Ullah, Abrar and Halim, Zahid and Ur Rahman, Atta},
  doi          = {10.1007/s00521-023-08830-4},
  journal      = {Neural Computing and Applications},
  month        = {4},
  number       = {11},
  pages        = {7705-7719},
  shortjournal = {Neural Comput. Appl.},
  title        = {Improving paraphrase generation using supervised neural-based statistical machine translation framework},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Detection of tampered real time videos using deep neural networks. <em>NCA</em>, <em>37</em>(11), 7691-7703. (<a href='https://doi.org/10.1007/s00521-024-09988-1'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, there has been a significant increase in the creation and sharing of videos that promote the utilization of digitally interactive multimedia, including music, graphics, and videos, across various devices. This trend encompasses both social networking applications and everyday tasks, mirroring the growing reliance on digital communication devices. Forgery techniques and motivations in the digital realm have undergone significant advancements. Previously, video editing methods were employed to enhance digital content. However, the proliferation of affordable and user-friendly video editing software has introduced several drawbacks and risks associated with these editing techniques. These editing tools can be misused to create misleading, altered, or fabricated videos for malicious purposes, such as spreading misinformation, deception, or defamation. In order to produce altered or fraudulent videos, additional footage is mixed, edited, or synthesized. Sophisticated editing techniques can make it challenging to detect forged videos, making it easier for forgeries to be mistakenly perceived as genuine. Existing method uses methods that detect forgery in videos with simply static backgrounds only. Proposed systems uses a deep learning strategy that incorporates transfer learning utilizing VGG16 and Customized CNN layers to categorize real time videos as tampered or authentic. With the aid of deep neural networks, the suggested method may identify forgery in films with both static and moving backgrounds. The experimental findings show that the suggested strategy is more accurate and effective than existing methods also it provides trustworthy results with low computing cost and strong detection performance.},
  archive      = {J_NCA},
  author       = {Koshy, Litty and Shyry, S. Prayla},
  doi          = {10.1007/s00521-024-09988-1},
  journal      = {Neural Computing and Applications},
  month        = {4},
  number       = {11},
  pages        = {7691-7703},
  shortjournal = {Neural Comput. Appl.},
  title        = {Detection of tampered real time videos using deep neural networks},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A study of progressive data flow knowledge tracing based on reconstructed attention mechanism. <em>NCA</em>, <em>37</em>(11), 7675-7689. (<a href='https://doi.org/10.1007/s00521-024-10011-w'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Knowledge tracing (KT) is an essential task in intellectual education, which measures learners’ ability to learn new knowledge by collecting historical learning information from learners. With the introduction of Recurrent Neural Networks (RNN) and Transformer into the field of KT, although effective, they focus only on the temporal order in which the learner information is affected. To model accurately, we propose a KT model BPKT (Bayesian-Attention mechanism Progressive data flow for KT) that allows exercise embedding to be layered in different forms and incorporated into the model multiple times. The BPKT model integrates the relationship between exercises covering knowledge points in both the temporal and spatial aspects, and defines a Bayesian-Attention mechanism based on this, with an in-depth analysis of the realistic meaning of the micro-parameters Q, K, and V in the mechanism. Through experiments on four real benchmark datasets, the results show that the BPKT model is helpful for predicting learners’ future responses on large-scale datasets.},
  archive      = {J_NCA},
  author       = {Wu, Qianxi and Wang, Min and Zhou, Guohui and Ji, Weidong},
  doi          = {10.1007/s00521-024-10011-w},
  journal      = {Neural Computing and Applications},
  month        = {4},
  number       = {11},
  pages        = {7675-7689},
  shortjournal = {Neural Comput. Appl.},
  title        = {A study of progressive data flow knowledge tracing based on reconstructed attention mechanism},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A deep perceptual framework for affective video tagging through multiband EEG signals modeling. <em>NCA</em>, <em>37</em>(11), 7657-7674. (<a href='https://doi.org/10.1007/s00521-023-09086-8'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Nowadays, multimedia content, such as photographs and movies, is ingrained in every aspect of human lives and has become a vital component of their entertainment. Multimedia content, such as videos or movie clips, is typically created with the intent to evoke certain feelings or emotions in viewers. Thus, by examining the viewer’s cognitive state while watching such content, its affectiveness can be evaluated. Considering the emotional aspect of videos, in this paper, a deep learning-based paradigm for affective tagging of video clips is proposed, in which participants’ irrational EEG responses are used to examine how people perceive videos. The information behind different brain regions, frequency waves, and connections among them play an important role in understanding a human’s cognitive state. Thus, here a contribution is made toward the effective modeling of EEG signals through two different representations, i.e., spatial feature matrix and combined power spectral density maps. The proposed feature representations highlight the spatial features of EEG signals and are therefore used to train a convolution neural network model for implicit tagging of two categories of videos in the Arousal domain, i.e., “Low Arousal” and “High Arousal.” The arousal emotional space represents the excitement level of the viewer; thus, this domain is selected to analyze the viewer’s engagement while watching video clips. The proposed model is developed using the EEG data taken from publicly available datasets “AMIGOS” and “DREAMER.” The model is tested using two different approaches, i.e., single-subject classification and multi-subject classification, and an average accuracy of 90%-95% and 90%-93% is achieved, respectively. The simulations presented in this paper show the pioneering applicability of the proposed framework for the development of brain–computer interface (BCI) devices for affective tagging of videos.},
  archive      = {J_NCA},
  author       = {Sharma, Shanu and Dubey, Ashwani Kumar and Ranjan, Priya and Rocha, Alvaro},
  doi          = {10.1007/s00521-023-09086-8},
  journal      = {Neural Computing and Applications},
  month        = {4},
  number       = {11},
  pages        = {7657-7674},
  shortjournal = {Neural Comput. Appl.},
  title        = {A deep perceptual framework for affective video tagging through multiband EEG signals modeling},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Denoising histopathology images for the detection of breast cancer. <em>NCA</em>, <em>37</em>(11), 7641-7655. (<a href='https://doi.org/10.1007/s00521-023-08771-y'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {One of the leading causes of mortality for women worldwide, both in developing and developed economies, is breast cancer. The gold standard for diagnosing cancer is still histological diagnosis, despite major advances in medical understanding. Admittedly, due to the sophistication of histopathology images and the significant increase in workload, this process takes a long time. Therefore, this field requires the development of automated and precise histopathology image analysis tools. Using deep learning, we proposed a system for denoising, detecting, and classifying breast cancer using deep learning architectures that are designed to solve certain related problems. CNN-based architectures are used to extract features from images, which are then put into a fully connected layer for the classification of malignant and benign cells, as well as their subclasses, in the suggested framework. The effectiveness of the suggested framework is evaluated through experiments leveraging accepted benchmark data sets. We achieve an accuracy of 94% and an F1 score of more than 90%.},
  archive      = {J_NCA},
  author       = {Zeb, Muhammad Haider and Al-Obeidat, Feras and Tubaishat, Abdallah and Qayum, Fawad and Fazeel, Ahsan and Amin, Muhammad},
  doi          = {10.1007/s00521-023-08771-y},
  journal      = {Neural Computing and Applications},
  month        = {4},
  number       = {11},
  pages        = {7641-7655},
  shortjournal = {Neural Comput. Appl.},
  title        = {Denoising histopathology images for the detection of breast cancer},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Automatic gait analysis through computer vision: A pilot study. <em>NCA</em>, <em>37</em>(11), 7619-7639. (<a href='https://doi.org/10.1007/s00521-023-08549-2'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Kinesiologists who study people's posture during walking depend on spreadsheets and visual posture reviews. Gold-standard evaluation relies on expert evaluation, not mediated by technology. However, today there are technological advances to automate specific processes adequately. Our proposal focuses on developing software based on computer vision and artificial intelligence (AI) to support recognition in the gait cycle and walking activities. The software is deployed in an architecture based on microservices to support the image analysis process with high concurrency. We opted for an open-source alternative, Openpose, because it is one of the most popular detection libraries for pose estimation and is capable of real-time multi-person pose analysis. We validate the choice through a proof of concept in which we prove that it can be possible to obtain valuable results for the kinesiology care process. This software assists specialists in analyzing and measuring lower extremity angles and distances during gait. We developed an information system based on open-source pose estimation algorithms for clinical decision-making. The technological approach was obtained by analyzing similar proposals and considering the characteristics of the clinic. We used a real-time multi-person pose estimation as an essential element enabling machines to visually comprehend and analyze humans and their interactions. In this instance, we identified accuracy metrics and optimized the evaluation process time. Using a non-probabilistic sample, we analyzed the videos of users performing the gait exercises. These results indicate that although the algorithms still need to achieve perfect accuracy, they save manual work for the final evaluation. On average, using the platforms reduces by about 50% the total time required to generate the final reports delivered by the kinesiology clinic. This proposal has always been justified as a support to the professional work and not as a replacement. We propose an information system based on open-source pose estimation algorithms for clinical decision-making. The technological approach was obtained by analyzing similar proposals and considering the characteristics of the clinic. We used a real-time multi-person pose estimation as an essential element enabling machines to visually comprehend and analyze humans and their interactions. While these recognition alternatives have been explored for some time, linking with particular needs and improving healthcare processes is critical.},
  archive      = {J_NCA},
  author       = {Díaz-Arancibia, Jaime and Córdova, Matías and Arango-López, Jeferson and Ahumada, Danay and Moreira, Fernando},
  doi          = {10.1007/s00521-023-08549-2},
  journal      = {Neural Computing and Applications},
  month        = {4},
  number       = {11},
  pages        = {7619-7639},
  shortjournal = {Neural Comput. Appl.},
  title        = {Automatic gait analysis through computer vision: A pilot study},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A non-invasive approach for calcium deficiency detection in pears using machine learning. <em>NCA</em>, <em>37</em>(11), 7609-7618. (<a href='https://doi.org/10.1007/s00521-023-08444-w'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A pear is a sweet fruit that is rich in dietary fiber, antioxidants, and plant compounds. The nutritional disorder in pears is either due to deficiency of nutrients or toxicity of nutrients. The techniques to identify the nutrients deficiencies include tissue testing, soil analysis, plant analysis, and visual deficiency symptoms. The effects of alfalfa greening, black end, and cork spot are minimised by correcting the calcium nutrition in the pear tree. In this paper, a two-class decision jungle model is proposed for the recognition of calcium deficiency in pears based on a non-invasive approach. The calcium deficiency in pears makes a bumpy fruit surface and leaves yellow color on the affected area than the rest of the skin results in the greyish corky lesion. The nutrient deficiency that results in serious disorders in pears not only influences the plant but also impacts the fruit quality. The introduction of artificial intelligence in the agriculture industry has helped farmers to produce healthier fruits. The artificial intelligence provides a real-time data for the classifier that results in increasing agricultural efficiencies, better crop yields and reduce fruit production costs by facilitating the routine and most complex tasks. The two-class decision jungle model achieves an accuracy of 98% with a database of 1000 samples. The other approaches, such as Boosted decision tree, Bayes point machine, Logistic regression, Neural Network, and SVM, have an accuracy of 92.20%, 84.3%, 72.5%, 82.4%, and 72.5%, respectively for the equivalent datasets. The highest accuracy is achieved with the proposed two class decision jungle that has non-linear decision boundaries and the performance is resilient in the presence of features that consist of noise. The number of calcium-deficient and healthy pears is 500 each. The geometrical features are extracted for the development of an artificial intelligence-based model for the classification of two classes like calcium deficient and healthy pear. The extracted features are split into training, validation, and testing. For training, validation, and testing, 80%, 10% and 10% samples are used respectively. The precision level is observed to be 0.974 and test accuracy is achieved as 98.7% and the overall accuracy 98% which are better than the existing 88.2% accuracy for pears using Support Vector Machine.},
  archive      = {J_NCA},
  author       = {Yogesh and Dubey, Ashwani Kumar and Rocha, Alvaro},
  doi          = {10.1007/s00521-023-08444-w},
  journal      = {Neural Computing and Applications},
  month        = {4},
  number       = {11},
  pages        = {7609-7618},
  shortjournal = {Neural Comput. Appl.},
  title        = {A non-invasive approach for calcium deficiency detection in pears using machine learning},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A systematic review of integrated information theory: A perspective from artificial intelligence and the cognitive sciences. <em>NCA</em>, <em>37</em>(11), 7575-7607. (<a href='https://doi.org/10.1007/s00521-023-08328-z'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The study of consciousness has gained momentum in recent years by the scientific community. In this same sense, the relationship between cognitive sciences and artificial intelligence presents a fundamental theoretical framework in the study of integrated information theory (IIT) as a theory that makes its way into the knowledge of consciousness. However, there are few studies that integrate these topics and a systematic review of the literature is highly pertinent. This paper seeks to identify methods, methodologies or computational solutions using artificial intelligence and cognitive science fundamentals that can provide some kind of solution to the challenges posed by IIT.},
  archive      = {J_NCA},
  author       = {Guerrero, Luz Enith and Castillo, Luis Fernando and Arango-López, Jeferson and Moreira, Fernando},
  doi          = {10.1007/s00521-023-08328-z},
  journal      = {Neural Computing and Applications},
  month        = {4},
  number       = {11},
  pages        = {7575-7607},
  shortjournal = {Neural Comput. Appl.},
  title        = {A systematic review of integrated information theory: A perspective from artificial intelligence and the cognitive sciences},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). SureUnet: Sparse autorepresentation encoder U-net for noise artifact suppression in low-dose CT. <em>NCA</em>, <em>37</em>(11), 7561-7573. (<a href='https://doi.org/10.1007/s00521-023-08847-9'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Low-dose computed tomography (LDCT) is desirable due to ionizing radiation, but the resulting images suffer from serious streak artifacts and spot noise. Recently, deep learning (DL)-based methods have emerged as promising alternatives for medical image processing. However, most DL-based methods are built intuitively and lack interpretability, and it is difficult to effectively separate the artifacts and noise in LDCT images. Obtaining diagnostically useful images, especially when using a low-dose scanner protocol, remains an open challenge. To improve the quality of LDCT images, we developed a novel processing network called the sparse autorepresentation U-Net (SureUnet). First, inspired by multilayer convolutional sparse coding (CSC), we constructed a sparse autorepresentation encoder to sufficiently capture and represent hierarchical image features. Then, we chose the widely used U-Net model for sparse autorepresentation block applications and designed SureUnet by adding a feature decoding block. Therefore, every module has well-defined interpretability in our network. Additionally, hybrid loss functions were specifically designed, including the mean absolute error, edge loss and perceptual loss. Through the cooperation of multiple loss functions, the noise artifact suppression effect of the network was improved. The visual results obtained on the MAYO and UIH datasets show that the proposed method’s noise artifact suppression effect was more significant. The quantitative results showed promising improvement levels compared to those of the other state-of-the-art methods. The SureUnet model significantly outperformed the compared methods on two datasets, with margins of 0.4 dB for the PSNR, 0.007 for the SSIM, and 1.6 for the FID on the MAYO dataset and margins of 0.5 dB for the PSNR, 0.004 for the SSIM and 2.9 for the FID on the UIH dataset. This work paves the way for sparse autorepresentation in DL for processing LDCT images. Experimental results have demonstrated the competitive performance of SureUnet in terms of noise suppression, structural fidelity and visual impression improvement.},
  archive      = {J_NCA},
  author       = {Liu, Jin and Zhang, Tingyu and Kang, Yanqin and Qiang, Jun and Hu, Dianlin and Zhang, Yikun},
  doi          = {10.1007/s00521-023-08847-9},
  journal      = {Neural Computing and Applications},
  month        = {4},
  number       = {11},
  pages        = {7561-7573},
  shortjournal = {Neural Comput. Appl.},
  title        = {SureUnet: Sparse autorepresentation encoder U-net for noise artifact suppression in low-dose CT},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). MixDA: Mixup domain adaptation for glaucoma detection on fundus images. <em>NCA</em>, <em>37</em>(11), 7541-7560. (<a href='https://doi.org/10.1007/s00521-023-08572-3'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep neural network has achieved promising results for automatic glaucoma detection on fundus images. Nevertheless, the intrinsic discrepancy across glaucoma datasets is challenging for the data-driven neural network approaches. This discrepancy leads to the domain gap that affects model performance and declines model generalization capability. Existing domain adaptation-based transfer learning methods mostly fine-tune pretrained models on target domains to reduce the domain gap. However, this feature learning-based adaptation method is implicit, and it is not an optimal solution for transfer learning on the diverse glaucoma datasets. In this paper, we propose a mixup domain adaptation (mixDA) method that bridges domain adaptation with domain mixup to improve model performance across divergent glaucoma datasets. Specifically, the domain adaptation reduces the domain gap of glaucoma datasets in transfer learning with an explicit adaptation manner. Meanwhile, the domain mixup further minimizes the risk of outliers after domain adaptation and improves the model generalization capability. Extensive experiments show the superiority of our mixDA on several public glaucoma datasets. Moreover, our method outperforms state-of-the-art methods by a large margin on four glaucoma datasets: REFUGE, LAG, ORIGA, and RIM-ONE.},
  archive      = {J_NCA},
  author       = {Yan, Ming and Lin, Yun and Peng, Xi and Zeng, Zeng},
  doi          = {10.1007/s00521-023-08572-3},
  journal      = {Neural Computing and Applications},
  month        = {4},
  number       = {11},
  pages        = {7541-7560},
  shortjournal = {Neural Comput. Appl.},
  title        = {MixDA: Mixup domain adaptation for glaucoma detection on fundus images},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Diabetic retinopathy classification based on dense connectivity and asymmetric convolutional neural network. <em>NCA</em>, <em>37</em>(11), 7527-7540. (<a href='https://doi.org/10.1007/s00521-022-07952-5'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Diabetic retinopathy (DR) is the leading cause of blindness in diabetics. The low contrast and microscopic nature of the lesions lead to a high false positive rate for automated DR screening. To address this issue, we propose a neural network named AC-DenseNet for the five-stage DR classification. In order to exploit the shallow features and enhance the feature extraction performance, dense connectivity is added to the convolution layer of AC-DenseNet. For the convolution layer to be more robust for DR detection in rotated or flipped pictures, asymmetric convolution branches are also introduced. In addition, attention mechanisms and auxiliary classifiers are incorporated into the network for the improvement of the performance of DR classification. We validate AC-DenseNet on the enhanced Kaggle dataset. The results show that AC-DenseNet can achieve 88.8% accuracy, 97.1% specificity, and 88.7% sensitivity, demonstrating that our model outperforms several state-of-the-art algorithms.},
  archive      = {J_NCA},
  author       = {Cao, Juan and Chen, Jiaran and Zhang, Xinying and Peng, Yang},
  doi          = {10.1007/s00521-022-07952-5},
  journal      = {Neural Computing and Applications},
  month        = {4},
  number       = {11},
  pages        = {7527-7540},
  shortjournal = {Neural Comput. Appl.},
  title        = {Diabetic retinopathy classification based on dense connectivity and asymmetric convolutional neural network},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). DH-GAC: Deep hierarchical context fusion network with modified geodesic active contour for multiple neurofibromatosis segmentation. <em>NCA</em>, <em>37</em>(11), 7511-7526. (<a href='https://doi.org/10.1007/s00521-022-07945-4'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Delineating accurately and simultaneously all lesions is vital and challenging for computer-aided diagnosis for multiple neurofibromatosis (NF). However, existing CNN-based segmentation methods paid little attention to weak boundaries. Moreover, due to the intensity-inhomogeneous distribution of medical images, the ambiguous boundaries, and highly variable locations, sizes and shapes of the lesions, delineating multiple lesions simultaneously remains quite challenging. To address these challenges, we introduce a novel end-to-end segmentation framework of multiple NF, deep hierarchical geodesic active contour (DH-GAC). It leverages the elaborately designed deep hierarchical context fusion network (DH-CFN) to improve the generalization and robustness of DH-GAC, and the modified geodesic active contour (MGAC) to delineate precisely all lesions as much as possible. Specifically, it employs DH-CFN to predict specific parameter maps of each image for MGAC and feeds them into the energy function of MGAC to delineate NF lesions, which makes DH-GAC end-to-end trainable. Moreover, to improve the generalization of DH-GAC, we adopt two different settings to initialize the surface for DH-GAC. Experimental results demonstrate that DH-GAC not only improves the segmentation precision, but also overcomes the intrinsic drawback of classical geodesic active contour in boundary delineation.},
  archive      = {J_NCA},
  author       = {Wu, Xiangqiong and Tan, Guanghua and Pu, Bin and Duan, Mingxing and Cai, Wenli},
  doi          = {10.1007/s00521-022-07945-4},
  journal      = {Neural Computing and Applications},
  month        = {4},
  number       = {11},
  pages        = {7511-7526},
  shortjournal = {Neural Comput. Appl.},
  title        = {DH-GAC: Deep hierarchical context fusion network with modified geodesic active contour for multiple neurofibromatosis segmentation},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). CSPP-IQA: A multi-scale spatial pyramid pooling-based approach for blind image quality assessment. <em>NCA</em>, <em>37</em>(11), 7499-7510. (<a href='https://doi.org/10.1007/s00521-022-07874-2'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The traditional image quality assessment (IQA) methods are usually based on convolutional neural networks (CNNs). For these IQA methods using CNNs, limited by the feature size of the fully connected layer, the input image needs be tailored to a pre-defined size, which usually results in destroying the original structure and content of the input image and thus reduces the accuracy of the quality assessment. In this paper, a blind image quality assessment method (named CSPP-IQA), which is based on multi-scale spatial pyramid pooling, is proposed. CSPP-IQA allows inputting the original image when assessing the image quality without any image adjustment. Moreover, by facilitating the convolutional block attention module and image understanding module, CSPP-IQA achieved better accuracy, generalization and efficiency than traditional IQA methods. The result of experiments running on real-scene IQA datasets in this study verified the effectiveness and efficiency of CSPP-IQA.},
  archive      = {J_NCA},
  author       = {Chen, Jingjing and Qin, Feng and Lu, Fangfang and Guo, Lingling and Li, Chao and Yan, Ke and Zhou, Xiaokang},
  doi          = {10.1007/s00521-022-07874-2},
  journal      = {Neural Computing and Applications},
  month        = {4},
  number       = {11},
  pages        = {7499-7510},
  shortjournal = {Neural Comput. Appl.},
  title        = {CSPP-IQA: A multi-scale spatial pyramid pooling-based approach for blind image quality assessment},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). CWC-transformer: A visual transformer approach for compressed whole slide image classification. <em>NCA</em>, <em>37</em>(11), 7485-7497. (<a href='https://doi.org/10.1007/s00521-022-07857-3'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The rapid development of Artificial Intelligence (AI) technology accelerates the application of computational pathology in clinical decision-making. Due to the restriction of computing resources and annotation information, it is challenging for AI-based computational pathology methods to effectively process and analyze the gigapixel whole slide image (WSI). Conventional methods utilize multiple instance learning (MIL) to convert WSI into patches for classification. However, without the patch-level annotation, it is difficult to extract discriminative features, even with pre-trained networks. Furthermore, forcibly applying the patch-level conversion will break the pathological characteristics of WSI from the spatial structure. In this study, we present a two-stage framework named Compressed WSI Classification (CWC-Transformer) to effectively solve the problems of feature extraction and spatial information loss in WSI classification. In the compression stage, we adopt contrastive learning to present a feature compression method, which not only extracts the discriminative features but also decreases the data deviation caused by staining and scanning inconsistency. In the learning stage, we extend the advantages of the convolutional neural network and transformer mechanism to enhance the co-relations between local and global information to provide the final results jointly. Experiments on three large-scale public datasets of different tasks show that our proposed framework outperforms other advanced methods in terms of robustness and interpretation.},
  archive      = {J_NCA},
  author       = {Wang, Yaowei and Guo, Jing and Yang, Yun and Kang, Yan and Xia, Yuelong and Li, Zhenhui and Duan, Yongchun and Wang, Kelong},
  doi          = {10.1007/s00521-022-07857-3},
  journal      = {Neural Computing and Applications},
  month        = {4},
  number       = {11},
  pages        = {7485-7497},
  shortjournal = {Neural Comput. Appl.},
  title        = {CWC-transformer: A visual transformer approach for compressed whole slide image classification},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Most relevant point query on road networks. <em>NCA</em>, <em>37</em>(11), 7473-7483. (<a href='https://doi.org/10.1007/s00521-022-07485-x'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Graphs are widespread in many real-life practical applications. One of a graph’s fundamental and popular researches is investigating the relations between two given vertices. The relationship between nodes in the graph can be measured by the shortest distance. Moreover, the number of paths is also a popular metric to assess the relationship of different nodes. In many location-based services, users make decisions on the basis of both the two metrics. To address this problem, we propose a new hybrid-metric based on the number of paths with a distance constraint for road networks, which are special graphs. Based on it, a most relevant node query on road networks is identified. To handle this problem, we first propose a Shortest-Distance Constrained DFS, which uses the shortest distance to prune unqualified nodes. To further improve query efficiency, we present Batch Query DFS algorithm, which only needs only one DFS search. Our experiments on four real-life road networks demonstrate the performance of the proposed algorithms.},
  archive      = {J_NCA},
  author       = {Zhang, Zining and Yang, Shenghong and Qin, Yunchuan and Yang, Zhibang and Huang, Yang and Zhou, Xu},
  doi          = {10.1007/s00521-022-07485-x},
  journal      = {Neural Computing and Applications},
  month        = {4},
  number       = {11},
  pages        = {7473-7483},
  shortjournal = {Neural Comput. Appl.},
  title        = {Most relevant point query on road networks},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multi-instance discriminative contrastive learning for brain image representation. <em>NCA</em>, <em>37</em>(11), 7459-7472. (<a href='https://doi.org/10.1007/s00521-022-07524-7'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper focuses on the problem of learning discriminative representation for brain images, which is a critical task toward understanding brain developments. Related studies usually extract manual and statistical features from the functional magnetic resonance images (MRIs) to differentiate brain patterns. However, these features fail to consider the implicit and high-order variances, and the existing representation methods often suffer from the weak manual features and the small-size sample. This paper introduces a weakly-supervised representation learning model, dubbed multi-instance discriminative contrastive learning (MIDCL), to identify the different MRI patterns. MIDCL yields two versions for each instance of a subject by introducing noise patterns and then achieves latent representations for them via training an encoder network and a projection network. Due to the multi-instance problem, MIDCL simultaneously minimizes an unsupervised contrastive loss (UCL) between the two representations at the level of instances and a supervised contrastive loss (SCL) between the two concatenated feature vectors at the level of subjects. We finally conducted experiments on two publicly available brain image datasets. The experiment results manifest that MIDCL could benefit from both UCL and SCL, thereby improving brain image classification performance in comparison with the state-of-the-art models.},
  archive      = {J_NCA},
  author       = {Zhang, Yupei and Liu, Shuhui and Qu, Xiran and Shang, Xuequn},
  doi          = {10.1007/s00521-022-07524-7},
  journal      = {Neural Computing and Applications},
  month        = {4},
  number       = {11},
  pages        = {7459-7472},
  shortjournal = {Neural Comput. Appl.},
  title        = {Multi-instance discriminative contrastive learning for brain image representation},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Retraction note: Research on mining collaborative behaviour patterns of dynamic supply chain network from the perspective of big data. <em>NCA</em>, <em>37</em>(10), 7457-7458. (<a href='https://doi.org/10.1007/s00521-025-11023-w'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_NCA},
  author       = {Leng, Kaijun and Jing, Linbo and Lin, I.-Ching and Chang, Sheng-Hung and Lam, Anthony},
  doi          = {10.1007/s00521-025-11023-w},
  journal      = {Neural Computing and Applications},
  month        = {4},
  number       = {10},
  pages        = {7457-7458},
  shortjournal = {Neural Comput. Appl.},
  title        = {Retraction note: Research on mining collaborative behaviour patterns of dynamic supply chain network from the perspective of big data},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). RETRACTED ARTICLE: Deep learning-based algorithm for optimum cluster head selection in sustainable wireless communication system. <em>NCA</em>, <em>37</em>(10), 7455. (<a href='https://doi.org/10.1007/s00521-023-08861-x'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_NCA},
  author       = {Revanesh, M. and Mary, S. A. Sahaaya Arul and Gnaneswari, G. and Jones, G. Maria and Kanimozhi, K. V. and Kamalam, G. K.},
  doi          = {10.1007/s00521-023-08861-x},
  journal      = {Neural Computing and Applications},
  month        = {4},
  number       = {10},
  pages        = {7455},
  shortjournal = {Neural Comput. Appl.},
  title        = {RETRACTED ARTICLE: Deep learning-based algorithm for optimum cluster head selection in sustainable wireless communication system},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). RETRACTED ARTICLE: Energy-efficient and sustainable communication in optical networks for eliminating path reservation criteria and providing guaranteed packet transmission between nodes. <em>NCA</em>, <em>37</em>(10), 7453. (<a href='https://doi.org/10.1007/s00521-023-08866-6'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_NCA},
  author       = {Asha, P. and Kalaavathi, B. and Shantha Kumari, K. and Malarvizhi, K. and Kishore Kumar, A. and Sobitha Ahila, S.},
  doi          = {10.1007/s00521-023-08866-6},
  journal      = {Neural Computing and Applications},
  month        = {4},
  number       = {10},
  pages        = {7453},
  shortjournal = {Neural Comput. Appl.},
  title        = {RETRACTED ARTICLE: Energy-efficient and sustainable communication in optical networks for eliminating path reservation criteria and providing guaranteed packet transmission between nodes},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). RETRACTED ARTICLE: Analysis of complex cognitive task and pattern recognition using distributed patterns of EEG signals with cognitive functions. <em>NCA</em>, <em>37</em>(10), 7451. (<a href='https://doi.org/10.1007/s00521-020-05439-9'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_NCA},
  author       = {Zhao, Jianyu and Li, Ke and Xi, Xi and Wang, Shanshan and Saravanan, Vijayalakshmi and Samuel, R. Dinesh Jackson},
  doi          = {10.1007/s00521-020-05439-9},
  journal      = {Neural Computing and Applications},
  month        = {4},
  number       = {10},
  pages        = {7451},
  shortjournal = {Neural Comput. Appl.},
  title        = {RETRACTED ARTICLE: Analysis of complex cognitive task and pattern recognition using distributed patterns of EEG signals with cognitive functions},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Correction: Leveraging large language models for word sense disambiguation. <em>NCA</em>, <em>37</em>(10), 7449-7450. (<a href='https://doi.org/10.1007/s00521-025-11082-z'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_NCA},
  author       = {Yae, Jung H. and Skelly, Nolan C. and Ranly, Neil C. and LaCasse, Phillip M.},
  doi          = {10.1007/s00521-025-11082-z},
  journal      = {Neural Computing and Applications},
  month        = {4},
  number       = {10},
  pages        = {7449-7450},
  shortjournal = {Neural Comput. Appl.},
  title        = {Correction: Leveraging large language models for word sense disambiguation},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Correction: Securing IIoT operations with recurrent federated network-based enhanced local search grasshopper. <em>NCA</em>, <em>37</em>(10), 7447. (<a href='https://doi.org/10.1007/s00521-024-10907-7'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_NCA},
  author       = {Alassafi, Madini O.},
  doi          = {10.1007/s00521-024-10907-7},
  journal      = {Neural Computing and Applications},
  month        = {4},
  number       = {10},
  pages        = {7447},
  shortjournal = {Neural Comput. Appl.},
  title        = {Correction: Securing IIoT operations with recurrent federated network-based enhanced local search grasshopper},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Correction: Software effort estimation using convolutional neural network and fuzzy clustering. <em>NCA</em>, <em>37</em>(10), 7445. (<a href='https://doi.org/10.1007/s00521-024-10906-8'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_NCA},
  author       = {Azzeh, Mohammad and Alkhateeb, Abedalrhman and Nassif, Ali Bou},
  doi          = {10.1007/s00521-024-10906-8},
  journal      = {Neural Computing and Applications},
  month        = {4},
  number       = {10},
  pages        = {7445},
  shortjournal = {Neural Comput. Appl.},
  title        = {Correction: Software effort estimation using convolutional neural network and fuzzy clustering},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Correction: An artificial intelligence strategy for the deployment of future microservice-based applications in 6G networks. <em>NCA</em>, <em>37</em>(10), 7443. (<a href='https://doi.org/10.1007/s00521-024-10754-6'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_NCA},
  author       = {Ssemakula, John Bosco and Gorricho, Juan-Luis and Kibalya, Godfrey and Serrat-Fernandez, Joan},
  doi          = {10.1007/s00521-024-10754-6},
  journal      = {Neural Computing and Applications},
  month        = {4},
  number       = {10},
  pages        = {7443},
  shortjournal = {Neural Comput. Appl.},
  title        = {Correction: An artificial intelligence strategy for the deployment of future microservice-based applications in 6G networks},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). CONELPABO: Composite networks learning via parallel bayesian optimization to predict remaining useful life in predictive maintenance. <em>NCA</em>, <em>37</em>(10), 7423-7441. (<a href='https://doi.org/10.1007/s00521-025-10995-z'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Maintaining equipment and machinery in industries is imperative for maximizing operational efficiency and prolonging their lifespan. The adoption of predictive maintenance enhances resource allocation, productivity, and product quality by proactively identifying and addressing potential equipment anomalies through rigorous data analysis before they escalate into critical issues. Consequently, these measures strengthen market competitiveness and generate favorable economic outcomes. In many applications, sensors operate at high frequencies or capture data over extended periods. This work introduces CONELPABO (Composite Networks Learning via Parallel Bayesian Optimization), a framework for analyzing long time series data, particularly for predicting the remaining useful life of a system or component. It uses a divide-and-conquer strategy to manage the exponential growth in the hyperparameter search space during Bayesian Optimization and to accelerate model training by 50%. Additionally, this strategy enables the training of deeper networks with limited resources. The usefulness of the framework is demonstrated through two case studies, in which it achieves state-of-the-art results, showing that CNN-CNN and RNN-RNN architectures are highly effective for long time-series data. These architectures outperform many existing approaches and challenge the common academic focus on CNN-RNN hybrids.},
  archive      = {J_NCA},
  author       = {Solís-Martín, David and Galán-Páez, Juan and Borrego-Díaz, Joaquín},
  doi          = {10.1007/s00521-025-10995-z},
  journal      = {Neural Computing and Applications},
  month        = {4},
  number       = {10},
  pages        = {7423-7441},
  shortjournal = {Neural Comput. Appl.},
  title        = {CONELPABO: Composite networks learning via parallel bayesian optimization to predict remaining useful life in predictive maintenance},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Dynamic economic dispatch with uncertain wind power generation using an enhanced artificial hummingbird algorithm. <em>NCA</em>, <em>37</em>(10), 7397-7422. (<a href='https://doi.org/10.1007/s00521-025-10982-4'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The optimum scheduling of the conventional thermal generators for three different dynamic test systems is percolated in this article. In pursuit of this objective, a developed version of a recent optimization algorithm, denoted as the leader artificial hummingbird algorithm, is introduced. The profile with the largest penetration of wind energy is obtained by calculating wind power from hourly wind speed using the Weibull distribution density function. After that, the test system and the wind profiles were connected to carry out dynamic economic dispatch (DED). The DED problem with wind uncertainty poses important challenges because of its complication, considered by multiple constraints including ramp rate limits and the valve-point effects (VPEs), nonconvexity, and nonlinearity, as well as the uncertainty of the wind energy. These complications make it critical to discover innovative optimization algorithms to find optimum solutions for the DED problem. First, in order to demonstrate the validity of the suggested LAHA approach in comparison with four contemporary techniques, simulations are run on 23 benchmark functions. Next, the 5-unit, 10-unit with/without transmission losses, 15-unit, modified 10-unit with transmission losses, and wind power test systems are used to evaluate the LAHA’s performance. The numerical results demonstrate how competitive the suggested approach is in reaching reduced total generation cost when compared to the other documented optimization algorithms.},
  archive      = {J_NCA},
  author       = {Hassan, Mohamed H. and Mohamed, Ehab Mahmoud and Kamel, Salah and Eslami, Mahdiyeh},
  doi          = {10.1007/s00521-025-10982-4},
  journal      = {Neural Computing and Applications},
  month        = {4},
  number       = {10},
  pages        = {7397-7422},
  shortjournal = {Neural Comput. Appl.},
  title        = {Dynamic economic dispatch with uncertain wind power generation using an enhanced artificial hummingbird algorithm},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). AutoML for shape-writing biometrics. <em>NCA</em>, <em>37</em>(10), 7379-7396. (<a href='https://doi.org/10.1007/s00521-025-10983-3'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Shape-writing is a text entry method that allows users to type words on mobile devices by gliding their finger across the keyboard from one character to the next. This creates a trajectory of touch coordinates that contains rich information about the user. Previous work exploited this information to create Machine Learning (ML) models to predict demographic and behavioral targets, such as age, nationality, or handedness. However, previous work used pseudo-grid search, which is a bit tedious and rather inefficient. We show how to find better models with Automated Machine Learning (AutoML), by completely automating the architecture design process, outperforming all models reported in previous work. Our study suggests that researchers should incorporate AutoML to their training pipelines, as classification performance will likely be better than manually designing the model architecture. Taken together, our results show that it is possible to decode user’s latent information from shape-writing trajectories with higher performance than previously reported.},
  archive      = {J_NCA},
  author       = {Weber, Louis and Leiva, Luis A.},
  doi          = {10.1007/s00521-025-10983-3},
  journal      = {Neural Computing and Applications},
  month        = {4},
  number       = {10},
  pages        = {7379-7396},
  shortjournal = {Neural Comput. Appl.},
  title        = {AutoML for shape-writing biometrics},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Enhancing facial expression recognition in uncontrolled environment: A lightweight CNN approach with pre-processing. <em>NCA</em>, <em>37</em>(10), 7363-7378. (<a href='https://doi.org/10.1007/s00521-025-10974-4'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Facial expressions play a key role in human non-verbal type of communication, providing key insights into emotions and intentions. These expressions serve as universal signals, helping individuals convey their internal states across various personal and social contexts. With the growing interest in automatic facial emotion recognition, deep neural networks have emerged as a popular approach for detecting human emotions, even under challenging, real-world conditions. However, external factors can affect the system's performance, degrading the quality of facial features and making emotion detection more difficult. In the presented paper, we propose a highly optimized lightweight convolutional neural network (LCNN) for emotion recognition in controlled and uncontrolled environments. The proposed model is designed to learn hidden nonlinear patterns from facial images. The proposed convolutional neural network consisting a series of convolutional layers followed by max-pooling layers. The model's performance is evaluated with and without pre-processing steps to highlight the importance of pre-processing in improving detection accuracy. The LCNN achieves 65% accuracy on the FER-2013 dataset and 98% on the CK + dataset.},
  archive      = {J_NCA},
  author       = {Grover, Richa and Bansal, Sandhya},
  doi          = {10.1007/s00521-025-10974-4},
  journal      = {Neural Computing and Applications},
  month        = {4},
  number       = {10},
  pages        = {7363-7378},
  shortjournal = {Neural Comput. Appl.},
  title        = {Enhancing facial expression recognition in uncontrolled environment: A lightweight CNN approach with pre-processing},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Enhancing electroencephalogram signal quality in epileptic patients using bidirectional stochastic long short-term memory network. <em>NCA</em>, <em>37</em>(10), 7339-7361. (<a href='https://doi.org/10.1007/s00521-025-10977-1'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Artifacts frequently disrupt electroencephalogram (EEG) signal recordings, originating from diverse sources such as eye-blinks and muscle twitches. These artifacts present significant challenges when employing automated systems for diagnosing neurological disorders. In this research, we introduce an innovative architectural solution designed to effectively eliminate these artifacts from EEG signals acquired from individuals with epilepsy. Our proposed framework combines bidirectional long short-term memory networks with bidirectional stochastic configuration networks (BSCN). This integration empowers the model to discern intricate patterns within both past and future time steps of the EEG signal. Furthermore, the non-iterative training characteristic of the BSCN-based classifier enhances training efficiency. To assess the effectiveness of our approach, we conducted experiments on four epilepsy datasets and a sleep dataset. The performance of our novel technique was evaluated using a range of performance metrics, and the results unequivocally indicate its superiority over existing artifact removal methods.},
  archive      = {J_NCA},
  author       = {Pandey, Anviti and Singh, Sanjay Kumar and Udmale, Sandeep S. and Shukla, K. K.},
  doi          = {10.1007/s00521-025-10977-1},
  journal      = {Neural Computing and Applications},
  month        = {4},
  number       = {10},
  pages        = {7339-7361},
  shortjournal = {Neural Comput. Appl.},
  title        = {Enhancing electroencephalogram signal quality in epileptic patients using bidirectional stochastic long short-term memory network},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A novel human actions recognition and classification using semantic segmentation with deep learning techniques. <em>NCA</em>, <em>37</em>(10), 7321-7337. (<a href='https://doi.org/10.1007/s00521-024-10962-0'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This study introduces a novel method for recognizing human actions through the semantic segmentation of images. The aim is to enhance action-motion dynamics by directing attention toward regions crucial for action recognition. The proposed approach utilizes a SegNet model with an incorporated attention mechanism and modified bidirectional gated recurrent unit (BiGRU) backbone. The process begins with the generation of binary masks for each frame in a video dataset, achieved through a combination of grayscale conversion, Gaussian blurring, and adaptive thresholding. The emphasis on crucial regions for action recognition and capturing temporal variations is heightened through the application of the frame-ranking method. In our experiments, we observed that the proposed method significantly enhances the dynamics of the action-motion representation. The SegNet architecture was designed for semantic segmentation tasks and features an encoder-decoder architecture. In this structure, the model performs hierarchical feature extraction from the input image via the encoder, whereas the decoder focuses on reconstructing the segmented output. Attention is paid to the encoded feature maps, augmenting the model's capability to capture dependencies over extensive spatial ranges. A bidirectional GRU layer is employed to capture the sequential dependencies in the concatenated feature maps. The integration of the SegNet model with the attention mechanism and a BiGRU backbone, featuring an encoder-decoder architecture for feature extraction, classification, and segmentation, demonstrated superior performance in capturing nuanced spatiotemporal features. The proposed method demonstrated an accuracy of 98.52% for UCF101 and 84.25% for HMDB51. The findings reveal that the model achieves state-of-the-art results in human action recognition tasks, outperforming the existing methods in terms of accuracy. The combination of semantic segmentation and BiGRU-based temporal modeling proved effective in discerning intricate patterns of human motion, showcasing its potential for real-world applications in video analysis and surveillance systems.},
  archive      = {J_NCA},
  author       = {Jayamohan, M. and Yuvaraj, S.},
  doi          = {10.1007/s00521-024-10962-0},
  journal      = {Neural Computing and Applications},
  month        = {4},
  number       = {10},
  pages        = {7321-7337},
  shortjournal = {Neural Comput. Appl.},
  title        = {A novel human actions recognition and classification using semantic segmentation with deep learning techniques},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Quasi-synchronization analysis of heterogeneous neural networks with multiple delays under impulsive control. <em>NCA</em>, <em>37</em>(10), 7303-7319. (<a href='https://doi.org/10.1007/s00521-024-10948-y'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, the quasi-synchronization of impulsive controlled heterogeneous dynamic neutral networks with time-varying delay, distributed delays and proportional delay is discussed. Compared with the existing literature, the significant advantage of this paper is that all three types of delays are taken into account. Here we consider time-varying delay depending on probability distribution conditions, so the results of this paper also rely on the problem of probability distribution of time-varying delay. By establishing a suitable comparison system, creating a new kind of impulsive delay inequality and applying Bernoulli distributions and Lyapunov theory, some conditions to realize quasi-synchronization of heterogeneous neural networks are studied. Finally we illustrate the validity of our theorem with numerical examples.},
  archive      = {J_NCA},
  author       = {Wang, Qing and Guo, Yingxin and Zhang, Chuan and Fu, Jianting},
  doi          = {10.1007/s00521-024-10948-y},
  journal      = {Neural Computing and Applications},
  month        = {4},
  number       = {10},
  pages        = {7303-7319},
  shortjournal = {Neural Comput. Appl.},
  title        = {Quasi-synchronization analysis of heterogeneous neural networks with multiple delays under impulsive control},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Enhanced model for abstractive arabic text summarization using natural language generation and named entity recognition. <em>NCA</em>, <em>37</em>(10), 7279-7301. (<a href='https://doi.org/10.1007/s00521-024-10949-x'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the rise of Arabic digital content, effective summarization methods are essential. Current Arabic text summarization systems face challenges such as language complexity and vocabulary limitations. We introduce an innovative framework using Arabic Named Entity Recognition to enhance abstractive summarization, crucial for NLP applications like question answering and knowledge graph construction. Our model, based on natural language generation techniques, adapts to diverse datasets. It identifies key information, synthesizes it into coherent summaries, and ensures grammatical accuracy through deep learning. Evaluated on the EASC dataset, our model achieved a 74% ROUGE1 score and a 97.6% accuracy in semantic coherence, with high readability and relevance scores. This sets a new standard for Arabic text summarization, greatly improving NLP information processing.},
  archive      = {J_NCA},
  author       = {Essa, Nada and El-Gayar, M. M. and El-Daydamony, Eman M.},
  doi          = {10.1007/s00521-024-10949-x},
  journal      = {Neural Computing and Applications},
  month        = {4},
  number       = {10},
  pages        = {7279-7301},
  shortjournal = {Neural Comput. Appl.},
  title        = {Enhanced model for abstractive arabic text summarization using natural language generation and named entity recognition},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Low-light few-shot object detection via curve contrast enhancement and flow-encoder-based variational autoencoder. <em>NCA</em>, <em>37</em>(10), 7261-7278. (<a href='https://doi.org/10.1007/s00521-024-10885-w'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Aiming at the problem of insufficient samples in low-light object detection in some environments, a low-light few-shot object detection method based on curve contrast enhancement and flow-encoder-based variational autoencoder (CCEFVAE) is proposed. Our approach involves designing a CCE module to enhance the detailed features and contrast of low-light images by deriving a relationship expression between the enhanced image and the low-light image through the recursive relationship of high-order curves. The lumination estimation module in the CCE module estimates the parameters of the expression to calculate the pixel values of the enhanced image. Moreover, we propose an FVAE module to improve the decoupling of support features by combining the flow model encoder with the variational autoencoder, facilitating subsequent feature aggregation and classification. To ensure the consistency of the loss function of the flow model with the few-shot object detection loss, we design a negative Jacobian determinant transformation function. This enables direct addition of the two losses, allowing for unified optimization. Experimental results demonstrate that our proposed algorithm outperforms mainstream few-shot object detection models by an average of 13.1–23% in average after training on the low-light dataset (ExDark), and shows an average improvement of 5.8% compared to the state-of-the-art (SOTA) few-shot object detection model VFA. When trained on the normal lighting dataset (PASCAL VOC), the proposed algorithm exhibits a 1.7% improvement in average compared to VFA.},
  archive      = {J_NCA},
  author       = {Jiang, Zetao and Jin, Xin and Kang, Junjie},
  doi          = {10.1007/s00521-024-10885-w},
  journal      = {Neural Computing and Applications},
  month        = {4},
  number       = {10},
  pages        = {7261-7278},
  shortjournal = {Neural Comput. Appl.},
  title        = {Low-light few-shot object detection via curve contrast enhancement and flow-encoder-based variational autoencoder},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Supercell thunderstorm algorithm (STA): A nature-inspired metaheuristic algorithm for engineering optimization. <em>NCA</em>, <em>37</em>(10), 7207-7260. (<a href='https://doi.org/10.1007/s00521-024-10848-1'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, an optimization algorithm called supercell thunderstorm algorithm (STA) is proposed. STA draws inspiration from the strategies employed by storms, such as spiral motion, tornado formation, and the jet stream. It is a computational algorithm specifically designed to simulate and model the behavior of supercell thunderstorms. These storms are known for their rotating updrafts, strong wind shear, and potential for generating tornadoes. The optimization procedures of the STA algorithm are based on three distinct approaches: exploring a divergent search space using spiral motion, exploiting a convergent search space through tornado formation, and navigating through the search space with the aid of the jet stream. To evaluate the effectiveness of the proposed STA algorithm in achieving optimal solutions for various optimization problems, a series of test sequences were conducted. Initially, the algorithm was tested on a set of 23 well-established functions. Subsequently, the algorithm’s performance was assessed on more complex problems, including ten CEC2019 test functions, in the second experimental sequence. Finally, the algorithm was applied to five real-world engineering problems to validate its effectiveness. The experimental results of the STA algorithm were compared to those of contemporary metaheuristic methods. The analysis clearly demonstrates that the developed STA algorithm outperforms other methods in terms of performance.},
  archive      = {J_NCA},
  author       = {Hassan, Mohamed H. and Kamel, Salah},
  doi          = {10.1007/s00521-024-10848-1},
  journal      = {Neural Computing and Applications},
  month        = {4},
  number       = {10},
  pages        = {7207-7260},
  shortjournal = {Neural Comput. Appl.},
  title        = {Supercell thunderstorm algorithm (STA): A nature-inspired metaheuristic algorithm for engineering optimization},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Beyond administrative reports: A deep learning framework for classifying and monitoring crime and accidents leveraging large-scale online news. <em>NCA</em>, <em>37</em>(10), 7183-7205. (<a href='https://doi.org/10.1007/s00521-024-10833-8'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The escalating prevalence of violent crimes and accidents underscores the urgent need for efficient and timely monitoring systems. Traditional methods reliant on administrative reports often suffer from significant delays. This paper proposes CRIMSON, a novel framework that leverages large-scale online news to provide real-time insights into crime and accident trends. CRIMSON utilizes a multi-label classification technique that leverages a fine-tuned, pre-trained, cross-lingual language model to accurately categorize news articles. Our experimental results, conducted on a substantial dataset of Thai news articles, demonstrate superior performance, achieving an average F1 score of 86%. Beyond classification, CRIMSON aggregates categorized news into real-time statistics, revealing strong correlations between news-reported incidents and official crime data. This study pioneers online news as a reliable and timely crime and accident monitoring source, offering valuable insights for law enforcement, policymakers, and researchers.},
  archive      = {J_NCA},
  author       = {Tuarob, Suppawong and Tatiyamaneekul, Phonarnun and Pongpaichet, Siripen and Tawichsri, Tanisa and Noraset, Thanapon},
  doi          = {10.1007/s00521-024-10833-8},
  journal      = {Neural Computing and Applications},
  month        = {4},
  number       = {10},
  pages        = {7183-7205},
  shortjournal = {Neural Comput. Appl.},
  title        = {Beyond administrative reports: A deep learning framework for classifying and monitoring crime and accidents leveraging large-scale online news},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). GP-PSENet: A group-related dilated and a parallel extensional dilation-wise residual encoder for scene text detection. <em>NCA</em>, <em>37</em>(10), 7159-7181. (<a href='https://doi.org/10.1007/s00521-024-10688-z'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {At present, scene text detection is grabbing more and more attention as an offshoot of machine vision. However, due to the existing long types of text instances and complex background context, less exact localization and higher missed detection cases still remain in text detection domain. Accordingly, with the aim of tackling these two issues, we propose a text detector named GP-PSENet that comprises a combination of a group-related dilated encoder, a parallel extensional dilation-wise residual encoder and a mixed upsample. Firstly, feature maps of the lowest level processed by the backbone network are sent to a dilated encoder with group linkage. And the group residual module provides stratification to join group coefficients and dilated factors. This module can enhance the correctness of predictions about longer boundary boxes. Secondly, semantic information from the highest level is fed into a parallel extensional dilation-wise residual encoder. The extensional dilation-wise module is capable of obtaining diverse receptive fields by more parallel branches. And it can alleviate error detection from interfering material in the background. Thirdly, the feature maps processed in the second step are given to the mixed upsample module for transforming so as to the next fuse. Finally, the processed two-level feature maps are fused and sent to the progressive scale expansion algorithm for the final post-processing to gain the predicted coordinate points. Ablation experiments are conducted on CTW1500, ICDAR15, MSRA-TD500 and Total-Text datasets to confirm the availability of the proposed method. The values of precision on these datasets reach 86.24%, 87.84%, 73.98% and 90.48%. The proposed method is also competitive with other scene detection methods.},
  archive      = {J_NCA},
  author       = {Huang, Liwen and Yang, Wenyuan},
  doi          = {10.1007/s00521-024-10688-z},
  journal      = {Neural Computing and Applications},
  month        = {4},
  number       = {10},
  pages        = {7159-7181},
  shortjournal = {Neural Comput. Appl.},
  title        = {GP-PSENet: A group-related dilated and a parallel extensional dilation-wise residual encoder for scene text detection},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Real-time optimal energy management of microgrid based on multi-agent proximal policy optimization. <em>NCA</em>, <em>37</em>(10), 7145-7157. (<a href='https://doi.org/10.1007/s00521-024-10654-9'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In order to achieve economic operation of the microgrid (MG), energy management problem (EMP) has attracted attention from scholars worldwide. In order to overcome the lack of flexibility when coping with uncertainties and topology changes, a multi-agent based proximal policy optimization algorithm (MAPPO) is proposed in this paper. Different from the offline training and online implementing mode, the proposed decentralized MAPPO algorithm has the characteristic of online training and online application, which can get higher optimization efficiency and lower communication burden. Taking into account users’ satisfaction, renewable energy utilization rate and operating costs, an optimization model is established. Aiming at the difficulty on satisfying the power balance constraint in EMPU using reinforcement learning (RL), a novel power imbalance penalty is designed. Compared with the traditional penalty function, the proposed penalty function can effectively avoid the phenomenon of power imbalance. Finally, 24-hour energy management results are provided to verify the effectiveness of the proposed algorithm. Moreover, the proposed MAPPO is compared with several popular multi-agent based RL algorithms. Simulation results show that the proposed algorithm has higher efficiency and can obtain better energy management strategies.},
  archive      = {J_NCA},
  author       = {Wang, Danlu and Sun, Qiuye and Su, Hanguang},
  doi          = {10.1007/s00521-024-10654-9},
  journal      = {Neural Computing and Applications},
  month        = {4},
  number       = {10},
  pages        = {7145-7157},
  shortjournal = {Neural Comput. Appl.},
  title        = {Real-time optimal energy management of microgrid based on multi-agent proximal policy optimization},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multi-modal consistent loss diffusion model for sentinel-3 single image super resolution. <em>NCA</em>, <em>37</em>(10), 7121-7143. (<a href='https://doi.org/10.1007/s00521-024-10573-9'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the context of Earth observation, the trade-off between spatial, spectral, and temporal resolution often limits the versatility of remote sensing images in many important applications. In response, this paper introduces a novel deep learning diffusion model, specifically tailored to improve the spatial resolution of the optical products acquired by the Sentinel-3 (S3) satellite. Our framework employs a diffusion probabilistic model, benefiting from the higher spatial resolution of the Sentinel-2 satellite during training via a new multi-modal loss formulation. This ensures consistency with the original S3 images while enhancing the spatial details. Two distinct conditional low-resolution encoders were experimented with, providing insights into their respective contributions to the diffusion process. The efficacy of the proposed model is demonstrated through extensive ablation studies and comparisons with state-of-the-art methods, using both synthetic and real S3 products. The findings indicate that our model successfully improves spatial resolution while maintaining the integrity of the spectral information, contributing to the field of remote sensing single-image super-resolution.},
  archive      = {J_NCA},
  author       = {Ibañez, Damian and Fernandez-Beltran, Ruben and Pla, Filiberto and Yokoya, Naoto and Xia, Junshi},
  doi          = {10.1007/s00521-024-10573-9},
  journal      = {Neural Computing and Applications},
  month        = {4},
  number       = {10},
  pages        = {7121-7143},
  shortjournal = {Neural Comput. Appl.},
  title        = {Multi-modal consistent loss diffusion model for sentinel-3 single image super resolution},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Dynamic generative R-CNN. <em>NCA</em>, <em>37</em>(10), 7107-7120. (<a href='https://doi.org/10.1007/s00521-024-10739-5'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Different techniques have been developed for object detection and recognition. These techniques can be divided into single-shot and two-shot methods. Single-shot methods focus on real-time applications, while two-shot methods are used in applications requiring higher accuracy. However, different versions of the two-shot techniques produce limited results in terms of accuracy and speed, or both. Therefore, this study proposes a novel model called dynamic generative R-CNN (DGR-CNN) that reduces the number of proposed regions using a dynamic programming model that applies the graph similarity method over graph-based image segmentation. Additionally, the proposed model employs DCGAN technique to improve detection performance. DGR-CNN reduces the overall detection and classification time and enhances the detection accuracy. The PASCAL VOC2007 and MS COCO datasets were utilized to evaluate the model. The results showed that DGR-CNN significantly reduces the number of candidate regions compared to the selective search algorithm employed in R-CNN and fast R-CNN. Although fast R-CNN utilizes 2000 regions and faster R-CNN utilizes 300 regions, DGR-CNN reduces the number of regions to approximately 130. The mean average precision of the proposed method was 75.1% on the PASCAL VOC2007, while fast and faster R-CNN scored 66.9% and 69.9%, respectively. Moreover, the DGR-CNN model significantly improved the classification accuracy when tested on the MS COCO dataset, achieving an MAP of 68.76%, compared with 32.64% and 42.3% for fast and faster R-CNN. This increase in accuracy was achieved without significantly compromising the speed compared with faster R-CNN.},
  archive      = {J_NCA},
  author       = {Saffarini, Rasha and Khamayseh, Faisal and Awwad, Yousef and Sabha, Muath and Eleyan, Derar},
  doi          = {10.1007/s00521-024-10739-5},
  journal      = {Neural Computing and Applications},
  month        = {4},
  number       = {10},
  pages        = {7107-7120},
  shortjournal = {Neural Comput. Appl.},
  title        = {Dynamic generative R-CNN},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Optimizing a continuous action learning automata (CALA) optimizer for training artificial neural networks. <em>NCA</em>, <em>37</em>(10), 7089-7105. (<a href='https://doi.org/10.1007/s00521-024-10546-y'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As deep artificial neural networks (ANNs) get bigger, deeper, and used in more challenging applications, the need for non-gradient based training methods becomes more desirable. This paper explores a new non-gradient-based method to train ANNs and deep ANNs, the Continuous Action Learning Automata (CALA) optimizer. The CALA optimizer assigns a Learning Automata agent to every weight in a neural network and uses game theory to coordinate actions of the agents. We show that the CALA optimizer is computationally efficient, that it converges to a desired error rate faster than current gradient-based methods like stochastic gradient descent (SGD) and show how one could use a Finite Action Learning Automata (FALA) algorithm to find optimal values for the hyper-parameters required to optimize the CALA controller. The CALA method contrasts itself against other non-gradient methods in that it approaches the computational efficiency of top gradient descent methods like SGD. The CALA method converges fast, and there is any easy-to-follow algorithm to tune the hyper-parameters of the algorithm. These advantages address weaknesses that other non-gradient methods suffer from. Therefore, the CALA controller has the potential to see far greater implementation than other non-gradient-based optimization methods for training deep ANNs.},
  archive      = {J_NCA},
  author       = {Lindsay, James and Givigi, Sidney},
  doi          = {10.1007/s00521-024-10546-y},
  journal      = {Neural Computing and Applications},
  month        = {4},
  number       = {10},
  pages        = {7089-7105},
  shortjournal = {Neural Comput. Appl.},
  title        = {Optimizing a continuous action learning automata (CALA) optimizer for training artificial neural networks},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). An adaptive dual-weighted feature network for insulator detection in transmission lines. <em>NCA</em>, <em>37</em>(10), 7067-7087. (<a href='https://doi.org/10.1007/s00521-024-10957-x'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the field of electrical power applications, high-voltage insulators necessitate routine inspection to assure the security and stability of the whole electric power system operation. Accurately positioning the insulator is extremely crucial for proceeding to the insulator defect detection. However, during UAV electrical line inspection, the presence of the electric power line magnetic field engenders a reduction in the pixel representation of the insulator within the image data, thereby diminishing the accuracy of insulator detection. In response to the prevailing issues, we present the creation of the adaptive dual-weighted feature network in this paper. Simultaneously, we create an insulator dataset to substantiate the effectiveness of enhanced model in detecting small insulators. Firstly, the integration of context fusion network is employed to capture comprehensive contextual features for each effective feature map. In addition, a cross-scale residual perception network is incorporated into the neck prior to three concatenation modules, facilitating the collection of diverse information across levels. Finally, a Dual-Weighted Feature Fusion module is designed to replace the conventional concatenation pattern within the neck, thus achieving a more precise representation of object features. Experiments are conducted on the insulator dataset, the RSOD dataset and the NWPU VHR-10 dataset to evaluate the designed model, resulting in mAP values that were 3.92%, 1.55% and 2.39% higher than the YOLOv7, respectively.},
  archive      = {J_NCA},
  author       = {Zhang, Jie and Wang, Xiabing and Li, Yinhua and Li, Dailin and Wang, Fengxian and Li, Linwei and Zhang, Huanlong and Shi, Xiaoping},
  doi          = {10.1007/s00521-024-10957-x},
  journal      = {Neural Computing and Applications},
  month        = {4},
  number       = {10},
  pages        = {7067-7087},
  shortjournal = {Neural Comput. Appl.},
  title        = {An adaptive dual-weighted feature network for insulator detection in transmission lines},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Toward better semantic segmentation by retaining spectral information using matched wavelet pooling. <em>NCA</em>, <em>37</em>(10), 7049-7066. (<a href='https://doi.org/10.1007/s00521-025-11008-9'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Pooling operations, such as average pooling, strided convolution, and max pooling, have become fundamental components of convolutional neural networks (CNNs) due to their ability to capture local features, expand receptive fields, and reduce computational costs. However, in the context of semantic segmentation, these pooling techniques can lead to the loss of crucial spatial details that are necessary for accurate pixel-level predictions. To tackle this issue, extensive research has focused on refining deep CNN models through architectural adaptations and novel training methods. Recent studies have demonstrated the importance of pooling layers, exemplified by innovations like the introduction of wavelet pooling. In our study, we highlight the value of incorporating our previously proposed matched wavelet pooling (MWP) into CNNs to enhance semantic segmentation pipelines. The core concept of MWP challenges the notion that including all sub-bands generated from wavelet decomposition consistently improves accuracy. Instead, we advocate for selecting specific sub-bands for the pooling process in each image during both training and testing. This approach introduces sub-band selection protocols customized for image-specific pooling, designed specifically for semantic segmentation CNN architectures, with a particular focus on the UNet and SegNet models. Across three widely used datasets, our proposed MWP- based pipeline, featuring the MWP-UNet architecture, consistently outperforms conventional pooling methods. It achieves a significant average improvement in intersection over union (IoU) of over 25% compared to recent literature. Additionally, our MWP-SegNet model outperformed the standard SegNet by 12.5% mIoU, further demonstrating the effectiveness of our matched wavelet pooling approach across different network architectures.},
  archive      = {J_NCA},
  author       = {El-Khamy, Said and El-Bana, Shimaa and Al-Kabbany, Ahmad and Elragal, Hassan},
  doi          = {10.1007/s00521-025-11008-9},
  journal      = {Neural Computing and Applications},
  month        = {4},
  number       = {10},
  pages        = {7049-7066},
  shortjournal = {Neural Comput. Appl.},
  title        = {Toward better semantic segmentation by retaining spectral information using matched wavelet pooling},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A comprehensive survey on arabic text augmentation: Approaches, challenges, and applications. <em>NCA</em>, <em>37</em>(10), 7015-7048. (<a href='https://doi.org/10.1007/s00521-025-11020-z'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Arabic is a linguistically complex language with a rich structure and valuable syntax that pose unique challenges for natural language processing (NLP), primarily due to the scarcity of large, reliable annotated datasets essential for training models. The varieties of dialects and mixtures of more than one language within a single conversation further complicate the development and efficacy of deep learning models targeting Arabic. Data augmentation (DA) techniques have emerged as a promising solution to tackle data scarcity and improve model performance. However, implementing DA in Arabic NLP presents its challenges, particularly in maintaining semantic integrity and adapting to the language’s intricate morphological structure. This survey comprehensively examines various aspects of Arabic data augmentation techniques, covering strategies for model training, methods for evaluating augmentation performance, understanding the effects and applications of augmentation on data, studying NLP downstream tasks, addressing augmentation problems, proposing solutions, conducting in-depth literature reviews, and drawing conclusions. Through detailed analysis of 75 primary and 9 secondary papers, we categorize DA methods into diversity enhancement, resampling, and secondary approaches, each targeting specific challenges inherent in augmenting Arabic datasets. The goal is to offer insights into DA effectiveness, identify research gaps, and suggest future directions for advancing NLP in Arabic.},
  archive      = {J_NCA},
  author       = {ElSabagh, Ahmed Adel and Azab, Shahira Shaaban and Hefny, Hesham Ahmed},
  doi          = {10.1007/s00521-025-11020-z},
  journal      = {Neural Computing and Applications},
  month        = {4},
  number       = {10},
  pages        = {7015-7048},
  shortjournal = {Neural Comput. Appl.},
  title        = {A comprehensive survey on arabic text augmentation: Approaches, challenges, and applications},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Decoding silent speech: A machine learning perspective on data, methods, and frameworks. <em>NCA</em>, <em>37</em>(10), 6995-7013. (<a href='https://doi.org/10.1007/s00521-024-10456-z'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {At the nexus of signal processing and machine learning (ML), silent speech recognition (SSR) has evolved as a game-changing technology that allows for communication without audible voice. This study offers a thorough overview of SSR, tracing its evolution from early waveform analysis to the most recent ML methods. We start by examining current SSR techniques using ML and determining the essential conditions for efficient SSR systems. After that, we look at the datasets and data collection techniques currently employed in SSR research, highlighting the difficulties posed by the variety of articulatory movements and the scarcity of data. Examining state-of-the-art SSR frameworks, the paper covers important topics such signal processing, feature extraction, ML techniques for decoding and optimizing and assessing the performance of SSR models. We emphasize how deep learning (DL) and ML models have evolved to increase SSR resilience and accuracy. The field's proposed procedures are examined, with an emphasis on sophisticated feature extraction and classification methods. Modern SSR techniques are compared in terms of performance, highlighting the advantages and disadvantages of different models. There is also discussion of ethical issues, especially those pertaining to privacy and consent. The integration of multimodal information—visual cues, electromyography signals, and neuroimaging data—to improve SSR systems is covered in this work. We investigate the functions of transfer learning and domain adaptation in handling cross-subject variability. Lastly, the study offers suggestions and future prospects for SSR research, providing practitioners, engineers, and academics with a road map. As SSR continues to push the frontiers of human–machine interaction, our study aims to increase our collective understanding of the technological advances and societal effects of SSR in the ML age.},
  archive      = {J_NCA},
  author       = {Chowdhury, Adiba Tabassum and Newaz, Mehrin and Saha, Purnata and AbuHaweeleh, Mohannad Natheef and Mohsen, Sara and Bushnaq, Diala and Chabbouh, Malek and Aljindi, Raghad and Pedersen, Shona and Chowdhury, Muhammad E. H.},
  doi          = {10.1007/s00521-024-10456-z},
  journal      = {Neural Computing and Applications},
  month        = {4},
  number       = {10},
  pages        = {6995-7013},
  shortjournal = {Neural Comput. Appl.},
  title        = {Decoding silent speech: A machine learning perspective on data, methods, and frameworks},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Real-time arabic sign language recognition system using sensory glove and machine learning. <em>NCA</em>, <em>37</em>(9), 6977-6993. (<a href='https://doi.org/10.1007/s00521-025-11010-1'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article aims to present a novel Arabic sign language recognition (SLR) strategy using sensory glove and machine learning. The article focuses on hand gesture recognition through the development of a glove-computer system designed for real-time hand posture detection and gesture-to-text translation. Gesture recognition plays a crucial role in enhancing interactions between humans and machines, making technology more intuitive and efficient. This technology has potential applications in various fields such as smart homes, gaming, automotive systems, and virtual reality. The primary goal of this article is then to create a supportive communication environment for individuals with speaking difficulties. The article began with the development of a sensory glove equipped with sensors to detect hand orientation and finger flexing, with data processed and transmitted wirelessly to a computer for machine learning prediction. A dynamic dataset, which included signs for letters and movement-based signs for words, was created and used to build two machine learning models: Support Vector Machine (SVM) model with feature extraction (SVM-FE model) and Long Short-Term Memory (LSTM) model. The proposed deep learning LSTM model demonstrated superior performance with accuracy of 99.6%. Based on these findings, a real-time recognition application was developed using the LSTM model, effectively showcasing the system's practical applicability in real-world scenarios.},
  archive      = {J_NCA},
  author       = {Halabi, Mohamad and Harkouss, Youssef},
  doi          = {10.1007/s00521-025-11010-1},
  journal      = {Neural Computing and Applications},
  month        = {3},
  number       = {9},
  pages        = {6977-6993},
  shortjournal = {Neural Comput. Appl.},
  title        = {Real-time arabic sign language recognition system using sensory glove and machine learning},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). An improved transfer learning model for detection of insulator defects in power transmission lines. <em>NCA</em>, <em>37</em>(9), 6951-6976. (<a href='https://doi.org/10.1007/s00521-025-11011-0'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Insulators are critical components of transmission lines but are prone to failures that can jeopardize the safe operation of electrical power systems. Accurate detection of insulator defects is essential for timely maintenance. With advancements in object detection algorithm and artificial intelligence, insulator defect detection has garnered significant attention. However, detection accuracy remains an issue. To address this, we propose an improved transfer learning model. Our approach incorporates the Mish activation function and a global context network module to enhance the model's performance. The improved YOLOv9 model is trained and tested using two public datasets: the insulator defect image dataset and the China power line insulator dataset. Experimental results demonstrate that our model achieves optimal detection precision and recall rates of 99.84 and 99.92%, respectively—improvements of 1.06 and 1.09% over the actual YOLOv9. Additionally, our model outperforms other algorithms, such as RTDETR and SSD, particularly in adapting to complex backgrounds and detecting small targets.},
  archive      = {J_NCA},
  author       = {Pradeep, V. and Baskaran, K. and Evangeline, S. Ida},
  doi          = {10.1007/s00521-025-11011-0},
  journal      = {Neural Computing and Applications},
  month        = {3},
  number       = {9},
  pages        = {6951-6976},
  shortjournal = {Neural Comput. Appl.},
  title        = {An improved transfer learning model for detection of insulator defects in power transmission lines},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). PDC-ViT: Source camera identification using pixel difference convolution and vision transformer. <em>NCA</em>, <em>37</em>(9), 6933-6949. (<a href='https://doi.org/10.1007/s00521-025-11004-z'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Source camera identification has emerged as a vital solution to unlock incidents involving critical cases like terrorism, violence, and other criminal activities. The ability to trace the origin of an image/video can aid law enforcement agencies in gathering evidence and constructing the timeline of events. Moreover, identifying the owner of a certain device narrows down the area of search in a criminal investigation where smartphone devices are involved. This paper proposes a new pixel-based method for source camera identification, integrating Pixel Difference Convolution (PDC) with a Vision Transformer network (ViT), and named PDC-ViT. While the PDC acts as the backbone for feature extraction by exploiting Angular PDC (APDC) and Radial PDC (RPDC). These techniques enhance the capability to capture subtle variations in pixel information, which are crucial for distinguishing between different source cameras. The second part of the methodology focuses on classification, which is based on a Vision Transformer network. Unlike traditional methods that utilize image patches directly for training the classification network, the proposed approach uniquely inputs PDC features into the Vision Transformer network. To demonstrate the effectiveness of the PDC-ViT approach, it has been assessed on five different datasets, which include various image contents and video scenes. The method has also been compared with state-of-the-art source camera identification methods. Experimental results demonstrate the effectiveness and superiority of the proposed system in terms of accuracy and robustness when compared to its competitors. For example, our proposed PDC-ViT has achieved an accuracy of 94.30%, 84%, 94.22% and 92.29% using the Vision dataset, Daxing dataset, Socrates dataset and QUFVD dataset, respectively.},
  archive      = {J_NCA},
  author       = {Elharrouss, Omar and Akbari, Younes and Almadeed, Noor and Al-Maadeed, Somaya and Khelifi, Fouad and Bouridane, Ahmed},
  doi          = {10.1007/s00521-025-11004-z},
  journal      = {Neural Computing and Applications},
  month        = {3},
  number       = {9},
  pages        = {6933-6949},
  shortjournal = {Neural Comput. Appl.},
  title        = {PDC-ViT: Source camera identification using pixel difference convolution and vision transformer},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). An effective deep learning-based automatic prediction and classification of alzheimer's disease using EGELU-SZN technique. <em>NCA</em>, <em>37</em>(9), 6915-6932. (<a href='https://doi.org/10.1007/s00521-025-10994-0'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, Alzheimer’s disease (AD) has become a serious hazard to human health. Therefore, an optimal strategy for formulating the treatment plan is the AD’s early diagnosis. In spite of this, no effective treatment or accurate diagnosis exists currently. Also, the pre-selection of brain regions is a complicated task. Therefore, an efficient AD classification is needed. Hence, by utilizing the Exponential Gaussian Error Linear Unit–Squeeze Net (EGELU-SZN) technique, an early prediction as well as classification of AD is proposed. Primarily, from the Alzheimer’s Disease Neuroimaging Initiative (ADNI) dataset, the input brain Magnetic Resonance Imaging (MRI) images are gathered. For the brain MRI images, a skull stripping process is executed. Here, by utilizing the Adaptive Median Otsu’s Thresholding (AMOT) technique, the skull along with cerebral tissues like fat and skin around the brain are removed. After that, to eliminate the noise, pre-processing is computed. After pre-processing, to segment the brain region accurately, segmentation is evaluated. Therefore, an effectual segmentation algorithm termed Rectilinear Mayfly Optimization-centric Automatic Seeded Region Growing algorithm (RMF-ASRG) has been utilized. Features are extracted as of the segmented brain region. Later, by utilizing the Reflective Correlation Principal Component Analysis (RCPCA) algorithm, the reduction of features is performed. Then, the predicted outcomes are classified as AD, Cognitive Normal (CN), as well as Mild Cognitive Impairment (MCI) by employing the EGELU-SZN Classifier. The proposed EGELU-SZN attained the accuracy, precision, recall, specificity, and sensitivity values of 95.9882%, 94.1661%, 93.2327%, 92.8744%, and 96.2327%, respectively, in the classification process. Experimental outcomes signified that superior performance was attained by the proposed methodology when analyzed with benchmark methodologies.},
  archive      = {J_NCA},
  author       = {Sathyabhama, B. and Kannan, M.},
  doi          = {10.1007/s00521-025-10994-0},
  journal      = {Neural Computing and Applications},
  month        = {3},
  number       = {9},
  pages        = {6915-6932},
  shortjournal = {Neural Comput. Appl.},
  title        = {An effective deep learning-based automatic prediction and classification of alzheimer's disease using EGELU-SZN technique},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Enhancing text understanding of decoder-based model by leveraging parameter-efficient fine-tuning method. <em>NCA</em>, <em>37</em>(9), 6899-6913. (<a href='https://doi.org/10.1007/s00521-025-10975-3'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Machine reading comprehension (MRC) is a fundamental natural language understanding task in natural language processing, which aims to comprehend the text of a given passage and answer questions based on it. Understanding implicit information, deducing the logical structure of information, and connecting context from different pieces of information make the MRC task difficult. Most current state-of-the-art approaches for MRC are using encoder-based models. However, no earlier research proposed a decoder-only model for MRC question-answering datasets, although language models based on this category achieved unprecedented performance in different generative tasks. In this paper, we propose a parameter-efficient fine-tuning framework that effectively increases MRC capabilities on decoder-only large language models. This framework designs the process for MRC and introduces the low-rank adaptation (LoRA) method to effectively fine-tune the large model with many parameters, even with lower hardware resource requirements than the previous methods. In addition, we also integrate a quantized model inference strategy for the fine-tuned model to improve practicability further. We conducted experiments on four types of MRC datasets. After extensive experiments, our results show that our model achieved a significant performance boost over baselines and outperformed other strong models for MRC.},
  archive      = {J_NCA},
  author       = {Feroze, Wasif and Cheng, Shaohuan and Jimale, Elias Lemuye and Jakhro, Abdul Naveed and Qu, Hong},
  doi          = {10.1007/s00521-025-10975-3},
  journal      = {Neural Computing and Applications},
  month        = {3},
  number       = {9},
  pages        = {6899-6913},
  shortjournal = {Neural Comput. Appl.},
  title        = {Enhancing text understanding of decoder-based model by leveraging parameter-efficient fine-tuning method},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). An enhanced neuro-adaptive PID sliding mode control for robot manipulators: Promoting sustainable automation. <em>NCA</em>, <em>37</em>(9), 6877-6898. (<a href='https://doi.org/10.1007/s00521-025-10980-6'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The control of robot manipulators presents significant challenges, primarily due to their complex, nonlinear dynamics. Another major difficulty arises from environmental and operational disturbances. Numerous control frameworks and approaches have been proposed in the literature to address these issues. However, many of these approaches are discontinuous or non-adaptive, which negatively affects their performance and makes them unsuitable for real-time applications. This paper proposes a novel integrated control scheme that combines adaptive control with an improved continuous second-order sliding mode control (CSOS), utilizing generalized artificial neural networks (GANN) for enhanced performance in robot manipulators. The proposed control method consists of two key components: An adaptive proportional-integral-derivative (PID) controller and an adaptive CSOS-based control module (CSOSSD-APID), designed to deliver superior transient and steady-state performance. In this approach, the adaptive CSOS benefits from GANN’s strong noise handling capabilities and its ability to estimate uncertainties effectively. This integration significantly enhances the robustness of robot manipulator control across various tracking tasks, using a single pre-trained GANN model with fine-tuned weights tailored to each task. Numerical simulations demonstrate the effectiveness and versatility of the proposed control scheme, particularly in managing highly time-varying trajectories while contributing to more sustainable and efficient automation practices.},
  archive      = {J_NCA},
  author       = {Elmogy, Ahmed and Alhemaly, Nagah and El-Ghaish, Hany and Elawady, Wael},
  doi          = {10.1007/s00521-025-10980-6},
  journal      = {Neural Computing and Applications},
  month        = {3},
  number       = {9},
  pages        = {6877-6898},
  shortjournal = {Neural Comput. Appl.},
  title        = {An enhanced neuro-adaptive PID sliding mode control for robot manipulators: Promoting sustainable automation},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). An active learning framework for adversarial training of deep neural networks. <em>NCA</em>, <em>37</em>(9), 6849-6876. (<a href='https://doi.org/10.1007/s00521-024-10851-6'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article introduces a novel approach to bolster the robustness of Deep Neural Network (DNN) models against adversarial attacks named “Targeted Adversarial Resilience Learning (TARL)”. The initial evaluation of a baseline DNN model reveals a significant accuracy decline when subjected to adversarial examples generated through techniques like FGSM, PGD, Carlini Wagner, and DeepFool attacks. To address this vulnerability, the article proposes an active learning framework, wherein the model iteratively identifies and learns from the most uncertain and misclassified instances. The key components of this approach include uncertainty estimation score in predicting the class of the input sample, selecting challenging samples based on this uncertainty score, labeling these challenging examples and augmenting them into the training set, and thereafter retraining the model with the expanded training set. The iterative active learning process, governed by parameters such as the number of iterations and batch size, demonstrates the potential to systematically enhance the resilience of DNN against adversarial threats. The proposed methodology has been investigated on several popular datasets such as the SARS-CoV-2 CT scan, MNIST, CIFAR-10, and Caltech-101, and demonstrated to be effective. Experiments illustrate that the learning framework improves the adversarial accuracies from 17.4% to 98.71% for the SARS-CoV-2 dataset, from 8.4% to 99.89% for the MNIST dataset, 1.6% to 78.84% for the CIFAR-10, and 12% to 92.92% for Caltech-101. Further, comparative analysis with several state-of-the-art methods suggests that the proposed framework offers superior defense against various attack methods and offers promising defensive mechanisms to deep neural networks.},
  archive      = {J_NCA},
  author       = {Ghosh, Susmita and Chatterjee, Abhiroop and Fiondella, Lance},
  doi          = {10.1007/s00521-024-10851-6},
  journal      = {Neural Computing and Applications},
  month        = {3},
  number       = {9},
  pages        = {6849-6876},
  shortjournal = {Neural Comput. Appl.},
  title        = {An active learning framework for adversarial training of deep neural networks},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Enhanced segmentation of optic disc and cup using attention-based U-net with dense dilated series convolutions. <em>NCA</em>, <em>37</em>(9), 6831-6847. (<a href='https://doi.org/10.1007/s00521-025-10989-x'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Delineating the boundaries of the optic disc and cup regions is a critical pre-requisite for glaucoma screening because it allows for precise measurement of key parameters, such as cup-to-disc ratio, which is a critical indicator of optic nerve head damage, a hallmark of glaucoma progression. Accurate segmentation enables early detection and monitoring of the disease, aiding in timely intervention to prevent vision loss. The main contribution of this research work is to develop an automated process to isolate and demarcate the optic disc and cup from retinal fundus images. To prevent the blood vessels from interfering with the segmentation process, a novel method is used for vessel mask generation and vessel inpainting. Most of the research works have used based encoder–decoder models like U-Net architecture or handcrafted feature extraction techniques such as hough transform, fuzzy clustering, etc. The proposed model has made significant modifications to the U-Net model. (1) Dual attention mechanism at every layer of decoder and (2) dense dilated series convolutions as skip connections to generate higher level feature map. The proposed model achieved benchmark accuracies - Dice score of 95.95% and IoU score of 92.22% for optic disc segmentation averaged over fivefold. For the task of outlining the optic cup region, it attained a Dice score of 88.7% and IoU of 79.72%.},
  archive      = {J_NCA},
  author       = {Kumar, G. Bharadwaja and Kumar, Soham},
  doi          = {10.1007/s00521-025-10989-x},
  journal      = {Neural Computing and Applications},
  month        = {3},
  number       = {9},
  pages        = {6831-6847},
  shortjournal = {Neural Comput. Appl.},
  title        = {Enhanced segmentation of optic disc and cup using attention-based U-net with dense dilated series convolutions},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Differential analysis of alternative splicing events in gene regions using residual neural networks. <em>NCA</em>, <em>37</em>(9), 6819-6829. (<a href='https://doi.org/10.1007/s00521-025-10992-2'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Several computational methods for the differential analysis of alternative splicing (AS) events among RNA-Seq samples typically rely on estimating isoform-level gene expression. However, these approaches are often error-prone due to the interplay of individual AS events, which results in different isoforms with locally similar sequences. Moreover, methods based on isoform-level quantification usually need annotated transcripts. In this work, we leverage the ability of deep learning networks to learn features from images and propose deepSpecas, a novel method for event-based AS differential analysis between two RNA-Seq samples. Our method does not rely on isoform abundance estimation, neither on a specific annotation. deepSpecas employs an image embedding scheme to represent the alignments of the two samples on the same region and utilizes a residual neural network to predict the AS events possibly expressed within that region. To our knowledge, deepSpecas is the first deep learning approach for performing an event-based AS analysis of RNA-Seq samples. To validate deepSpecas, we also address the lack of high quality AS benchmark datasets. For this purpose, we manually curated a set of regions exhibiting AS events. These regions were used for training our model and for assessing the predictions of our method. Our results highlight that deepSpecas achieves higher precision at the expense of a small reduction in sensitivity. The tool and the manually curated regions are available at https://github.com/sciccolella/deepSpecas .},
  archive      = {J_NCA},
  author       = {Ciccolella, Simone and Denti, Luca and Avila Cartes, Jorge and Della Vedova, Gianluca and Pirola, Yuri and Rizzi, Raffaella and Bonizzoni, Paola},
  doi          = {10.1007/s00521-025-10992-2},
  journal      = {Neural Computing and Applications},
  month        = {3},
  number       = {9},
  pages        = {6819-6829},
  shortjournal = {Neural Comput. Appl.},
  title        = {Differential analysis of alternative splicing events in gene regions using residual neural networks},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). CTMBIDS: Convolutional tsetlin machine-based intrusion detection system for DDoS attacks in an SDN environment. <em>NCA</em>, <em>37</em>(9), 6795-6818. (<a href='https://doi.org/10.1007/s00521-025-10976-2'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Software Defined Networks (SDN) face many security challenges today. A great deal of research has been done within the field of Intrusion Detection Systems (IDS) in these networks. Yet, numerous approaches still rely on deep learning algorithms, but these algorithms suffer from complexity in implementation, the need for high processing power, and high memory consumption. In addition to security issues, firstly, the number of datasets that are based on SDN protocols are very small. Secondly, the ones that are available encompass a variety of attacks in the network and do not focus on a single attack. For this reason, to introduce an SDN-based IDS with a focus on Distributed Denial of Service (DDoS) attacks, it is necessary to generate a DDoS-oriented dataset whose features can train a high-quality IDS. In this work, in order to address two important challenges in SDNs, in the first step, we generate three DDoS attack datasets based on three common and different network topologies. Then, in the second step, using the Convolutional Tsetlin Machine (CTM) algorithm, we introduce a lightweight IDS for DDoS attack dubbed "CTMBIDS," with which we implement an anomaly-based IDS. The lightweight nature of the CTMBIDS stems from its low memory consumption and also its interpretability compared to the existing complex deep learning models. The low usage of system resources for the CTMBIDS makes it an ideal choice for an optimal software that consumes the SDN controller’s least amount of memory. Also, in order to ascertain the quality of the generated datasets, we compare the empirical results of our work with the DDoS attacks of the KDDCup99 benchmark dataset as well. Since the main focus of this work is on a lightweight IDS, the results of this work show that the CTMBIDS performs much more efficiently than traditional and deep learning based machine learning algorithms. Furthermore, the results also show that in most datasets, the proposed method has relatively equal or better accuracy and also consumes much less memory than the existing methods.},
  archive      = {J_NCA},
  author       = {Jafari Gohari, Rasoul and Aliahmadipour, Laya and Kuchaki Rafsanjani, Marjan},
  doi          = {10.1007/s00521-025-10976-2},
  journal      = {Neural Computing and Applications},
  month        = {3},
  number       = {9},
  pages        = {6795-6818},
  shortjournal = {Neural Comput. Appl.},
  title        = {CTMBIDS: Convolutional tsetlin machine-based intrusion detection system for DDoS attacks in an SDN environment},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A self-supervised graph convolutional model for recommendation with exponential moving average. <em>NCA</em>, <em>37</em>(9), 6777-6793. (<a href='https://doi.org/10.1007/s00521-024-10933-5'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recommendation algorithms based on graph convolutional networks can integrate user and item node information along with their interaction topology, better capturing the intricate relationships between users and items, thereby enhancing the accuracy of recommender systems. However, existing methods often overlook the impact of noise in user behavior data on recommendation performance. Additionally, when there are too many convolutional layers in the graph, the node representations tend to smoothing, resulting in an inability to accurately distinguish user preferences. To address these issues, we propose a self-supervised graph convolutional model for recommendation with exponential moving average (SGCERec). Specifically, we first employ exponential moving average (EMA) techniques from the field of time-series analysis to denoise the raw user interaction data. Then, by applying layer filtering technique to update the propagation of information and the representation of nodes within the graph convolutional network, we effectively deepen the model hierarchy, enabling the model to gain a deeper understanding of the features and structures of the graph data, thereby improving the performance and effectiveness of the recommender systems. Finally, experimental results on three real datasets show that SGCERec outperforms state-of-the-art recommendation methods across various common evaluation metrics.},
  archive      = {J_NCA},
  author       = {Chen, Rui and Pang, Kangning and Wang, Zonglin and Liu, Qingfang and Tang, Cundong and Chang, Yanshuo and Huang, Min},
  doi          = {10.1007/s00521-024-10933-5},
  journal      = {Neural Computing and Applications},
  month        = {3},
  number       = {9},
  pages        = {6777-6793},
  shortjournal = {Neural Comput. Appl.},
  title        = {A self-supervised graph convolutional model for recommendation with exponential moving average},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Automatic channel selection using multi-objective prioritized jellyfish search (MPJS) algorithm for motor imagery classification using modified DB-EEGNET. <em>NCA</em>, <em>37</em>(9), 6749-6776. (<a href='https://doi.org/10.1007/s00521-025-10979-z'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A brain–computer interface (BCI) enables the device to communicate directly with the brain by decoding neural signals, particularly electroencephalograms (EEGs). EEG signals are used in a variety of applications, especially motor imagery detection, due to their noninvasive nature, real-time monitoring capabilities, and cost-effectiveness. Often, EEG data consists of multi-channel signals; the presence of multiple channels leads to computational complexity and the presence of redundant channel signals. To avoid these, the channel selection algorithm is currently being used, particularly optimization-based channel selection. However, the optimization-based channel selection method may have limitations, such as eliminating the most important channel due to poor initialization and failing to achieve optimal performance due to the lack of an efficient multi-objective fitness function. To address these limitations, we proposed a new channel selection mechanism called the multi-objective prioritized jellyfish search algorithm (MPJS), which has two significant improvements. First, domain-specific initialization is employed to select the most important channels at the initialization stage, which ensures that no important channels are omitted. Second, using a multi-objective fitness function instead of a single objective one to select the most relevant and informative channels ensures that the selected channels meet the number criteria and include candidates’ channels. Prior work primarily focused on two-class MI classification, with only a few studies examining four-class MI classification; however, these four-class classification methods fail to achieve optimal performance. To address these research gaps and achieve optimal performance in four-class MI detection, we proposed an improved double-branch EEGNET (DB-EEGNET). This proposed work performance was evaluated by using benchmark datasets, including BCI Competition IV-2008-2A, BCI Competition III-2008-A, and the High Gamma dataset (HGD). Our proposed MJPS channel selection and DB-EEGNET classification method outperformed the baseline algorithm on the BCI IV-IIA, IIIA, and HGD datasets, with an average accuracy of 83.9%, 84.46%, and 94.78%, respectively.},
  archive      = {J_NCA},
  author       = {Vadivelan, D. Senthil and Sethuramalingam, Prabhu},
  doi          = {10.1007/s00521-025-10979-z},
  journal      = {Neural Computing and Applications},
  month        = {3},
  number       = {9},
  pages        = {6749-6776},
  shortjournal = {Neural Comput. Appl.},
  title        = {Automatic channel selection using multi-objective prioritized jellyfish search (MPJS) algorithm for motor imagery classification using modified DB-EEGNET},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). AFuNet: An attention-based fusion network to classify texts in a resource-constrained language. <em>NCA</em>, <em>37</em>(9), 6725-6748. (<a href='https://doi.org/10.1007/s00521-024-10953-1'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the era of widespread Internet use and extensive social media interaction, the digital realm is accumulating vast amounts of unstructured text data. This unstructured data often contain undesirable information, necessitating time-consuming manual classification efforts. An intelligent text classification system capable of automatically categorizing digitized texts based on semantic meaning is crucial. However, this task is particularly challenging for low-resource languages like Bengali due to a shortage of annotated corpora, issues with out-of-vocabulary words, lack of domain-specific hyperparameter tuning, limited ability to extract generalized text features, and class imbalances within the corpus. AFuNet: an attention-based fusion network to classify texts in a resource-constrained language. AFuNet undergoes a comprehensive four-phase experimental process, including baseline model evaluation and hyperparameter tuning, late fusion and model selection, attention-based early fusion and model identification, and an ablation study with impact analysis. Fine-tuned based on five Bengali text classification corpora, AFuNet achieves impressive accuracies: 96.60 ± 0.2 (BTCC11), 85.37 ± 0.2 (OSBC), 97.35 ± 0.2 (BARD), 93.74 ± 0.2 (IndicNLP), and 96.51 ± 0.2 (ProthomAlo). In comparison with previous state-of-the-art models on these corpora, AFuNet demonstrates significant accuracy improvements ranging from 0.54% to 4.49%, showcasing its effectiveness in advancing text classification capabilities for the Bengali language.},
  archive      = {J_NCA},
  author       = {Hossain, Md. Rajib and Hoque, Mohammed Moshiul and Dewan, M. Ali Akber and Hoque, Enamul and Siddique, Nazmul},
  doi          = {10.1007/s00521-024-10953-1},
  journal      = {Neural Computing and Applications},
  month        = {3},
  number       = {9},
  pages        = {6725-6748},
  shortjournal = {Neural Comput. Appl.},
  title        = {AFuNet: An attention-based fusion network to classify texts in a resource-constrained language},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Metamorphic testing of deep neural network-based autonomous driving systems using behavioural domain adequacy. <em>NCA</em>, <em>37</em>(9), 6677-6724. (<a href='https://doi.org/10.1007/s00521-024-10794-y'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep neural networks (DNNs) are crucial in autonomous driving systems (ADSs) for tasks like steering control, but model inaccuracies, biased training data, and incorrect runtime parameters can compromise their reliability. Metamorphic testing (MT) enhances reliability by generating follow-up tests from mutated DNN source inputs, identifying inconsistencies as defects. Various MT techniques for ADSs include generative/transfer models, neuron-based coverage maximization, and adaptive test selection. Despite these efforts, significant challenges remain, including the ambiguity of neuron coverage’s correlation with misbehaviour detection, a lack of focus on DNN critical pathways, inadequate use of search-based methods, and the absence of an integrated method that effectively selects sources and generates follow-ups. This paper addresses such challenges by introducing DeepDomain, a grey-box multi-objective test generation approach for DNN models. It involves adaptively selecting diverse source inputs and generating domain-oriented follow-up tests. Such follow-ups explore critical pathways, extracted by neuron contribution, with broader coverage compared to their source tests (inter-behavioural domain) and attaining high neural boundary coverage of the misbehaviour regions detected in previous follow-ups (intra-behavioural domain). An empirical evaluation of the proposed approach on three DNN models used in the Udacity self-driving car challenge, and 18 different MRs demonstrates that relying on behavioural domain adequacy is a more reliable indicator than coverage criteria for effectively guiding the testing of DNNs. Additionally, DeepDomain significantly outperforms selected baselines in misbehaviour detection by up to 94 times, fault-revealing capability by up to 79%, output diversity by 71%, corner-case detection by up to 187 times, identification of robustness subdomains of MRs by up to 33 percentage points, and naturalness by two times. The results confirm that state-of-the-art coverage metrics are inadequate in misbehaviour-inducing test generation. Furthermore, black-box diversity-based test generation is less effective than the grey-box approach.},
  archive      = {J_NCA},
  author       = {Kalaee, Akram and Parsa, Saeed},
  doi          = {10.1007/s00521-024-10794-y},
  journal      = {Neural Computing and Applications},
  month        = {3},
  number       = {9},
  pages        = {6677-6724},
  shortjournal = {Neural Comput. Appl.},
  title        = {Metamorphic testing of deep neural network-based autonomous driving systems using behavioural domain adequacy},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A novel augmentation strategy for credit scoring modeling. <em>NCA</em>, <em>37</em>(9), 6663-6675. (<a href='https://doi.org/10.1007/s00521-024-10452-3'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In last years, social lending platforms have been increasingly used as virtual environments where borrowers can directly interact with lenders without any intermediary. As a result, a reliable credit scoring strategy, i.e., assessing whether a client is able to fully repay a loan, became of utmost importance to reduce the risk of not repaying the lenders. In this context, machine learning tools are being increasingly adopted to design automatic credit scoring systems but the data imbalance problem still penalizes their predictive performance, i.e., the greatest majority of clients can afford the repayment and learning to classify ”bad” borrowers depends on few instances where the loan was not paid back. In this paper, we target the data imbalance problem and propose a novel data augmentation strategy to improve the predictive performance of credit scoring models. The proposed methodology performs data augmentation by injecting synthetic instances in the dataset generated along the decision boundary of the decision model. We assessed the effectiveness of the proposed augmentation strategy on a million-scale dataset from Lending Club, the largest Social Lending platform, and found that it improves the performance of several classification models, also in comparison to other state-of-the-art approaches.},
  archive      = {J_NCA},
  author       = {La Gatta, Valerio and Postiglione, Marco and Sperlì, Giancarlo},
  doi          = {10.1007/s00521-024-10452-3},
  journal      = {Neural Computing and Applications},
  month        = {3},
  number       = {9},
  pages        = {6663-6675},
  shortjournal = {Neural Comput. Appl.},
  title        = {A novel augmentation strategy for credit scoring modeling},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). ViF-SD2E: A robust weakly-supervised framework for neural decoding. <em>NCA</em>, <em>37</em>(9), 6645-6661. (<a href='https://doi.org/10.1007/s00521-024-10958-w'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Neural decoding plays a vital role in the interaction between the brain and the outside world. Our task in this paper is to decode the movement track of a finger directly based on the neural data. Existing neural decoding solutions primarily perform some preprocessing operations on neural data before feeding them into existing models (such as LSTM) for decoding. However, these solutions either are prone to overfitting or cannot well exploit the spatial and temporal information. In our previous observations, there is a symmetrical phenomenon between the unsupervised decoded trajectory and the ground truth trajectory within the activity space. This precisely motivates us to propose (or derive) a robust weakly-supervised framework (or model structure), called ViF-SD2E, for neural decoding. In particular, it consists of a space-division (SD) module and an exploration–exploitation (2E) strategy, to effectively exploit both the spatial information of the outside world and the temporal information of neural activity, where the SD2E output is analogized with the weak 0/1 vision feedback (ViF) label for training. Extensive experiments demonstrate the effectiveness of our method, which can sometimes be comparable to supervised counterparts. Therefore, we redirect our attention to the information (hidden in data) ViF-SD2E conveys to us. In other words, we believe that the advantage of ViF-SD2E lies in the fact that its processing steps are objectively determined by the inherent attributes (i.e., symmetry) of the neural data, or rather, the model structure is fixed.},
  archive      = {J_NCA},
  author       = {Feng, Jingyi and Luo, Yong and Song, Shuang and Hu, Han},
  doi          = {10.1007/s00521-024-10958-w},
  journal      = {Neural Computing and Applications},
  month        = {3},
  number       = {9},
  pages        = {6645-6661},
  shortjournal = {Neural Comput. Appl.},
  title        = {ViF-SD2E: A robust weakly-supervised framework for neural decoding},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Fixed/preassigned-time synchronization of complex networks under aperiodically intermittent event-triggered control. <em>NCA</em>, <em>37</em>(9), 6633-6643. (<a href='https://doi.org/10.1007/s00521-024-10918-4'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper considers the fixed-/preassigned-time synchronization of complex networks via aperiodically intermittent event-triggered control. A novel intermittent control lemma is developed to ensure fixed-time synchronization with the help of an event-triggered method. Some criteria are proposed to achieve the synchronization goal for complex networks within a fixed/preassigned time via two types of aperiodically intermittent event-triggered controllers. A numerical example is given to illustrate the validity of the new theoretical results.},
  archive      = {J_NCA},
  author       = {Dong, Ziyu and Hu, Yuanfa and Liu, Xiaoyang and Cao, Jinde},
  doi          = {10.1007/s00521-024-10918-4},
  journal      = {Neural Computing and Applications},
  month        = {3},
  number       = {9},
  pages        = {6633-6643},
  shortjournal = {Neural Comput. Appl.},
  title        = {Fixed/preassigned-time synchronization of complex networks under aperiodically intermittent event-triggered control},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A robust entropy regularized K-means clustering algorithm for processing noise in datasets. <em>NCA</em>, <em>37</em>(9), 6617-6632. (<a href='https://doi.org/10.1007/s00521-024-10899-4'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {K-means is one of the clustering algorithms. Due to its simple implementation and powerful functionality, it is widely used in fields such as data mining, cluster analysis, data preprocessing, and unsupervised learning. However, the K-means algorithm suffers from the problem of being sensitive to outliers. If there are a certain number of outliers in a low-dimensional sample set, the resulting cluster centers will be greatly disturbed, affecting the clustering results. We can certainly detect outliers before clustering, but this phased approach has an impact on the accuracy of clustering results. To address this issue, we propose an improved robust Entropy Regularized K-Means clustering algorithm. Our method is based on the Entropy Regularized K-Means clustering algorithm and adds a weight value to the optimization function to ignore out-of-bounds data, and obtain a more accurate number of clusters in the dataset, thereby achieving synchronous clustering and detection. The advantages of this algorithm are strong anti-interference ability, the ability to ignore the influence of outliers on cluster centers, and synchronous clustering and detection. We tested our improved algorithm on artificial and real datasets, demonstrating that it can better determine cluster centers and find some outlier data.},
  archive      = {J_NCA},
  author       = {Jiang, Peilin and Cao, Junnan and Yu, Weizhong and Nie, Feiping},
  doi          = {10.1007/s00521-024-10899-4},
  journal      = {Neural Computing and Applications},
  month        = {3},
  number       = {9},
  pages        = {6617-6632},
  shortjournal = {Neural Comput. Appl.},
  title        = {A robust entropy regularized K-means clustering algorithm for processing noise in datasets},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Enhancing intra-aural disease classification with attention-based deep learning models. <em>NCA</em>, <em>37</em>(9), 6601-6616. (<a href='https://doi.org/10.1007/s00521-025-10990-4'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Ear diseases are defined as pathological conditions that indicate dysfunction or abnormal function of the ear organ, which is part of the auditory system of living organisms that regulates hearing and balance functions. These diseases usually manifest as conditions that affect the internal components of the ear structure and can manifest themselves with symptoms such as hearing loss, ear pain, balance problems, and fluid accumulation in the ear. The accuracy of the diagnosis depends on expert knowledge and subjective opinion. This method is prone to human error. This study presents a novel computer-aided diagnosis system for otoscope images of ear diseases, utilizing a vision transformer-based feature extractor combined with machine learning classifiers to provide accurate second opinions for ENT specialists. For this purpose, a new model based on state-of-the-art vision transformer feature extractor and machine learning models is proposed. In the experimental study, the dataset, comprising 880 eardrum images categorized into four classes (CSOM, earwax, myringosclerosis, and normal), was split into training (70%), validation (10%), and testing (20%) subsets. Each image was preprocessed to 420 × 380 pixels to fit the input dimensions of the models. The vision transformer architecture was utilized for feature extraction, followed by classification using various machine learning algorithms including kNN, SVM, and random forest. As a result, the model using vision transformer feature extractor and k-nearest neighbors (kNN) algorithm achieved 99.00% accuracy. In this study, a deep learning-based and computer-aided diagnosis system, in other words, a computational model, was developed instead of the current human error-prone disease diagnosis method used by ear nose throat (ENT) specialists. The main purpose of the deep learning-based decision support system is to support the diagnosis process where expert knowledge is difficult to access and to provide an alternative opinion to the expert diagnosis.},
  archive      = {J_NCA},
  author       = {Demircan, Furkancan and Ekinci, Murat and Cömert, Zafer},
  doi          = {10.1007/s00521-025-10990-4},
  journal      = {Neural Computing and Applications},
  month        = {3},
  number       = {9},
  pages        = {6601-6616},
  shortjournal = {Neural Comput. Appl.},
  title        = {Enhancing intra-aural disease classification with attention-based deep learning models},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). ViT-SENet-tom: Machine learning-based novel hybrid squeeze–excitation network and vision transformer framework for tomato fruits classification. <em>NCA</em>, <em>37</em>(9), 6583-6600. (<a href='https://doi.org/10.1007/s00521-025-10973-5'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Tomatoes are essential fruits in numerous nations for their vast demand. It is very important to maintain the freshness of tomatoes. One of the primary challenges in the recent culinary landscape is accurately identifying healthy tomatoes while effectively eliminating damaged or rejected ones. Existing approaches employ various strategies for categorizing tomato fruit, but they often suffer from inaccuracies, slow detection, and suboptimal performance. Thus, motivated by this gap, in this paper, we propose a novel machine learning (ML) framework, ViT-SENet-Tom, which is a hybrid vision transformer (ViT) model with squeeze and excitation (SENet) block network for fast, accurate, and efficient tomato fruit classification. The framework works on three tomato classes, respectively, the ripe, unripe, and reject. In developing the proposed model, we utilized advanced and newly designed layers and functions. This integration created a more complex and sophisticated neural network, significantly enhancing efficiency and contributing to the model’s novelty. Our chosen dataset was small initially, but we implemented augmentation techniques to increase its size. This approach made our system more reliable, efficient, and effective. The hybrid ViT-SENet framework employs encoders and self-attention networks with squeeze and excitation channel functions to allow precise, robust, fast, and efficient tomato classification. In simulation, the framework achieves a training accuracy of 99.87% and validation accuracy of 93.87%, indicating the precise classification of tomatoes. Besides, this work tests accuracy using fivefold cross-validation. The highest accuracy seen at fold-5 is 99.90%. These testing results demonstrate the efficacy of the proposed framework in real-deployment scenarios. The implementation has the potential to provide enhanced and more sustainable food security and safety in future.},
  archive      = {J_NCA},
  author       = {Swapno, S M Masfequier Rahman and Nobel, S. M. Nuruzzaman and Islam, Md Babul and Bhattacharya, Pronaya and Mattar, Ebrahim A.},
  doi          = {10.1007/s00521-025-10973-5},
  journal      = {Neural Computing and Applications},
  month        = {3},
  number       = {9},
  pages        = {6583-6600},
  shortjournal = {Neural Comput. Appl.},
  title        = {ViT-SENet-tom: Machine learning-based novel hybrid squeeze–excitation network and vision transformer framework for tomato fruits classification},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Structure-to-word dynamic interaction model for abstractive sentence summarization. <em>NCA</em>, <em>37</em>(9), 6567-6581. (<a href='https://doi.org/10.1007/s00521-024-10970-0'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstractive text summarization aims to capture important information from text and integrate contextual information to guide the summary generation. However, effective integration of important and relevant information remains a challenging problem. Existing graph-based methods only consider either word relations or structure information, but neglect the correlation between them. To simultaneously capture the word relations and structure information from sentences, we propose a novel Structure-to-Word dynamic interaction model for Abstractive Sentence Summarization (SWSum). Specifically, we first represent structure and word relation information of sentences by constructing semantic scenario graph and semantic word relation graph based on FrameNet. We subsequently stack multiple graph-based dynamic interaction layers that iteratively enhance their correlation to learn node representations. Finally, a graph fusion module is designed to obtain better overall graph representations, which provide an attention-based context vector for the decoder to generate summary. Experimental results demonstrate our model outperforms existing state-of-the-art methods on two popular benchmark datasets, i.e., Gigaword and DUC 2004.},
  archive      = {J_NCA},
  author       = {Guan, Yong and Guo, Shaoru and Li, Ru},
  doi          = {10.1007/s00521-024-10970-0},
  journal      = {Neural Computing and Applications},
  month        = {3},
  number       = {9},
  pages        = {6567-6581},
  shortjournal = {Neural Comput. Appl.},
  title        = {Structure-to-word dynamic interaction model for abstractive sentence summarization},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). R-WhONet: Recalibrated wheel odometry neural network for vehicular positioning using transfer learning. <em>NCA</em>, <em>37</em>(9), 6547-6565. (<a href='https://doi.org/10.1007/s00521-024-10046-z'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper proposes a transfer learning approach to recalibrate our previously developed Wheel Odometry Neural Network (WhONet) for vehicle positioning in environments where Global Navigation Satellite Systems (GNSS) are unavailable. The WhONet has been shown to possess the capability to learn the uncertainties in the wheel speed measurements needed for correction and accurate positioning of vehicles. These uncertainties may be manifested as tyre pressure changes from driving on muddy and uneven terrains or wheel slips. However, a common cause for concern for data-driven approaches, such as the WhONet model, is usually the inability to generalise the models to a new vehicle. In scenarios where machine learning models are trained in a specific domain but deployed in another domain, the model’s performance degrades. In real-life scenarios, several factors are influential to this degradation, from changes to the dynamics of the vehicle to new pattern distributions of the sensor’s noise, and bias will make the test sensor data vary from training data. Therefore, the challenge is to explore techniques that allow the trained machine learning models to spontaneously adjust to new vehicle domains. As such, we propose the Recalibrated-Wheel Odometry neural Network, based on transfer learning, that adapts the WhONet model from its source domain (a vehicle and environment on which the model is initially trained) to the target domain (a new vehicle on which the trained model is to be deployed). Through a performance evaluation on several GNSS outage scenarios—short-term complex driving scenarios such as on roundabouts, sharp cornering, hard-brake and wet roads (drifts), and on longer-term GNSS outage scenarios of 30s, 60s, 120s and 180s duration—we demonstrate that a model trained in the source domain does not generalise well to a new vehicle in the target domain. However, we show that our new proposed framework improves the generalisation of the WhONet model to new vehicles in the target domains by an average of 32% (i.e. 32% reduction in the vehicle position error estimation across the scenarios investigated).},
  archive      = {J_NCA},
  author       = {Onyekpe, Uche and Szkolnik, Alicja and Palade, Vasile and Kanarachos, Stratis and Fitzpatrick, Michael E.},
  doi          = {10.1007/s00521-024-10046-z},
  journal      = {Neural Computing and Applications},
  month        = {3},
  number       = {9},
  pages        = {6547-6565},
  shortjournal = {Neural Comput. Appl.},
  title        = {R-WhONet: Recalibrated wheel odometry neural network for vehicular positioning using transfer learning},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Enhancing financial time series forecasting through topological data analysis. <em>NCA</em>, <em>37</em>(9), 6527-6545. (<a href='https://doi.org/10.1007/s00521-024-10787-x'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Topological data analysis (TDA) is increasingly acknowledged within financial markets for its capacity to manage complexity and discern nuanced patterns and structures. It has been applied effectively to uncover intricate relationships and capture non-linear dependencies inherent in market data. This manuscript presents a groundbreaking study that delves into integrating features derived from TDA to improve the performance of forecasting models for univariate time series prediction. The research specifically examines whether incorporating features extracted from TDA-such as entropy, amplitude, and the number of points obtained from persistent diagrams can provide valuable supplementary information to the baseline forecasting model. Thus, the aim is to determine if these TDA-derived features can boost forecasting accuracy by offering additional insights that existing models might overlook. The N-BEATS model serves as the baseline forecasting model due to its robust generalization capabilities and flexibility in incorporating additional features into the model. The proposed methodology is compared against a univariate N-BEATS model without additional features and other strategies incorporating supplementary features such as temporal decomposition and time delay embeddings. The evaluation includes forecasting for six cryptocurrencies across four distinct time scenarios and four traditional financial instruments across two scenarios each, resulting in 32 datasets. The results obtained were promising, as the proposed method, $$\texttt {N-BEATS}_\mathrm {+TDA}$$ , achieved the best results in mean performance and mean ranking for the three metrics considered (MAPE, MAE, and RMSE). Significant differences were observed with the rest of the proposed methods using a significance level of $$\alpha = 0.10$$ , highlighting the effectiveness of integrating TDA features to enhance forecasting models.},
  archive      = {J_NCA},
  author       = {de Jesus, Luiz Carlos and Fernández-Navarro, Francisco and Carbonero-Ruz, Mariano},
  doi          = {10.1007/s00521-024-10787-x},
  journal      = {Neural Computing and Applications},
  month        = {3},
  number       = {9},
  pages        = {6527-6545},
  shortjournal = {Neural Comput. Appl.},
  title        = {Enhancing financial time series forecasting through topological data analysis},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Fairness-driven federated learning-based spam email detection using clustering techniques. <em>NCA</em>, <em>37</em>(9), 6515-6526. (<a href='https://doi.org/10.1007/s00521-024-10969-7'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the world of emails, spam messages present a significant challenge, leading to inconveniences and potential security risks. Addressing this issue, the task of spotting spam in emails is critical for ensuring secure and trustworthy communication. However, two prevalent approaches have their challenges. The centralized model, gathering data in one place, raises privacy issues. Conversely, the federated learning model, which focuses on privacy, can lead to a compromise in accuracy. This research paper presents a novel federated learning-based fair clustering technique for spam email detection. By addressing privacy concerns and aiming for accurate classification, the proposed approach Fair Clustering model combines the strengths of federated learning and data clustering. Through experimental evaluation, the Fair Clustering model is evaluated against both a centralized and federated learning model. Different metrics, such as accuracy, recall, precision, and F1-score, are used to evaluate and compare the performance of these models. The results demonstrate that the Fair Clustering model outperforms the federated learning model, showcasing the effectiveness of fair clustering in selecting representative clients and improving classification performance.},
  archive      = {J_NCA},
  author       = {Kaushal, Vishal and Sharma, Sangeeta},
  doi          = {10.1007/s00521-024-10969-7},
  journal      = {Neural Computing and Applications},
  month        = {3},
  number       = {9},
  pages        = {6515-6526},
  shortjournal = {Neural Comput. Appl.},
  title        = {Fairness-driven federated learning-based spam email detection using clustering techniques},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Stylometry-driven framework for urdu intrinsic plagiarism detection: A comprehensive analysis using machine learning, deep learning, and large language models. <em>NCA</em>, <em>37</em>(9), 6479-6513. (<a href='https://doi.org/10.1007/s00521-024-10966-w'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Detecting plagiarism in documents is a well-established task in natural language processing (NLP). Broadly, plagiarism detection is categorized into two types (1) intrinsic: to check the whole document or all the passages have been written by a single author; (2) extrinsic: where a suspicious document is compared with a given set of source documents to figure out sentences or phrases which appear in both documents. In the pursuit of advancing intrinsic plagiarism detection, this study addresses the critical challenge of intrinsic plagiarism detection in Urdu texts, a language with limited resources for comprehensive language models. Acknowledging the absence of sophisticated large language models (LLMs) tailored for Urdu language, this study explores the application of various machine learning, deep learning, and language models in a novel framework. A set of 43 stylometry features at six granularity levels was meticulously curated, capturing linguistic patterns indicative of plagiarism. The selected models include traditional machine learning approaches such as logistic regression, decision trees, SVM, KNN, Naive Bayes, gradient boosting and voting classifier, deep learning approaches: GRU, BiLSTM, CNN, LSTM, MLP, and large language models: BERT and GPT-2. This research systematically categorizes these features and evaluates their effectiveness, addressing the inherent challenges posed by the limited availability of Urdu-specific language models. Two distinct experiments were conducted to evaluate the impact of the proposed features on classification accuracy. In experiment one, the entire dataset was utilized for classification into intrinsic plagiarized and non-plagiarized documents. Experiment two categorized the dataset into three types based on topics: moral lessons, national celebrities, and national events. Both experiments are thoroughly evaluated through, a fivefold cross-validation analysis. The results show that the random forest classifier achieved an exceptional accuracy of 98.81% in experiment 1. On the other hand, in experiment 2, the extreme gradient boosting classifier attained an overall accuracy of 99.00% highlighting its superior capability in distinguishing nuanced stylistic features across different topics. Overall, machine learning models showcasing superior performance utilizing the proposed set of stylometry features over deep learning approaches and LLMs.},
  archive      = {J_NCA},
  author       = {Manzoor, Muhammad Faraz and Farooq, Muhammad Shoaib and Abid, Adnan},
  doi          = {10.1007/s00521-024-10966-w},
  journal      = {Neural Computing and Applications},
  month        = {3},
  number       = {9},
  pages        = {6479-6513},
  shortjournal = {Neural Comput. Appl.},
  title        = {Stylometry-driven framework for urdu intrinsic plagiarism detection: A comprehensive analysis using machine learning, deep learning, and large language models},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Constructing a smoothed leaky ReLU using a linear combination of the smoothed ReLU and identity function. <em>NCA</em>, <em>37</em>(9), 6465-6478. (<a href='https://doi.org/10.1007/s00521-024-10935-3'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Convolutional neural networks (CNNs) have made tremendous progress in solving many challenging problems. Good activation functions can improve the performance of CNNs. The existing activation functions exhibit inconsistent performance gains across different training settings, models, datasets and tasks. To solve this problem, we propose a general smoothed approximation for the maximum function $$\max (x_i, \alpha x_i)$$ using the linear combination of the smoothed rectified linear unit and the identity function. And we use exponential moving average to training the negative slope in this smoothed approximation. To validate the effectiveness of our approach, we also present a smoothed approximation case named leaky power function linear unit (LPFLU) to compare with the current state-of-the-art activation functions. Experimental results demonstrate that our LPFLU outperforms the existing state-of-the-art activation functions in improved robustness across different training settings, models, datasets and tasks.},
  archive      = {J_NCA},
  author       = {Zhu, Meng and Min, Weidong and Li, Jiahao and Liu, Mengxue and Deng, Ziyang and Zhang, Yao},
  doi          = {10.1007/s00521-024-10935-3},
  journal      = {Neural Computing and Applications},
  month        = {3},
  number       = {9},
  pages        = {6465-6478},
  shortjournal = {Neural Comput. Appl.},
  title        = {Constructing a smoothed leaky ReLU using a linear combination of the smoothed ReLU and identity function},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Edge AI-powered marine pollution classification with customized CNN model. <em>NCA</em>, <em>37</em>(9), 6449-6463. (<a href='https://doi.org/10.1007/s00521-024-10959-9'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The increasing production of disposable plastic products contributes greatly to marine pollution and its impact on the marine ecosystem and organisms consuming ocean-derived food. To address this issue, this paper proposes a new customized convolutional neural network (CNN) model for categorizing the level of marine pollution in underwater ocean regions using image classification. The customized CNN model is developed and compared with five preexisting models, including DenseNet121, Inception-ResNetV2, InceptionV3, VGG-19, and VGG-16. The results show that the customized model achieves an accuracy of 99.5% and performs optimally according to various performance metrics. The model is implemented on an edge AI device, such as Raspberry Pi, to bring it to practical use.},
  archive      = {J_NCA},
  author       = {Palanisamy, Sanjai and Bonny, Talal and Nasir, Nida and Al Shabi, Mohammad and Al Shammaa, Ahmed},
  doi          = {10.1007/s00521-024-10959-9},
  journal      = {Neural Computing and Applications},
  month        = {3},
  number       = {9},
  pages        = {6449-6463},
  shortjournal = {Neural Comput. Appl.},
  title        = {Edge AI-powered marine pollution classification with customized CNN model},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Personalized medical recommendation system with machine learning. <em>NCA</em>, <em>37</em>(9), 6431-6447. (<a href='https://doi.org/10.1007/s00521-024-10916-6'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper presents a machine learning-based healthcare recommendation system designed to provide personalized medical advice by accurately predicting diseases from patient symptoms. The system utilizes a comprehensive symptom–disease dataset, leveraging support vector classifier (SVC) and random forest (RF) models, achieving outstanding accuracies of 97.75% in disease prediction. These results surpass those of similar studies, such as one employing hybrid CNN and fuzzy logic techniques, which achieved 99% accuracy but relied on smaller datasets with limited diversity. The proposed system not only excels in diagnosis but also integrates tailored recommendations, including medication, dietary plans, and exercise regimens, to address the specific needs of patients. These personalized recommendations enhance practical utility, offering a patient-centered approach that promotes proactive health management. By focusing on diseases with high and moderate predictive performance, the system addresses both common and complex conditions effectively. The study demonstrates the transformative potential of machine learning in developing scalable and efficient healthcare systems, bridging the gap between accurate prediction and actionable treatment strategies. Future research will aim to incorporate larger and more diverse datasets, address underrepresented diseases, and refine feature engineering to enhance model generalizability and the system's overall effectiveness.},
  archive      = {J_NCA},
  author       = {Hassan, Basma M. and Elagamy, Shahd Mohamed},
  doi          = {10.1007/s00521-024-10916-6},
  journal      = {Neural Computing and Applications},
  month        = {3},
  number       = {9},
  pages        = {6431-6447},
  shortjournal = {Neural Comput. Appl.},
  title        = {Personalized medical recommendation system with machine learning},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Collision avoidance and routing based on location access (CARLA) of mobile robots. <em>NCA</em>, <em>37</em>(9), 6401-6430. (<a href='https://doi.org/10.1007/s00521-024-10914-8'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The paper introduces a new path-planning robotic system methodology called Collision Avoidance and Routing based on Location Access (CARLA) for use in critical environments such as hospitals and crises where quick action and saving human lives are vital. The main focus of our framework is on accuracy and fast responses, such as delivering tools or items in a specific area while avoiding collisions with other robots and obstacles. CARLA is designed to provide quick responses during emergencies, unlike most existing algorithms that are integrated into site control units or distributed among mobile robots on-site. By being loaded onto a remote server node rather than individual robots, CARLA helps to conserve the robots' capabilities, hardware resources, and power consumption. Additionally, our system utilizes cloud computing and Fog servers technology to improve data transmission times between the cloud and smart devices, especially for applications with strict timing requirements like emergency response. The Fog platform is also leveraged to enhance on-site access to real-time interaction and location-based services by bringing processing power closer to the robots from far-off Cloud servers. CARLA has various applications, such as in factories and warehouses, where mobile robots need to be selected and directed by a central control system remotely. The proposed framework consists of three main modules: Robot Knowledge Module, Robot Selection Module, and Route Reservation Module, which will all be discussed in detail in this paper. The results of simulations using this framework show that the robots have improved flexibility and efficiency in terms of computing paths and successfully fulfiling requests without colliding, compared to traditional methods used in similar scenarios.},
  archive      = {J_NCA},
  author       = {ElSayyad, Shimaa Ezzat and Saleh, Ahmed I. and Ali, Hesham A. and Saraya, M. S. and Rabie, Asmaa H. and Abdelsalam, Mohamed M.},
  doi          = {10.1007/s00521-024-10914-8},
  journal      = {Neural Computing and Applications},
  month        = {3},
  number       = {9},
  pages        = {6401-6430},
  shortjournal = {Neural Comput. Appl.},
  title        = {Collision avoidance and routing based on location access (CARLA) of mobile robots},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Exponential synchronization of bi-directional associative memory neural networks with delay on arbitrary time domains. <em>NCA</em>, <em>37</em>(8), 6383-6400. (<a href='https://doi.org/10.1007/s00521-024-10820-z'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We investigate the exponential synchronization of bi-directional associative memory neural networks with delays on a family of different time domains. By utilizing the theory of time scales, we provide stabilization results that are applicable to continuous-time, discrete-time, and general nonuniform hybrid time domains. Our approach employs a unified matrix-measure theory, a recent alternative to traditional Lyapunov functions, to establish exponential synchronization and design effective feedback laws. Notably, our methodology does not require symmetry or diagonality in the control gain matrix, distinguishing it from prior works. Furthermore, we explore various special cases of the considered systems and provide a detailed discussion highlighting the advantages of our findings over existing results. The effectiveness of our proposed criteria is demonstrated through small-scale and medium-scale simulated numerical examples across different time domains. Additionally, we apply our results to an example from the literature, showcasing the broad applicability and improved performance of our method in comparison to previous approaches.},
  archive      = {J_NCA},
  author       = {Kumar, Vipin and Heiland, Jan and Benner, Peter},
  doi          = {10.1007/s00521-024-10820-z},
  journal      = {Neural Computing and Applications},
  month        = {3},
  number       = {8},
  pages        = {6383-6400},
  shortjournal = {Neural Comput. Appl.},
  title        = {Exponential synchronization of bi-directional associative memory neural networks with delay on arbitrary time domains},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Composite neural learning-based adaptive actuator failure compensation control for full-state constrained autonomous surface vehicle. <em>NCA</em>, <em>37</em>(8), 6369-6381. (<a href='https://doi.org/10.1007/s00521-024-10651-y'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article studies composite neural learning-based adaptive failure compensation control issues for the autonomous surface vehicle with full-state constraints. Initially, the control strategy solve the problems of computational complexity and state constraints and eliminate the negative effect of filter error on tracking performance by integrating with the command-filtered backstepping technique and barrier Lyapunov functions. Then, a composite neural learning framework is established, where the effect caused by approximation error on tracking accuracy can be efficiently reduced by constructing the serial-parallel estimation model to obtain the estimations of the system states. Furthermore, an adaptive resilient trajectory tracking controller is designed, which can ensure that all the signals of the closed-loop system are semi-globally uniformly ultimately bounded satisfying the preset constraints even if the expected actuator faults occur suddenly. Finally, the feasibility and superiority of the designed control strategy are clarified by simulation results.},
  archive      = {J_NCA},
  author       = {Song, Shuai and Jiang, Yu and Song, Xiaona and Stojanovic, Vladimir},
  doi          = {10.1007/s00521-024-10651-y},
  journal      = {Neural Computing and Applications},
  month        = {3},
  number       = {8},
  pages        = {6369-6381},
  shortjournal = {Neural Comput. Appl.},
  title        = {Composite neural learning-based adaptive actuator failure compensation control for full-state constrained autonomous surface vehicle},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Correlation-based pruning algorithm with weight compensation for feedforward neural networks. <em>NCA</em>, <em>37</em>(8), 6351-6367. (<a href='https://doi.org/10.1007/s00521-024-10932-6'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Optimizing neural network architectures through effective pruning techniques has become essential to balancing model complexity and accuracy. This study introduces a novel correlation-based approach to systematically reduce network size by identifying and removing redundant neurons based on their activation correlations. By selectively pruning neurons while compensating for their contributions, the method maintains model fidelity across diverse datasets. Results demonstrate substantial architecture reductions with minimal performance impact: For the MNIST dataset, the number of neurons in hidden layers was reduced from 128-128 to 118-93, while maintaining a high accuracy of 97.59%. Comparative analysis indicates that this pruning approach achieves competitive or superior results compared to state-of-the-art methods while reducing computational complexity and memory requirements by up to 25%. The findings highlight the potential of correlation-driven pruning strategies to optimize neural networks, making them more efficient and adaptable to resource-constrained environments.},
  archive      = {J_NCA},
  author       = {Ebid, Shaimaa E. K. and El-Tantawy, Samah and Shawky, Doaa and Abdel-Malek, Hany L.},
  doi          = {10.1007/s00521-024-10932-6},
  journal      = {Neural Computing and Applications},
  month        = {3},
  number       = {8},
  pages        = {6351-6367},
  shortjournal = {Neural Comput. Appl.},
  title        = {Correlation-based pruning algorithm with weight compensation for feedforward neural networks},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Polyp image segmentation based on improved planet optimization algorithm using reptile search algorithm. <em>NCA</em>, <em>37</em>(8), 6327-6349. (<a href='https://doi.org/10.1007/s00521-024-10667-4'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To recognize the potential for colon polyps to develop into cancer over time, early diagnosis is crucial for preventative healthcare. Timely identification significantly improves the prognosis and treatment outcomes for colorectal cancer patients. Image segmentation is crucial in medical image analysis for accurate diagnosis and treatment planning. Therefore, in this study, we present an alternative multilevel thresholding polyp segmentation method (MPOA) to enhance the segmentation of polyp images. The proposed method is based on enhancing the planet optimization algorithm (POA) by integrating operators from the reptile search algorithm (RSA). The evaluation of the developed MPOA is tested with different polyp images and compared with other image segmentation approaches. The results highlight the superior capability of MPOA, as evidenced by various performance measures in effectively segmenting polyp images. Furthermore, metrics such as peak signal-to-noise ratio (PSNR), structural similarity index (SSIM), and fitness values demonstrate that MPOA outperforms the basic version of POA and other methods. The evaluation outcomes underscore the significant impact of RSA in enhancing the performance of POA for the segmentation of polyp images.},
  archive      = {J_NCA},
  author       = {Abd Elaziz, Mohamed and Al-qaness, Mohammed A. A. and Al-Betar, Mohammed Azmi and Ewees, Ahmed A.},
  doi          = {10.1007/s00521-024-10667-4},
  journal      = {Neural Computing and Applications},
  month        = {3},
  number       = {8},
  pages        = {6327-6349},
  shortjournal = {Neural Comput. Appl.},
  title        = {Polyp image segmentation based on improved planet optimization algorithm using reptile search algorithm},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). ResdenseNet: A lightweight dense ResNet enhanced with depthwise separable convolutions and its applications for early plant disease classification. <em>NCA</em>, <em>37</em>(8), 6305-6326. (<a href='https://doi.org/10.1007/s00521-024-10972-y'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, artificial intelligence has undergone robust development, leading to the emergence of numerous autonomous AI applications. However, a crucial challenge lies in optimizing computational efficiency and reducing training time while maintaining high accuracy with limited hardware resources. This paper introduces ResdenseNet, a model built upon the MobileNet, DenseNet, and ResNet architectures. ResdenseNet combines dense blocks and residual blocks from the DenseNet and ResNet architectures. In these dense blocks, the standard convolutional units are replaced by depthwise separable convolutional units, a significant part of the MobileNet architecture. The experimental outcomes are contrasted with established models and their iterations, including ResNet-50, ResNet-101, MobileNet-V1, MobileNet-V2, DenseNet-121, and DenseNet-169. The proposed model is tested on benchmark and proposed datasets, showcasing its efficiency in reducing computations and accelerating the training process. Emphasizing hyperparameter importance, ResdenseNet, optimized with a growth rate of 64, 6 layers, and ReLU activation, achieves an accuracy of (98.73%) and a F1-score of (98.20%) on the wheat and barley dataset. The results indicate that ResdenseNet significantly decreases the number of parameters to 0.72M and efficiently shortens training time to 5983.54 s. Particularly noteworthy is ResdenseNet’s superiority over other models in terms of having the fewest parameters, the shortest training time, and the highest accuracy, especially when dealing with wheat, barley, and maize datasets.},
  archive      = {J_NCA},
  author       = {Nagpal, Jyoti and Goel, Lavika},
  doi          = {10.1007/s00521-024-10972-y},
  journal      = {Neural Computing and Applications},
  month        = {3},
  number       = {8},
  pages        = {6305-6326},
  shortjournal = {Neural Comput. Appl.},
  title        = {ResdenseNet: A lightweight dense ResNet enhanced with depthwise separable convolutions and its applications for early plant disease classification},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A deep learning framework for automatic assessment of presence in virtual reality using multimodal behavioral cues. <em>NCA</em>, <em>37</em>(8), 6283-6303. (<a href='https://doi.org/10.1007/s00521-024-10943-3'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Effective development of virtual reality (VR) applications is heavily reliant on the evaluation of user experience (UX). However, traditional methods such as questionnaires have inherent limitations which hinder their ability to capture nuanced behavioral responses and impede the agility of VR content creation: They are time-consuming, burdensome for users, and require significant human effort for interpretation. This study introduces an automated framework aimed at addressing the limitations of questionnaire-based evaluations to assess UX in VR. Our primary focus is to validate the concept of this framework through assessment of the sense of presence (SOP), a crucial psychological perception with significant impact on VR UX. Our proposed framework utilizes a deep neural network (DNN) to analyze patterns in multimodal behavioral cues, including facial expressions, head movements, and hand movements, to predict scores from the Igroup Presence Questionnaire (IPQ). Additionally, we introduce two statistical profiles: the Visual Entropy Profile (VEP), which offers insights into visual complexity by depicting scene entropy, and the Experiential Presence Profile (EPP), which is designed to capture users’ historical SOP levels to enable personalized baseline and sensitivity estimation. The proposed framework achieves a significant correlation between actual and predicted IPQ scores, with a Spearman’s rank correlation coefficient of 0.7303, showcasing the potential of DNNs in analyzing complex behavioral signals and automating SOP assessment. This study represents a pioneering effort in leveraging DNNs for the automatic assessment of SOP and paves the way for future advances in automatically assessing VR UX and unlocking new opportunities in the field.},
  archive      = {J_NCA},
  author       = {Pannattee, Peerawat and Shimada, Shogo and Yem, Vibol and Nishiuchi, Nobuyuki},
  doi          = {10.1007/s00521-024-10943-3},
  journal      = {Neural Computing and Applications},
  month        = {3},
  number       = {8},
  pages        = {6283-6303},
  shortjournal = {Neural Comput. Appl.},
  title        = {A deep learning framework for automatic assessment of presence in virtual reality using multimodal behavioral cues},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Enhancing link prediction in graph data augmentation through graphon mixup. <em>NCA</em>, <em>37</em>(8), 6267-6282. (<a href='https://doi.org/10.1007/s00521-024-10923-7'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Link prediction in complex networks is a fundamental problem with applications in diverse domains, from social networks to biological systems. Traditional approaches often struggle to capture intricate relationships in graphs, leading to suboptimal predictions. To address this, we introduce a novel method called graphon mixup (GM), which leverages the power of graphons to enhance link prediction. The augmentation strategy involves generating a synthetic graph by combining the original graph with a graphon-based synthetic graph. This process, expressed as a weighted combination of adjacency matrices, strategically blends real and synthetic information, enriching the training dataset. GM formulates link prediction as a joint optimization problem, aligning the characteristics of the synthetic graph with the true underlying structure. The objective is to minimize cross-entropy loss between predicted and true edge probabilities. A detailed computational complexity analysis evaluates the time and space requirements, aiding in understanding the efficiency and scalability of GM across different datasets and network sizes. Empirical validation on benchmark datasets demonstrates GM’s effectiveness in consistently improving average precision across diverse network types. The proposed method enhances the generalization capabilities of link prediction models, providing a more robust framework capable of accurate predictions even in the presence of noise or unseen patterns.},
  archive      = {J_NCA},
  author       = {Sultana, Tangina and Hossain, Md. Delowar and Morshed, Md. Golam and Lee, Young-Koo},
  doi          = {10.1007/s00521-024-10923-7},
  journal      = {Neural Computing and Applications},
  month        = {3},
  number       = {8},
  pages        = {6267-6282},
  shortjournal = {Neural Comput. Appl.},
  title        = {Enhancing link prediction in graph data augmentation through graphon mixup},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Outlier-resistant state estimation for memristor-based BAM neural networks with probabilistic time-varying delays. <em>NCA</em>, <em>37</em>(8), 6251-6265. (<a href='https://doi.org/10.1007/s00521-024-10890-z'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This study considers the issue of nonfragile state estimation (SE) for memristor-based bidirectional associative memory (MBAM) neural networks with probabilistic time-varying delays. The primary objective is to develop an effective estimator for accurately assessing neuron states, which are crucial in various engineering applications. Furthermore, we consider a scenario in which the measurement outputs of the neural networks may be influenced by abnormal disturbances, which could have a negative impact on the performance of the estimator. In this case, a factitious saturation constraint is introduced to mitigate the adverse effects on the designed outlier-resistant estimator, thereby improving the reliability of the estimator. Through constructing sensible Lyapunov–Krasovskii functional (LKF), a delay-dependent criterion is derived to guarantee the exponential stability of the augmented system. Finally, the effectiveness of the desired estimation scheme is demonstrated via two simulation examples.},
  archive      = {J_NCA},
  author       = {Shao, Xiaoguang and Zhang, Jie and Lyu, Ming and Lu, Yanjuan},
  doi          = {10.1007/s00521-024-10890-z},
  journal      = {Neural Computing and Applications},
  month        = {3},
  number       = {8},
  pages        = {6251-6265},
  shortjournal = {Neural Comput. Appl.},
  title        = {Outlier-resistant state estimation for memristor-based BAM neural networks with probabilistic time-varying delays},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Explainable energy consumption and speed prediction in sustainable cities using deep learning. <em>NCA</em>, <em>37</em>(8), 6233-6249. (<a href='https://doi.org/10.1007/s00521-024-10850-7'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The idea of sustainable cities has drawn a lot of attention due to the quick expansion of metropolitan areas as well as the growing problems brought on by resource scarcity and climate change. Cities that prioritize sustainable practices are those that minimize their negative effects on the environment, maximize resource efficiency, and improve the standard of living for their citizens. Therefore, for sustainable cities, this paper uses the vehicle energy dataset (VED) to estimate travel times and calculate vehicle energy consumption. The dataset contains 12,609,170 road elevation tracks, 12,203,044 speed limit tracks, and 12,281,719 speed limit records with direction. An open-source routing engine called Valhalla is utilized to do a variety of tasks, including finding paths, matching maps, and creating maneuvers based on paths. The three primary stages of the suggested model are data pre-processing, feature extraction, and result interpretation. In the data pre-processing stage, null values are first eliminated and data normalization is implemented. Then, three techniques known as the gated recurrent unit (GRU), recurrent neural network (RNN), and long short-term memory (LSTM) are used to optimize the model. Finally, the results are interpreted through the use of SHAP (SHapley Additive explanations) in explainable artificial intelligence (XAI) techniques. The LSTM model yields the best prediction results, achieving 15.2662 RMSE, 11.7266 MAE, and 0.6696 R2 at 8 batch size, according to the evaluation results. Additional experiments are carried out in batch sizes of 8, 16, 32, and 64.The lowest metrics are produced by batch sizes of 64, while the best metrics are produced by batch sizes of 8.},
  archive      = {J_NCA},
  author       = {Abd El-Latif, Eman I. and El-dosuky, Mohamed},
  doi          = {10.1007/s00521-024-10850-7},
  journal      = {Neural Computing and Applications},
  month        = {3},
  number       = {8},
  pages        = {6233-6249},
  shortjournal = {Neural Comput. Appl.},
  title        = {Explainable energy consumption and speed prediction in sustainable cities using deep learning},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Distance-based mutual congestion feature selection with genetic algorithm for high-dimensional medical datasets. <em>NCA</em>, <em>37</em>(8), 6217-6232. (<a href='https://doi.org/10.1007/s00521-024-10837-4'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Feature selection poses a challenge in high-dimensional datasets, where the number of features exceeds the number of observations, as seen in microarray, gene expression, and medical datasets. There is not a universally optimal feature selection method applicable to any data distribution, and as a result, the literature consistently endeavors to address this issue. One recent approach in feature selection is termed frequency-based feature selection. However, existing methods in this domain tend to overlook feature values, focusing solely on the distribution in the response variable. In response,this paper introduces the Distance-based Mutual Congestion (DMC) as a filter method that considers both the feature values and the distribution of observations in the response variable. DMC sorts the features of datasets, and the top 5% are retained and clustered by KMeans to mitigate multicollinearity. This is achieved by randomly selecting one feature from each cluster. The selected features form the feature space, and the search space for the Genetic Algorithm with Adaptive Rates (GAwAR) will be approximated using this feature space. GAwAR approximates the combination of the top 10 features that maximizes prediction accuracy within a wrapper scheme. To prevent premature convergence, GAwAR adaptively updates the crossover and mutation rates. The hybrid DMC-GAwAR is applicable to binary classification datasets, and experimental results demonstrate its superiority over some recent works.},
  archive      = {J_NCA},
  author       = {Nematzadeh, Hossein and Mani, Joseph and Nematzadeh, Zahra and Akbari, Ebrahim and Mohamad, Radziah},
  doi          = {10.1007/s00521-024-10837-4},
  journal      = {Neural Computing and Applications},
  month        = {3},
  number       = {8},
  pages        = {6217-6232},
  shortjournal = {Neural Comput. Appl.},
  title        = {Distance-based mutual congestion feature selection with genetic algorithm for high-dimensional medical datasets},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Adaptive loss optimization for enhanced learning performance: Application to image-based rock classification. <em>NCA</em>, <em>37</em>(8), 6199-6215. (<a href='https://doi.org/10.1007/s00521-024-10965-x'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the geoscience domain, mainly within the oil and gas industry, getting the correct category of rock samples is crucial. Machine learning models deployed for rock classification often use the categorical cross-entropy loss function. This loss function may struggle when the rocks are either very similar or have too much intraclass variability. Furthermore, categorical cross-entropy loss may ignore subtle but significant differences between classes. This results in the ignoring of hard-to-classify samples and fails to prioritize learning from these challenging patterns. This can lead to models biased toward more common classes or mislabeling of underrepresented rock types, especially when dealing with noisy or inconsistent data. To bridge those gaps, we propose a new hybrid loss function. It combines the traditional categorical cross-entropy loss function with Online Hard Example Mining (OHEM), a method originally formulated for object detection tasks focusing on hard-to-classify samples. We designed this function to be adjustable, making it adaptable to various challenges inherent to the rock classification. We evaluated this technique on a diverse, highly heterogeneous, and challenging dataset provided by Shell Brazil. The available techniques were tested and encountered problems with challenging classes. However, our new technique improved classification accuracy for both challenging and easier samples. In addition to rock classification, this technique may serve as a blueprint for addressing complicated classification problems in other fields.},
  archive      = {J_NCA},
  author       = {Salavati, Soroor and Mendes Júnior, Pedro Ribeiro and Rocha, Anderson and Ferreira, Alexandre},
  doi          = {10.1007/s00521-024-10965-x},
  journal      = {Neural Computing and Applications},
  month        = {3},
  number       = {8},
  pages        = {6199-6215},
  shortjournal = {Neural Comput. Appl.},
  title        = {Adaptive loss optimization for enhanced learning performance: Application to image-based rock classification},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). FLAME: Fire detection in videos combining a deep neural network with a model-based motion analysis. <em>NCA</em>, <em>37</em>(8), 6181-6197. (<a href='https://doi.org/10.1007/s00521-024-10963-z'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Among the catastrophic natural events posing hazards to human lives and infrastructures, fire is the phenomenon causing more frequent damages. Thanks to the spread of smart cameras, video fire detection is gaining more attention as a solution to monitor wide outdoor areas where no specific sensors for smoke detection are available. However, state-of-the-art fire detectors assure a satisfactory Recall but exhibit a high false-positive rate that renders the application practically unusable. In this paper, we propose FLAME, an efficient and adaptive classification framework to address fire detection from videos. The framework integrates a state-of-the-art deep neural network for frame-wise object detection, in an automatic video analysis tool. The advantages of our approach are twofold. On the one side, we exploit advances in image detector technology to ensure a high Recall. On the other side, we design a model-based motion analysis that improves the system’s Precision by filtering out fire candidates occurring in the scene’s background or whose movements differ from those of the fire. The proposed technique, able to be executed in real-time on embedded systems, has proven to surpass the methods considered for comparison on a recent literature dataset representing several scenarios. The code and the dataset used for designing the system have been made publicly available by the authors at ( https://mivia.unisa.it/large-fire-dataset-with-negative-samples-lfdn/ ).},
  archive      = {J_NCA},
  author       = {Gragnaniello, Diego and Greco, Antonio and Sansone, Carlo and Vento, Bruno},
  doi          = {10.1007/s00521-024-10963-z},
  journal      = {Neural Computing and Applications},
  month        = {3},
  number       = {8},
  pages        = {6181-6197},
  shortjournal = {Neural Comput. Appl.},
  title        = {FLAME: Fire detection in videos combining a deep neural network with a model-based motion analysis},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Leveraging ChatGPT for enhanced stock selection and portfolio optimization. <em>NCA</em>, <em>37</em>(8), 6163-6179. (<a href='https://doi.org/10.1007/s00521-024-10928-2'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The public release of ChatGPT represents a significant milestone in generative AI technology, enabling the autonomous generation of content based on pre-training. This breakthrough presents new opportunities for advancements in the field of portfolio selection. This paper aims to introduce a comprehensive portfolio selection method by applying ChatGPT for stock selection and combining it with optimization algorithms to jointly optimize portfolio selection. Compared to randomly selected stocks, the portfolios optimized using ChatGPT-selected stocks and solved with the egret swarm optimization algorithm (ESOA) demonstrate higher diversification and lower volatility, leading to superior portfolio optimization results. Additionally, to validate ESOA’s superiority, its performance is compared against genetic algorithm (GA) and particle swarm optimization (PSO) on five metrics: risk, expected return, Sharpe ratio, objective value, and penalty term. Under equivalent experimental setting, ESOA exhibits a better ability to balance the relationship between risk and return.},
  archive      = {J_NCA},
  author       = {Huang, Zhendai and Liao, Bolin and Hua, Cheng and Cao, Xinwei and Li, Shuai},
  doi          = {10.1007/s00521-024-10928-2},
  journal      = {Neural Computing and Applications},
  month        = {3},
  number       = {8},
  pages        = {6163-6179},
  shortjournal = {Neural Comput. Appl.},
  title        = {Leveraging ChatGPT for enhanced stock selection and portfolio optimization},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Image quality assessment by enabling inter-patch message passing via graph convolutional networks. <em>NCA</em>, <em>37</em>(8), 6145-6161. (<a href='https://doi.org/10.1007/s00521-024-10893-w'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {There is a crucial problem posing great challenges to the image quality assessment (IQA), that is, how to accurately regress the visual quality score of an entire image from its patches. The vast majority of existing patch-based IQA methods treat each patch independently. In this paper, we innovatively enable inter-patch message passing (MP) for the proposed IQA via graph convolutional networks (IQG). The patches are embedded into the graph by treating the low-dimensional vector representation of each patch as a node and the inter-patch intrinsic correlation as an edge. Since the intrinsic correlation is not directly available, an adaptive edge generator is proposed to adaptively construct the directed weighted edges by separately obtaining the patch-connected mask and the edge weights. To mitigate the overfitting that may occur when adaptive MP is enabled, we attach an embedding approach that creates the undirected unweighted edge between any two patches to enable each node in the graph to connect to every other node, thus passing the information that otherwise would be neglected. Extensive experiments demonstrate the state-of-the-art performance of our proposed IQG in complete scenarios, including full-reference and no-reference IQA tasks on benchmark IQA databases.},
  archive      = {J_NCA},
  author       = {Liu, Yufan and Guo, Jiefeng},
  doi          = {10.1007/s00521-024-10893-w},
  journal      = {Neural Computing and Applications},
  month        = {3},
  number       = {8},
  pages        = {6145-6161},
  shortjournal = {Neural Comput. Appl.},
  title        = {Image quality assessment by enabling inter-patch message passing via graph convolutional networks},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Revolutionizing online shopping with FITMI: A realistic virtual try-on solution. <em>NCA</em>, <em>37</em>(8), 6125-6144. (<a href='https://doi.org/10.1007/s00521-024-10843-6'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In today’s digital age, consumers increasingly rely on online shopping for convenience and accessibility. However, a significant drawback of online shopping is the inability to physically try on clothing before purchasing. This limitation often leads to uncertainty regarding fit and style, resulting in customer post-purchase dissatisfaction and higher return rates. Research indicates that online items are three times more likely to be returned than in-store ones, especially during the pandemic. To address this challenge, we propose a virtual try-on method called FITMI, an enhanced Latent Diffusion Textual Inversion model for virtual try-on purposes. The proposed architecture aims to bridge the gap between traditional in-store try-ons and online shopping by offering users a realistic and interactive virtual try-on experience. Although virtual try-on solutions already exist, recent advancements in artificial intelligence have significantly enhanced their capabilities, enabling more sophisticated and realistic virtual try-on experiences than ever before. Building on these advancements, FITMI surpasses ordinary virtual try-ons relying on generative adversarial networks, often producing unrealistic outputs. Instead, FITMI utilizes latent diffusion models to generate high-quality images with detailed textures. As a web application, FITMI facilitates virtual try-ons by seamlessly integrating images of users with garments from catalogs, providing a true-to-life representation of how the items would look. This approach differentiates us from competitors. FITMI is validated using two widely recognized benchmarks: the Dress-Code and Viton-HD datasets. Additionally, FITMI acts as a trusted style advisor, enhancing the shopping experience by recommending complementary items to elevate the chosen garment and suggesting similar options based on user preferences.},
  archive      = {J_NCA},
  author       = {Samy, Tassneam M. and Asham, Beshoy I. and Slim, Salwa O. and Abohany, Amr A.},
  doi          = {10.1007/s00521-024-10843-6},
  journal      = {Neural Computing and Applications},
  month        = {3},
  number       = {8},
  pages        = {6125-6144},
  shortjournal = {Neural Comput. Appl.},
  title        = {Revolutionizing online shopping with FITMI: A realistic virtual try-on solution},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multi-orthogonal-oppositional enhanced african vultures optimization for combined heat and power economic dispatch under uncertainty. <em>NCA</em>, <em>37</em>(8), 6097-6123. (<a href='https://doi.org/10.1007/s00521-024-10715-z'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper attempts to conceptualize a potent methodology by combining the African vultures optimization algorithm (AVOA) with a multi-orthogonal-oppositional strategy (M2OS), named AVO-M2OS, to address the nonconvexity and multidimensional nature of the combined heat and power economic dispatch (CHPED) problem under both crisp and uncertainty aspects. The AVO-M2OS uses the M2OS to simultaneously explore the search region, improving solutions’ diversity as well as solution quality. Therefore, AVO-M2OS can perform deeper exploration and exploitation features and thus mitigate the trapping at local optima, especially when tackling the more complicated nature of the CHPED problem. A three-stage analysis is conducted to assess the effectiveness of the proposed AVO-M2OS algorithm. During the first stage, the algorithm’s performance is evaluated on benchmark problems such as CEC 2005 and CEC 2019, employing statistical verifications and convergence characteristics. In the second stage, the significance of the results is evaluated using the nonparametric Friedman test to demonstrate that the results did not occur by chance. The results indicate that the AVO-M2OS algorithm outperforms the best existing algorithm (AVOA) by an average rank of the Friedman test exceeding 26% for the CEC 2005 suite while outperforming the gray wolf optimization (GWO) by 60% for the CEC 2019 suite. Moreover, the AVO-M2OS demonstrates exceptional performance compared to existing state-of-the-art algorithms, surpassing the best algorithm available by an average rank of the Friedman test that exceeds 41%. Finally, the AVO-M2OS’s applicability is achieved by minimizing the operational costs by finding the optimal power and heat generation scheduling for the CHPED problem. The recorded results realize that the AVO-M2OS algorithm offers accurate performance compared to competing optimizers, where it saves the operational cost of the 48-unit system by 24% on the original AVO variant. Furthermore, the uncertainty aspect of CHPED, called UCHPED, is investigated using intuitionistic fuzzy numbers (IFN) to simulate the fluctuation based on customers’ varying energy demands across different time periods, posing a significant challenge for timely and equitable energy allocation. The optimization results show that the suggested AVO-M2OS algorithm provides more robust and reliable optimal solutions compared to other methods in most of the studied benchmark functions, including the CHPED problem, addressing both crisp and uncertain aspects. Therefore, it is a potential alternative for both real-world operations and modeling situations.},
  archive      = {J_NCA},
  author       = {Rizk-Allah, Rizk M. and Snášel, Václav and Hassanien, Aboul Ella},
  doi          = {10.1007/s00521-024-10715-z},
  journal      = {Neural Computing and Applications},
  month        = {3},
  number       = {8},
  pages        = {6097-6123},
  shortjournal = {Neural Comput. Appl.},
  title        = {Multi-orthogonal-oppositional enhanced african vultures optimization for combined heat and power economic dispatch under uncertainty},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). MIG-DARTS: Towards effective differentiable architecture search by gradually mitigating the initial-channel gap between search and evaluation. <em>NCA</em>, <em>37</em>(8), 6085-6096. (<a href='https://doi.org/10.1007/s00521-024-10681-6'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Neural architecture search (NAS) based on differentiable methods has made significant progress in both search cost (GPU-days and GPU memory consumption) and network performance. However, there still exists a large gap between the search and evaluation due to the unaffordable search cost, which will cause the searched architecture to be suboptimal in the evaluation. Based on the observation of the large initial-channel gap between search and evaluation, this paper is the first to propose to gradually mitigate the initial-channel gap as the search stage proceeds to elevate the performance of evaluation architecture; meanwhile, we remove poorly performing candidate operations after each search stage to keep an acceptable search cost. To further alleviate the excessive growth of search cost brought by the progressive increase of initial-channels, this paper proposes to separate the search space, by which an individual search space with reduced candidate operations is built for normal cell and reduction cell, respectively. Moreover, this paper proposes a stability-aware stopping strategy to alleviate the problem of invalid search to reduce the search cost in GPU-days. By conducting experiments on CIFAR10 and CIFAR100 datasets, the results show that the proposed method can achieve state-of-the-art performance with a small search cost.},
  archive      = {J_NCA},
  author       = {Hao, Debei and Pei, Songwei},
  doi          = {10.1007/s00521-024-10681-6},
  journal      = {Neural Computing and Applications},
  month        = {3},
  number       = {8},
  pages        = {6085-6096},
  shortjournal = {Neural Comput. Appl.},
  title        = {MIG-DARTS: Towards effective differentiable architecture search by gradually mitigating the initial-channel gap between search and evaluation},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Time-variant quadratic programming solving by using finitely-activated RNN models with exact settling time. <em>NCA</em>, <em>37</em>(8), 6067-6084. (<a href='https://doi.org/10.1007/s00521-024-10922-8'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recurrent neural networks (RNNs) are well established with comprehensive models capable of solving zero finding problems. Most of the conventional designs apply the infinite activation, but may not be practical for implementation. This paper presents model designs of finitely-activated zeroing neural networks (structure-like RNNs), possessing the finite-time convergence property as well, for solving time-variant convex quadratic programming. Two techniques for realizing finite activation are provided, based on which novel activation functions (AFs) are constructed, including the conic AFs and the finitely-valued power-rate AFs. Theoretical analyses of finite-time convergence are presented in detail and settling time is exactly established for each model. It is shown that finitely-valued AFs can approximate or even outperform the original power-rate AFs. The proposed neural network models are applied to solve an example of time-variant quadratic programming, and the repetitive motion planning of redundant robots with joint angle and joint velocity constraints, where the anti-disturbance capability of the finitely-activated integral neural network models has been examined and verified. The obtained numerical results demonstrate effectiveness of the computing schemes.},
  archive      = {J_NCA},
  author       = {Sun, Mingxuan and Zhang, Yu and Wang, Liming and Wu, Yuxin and Zhong, Guomin},
  doi          = {10.1007/s00521-024-10922-8},
  journal      = {Neural Computing and Applications},
  month        = {3},
  number       = {8},
  pages        = {6067-6084},
  shortjournal = {Neural Comput. Appl.},
  title        = {Time-variant quadratic programming solving by using finitely-activated RNN models with exact settling time},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). PartialST: Partial spatial–temporal learning for urban flow prediction. <em>NCA</em>, <em>37</em>(8), 6053-6066. (<a href='https://doi.org/10.1007/s00521-024-10888-7'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Accurate urban flow prediction plays a crucial role in transportation management, as it enables optimized resource allocation and improved traffic efficiency. Although current methods have made advances, they still encounter challenges such as high computational overhead and the risk of overfitting complex models. To tackle these issues, we introduce the partial channel connection to urban flow prediction, aiming to reduce complexity while keeping competitive performance. The partial channel connection selectively engages a specific subset of channels for operations in a single step, while preserving the identity mapping for the remaining channels. This approach ensures comprehensive training of all channels throughout the entirety of the training process and mitigates computational overheads at each step. In this paper, we apply the partial channel connection across various spatial and temporal encoders, undertaking a thorough investigation into their predictive accuracy. Based on these insights, we design a model named PartialST for urban flow prediction, which effectively captures the temporal and spatial correlations. We evaluate PartialST through comparative experiments against other state-of-the-art methods on two real-world datasets. The results demonstrate not only the superior performance of our model over other comparative models but also the effectiveness of the partial channel connection approach.},
  archive      = {J_NCA},
  author       = {Wang, Yong and Li, Xiaoyu and Zhang, Xinxin and Liu, Rui and Gong, Yongshun},
  doi          = {10.1007/s00521-024-10888-7},
  journal      = {Neural Computing and Applications},
  month        = {3},
  number       = {8},
  pages        = {6053-6066},
  shortjournal = {Neural Comput. Appl.},
  title        = {PartialST: Partial spatial–temporal learning for urban flow prediction},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). An integrated decision-making process for risk analysis of decentralized finance. <em>NCA</em>, <em>37</em>(8), 6021-6051. (<a href='https://doi.org/10.1007/s00521-024-10839-2'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Decentralized finance is upending the financial system through innovative, open, and interoperable financial solutions. Decentralized finance is a rapidly emerging field based on distributed ledger technology. Decentralized banking protocols are witnessing a perfect storm in terms of growth. However, because these financial innovations pose specific risks to consumers, creators, regulators, and other stakeholders, this emerging subject demands careful investigation. The current study tries to categorize and rate the risks connected with decentralized finance. The current study seeks to identify the multiple risks associated with decentralized finance through a thorough literature analysis. Data gathered from specialists in prior research were incorporated into the study used for empirical analysis. As MCDM techniques, IVFF-based DEMATEL, AHP, and TOPSIS are first used, and then the IVFF-ARAS method and sensitivity analysis are used for performance evaluation and verification. The findings of this study have several ramifications for legislators, businesspeople, technologists, and practitioners. These stakeholders can concentrate on these weaknesses in the future and provide longer-lasting solutions.},
  archive      = {J_NCA},
  author       = {Kirişci, Murat},
  doi          = {10.1007/s00521-024-10839-2},
  journal      = {Neural Computing and Applications},
  month        = {3},
  number       = {8},
  pages        = {6021-6051},
  shortjournal = {Neural Comput. Appl.},
  title        = {An integrated decision-making process for risk analysis of decentralized finance},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Machine learning-based gait cycle segmentation using instantaneous knee and hip-extension angles for biomechanical analysis. <em>NCA</em>, <em>37</em>(8), 6009-6019. (<a href='https://doi.org/10.1007/s00521-024-10720-2'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The biomechanical analysis of human movement, particularly gait, is crucial in fields such as clinical medicine, sports, and rehabilitation. While traditional motion capture (Mocap) systems are effective, they are often limited by their complexity, high cost, and the unnatural settings they require in terms of the gesture and motion environment. Emerging tools like inertial sensors and markerless video-based systems offer greater flexibility but encounter challenges in motion cycle segmentation, as they present kinematic data as time series, adding new difficulties to the analysis. This paper introduces a novel machine learning-based system for automatic gait cycle segmentation using features extracted from two easily measurable lower limb kinematic variables: hip and knee extension angles. The proposed method leverages instantaneous information from these angles for segmentation, ensuring versatility and independence from specific data collection methods. This allows for rapid segmentation and potential implementation on lower-performance processors. Experimental results demonstrate the high accuracy and efficiency of the proposed algorithm segmenting the gait cycle. The F1-score was 0.997. By using readily available hip and knee kinematic data and identifying crucial biomechanical relationships, our method offers a versatile and practical solution for motion analysis across various clinical and sports applications.},
  archive      = {J_NCA},
  author       = {Solórzano, Brayan David and Chavez, Susana and Giraldo, Luis Felipe and De la Portilla, Christian Cifuentes},
  doi          = {10.1007/s00521-024-10720-2},
  journal      = {Neural Computing and Applications},
  month        = {3},
  number       = {8},
  pages        = {6009-6019},
  shortjournal = {Neural Comput. Appl.},
  title        = {Machine learning-based gait cycle segmentation using instantaneous knee and hip-extension angles for biomechanical analysis},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Integrating contextual intelligence with mixture of experts for signature and anomaly-based intrusion detection in CPS security. <em>NCA</em>, <em>37</em>(8), 5991-6007. (<a href='https://doi.org/10.1007/s00521-024-10967-9'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The adoption of IoT and cyber-physical systems (CPSs) in smart homes and critical infrastructures has led to the possibility of physical damage from system compromises. Security failures in sectors like power, transport, and public safety can have more severe physical impacts than just information loss. Intrusion detection systems (IDSs) are crucial in a defense-in-depth approach. We propose a detection engine to prevent CPS from transitioning into unsafe states beyond critical limits, thresholds, and behavioral normalicies. A novel host-based IDS using a mixture-of-experts (MoE) model is introduced in the CPS security paradigm. For signature-based protection, we developed a context-aware CPS-SNORT ruleset for deep packet inspection (DPI) of Gcode instructions (NIST RS-274/ISO 6983-1:2009) used in numerical control of machines like CNCs and 3D printers. A new Gcode dataset was developed on a CPS test bed. In a supervised learning approach, we achieved over 99% accuracy with random tree for known attack detection. In a semi-supervised approach, logistic regression achieved 85% accuracy. For behavioral anomaly detection, LSTM achieved 99.9% accuracy, outperforming isolation forest and local outlier factor.},
  archive      = {J_NCA},
  author       = {Rahim, Kashif and Nasir, Zia Ul Islam and Ikram, Nassar and Qureshi, Hassaan Khaliq},
  doi          = {10.1007/s00521-024-10967-9},
  journal      = {Neural Computing and Applications},
  month        = {3},
  number       = {8},
  pages        = {5991-6007},
  shortjournal = {Neural Comput. Appl.},
  title        = {Integrating contextual intelligence with mixture of experts for signature and anomaly-based intrusion detection in CPS security},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Taking class imbalance into account in open set recognition evaluation. <em>NCA</em>, <em>37</em>(8), 5975-5989. (<a href='https://doi.org/10.1007/s00521-024-10960-2'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, deep neural networks have been employed increasingly often, which correlates with them receiving growing user trust. However, such systems cannot identify samples from unknown classes and often induce an incorrect decision with high confidence. This is aimed to be solved by open set recognition methods. The presented work looks at the evaluation protocols of existing approaches in the field. A particular focus is being placed on the impact of class imbalance, especially in the dichotomy between known and unknown samples, which is a rarely considered factor in the experimental environment. The work analyzes current evaluation strategies—regarding dataset construction and metric selection—noting that the class imbalance can significantly impact the obtained results. We analyze the effect of using the popular baseline metrics (accuracy, balanced accuracy, and F1-score) for method quality assessment and introduce a protocol extension to four recognition quality measures that can be built upon those baselines. The analysis of base measures revealed that the choice of baseline metric could significantly impact the computed criterion values when the class imbalance of the recognized problem appears. The proposed experimental environment was used in an example experiment on commonly used computer vision datasets. As an outcome of problem analysis, we present a set of guidelines for evaluating open set recognition methods.},
  archive      = {J_NCA},
  author       = {Komorniczak, Joanna and Ksieniewicz, Paweł},
  doi          = {10.1007/s00521-024-10960-2},
  journal      = {Neural Computing and Applications},
  month        = {3},
  number       = {8},
  pages        = {5975-5989},
  shortjournal = {Neural Comput. Appl.},
  title        = {Taking class imbalance into account in open set recognition evaluation},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A momentum accelerated stochastic method and its application on policy search problems. <em>NCA</em>, <em>37</em>(8), 5957-5973. (<a href='https://doi.org/10.1007/s00521-024-10883-y'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the dramatic increase in model complexity and problem scales in the machine learning area, researches on the first-order stochastic methods and its accelerated variants for non-convex problems have attracted wide research interest. However, most works on convergence analysis of accelerated methods focus on general convex or strongly convex objective functions. In this paper, we consider an accelerated scheme coming from dynamic systems and ordinary differential equations, which has a simpler and more direct form than the traditional scheme. We construct auxiliary sequences of iteration points as analysis tools, which can be interpreted as extension of Nesterov’s estimate sequence in non-convex case. We analyze the convergence property under different cases when momentum parameters are fixed or varying over iterations. For non-smooth and general convex objective functions, we give a relaxed step-size requirement to ensure convergence. For the non-convex policy search problem in classical reinforcement learning, we propose an accelerated stochastic policy gradient method with restart technique and construct numerical experiments to verify its effectiveness.},
  archive      = {J_NCA},
  author       = {Jiang, Boou and Yuan, Ya-xiang},
  doi          = {10.1007/s00521-024-10883-y},
  journal      = {Neural Computing and Applications},
  month        = {3},
  number       = {8},
  pages        = {5957-5973},
  shortjournal = {Neural Comput. Appl.},
  title        = {A momentum accelerated stochastic method and its application on policy search problems},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Do as you teach: A multi-teacher approach to self-play in deep reinforcement learning. <em>NCA</em>, <em>37</em>(8), 5945-5956. (<a href='https://doi.org/10.1007/s00521-024-10829-4'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A long-running challenge in the reinforcement learning (RL) community has been to train a goal-conditioned agent in sparse reward environment such that it also generalizes to unseen goals. We propose a novel goal-conditioned RL algorithm; Multi-Teacher Asymmetric Self-Play, which allows multiple agents (i.e., the teachers) to create a successful curriculum for another agent (i.e., the student) and empirically demonstrate its effectiveness on complex domains like FetchReach and a novel driving simulator designed for goal-conditioned RL. Our results show a 30-40% improvement over the baseline while also improving the learning speed of the student. We attribute this improvement in performance to the better exploration and coverage of the state space by multiple teacher agents. In addition, the results show that completely new students can learn offline from the goals generated by teachers trained with a previous student, reducing the computational cost by around 95%. This is crucial in the context of application domains where repeatedly training a teacher agent is expensive or even infeasible.},
  archive      = {J_NCA},
  author       = {Kharyal, Chaitanya and Gottipati, Sai Krishna and Sinha, Tanmay Kumar and Abdollahi, Fatemeh and Das, Srijita and Taylor, Matthew E.},
  doi          = {10.1007/s00521-024-10829-4},
  journal      = {Neural Computing and Applications},
  month        = {3},
  number       = {8},
  pages        = {5945-5956},
  shortjournal = {Neural Comput. Appl.},
  title        = {Do as you teach: A multi-teacher approach to self-play in deep reinforcement learning},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). TSUBF-net: Trans-spatial UNet-like network with bi-direction fusion for segmentation of adenoid hypertrophy in CT. <em>NCA</em>, <em>37</em>(8), 5927-5943. (<a href='https://doi.org/10.1007/s00521-024-10824-9'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Adenoid hypertrophy stands as a common cause of obstructive sleep apnea–hypopnea syndrome in children. It is characterized by snoring, nasal congestion, and growth disorders. Computed tomography (CT) emerges as a pivotal medical imaging modality, utilizing X-rays and advanced computational techniques to generate detailed cross-sectional images. Within the realm of pediatric airway assessments, CT imaging provides an insightful perspective on the shape and volume of enlarged adenoids. Despite the advances of deep learning methods for medical imaging analysis, there remains an emptiness in the segmentation of adenoid hypertrophy in CT scans. To address this research gap, we introduce TSUBF-Net (Trans-Spatial UNet-like Network based on Bi-direction Fusion), a 3D medical image segmentation framework. TSUBF-Net is engineered to effectively discern intricate 3D spatial interlayer features in CT scans and enhance the extraction of boundary-blurring features. Notably, we propose two innovative modules within the U-shaped network architecture: the Trans-Spatial Perception (TSP) module and the Bi-directional Sampling Collaborated Fusion (BSCF) module. These two modules are in charge of operating during the sampling process and strategically fusing down-sampled and up-sampled features, respectively. Furthermore, we introduce the Sobel loss term, which optimizes the smoothness of the segmentation results and enhances model accuracy. Extensive 3D segmentation experiments are conducted on several datasets. TSUBF-Net is superior to the state-of-the-art methods with the lowest HD95: 7.03, IoU: 85.63, and DSC: 92.26 on our own AHSD dataset. The results in the other two public datasets also demonstrate that our methods can robustly and effectively address the challenges of 3D segmentation in CT scans.},
  archive      = {J_NCA},
  author       = {Zhou, Rulin and Feng, Yingjie and Wang, Guankun and Zhong, Xiaopin and Wu, Zongze and Wu, Qiang and Zhang, Xi},
  doi          = {10.1007/s00521-024-10824-9},
  journal      = {Neural Computing and Applications},
  month        = {3},
  number       = {8},
  pages        = {5927-5943},
  shortjournal = {Neural Comput. Appl.},
  title        = {TSUBF-net: Trans-spatial UNet-like network with bi-direction fusion for segmentation of adenoid hypertrophy in CT},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). SE-GCL: An event-based simple and effective graph contrastive learning for text representation. <em>NCA</em>, <em>37</em>(8), 5913-5926. (<a href='https://doi.org/10.1007/s00521-024-10686-1'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Text representation learning is significant as the cornerstone of natural language processing. In recent years, graph contrastive learning (GCL) has been widely used in text representation learning due to its ability to represent and capture complex text information in a self-supervised setting. However, the current mainstream graph contrastive learning methods often require the incorporation of domain knowledge or cumbersome computations to guide the data augmentation process, which significantly limits the application efficiency and scope of GCL. Additionally, many methods learn text representations only by constructing word-document relationships, which overlooks the rich contextual semantic information in the text. To address these issues and exploit representative textual semantics, we present an event-based, simple, and effective graph contrastive learning (SE-GCL) for text representation. Precisely, we extract event blocks from text and construct internal relation graphs to represent inter-semantic interconnections, which can ensure that the most critical semantic information is preserved. Then, we devise a streamlined, unsupervised graph contrastive learning framework to leverage the complementary nature of the event semantic and structural information for intricate feature data capture. In particular, we introduce the concept of an event skeleton for core representation semantics and simplify the typically complex data augmentation techniques found in existing graph contrastive learning to boost algorithmic efficiency. We employ multiple loss functions to prompt diverse embeddings to converge or diverge within a confined distance in the vector space, ultimately achieving a harmonious equilibrium. We conducted experiments on the proposed SE-GCL on four standard data sets (AG News, 20NG, SougouNews, and THUCNews) to verify its effectiveness in text representation learning. The accuracy achieved on the respective datasets is 91.56, 86.76, 98.03, and 97.79%, demonstrating superior performance on most datasets compared to baseline methods.},
  archive      = {J_NCA},
  author       = {Meng, Tao and Ai, Wei and Li, Jianbin and Wang, Ze and Li, Keqin},
  doi          = {10.1007/s00521-024-10686-1},
  journal      = {Neural Computing and Applications},
  month        = {3},
  number       = {8},
  pages        = {5913-5926},
  shortjournal = {Neural Comput. Appl.},
  title        = {SE-GCL: An event-based simple and effective graph contrastive learning for text representation},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Masked hybrid attention with laplacian query fusion and tripartite sequence matching for medical image segmentation. <em>NCA</em>, <em>37</em>(8), 5891-5911. (<a href='https://doi.org/10.1007/s00521-024-10934-4'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Medical image segmentation is pivotal in computer-aided diagnosis systems, demanding high precision and contextual understanding. Vision Transformer-based approaches have gained much attention recently due to their excellent performance and ability to capture long-range dependencies in medical images. However, research shows they suffer from inadequate multi-scale feature integration, poor object localization, and inconsistent mask predictions, leading to sub-optimal segmentation performance. This paper addresses these challenges by redefining semantic medical image segmentation through learnable object queries within an enhanced transformer framework with a masked hybrid attention querying mechanism, optimizing multi-scale feature fusion, object localization, and instance-specific segmentation. First, this study presents a novel transformer-based masked hybrid attention mechanism using Laplacian query fusion on learnable query features and incorporating a novel tripartite sequence matching technique as part of the enhanced decoder block to improve the consistency of mask predictions and optimize decoder queries. The designed hybrid multi-head self- and cross-attention mechanisms aim to selectively integrate multi-scale features, ensuring optimal feature combinations for precise segmentation. Secondly, multiple class tokens are incorporated to improve object localization and capture class-specific characteristics within the transformer framework, leveraging the transformer decoders’ ability to learn distinct instance representations. Experimental results and extensive ablation studies demonstrate the effectiveness of the proposed approach on three publicly available datasets, obtaining better segmentation results compared to various state-of-the-art approaches using various evaluation metrics. Specifically, the proposed model achieves a Dice Score of 95.25%, 92.75%, and 85.25% on LUNA, ISIC, and DRIVE datasets, respectively.},
  archive      = {J_NCA},
  author       = {Ekong, Favour and Yu, Yongbin and Patamia, Rutherford Agbeshi and Sarpong, Kwabena and Ukwuoma, Chiagoziem C. and Wang, Xiangxiang and Ukot, Akpanika Robert and Cai, Jingye},
  doi          = {10.1007/s00521-024-10934-4},
  journal      = {Neural Computing and Applications},
  month        = {3},
  number       = {8},
  pages        = {5891-5911},
  shortjournal = {Neural Comput. Appl.},
  title        = {Masked hybrid attention with laplacian query fusion and tripartite sequence matching for medical image segmentation},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). DMVL4AVD: A deep multi-view learning model for automated vulnerability detection. <em>NCA</em>, <em>37</em>(8), 5873-5889. (<a href='https://doi.org/10.1007/s00521-024-10892-x'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Automated vulnerability detection is crucial to protect software systems. However, state-of-the-art approaches mainly focus on a single view of the source code, which often leads to incomplete code representation and low detection accuracy. To solve these problems, this paper proposes a novel automatic vulnerability detection model, DMVL4AVD, based on deep multi-view learning that represents source codes from three distinct views: code sequences, code property graphs, and code metrics. Different deep models are employed to extract features from each view. Firstly, the [CLS] vectors derived from encoder layers 1 to 12 of GraphCodeBERT are used as code sequence features which contain rich semantic information. Next, the gated graph neural network (GGNN) is exploited to learn the features of nodes in the code property graph, encompassing both syntactic and dependency information of the source code. During the extraction of graph features, node representation is augmented by incorporating the degree centrality of each node, along with its corresponding code and type attributes, resulting in a more comprehensive depiction of the graph's structure. Statistical metrics generated by the code analysis tool SourceMonitor are then processed through a 1-dimensional (1-D) CNN to produce metric features. Fused features from these three views are learned by a multilayer perceptron (MLP) to yield final classification results. Experimental results demonstrate the superiority of DMVL4AVD over existing approaches. The model performs significantly better than the studied baselines, achieving an average increase in accuracy of 6.79% and an average boost of 6.94% in precision compared to the approaches in the literature.},
  archive      = {J_NCA},
  author       = {Du, Xiaozhi and Zhou, Yanrong and Du, Hongyuan},
  doi          = {10.1007/s00521-024-10892-x},
  journal      = {Neural Computing and Applications},
  month        = {3},
  number       = {8},
  pages        = {5873-5889},
  shortjournal = {Neural Comput. Appl.},
  title        = {DMVL4AVD: A deep multi-view learning model for automated vulnerability detection},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Enhancing affordable EEG to act as a quantitative EEG for inattention treatment using MATLAB. <em>NCA</em>, <em>37</em>(8), 5849-5871. (<a href='https://doi.org/10.1007/s00521-024-10835-6'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Lack of attention is a chronic behavior in attention deficit hyperactivity disorder (ADHD), autism spectrum disorder (ASD), and other disorders that harm academic and social performance. ADHD is a disorder whose typical symptoms include inattention, hyperactivity, and impulsivity. They have a major impact on the affected person’s function and development. The electroencephalogram (EEG) device is a diagnostic tool, whereas the quantitative EEG (QEEG) device is a diagnostic and therapeutic tool for most mental disorders. QEEG applies the neurofeedback method in treatment. Neurofeedback is a technique for training brain functions and is an alternative to the traditional oral treatment of inattention disorders due to its numerous side effects. The proposed software can upgrade most EEG devices in hospitals and clinics into QEEGs capable of neurofeedback. The upgrading tools and stages are introduced in this study. The cost of upgrading an EEG device is 25 times less than the purchase price of a QEEG device. The EEG device (Open BCI) has been upgraded with MATLAB to function as a QEEG system, integrating a variety of feature extraction methods for inattention detection such as fractal dimension (FD), wavelet transform (WT), multi-resolution techniques (MR), and empirical mode decomposition (EMD) which signified a notable progress in the field. Furthermore, the implemented software is easily customizable to include any forthcoming superior techniques that may arise. Earlier research distinguished the differences between states of relaxation and concentration using a simple fixed threshold. In this paper, short training has been utilized to calculate adaptive thresholds to optimize individual effects. Different thresholding techniques were employed with the EMD_Dt technique to distinguish between focused and unfocused epochs. The adaptive threshold method results have been more accurate reaching the benchmark of 99.82%, as opposed to the fixed threshold method, which reaches an accuracy of 97.73%. The findings were assessed through a pilot study involving 3483 epochs collected across 24 sessions from male and female children aged between 5 and 16. The proposed QEEG software was evaluated to be Specific, Measurable, Achievable, Realistic, and Timed (SMART) with an effect size of 0.85528336, which is significant.},
  archive      = {J_NCA},
  author       = {Magdy Rady, Radwa and Elsalamawy, Doaa and Rizk, M. R. M. and Abdel Alim, Onsy and Diaa Moussa, Nancy},
  doi          = {10.1007/s00521-024-10835-6},
  journal      = {Neural Computing and Applications},
  month        = {3},
  number       = {8},
  pages        = {5849-5871},
  shortjournal = {Neural Comput. Appl.},
  title        = {Enhancing affordable EEG to act as a quantitative EEG for inattention treatment using MATLAB},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Historical states modeling for visual tracking. <em>NCA</em>, <em>37</em>(7), 5831-5848. (<a href='https://doi.org/10.1007/s00521-024-10921-9'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Extracting additional spatiotemporal information from video sequences is critical for accurately perceiving target appearance changes during visual tracking. However, most learning-based trackers utilize only a single search image and template from a video for training, resulting in a lack of temporal information and low data utilization. To address these issues, we present an innovative Trajectory Guided Tracking (TGTrack) framework, which leverages the historical states of the target to predict its current location. Specifically, we construct trajectory tokens derived from tracking results in historical frames, integrating the position and scale information of the target. We propose a trajectory prediction module to utilize these trajectory tokens to generate the potential scope of current target. Furthermore, to enhance the inference efficiency of the tracker, we eliminate manually customized heads and post-processing steps. Consequently, we achieve a good balance between inference speed and effectiveness. Extensive experimental results demonstrate that our TGTrack achieves leading performance across multiple benchmarks.},
  archive      = {J_NCA},
  author       = {Shi, Junze and Yu, Yang and Hui, Bin and Shi, Jian and Luo, Haibo},
  doi          = {10.1007/s00521-024-10921-9},
  journal      = {Neural Computing and Applications},
  month        = {3},
  number       = {7},
  pages        = {5831-5848},
  shortjournal = {Neural Comput. Appl.},
  title        = {Historical states modeling for visual tracking},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A surface defect detection instrument for large aperture spherical optical elements. <em>NCA</em>, <em>37</em>(7), 5815-5829. (<a href='https://doi.org/10.1007/s00521-024-10889-6'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Spherical optical elements are an important classification of modern optical system components. Surface defects on optical elements can cause serious problems to the optical system. Fast and accurate defect detection for large aperture (200 mm) spherical optical elements is a challenge for industrial applications due to the tiny scale of the defects and the non-flat and super-smooth surface. A surface defect detection instrument for large aperture spherical optical elements is established in this paper, while the main contributions are: (1) A 5-axis motion system with microscopic imaging and multi-angle lighting system is designed for defect detection. (2) To image the spherical element surface clearly and completely, an adaptive path planning method that suits the 5-axis motion system is raised. (3) Aiming at tiny and weak defects of precise optical elements, an effective defect detection algorithm based on reverse attention is proposed for the instrument, which outperforms the baseline methods. Experiments display the instrument’s superior capability to existing instruments that, for a spherical optical element of 200 mm aperture and 200 mm radius of the sphere surface, the instrument can achieve surface defect detection with the precision of 2 μm in less than 10 min.},
  archive      = {J_NCA},
  author       = {Li, Mingwei and Shi, Yali and Zhang, Zhengtao and Tao, Xian and Shang, Xiuqin},
  doi          = {10.1007/s00521-024-10889-6},
  journal      = {Neural Computing and Applications},
  month        = {3},
  number       = {7},
  pages        = {5815-5829},
  shortjournal = {Neural Comput. Appl.},
  title        = {A surface defect detection instrument for large aperture spherical optical elements},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Intelligent recognition system of in-service tire damage driven by strong combination augmentation and contrast fusion. <em>NCA</em>, <em>37</em>(7), 5795-5813. (<a href='https://doi.org/10.1007/s00521-024-10898-5'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the advancement of computer technology, dynamic detection of damage to tire in-service has become feasible. However, current methods often struggle with accuracy limitations when confronted with specific working conditions and external factors. To address this challenge, we propose an intelligent recognition system for in-service tire damage driven by Strong Combination Augmentation and Contrast Fusion. The system uses a key feature learning enhancement method to address the problem. It uses the Hough transform and the Perceptual Hash algorithm to perform secondary feature comparison, enabling tire region detection even in low-resolution and high-interference scenarios. To effectively eliminate interference caused by wear, stains, and similar factors, we also introduce an efficient damage detection network called CA-EffNet. This network employs a strategic approach that combines various augmentation techniques and parameters with contrast fusion within a supervised learning framework. By integrating these elements, CA-EffNet expands the feature exploration space and effectively captures key damage features. The results show that the system efficiently achieves real-time detection within just 0.7 s at speeds of up to 15 km/h, meeting strict detection requirements. These results highlight the potential of the system to significantly advance the field of tire damage detection and ultimately contribute to safer roads.},
  archive      = {J_NCA},
  author       = {Shen, Dagang and Cao, Jinfeng and Liu, Peng and Guo, Jihong},
  doi          = {10.1007/s00521-024-10898-5},
  journal      = {Neural Computing and Applications},
  month        = {3},
  number       = {7},
  pages        = {5795-5813},
  shortjournal = {Neural Comput. Appl.},
  title        = {Intelligent recognition system of in-service tire damage driven by strong combination augmentation and contrast fusion},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Fake news detection based on multi-modal domain adaptation. <em>NCA</em>, <em>37</em>(7), 5781-5793. (<a href='https://doi.org/10.1007/s00521-024-10896-7'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The rapid development of social media has led to the unguaranteed authenticity of news, and the role of fake news detection in cybersecurity governance has become increasingly prominent. In this paper, we mine information from multiple modalities, such as the text and images of news, and propose a multi-modal fake news detection model based on the multi-stage domain adaptation for the differences existing between source and task domains and between different modalities. The multi-modal feature extraction network of BERT combined with EfficientNet is used to deeply analyze the features of social media data, and the multi-modal domain adaptation network is used to reduce the domain shift of different domains and different modalities of news data and to capture the correlation between events by adversarial ideas. Experimental results on public datasets of Weibo and Twitter show that the model significantly improves the effectiveness of the fake news detection task.},
  archive      = {J_NCA},
  author       = {Wang, Xiaopei and Meng, Jiana and Zhao, Di and Meng, Xuan and Sun, Hewen},
  doi          = {10.1007/s00521-024-10896-7},
  journal      = {Neural Computing and Applications},
  month        = {3},
  number       = {7},
  pages        = {5781-5793},
  shortjournal = {Neural Comput. Appl.},
  title        = {Fake news detection based on multi-modal domain adaptation},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A comprehensive study of fisheye image compression and perception for autonomous driving. <em>NCA</em>, <em>37</em>(7), 5765-5780. (<a href='https://doi.org/10.1007/s00521-024-10831-w'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Fisheye cameras are widely used in various fields, including automotive contexts for $$360^{\circ }$$ near-field vision around vehicles, as well as in photography, robotics, underwater imaging, and virtual reality. However, conventional image compression techniques do not take into account the specific characteristics of fisheye images, such as radial distortion and wide-angle field of view, especially when operating at low bit rates. This can lead to degradation of image quality and distortion of geometric features that are essential for computer vision (CV) applications such as object detection, semantic segmentation, and motion estimation. Recent studies have highlighted the impact of various noise factors on automotive camera sensors, the challenges of correcting radial lens distortion, and the effects of image compression artifacts on fisheye camera visual perception tasks. In this work, a comprehensive study of fisheye image compression and perception using deep learning-based techniques is conducted. It is demonstrated that deep learning-based techniques achieve better compression performance and perceptual quality than conventional techniques, particularly at low bitrates crucial for automotive applications.},
  archive      = {J_NCA},
  author       = {Barakat, Basem and Sobh, Ibrahim and Wong, Chup-Chung and Islam, Muahmmad},
  doi          = {10.1007/s00521-024-10831-w},
  journal      = {Neural Computing and Applications},
  month        = {3},
  number       = {7},
  pages        = {5765-5780},
  shortjournal = {Neural Comput. Appl.},
  title        = {A comprehensive study of fisheye image compression and perception for autonomous driving},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Detecting and assessing weak adhesion in structural single lap joints using a machine learning pipeline with lamb waves data. <em>NCA</em>, <em>37</em>(7), 5751-5764. (<a href='https://doi.org/10.1007/s00521-024-10819-6'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Adhesive joints are widely used in industries such as aerospace and automotive due to their lightweight and high mechanical performance. However, weak adhesion remains a significant issue affecting the structural integrity of these joints. Current detection methods of weak adhesion rely on destructive testing, which limits the widespread use of adhesive primary structures. This study proposes a novel nondestructive testing (NDT) technique to detect, evaluate the intensity, and localize weak adhesion in single lap joints (SLJs) using lamb waves (LWs) and machine learning (ML). The aim is to develop a ML-based pipeline capable of identifying weak adhesion with high accuracy and sensitivity, based on data from simulated and experimental SLJ samples. The proposed technique integrates LW data with convolutional neural networks (CNNs) in a ML pipeline for weak adhesion detection in SLJs. The use of a large simulated dataset combined with transfer learning allows for effective adaptation to experimental conditions, improving both the detection and localization of damage. This approach offers a significant advancement over traditional destructive testing techniques. The pipeline begins with the generation of simulated LW time-series data for SLJs with varying adhesion levels, damage locations, and sizes. After preprocessing, the data are input into a CNN, which is initially trained on synthetic data. Transfer learning is employed to fine-tune the model using a small experimental dataset. The final trained model is then applied to detect weak adhesion, estimate its intensity, and localize the damage. The proposed pipeline demonstrated high performance in both simulated and experimental datasets: regarding detection, the algorithm achieved over 95.3% accuracy in identifying damage from simulated data and near 100% detection of damaged cases in experimental data; for intensity estimation, the algorithm showed an average loss of approximately 45 MPa for weak adhesion intensity in experimental validation, with an average error of about 140 MPa and a best-case error of just near 3.6 MPa; in terms of localization, the average localization error was approximately 8 mm in the synthetic validation dataset; with respect to flexibility, the methodology is adaptable to different damage characteristics, such as existence, intensity, and localization, without requiring substantial modifications. Summing up, this study presents a novel NDT approach using ML and LW data that significantly improves the detection, evaluation, and localization of weak adhesion in adhesive joints. Its high accuracy and adaptability have the potential to enhance structural health monitoring, ensuring the safety and durability of bonded structures in critical industries.},
  archive      = {J_NCA},
  author       = {Ramalho, Gabriel M. F. and Lopes, António M. and da Silva, Lucas F. M.},
  doi          = {10.1007/s00521-024-10819-6},
  journal      = {Neural Computing and Applications},
  month        = {3},
  number       = {7},
  pages        = {5751-5764},
  shortjournal = {Neural Comput. Appl.},
  title        = {Detecting and assessing weak adhesion in structural single lap joints using a machine learning pipeline with lamb waves data},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A hybrid meta-heuristic algorithm for optimization of capuchin search algorithm for high-dimensional biological data classification. <em>NCA</em>, <em>37</em>(7), 5719-5750. (<a href='https://doi.org/10.1007/s00521-024-10815-w'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Feature selection (FS) is a preprocessing technique that diminishes redundant and non-informative features to enhance data classification methods. This technique has gained global significance with the expansion of real-world data, particularly high-dimensional biological datasets. This study introduces a distinctive wrapper-based FS model, built upon the capuchin search algorithm (CapSA). CapSA is a recent swarm intelligence algorithm inspired by the foraging behaviors of Capuchin monkeys. Despite the strengths of the standard CapSA, it has notable limitations that this paper aims to address through purposeful modifications to the algorithm. These modifications include incorporating genetic algorithm operators (crossover and mutation), a dynamic mechanism for determining the number of leaders, and incorporating adaptive inertia weight. The proposed enhanced variant, named CapSA-CM, aims to achieve a more effective balance between the exploration and exploitation phases of the algorithm. The proposed methods are assessed using high-dimensional, low-sample biological datasets. The CapSA-CM approach is validated by comparing its efficacy with basic and hybrid meta-heuristic algorithms. Statistical analysis demonstrates the superiority of the CapSA-CM in terms of feature reduction, accuracy rates, and fitness values compared to the original CapSA and other comparable algorithms.},
  archive      = {J_NCA},
  author       = {Jaber, Iyad and Hassouneh, Yousef and Khemaja, Maha},
  doi          = {10.1007/s00521-024-10815-w},
  journal      = {Neural Computing and Applications},
  month        = {3},
  number       = {7},
  pages        = {5719-5750},
  shortjournal = {Neural Comput. Appl.},
  title        = {A hybrid meta-heuristic algorithm for optimization of capuchin search algorithm for high-dimensional biological data classification},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). IMU-trans: Imputing missing motion capture data with unsupervised transformers. <em>NCA</em>, <em>37</em>(7), 5699-5717. (<a href='https://doi.org/10.1007/s00521-024-10946-0'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Motion capture (mocap) systems are extensively utilized in healthcare for monitoring rehabilitation programs, facilitating clinical gait assessments for early Alzheimer’s diagnosis, managing walking disorders, and developing exoskeleton suits. However, like many other healthcare technologies, mocap systems have some flaws, like missing markers and occlusions. Given mocap data’s sequential and temporal nature, understanding marker relationships and capturing global dependencies are crucial for effective human motion recovery applications. To address these challenges, we proposed an unsupervised transformers framework for human motion recovery, called IMU-Trans. We evaluated our framework’s generalizability across two clinical datasets and tested its robustness by adjusting the missing marker rates, comparing its performance against low-dimensional Kalman filtering, long short-term memory (LSTM), and gated recurrent unit (GRU) models. Our experimental results demonstrated that IMU-Trans outperforms state-of-the-art models by training in an unsupervised manner. The closest competitor, GRU, demonstrated an RMSE of 1.35 ± 0.82, 2.36 ± 1.26, 3.43 ± 1.73, and 4.39 ± 2.18 cm for 20%, 30%, 40%, and 50% missing rates, respectively. IMU-Trans outperformed GRU with an RMSE of 1.26 ± 0.60, 2.06 ± 0.88, 2.68 ± 1.04, and 3.05 ± 1.22 for the same rates. Notably, our framework performs well even with higher missing data rates, creating opportunities for advancements in data analytics and indicating a promising future for motion capture in healthcare.},
  archive      = {J_NCA},
  author       = {Avdan, Goksu and Onal, Sinan and Lu, Chao},
  doi          = {10.1007/s00521-024-10946-0},
  journal      = {Neural Computing and Applications},
  month        = {3},
  number       = {7},
  pages        = {5699-5717},
  shortjournal = {Neural Comput. Appl.},
  title        = {IMU-trans: Imputing missing motion capture data with unsupervised transformers},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Navigation variable-based multi-objective particle swarm optimization for UAV path planning with kinematic constraints. <em>NCA</em>, <em>37</em>(7), 5683-5697. (<a href='https://doi.org/10.1007/s00521-024-10945-1'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Path planning is essential for unmanned aerial vehicles (UAVs) as it determines the path that the UAV needs to follow to complete a task. This work addresses this problem by introducing a new algorithm called navigation variable-based multi-objective particle swarm optimization (NMOPSO). It first models path planning as an optimization problem via the definition of a set of objective functions that include optimality and safety requirements for UAV operation. The NMOPSO is then used to minimize those functions through Pareto optimal solutions. The algorithm features a new path representation based on navigation variables to include kinematic constraints and exploit the maneuverable characteristics of the UAV. It also includes an adaptive mutation mechanism to enhance the diversity of the swarm for better solutions. Comparisons with various algorithms have been carried out to benchmark the proposed approach. The results indicate that the NMOPSO performs better than not only other particle swarm optimization variants but also other state-of-the-art multi-objective and meta-heuristic optimization algorithms. Experiments have also been conducted with real UAVs to confirm the validity of the approach for practical flights. The source code of the algorithm is available at https://github.com/ngandng/NMOPSO .},
  archive      = {J_NCA},
  author       = {Duong, Thi Thuy Ngan and Bui, Duy-Nam and Phung, Manh Duong},
  doi          = {10.1007/s00521-024-10945-1},
  journal      = {Neural Computing and Applications},
  month        = {3},
  number       = {7},
  pages        = {5683-5697},
  shortjournal = {Neural Comput. Appl.},
  title        = {Navigation variable-based multi-objective particle swarm optimization for UAV path planning with kinematic constraints},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Velocity control of a stephenson III six-bar linkage-based gait rehabilitation robot using deep reinforcement learning. <em>NCA</em>, <em>37</em>(7), 5671-5682. (<a href='https://doi.org/10.1007/s00521-024-10944-2'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Lower limb rehabilitation robots can help to improve the locomotor capabilities of patients experiencing gait impairments and help medical workers by reducing strain on them. However, since commercially available exoskeletons are expensive and there is a lack of number of physiotherapists many patients are still not able to get proper rehabilitation training. The closed-loop linkage mechanisms have recently drawn much attention in the realization of gait rehabilitation robots. Such mechanisms are affordable and capable of providing suitable trajectories for gait training therapy. In this work, we have proposed a fully operational one degree-of-freedom mechanism which can generate complex naturalistic lower limb trajectories. Although in theory, it is assumed that the constant speed applied at the input crank is sufficient to control the system, in reality, the external forces exerted by human legs and the inertia of the links can greatly alter the rotational velocity at the crank, which may negatively affect the training process. Therefore, we have explored the performance of a deep reinforcement learning-based control algorithm designed to regulate the speed of the input crank to reach satisfactory performance needed for gait rehabilitation training. Experimental evaluations with healthy human subjects were conducted to demonstrate that the mechanism is capable of directing lower limbs on naturalistic gait trajectories with a required walking speed.},
  archive      = {J_NCA},
  author       = {Kapsalyamov, Akim and Brown, Nicholas A. T. and Goecke, Roland and Jamwal, Prashant K. and Hussain, Shahid},
  doi          = {10.1007/s00521-024-10944-2},
  journal      = {Neural Computing and Applications},
  month        = {3},
  number       = {7},
  pages        = {5671-5682},
  shortjournal = {Neural Comput. Appl.},
  title        = {Velocity control of a stephenson III six-bar linkage-based gait rehabilitation robot using deep reinforcement learning},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Intrusion detection system model: A white-box decision tree with feature selection optimization. <em>NCA</em>, <em>37</em>(7), 5655-5670. (<a href='https://doi.org/10.1007/s00521-024-10942-4'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Intrusion detection has been an active development area due to its importance in highly digitally connected ecosystems. Most of the existing developments have focused on the use of complex machine learning models that are black-box in nature. There is an urgent need to investigate a more transparent model approach for determining the features associated with intrusion detection. In this paper, a feature selection is proposed for a decision tree (DT)-based classifier. In particular, a stochastic optimization technique based on differential evolution (DE) is used to create the DT for optimizing feature selection. The contribution of this paper is twofold. First, a white-box machine learning model using DT is implemented. Second, an optimal feature reduction approach is embedded in the process of building the DT. The results demonstrate an improvement over the non-feature selection approach and the black-box neural network and are comparable to other state-of-the-art models. This shows that it is possible to achieve high performance despite using a minimal transparent model by eliminating non-contributing features. This is the essence of Occam’s razor principle, which states that a more condensed model contributes to better generalization. There is an evident improvement in the generalization of the DT model after optimization of features. Despite often being associated with a weaker machine learning model, the results show comparative results on independent datasets, indicating the suitability for such a task. It is worth mentioning that the final model only utilizes a fraction of the full feature set. Although the generalization performance only improved less than 1% in comparison with the non-feature selection counterpart, the proposed approach suggests that a condensed model yielding a similar performing model should be considered.},
  archive      = {J_NCA},
  author       = {Wong, W. K. and Juwono, Filbert H. and Eswaran, Sivaraman and Motelebi, Foad},
  doi          = {10.1007/s00521-024-10942-4},
  journal      = {Neural Computing and Applications},
  month        = {3},
  number       = {7},
  pages        = {5655-5670},
  shortjournal = {Neural Comput. Appl.},
  title        = {Intrusion detection system model: A white-box decision tree with feature selection optimization},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Course time scheduling problem for distance education considering server load balancing: A case of an engineering faculty. <em>NCA</em>, <em>37</em>(7), 5635-5653. (<a href='https://doi.org/10.1007/s00521-024-10941-5'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {During the Covid-19 pandemic and mass disaster, universities have had to continue their courses with distance education. Internet servers in educational institutions slowed down for some periods, and courses could not be processed. The interruptions are due to the intensity of users on the servers and the internet congestion in the country during some time periods. For these reasons, the course time scheduling problem for distance education is discussed in this study. This study aims to balance the number of users in the system to prevent internet congestion. To solve the problem, two different mathematical models are developed. The first model obtains the balanced course scheduling for any time period. The second model is provided by the course assignment, considering internet congestion that occurred during any period. The proposed models were tested in a real case study dealing with the charting of courses offered in all departments of the engineering faculty of a university in Turkey. A problem-specific heuristic model is developed to solve the problem since a solution could not be obtained from the mathematical models in polynomial time due to the large size of the actual data set. The comparative results obtained from mathematical models and problem-specific heuristics are reported, and their performance is discussed. According to the comparative results, problem-specific heuristic outperforms mathematical models in obtaining a balanced schedule and solution time.},
  archive      = {J_NCA},
  author       = {Alakaş, Hacı Mehmet and Pınarbaşı, Mehmet and Sarımehmet, Bedirhan and Eren, Tamer},
  doi          = {10.1007/s00521-024-10941-5},
  journal      = {Neural Computing and Applications},
  month        = {3},
  number       = {7},
  pages        = {5635-5653},
  shortjournal = {Neural Comput. Appl.},
  title        = {Course time scheduling problem for distance education considering server load balancing: A case of an engineering faculty},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Radial basis function network-based optimization of the hard self-propelled rotary turning titanium. <em>NCA</em>, <em>37</em>(7), 5607-5634. (<a href='https://doi.org/10.1007/s00521-024-10940-6'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Machining of difficult-to-cut materials such as high-temperature metals is challenging due to their low machinability resulting in reduced productivity and high manufacturing cost. This investigation develops and optimizes the hard self-propelled rotary turning (HSPRT) operation, in which an efficient self-propelled rotary tool is proposed and fabricated. The HSPRT responses (total carbon emission—TC, machined roughness—MR, and noise emission—EN) are minimized using optimal process variables (inclination angle—A, turning depth—D, turning speed—V, and rake angle—R). The TC, MR, and EN models are developed in terms of the HSPRT inputs using the radial basis function network and response surface method, while the weights were computed using the removal effects of criteria, EQUAL, and rank order centroid methods. The improved quantum-behaved particle swarm optimization algorithm and method based on the multi-attributive border approximation area comparison were applied to produce feasible solutions and select the best optimal point. The optimization findings of the V, D, f, and R were 32 deg., 0.2 mm, 137 m/min, and 20 deg., while the TC, MR, and EN were saved by 42.8%, 24.1%, and 20.0%, respectively. The HSPRT performances were primarily affected by the turning depth and speed, respectively. The valuable outcomes could be applied to the practice to boost HSPRT performances, while the developed HSPRT operation could be utilized for machining alloys and hardened steels. The technique could be applied to treat optimization problems for other rotary turning processes.},
  archive      = {J_NCA},
  author       = {Nguyen, Trung-Thanh and Dang, Xuan-Ba},
  doi          = {10.1007/s00521-024-10940-6},
  journal      = {Neural Computing and Applications},
  month        = {3},
  number       = {7},
  pages        = {5607-5634},
  shortjournal = {Neural Comput. Appl.},
  title        = {Radial basis function network-based optimization of the hard self-propelled rotary turning titanium},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Finite-time fault-tolerant control of attitude control system of quadrotor UAV based on neural network disturbance observer. <em>NCA</em>, <em>37</em>(7), 5597-5606. (<a href='https://doi.org/10.1007/s00521-024-10927-3'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper investigates a new finite-time fault-tolerant control scheme for a quadrotor unmanned aerial vehicle (UAV). Firstly, a novel neural network disturbance observer with an auxiliary system is designed to compensate for actuator faults and external disturbances. In addition, in order to ensure the system’s rapidity, the finite-time theory is incorporated into the observer design and the controller design. The hyperbolic tangent function is also introduced to process the input signals, which ensures that the UAV receives relatively the smooth input signals. The design scheme takes into account the rapidity and robustness of the system, which makes the performance of the UAV better. Finally, the advantages of the proposed scheme are demonstrated through comparative experiments and the feasibility of the scheme is further verified through actual physical experiments.},
  archive      = {J_NCA},
  author       = {Li, Boning and Chen, Ming and Qi, Shuchang and Peng, Kaixiang},
  doi          = {10.1007/s00521-024-10927-3},
  journal      = {Neural Computing and Applications},
  month        = {3},
  number       = {7},
  pages        = {5597-5606},
  shortjournal = {Neural Comput. Appl.},
  title        = {Finite-time fault-tolerant control of attitude control system of quadrotor UAV based on neural network disturbance observer},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). DeepCancer: Deep learning for brain tumor detection-based application system. <em>NCA</em>, <em>37</em>(7), 5577-5596. (<a href='https://doi.org/10.1007/s00521-024-10926-4'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A brain tumor is the abnormal cells that growth in the brain, and it is considered as one of the most dangerous diseases that lead to the cause of death. Diagnosis at early is important for increasing the survival rate from the brain tumors. Specialists can identify the tumors manually, but it is very time and effort consuming, and are subject to human error, especially when dealing with large amounts of images. The automatic identification algorithms-based applications can facilitate the process. This study aimed to investigate the possibility of detecting brain cancer based on images using Deep Learning (DL) techniques and statistical operations. The features were extracted using two models of Convolutional Neural Network (CNN), (VGG-19 and AlexNet), then they were used to generate new datasets for statistical operations. CNN is used to extract features with distinct details from brain MRI images. The data were trained in three different training–testing data splitting ratios. Then, the features were classified based on the KNN, RF, and SVM to find the best accuracy of brain MRI image. At the end, the obtained classification accuracy was in favor of statistical operations especially for Large-Value, and Merge between features using KNN (99.1) and SVM (99.1). The features that extracted used in this study can provide high influence on the classification accuracy. The results across all three training–testing data splitting ratios were almost similar, and this approves that the brain cancer can be identified with high accuracy even if the training dataset sizes were minimal.},
  archive      = {J_NCA},
  author       = {AlShowarah, Suleyman A.},
  doi          = {10.1007/s00521-024-10926-4},
  journal      = {Neural Computing and Applications},
  month        = {3},
  number       = {7},
  pages        = {5577-5596},
  shortjournal = {Neural Comput. Appl.},
  title        = {DeepCancer: Deep learning for brain tumor detection-based application system},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Hmltnet: Multi-modal fake news detection via hierarchical multi-grained features fused with global latent topic. <em>NCA</em>, <em>37</em>(7), 5559-5575. (<a href='https://doi.org/10.1007/s00521-024-10924-6'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Since the adverse impact of fake news, especially multi-modal fake news, on public decision-making and social governance, multi-modal fake news detection has lately attracted increasing attention. However, many existing methods ultimately exploit multi-modal features without detail fusion to complete detection and insufficiently consider the intrinsic features in news content, resulting in poor performance. To tackle these issues, we propose a network for multi-modal fake news detection that uses hierarchical multi-grained features fused with global latent topic (HMLTNet). Specifically, we first construct Hierarchical Multi-grained Encoding Module to capture convolutional and hierarchical textual features. Then, Cross-modal Shared Attention Module completes detail compensation in the multi-modal features by fusing textual and visual features and jointly modeling inter- and intra-modality correlations. Finally, the global latent topic features are excavated and stocked from multi-modal features by utilizing Latent Topic Memory Module. Furthermore, we design an Enhanced Similarity Module and introduce a dense-like strategy together to alleviate the adverse effects of cross-modal semantic gap. Extensive experiments on three public datasets indicate that the presented network reaches the best accuracy compared to state-of-the-art methods.},
  archive      = {J_NCA},
  author       = {Cui, Shaoguo and Gong, Linfeng and Li, Tiansong},
  doi          = {10.1007/s00521-024-10924-6},
  journal      = {Neural Computing and Applications},
  month        = {3},
  number       = {7},
  pages        = {5559-5575},
  shortjournal = {Neural Comput. Appl.},
  title        = {Hmltnet: Multi-modal fake news detection via hierarchical multi-grained features fused with global latent topic},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Modified U-net with attention gate for enhanced automated brain tumor segmentation. <em>NCA</em>, <em>37</em>(7), 5521-5558. (<a href='https://doi.org/10.1007/s00521-024-10919-3'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This study addresses the formidable challenges encountered in automated brain tumor segmentation, including the complexities of irregular shapes, ambiguous boundaries, and intensity variations across MRI modalities. Manual segmentation, plagued by subjectivity and time constraints, further exacerbates the problem. To address these issues, we propose a modified U-Net architecture with an integrated attention gate. The proposed model demonstrates high performance, with notable Dice Similarity Coefficient (DSC) and Jaccard Index (JI) values across various tumor classes, consistently exceeding 0.93 and 0.87, respectively. Incorporating Contrast-Limited Adaptive Histogram Equalization and Histogram Equalization improves segmentation accuracy, particularly in cases of Meningioma. Comparative analyses against established models reveal a DSC of 0.9521 and a JI of 0.9093, underscoring the superiority of our method. Validation in the BraTS 2021 dataset underscores the robustness of the method, achieving high DSC and JI scores in four MRI modalities, with the T2 modality demonstrating the highest performance (DSC: 0.9216, JI: 0.8556). While acknowledging these achievements, we recognize challenges related to dataset specificity and computational intensity associated with the attention gate. Future research efforts should address these issues to improve the generalizability and applicability of the method in real-world scenarios. In addition to presenting a novel automated brain tumor segmentation method, this study contributes comprehensive result values and comparative analyses with previous research, providing valuable insights into the evolving landscape of medical image analysis.},
  archive      = {J_NCA},
  author       = {Saifullah, Shoffan and Dreżewski, Rafał and Yudhana, Anton and Wielgosz, Maciej and Caesarendra, Wahyu},
  doi          = {10.1007/s00521-024-10919-3},
  journal      = {Neural Computing and Applications},
  month        = {3},
  number       = {7},
  pages        = {5521-5558},
  shortjournal = {Neural Comput. Appl.},
  title        = {Modified U-net with attention gate for enhanced automated brain tumor segmentation},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). DNA sequence design model for multi-scene fusion. <em>NCA</em>, <em>37</em>(7), 5499-5520. (<a href='https://doi.org/10.1007/s00521-024-10905-9'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Due to its unique properties and excellent sequence design methods, DNA finds wide applications in computing, information storage, molecular circuits, and biological diagnosis. Previous efforts to enhance the efficiency and precision of DNA sequence design have led to the proposal of various universal DNA sequence design methods. These methods optimize the arrangement of the four bases to reduce sequence similarity and meet specific criteria. However, prior investigations have predominantly focused on sequence design within single-scene frameworks, overlooking the complexities associated with designing for multi-scene fusion, such as ion-bridge mismatch, tri-base sequence design, and others. To address this gap, we fused four common scenes and introduced two novel constraint models to facilitate DNA sequence design for multi-scene fusion. Additionally, we developed a dynamic virus spread algorithm as the core for optimizing DNA sequences and evaluated it using 23 well-known benchmark functions. Furthermore, our algorithm outperformed eight popular swarm evolutionary algorithms in eight dominant results. Finally, we simulated the optimization of four distinct scenes, demonstrating that our sequences met expected performance levels in their respective areas. Thus, our work provides a practical tool for designing DNA sequences tailored to various specific applications.},
  archive      = {J_NCA},
  author       = {Yao, Yao and Zheng, Yanfen and Cui, Shuang and Hou, Yaqing and Zhang, Qiang and Wei, Xiaopeng},
  doi          = {10.1007/s00521-024-10905-9},
  journal      = {Neural Computing and Applications},
  month        = {3},
  number       = {7},
  pages        = {5499-5520},
  shortjournal = {Neural Comput. Appl.},
  title        = {DNA sequence design model for multi-scene fusion},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). An efficient and scalable semi-supervised framework for semantic segmentation. <em>NCA</em>, <em>37</em>(7), 5481-5497. (<a href='https://doi.org/10.1007/s00521-024-10891-y'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Semantic segmentation has succeeded remarkably in various applications, such as autonomous vehicles and robotic systems. However, the training process for such techniques necessitates a significant amount of labeled data. Although semi-supervised frameworks can alleviate this issue, advanced approaches typically require multiple baseline models to form a dual model, which is costly in space computation complexity. To relieve the undesired computational cost for systems with precious computation and memory resources, we propose an efficient and scalable semi-supervised learning framework to significantly improve the performance with a few additional parameters concerning the baseline models. This framework includes a pseudo-dual module and a self-rectification module. The overall structure comprises three parts: an encoder, a shallow decoder, and a deep decoder. The deep decoder is connected to a deep layer of the encoder, and the shallow decoder is connected to a shallow layer. As knowledge distillation transfers knowledge from one model to another, the pseudo-dual module can distill knowledge from the ensemble of two decoders to improve the encoder, which can implicitly form a pseudo-dual model. The self-rectification module calculates class-wise likelihoods according to the similarity between features and class prototypes learned from different decoders and rectifies low-confidence pseudo-labels. The effectiveness of such rectification is justified theoretically and numerically. In our experiments with DeepLabV2, our methods outperform others in mIoU by over 1.21% with 1/8 labeled data using the Cityscapes dataset and by 0.38% with 1/8 labeled data using PASCAL VOC 2012 datasets. In most cases, our approach can also save more than 30% of memory costs during training. Nevertheless, the effectiveness of the proposed approach also depends on the quality of pseudo-labels generated by backbone models and may encounter challenges when handling data with highly imbalanced classes.},
  archive      = {J_NCA},
  author       = {Hao, Huazheng and Xiao, Hui and Xiong, Junjie and Dong, Li and Yan, Diqun and Liang, Dongtai and Zhuang, Jiayan and Peng, Chengbin},
  doi          = {10.1007/s00521-024-10891-y},
  journal      = {Neural Computing and Applications},
  month        = {3},
  number       = {7},
  pages        = {5481-5497},
  shortjournal = {Neural Comput. Appl.},
  title        = {An efficient and scalable semi-supervised framework for semantic segmentation},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). MGAN-CRCM: A novel multiple generative adversarial network and coarse refinement-based cognizant method for image inpainting. <em>NCA</em>, <em>37</em>(7), 5459-5480. (<a href='https://doi.org/10.1007/s00521-024-10886-9'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Image inpainting is a recognized method for restoring the properties of pixels in damaged or incomplete images in computer vision technology. Some recent techniques based on generative adversarial network (GAN) image inpainting have outperformed traditional approaches due to their excellent deep learning capability and adaptability to various image domains. Since residual networks (ResNet) also gained popularity over time due to their property as a generative model, offering better feature representation and compatibility with other architectures, how could we leverage both of these models to result in even greater success in image inpainting? This paper proposes a novel architecture for image inpainting based on GAN and residual networks. Our proposed architecture consists of three models: Transpose Convolution-based GAN, Fast ResNet-Convolutional Neural Network, and Co-Modulation GAN. Transpose Convolution-based GAN is our newly designed architecture. It produces guided and blind image inpainting, and FR-CNN performs the object removal case. Co-Mod GAN acts as a refinement layer because it refines the results from Transpose Convolution-based GAN and FR-CNN. To train and evaluate our proposed architecture on publicly available benchmark datasets: CelebA, Places2, and ImageNet are used. Our approach proves our hypothesis, and our proposed model acquires the highest accuracy of 96.59% in the ImageNet dataset, FR-CNN acquires the highest accuracy of 96.70% in the Places2 dataset, and Co-Mod GAN acquires the highest accuracy of 96.16% in the CelebA dataset. Through an analysis of both qualitative and quantitative comparisons, it is evident that our proposed model exceeds existing architectures in performance.},
  archive      = {J_NCA},
  author       = {Asad, Nafiz Al and Pranto, Md. Appel Mahmud and Shiam, Shbiruzzaman and Akand, Musaddeq Mahmud and Yousuf, Mohammad Abu and Hasan, Khondokar Fida and Moni, Mohammad Ali},
  doi          = {10.1007/s00521-024-10886-9},
  journal      = {Neural Computing and Applications},
  month        = {3},
  number       = {7},
  pages        = {5459-5480},
  shortjournal = {Neural Comput. Appl.},
  title        = {MGAN-CRCM: A novel multiple generative adversarial network and coarse refinement-based cognizant method for image inpainting},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Develop a novel, faster mask region-based convolutional neural network model with leave-one-subject-out to predict freezing of gait abnormalities of parkinson’s disease. <em>NCA</em>, <em>37</em>(7), 5441-5457. (<a href='https://doi.org/10.1007/s00521-024-10832-9'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A common symptom of severe Parkinson’s disease (PD) is Freezing of Gait (FoG), a gait disorder that causes sudden difficulty in initiating or maintaining walking. FoG frequently leads to falls and has a detrimental impact on a patient’s regular life. Real-time detection algorithms identify FoG occurrences using wearable sensors. Anticipating FoG in advance allows for pre-emptive cueing, which may prevent the episodes or reduce their severity and duration. This research proposes a Faster Mask Region-based Convolutional Neural Network (FMRCNN) signal processing approach for FoG identification. The model captured gyroscope, magnetometer, and tri-axial accelerometer signals using an inertial measurement device on the left side of the abdomen. The experimental results demonstrate a reduction in the equal error rate to 1.9% in the Leave-One-Subject-Out (LOSO) Cross-Validation (CV) with Long Short Term Memory (LSTM) assessment. Additionally, the tenfold CV evaluation enhances specificity and sensitivity by 0.045 and 0.017, respectively, compared to previous best results. It takes only 0.52 ms to detect a 256-data section. The proposed work uses the LOSO-CV-LSTM to evaluate various machine learning (ML) and deep learning (DL) techniques for FoG detection. The proposed system not only detects FoG but also enhances the automation of PD detection and therapy at an earlier stage. The results demonstrate that the proposed system improves performance measures compared to existing systems.},
  archive      = {J_NCA},
  author       = {Ezhilarasi, J. and Senthil Kumar, T.},
  doi          = {10.1007/s00521-024-10832-9},
  journal      = {Neural Computing and Applications},
  month        = {3},
  number       = {7},
  pages        = {5441-5457},
  shortjournal = {Neural Comput. Appl.},
  title        = {Develop a novel, faster mask region-based convolutional neural network model with leave-one-subject-out to predict freezing of gait abnormalities of parkinson’s disease},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Event-triggered adaptive fuzzy inverse optimal control of steer-by-wire vehicle systems. <em>NCA</em>, <em>37</em>(7), 5429-5439. (<a href='https://doi.org/10.1007/s00521-024-10768-0'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, an event-triggered adaptive fuzzy inverse optimal control issue is investigated for the steer-by-wire vehicle (SBWV) systems. Firstly, fuzzy logic systems are adopted to model the unknown nonlinear dynamics and an auxiliary nonlinear system is established. Then, an event-triggered mechanism (ETM) is established to reduce the numbers of controller execution times. Subsequently, based on designed auxiliary nonlinear system and ETM, an event-triggered adaptive fuzzy inverse optimal control algorithm is proposed by employing the backstepping control technique. It is proved that the developed control method can ensure the stability of SBWV systems and the tracking error converges to the neighborhood of zero. Finally, the simulation results are provided to demonstrate the effectiveness of presented control approach.},
  archive      = {J_NCA},
  author       = {Zhang, Jiaming and Zuo, Yi and Tong, Shaocheng},
  doi          = {10.1007/s00521-024-10768-0},
  journal      = {Neural Computing and Applications},
  month        = {3},
  number       = {7},
  pages        = {5429-5439},
  shortjournal = {Neural Comput. Appl.},
  title        = {Event-triggered adaptive fuzzy inverse optimal control of steer-by-wire vehicle systems},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A novel w13 deep CNN structure for improved semantic segmentation of multiple objects in remote sensing imagery. <em>NCA</em>, <em>37</em>(7), 5397-5427. (<a href='https://doi.org/10.1007/s00521-024-10765-3'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper proposes a novel convolutional neural network (CNN) architecture designed for semantic segmentation in remote sensing images. The proposed W13 Net model addresses the inherent challenges of segmentation tasks through a carefully crafted architecture, combining the strengths of multistage encoding–decoding, skip connections, combined weighted output, and concatenation techniques. Compared with different segmentation models, the suggested model performs better. A comprehensive analysis of different segmentation models has been carried out, resulting in an extensive comparison between the proposed W13 Net and five existing state-of-the-art segmentation architectures. Utilizing two standardized datasets, the Dense Labeling Remote Sensing Dataset Termed (DLRSD), and the Mohammad Bin Rashid Space Center (MBRSC) Dubai Aerial Imagery Dataset, the evaluation entails training, testing, and validation across different classes. The W13 Net demonstrates adaptability, generalization capabilities, and superior results in key metrics, all while displaying robustness across a variety of datasets. A number of metrics, including accuracy, precision, recall, F1 score, and IOU, were used to evaluate the system’s performance. According to the experimental results, the W13 Net model obtained an accuracy of 87.8%, precision of 0.88, recall of 0.88, F1 score of 0.88, and IOU of 0.74. The suggested model showed a significant improvement in segmentation IOU, with an increase of up to 18%, when compared to other with the recent segmentation models taking into consideration the model’s comparatively low number of parameter (2.2 million) in comparison with the recent models.},
  archive      = {J_NCA},
  author       = {Elgamily, Khaled Mohammed and Mohamed, M. A. and Abou-Taleb, Ahmed Mohamed and Ata, Mohamed Maher},
  doi          = {10.1007/s00521-024-10765-3},
  journal      = {Neural Computing and Applications},
  month        = {3},
  number       = {7},
  pages        = {5397-5427},
  shortjournal = {Neural Comput. Appl.},
  title        = {A novel w13 deep CNN structure for improved semantic segmentation of multiple objects in remote sensing imagery},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Stress detection based EEG under varying cognitive tasks using convolution neural network. <em>NCA</em>, <em>37</em>(7), 5381-5395. (<a href='https://doi.org/10.1007/s00521-024-10737-7'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {One tool for promoting mental health is human stress detection through multitasks of electroencephalography (EEG) recordings. This study proposed a short-term stress detection approach using VGGish as a feature extraction and convolution neural network (CNN) as a classifier based on EEG signals from the SAM 40 dataset. This database was recently available and was collected from 40 patients using 32 channels to identify performance on four tasks including Stroop color-word test (SCWT), answering arithmetic problems, finding mirror-identical images, and relaxing. Each task took 25 s to complete and was then repeated three times to record three trials. This means that the total EEG data contain 480 signals for four tasks recorded using 120 trials per task. The primary objective of this research was to track the amount of short-term stress that patients experienced while they engaged in the four mental tasks. Moreover, the VGGish-CNN model is applied to the SAM 40 dataset using five stages including signal preprocessing, segmentation, filtration, spectrogram, and classification process. We compared the VGGish-CNN model and the VGGish model for stress-based EEG classification to determine the best classification accuracy. The proposed approach for stress detection is the preliminary study that achieved an accuracy of 99.25% using the VGGish-CNN model on the SAM 40 dataset. Next, k-fold cross validation is performed to verify the efficiency of the VGGish-CNN model. This study can advance the application of brain–computer interface (BCI) and its use to identify patterns in EEG data that invoke stress-related inferences to aid in the diagnosis of mental disorders. In the future, investigation of human stress using EEG data will be useful in neurorehabilitation.},
  archive      = {J_NCA},
  author       = {Afify, Heba M. and Mohammed, Kamel K. and Hassanien, Aboul Ella},
  doi          = {10.1007/s00521-024-10737-7},
  journal      = {Neural Computing and Applications},
  month        = {3},
  number       = {7},
  pages        = {5381-5395},
  shortjournal = {Neural Comput. Appl.},
  title        = {Stress detection based EEG under varying cognitive tasks using convolution neural network},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A robust pointer meter reading method for inspection robots in real industrial scenarios. <em>NCA</em>, <em>37</em>(7), 5369-5379. (<a href='https://doi.org/10.1007/s00521-024-10682-5'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In real industrial scenarios, inspection robots can replace humans to automatically read pointer meters, which greatly improves productivity and safety. However, existing automatic reading methods perform poorly in complex robot operating environments. To this end, we propose an automatic reading method for pointer meters, which can be better applied to inspection robot working conditions. Firstly, we propose a meter detection network, Yolo_Meter, which combines an attention mechanism and an adaptive feature fusion module. This network can accurately locate the meter from the perspective of robots and crop out images that are suitable for automatic meter readings. Secondly, we propose an oriented pointer detection network (OPDNet) to fit the tip position of the pointer precisely. Thirdly, we design a deep neural network OCR_Meter to obtain the scale and unit information of the meter by text detection and a filtering algorithm, which is adaptable to multiple types of meters. Finally, we propose a polar pixel method for locating the main scale lines and design the local angle method to calculate the readings of the pointer meters. Adequate experiments demonstrate the high accuracy and robustness of our method in real-world scenarios, with an average global error of only 0.73%.},
  archive      = {J_NCA},
  author       = {Huang, Zhiqing and Wang, Yuchao and Zhang, Yanxin and Zhang, Chenguang},
  doi          = {10.1007/s00521-024-10682-5},
  journal      = {Neural Computing and Applications},
  month        = {3},
  number       = {7},
  pages        = {5369-5379},
  shortjournal = {Neural Comput. Appl.},
  title        = {A robust pointer meter reading method for inspection robots in real industrial scenarios},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A novel approach to predict the traffic accident assistance based on deep learning. <em>NCA</em>, <em>37</em>(7), 5343-5368. (<a href='https://doi.org/10.1007/s00521-024-10939-z'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {According to the World Health Organization, thousands of people die every year in road traffic accidents. A crucial problem is the prediction of medical assistance in these accidents. For this purpose, we propose a new deep learning model whose goal is to distinguish whether a traffic accident requires medical assistance. The proposed perspective is general, so the model is valid for any dataset from any city. For this purpose, we present a model divided into three differentiated stages. In the first pre-processing stage, a general data treatment is performed, from data collection and cleaning to balancing. Secondly, the post-processing stage employs genetic and boosting algorithms to obtain the importance of all the data set variables used in the prediction. In the last stage, Model Training, a new model based on two-dimensional convolutional neural networks is applied to obtain a prediction of the need for medical assistance in traffic accidents. Finally, we test the effectiveness and accuracy of the proposed model by applying it to traffic accident datasets in six different cities. The obtained experimental results show that our framework achieves higher accuracy in all cities compared to six state-of-the-art models, confirming its suitability and applicability, even in real time.},
  archive      = {J_NCA},
  author       = {Vicent, José F. and Curado, Manuel and Oliver, José L. and Pérez-Sala, Luis},
  doi          = {10.1007/s00521-024-10939-z},
  journal      = {Neural Computing and Applications},
  month        = {3},
  number       = {7},
  pages        = {5343-5368},
  shortjournal = {Neural Comput. Appl.},
  title        = {A novel approach to predict the traffic accident assistance based on deep learning},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Numerical prediction of ureter stone size using an integrated CFD-ML approach. <em>NCA</em>, <em>37</em>(7), 5325-5341. (<a href='https://doi.org/10.1007/s00521-024-10880-1'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Ureteral flow parameters provide significant details about its physical attributes. Ureter is a single transport medium for urine transmission from kidney to ureter and its health is very important for a healthy human body. Understanding the fluid flow behavior can contribute toward the ureter health monitoring including estimation of any kind of blockage in the flow. Using ANSYS Fluent, Computational Fluid Dynamics (CFD) analysis and the grid independence study are carried out through iterative simulation process to achieve the solution independence. The CFD modeling provides tools and techniques to observe varying fluid parameters such as pressure, velocity and effect of the flow on smooth walls. Fluid Structure Interaction (FSI), an effective technique to analyze the effects of such flows on the ureter walls is also employed. Although the exact modeling of the ureter wall is not possible due to its complex physical parameters, some of its available physiological properties can be used to visualize the model of the ureter numerically. The present study is intended to predict the ureter stone size by using the FSI analysis. The simulations are carried out by increasing the stone size gradually from 1.7 to 3.4 mm and the input flow parameters are compared with the output flow parameters within the same solution setup and boundary conditions via artificial neural network in MATLAB. The output results obtained from the FSI simulations are then utilized to generate a prediction model for the ureter stone size. It is observed that the increasing stone size has a significant effect on the ureter wall, causing high stress regions in the point of interaction. The findings also revealed that the predicted size of the ureter stone is the closest to the actual size and with the least mean squared error at 80 optimal neurons.},
  archive      = {J_NCA},
  author       = {Ashraf, Muhammad Mubashar and Kamal, Khurram and Fahad, Muhammad and Noor, N. F. M. and Ratlamwala, Tahir Abdul Hussain},
  doi          = {10.1007/s00521-024-10880-1},
  journal      = {Neural Computing and Applications},
  month        = {3},
  number       = {7},
  pages        = {5325-5341},
  shortjournal = {Neural Comput. Appl.},
  title        = {Numerical prediction of ureter stone size using an integrated CFD-ML approach},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Single-pass end-to-end neural decompilation using copying mechanism. <em>NCA</em>, <em>37</em>(7), 5309-5323. (<a href='https://doi.org/10.1007/s00521-024-10735-9'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Traditional decompilers utilize countless hardcoded rules written by subject matter experts, making them inflexible. Some recent systems address this using deep learning. The current consensus is that these systems have to include considerable domain knowledge and iterative heuristic components to solve parts of the decompilation problem, particularly the problem of predicting identifiers and literals. In this paper, we present a single-pass end-to-end neural decompilation system that utilizes copying mechanism. The copying mechanism is able to copy the literals and (offsets of) variables directly from the assembly code, in a single step, as part of the single forward pass through the model. Additionally, we take a further step toward decompiling real-world code by addressing important programming constructs like switch statements, function definitions, and function calls. We compile a dataset of real-world programming competition code and evaluate our model on it. The method achieves a program accuracy of 73% on the hardest complexity level of our generated dataset and 51% on the real-world examples without any additional error correction (EC) techniques, which surpasses the results of previous works without EC.},
  archive      = {J_NCA},
  author       = {Szalay, Gergő and Poór, Máté Bálint and Pintér, Balázs and Gregorics, Tibor},
  doi          = {10.1007/s00521-024-10735-9},
  journal      = {Neural Computing and Applications},
  month        = {3},
  number       = {7},
  pages        = {5309-5323},
  shortjournal = {Neural Comput. Appl.},
  title        = {Single-pass end-to-end neural decompilation using copying mechanism},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Driving behaviors analysis for public transport drivers in kuwait: A machine learning approach to drivers safety. <em>NCA</em>, <em>37</em>(7), 5289-5307. (<a href='https://doi.org/10.1007/s00521-024-10964-y'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This research paper addresses the critical concern of evaluating driving behaviors among bus drivers in Kuwait to enhance road safety and prevent accidents. Real driving data from 73 bus drivers working in Kuwait Public Transport Company (KPTC), collected through Teltonika devices, forms the basis of the quantitative analysis. The OPTICS (Ordering Points to Identify the Clustering Structure) algorithm and Expectation–Maximization (EM) clustering were employed using Gaussian Mixture Models (EM-GMM) to classify drivers into distinct behavioral categories. Correlation analyses were then conducted to pinpoint factors influencing risky driving. It was revealed that over speeding is the predominant contributor, accounting for 84.89% of unsafe behaviors. Predictive modeling is undertaken using Gradient Boosted Trees (GBT) and discriminant analysis, with GBT emerging as the most effective, achieving the highest accuracy. Risk indices for each driver cluster are calculated, showing that 28% of drivers exhibit unsafe practices. The probability of accidents for drivers with hazardous tendencies was determined to be 0.772, while the general likelihood of accidents among bus drivers in Kuwait is calculated at 0.318. Surprisingly, no significant correlation is found between age and driving behavior, highlighting the influence of factors such as psychological conditions, fatigue, weather, and road conditions on driving conduct. The findings contribute valuable insights for developing targeted interventions to mitigate risky driving behaviors and enhance overall road safety in the region.},
  archive      = {J_NCA},
  author       = {AlKheder, Sharaf and Al-Saleh, Hanaa},
  doi          = {10.1007/s00521-024-10964-y},
  journal      = {Neural Computing and Applications},
  month        = {3},
  number       = {7},
  pages        = {5289-5307},
  shortjournal = {Neural Comput. Appl.},
  title        = {Driving behaviors analysis for public transport drivers in kuwait: A machine learning approach to drivers safety},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Chicken moth flame optimization and region-based convolution neural network for water quality prediction. <em>NCA</em>, <em>37</em>(7), 5271-5288. (<a href='https://doi.org/10.1007/s00521-024-10878-9'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Water is an important source for the sustenance of life, and its quality has a direct impact on the environment and public health. Water is utilized for various practices, such as agriculture, industry, and drinking. Geo-environmental pollution caused by various types of waste such as municipal, industrial, medical, solid, and agricultural fields makes the water unsuitable for usage. Water quality is primarily impacted by the discharge of agricultural and industrial effluents into the environment, which disrupts biological systems. Predicting water quality is crucial for environmental monitoring, ecosystem sustainability, and aquaculture. Accurate water quality prediction is essential for sustainable water management. Hence, the quality of the water should be maximized by managing water resources. In this research, an optimization-enabled deep learning model named chicken moth flame–region-based convolution neural network (CMF-RCNN) is introduced to predict water quality. Here, hidden properties of water are analyzed and utilized to predict the water quality characteristics. Moreover, input data are normalized using Z-score normalization and optimal features are selected via correlation analysis. Later, the water quality is predicted from the selected features using RCNN. The prediction performance of RCNN is enhanced by fine-tuning its weights by utilizing CMF. Moreover, the performance of CMF-RCNN is analyzed with respect to existing water quality prediction models, and the CMF-RCNN attained superior performance with precision of 0.927, recall of 0.946, and F1-score of 0.936, respectively.},
  archive      = {J_NCA},
  author       = {Jose, D. Justin and Sulochana, C. Helen},
  doi          = {10.1007/s00521-024-10878-9},
  journal      = {Neural Computing and Applications},
  month        = {3},
  number       = {7},
  pages        = {5271-5288},
  shortjournal = {Neural Comput. Appl.},
  title        = {Chicken moth flame optimization and region-based convolution neural network for water quality prediction},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Assessing the impact of differential privacy in transfer learning with deep neural networks and transformer language models. <em>NCA</em>, <em>37</em>(6), 5097-5119. (<a href='https://doi.org/10.1007/s00521-024-10547-x'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The realization of trustworthy artificial intelligence strongly relies on privacy, fairness, and accountability requirements. Although model trustworthiness results from the synergy between these requirements, some effects often clash with one another and may propagate issues into downstream tasks. This paper empirically indicates that optimization algorithms with differential privacy (DP), such as differentially private stochastic gradient descent (DP-SGD) and differentially private Adam (DP-Adam), disproportionately increase differences in accuracy between groups and domains in transfer learning based on convolutional neural network (CNN) and long-short-term memory (LSTM) models. We investigate the impact of DP-SGD and DP-Adam in transfer learning models, which receive DP noise to hinder privacy risks and perform predictions on target domain data. In addition, after extensive experimental evaluation, we provide evidence that fine-tuning may reduce the differences in accuracy for most groups compared to gradient boosting and ensemble methods. However, differentially private optimizers may reverse gains in accuracy from transfer learning for CNNs and LSTMs. Unevenly reduced model accuracy may jeopardize the predictions’ fairness, causing numerous real-world implications, such as denying credit to people in need or undermining complex language processing. Identifying scenarios where privacy protection conflicts with fairness for transfer learning is a problem often overlooked yet directly aligned with the accountability requirement in trustworthy artificial intelligence. Therefore, assessing the impact of differentially private optimizers in transfer learning is the first step to developing mitigation strategies.},
  archive      = {J_NCA},
  author       = {Sousa, Samuel and Trügler, Andreas and Kern, Roman},
  doi          = {10.1007/s00521-024-10547-x},
  journal      = {Neural Computing and Applications},
  month        = {2},
  number       = {6},
  pages        = {5097-5119},
  shortjournal = {Neural Comput. Appl.},
  title        = {Assessing the impact of differential privacy in transfer learning with deep neural networks and transformer language models},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Simultaneous and continuous estimation of upper limb kinematics of shoulder press movements: State-space EMG model. <em>NCA</em>, <em>37</em>(6), 5077-5095. (<a href='https://doi.org/10.1007/s00521-024-10813-y'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, a state space electromyography (EMG) model is proposed to predict multi-DOF upper limb joint angles continuously and simultaneously. The two main approaches to estimating multi-joint kinematics are numerical and model-based methods. The high computational cost of the numerical algorithms causes delays. On the other hand, the open-loop model-based systems would not be effective to estimate the joint angles due to the accumulated errors that are not compensated. To overcome this problem, a relation between EMG signal features and joint angles is trained and developed offline to form measurement equations. Then, the integration of the Hill-based musculoskeletal model and the trained measurement equations is used in a state-space model. By using Extended Kalman Filter (EKF), the multi-joint angles are estimated continuously and simultaneously. In this paper, the shoulder abduction–adduction and elbow flexion–extension angles of the human arm during dumbbell shoulder press are estimated using the EMG signals. The average root mean squared error (RMSE) of the estimation and real joint angles of the shoulder and elbow to be 0.26 and 0.27, respectively, indicates that the proposed model-based method can be used to estimate the joint angles continuously and simultaneously reducing the computational time of numerical estimations.},
  archive      = {J_NCA},
  author       = {Katibeh, Fatemeh and Haghpanah, Seyyed Arash and Taghvaei, Sajjad and Eftekhari, Fereshte},
  doi          = {10.1007/s00521-024-10813-y},
  journal      = {Neural Computing and Applications},
  month        = {2},
  number       = {6},
  pages        = {5077-5095},
  shortjournal = {Neural Comput. Appl.},
  title        = {Simultaneous and continuous estimation of upper limb kinematics of shoulder press movements: State-space EMG model},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Determination of the common electrodes for users and increasing the classification accuracy of motor imagery EEG. <em>NCA</em>, <em>37</em>(6), 5057-5076. (<a href='https://doi.org/10.1007/s00521-024-10789-9'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent studies, it is observed that the success rates for the classification of the EEG (electroencephalogram) data have increased by using deep neural networks. However, the portability and practicability of the brain computer interface systems are still not good since too many electrodes are used, which makes the system expensive, time-consuming to setup and uncomfortable for the users. There are studies that reduce the number of electrodes to increase the classification accuracy and to reduce the computing load. For this purpose, the number of electrodes is reduced by performing some operations on the existing set of electrodes, and a subset is determined for each subject. In this study, a set of five electrodes is selected to classify the EEG signals. Electrodes that would be common to all subjects are investigated by using the Rayleigh coefficient map and divergence measure. After the common electrodes are determined, the data belonging to the other electrodes are removed from the datasets. For the classification of the EEG signals a divergence-based deep neural network (DivFE), which had previously shown high performances, is employed. The preprocesses such as filtering, continuous wavelet transform, common spatial patterns, short-time Fourier transform and independent components analysis are examined to improve the classification accuracy of the DivFE. The 2005 BCI III-3a, 2008 BCI IV-2a and an artificial EEG dataset are used for the training and test processes. Classification success rates of 80.3, 65.1 and 77.1% are obtained for four classes in the 2005 BCI III-3a dataset, for four classes in the 2008 BCI IV-2a dataset and for ten classes in the artificial EEG dataset, respectively. It is observed that the classification accuracies obtained in the literature can be achieved by using only five common electrodes.},
  archive      = {J_NCA},
  author       = {Özkahraman, Ali and Ölmez, Tamer and Dokur, Zümray},
  doi          = {10.1007/s00521-024-10789-9},
  journal      = {Neural Computing and Applications},
  month        = {2},
  number       = {6},
  pages        = {5057-5076},
  shortjournal = {Neural Comput. Appl.},
  title        = {Determination of the common electrodes for users and increasing the classification accuracy of motor imagery EEG},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Decoding BatchNorm statistics via anchors pool for data-free models based on continual learning. <em>NCA</em>, <em>37</em>(6), 5039-5055. (<a href='https://doi.org/10.1007/s00521-024-10904-w'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Generating high-quality samples reversely from existing models is a significant technique in continual learning and knowledge distillation. Existing approaches either fail to generate valid samples from models or require massive storage space to accommodate a large number of examples generated at once. To address the above issues, this paper presents a simple but effective method called DBSAP, referring to decoding Batchnorm statistics via anchors pool. In the proposed method, an anchor pool is constructed for each BatchNorm layer in a neural network to store the corresponding feature representation of a small number of examples sampled. During reverse-generating samples, the anchors are further sampled from anchor pools to participate in the optimization of BN loss, which can generate valid samples even with a small batch size, i.e., very low storage requirement. When multiple models are available in continual learning, more loss optimizations can be performed to further guarantee the validity of generated samples. The extensive experiments on datasets CIFAR10, CIFAR100, ImageNet-200 and ImiageNet-1000 demonstrate the effectiveness and superiority of the proposed method. Compared with the state-of-the-art methods, the DBSAP method can generate the images samples with much higher quality based on multiple metrics and require only about 1/4 GPU memory on average for sample generation.},
  archive      = {J_NCA},
  author       = {Li, Xiaobin and Wang, Weiqiang and Xu, Guangluan},
  doi          = {10.1007/s00521-024-10904-w},
  journal      = {Neural Computing and Applications},
  month        = {2},
  number       = {6},
  pages        = {5039-5055},
  shortjournal = {Neural Comput. Appl.},
  title        = {Decoding BatchNorm statistics via anchors pool for data-free models based on continual learning},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). An algorithmic approach to minimize road accidents in the highway system using hamiltonian fuzzy influence graphs. <em>NCA</em>, <em>37</em>(6), 5019-5038. (<a href='https://doi.org/10.1007/s00521-024-10874-z'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Bottlenecks in interconnection networks cause delays, traffic jams and hinder traffic flow efficiency. Fuzzy influence graphs offer a solution by enabling multiple high-capacity pathways. These graphs are well-organized, practical, applicable, and effective for handling ambiguity in real-world problems involving fuzzy data and information. They could provide knowledge regarding the influence of a vertex on a vertex or an edge of the same or another graph, whether they are connected or unconnected. This research introduces domination in Hamiltonian fuzzy influence graphs (HFIGs), expanding fuzzy graph theory. We compute key parameters using strong fuzzy influence pairs and propose an algorithm for the minimum domination number, applicable to artificial intelligence. A highway accident minimization model using HFIGs is analyzed using the TOPSIS, VIKOR and EDAS methods.},
  archive      = {J_NCA},
  author       = {Hussain, Muhammad Tanveer and Ur Rehman, Fahad and Rashid, Tabasam},
  doi          = {10.1007/s00521-024-10874-z},
  journal      = {Neural Computing and Applications},
  month        = {2},
  number       = {6},
  pages        = {5019-5038},
  shortjournal = {Neural Comput. Appl.},
  title        = {An algorithmic approach to minimize road accidents in the highway system using hamiltonian fuzzy influence graphs},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Optimized neuro fuzzy convolutional networks in gene expression analysis in microscopic image feature extraction and classification by deep learning architectures. <em>NCA</em>, <em>37</em>(6), 5005-5017. (<a href='https://doi.org/10.1007/s00521-024-10858-z'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the use of gene expression microarrays, also referred to as "gene chips," scientists can monitor the pace at which hundreds of genes in each cell or tissue are expressed and converted into proteins. These gene expression pictures of biological activity can help with disease diagnosis, prognosis, and treatment planning. They can also reveal new targets for drug discovery and infer regulatory circuits in cells. The popularity of deep learning methodologies has grown recently because of their wide range of applications in several sectors for inference and prediction. This research proposes novel techniques in gene expression analysis in microscopic image data feature extraction with classification. Input is collected as microscopic image which is processed for noise removal and image smoothening. Then, these image features were extracted using neuro fuzzy convolutional networks. The extracted features have been classified using a self-learning-based genetic algorithm to obtain input images' gene expression. Using these organized gene expression immunity system and abnormality of the body function have been analyzed. It is proposed that existing approaches be improved. I developed a multivariate and hybrid feature selection strategy to achieve good classification performance for high-dimension classification issues. Experimental analysis is carried out for various microscopic image datasets in terms of accuracy, recall, precision, F-Score, RMSE, and MAP.},
  archive      = {J_NCA},
  author       = {Qamar, Shamimul},
  doi          = {10.1007/s00521-024-10858-z},
  journal      = {Neural Computing and Applications},
  month        = {2},
  number       = {6},
  pages        = {5005-5017},
  shortjournal = {Neural Comput. Appl.},
  title        = {Optimized neuro fuzzy convolutional networks in gene expression analysis in microscopic image feature extraction and classification by deep learning architectures},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). DNN-STACK: A stacking technique based on deep neural network for detecting copy-move forgery. <em>NCA</em>, <em>37</em>(6), 4989-5004. (<a href='https://doi.org/10.1007/s00521-024-10804-z'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, detecting image forgery has become an important topic because of the availability of efficient and sophisticated image-editing software. Copy-move forgery, which involves copying a section of an image and pasting it to a different location inside the same image, is one of the most popular tampering techniques that alter the identity of a given image. This paper presents DNN-STACK, a deep neural network-based stacking scheme to identify copy-move forgery by effectively combining the predictions yielded by different base-level models. To extract hierarchical representations from the given images and generate base-level predictions, the proposed approach uses five distinct but complementary deep learning-based base-level models. After that, a consensus prediction is generated using the proposed DNN-STACK model, which can infer the nonlinear relationship between the base-level predictions. Experimental evaluation on three publicly available datasets, such as MICC-F600, MICC-F2000, and FAU, reveals the fact that the proposed DNN-STACK model outperforms the existing forgery detection techniques, significantly improving detection accuracy across varying image resolutions and attack types, including rotation, scaling, noise, and compression levels.},
  archive      = {J_NCA},
  author       = {Krishnalal, G. and Jagathy Raj, V. P. and Madhu, G. and Arun, K. S.},
  doi          = {10.1007/s00521-024-10804-z},
  journal      = {Neural Computing and Applications},
  month        = {2},
  number       = {6},
  pages        = {4989-5004},
  shortjournal = {Neural Comput. Appl.},
  title        = {DNN-STACK: A stacking technique based on deep neural network for detecting copy-move forgery},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Machine learning-based forecasts of residential property prices in hangzhou city, zhejiang province, china. <em>NCA</em>, <em>37</em>(6), 4971-4988. (<a href='https://doi.org/10.1007/s00521-024-10726-w'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Chinese real estate market has grown at such a quick rate over the last few decades, up to the current falling patterns that began at the end of 2021. This difficulty has made it more difficult for the government and investors to predict future property prices effectively. This has occurred as a result of the state of the economy at the moment. In this research, we examine the monthly residential property prices in Hangzhou City, Zhejiang Province, China, using Gaussian process regressions with a variety of kernels and basis functions. This research spans the months of January 2009 through July 2024. We use estimated models in our forecasting efforts. A combination of cross-validation and Bayesian optimisations is used to train these models. The prices that would be seen outside of the sample from June 2021 to July 2024 were successfully predicted by the generated models. These models have an accuracy of 1.0419 per cent for the relative root mean square error. It is plausible that our findings may be used alone or in combination with further projections to formulate theories about fluctuations in residential real estate prices and to conduct supplementary policy analysis.},
  archive      = {J_NCA},
  author       = {Jin, Bingzi and Xu, Xiaojie},
  doi          = {10.1007/s00521-024-10726-w},
  journal      = {Neural Computing and Applications},
  month        = {2},
  number       = {6},
  pages        = {4971-4988},
  shortjournal = {Neural Comput. Appl.},
  title        = {Machine learning-based forecasts of residential property prices in hangzhou city, zhejiang province, china},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Evolutionary recurrent neural network based on equilibrium optimization method for cloud-edge resource management in internet of things. <em>NCA</em>, <em>37</em>(6), 4957-4969. (<a href='https://doi.org/10.1007/s00521-024-10929-1'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cloud computing is an emerging field in information technology, enabling users to access a shared pool of computing resources. Despite its potential, cloud technology presents various challenges, with one of the most significant being resource management within the cloud environment. Specifically, the allocation of physical machines (PMs) to virtual machines (VMs) in the Infrastructure as a service (IaaS) layer aims to achieve optimal efficiency. To address this challenge, we propose an evolutionary recurrent neural network (RNN) for resource allocation, enhanced by the equilibrium optimization (EO) algorithm. This method improves the generalization performance and reduces the architecture complexity of RNNs, while ensuring better adaptability to various demand patterns in cloud environments. Additionally, the effectiveness of RNNs heavily relies on the quality of training data; hence, we generate appropriate training data to facilitate the development of an efficient resource management model. Through extensive simulations, our method demonstrates superior performance compared to traditional algorithms. However, it is worth noting that this study is primarily based on simulation results, highlighting the need for real-world validation to confirm its practical applicability. The proposed method's applicability to different cloud computing environments and scenarios, including various service models (IaaS, PaaS, and SaaS) and deployment models (public, private, and hybrid), is also briefly discussed. Future studies should explore the computational efficiency and potential optimization techniques of the EO-RNN approach and consider real-world implementations to validate its effectiveness and uncover practical challenges.},
  archive      = {J_NCA},
  author       = {Ebrahimi Mood, Sepehr and Rouhbakhsh, Adel and Souri, Alireza},
  doi          = {10.1007/s00521-024-10929-1},
  journal      = {Neural Computing and Applications},
  month        = {2},
  number       = {6},
  pages        = {4957-4969},
  shortjournal = {Neural Comput. Appl.},
  title        = {Evolutionary recurrent neural network based on equilibrium optimization method for cloud-edge resource management in internet of things},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Epileptic signal classification using convolutional neural network and shapley additive explainable artificial intelligence method. <em>NCA</em>, <em>37</em>(6), 4937-4955. (<a href='https://doi.org/10.1007/s00521-024-10915-7'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Neurological disorder epilepsy can cause life-threatening seizures which may produce abnormal patterns in electroencephalogram (EEG) signals. However, manual tracking of abnormal EEG patterns is subjective, and only artificial intelligence (AI)-based identification lacks trustworthiness. Therefore, we proposed an explainable AI (XAI)-based method to classify epileptic and non-epileptic patients. The proposed method has four steps. The first step creatively preprocesses EEG signals into a stack of spectrogram images and enhances them using filters. The second step employs convolutional neural networks (Densenet121, Resnet18 and VGG16) to classify images. The third step incorporates the SHapley Additive exPlanations (SHAP) method to explain the classification results. The fourth step identifies significant pixels and top contributing EEG channels for each classification. Moreover, we comparatively found the best AI and XAI combination among Densenet121, Resnet18 and VGG16, with SHAP using area over perturbation curve (AOPC). We used the Temple University Hospital EEG Epilepsy Corpus dataset (100 subjects: 50 epileptic, 50 non-epileptic) with k-fold cross-validation to evaluate the classification accuracy. Proposed VGG16-, Densenet121- and Resnet18-based approaches achieved 97.55%, 91.29% and 95% accuracy, respectively. Moreover, VGG16-, Densenet121- and Resnet18-based approaches with SHAP reported AOPC values of 197,600, 355,350 and 240,150, respectively. Additionally, we identified the top contributing EEG channels for each patient. The proposed VGG16-based approach achieved the highest accuracy compared to the literature using the same dataset. Densenet121 with SHAP reported the best explainability. Such an approach can help EEG technologists and neurologists to identify epileptic patients automatically by using EEG.},
  archive      = {J_NCA},
  author       = {Rathod, Prajakta and Naik, Shefali and Bhalodiya, Jayendra M.},
  doi          = {10.1007/s00521-024-10915-7},
  journal      = {Neural Computing and Applications},
  month        = {2},
  number       = {6},
  pages        = {4937-4955},
  shortjournal = {Neural Comput. Appl.},
  title        = {Epileptic signal classification using convolutional neural network and shapley additive explainable artificial intelligence method},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Blockchain-enhanced artificial intelligence for advanced collision avoidance in the internet of vehicles (IoV). <em>NCA</em>, <em>37</em>(6), 4915-4936. (<a href='https://doi.org/10.1007/s00521-024-10842-7'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The convergence of blockchain technology and artificial intelligence (AI) presents a promising solution for enhancing safety within the Internet of Vehicles (IoV) ecosystem. This paper introduces the "Blockchain-Based Collision Avoidance with AI for Vehicles" (BCA-CAR) algorithm, which aims to provide advanced and intelligent collision avoidance capabilities in IoV. BCA-CAR combines the security and data integrity features of blockchain with the real-time decision-making capabilities of AI to prevent collisions and improve road safety. The algorithm consists of five key phases: Data Collection and Processing, AI Collision Risk Assessment, Decision and Smart Contract Execution, Data Validation and Trust (Blockchain Integration), and Learning and Improvement. In the Data Collection and Processing phase, data from vehicle sensors, cameras, V2V and V2I communication, and external infrastructure is collected and preprocessed. The AI Collision Risk Assessment phase utilizes machine learning models to analyze real-time data and predict collision risks. In the Decision and Smart Contract Execution phase, smart contracts on the blockchain automate collision avoidance actions. The Data Validation and Trust phase ensures the authenticity and integrity of data through blockchain technology. Finally, the Learning and Improvement phase leverages historical collision data to enhance predictive models and overall system performance. BCA-CAR's primary objective is to enhance safety by preventing collisions, ensuring data trustworthiness, and providing intelligent collision avoidance capabilities. This innovative algorithm has the potential to revolutionize road safety in the era of IoV by reducing accidents, improving traffic management, and enhancing the security and privacy of vehicular communication. The findings highlight that Support Vector Regression (SVR) demonstrates strong predictive accuracy and adaptability within the Internet of Vehicles (IoV), offering a reliable modeling tool for precise forecasting while emphasizing the importance of maintaining high data quality standards in IoV applications.},
  archive      = {J_NCA},
  author       = {Talaat, Fatma M. and Hamza, Alyaa A.},
  doi          = {10.1007/s00521-024-10842-7},
  journal      = {Neural Computing and Applications},
  month        = {2},
  number       = {6},
  pages        = {4915-4936},
  shortjournal = {Neural Comput. Appl.},
  title        = {Blockchain-enhanced artificial intelligence for advanced collision avoidance in the internet of vehicles (IoV)},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Novel methodology for apple leaf disease classification with PCNN-IELM. <em>NCA</em>, <em>37</em>(6), 4895-4913. (<a href='https://doi.org/10.1007/s00521-024-10816-9'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Agriculture is crucial to the global economy, particularly in ensuring food security. Recent trends indicate that various plant diseases are causing substantial financial losses in the agricultural sector worldwide. Traditional manual inspection methods for detecting fruit and plant diseases are labor-intensive and inefficient. Adopting automated disease detection technologies could significantly enhance early diagnosis and reduce the economic impact of these diseases on agriculture. This study introduces an advanced model for classifying apple diseases by integrating a pre-trained convolutional neural network (PCNN), such as VGG16, VGG19, or ResNet50, with an incremental extreme learning machine (I-ELM) for efficient feature extraction and classification. A key innovation of this model is replacing the PCNN’s fully connected layer with the I-ELM, which eliminates the lengthy back-propagation process and significantly reduces training time. Integrating I-ELM with PCNN harnesses the rapid learning capabilities and robust generalization of I-ELM with the superior feature extraction abilities of CNNs. I-ELM simplifies the network architecture by avoiding the complex neural networks commonly used in other methods. The model’s effectiveness is rigorously evaluated on the well-known Plant Village dataset, demonstrating its ability to identify various apple diseases through performance metrics such as precision, sensitivity, specificity, accuracy, and the F1-score. Comparing existing deep learning models using these metrics highlights its superior performance. This innovation is up-and-coming for intelligent agricultural systems, offering an effective solution for classifying apple diseases and enabling timely and innovative farming practices.},
  archive      = {J_NCA},
  author       = {Navpreet and Roul, Rajendra Kumar},
  doi          = {10.1007/s00521-024-10816-9},
  journal      = {Neural Computing and Applications},
  month        = {2},
  number       = {6},
  pages        = {4895-4913},
  shortjournal = {Neural Comput. Appl.},
  title        = {Novel methodology for apple leaf disease classification with PCNN-IELM},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). An optimized ensemble model based on meta-heuristic algorithms for effective detection and classification of breast tumors. <em>NCA</em>, <em>37</em>(6), 4881-4894. (<a href='https://doi.org/10.1007/s00521-024-10719-9'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {One of the most common cancers among women worldwide is breast cancer (BC), and early diagnosis can save lives. Early detection of BC increases the likelihood of a successful outcome by enabling treatment to start sooner. Even in areas without access to a specialist physician, machine learning (ML) aids in early BC detection. The medical imaging community is becoming more interested in using ML, and deep learning (DL) to increase the accuracy of cancer screening. Many disease-related data are sparse. However, for DL models to perform well, a large amount of data is required. Because of this, the DL models that are currently in use on medical images are not as effective as they could be. Convolutional neural network (CNN) models have recently gained popularity in the medical industry, and they perform admirably in terms of high performance and robustness at image classification. The proposed method classifies data using ensemble pre-trained models such as the dense convolutional network (DenseNet)-121 and EfficientNet-B5 feature extractor networks, as well as the support vector machine for classification. Using a modified meta-heuristic optimizer, the selected pre-trained CNN hyperparameters were optimized to improve the performance. The experimental results for the presented model on the INbreast dataset show that the EfficientNet-B5 model is effective for BC classification, with overall accuracy, sensitivity, specificity, precision, and area under the ROC curve (AUC) values of 99.9%, 99.9%, 99.8%, 99.1%, 1.0, respectively.},
  archive      = {J_NCA},
  author       = {Saber, Abeer and Elbedwehy, Samar and Awad, Wael A. and Hassan, Esraa},
  doi          = {10.1007/s00521-024-10719-9},
  journal      = {Neural Computing and Applications},
  month        = {2},
  number       = {6},
  pages        = {4881-4894},
  shortjournal = {Neural Comput. Appl.},
  title        = {An optimized ensemble model based on meta-heuristic algorithms for effective detection and classification of breast tumors},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Accelerating deep learning with fixed time budget. <em>NCA</em>, <em>37</em>(6), 4869-4879. (<a href='https://doi.org/10.1007/s00521-024-10637-w'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The success of modern deep learning is attributed to two key elements: huge amounts of training data and large model sizes. Where a vast amount of data allow the model to learn more features, the large model architecture boosts the learning capability of the model. However, both these factors result in prolonged training time. In some practical applications such as edge-based learning and federated learning, limited-time budgets necessitate more efficient training methods. This paper proposes an effective technique for training arbitrary deep learning models within fixed time constraints utilizing sample importance and dynamic ranking. The proposed method is extensively evaluated in both classification and regression tasks in computer vision. The results consistently show clear gains achieved by the proposed method in improving the learning performance of various state-of-the-art deep learning models in both regression and classification tasks.},
  archive      = {J_NCA},
  author       = {Khan, Muhammad Asif and Hamila, Ridha and Menouar, Hamid},
  doi          = {10.1007/s00521-024-10637-w},
  journal      = {Neural Computing and Applications},
  month        = {2},
  number       = {6},
  pages        = {4869-4879},
  shortjournal = {Neural Comput. Appl.},
  title        = {Accelerating deep learning with fixed time budget},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A multistage intrusion detection method for alleviating class overlapping problem. <em>NCA</em>, <em>37</em>(6), 4853-4867. (<a href='https://doi.org/10.1007/s00521-024-10903-x'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Intrusion detection system (IDS) can identify abnormal network traffic and attacks, which is an important means of network security defense. However, some intrusion data are often disguised as normal data for transmission, which increases the difficulty of intrusion data classification. In addition, the existing packet-based or flow-based data feature extraction methods result in low feature dimensions, causing the problem of class overlapping between different categories with the same features. To clarify, overlapping samples are those that overlap between erroneous samples and correct samples. Nonoverlapping samples are those in the test set that do not match the characteristics of the already identified overlapping samples and are therefore considered nonoverlapping samples. Therefore, the detection effect of some attacks with high concealment is poor. In order to solve the above problems, this paper proposes a multistage intrusion detection method: an existing intrusion detection model with higher classification performance (OBLR) is used to predict the data in the first stage. In the second stage, for the overlapping data in the confusing data, the method learns the distribution of each feature group according to the randomly divided “intermediary set,” and realizes the prediction of overlapping samples through the prior distribution knowledge, and achieves efficient classification of overlapping samples without increasing the computational burden of the model. For nonoverlapping data in the confusing data, KPCA (kernel principal component analysis) dimension elevation is used in the third stage to capture more detailed difference information between samples, and GMM (Gaussian mixed model) is combined with the “representative samples” proposed in this paper to assist classifier classification. At the same time, all the base classifiers are integrated through LTR (learning to rank) to improve the classification effect of the model for nonoverlapping data in the confusing data. The experimental results show that 99.71% accuracy and 0.158% false positive rate are achieved on the complex intrusion dataset UNSW-NB15, which is better than the existing methods. In particular, this method can increase the accuracy of 38.1% for the confusing samples that cannot be correctly detected by the existing model.},
  archive      = {J_NCA},
  author       = {Pang, He and Jin, Fusheng and Chen, Mengnan and Jiang, Yutong and Yuan, Ye},
  doi          = {10.1007/s00521-024-10903-x},
  journal      = {Neural Computing and Applications},
  month        = {2},
  number       = {6},
  pages        = {4853-4867},
  shortjournal = {Neural Comput. Appl.},
  title        = {A multistage intrusion detection method for alleviating class overlapping problem},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Selection of test samples to improve DNN test efficiency based on neuron clusters. <em>NCA</em>, <em>37</em>(6), 4837-4852. (<a href='https://doi.org/10.1007/s00521-024-10894-9'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep neural networks (DNNs) have complex structures and operation methods, making it difficult to pinpoint the cause of performance degradation. Moreover, DNNs are widely used in various domains, and the significance of their quality is emphasized. In the test process, selecting test samples that cause model misclassification can detect model vulnerabilities at an early stage. Improving test efficiency, defined as the number of misclassified samples compared to the number of selected test samples, can reduce the time and cost required for testing. Therefore, it helps to perform more tests within a limited time and cost. In existing sample selection studies to improve test efficiency, test samples are selected based on coverage. However, samples achieving high coverage do not necessarily guarantee improved test efficiency. This paper proposes a test sample selection method for improving test efficiency based on neuron clusters. The neuron cluster is a simple and effective concept that abstracts DNN’s computational process for human understanding. The most significant contribution of this paper is to improve test efficiency by selecting samples that can be misclassified based on neuron clusters. Through experiments using public datasets and ResNet models, we compare the proposed method with coverage-based test sample selection methods in terms of the test efficiency. Our proposed method is at least 8%p more efficient than the coverage-based test sample selection methods, although it varies somewhat among datasets. We can apply neuron clusters in various ways to DNN testing, including identifying untrained class samples and selecting test samples.},
  archive      = {J_NCA},
  author       = {Lee, Young-Woo and Chae, Heung-Seok},
  doi          = {10.1007/s00521-024-10894-9},
  journal      = {Neural Computing and Applications},
  month        = {2},
  number       = {6},
  pages        = {4837-4852},
  shortjournal = {Neural Comput. Appl.},
  title        = {Selection of test samples to improve DNN test efficiency based on neuron clusters},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Estimation of leaf mg values of apple trees in early period with machine learning methods. <em>NCA</em>, <em>37</em>(6), 4823-4835. (<a href='https://doi.org/10.1007/s00521-024-10871-2'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This study employs machine learning methods to estimate magnesium (Mg) levels in apple leaves at mid-vegetation from early period values. Leaf samples were collected from 150 apple orchards over two years, both at early and mid-vegetation stages. Soil samples were taken at 0–30 cm below the canopy. Leaf analyses included N, P, K, Ca, Mg, Fe, Cu, Mn, Zn, and B, while soil analyses covered pH, salinity, lime, organic matter, sand, clay, and silt percentages. The mid-vegetation Mg value of the leaves was the target variable, and other parameters served as predictors. The top five machine learning models (Nu support vector regressor, random forest regressor, histogram-based gradient boosting regression tree, K-nearest neighbors regressor, Bayesian ridge) were utilized. Using feature selection, it was found that reliable predictions could be made with five out of 17-input variables (Mg21, lime, Ca21, P21, sand). The models achieved R2 values of approximately 0.59 with categorical data and 0.51 without, with corresponding MAE values of 0.03 and 0.04. These results, compared with existing literature, confirm that machine learning is an effective tool for early-stage Mg status prediction in apple trees.},
  archive      = {J_NCA},
  author       = {Uçgun, Kadir and Navruz, Mustafa},
  doi          = {10.1007/s00521-024-10871-2},
  journal      = {Neural Computing and Applications},
  month        = {2},
  number       = {6},
  pages        = {4823-4835},
  shortjournal = {Neural Comput. Appl.},
  title        = {Estimation of leaf mg values of apple trees in early period with machine learning methods},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A mathematical multi-objective optimization model and metaheuristic algorithm for effective advertising in the social internet of things. <em>NCA</em>, <em>37</em>(6), 4797-4821. (<a href='https://doi.org/10.1007/s00521-024-10793-z'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The integration of social networks with the Internet of Things (IoT) has been explored in recent research, giving rise to the Social Internet of Things (SIoT). One promising application of SIoT is viral marketing, which plays a critical role in modern advertising. However, current solutions proposed for viral marketing are often impractical in real-world scenarios, and there has been limited investigation into viral marketing within the context of SIoT. To address this issue, we present a multi-objective integer linear programming model named ILP-ASoT that facilitates advertising through SIoT. Our approach introduces two realistic parameters: the node sociality rate and the node appropriateness rate as an advertising destination. By balancing the cost of selecting a seed set with the number of users who receive the advertisement, our model offers an appropriate trade-off. Since this is proven as an NP-hard problem, A PSO-based metaheuristic algorithm is proposed to provide near-optimal solutions for advertising in SIoT, called PSO-ASoT. Through simulations, we show that our PSO-ASoT algorithm outperforms state-of-the-art algorithms in several metrics, such as influence spread, advertising cost, and duplicated advertising rate.},
  archive      = {J_NCA},
  author       = {Molaei, Reza and Rahsepar Fard, Kheirollah and Bouyer, Asgarali},
  doi          = {10.1007/s00521-024-10793-z},
  journal      = {Neural Computing and Applications},
  month        = {2},
  number       = {6},
  pages        = {4797-4821},
  shortjournal = {Neural Comput. Appl.},
  title        = {A mathematical multi-objective optimization model and metaheuristic algorithm for effective advertising in the social internet of things},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Graph neural networks-based spatiotemporal prediction of photovoltaic power: A comparative study. <em>NCA</em>, <em>37</em>(6), 4769-4795. (<a href='https://doi.org/10.1007/s00521-024-10751-9'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Accurate forecasting of photovoltaic (PV) energy production with high spatiotemporal resolution is important for efficiently integrating renewable energy sources into the power grid. In this paper, we explore the application of graph neural networks (GNNs), specifically Graph Convolutional Networks (GCN), Graph Attention Networks (GAT), and GraphSage, for spatiotemporal PV energy prediction. The GNNs leverage the spatial and temporal dependencies among PV systems by modeling them as signals on a graph, capturing their intricate relationships, and enhancing forecasting accuracy. We investigate the impact of different graph neural network topologies on prediction performance, including distance-based and fully connected graphs. Moreover, we propose a composite model that predicts the PV energy output for any node within the stations’ network, enabling localized and accurate forecasts. The composite model is further extended to handle various prediction horizons ranging from one minute to 30 min ahead. To evaluate the effectiveness of the considered models, data from seven distinct PV systems in Brisbane, Australia, are used to evaluate the prediction performance of the three GNN models. Results demonstrate the effectiveness of graph neural networks in achieving superior forecasting accuracy and underscore their potential in revolutionizing the prediction of PV energy under spatiotemporal constraints, thus contributing to the advancement of renewable energy integration and grid management.},
  archive      = {J_NCA},
  author       = {Abdelkader, Dairi and Fouzi, Harrou and Belkacem, Khaldi and Ying, Sun},
  doi          = {10.1007/s00521-024-10751-9},
  journal      = {Neural Computing and Applications},
  month        = {2},
  number       = {6},
  pages        = {4769-4795},
  shortjournal = {Neural Comput. Appl.},
  title        = {Graph neural networks-based spatiotemporal prediction of photovoltaic power: A comparative study},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). The binary crayfish optimization algorithm with bitwise operator and repair method for 0–1 knapsack problems: An improved model. <em>NCA</em>, <em>37</em>(6), 4733-4767. (<a href='https://doi.org/10.1007/s00521-024-10738-6'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this study, Crayfish Optimization Algorithm (COA) was examined. COA, which simulates crayfish’s summer resort behavior, competition behavior, and foraging behavior. COA is a successful heuristic algorithm originally proposed for continuous optimization problems. In this study, the continuous search space of COA was converted into a binary search space with eight different S- and V-shaped transfer functions. Thus, the Binary COA (BinCOA) algorithm was proposed to the literature. The success of BinCOA variations was analyzed on 25 different knapsack problems of different sizes. The most successful transfer function was determined as BinCOAV1. Since the success of BinCOAV1 fell behind many binary heuristic algorithms in the literature, BinCOA was developed with two different methods (bitwise operator and repair method). Thus, the Improved BinCOA (IBinCOA_RX) algorithm was proposed in this study. BinCOA’s local search ability and discovery ability in the binary search space have been improved. The resulting improved BinCOA variations (IBinCOAX (BinCOA with bitwise operator), IBinCOAR (BinCOA with repair method), and IBinCOA_RX (BinCOA with bitwise operator and repair method)) were analyzed in detail and the effect of each method added to BinCOA was detailed in the paper. The success of IBinCOA_RX has been proven by comparing it with eight different binary heuristic algorithms selected from the literature. According to the results, the IBinCOA_RX algorithm showed preferable success for binary optimization problems. In addition, in this study, the effectiveness of BinCOAV1 and IBinCOAX algorithms is also shown on a different binary problem, namely the uncapacitated facility layout problem (UFLP).},
  archive      = {J_NCA},
  author       = {Bas, Emine and Guner, Lütfi Batuhan},
  doi          = {10.1007/s00521-024-10738-6},
  journal      = {Neural Computing and Applications},
  month        = {2},
  number       = {6},
  pages        = {4733-4767},
  shortjournal = {Neural Comput. Appl.},
  title        = {The binary crayfish optimization algorithm with bitwise operator and repair method for 0–1 knapsack problems: An improved model},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Advanced brain tumor analysis: A novel strategy for segmentation and classification using modern computational methods. <em>NCA</em>, <em>37</em>(6), 4697-4731. (<a href='https://doi.org/10.1007/s00521-024-10629-w'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Despite their low incidence, brain tumors are one of the most invasive cancer types, constituting a significant burden of death and disease in all age groups. Early and accurate diagnosis of brain tumors plays a vital role in reducing mortality rates. The heterogeneous nature of brain tumors and the diversity of tumor lesions may make it difficult for radiologists to make the right decision in the manual diagnosis process. This study proposes the use of machine learning methods for the classification of brain tumors (pituitary, meningioma, and glioma) and the use of metaheuristic algorithms graph theory, and random walker algorithms in the segmentation of brain tumors. The classification performed with the proposed method obtained an overall accuracy rate of 98.33%. In addition, the classification accuracy of 99.50%, 99.50%, 98.67%, and 99.00% was achieved for no tumor, pituitary, meningioma, and glioma, respectively. Experiments in the segmentation process show that metaheuristic algorithms and max-flow graph cut approach produce successful results.},
  archive      = {J_NCA},
  author       = {Reis, Hatice Catal and Turk, Veysel},
  doi          = {10.1007/s00521-024-10629-w},
  journal      = {Neural Computing and Applications},
  month        = {2},
  number       = {6},
  pages        = {4697-4731},
  shortjournal = {Neural Comput. Appl.},
  title        = {Advanced brain tumor analysis: A novel strategy for segmentation and classification using modern computational methods},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). MI-BMPI motor imagery brain–mobile phone dataset and performance evaluation of voting ensembles utilizing QPDM. <em>NCA</em>, <em>37</em>(6), 4679-4696. (<a href='https://doi.org/10.1007/s00521-024-10917-5'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {EEG-based interfaces are an active research area with great potential. We, therefore, focused on classifying motor imaging (MI) tasks from various problem areas. Because of that, we applied MI patterns to voting ensembles differently and constructed voters. They employ quasi-probabilistic distribution models based on sub-classifiers of different frequency bands and time segments. Much previous work focused on just a few MI tasks for BCIs. To that end, we constructed a new mobile EEG dataset, abbreviated as MI-BMPI, containing two significant gestures for mobile phone interfaces. The research experiments used a consumer market EEG system, the mobile wireless Emotiv EPOC Flex neuroheadset. Experiments were carried out on the BCI Competition IV Dataset 2a and MI-BMPI. On the BCI and BMPI datasets, promising results were obtained in classifying various MI tasks. In conclusion, new solutions were introduced for tougher EEG-based interfaces, which have the potential to classify MI tasks and develop EEG-based interfaces. In addition to the average performance improvements, more stable results were achieved for subject and task variations.},
  archive      = {J_NCA},
  author       = {Yilmaz, Cagatay Murat and Yilmaz, Bahar Hatipoglu and Kose, Cemal},
  doi          = {10.1007/s00521-024-10917-5},
  journal      = {Neural Computing and Applications},
  month        = {2},
  number       = {6},
  pages        = {4679-4696},
  shortjournal = {Neural Comput. Appl.},
  title        = {MI-BMPI motor imagery brain–mobile phone dataset and performance evaluation of voting ensembles utilizing QPDM},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). CBIR: A novel identification approach for college students in need based on consumer behavior psychology theory. <em>NCA</em>, <em>37</em>(6), 4663-4677. (<a href='https://doi.org/10.1007/s00521-024-10900-0'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The accurate identification of students in need is crucial for governments and colleges to allocate resources more effectively and enhance social equity and educational fairness. Existing approaches to identifying students in need rely on manual operations that include manually extracting consumption behavior information, statistical consumption characteristics and principal component analysis. However, this issue may lead to low prediction accuracy and inefficiency in identifying students in need. We design a three-stage framework to accurately identify college students in need from the perspective of consumer behavior psychology. The consumption behavior information is first obtained from the student consumption records using the consumption behavior clustering approach. The consumption behavior matrix is then built by extracting consumption and spatiotemporal information in different periods. Finally, a novel consumption behavior identification ResNeSt (CBIR) model is proposed to identify college students in need accurately. The experimental results on real datasets show that the CBIR model has higher prediction accuracy than the baseline models.},
  archive      = {J_NCA},
  author       = {Liu, Xinze and Liu, Shixi and Hu, Xiaojing and Zhang, Yudong and Fang, Xianwen},
  doi          = {10.1007/s00521-024-10900-0},
  journal      = {Neural Computing and Applications},
  month        = {2},
  number       = {6},
  pages        = {4663-4677},
  shortjournal = {Neural Comput. Appl.},
  title        = {CBIR: A novel identification approach for college students in need based on consumer behavior psychology theory},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). MLPFormer: MLP-integrated transformer for colorectal histopathology whole slide image segmentation. <em>NCA</em>, <em>37</em>(6), 4651-4661. (<a href='https://doi.org/10.1007/s00521-024-10884-x'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Colorectal intraepithelial neoplasia is a precancerous lesion of colorectal cancer, which is mainly diagnosed using pathological images. According to the characteristics of lesions, precancerous lesions can be classified into four different grades, i.e., background, normal, low and high level. However, due to the continuity of lesion development, tissue in different stages has high similarity. Recently, visual transformers have achieved impressive results in different visual tasks, due to their capacity of capturing long-range dependencies. However, due to computational cost, transformer cannot well extract detailed feature, which is important for lesion grading. To accurately segment pathological tissues with high similarity, in this work, we embed the multi-head MLP mixer module in the transformer network to extract cell and edge information. Furthermore, we propose a hierarchical MLP decoder to facilitate multi-scale feature fusion. The proposed transformer, namely MLPFormer, achieves remarkable results against competitive baselines on the Histo-CRC Biopsy dataset. The experimental results demonstrate that MLPFormer significantly outperforms the competitive baselines, i.e., a 3% dice improvement is achieved over the SegFormer series.},
  archive      = {J_NCA},
  author       = {Wang, Yuxuan and Li, Dan and Li, Xuechen and Guo, Yan and Zuo, Yanfei and Shen, Linlin},
  doi          = {10.1007/s00521-024-10884-x},
  journal      = {Neural Computing and Applications},
  month        = {2},
  number       = {6},
  pages        = {4651-4661},
  shortjournal = {Neural Comput. Appl.},
  title        = {MLPFormer: MLP-integrated transformer for colorectal histopathology whole slide image segmentation},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Accurate breast cancer diagnosis strategy (BCDS) based on deep learning techniques. <em>NCA</em>, <em>37</em>(6), 4617-4650. (<a href='https://doi.org/10.1007/s00521-024-10849-0'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Breast cancer (BC) survival rates and the patient's quality of life are boosted by early detection and timely therapy. It is the most prominent cancer and the primary trigger for deaths due to cancer in women around the world. As a result, a variety of artificial intelligence-based computer-assisted procedures are being included in the conventional diagnostic workflow. This study proposes an accurate Breast Cancer Diagnosis Strategy (BCDS) based on deep learning techniques. A framework for BCDS will be presented to consolidate and improve BC detection by defining three stages of BCDS: (i) Preprocessing Stage (PS), (ii) Classification Stage (CS), and (iii) Ensemble Voting Stage (EVS). In PS, three preprocessing operations which are image resizing using bilinear interpolation, data augmentation using Conditional- Convolutional Generative Adversarial Network (C-DCGAN) with Adversarial Feedback Loop (AFL) and data enhancement using Multiscale Retinex with Color Restoration (MSRCR) algorithm will be performed to enhance images and increase the performance of diagnostic model. In CS, an ensemble learning-based technique that includes three classifiers called Xception, Inception-ResNet-V2, and Visual Geometry Group (VGG16) will be applied to accurately diagnose BC patients. Finally, in EVS, majority voting and weighted random forest based on accurate voting techniques will be provided to get the most optimal diagnosis. In the benchmark BreakHis dataset, test results illustrated that the three fine-tuned classifiers (Xception, Inception-ResNet-V2, and VGG16) of BCDS provide accuracy values equal 97%, 98%, and 99.28% for multi-classification. These fine-tuned classifiers yield accuracy scores of 99%, 99%, and 100% based on binary jobs. Results indicate that the BCDS model achieves 100% accuracy for binary tasks and 99.89% accuracy for multi-classification tasks. Physicians can utilize BCDS as a decision-support framework, especially in nations of poverty when resources and knowledge are a handful. Early and accurate identification of the tumor's type lessens the possibility of a botched treatment and lowers the death rate from tumors in the breast.},
  archive      = {J_NCA},
  author       = {Ibrahim, Taghreed S. and Saraya, M. S. and Saleh, Ahmed I. and Rabie, Asmaa H.},
  doi          = {10.1007/s00521-024-10849-0},
  journal      = {Neural Computing and Applications},
  month        = {2},
  number       = {6},
  pages        = {4617-4650},
  shortjournal = {Neural Comput. Appl.},
  title        = {Accurate breast cancer diagnosis strategy (BCDS) based on deep learning techniques},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Comparative analysis and statistical performance evaluation of fractional PID controllers in fuel cell choppers optimized with metaheuristic algorithms. <em>NCA</em>, <em>37</em>(6), 4585-4615. (<a href='https://doi.org/10.1007/s00521-024-10909-5'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Fuel cells (FCs) are significant renewable energy sources that produce electrical energy, similar to photovoltaic solar panels and wind turbines. These cells use hydrogen as a fuel and produce pure water as waste, making them potential alternatives to batteries. This paper compares the parameters of a buck-type converter designed for use in fuel cell (FC) energy production systems. These parameters were optimized using five optimization algorithms. To ensure high quality, the converters that adjust the desired current and voltage values on the load must be controlled optimally. In this context, fractional-order PID (FOPID) controllers, which are advanced versions of standard PID controllers, were used to adjust the voltage of a load to its desired reference value. Five parameters (KP, KI, KD, $$\mu$$ , and $$\lambda$$ ) of this controller were adjusted using various algorithms: artificial bee colony (ABC) optimization, atom search optimization (ASO), Henry gas solubility optimization (HGSO), the JAYA algorithm, and the sine cosine algorithm (SCA) optimization. Among these algorithms, the HGSO algorithm demonstrated the best results. However, after a comprehensive statistical analysis, different insights emerged. The findings are then presented in a comparative analysis. Simulation studies were performed in the MATLAB/Simulink environment and the coding sections of MATLAB. The system under study comprises an FC, a converter, and loads. The responsiveness of the controller to variables and disturbances was examined by continuously adjusting both the reference value and load. The results derived from these calculations provide a detailed analysis of the controller’s ability to ensure that the voltage on the load aligns with the reference values.},
  archive      = {J_NCA},
  author       = {Güven, Aykut Fatih and Mengi, Onur Özdal},
  doi          = {10.1007/s00521-024-10909-5},
  journal      = {Neural Computing and Applications},
  month        = {2},
  number       = {6},
  pages        = {4585-4615},
  shortjournal = {Neural Comput. Appl.},
  title        = {Comparative analysis and statistical performance evaluation of fractional PID controllers in fuel cell choppers optimized with metaheuristic algorithms},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Enhancing waste sorting and recycling efficiency: Robust deep learning-based approach for classification and detection. <em>NCA</em>, <em>37</em>(6), 4567-4583. (<a href='https://doi.org/10.1007/s00521-024-10855-2'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Given the severity of waste pollution as a major environmental concern, intelligent and sustainable waste management is becoming increasingly crucial in both developed and developing countries. The material composition and volume of urban solid waste are key considerations in processing, managing, and utilizing city waste. Deep learning technologies have emerged as viable solutions to address waste management issues by reducing labor costs and automating complex tasks. However, the limited number of trash image categories and the inadequacy of existing datasets have constrained the proper evaluation of machine learning model performance across a large number of waste classes. In this paper, we present robust waste image classification and object detection studies using deep learning models, utilizing 28 distinct recyclable categories of waste images comprising a total of 10,406 images. For the waste classification task, we proposed a novel dual-stream network that outperformed several state-of-the-art models, achieving an overall classification accuracy of 83.11%. Additionally, we introduced the GELAN-E (generalized efficient layer aggregation network) model for waste object detection tasks, obtaining a mean average precision (mAP50) of 63%, surpassing other state-of-the-art detection models. These advancements demonstrate significant progress in the field of intelligent waste management, paving the way for more efficient and effective solutions.},
  archive      = {J_NCA},
  author       = {Sayem, Faizul Rakib and Islam, Md. Sakib Bin and Naznine, Mansura and Nashbat, Mohammad and Hasan-Zia, Mazhar and Kunju, Ali K Ansaruddin and Khandakar, Amith and Ashraf, Azad and Majid, Molla Ehsanul and Kashem, Saad Bin Abul and Chowdhury, Muhammad E. H.},
  doi          = {10.1007/s00521-024-10855-2},
  journal      = {Neural Computing and Applications},
  month        = {2},
  number       = {6},
  pages        = {4567-4583},
  shortjournal = {Neural Comput. Appl.},
  title        = {Enhancing waste sorting and recycling efficiency: Robust deep learning-based approach for classification and detection},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). An explainable lightweight parallel depth-wise separable model for lung infection detection from chest X-rays. <em>NCA</em>, <em>37</em>(6), 4545-4566. (<a href='https://doi.org/10.1007/s00521-024-10854-3'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Respiratory diseases including severe acute respiratory syndrome (SARS), Middle East respiratory syndrome (MERS), HIV, influenza A (H1N1), and COVID-19 can cause lung infections. Among these, COVID-19 has been considered as an epidemic in the past few years. According to the World Health Organization (WHO), about 2.7 million deaths occurred due to lung infection caused by COVID-19. Therefore, most of the countries implemented full or partial lockdown measures to slow down the spread of the disease. However, early detection of viral infection is crucial to reduce the spreading as well as minimizing the pandemic situation. Common lung infection detection methods include X-ray image analysis, reverse transcription–polymerase chain reaction (RT-PCR), and computed tomography (CT) scans. RT-PCR and CT scans have limitations such as being error-prone, slow, and requiring specialized machinery. CT scans are expensive and expose patients to radiation. Similarly, manual assessment of X-ray images is challenging due to the limited availability of skilled medical professionals. To mitigate this problem, convolutional neural networks (CNNs) have shown potential in automating the detection of lung infections from X-ray images. However, the traditional CNN models have a large number of parameters and require high computational resources. To address these issues, this research proposes a novel parallel lightweight diagnosis model based on depth-wise separable CNN (LW-PDS-CovidNet). Additionally, the proposed model’s explainability is enhanced by utilizing Shapley additive explanations (SHAP) and gradient-weighted class activation mapping (GRAD CAM) to highlight the most important features. The proposed LW-PDS-CovidNet approach was tested on real chest X-ray images to predict them as normal, COVID-19, viral pneumonia, and lung opacity. Experimental outcomes demonstrate that the proposed method outperforms the baseline methods and achieved an accuracy of 98.06, 97.43, and 92% for two-class, three-class, and four-class classification respectively with a low number of parameters. Notably, the proposed model achieves these results with only 0.53 million parameters, whereas traditional CNN models require nearly 2.7 million parameters. This ability of the proposed technique makes it suitable for low-computing devices. Therefore, it is capable of significantly reducing cost and memory requirements for computing resources.},
  archive      = {J_NCA},
  author       = {Kibria, Hafsa Binte and Hossain, Md Ali and Rehman, Shazia and Alahakoon, Damminda and Rahman, Md Anisur},
  doi          = {10.1007/s00521-024-10854-3},
  journal      = {Neural Computing and Applications},
  month        = {2},
  number       = {6},
  pages        = {4545-4566},
  shortjournal = {Neural Comput. Appl.},
  title        = {An explainable lightweight parallel depth-wise separable model for lung infection detection from chest X-rays},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Performance evaluation of different deep learning models used for the purpose of healthy and diseased leaves classification of cherimoya (Annona cherimola) plant. <em>NCA</em>, <em>37</em>(6), 4531-4544. (<a href='https://doi.org/10.1007/s00521-024-10830-x'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Controlling plant leaves disease helps in upholding their health. This augments the overall strength of the plant productivity both in terms of higher quality and quantity. Recently, with the expansion of high-end computing devices, Artificial Intelligence (AI) techniques have allied in almost all applications. In agriculture, AI has been used in crop surveillance, soil health monitoring, nutrient deficiency estimation, flower and fruits quality assessment, weed estimation, and crop health diagnosis among all. Therefore, in this work, five deep learning approaches, namely EfficientNet, MobileNetV2, BiT, EANet, and Swin Transformers, have been used for the classification of healthy and diseased leaves of Cherimoya plant. EANet model with precision = 0.9929%, recall = 0.9536%, F1-score = 0.9846%, training accuracy = 0.9775%, and testing accuracy = 0.9689% achieved superior performance among all. We believe that the proposed work can be very useful in providing an accurate and timely plant disease diagnosing system.},
  archive      = {J_NCA},
  author       = {Chouhan, Siddharth Singh and Singh, Uday Pratap and Jain, Sanjeev},
  doi          = {10.1007/s00521-024-10830-x},
  journal      = {Neural Computing and Applications},
  month        = {2},
  number       = {6},
  pages        = {4531-4544},
  shortjournal = {Neural Comput. Appl.},
  title        = {Performance evaluation of different deep learning models used for the purpose of healthy and diseased leaves classification of cherimoya (Annona cherimola) plant},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). AECT-GAN: Reconstructing CT from biplane radiographs using auto-encoding generative adversarial networks. <em>NCA</em>, <em>37</em>(6), 4511-4530. (<a href='https://doi.org/10.1007/s00521-024-10690-5'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cormputed tomography (CT) scanning is an effective medical imaging modality widely used in clinical medicine for diagnosing various conditions. CT can generate three-dimensional images, thus providing more information than traditional two-dimensional radiographs. However, this comes at a cost, as it involves higher radiation, increased expense, and more time consumption. With the advancement of artificial intelligence, particularly the rise of computer vision, researchers have explored using deep convolutional neural networks and generative adversarial networks for low-radiation CT reconstruction tasks. Yet, existing CT reconstruction methods from X-ray images often focus on pixel-level difference metrics, neglecting the perceptual differences considered by the human visual system, especially in accurately restoring internal details in medical images. In response to this, this paper introduces the auto-encoder-based generative adversarial network (AECT-GAN) model, which integrates an auto-encoder structure and Sobel Gradient Guidance (SGG) mechanism within the discriminator, aiming to enhance the fidelity of image detail reproduction. Experimental validation on the LIDC-IDRI lung CT dataset has demonstrated our AECT-GAN method’s superiority in qualitative and quantitative evaluations, notably achieving significant improvements in preserving fine contours and textures in reconstructed images. Furthermore, applying this model to the IXI brain MRI dataset conclusively proves its widespread applicability and outstanding performance in the medical imaging domain.},
  archive      = {J_NCA},
  author       = {Cheng, Shuangqin and Chen, Qingliang and Zhang, Qiyi and Li, Ming and Alike, Yamuhanmode and Su, Kaile and Wen, Pengcheng},
  doi          = {10.1007/s00521-024-10690-5},
  journal      = {Neural Computing and Applications},
  month        = {2},
  number       = {6},
  pages        = {4511-4530},
  shortjournal = {Neural Comput. Appl.},
  title        = {AECT-GAN: Reconstructing CT from biplane radiographs using auto-encoding generative adversarial networks},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Few-shot remaining useful life prognostics through auxiliary training with related dataset. <em>NCA</em>, <em>37</em>(6), 4493-4509. (<a href='https://doi.org/10.1007/s00521-024-10431-8'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Predicting the remaining useful life of equipment can help organizations improve efficiency, reduce costs and enhance safety by enabling them to plan maintenance and repairs, and optimize their use of equipment. A major challenge in this field is the development of accurate deep learning models, particularly when data is limited to a few run-to-failure trajectories. Traditional methods, such as pre-training on a larger dataset followed by fine-tuning on the main data, often fall short under these constraints. To address this challenge, we propose an auxiliary training approach that integrates auxiliary objectives from related but distinct datasets. This approach enriches the learning process, utilizing knowledge from a broader data range and acting as a regularization mechanism to improve generalization from limited data. The effectiveness of the proposed method is demonstrated by experiments on two well-known public datasets, CMAPSS and N-CMAPSS, across eight distinct settings, and is shown to outperform state-of-the-art approaches such as single-task learning and pre-training followed by fine-tuning.},
  archive      = {J_NCA},
  author       = {Chaoub, Alaaeddine and Voisin, Alexandre and Cerisara, Christophe and Iung, Benoît},
  doi          = {10.1007/s00521-024-10431-8},
  journal      = {Neural Computing and Applications},
  month        = {2},
  number       = {6},
  pages        = {4493-4509},
  shortjournal = {Neural Comput. Appl.},
  title        = {Few-shot remaining useful life prognostics through auxiliary training with related dataset},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Eigen time series modeling: A breakthrough approach to spatio-temporal rainfall forecasting in basins. <em>NCA</em>, <em>37</em>(6), 4471-4492. (<a href='https://doi.org/10.1007/s00521-024-10864-1'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Rainfall is crucial for understanding local climate systems and their distribution patterns in watersheds. Accurate regional rainfall forecasting is vital for flood and drought mitigation efforts. However, understanding the deterministic and stochastic properties of rainfall data is essential before using it in prediction models due to its chaotic nature. This study introduces the use of eigen time series to provide a compact and comprehensive representation of rainfall data across all stations within a watershed. By identifying key patterns, extracting relevant features, and reducing dimensionality, the eigen time series method enables the entire basin to be effectively modeled with a single dataset. Additionally, the prediction models in this approach utilize a dual-layer stacking ensemble framework, which enhances both the precision and reliability of the rainfall predictions. The study presents an innovative methodology by both deriving the eigen time series representation and integrating the prediction models based on these eigen time series into a stacking ensemble model. The results of this study are evaluated based on diagnostic metrics mean squared error (MSE), Nash–Sutcliffe efficiency coefficient (CE), Wilmott's refined index (WI), and mean absolute error (MAE). The introduced methodological approach exhibits exceptional performance in forecasting rainfall data for 40 stations solely based on eigen rainfall time series, surpassing a CE value of 0.95. The utilization of the eigen time series methodology alongside the stacking ensemble prediction model highlights their capacity as robust instruments, not only for addressing rainfall-related challenges but also for prospective applications within the engineering and scientific domains. These approaches exhibit promising capabilities in predicting future spatial–temporal patterns without being reliant on a priori assumptions.},
  archive      = {J_NCA},
  author       = {Küllahcı, Kübra and Altunkaynak, Abdüsselam},
  doi          = {10.1007/s00521-024-10864-1},
  journal      = {Neural Computing and Applications},
  month        = {2},
  number       = {6},
  pages        = {4471-4492},
  shortjournal = {Neural Comput. Appl.},
  title        = {Eigen time series modeling: A breakthrough approach to spatio-temporal rainfall forecasting in basins},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Author profiling from romanized urdu text using transfer learning models. <em>NCA</em>, <em>37</em>(6), 4455-4470. (<a href='https://doi.org/10.1007/s00521-024-10857-0'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This research concentrates on author profiling using transfer learning models for classifying age and gender. The investigation encompassed a diverse set of transfer learning techniques, including Roberta, BERT, ALBERT, Distil BERT, Distil Roberta, ELECTRA, and XLNet. Through meticulous evaluation using metrics such as the Matthews Correlation Coefficient, Accuracy, Precision, Recall, and F1 Score, the study examined the efficacy of these models. The curated dataset was divided for gender and age tasks, resulting in robust gender prediction with the XLNet model and age prediction with the BERT model. Notably, the XLNet model achieved the highest MCC (0.7946), Accuracy (0.8957), Precision (0.8992), Recall (0.8957), and F1 Score (0.8958) values in gender classification, while the BERT model excelled in age prediction with an MCC of (0.7338), Accuracy of (0.8220), Precision of (0.8324), Recall of (0.8220), and F1 Score of (0.8243). Visualized outcomes provide valuable insights into the model’s performance nuances, paving the way for their practical implementation. This research offers novel contributions to author profiling tasks, bridging the gap between theory and real-world applications.},
  archive      = {J_NCA},
  author       = {Ali, Abid and khan, Muhammad Sohail and Khan, Muhammad Amin and Khan, Sajid Ullah and Khan, Faheem},
  doi          = {10.1007/s00521-024-10857-0},
  journal      = {Neural Computing and Applications},
  month        = {2},
  number       = {6},
  pages        = {4455-4470},
  shortjournal = {Neural Comput. Appl.},
  title        = {Author profiling from romanized urdu text using transfer learning models},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). DCNN-RFF-NFC: A novel design of NFC security using deep convolution neural network-based RF fingerprinting. <em>NCA</em>, <em>37</em>(6), 4439-4453. (<a href='https://doi.org/10.1007/s00521-024-10861-4'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To avoid tag misuse, especially desire Near-Field Communication (NFC) tag authentication. The most current fingerprinting approaches employ the physical-layer signal, which embeds the tag hardware faults for authentication. Nevertheless, the poor scalability of current NFC fingerprinting techniques for a large number of tags or their incompatibility with NFC protocols impedes the practical implementation of NFC authentication systems. Radiofrequency (RF) fingerprinting verifies by using distinct and random distortions in a received signal that are dictated by the transmitter’s attributes. Here, introduce a new deep learning-based RF fingerprinting system authentication technique. Although the accuracy of radio fingerprinting has been increased by the use of neural networks, there are two main causes behind this research. First, for pre-processing over radiofrequency signals, there are not many useful parameter possibilities. Second, in the case of crossing days, radio fingerprinting based on deep learning performs badly concerning temporal variations. Deep convolutional neural network designs can recognize devices in a broad range of realistic circumstances, including channel shifts, noise levels, training data quantities, and processing overheads, using both raw and processed IQ samples as input. Using an RF signal that corresponds to a one-bit transmission, the RF properties of the NFC tag are extracted to facilitate fast identification. Convolutional, recurrent, and fully connected layer-based neural networks are the three types of deep neural networks that are employed. Here, confirm through testing that deep learning-based algorithms are capable of uniquely identifying up to 98% of NFC tags.},
  archive      = {J_NCA},
  author       = {Adigopula, Sritulasi and Subramanyam, M. V.},
  doi          = {10.1007/s00521-024-10861-4},
  journal      = {Neural Computing and Applications},
  month        = {2},
  number       = {6},
  pages        = {4439-4453},
  shortjournal = {Neural Comput. Appl.},
  title        = {DCNN-RFF-NFC: A novel design of NFC security using deep convolution neural network-based RF fingerprinting},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A new approach for estimating water saturation in low-resistivity hydrocarbon-bearing reservoirs using artificial neural network (ANN). <em>NCA</em>, <em>37</em>(6), 4409-4437. (<a href='https://doi.org/10.1007/s00521-024-10777-z'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The low-resistivity pay zone phenomenon is among the main challenges in the oil industry, where the hydrocarbon-bearing pay zone shows low-resistivity values instead of declaring high-resistivity values. It was first noticed in the Algerian Southern Field, and several studies have been conducted to correct the underestimation of the hydrocarbon volume obtained from the petrophysical logs, specifically resistivity logs, whereas it has been established that heavy and electrically conductive minerals are the primary cause of the underestimated results. Furthermore, in previous literature, some researchers suggested using a modified Simandoux equation with compensating terms. Nonetheless, these terms are added empirically with no practical background, making generalizations on similar cases in the same field complicated and of low reliability. This study’s main goal is to design a modified model for estimating water saturation (Sw) in low-resistivity reservoirs based on a trained artificial intelligence algorithm. This main contribution of this paper is that the developed algorithm is based on combining the artificial neural network-derived model with a support vector machine technique (ANNSVM). The estimated water saturation values using this algorithm are in full agreement and better matched with the field measurements derived using the drill stem tests and the modular dynamic tester, indicating its applicability and accuracy to estimate the Sw in this kind of challenging reservoir. Moreover, in comparison to the conventional water saturation models, the developed ANNSVM model enables predicting more reliable data corresponding to real data. Therefore, this proposed algorithm could be successfully applied to accurately estimate the Sw in other low-resistivity reservoirs in the Algerian basins and worldwide.},
  archive      = {J_NCA},
  author       = {Doghmane, Mohamed Zinelabidine and Ouadfeul, Sid Ali and Eladj, Said and Dejzzar, Sofiane and Nabawy, Bassem S. and Benabid, Mouna Keltoum and Bennani, Roufeida},
  doi          = {10.1007/s00521-024-10777-z},
  journal      = {Neural Computing and Applications},
  month        = {2},
  number       = {6},
  pages        = {4409-4437},
  shortjournal = {Neural Comput. Appl.},
  title        = {A new approach for estimating water saturation in low-resistivity hydrocarbon-bearing reservoirs using artificial neural network (ANN)},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Point cloud classification and part segmentation of steel structure elements. <em>NCA</em>, <em>37</em>(6), 4387-4407. (<a href='https://doi.org/10.1007/s00521-024-10733-x'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The classification and part segmentation of point clouds have gained significant attention in the field of artificial intelligence (AI), especially in the construction industry. However, addressing the dataset directly in AI models remains a challenge, as most existing methods are not well-suited for processing point cloud data. PointNet has emerged as an AI architecture algorithm. It transforms individual points independently to learn local and global features. This research aims to develop a comprehensive framework for classification and part segmentation for point clouds of steel structure elements. The framework enhances the accuracy of the PointNet algorithm, and it consists of three stages: (1) dataset creation; (2) model classification; and (3) part segmentation. First, the dataset creation procedure encompasses modeling steel columns, beams, and braces using Autodesk Revit software. For the classification dataset, a dataset comprising 580 columns and 920 beams is obtained. In the part segmentation dataset, five categories of steel braced frame elements are generated, yielding a total of 21,870 elements for braced frame structures. Several point cloud experiments have been applied, including adjusting the number of points in the point cloud, altering the batch size, and fine-tuning the number of epochs. These experimental settings were systematically investigated to identify the optimal combination that yields the highest (AI) model accuracy. PointNet model achieved 100% accuracy across all classification experiments, while part segmentation experiments reached up to 97.10% accuracy, with a mean intersection over union (MIOU) of 93.70%. The comprehensive analysis of the point cloud dataset is applied on an actual case study to demonstrate the practical features of the proposed research.},
  archive      = {J_NCA},
  author       = {Daif, Hassan and Marzouk, Mohamed},
  doi          = {10.1007/s00521-024-10733-x},
  journal      = {Neural Computing and Applications},
  month        = {2},
  number       = {6},
  pages        = {4387-4407},
  shortjournal = {Neural Comput. Appl.},
  title        = {Point cloud classification and part segmentation of steel structure elements},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). BERTweet.BR: A pre-trained language model for tweets in portuguese. <em>NCA</em>, <em>37</em>(6), 4363-4385. (<a href='https://doi.org/10.1007/s00521-024-10711-3'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent advancements in neural language models have been primarily centered around English, with limited focus on the more than seven thousand other languages. This includes Portuguese, which, despite being the sixth most spoken language globally, has markedly fewer neural-based linguistic resources available when compared to English. Notably, Portuguese speakers compose one of the most active groups of Twitter users; however, no pre-trained language model in Portuguese tweets is extensively studied in the literature. Besides the language, tweets-based pre-trained models must account for the cultural code, informal linguistic style, code-switching, and the limited number of characters. This manuscript tackles this gap by introducing BERTweet.BR, the first public large-scale pre-trained model specific to the Brazilian Portuguese tweets domain. BERTweet.BR has the same architecture of BERTweetbase, a BERT-based English-tweets model, and was trained from scratch following the RoBERTa pre-training procedure on a 100-M Portuguese tweets corpus. On the sentiment analysis task, experiments show that BERTweet.BR outperforms three multilingual Transformers and BERTimbau, a monolingual general-domain Brazilian Portuguese language model. We release our model in the transformers library aiming at promoting future research in analytical tasks for Portuguese tweets. The BERTweet.BR code, experimental results, and related documentation are publicly available on Github.},
  archive      = {J_NCA},
  author       = {Carneiro, Fernando and Vianna, Daniela and Carvalho, Jonnathan and Plastino, Alexandre and Paes, Aline},
  doi          = {10.1007/s00521-024-10711-3},
  journal      = {Neural Computing and Applications},
  month        = {2},
  number       = {6},
  pages        = {4363-4385},
  shortjournal = {Neural Comput. Appl.},
  title        = {BERTweet.BR: A pre-trained language model for tweets in portuguese},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). PP-CNN: Probabilistic pooling CNN for enhanced image classification. <em>NCA</em>, <em>37</em>(6), 4345-4361. (<a href='https://doi.org/10.1007/s00521-024-10862-3'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This study introduces a novel probabilistic pooling convolutional neural network (PP-CNN) classifier designed to enhance image classification. The PP-CNN integrates probabilistic outputs from three distinct architectures utilizing MaxPooling, MinPooling, and MaxMinPooling layers. By averaging these probabilities, the model achieves improved accuracy compared to individual models. Notably, the proposed CNN model employs Positive ReLU activation with MaxPooling, Negative ReLU activation with MinPooling, and both Positive and Negative ReLU activations with MaxMinPooling. This strategy ensures the retention of both positive and negative relevant features, enhancing the classification performance by capturing a broader range of critical information. The proposed model has been comprehensively evaluated for its generalizability on four diverse datasets: CIFAR-10, CIFAR-100, CT scan, and X-ray images. Experimental results demonstrate a consistent improvement in classification accuracy across all datasets, highlighting the versatility and effectiveness of the proposed model. The proposed model applies to various image classification tasks, specifically illustrating its utility by detecting COVID-19 from medical images. This work presents the design, implementation, and performance evaluation of the proposed model, underscoring its potential to significantly improve image classification and diagnostic accuracy in medical imaging applications.},
  archive      = {J_NCA},
  author       = {Mishra, Narendra Kumar and Singh, Pushpendra and Gupta, Anubha and Joshi, Shiv Dutt},
  doi          = {10.1007/s00521-024-10862-3},
  journal      = {Neural Computing and Applications},
  month        = {2},
  number       = {6},
  pages        = {4345-4361},
  shortjournal = {Neural Comput. Appl.},
  title        = {PP-CNN: Probabilistic pooling CNN for enhanced image classification},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Evaluating the adversarial robustness of arabic spam classifiers. <em>NCA</em>, <em>37</em>(6), 4323-4343. (<a href='https://doi.org/10.1007/s00521-024-10778-y'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Several studies have exposed the vulnerability of Natural Language Processing (NLP) models to adversarial attacks, which are inputs crafted by attackers to deceive NLP models. Adversarial robustness measures the performance of these systems under such attacks. In the Arabic literature, very limited contributions exist in the field of NLP adversarial robustness. Hence, this study focuses on examining the adversarial robustness of an Arabic NLP model, especially in a classic black-box spam evasion scenario. This work introduces eight diverse adversarial attacks (character, word, sentence, and multi-level) against Arabic NLP models. Moreover, we employ local post-hoc explanations such as SHapely Additive exPlanations (SHAP) to optimize the attacks strategies. Despite the excellent unfortified model’s accuracy of 99.4%, three of the attacks reduced the accuracy by more than 90%. This work also employs local post-hoc explanations such as SHapely Additive exPlanations (SHAP) Nevertheless, the proposed adversarial attacks were effective in terms of maintaining high semantic similarity and low perturbation distance.},
  archive      = {J_NCA},
  author       = {Alajmi, Anwar and Ahmad, Imtiaz and Mohammed, Ameer},
  doi          = {10.1007/s00521-024-10778-y},
  journal      = {Neural Computing and Applications},
  month        = {2},
  number       = {6},
  pages        = {4323-4343},
  shortjournal = {Neural Comput. Appl.},
  title        = {Evaluating the adversarial robustness of arabic spam classifiers},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Drug–target interactions prediction based on similarity graph features extraction and deep learning. <em>NCA</em>, <em>37</em>(6), 4303-4322. (<a href='https://doi.org/10.1007/s00521-024-10714-0'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Identifying drug–target interactions (DTIs) is a critical step in both drug repositioning. The labor-intensive, time-consuming, and costly nature of classic DTI laboratory studies makes it imperative to create efficient computer algorithms to forecast possible DTIs. However, current computational approaches that predict potential drug–target interactions (DTIs) suffer from some limitations, like finding the best similarity measures or negative samples, and thus require substantial performance improvement. This study proposes an integrated approach based on feature representation and deep learning to predict DTIs. We extract the relevant features of drugs and proteins from heterogeneous networks using graph mining techniques. The proposed approach constructs a heterogeneous graph from the known drug–protein interactions, protein–protein, and drug–drug similarities. Then applying two feature extraction techniques to extract the features, then utilizing these features in training a deep learning model to predict the potential DTIs. Also, a novel algorithm is proposed to find the negative samples based on the drug and protein similarity matrices. Four Benchmark datasets are used to evaluate the proposed approach. Our approach achieves the highest AUC (area under the ROC curve) across all datasets (0.98) with around 2% increases over the existing methods. Experimental results demonstrate that our proposed approach outperforms the baseline methods in predicting DTI, and our negative sample-identifying algorithm could be established as a competitive solution.},
  archive      = {J_NCA},
  author       = {Torkey, Hanaa and El-Behery, Heba and Attia, Abdel-Fattah and El-Fishawy, Nawal},
  doi          = {10.1007/s00521-024-10714-0},
  journal      = {Neural Computing and Applications},
  month        = {2},
  number       = {6},
  pages        = {4303-4322},
  shortjournal = {Neural Comput. Appl.},
  title        = {Drug–target interactions prediction based on similarity graph features extraction and deep learning},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Robot failure mode prediction with deep learning sequence models. <em>NCA</em>, <em>37</em>(6), 4291-4302. (<a href='https://doi.org/10.1007/s00521-024-10856-1'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Detecting and preventing an impending robot grasp failure can prevent object damage during robotic manipulation. When a failure is predicted, a human can opt for teleoperation rather than automation of the grasping task. The operator can also intervene to stop the robot from adjusting, such as tightening the grasp or reducing the speed. In this paper, we propose new Machine Learning models for failure prediction based on recurrent neural networks, introducing two innovative approaches: Generative-interpretive grasp prediction (GIGP), which emphasizes both the predictive and interpretive aspects of the model, and adaptive time to failure analysis (ATFA), which also predicts the time to failure. GIGP, can perform an “early” failure prediction, which is not possible with existing models. Both GIGP and ATFA perform competitively or outperform state of the art methods while having the additional desired feature of being interpretable. We evaluate our methods on the problem of early failure prediction in robot grasping. We reach competitive results exceeding 80% in predictive accuracy and agenerative model and full sequence for discriminative modelrea under the ROC curve for the tasks of failure prediction after a limited number of observations of the grasper while being able to make interpretable predictions.},
  archive      = {J_NCA},
  author       = {Damak, Khalil and Boujelbene, Mariem and Acun, Cagla and Alvanpour, Aneseh and Das, Sumit K. and Popa, Dan O. and Nasraoui, Olfa},
  doi          = {10.1007/s00521-024-10856-1},
  journal      = {Neural Computing and Applications},
  month        = {2},
  number       = {6},
  pages        = {4291-4302},
  shortjournal = {Neural Comput. Appl.},
  title        = {Robot failure mode prediction with deep learning sequence models},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Stability-constrained reinforcement learning for level control of nonlinear coupled tank system: An experimental study. <em>NCA</em>, <em>37</em>(6), 4277-4290. (<a href='https://doi.org/10.1007/s00521-024-10852-5'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Neural network (NN) control systems face significant challenges in the theoretical examination of closed-loop stability, despite their success. This paper presents a reinforcement learning (RL) technique with stability constraints for training NN controllers (NNCs) for level control of nonlinear coupled tank systems. The RL adopted in this paper is the deep deterministic policy gradient (DDPG) algorithm. By describing the system in a linear-time-invariant interval system form and utilizing the vertex matrices of the system matrix, a set of Lyapunov-based stability conditions is derived and then employed as training constraints in the DDPG. In each training step, the training constraints are satisfied by utilizing a gradient projection technique. A Lyapunov function, as a result of training, certifies the stability. Simulation and experiment results demonstrate that the constrained NNC works effectively.},
  archive      = {J_NCA},
  author       = {Phothongkum, Kraisak and Kuntanapreeda, Suwat},
  doi          = {10.1007/s00521-024-10852-5},
  journal      = {Neural Computing and Applications},
  month        = {2},
  number       = {6},
  pages        = {4277-4290},
  shortjournal = {Neural Comput. Appl.},
  title        = {Stability-constrained reinforcement learning for level control of nonlinear coupled tank system: An experimental study},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Machine learning and interactive GUI for estimating roller length of hydraulic jumps. <em>NCA</em>, <em>37</em>(6), 4247-4276. (<a href='https://doi.org/10.1007/s00521-024-10846-3'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Hydraulic jumps reduce kinetic energy after ogee spillways, improve wastewater chlorination, and serve many hydraulic applications. This study utilized seven Machine Learning (ML) models: Artificial Neural Network (ANN), Gene Expression Programming (GEP), Random Forest (RF), Adaptive Boosting (AdaBoost), Extreme Gradient Boosting (XGBoost), Light Gradient Boosting Machine (LightGBM), and Categorical Gradient Boosting (CatBoost) to predict the roller length of a hydraulic jump (RLHJ) over a rough bed channel. Firstly, 367 experimental datasets were gathered from a previous research database, allocating 70% and 30% for the training and testing stages. Then, these datasets were assessed for descriptive statistics and correlation analysis. Three dimensionless inputs were considered: upstream Froude number (Fr1), relative HJ depths (h2/h1), and relative bed roughness to the HJ initial depth (ks/h1). Relative roller length to the HJ initial depth (Lr/h1) was the output from the models. The hyperparameters of the models were optimized using the Bayesian optimization technique, with fivefold cross-validation ensuring high performance. Comprehensive analyses were employed to validate model effectiveness, including visual and quantitative methods. Results found that the XGBoost and CatBoost models surpassed the other models, achieving the highest determination coefficients (R2) of 0.938 and 0.936, along with the lowest Mean-Absolute-Relative-Error of 7.3% and 7.1%, respectively, thereby establishing them as trustworthy techniques for RLHJ prediction. RF and LightGBM follow closely behind, exhibit strong generalization, whereas GEP and ANN demonstrate commendable performance, although with a marginal rise in error metrics. The study also incorporated Shapley-Additive-exPlanations (SHAP) and Partial-Dependence-Plot analyses, revealing that the relative HJ depths had the most significant impact on the RLHJ predictions. Finally, an interactive Graphical User Interface was later built to allow designers to predict RLHJ quickly and more economically than costly computational or experimental tests.},
  archive      = {J_NCA},
  author       = {Elshaarawy, Mohamed Kamel and Hamed, Abdelrahman Kamal},
  doi          = {10.1007/s00521-024-10846-3},
  journal      = {Neural Computing and Applications},
  month        = {2},
  number       = {6},
  pages        = {4247-4276},
  shortjournal = {Neural Comput. Appl.},
  title        = {Machine learning and interactive GUI for estimating roller length of hydraulic jumps},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A novel approach to enhanced fall detection using STFT and magnitude features with CNN autoencoder. <em>NCA</em>, <em>37</em>(6), 4229-4245. (<a href='https://doi.org/10.1007/s00521-024-10845-4'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The ability to accurately detect and classify falls is critical for ensuring timely medical intervention, especially for the elderly, who face a significantly higher risk of severe injuries, loss of independence, or fatal outcomes from falls. This paper introduces a novel fall detection approach that addresses these urgent needs by using short-time Fourier transform (STFT) images and the magnitude of quaternion (MQ) signals, fused into STFT-MQ images. The proposed method leverages STFT’s time–frequency representation to capture rapid changes and dynamic characteristics in human motion data from wearable sensors, enhancing its ability to distinguish between fall and non-fall incidents. Utilizing a convolutional neural network autoencoder (CNN-AE), an unsupervised learning model, this approach analyzes transformed data without extensive labeled datasets, offering a scalable solution in diverse settings. Tested on the HIFD dataset with heart rate and IMU sensor data, the STFT-MQ-AE method achieves remarkable sensitivity of 98.08%, specificity of 98.78%, and an overall accuracy of 98.57%, setting a new benchmark in fall detection accuracy. Furthermore, the model’s reliance on an N-way K-shot learning approach enables it to manage unforeseen fall cases effectively without retraining, enhancing adaptability and real-world utility. The model achieves the highest Youden’s index (YI) of 96.85%, underlining balanced performance between fall and non-fall classification. Consistent performance across varied training scenarios yields an average accuracy of 96.10%, making this approach highly reliable. This advancement in fall detection technology offers a practical, effective solution to reduce fall-related injuries and enable timely assistance, thereby promoting safer, more independent living for at-risk populations.},
  archive      = {J_NCA},
  author       = {Soontornnapar, Tomorn and Ploysuwan, Tuchsanai},
  doi          = {10.1007/s00521-024-10845-4},
  journal      = {Neural Computing and Applications},
  month        = {2},
  number       = {6},
  pages        = {4229-4245},
  shortjournal = {Neural Comput. Appl.},
  title        = {A novel approach to enhanced fall detection using STFT and magnitude features with CNN autoencoder},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Partial multi-label feature selection based on label matrix decomposition. <em>NCA</em>, <em>37</em>(6), 4207-4227. (<a href='https://doi.org/10.1007/s00521-024-10822-x'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In practice, each instance may be labeled with a candidate label set that contains all relevant labels and some noisy labels, which is known as the partial multi-label learning problem. Since it is difficult for existing multi-label feature selection methods to select the most discriminative features in a data set containing noisy labels, this paper proposes a partial multi-label feature selection method based on label matrix decomposition. Specifically, the method first decomposes the label matrix into two parts: the ground-truth label matrix and the noisy label matrix. Next, a low-rank restriction is applied to the ground-truth label matrix to utilize the correlation information among the ground-truth labels more efficiently; the noisy label matrix is constrained to be sparse, assuming that noise is typically sparse in practical applications. Second, graph Laplacian regularization is introduced to capture the local relevance information of the instances, thereby enabling more accurate identification of the ground-truth labels and allowing the most discriminative features to be selected. Third, a robust $$\sigma$$ -norm is introduced to suppress noise, thus utilizing the available label information more efficiently and improving model performance. Finally, a more flexible $$l_{2,p}$$ -norm is chosen, which helps in better feature selection. Experiments performed on three real-world partial multi-label data sets and six synthetic multi-label data sets show the superiority of our proposed algorithm over several state-of-the-art multi-label feature selection algorithms.},
  archive      = {J_NCA},
  author       = {Liu, Guanghui and Li, Qiaoyan and Yang, Xiaofei and Xing, Zhiwei and Ma, Yingcang},
  doi          = {10.1007/s00521-024-10822-x},
  journal      = {Neural Computing and Applications},
  month        = {2},
  number       = {6},
  pages        = {4207-4227},
  shortjournal = {Neural Comput. Appl.},
  title        = {Partial multi-label feature selection based on label matrix decomposition},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Simultaneous segmentation and classification of lung CT scans for COVID-19 diagnosis: A deep multi-task learning perspective. <em>NCA</em>, <em>37</em>(6), 4185-4205. (<a href='https://doi.org/10.1007/s00521-024-10809-8'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In response to the pressing need for intelligent systems to identify COVID-19 cases and detect infected areas in lung CT scan images during the ongoing pandemic, there is a growing demand for integrated solutions that can effectively handle both segmentation and classification tasks simultaneously. Such systems can provide valuable support to clinicians while enhancing diagnostic accuracy by leveraging shared features. This paper introduces a multi-task architecture designed to concurrently process lung CT scan images for both segmentation and classification tasks. The proposed approach initially employs a shared encoder–decoder architecture based on U-Net, augmented by an additional branch dedicated to classification using a perceptron. To address performance disparities between these tasks during preprocessing, a combination of binary preprocessing algorithms is introduced to establish task equilibrium. Furthermore, convolution block attention module is incorporated into the encoder levels to enhance inter-task coherence. Additionally, the application of conditional random field serves for post-processing in the segmentation task. The effectiveness of the proposed structure is evaluated across four datasets, demonstrating its superiority over previous studies and pre-trained models. The results showcase the highest classification accuracy at 98.14 ± 1.00, accompanied by a Dice index of 89.91 ± 2.00 in segmentation, alongside other relevant evaluation metrics. Furthermore, the multi-task framework is successfully applied to U-Net++ and ResUNet architectures, yielding favorable outcomes. Our proposed model provides a robust solution for accurate COVID-19 analysis, offering valuable support to medical professionals in making diagnostic and treatment decisions.},
  archive      = {J_NCA},
  author       = {Kordnoori, Shirin and Sabeti, Maliheh and Mostafaei, Hamidreza and Banihashemi, Saeed Seyed Agha},
  doi          = {10.1007/s00521-024-10809-8},
  journal      = {Neural Computing and Applications},
  month        = {2},
  number       = {6},
  pages        = {4185-4205},
  shortjournal = {Neural Comput. Appl.},
  title        = {Simultaneous segmentation and classification of lung CT scans for COVID-19 diagnosis: A deep multi-task learning perspective},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Evolutionary optimization of Yagi–Uda antenna design using grey wolf optimizer. <em>NCA</em>, <em>37</em>(6), 4155-4183. (<a href='https://doi.org/10.1007/s00521-024-10806-x'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Meta-heuristic optimization algorithms have become widely used due to their outstanding features, such as gradient-free mechanisms, high flexibility, and great potential for avoiding local optimal solutions. This research explored the grey wolf optimizer (GWO) to find the ideal configuration for a six-element Yagi–Uda antenna. The GWO algorithm adjusted the lengths of the antenna wires and the spacings between them. The goal was to maximize the antenna’s ability to transmit signals (throughput gain). Optimal antenna selection relies on various parameters, including gain, bandwidth, impedance matching, frequency, side-lobe levels, etc. The optimization of a six-element Yagi–Uda antenna presents a challenging engineering design problem due to its multimodal and nonlinear nature. Achieving optimal performance hinges on the intricate interplay between the lengths of the constituent elements and the spacing configurations. To this end, a multiobjective function was adopted to design this antenna. The performance of several meta-heuristic algorithms, including genetic algorithms, biogeography-based optimization, simulated annealing, and grey wolf optimizer, was compared. The GWO-based approach has performed better than its competitors. This optimized antenna design based on GWO reported a gain of 14.21 decibel. Therefore, the GWO-based method optimizes antennas that can be further investigated for other antenna design problems.},
  archive      = {J_NCA},
  author       = {Braik, Malik and Sheta, Alaa and Aljahdali, Sultan and El-Hefnawi, Fatma and Al-Hiary, Heba and Elashmawi, Walaa H.},
  doi          = {10.1007/s00521-024-10806-x},
  journal      = {Neural Computing and Applications},
  month        = {2},
  number       = {6},
  pages        = {4155-4183},
  shortjournal = {Neural Comput. Appl.},
  title        = {Evolutionary optimization of Yagi–Uda antenna design using grey wolf optimizer},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Domain adaptation to enhance (2 + 1)D CNN dynamic analysis of cell collective behavior in time-lapse microscopy videos. <em>NCA</em>, <em>37</em>(6), 4133-4153. (<a href='https://doi.org/10.1007/s00521-024-10767-1'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, 2D CNNs have excelled in analyzing single-frame video sequences, prompting the evolution of standard architectures toward full 3D CNNs. This transition, while enhancing the modeling of spatial and temporal information in video activities, demanded substantial data for effective training. To alleviate this challenge, we introduced a switched multitask training strategy. This approach involves factorizing 3D layers into (2 + 1)D convolutions, training only 2D (1D) layers for spatial (temporal) tasks and switching off the training of the 1D (2D) layers. Additionally, we addressed data scarcity by generating synthetic stylized video sequences. These were crafted using stochastic particle models of collective cell motions, further modified through neural style transfer to mimic real video data. Such a domain adaptation strategy facilitated the creation of training data impractical to obtain in the real world. Transferring knowledge from the switched (2 + 1) CNN to real video data, we encoded wound healing experiments of three distinct cell lines—human melanocytes cells M14, mouse neuroblastoma cells N1, and human prostate cells PC3—into deep features. Employing a novel feature selection strategy based on robustness to disturbances, we discriminated the three wound healing processes. Average classification accuracy of 92.91% (0.11%), 91.50% (0.37%), and 88.81% (0.29%) was obtained for the original real videos, the real videos with progressive altered levels of focus, and levels of brightness, respectively. The proposed approach proved to be a powerful tool for analyzing the spatiotemporal dynamics of biological systems, even in the presence of fluctuations.},
  archive      = {J_NCA},
  author       = {D’Orazio, Michele and Pastore, Donatella and Mencattini, Arianna and Filippi, Joanna and Antonelli, Gianni and Corsi, Francesca and Casti, Paola and Curci, Giorgia and Salmeri, Marcello and Pacifici, Francesca and Ghibelli, Lina and Canosci, David Della-Morte and Martinelli, Eugenio},
  doi          = {10.1007/s00521-024-10767-1},
  journal      = {Neural Computing and Applications},
  month        = {2},
  number       = {6},
  pages        = {4133-4153},
  shortjournal = {Neural Comput. Appl.},
  title        = {Domain adaptation to enhance (2 + 1)D CNN dynamic analysis of cell collective behavior in time-lapse microscopy videos},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Non-local physics informed neural networks for forward and inverse problems containing non-local operators. <em>NCA</em>, <em>37</em>(6), 4111-4132. (<a href='https://doi.org/10.1007/s00521-024-10752-8'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A novel idea, physics informed neural networks introduced a few years back to solve forward and inverse problems for differential equations using the physics information that lies inside them. The central pillar of the physics informed neural networks is automatic differentiation, which is based on the chain rule of differentiation. Automatic differentiation is not applicable to non-local operators because the standard chain rule of differentiation is not valid for non-local operators. Therefore, this work presents non-local physics informed neural networks, which use standard approximation methods for non-local operators and automatic differentiation for local operators to solve differential equations containing non-local operators (forward problems) as well as learn differential equations involving non-local operators (inverse problems). In this work, we consider the Caputo fractional derivative, Volterra integral, and Itô integral as non-local operators. Moreover, we demonstrate the efficiency of the non-local physics informed neural networks with different test examples like the time-fractional diffusion equation in one and two dimensions, the time-fractional Burgers’ equation (both equations involving Caputo fractional derivative as non-local operators), the fractional integro-differential equation (Caputo fractional derivative and Volterra integral as non-local operators), and the stochastic fractional integro-differential equation (Caputo fractional derivative, Itô integral, and Volterra integral as non-local operators). Furthermore, for the non-smooth solution, we use the approximation method on non-uniform mesh for non-local operators and compare the results with the approximation method on uniform mesh. We also discuss the error analysis and convergence of the proposed non-local physics informed neural networks. Finally, we take real-world data, which is described by the differential equation containing non-local operators, and show the effectiveness of non-local physics-informed neural networks in addressing practical applications.},
  archive      = {J_NCA},
  author       = {Singh, Abhishek Kumar and Mehra, Mani and Pulch, Roland},
  doi          = {10.1007/s00521-024-10752-8},
  journal      = {Neural Computing and Applications},
  month        = {2},
  number       = {6},
  pages        = {4111-4132},
  shortjournal = {Neural Comput. Appl.},
  title        = {Non-local physics informed neural networks for forward and inverse problems containing non-local operators},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Leveraging large language models for word sense disambiguation. <em>NCA</em>, <em>37</em>(6), 4093-4110. (<a href='https://doi.org/10.1007/s00521-024-10747-5'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Natural language processing (NLP) is difficult because human language contains ambiguity. The same word can have a different meaning depending on the context and may result in different interpretations given biases held by a NLP technique. Correctly interpreting this ambiguity is not simply an important task in its own right but is a key enabler to major NLP activities such as machine translation and question answering. This research proposes three techniques to evaluate a large language models’(LLMs) ability to perform word sense disambiguation (WSD) and explores the efficacy of seven generative LLMs. The first technique assesses whether LLMs can, given a context sentence, select the correct word sense from a menu of options. The second asks LLMs, without options provided, to state whether or not a provided word sense is correct. The third technique presents the LLMs with context and an unseen word, assessing whether the LLMs can infer from context the sense of a word that it has not seen during training. Results demonstrate a strong relationship between model size and performance. Applications of WSD are demonstrated as part of an information extraction pipelines supporting sentiment analysis and as part of an LLM-evaluation suite to support machine learning operations.},
  archive      = {J_NCA},
  author       = {Yae, Jung H. and Skelly, Nolan C. and Ranly, Neil C. and LaCasse, Phillip M.},
  doi          = {10.1007/s00521-024-10747-5},
  journal      = {Neural Computing and Applications},
  month        = {2},
  number       = {6},
  pages        = {4093-4110},
  shortjournal = {Neural Comput. Appl.},
  title        = {Leveraging large language models for word sense disambiguation},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Gender bias detection on hate speech classification: An analysis at feature-level. <em>NCA</em>, <em>37</em>(5), 3887-3905. (<a href='https://doi.org/10.1007/s00521-024-10841-8'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Hate speech is a growing problem on social media due to the larger volume of content being shared. Recent works demonstrated the usefulness of distinct machine learning algorithms combined with natural language processing techniques to detect hateful content. However, when not constructed with the necessary care, learning models can magnify discriminatory behaviour and lead the model to incorrectly associate comments with specific identity terms (e.g., woman, black, and gay) with a particular class, such as hate speech. Moreover, some specific characteristics should be considered in the test set when evaluating the presence of bias, considering that the test set can follow the same biased distribution of the training set and compromise the results obtained by the bias metrics. This work argues that considering the potential bias in hate speech detection is needed and focuses on developing an intelligent system to address these limitations. Firstly, we proposed a comprehensive, unbiased dataset to unintended gender bias evaluation. Secondly, we propose a framework to help analyse bias from feature extraction techniques. Then, we evaluate several state-of-the-art feature extraction techniques, specifically focusing on the bias towards identity terms. We consider six feature extraction techniques, including TF, TF-IDF, FastText, GloVe, BERT, and RoBERTa, and six classifiers, LR, DT, SVM, XGB, MLP, and RF. The experimental study across hate speech datasets and a range of classification and unintended bias metrics demonstrates that the choice of the feature extraction technique can impact the bias on predictions, and its effectiveness can depend on the dataset analysed. For instance, combining TF and TF-IDF with DT and MLP resulted in higher bias, while BERT and RoBERTa showed lower bias with the same classifier for the HE and WH datasets. The proposed dataset and source code will be publicly available when the paper is published.},
  archive      = {J_NCA},
  author       = {Nascimento, Francimaria R. S. and Cavalcanti, George D. C. and Costa-Abreu, Marjory Da},
  doi          = {10.1007/s00521-024-10841-8},
  journal      = {Neural Computing and Applications},
  month        = {2},
  number       = {5},
  pages        = {3887-3905},
  shortjournal = {Neural Comput. Appl.},
  title        = {Gender bias detection on hate speech classification: An analysis at feature-level},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A novel forecasting model for time series using optimized interval division and fuzzy relationships. <em>NCA</em>, <em>37</em>(5), 3869-3885. (<a href='https://doi.org/10.1007/s00521-024-10727-9'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This study builds an effective forecasting model for time series based on significant improvements of the fuzzy clustering algorithm. Firstly, we use the universal set, which is the percentage change between two consecutive time points, and divide it into unequal intervals using the automatic clustering algorithm that adjusts the number of clusters. Next, we propose the networks so that establish the fuzzy relationship of the elements in the universal set and the intervals based on the improved inverse fuzzy number. Finally, a new principle for future forecasting is built based on these two improvements. The proposed model details the steps and is clearly illustrated through a specific numerical. The convergence of the proposed algorithm is also considered and addressed. The proposed model has demonstrated effectiveness by outperforming many other models applied to the M3 dataset with 3,003 series, the M4 dataset with 100,000 series, and the well-known benchmark dataset. The proposed model is also applied to forecast the number of people infected and deceased due to COVID-19 in Southeast Asian countries. This application also demonstrates the advantages of the proposed model over many existing models.},
  archive      = {J_NCA},
  author       = {PhamToan, Dinh and Vo-Van, Tai},
  doi          = {10.1007/s00521-024-10727-9},
  journal      = {Neural Computing and Applications},
  month        = {2},
  number       = {5},
  pages        = {3869-3885},
  shortjournal = {Neural Comput. Appl.},
  title        = {A novel forecasting model for time series using optimized interval division and fuzzy relationships},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Improving chinese word segmentation with character–lexicon class attention. <em>NCA</em>, <em>37</em>(5), 3857-3867. (<a href='https://doi.org/10.1007/s00521-024-10693-2'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Character–lexicon matching positions are important in Chinese word segmentation (CWS). Inspired by wordhood memory segmentation (WMSEG) and non-flat-lattice transformer (NFLAT), we consider character–lexicon attention with word match position and calculate the probability of characters appearing at the beginning (B), end (E), middle (M), and single (S) of words. We therefore propose a neural framework, CLSEG, which incorporates mask and class attention mechanisms to generate informative position features for characters, based on both character and lexicon contexts. The framework encodes character in base model (TENER) and pretrained models (ZEN, BERT-wwm and BABERT) with context feature and makes lexicon and tag fusion in attention. Compared with NFLAT, CLSEG changes lexicon values to character–tag values which are assigned weights from character–lexicon class attention. Compared with WMSEG, CLSEG decouples character–lexicon attention from NFLAT with multi-head and relative attention. Experimental studies incorporates YJ and LS lexicon for encoding. Experimental results and analyses on four benchmark CWS datasets demonstrate the superiority of CLSEG over existing approaches. The source code of the proposed method is publicly available at https://github.com/na978292231/CLSEG/tree/main/CLSSEG4CWS-main .},
  archive      = {J_NCA},
  author       = {Xu, Zhongguo and Xiang, Yang},
  doi          = {10.1007/s00521-024-10693-2},
  journal      = {Neural Computing and Applications},
  month        = {2},
  number       = {5},
  pages        = {3857-3867},
  shortjournal = {Neural Comput. Appl.},
  title        = {Improving chinese word segmentation with character–lexicon class attention},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Path tracking control of autonomous vehicles via prescribed performance approach. <em>NCA</em>, <em>37</em>(5), 3839-3856. (<a href='https://doi.org/10.1007/s00521-024-10680-7'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper addresses the path tracking control problem for autonomous vehicles subject to the compound disturbance and unavailable variable. Before the controller design, a state observer and a disturbance observer are constructed to estimate the sideslip angle and the compound disturbance, respectively. Then, a prescribed performance function is proposed to bound the tracking error to improve the tracking accuracy. Finally, the controller is designed based on the backstepping scheme. The Lyapunov stability theory shows that the tracking error converges to a preset range. The effectiveness of the proposed method is verified by hardware-in-the-loop and real vehicle experiments.},
  archive      = {J_NCA},
  author       = {Zhang, Sucai and Wang, Yongfu and Li, Gang and Wang, Yunlong},
  doi          = {10.1007/s00521-024-10680-7},
  journal      = {Neural Computing and Applications},
  month        = {2},
  number       = {5},
  pages        = {3839-3856},
  shortjournal = {Neural Comput. Appl.},
  title        = {Path tracking control of autonomous vehicles via prescribed performance approach},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Microstrip patch antenna modeling using neural networks with knowledge-based regularization. <em>NCA</em>, <em>37</em>(5), 3827-3837. (<a href='https://doi.org/10.1007/s00521-024-10860-5'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Neural networks (NNs) have proven a useful surrogate model for the design and optimization of high frequency structures including antennas. Black-box NNs are known to have scalability and accuracy problems as the dimension of the problem increases. This study proposes knowledge-based regularization methods, referred to as derivative, spectral, and magnitude regularization to address these issues. The proposed methods utilize the functional properties of S-parameters to improve the accuracy and prevent unphysical predictions. The NNs are trained and tested using a data set of 5000 samples generated by Latin Hypercube Sampling and simulated by Ansys HFSS. The goodness of fit is evaluated using Relative Squared Error. Derivative and spectral regularizations reduce the RSE loss from 0.052 to 0.046 and 0.043, respectively. When combined with magnitude regularization, up to 17 $$\%$$ and 88 $$\%$$ reduction in loss and passivity violations can be achieved, at the expense of a 37 $$\%$$ increase in training time. Moreover, 25 $$\%$$ less data is required, to maintain a similar loss to the reference NN.},
  archive      = {J_NCA},
  author       = {Saçın, Ekin Su and Durgun, Ahmet Cemal},
  doi          = {10.1007/s00521-024-10860-5},
  journal      = {Neural Computing and Applications},
  month        = {2},
  number       = {5},
  pages        = {3827-3837},
  shortjournal = {Neural Comput. Appl.},
  title        = {Microstrip patch antenna modeling using neural networks with knowledge-based regularization},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). AGBUNet: An enhanced CNN-UNET architecture for the prediction of above ground biomass using deep learning. <em>NCA</em>, <em>37</em>(5), 3809-3826. (<a href='https://doi.org/10.1007/s00521-024-10840-9'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Accurate prediction of above ground biomass (AGB) is critical for monitoring forest health and carbon cycling. It is crucial for understanding and managing forest ecosystems. In this paper, we propose an enhanced framework combining convolutional neural network (CNN) and UNet, termed AGBUNet, specifically designed for predicting AGB using remote sensing data. The framework consists of separate CNNs for processing each type of image, whose outputs are subsequently fused in a UNet architecture to enhance prediction accuracy. These modifications include customized convolutional layers, advanced preprocessing techniques, and a novel integration of data prompts from separate Sentinel-1 and Sentinel-2 image processing streams. The AGBUNet integrates Sentinel-1 and Sentinel-2 images to leverage complementary information from synthetic aperture radar (SAR) and optical sensors. This study underscores the potential of the AGBUNet model for enhancing biomass estimation from remote sensing data, contributing to better forest management and ecological monitoring. The performance measures obtained are compared with the other models, and the following results are obtained as follows for MSE value of 298.25, RMSE value of 15.27 and MAE value of 12.21, and the values are satisfying compared with earlier benchmarks.},
  archive      = {J_NCA},
  author       = {Arumai Shiney, S. and Geetha, R.},
  doi          = {10.1007/s00521-024-10840-9},
  journal      = {Neural Computing and Applications},
  month        = {2},
  number       = {5},
  pages        = {3809-3826},
  shortjournal = {Neural Comput. Appl.},
  title        = {AGBUNet: An enhanced CNN-UNET architecture for the prediction of above ground biomass using deep learning},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Scour prediction downstream of an ogee weir using group method of data handling neural network. <em>NCA</em>, <em>37</em>(5), 3793-3807. (<a href='https://doi.org/10.1007/s00521-024-10838-3'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The paper presents the prediction of scour hole parameters downstream of an ogee weir using a group method of data handling (GMDH) employing laboratory experimental results. The results of GMDH are compared with those of support vector machine (SVM) models. The experimental results are used to find the effects of flow and sediment characteristics on nondimensional equilibrium scour depth and its nondimensional location. Both the magnitude of nondimensional equilibrium scour depth, and its location increase with the increase in the densimetric Froude number, head over weir crest, and sediment size for uniform sediments. However, for nonuniform sediments, the parameters have lower values indicating the segregation and armor layer formation, which arrest the erosion. The prediction of nondimensional equilibrium scour depth and its location by GMDH is found to be better than various SVM models in terms of statistical indices. The R2-value for GMDH is greater than 0.99, while by SVM models it is lesser as presented through the Taylor diagram. Also, all error indicators are lower in case of GMDH as compared with SVM models. Therefore, GMDH can be proposed to be more suitable predictor for small size data set as in the present study.},
  archive      = {J_NCA},
  author       = {Raikar, Rajkumar V. and Khanai, Rajashri and Torse, Dattaprasad A. and Doshi, Tejas D. and Tapale, Manisha},
  doi          = {10.1007/s00521-024-10838-3},
  journal      = {Neural Computing and Applications},
  month        = {2},
  number       = {5},
  pages        = {3793-3807},
  shortjournal = {Neural Comput. Appl.},
  title        = {Scour prediction downstream of an ogee weir using group method of data handling neural network},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Enhanced facial action unit detection with adaptable patch sizes on representative landmarks. <em>NCA</em>, <em>37</em>(5), 3777-3791. (<a href='https://doi.org/10.1007/s00521-024-10836-5'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The human face displays expressions through the contraction of various facial muscles. The Facial Action Coding System (FACS) is a widely accepted taxonomy that describes all visible changes in the face in terms of action units (AUs). In this study, AUs are examined by finding the most active landmarks of the face and then examining the most representative patch sizes of each landmark for the AU detection task. Sparse learning is employed to learn the most active landmarks for each AU, and then the active landmark patches are fed to ViT and Perceiver mechanisms independently. Experiments indicate that using active landmark patches with their most representative size improves the results when compared to using all the landmarks, especially when it is used on more challenging datasets as a support for the attention mechanism of the classifier. The results demonstrate that the proposed method improves the performance of the employed models and are further supported by experiments conducted across different datasets.},
  archive      = {J_NCA},
  author       = {Cakir, Duygu and Yilmaz, Gorkem and Arica, Nafiz},
  doi          = {10.1007/s00521-024-10836-5},
  journal      = {Neural Computing and Applications},
  month        = {2},
  number       = {5},
  pages        = {3777-3791},
  shortjournal = {Neural Comput. Appl.},
  title        = {Enhanced facial action unit detection with adaptable patch sizes on representative landmarks},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Efficient breast cancer detection using neural networks and explainable artificial intelligence. <em>NCA</em>, <em>37</em>(5), 3759-3776. (<a href='https://doi.org/10.1007/s00521-024-10790-2'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The growing dependence on deep learning models for medical diagnosis underscores the critical need for robust interpretability and transparency to instill trust and ensure responsible usage. This study investigates the efficacy of various explainable artificial intelligence (XAI) techniques in comprehending deep learning models utilized for breast cancer classification from down sampled histopathology images. A comparative assessment of multiple convolutional neural network (CNN) architectures, encompassing standard CNNs, ResNet, VGG-16, and VGG-19, on down sampled images was conducted. The primary goal is to pinpoint the model exhibiting the highest accuracy and subsequently employ three prominent XAI methods—LIME, SHAP, and Saliency Maps—to get insights into the top-performing model. This study identifies VGG-19 as the best-performing model with an accuracy of 92.59% and demonstrates that among various XAI techniques, LIME provides the most accurate and clinically relevant explanations for breast cancer classification from down sampled histopathology images. These findings, validated by medical professionals, enhance the interpretability and reliability of deep learning models in clinical settings, promoting their responsible integration into healthcare practices. This validation was further corroborated through consultation with medical professionals, including doctors specializing in breast cancer diagnosis. This research endeavors to deepen the understanding of the model’s rationale and instill confidence in its outputs. The outcomes of this study hold significant promise in elevating the interpretability and reliability of deep learning models tailored for breast cancer diagnosis, thus facilitating their responsible integration into clinical settings.},
  archive      = {J_NCA},
  author       = {Murugan, Tamilarasi Kathirvel and Karthikeyan, Pritikaa and Sekar, Pavithra},
  doi          = {10.1007/s00521-024-10790-2},
  journal      = {Neural Computing and Applications},
  month        = {2},
  number       = {5},
  pages        = {3759-3776},
  shortjournal = {Neural Comput. Appl.},
  title        = {Efficient breast cancer detection using neural networks and explainable artificial intelligence},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Motion planning system for unmanned aerial vehicles in dynamic three-dimensional space: A machine learning approach. <em>NCA</em>, <em>37</em>(5), 3733-3757. (<a href='https://doi.org/10.1007/s00521-024-10784-0'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Unmanned aerial vehicles (UAVs) have been highly enhanced in the last decade, targeting the deployment of UAV technology in different new applications in many fields. However, the motion control of the UAV during autonomous flight is still under development, especially in unstructured or dynamic environments. This paper presents an intelligent motion control framework using artificial potential field neural networks (APF-NNs) for UAVs in dynamic 3D environments. This approach is developed using potential field models for repulsive and attractive forces. Using the control commands of the APF-NN intelligent system, the UAV changes its planner position and altitude simultaneously to perform the path planning task. In other methods, the UAV only changes its planner positions for path planning task. The presented work is validated using simulations and field experiments. The proposed approach is outperforming existing fuzzy logic and classical potential field-based methods with a 75% and 86% reduction in computational time, 22% and 35% increase in inside success rate, 1.5% and 14.7% increase in the speed and 6.2% and 11% reduction in total traveled distance, respectively.},
  archive      = {J_NCA},
  author       = {Garibeh, Mohammad H. and Al-Dahidi, Sameer and Hayajneh, Mohammad and Jaradat, Mohammad A. and Alshorman, Ahmad},
  doi          = {10.1007/s00521-024-10784-0},
  journal      = {Neural Computing and Applications},
  month        = {2},
  number       = {5},
  pages        = {3733-3757},
  shortjournal = {Neural Comput. Appl.},
  title        = {Motion planning system for unmanned aerial vehicles in dynamic three-dimensional space: A machine learning approach},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Parameter identification method for sub-synchronous oscillation signals in power systems based on improved multi-synchrosqueezing transform and automatic recognition algorithm. <em>NCA</em>, <em>37</em>(5), 3719-3732. (<a href='https://doi.org/10.1007/s00521-024-10779-x'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Currently, the identification methods for sub-synchronous oscillation (SSO) signals typically involve parameter identification for each modal component. However, in practical scenarios, we are primarily interested in the divergent modal components. This leads to a considerable amount of unnecessary computations, thereby reducing the speed and efficiency of parameter identification. Therefore, this paper proposes a method that only requires the identification of divergent SSO modal components. Firstly, an improved multi-synchrosqueezing transform (IMSST) is introduced to handle SSO signals. IMSST not only obtains the time-frequency (TF) spectrum of the signal but also achieves modal decomposition. Subsequently, an automatic recognition algorithm (ARA) is introduced, which assesses the attenuation characteristics of each intrinsic mode function (IMF) of the SSO signal in the TF spectrum. The ARA ignores components with attenuation and reconstructs the divergent components. Finally, the reconstructed components are subjected to modal parameter identification using the hilbert transform (HT) and least squares fitting (LSF). This method improves the speed of parameter identification while ensuring accuracy and robustness. The effectiveness and efficiency of the proposed method are validated through comprehensive comparative analyses involving numerical simulations, model simulations, and practical SSO signals data, utilizing various techniques.},
  archive      = {J_NCA},
  author       = {Lin, Quan and Zhang, Yong-Feng and Xue, Fei and Yu, Gang},
  doi          = {10.1007/s00521-024-10779-x},
  journal      = {Neural Computing and Applications},
  month        = {2},
  number       = {5},
  pages        = {3719-3732},
  shortjournal = {Neural Comput. Appl.},
  title        = {Parameter identification method for sub-synchronous oscillation signals in power systems based on improved multi-synchrosqueezing transform and automatic recognition algorithm},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Estimation of orange tree production by regression from video segments under uncontrolled conditions. <em>NCA</em>, <em>37</em>(5), 3703-3717. (<a href='https://doi.org/10.1007/s00521-024-10772-4'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Precision agriculture covers a wide range of information and communications technologies aimed at supporting current challenges in crop estimation, productivity increase, and food security. In particular, crop yield estimation can provide very valuable information on crop distribution, with the aim of optimising logistics and harvest timing. This paper focuses on deep learning-based regression solutions for estimating the number of visible oranges on trees, from real-world crop row videos captured by a camera placed on a farm vehicle. Count predictions based on individual frames were compared with those based on variable size sequences of frames centred on each tree (video segments). The performance of three deep neural networks designed for regression was evaluated in terms of the regression error and the uncertainty of the estimates, and differences were analysed using nonparametric hypothesis testing. Experiments were conducted on a new dataset composed of annotated video segments of orange tree rows acquired under uncontrolled conditions, which has been made publicly available. Results statistically prove the value of considering multiple frames and the feasibility of yield estimation by regression in the wild. These findings are expected to contribute to optimising decision-making in crop resource management. Unlike most efforts so far, which involve counting fruits by detection in tree images usually captured manually, this work explores counting fruits by regression on trees from real-world video data.},
  archive      = {J_NCA},
  author       = {Mollineda, Ramón A. and Sandoval, Javier and Rodríguez, Christian D. and Heredia, José A.},
  doi          = {10.1007/s00521-024-10772-4},
  journal      = {Neural Computing and Applications},
  month        = {2},
  number       = {5},
  pages        = {3703-3717},
  shortjournal = {Neural Comput. Appl.},
  title        = {Estimation of orange tree production by regression from video segments under uncontrolled conditions},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Deep learning framework for wind speed prediction in saudi arabia. <em>NCA</em>, <em>37</em>(5), 3685-3701. (<a href='https://doi.org/10.1007/s00521-024-10766-2'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Increased utilization of conventional energy sources severely impacts the environment by increasing the global temperature and contributing to global warming. Sustainable energy sources could contribute to handling the increment while also providing cheap, clean, and abundant energy. Wind energy is considered one of the most reliable sources of sustainable energy due to its abundance and availability during the day and night compared to other renewable resources. However, effective forecasting of such intermittent resources is considered a key challenge for power system operators. This paper develops a novel deep learning framework for forecasting the wind speed in Dhahran City, Saudi Arabia, using two years of data acquired at different heights from the light detection and ranging device, an active remote sensing wind monitoring system. It also segments the data and removes wrong measurements through data pre-processing. The article identifies the best configuration for deep learning models, such as long short-term memory , through a systematic approach. The presented results confirm the efficacy of the developed models against the selected statistical performance measures. The developed model performs better with large data volumes than with lower volumes. Finally, the comparative analysis with the literature-reported results provides confidence in the competency of the proposed model in predicting wind speed for various periods.},
  archive      = {J_NCA},
  author       = {Alabdulhadi, Arwa Ahmed and Rehman, Shafiqur and Ali, Amjad and Shafiullah, Md},
  doi          = {10.1007/s00521-024-10766-2},
  journal      = {Neural Computing and Applications},
  month        = {2},
  number       = {5},
  pages        = {3685-3701},
  shortjournal = {Neural Comput. Appl.},
  title        = {Deep learning framework for wind speed prediction in saudi arabia},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Starfish optimization algorithm (SFOA): A bio-inspired metaheuristic algorithm for global optimization compared with 100 optimizers. <em>NCA</em>, <em>37</em>(5), 3641-3683. (<a href='https://doi.org/10.1007/s00521-024-10694-1'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This work presents the starfish optimization algorithm (SFOA), a novel bio-inspired metaheuristic for solving optimization problems, which simulates behaviors of starfish, including exploration, preying, and regeneration. SFOA consists of two main phases of exploration and exploitation. The exploration phase mimics the explorative behavior of starfish by the hybrid search pattern of combining with the five-dimensional and unidimensional search patterns to increase the computational efficiency and ensure the search capacity. The exploitation phase simulates the preying and regeneration behaviors of starfish, with a two-directional search strategy and special movement, to ensure convergence in exploitation. This work validates SFOA’s performance on 65 benchmark functions from classical functions, CEC 2017 and CEC 2022 test suites, and compares with 100 different metaheuristic algorithms, including state-of-the-art optimizers, such as marine predators algorithm, water flow optimizer (WFO), LSHADE, LSHADE-cnEpSin, and LSHADE-SPACMA. Statistical results from one-on-one comparisons demonstrate that the proposed SFOA outperforms 95 compared algorithms in accuracy and 97 algorithms in efficiency, which is only worse than WFO both in accuracy and efficiency. The scalability analysis also demonstrates that SFOA has the capacity to solve high-dimensional benchmark functions. Furthermore, ten real-world engineering optimization problems illustrate the effectiveness of SFOA to achieve global solutions and exhibit stable results. In conclusion, SFOA is promising for solving various optimization problems. The source code of SFOA is publicly available at: https://ww2.mathworks.cn/matlabcentral/fileexchange/173735-starfish-optimization-algorithm-sfoa .},
  archive      = {J_NCA},
  author       = {Zhong, Changting and Li, Gang and Meng, Zeng and Li, Haijiang and Yildiz, Ali Riza and Mirjalili, Seyedali},
  doi          = {10.1007/s00521-024-10694-1},
  journal      = {Neural Computing and Applications},
  month        = {2},
  number       = {5},
  pages        = {3641-3683},
  shortjournal = {Neural Comput. Appl.},
  title        = {Starfish optimization algorithm (SFOA): A bio-inspired metaheuristic algorithm for global optimization compared with 100 optimizers},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multi-lag latent variable models for industrial process monitoring in dynamic and static states. <em>NCA</em>, <em>37</em>(5), 3621-3639. (<a href='https://doi.org/10.1007/s00521-024-10685-2'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Process data collected in modern complex industries have both static and dynamic features, and current process monitoring algorithms only focus on analyzing the two features individually, ignoring the coupling between the two features. This paper proposes a multi-lag latent variable model (MLVM) capable of monitoring both static and dynamic features of industrial processes and addressing the shortcomings of multi-lag slow feature analysis in applications. Firstly, the slow features with multi-lag autocorrelation are extracted separately using slow feature analysis, and the autocorrelation coefficients of the slow features are calculated. Multi-lag dynamic and static features are obtained by setting thresholds, and the static characteristics are further analyzed using independent component analysis. Finally, multi-lag dynamic, static, and global statistics are obtained using Bayesian inference. In addition, the averaging process under online monitoring is proposed to reduce the noise impact on MLVM. The reconstruction-based contribution index for the MLVM is derived to diagnosis after a fault. Based on the Tennessee Eastman process, the superiority of MLVM over the known algorithm is verified, and the validity and interpretability of MLVM are proved.},
  archive      = {J_NCA},
  author       = {Liu, Chaolu and Ren, Yuwei and Fang, Yixian},
  doi          = {10.1007/s00521-024-10685-2},
  journal      = {Neural Computing and Applications},
  month        = {2},
  number       = {5},
  pages        = {3621-3639},
  shortjournal = {Neural Comput. Appl.},
  title        = {Multi-lag latent variable models for industrial process monitoring in dynamic and static states},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Traffic sign recognition model based on scale sequence features and high-order spatial interactions. <em>NCA</em>, <em>37</em>(5), 3605-3620. (<a href='https://doi.org/10.1007/s00521-024-10677-2'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Traffic sign recognition is an integral part of driver assistance systems play a crucial role in enhancing road safety. Due to a large number of challenging targets, such as occlusion, distortion, and small targets in actual scenes, existing methods still have bottlenecks in the accuracy of detection and recognition. Therefore, this paper proposes a real-time traffic sign recognition model based on scale sequence features (SSFs) and high-order spatial interactions (HOSIs) on the basis of YOLOv5. This model extracted scale-invariant SSF using a high-dimensional convolution on the underlying feature map to improve the detection effect of multi-scale targets. At the same time, recursive gated convolution modules are integrated into the feature pyramid network, expanding the interactions to arbitrary order and providing richer feature information for high-order convolutions. In addition, a cross-stage partial (CSP) structure is incorporated into the spatial pooling pyramid, which improves the structure’s performance without introducing excessive computational complexity. The experimental results show that the proposed model achieves 95.5% and 99.92% of mAP on Tsinghua–Tencent 100K (TT100K) and German Traffic Sign Recognition Benchmark (GTSRB), respectively, which surpass the performance of the most advanced model and meets the requirements of real-time detection.},
  archive      = {J_NCA},
  author       = {Zhang, Gan and Wang, Yafei and Li, Wenju and Fu, Xianping},
  doi          = {10.1007/s00521-024-10677-2},
  journal      = {Neural Computing and Applications},
  month        = {2},
  number       = {5},
  pages        = {3605-3620},
  shortjournal = {Neural Comput. Appl.},
  title        = {Traffic sign recognition model based on scale sequence features and high-order spatial interactions},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multilevel chaotic encryption model with cyclic redundancy check for medical data. <em>NCA</em>, <em>37</em>(5), 3581-3603. (<a href='https://doi.org/10.1007/s00521-024-10800-3'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In telemedicine applications, it is crucial to ensure the authentication, confidentiality, and privacy of medical data due to its sensitive nature and the importance of the patient information it contains. Communication through open networks is insecure and has many vulnerabilities, making it susceptible to unauthorized access and misuse. Encryption models are used to secure medical data from unauthorized access. In this work, we propose a bit-level encryption model having three phases: preprocessing, confusion, and diffusion. This model is designed for different types of medical data including patient information, clinical data, medical signals, and images of different modalities. Also, the proposed model is effectively implemented for grayscale and color images with varying aspect ratios. Preprocessing has been applied based on the type of medical data. A random permutation has been used to scramble the data values to remove the correlation, and multilevel chaotic maps are fused with the cyclic redundancy check method. A circular shift is used in the diffusion phase to increase randomness and security, providing protection against potential attacks. The CRC method is further used at the receiver side for error detection. The performance efficiency of the proposed encryption model is proved in terms of histogram analysis, information entropy, correlation analysis, signal-to-noise ratio, peak signal-to-noise ratio, number of pixels changing rate, and unified average changing intensity. The proposed bit-level encryption model therefore achieves information entropy values ranging from 7.9669 to 8.000, which is close to the desired value of 8. Correlation coefficient values of the encrypted data approach to zero or are negative, indicating minimal correlation in encrypted data. Resistance against differential attacks is demonstrated by NPCR and UACI values exceeding 0.9960 and 0.3340, respectively. The key space of the proposed model is $$10^{96}$$ , which is substantially more significant than the required value of $$2^{128}$$ necessary to provide security against brute force attacks. Moreover, even a minor change of $$10^{-16}$$ in the key values significantly affects the proposed encryption model. The experimental results show that the proposed encryption model is secure from statistical, entropy, differential, cropping, noise, and brute force attacks. The proposed model achieves the desired security level for encrypting confidential medical data. Additionally, the effectiveness of the proposed encryption model is assessed against other benchmark encryption techniques. It surpasses existing state-of-the-art encryption techniques, demonstrating a strong negative correlation coefficient, high NPCR (99.74%), the highest UACI (33.54%), and optimal entropy (7.9994). These results highlight its excellent resistance to attacks and significant improvements in security.},
  archive      = {J_NCA},
  author       = {Sharma, Suvita Rani and Singh, Birmohan and Kaur, Manpreet},
  doi          = {10.1007/s00521-024-10800-3},
  journal      = {Neural Computing and Applications},
  month        = {2},
  number       = {5},
  pages        = {3581-3603},
  shortjournal = {Neural Comput. Appl.},
  title        = {Multilevel chaotic encryption model with cyclic redundancy check for medical data},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). FabuLight-ASD: Unveiling speech activity via body language. <em>NCA</em>, <em>37</em>(5), 3561-3579. (<a href='https://doi.org/10.1007/s00521-024-10792-0'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Active speaker detection (ASD) in multimodal environments is crucial for various applications, from video conferencing to human-robot interaction. This paper introduces FabuLight-ASD, an advanced ASD model that integrates facial, audio, and body pose information to enhance detection accuracy and robustness. Our model builds upon the existing Light-ASD framework by incorporating human pose data, represented through skeleton graphs, which minimises computational overhead. Using the Wilder Active Speaker Detection (WASD) dataset, renowned for reliable face and body bounding box annotations, we demonstrate FabuLight-ASD’s effectiveness in real-world scenarios. Achieving an overall mean average precision (mAP) of 94.3%, FabuLight-ASD outperforms Light-ASD, which has an overall mAP of 93.7% across various challenging scenarios. The incorporation of body pose information shows a particularly advantageous impact, with notable improvements in mAP observed in scenarios with speech impairment, face occlusion, and human voice background noise. Furthermore, efficiency analysis indicates only a modest increase in parameter count (27.3%) and multiply-accumulate operations (up to 2.4%), underscoring the model’s efficiency and feasibility. These findings validate the efficacy of FabuLight-ASD in enhancing ASD performance through the integration of body pose data. FabuLight-ASD’s code and model weights are available at https://github.com/knowledgetechnologyuhh/FabuLight-ASD .},
  archive      = {J_NCA},
  author       = {Carneiro, Hugo and Wermter, Stefan},
  doi          = {10.1007/s00521-024-10792-0},
  journal      = {Neural Computing and Applications},
  month        = {2},
  number       = {5},
  pages        = {3561-3579},
  shortjournal = {Neural Comput. Appl.},
  title        = {FabuLight-ASD: Unveiling speech activity via body language},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Empowering few-shot learning: A multimodal optimization framework. <em>NCA</em>, <em>37</em>(5), 3539-3560. (<a href='https://doi.org/10.1007/s00521-024-10780-4'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The development of transformer-based models has significantly advanced research in natural language processing and computer vision, allowing us to create models with excellent results across various domains. However, in real-world scenarios, the model may lack generalization ability and perform poorly due to data distribution shifts, insufficient training data, or low-quality data. This work proposes the generic multimodal optimization-based few-shot learning framework (GoFSL). The framework leverages few-shot learning to learn from a few data, multimodal learning to learn a rich representation of image and text data, and meta-learning to help the model generalization. We evaluated the framework using ten datasets from various domains and characteristics, including short texts from Twitter, legal domain long text, text with alphabetic (English and Portuguese) and non-alphabetic (Japanese) languages, images from the medical domain, and multimodal benchmark datasets. GoFSL outperformed the state-of-the-art model ALMO by 1.05% with CUB-200-2011 and multimodal ProtoNet by 0.86% with Oxford-102 dataset. GoFSL is a small but efficient model, with low CO2 estimated emissions (0.01 kgCO2eq), and adaptable to different domains, data modalities, and languages.},
  archive      = {J_NCA},
  author       = {Enamoto, Liriam and Rocha Filho, Geraldo Pereira and Weigang, Li},
  doi          = {10.1007/s00521-024-10780-4},
  journal      = {Neural Computing and Applications},
  month        = {2},
  number       = {5},
  pages        = {3539-3560},
  shortjournal = {Neural Comput. Appl.},
  title        = {Empowering few-shot learning: A multimodal optimization framework},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A cost estimation recommendation system for improved contingency management in construction projects. <em>NCA</em>, <em>37</em>(5), 3521-3538. (<a href='https://doi.org/10.1007/s00521-024-10740-y'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Management of contingency reserves involves identifying and prioritizing potential high-cost impact events, serving as a cushion for absorbing the financial risks of projects. Machine learning (ML) models exist for estimating rework costs; however, they cannot recommend related activities that influence contingency costs. This research proposes a novel approach that integrates a construction contingency network, advanced node2vec algorithms, and cosine similarity measures to identify construction activities with similar contingency costs, facilitating the management and planning of rework costs. The proposed system offers tailored recommendations and aids in project management by reducing guesswork using the design science research (DSR) methodology that combines advanced ML techniques with practical construction management strategies to provide a robust tool for navigating the complexities of rework costs. The configured recommendation system achieved an 82% accuracy in its suggestions for critical construction activities with a high-cost impact, along with a 4% loss, demonstrating good generalization. Novelty of this research lies in its first-time development of a recommendation model capable of generating dynamic recommendations of the activities that impact the contingency budget, supporting the existing cost forecasting model.},
  archive      = {J_NCA},
  author       = {Mostofi, Fatemeh and Toğan, Vedat and Tokdemir, Onur Behzat and Arayici, Yusuf},
  doi          = {10.1007/s00521-024-10740-y},
  journal      = {Neural Computing and Applications},
  month        = {2},
  number       = {5},
  pages        = {3521-3538},
  shortjournal = {Neural Comput. Appl.},
  title        = {A cost estimation recommendation system for improved contingency management in construction projects},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Quantification using permutation-invariant networks based on histograms. <em>NCA</em>, <em>37</em>(5), 3505-3520. (<a href='https://doi.org/10.1007/s00521-024-10721-1'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Quantification, also known as class prevalence estimation, is the supervised learning task in which a model is trained to predict the prevalence of each class in a given bag of examples. This paper investigates the application of deep neural networks for tasks of quantification in scenarios where it is possible to apply a symmetric supervised approach that eliminates the need for classification as an intermediate step, thus directly addressing the quantification problem. Additionally, it discusses existing permutation-invariant layers designed for set processing and assesses their suitability for quantification. Based on our analysis, we propose HistNetQ, a novel neural architecture that relies on a permutation-invariant representation based on histograms that is especially suited for quantification problems. Our experiments carried out in two standard competitions, which have become a reference in the quantification field, show that HistNetQ outperforms other deep neural network architectures designed for set processing, as well as the current state-of-the-art quantification methods. Furthermore, HistNetQ offers two significant advantages over traditional quantification methods: i) it does not require the labels of the training examples but only the prevalence values of a collection of training bags, making it applicable to new scenarios; and ii) it is able to optimize any custom quantification-oriented loss function.},
  archive      = {J_NCA},
  author       = {Pérez-Mon, Olaya and Moreo, Alejandro and Coz, Juan José del and González, Pablo},
  doi          = {10.1007/s00521-024-10721-1},
  journal      = {Neural Computing and Applications},
  month        = {2},
  number       = {5},
  pages        = {3505-3520},
  shortjournal = {Neural Comput. Appl.},
  title        = {Quantification using permutation-invariant networks based on histograms},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Can functional connectivity be used to refine structural connectivity strength by combining neural computational model and generative adversarial network?. <em>NCA</em>, <em>37</em>(5), 3489-3504. (<a href='https://doi.org/10.1007/s00521-024-10703-3'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Currently, structural connectivity (SC) strength is usually non-invasively estimated as streamline density or connection probability, via tractography based on diffusion magnetic resonance imaging (dMRI). However, nonnegligible tracking biases are still unavoidably introduced into SC strength computation, thus affecting exploration of anatomical mechanism underlying functional interactions. Here, we refined SC strength from empirical functional connectivity (FC) by combining neural computational model and deep-learning techniques. First, the neural computation model (Generic2dOscillator) was employed to generate simulated FCs from dMRI-tracked SCs. Then, these two kinds of connectivity matrices were used as samples to train generative adversarial network (GAN) incorporating graph attention (GAT). Last, empirical FCs were fed to the trained GAN to infer SCs. Global topological metrics of the predicted SCs have better consistency, no matter whether brains were parcellated into 62 or 132 subregions. More reasonable SC strength was obtained for the specific structural connections that are prone to tracking bias. These findings help us in understanding of the brain’s white matter organization, and in integrating dMRI and fMRI images to study coupling relationship between SC and FC networks.},
  archive      = {J_NCA},
  author       = {Wu, Zhanxiong and Yu, Jiangnan and Chen, Xuanheng},
  doi          = {10.1007/s00521-024-10703-3},
  journal      = {Neural Computing and Applications},
  month        = {2},
  number       = {5},
  pages        = {3489-3504},
  shortjournal = {Neural Comput. Appl.},
  title        = {Can functional connectivity be used to refine structural connectivity strength by combining neural computational model and generative adversarial network?},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Regulating the level of manipulation in text augmentation with systematic adjustment and advanced sentence embedding. <em>NCA</em>, <em>37</em>(5), 3473-3487. (<a href='https://doi.org/10.1007/s00521-024-10663-8'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Text augmentation, a method for generating samples by applying combinations, noise, and other manipulations to a small dataset, is a crucial technique in natural language processing research. It introduced diversity into the training process, thereby enabling the construction of robust models. The level of manipulation is the most important issue in text augmentation; low-level manipulation generates data similar to the original, resulting in inefficient augmentation because it cannot ensure diversity, whereas high-level manipulation causes reliability issues for labels and degrades the model’s performance. Therefore, this paper proposes a systematically adjustable text augmentation technique to address the “level of manipulation” issue. Specifically, it proposes a method for systematically adjusting the data candidate pool for manipulation to provide an appropriate level of randomness during the augmentation process. Furthermore, we propose an advanced sentence embedding methodology to achieve robust pseudo-labeling at the manipulation level. In other words, we leverage combined sentence embedding, which incorporates sentence embedding, document embedding, and XAI information from the original data to assign reliable pseudo-labels. We conducted performance comparisons with existing text augmentation approaches to validate the effectiveness of our proposed methodology. The experimental results demonstrate that the proposed method achieves the highest performance improvement across all the experimental datasets.},
  archive      = {J_NCA},
  author       = {Cha, Yuho and Lee, Younghoon},
  doi          = {10.1007/s00521-024-10663-8},
  journal      = {Neural Computing and Applications},
  month        = {2},
  number       = {5},
  pages        = {3473-3487},
  shortjournal = {Neural Comput. Appl.},
  title        = {Regulating the level of manipulation in text augmentation with systematic adjustment and advanced sentence embedding},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Eleven-point discrete perturbation-handling ZNN algorithm applied to tracking control of MIMO nonlinear system under various disturbances. <em>NCA</em>, <em>37</em>(5), 3455-3472. (<a href='https://doi.org/10.1007/s00521-024-10653-w'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The multiple-input–multiple-output nonlinear system (MIMO-NS) is a hot research topic in various science and application fields. To solve the tracking control (TC) problem of MIMO-NS effectively, a zeroing neuro-PID controller and a continuous perturbation-handling ZNN (CPHZNN) model are proposed. The proposed model robustly handles various internal perturbations generated by external disturbances. To achieve high-precision computation, a novel 11-point Zhang time discretization (ZTD) formula with order-5 precision is designed. By employing this formula, an 11-point discrete perturbation-handling ZNN (11P-DPHZNN) algorithm with order-6 precision is proposed, aiming at providing real-time solution to the TC problem. Theoretical analyses verify the convergence performance of the proposed 11P-DPHZNN algorithm under four types of perturbations. Moreover, numerical and comparative experiments substantiate the effectiveness, superiority, and perturbation resistance of the proposed 11P-DPHZNN algorithm under various perturbations.},
  archive      = {J_NCA},
  author       = {Huang, Meichun and Mao, Mingzhi and Zhang, Yunong},
  doi          = {10.1007/s00521-024-10653-w},
  journal      = {Neural Computing and Applications},
  month        = {2},
  number       = {5},
  pages        = {3455-3472},
  shortjournal = {Neural Comput. Appl.},
  title        = {Eleven-point discrete perturbation-handling ZNN algorithm applied to tracking control of MIMO nonlinear system under various disturbances},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Real-time drone detection framework based on advanced texture feature extraction and pattern recognition model using GUI. <em>NCA</em>, <em>37</em>(5), 3435-3454. (<a href='https://doi.org/10.1007/s00521-024-10440-7'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The increasing prevalence of drones has raised significant concerns regarding their potential for misuse in activities such as smuggling, terrorism, and unauthorized access to restricted airspace. Consequently, the development of robust and efficient drone detection systems has become paramount. Traditional detection methods such as radar and acoustic sensors have limitations in detecting small drones and can be costly to implement. This can lead to lower detection rates and increased security breaches and safety concerns. However, prevailing methodologies often falter in achieving real-time detection with requisite precision and efficacy, particularly within complex operational environments. Motivated by the imperative to surmount these challenges, our research endeavors to introduce a pioneering real-time drone detection framework. Our study encapsulates several seminal advancements. Firstly, we proffer a groundbreaking framework that synergizes advanced integration technique based on texture feature extraction and pattern recognition techniques for real-time drone detection to increase accuracy to detect drones in different conditions such as bad weather and low resolution. Secondly, we introduce an intuitive graphical user interface, enhancing the usability and accessibility of the system in real-time scenario. Lastly, through exhaustive evaluation and comparative analysis, we substantiate the superior performance of our framework in terms of accuracy, precision, and real-time detection capabilities compared to conventional DDS methodologies. Its stability and effectiveness render it a compelling solution for security-focused entities, notably those within air force and military systems. Our experimental results reveal a commendable accuracy rate of 97%, affirming the reliability and precision 98% and recall parameter 98% of our framework in accurately identifying and detecting drones, thus surpassing recent models in the field.},
  archive      = {J_NCA},
  author       = {Hussen, Noha and Salem, Mofreh and Eldesouky, Ali I. and Sakr, Noha and Elghamrawy, Sally},
  doi          = {10.1007/s00521-024-10440-7},
  journal      = {Neural Computing and Applications},
  month        = {2},
  number       = {5},
  pages        = {3435-3454},
  shortjournal = {Neural Comput. Appl.},
  title        = {Real-time drone detection framework based on advanced texture feature extraction and pattern recognition model using GUI},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Analyzing feature importance with neural-network-derived trees. <em>NCA</em>, <em>37</em>(5), 3419-3433. (<a href='https://doi.org/10.1007/s00521-024-10811-0'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This research focuses on the exploitation of transformations allowing the derivation of tree-like models from pre-trained neural networks to enhance their explainability and interpretability. Building upon the latest works that find that training a neural network supposes finding a partition of the input space in which to train local linear models, we use an analytical approach on these models and the decision boundaries in classification problems to study obtain an importance measure of each feature. A comparative analysis across models trained on diverse datasets, where problem explainability is relevant aims to derive an equivalent representation between a black-box model and a white-box one. From this new representation, a statistic-based methodology is proposed to determine the relevance of the input features in the problem, thereby gaining interpretability of the model at hand.},
  archive      = {J_NCA},
  author       = {Vieira-Manzanera, Ernesto and Patricio, Miguel A. and Berlanga, Antonio and Molina, José M.},
  doi          = {10.1007/s00521-024-10811-0},
  journal      = {Neural Computing and Applications},
  month        = {2},
  number       = {5},
  pages        = {3419-3433},
  shortjournal = {Neural Comput. Appl.},
  title        = {Analyzing feature importance with neural-network-derived trees},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A novel shilling attack on black-box recommendation systems for multiple targets. <em>NCA</em>, <em>37</em>(5), 3399-3417. (<a href='https://doi.org/10.1007/s00521-024-10798-8'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the digital era, e-platforms ubiquitously deploy recommendation systems, utilizing machine learning paradigms to tailor content according to user preferences and needs. Yet, the integrity of these systems is often compromised by shilling attacks, where malicious entities inject fake user profiles to skew product exposure and sales. This vulnerability has catalyzed research efforts to bolster the robustness and security of recommendation systems, primarily through controlled attacks and reinforcement training. This paper introduces an innovative approach to combat the inefficiency of traditional single-target shilling attacks. We propose a novel multi-target shilling strategy for black-box recommendation systems, capable of generating convincing and aggressive fake user profiles. Given the complexity of the recommendation system algorithms, we employ surrogate models to replicate and understand the target system’s behavior. The surrogate model is then subjected to attacks using our proposed methodology. An attack is deemed successful when it achieves an 80% or higher success rate against the surrogate, preluding an attack on the actual recommendation system. Our model, RWA-GAN, stands out by integrating two components: a generative adversarial network (GAN) for crafting realistic fake user profiles, and a mechanism to enhance attack success rates. This dual approach not only expedites the attack process but also ensures the generation of more authentic and well-distributed fake profiles, making the model easier to migrate. Additionally, the paper delineates several defense strategies against such attacks, contributing significantly to the discourse on enhancing the robustness of recommender systems.},
  archive      = {J_NCA},
  author       = {Liu, Shuangyu and Yu, Siyang and Li, Huan and Yang, Zhibang and Duan, Mingxing and Liao, Xiangke},
  doi          = {10.1007/s00521-024-10798-8},
  journal      = {Neural Computing and Applications},
  month        = {2},
  number       = {5},
  pages        = {3399-3417},
  shortjournal = {Neural Comput. Appl.},
  title        = {A novel shilling attack on black-box recommendation systems for multiple targets},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Automatic centroid initialization in k-means using artificial hummingbird algorithm. <em>NCA</em>, <em>37</em>(5), 3373-3398. (<a href='https://doi.org/10.1007/s00521-024-10764-4'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {K-means is a widely used technique that heavily relies on the initial cluster centroid location. Poorly chosen centroids can cause the algorithm to get trapped in suboptimal solutions. Additionally, determining the optimal number of clusters for large datasets is computationally expensive. To address these challenges, a recently developed Artificial Hummingbird Algorithm (AHA) is used to initialize cluster centroid locations and automatically determine the best estimate for the number of clusters. AHA simulates the specialized flight skills and intelligent foraging strategies of hummingbirds, striking a fine balance between exploration and exploitation during the search process. Unlike other data clustering approaches that use a fixed threshold in heuristic methods, we propose a dynamic threshold based on the variance of the data with respect to its centroids for activating cluster centroids in AHA. The data are automatically partitioned into k cluster centroids such that cohesion, measured by cluster diameters, and separation, measured by nearest neighbor distance, are optimized. The algorithm is tested on various datasets, including real-world data, fundamental clustering benchmarks, synthetic data, and high-dimensional data. To evaluate performance, metrics such as fitness value, inter-cluster distance, and intra-cluster distance were used. Results indicate that the proposed method ranked first and achieved superior clustering performance compared to state-of-the-art algorithms.},
  archive      = {J_NCA},
  author       = {Preeti and Deep, Kusum},
  doi          = {10.1007/s00521-024-10764-4},
  journal      = {Neural Computing and Applications},
  month        = {2},
  number       = {5},
  pages        = {3373-3398},
  shortjournal = {Neural Comput. Appl.},
  title        = {Automatic centroid initialization in k-means using artificial hummingbird algorithm},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Fuzzy rule-based variable neighborhood search algorithm for single-machine weighted earliness/tardiness scheduling with common due date. <em>NCA</em>, <em>37</em>(5), 3355-3371. (<a href='https://doi.org/10.1007/s00521-024-10844-5'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We introduce a fuzzy rule-based variable neighborhood search (FVNS) algorithm for a single-machine weighted earliness/tardiness scheduling problem with common due date. The problem’s optimal sequence has some properties. We adapt these properties to our proposed neighborhood structures. Some existing variants that use reinforcement learning, tabu search and roulette wheel selection within variable neighborhood search (VNS) are compared with our proposed FVNS. Using well-known test instances for the problem, our experimental study reveals that our proposed FVNS algorithm produces better solutions in view of solution quality than other considered VNS variants. While FVNS generally presents better solutions for the problem, we explore significant differences in algorithm solution quality by examining solutions in detail. We consider parameters in the experiment one by one to elucidate these variances. We discuss our findings about considered VNS variants in the experimental study.},
  archive      = {J_NCA},
  author       = {Arık, Oğuzhan Ahmet},
  doi          = {10.1007/s00521-024-10844-5},
  journal      = {Neural Computing and Applications},
  month        = {2},
  number       = {5},
  pages        = {3355-3371},
  shortjournal = {Neural Comput. Appl.},
  title        = {Fuzzy rule-based variable neighborhood search algorithm for single-machine weighted earliness/tardiness scheduling with common due date},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Simulation investigation of autonomous route planning for unmanned aerial vehicles based on an improved genetic algorithm. <em>NCA</em>, <em>37</em>(5), 3343-3354. (<a href='https://doi.org/10.1007/s00521-024-10817-8'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the rapid development of robotics technology and the increasing maturity of flight control technology, unmanned aerial vehicles are being widely used in an increasing number of fields. Path planning is an important component of unmanned aerial vehicle autonomous flight, and planning high-quality paths is the key to ensuring that unmanned aerial vehicles can safely and quickly reach their destination while performing tasks. This paper aims to simulate the autonomous route planning of unmanned aerial vehicles based on an improved genetic algorithm. In this paper, an evaluation of the use of a genetic algorithm for path planning and improvements to the adaptive genetic algorithm are proposed. In this study, a simulation experiment involving unmanned aerial vehicle autonomous route planning based on an improved genetic algorithm is performed. The experimental results in this paper show that through the comparative analysis of the two evolutionary algorithms, the improved genetic algorithm cost 22.91 s, and the traditional genetic algorithm cost 40.22 s. The consumption time increases by 43.04%. The results showed that the path graph obtained by this method performs better than that of the unoptimized genetic algorithm. Based on the above results, this paper proposes a multi-UAV route planning method based on a genetic algorithm. According to the experimental results, the improved genetic algorithm based on artificial intelligence is efficient and feasible for autonomous UAV route planning. For enterprises, secure and efficient data security and privacy management are crucial. This requires companies to use artificial intelligence technology when developing security products and services. The experimental results indicate that unmanned aerial vehicles based on improved genetic algorithms have played an important role in civil aviation. An increasing number of drones are being used for actual flight missions, which is expected to increase the convenience of unmanned aerial vehicles.},
  archive      = {J_NCA},
  author       = {Cao, Zhengyang},
  doi          = {10.1007/s00521-024-10817-8},
  journal      = {Neural Computing and Applications},
  month        = {2},
  number       = {5},
  pages        = {3343-3354},
  shortjournal = {Neural Comput. Appl.},
  title        = {Simulation investigation of autonomous route planning for unmanned aerial vehicles based on an improved genetic algorithm},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A multi-criteria sustainability performance assessment based on the extended CRADIS method under intuitionistic fuzzy environment: A case study of turkish non-life insurers. <em>NCA</em>, <em>37</em>(5), 3317-3342. (<a href='https://doi.org/10.1007/s00521-024-10803-0'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {ESG (environmental, social and governance) indicators play a strategically vital role in improving the sustainability performance of companies. A truly sustainable company could succeed and survive by taking into account the environmental, social, and governance aspects of sustainability in line with the goals and values of society within the framework of corporate social responsibility. However, no past study has explored all aspects of sustainability, particularly in the context of evaluating insurance companies in emerging economies. Due to climate change and population growth, the need for products and services provided by insurance companies is increasing day by day. To address this issue, the current study puts forward an intuitionistic fuzzy (IF) decision support framework for assessing the sustainability performance of insurance companies based on ESG factors. More specifically, the present paper develops a novel fuzzy multi-criteria decision-making (MCDM) methodology to rank and assess the ESG performance of non-life insurance companies in Turkey through the combination of the compromise ranking of alternatives from distance to ideal solution (CRADIS) framework and IF sets, which results in a new approach called intuitionistic fuzzy CRADIS (IF-CRADIS). Concerning the results, human right, workforce, and environmental innovations are the vital drivers for the ESG performance of insurance companies. Additionally, the results of IF-CRADIS indicate that Türkiye Sigorta is the top-performing insurer, whereas Eureko Sigorta is the one with the lowest-performance. Finally, sensitivity analysis demonstrates that the introduced model has a maximally consistent and stable MCDM framework.},
  archive      = {J_NCA},
  author       = {Işık, Özcan and Adalar, İbrahim},
  doi          = {10.1007/s00521-024-10803-0},
  journal      = {Neural Computing and Applications},
  month        = {2},
  number       = {5},
  pages        = {3317-3342},
  shortjournal = {Neural Comput. Appl.},
  title        = {A multi-criteria sustainability performance assessment based on the extended CRADIS method under intuitionistic fuzzy environment: A case study of turkish non-life insurers},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A new discrete differential evolution algorithm coupled with simulation–optimization model for groundwater management problems. <em>NCA</em>, <em>37</em>(5), 3303-3316. (<a href='https://doi.org/10.1007/s00521-024-10785-z'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Discrete differential evolution (DDE) is a promising algorithm specifically developed to solve discrete problems. In this study, we aim to apply DDE to groundwater management problems and to compare its performance and discrete space search capabilities with the well-known genetic algorithm (GA) techniques. Local search process was used to enhance the performance of GA algorithm. Metaheuristic algorithms are used for finding location of wells as a hybrid optimization procedure. Two examples from the groundwater management literature were selected to test the performance of the algorithm. The main novelty and objective of this study lie in the comparison of the discrete space search capabilities of the mentioned metaheuristics algorithms using the groundwater management problems. In the first test example, discrete space search performances of algorithms are 15% and 93% for GA and DDE, respectively. In the second test example, DDE exhibited a significantly higher test results (77%) compared to GA (1%). The analysis revealed that GA often prematurely converged and was insufficient to produce the optimum result. DDE reaches the solution considerably faster than the other algorithms. The results showed the superior performance of DDE in the discrete space. As the problem becomes more discrete, the performance of the DDE algorithm in finding the optimum solution increases considerably. Thus, it can be revealed that DDE can also be applied to a wider range of water resource management problems as an effective discrete optimization algorithm.},
  archive      = {J_NCA},
  author       = {Şahin, Onur Güngör and Gurarslan, Gurhan and Gündüz, Orhan},
  doi          = {10.1007/s00521-024-10785-z},
  journal      = {Neural Computing and Applications},
  month        = {2},
  number       = {5},
  pages        = {3303-3316},
  shortjournal = {Neural Comput. Appl.},
  title        = {A new discrete differential evolution algorithm coupled with simulation–optimization model for groundwater management problems},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Chemical environment adaptive learning for optical band gap prediction of doped graphitic carbon nitride nanosheets. <em>NCA</em>, <em>37</em>(5), 3287-3301. (<a href='https://doi.org/10.1007/s00521-024-10775-1'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This study presents a new machine learning algorithm, named Chemical Environment Graph Neural Network (ChemGNN), designed to accelerate materials property prediction and advance new materials discovery. Graphitic carbon nitride (g-C3N4) and its doped variants have gained significant interest for their potential as optical materials. Accurate prediction of their band gaps is crucial for practical applications; however, traditional quantum simulation methods are computationally expensive and challenging to explore the vast space of possible doped molecular structures. The proposed ChemGNN leverages the learning ability of current graph neural networks (GNNs) to satisfactorily capture the characteristics of atoms' chemical environment underlying complex molecular structures. Our experimental results demonstrate more than 100% improvement in band gap prediction accuracy over existing GNNs on g-C3N4. Furthermore, the general ChemGNN model can precisely foresee band gaps of various doped g-C3N4 structures, making it a valuable tool for performing high-throughput prediction in materials design and development.},
  archive      = {J_NCA},
  author       = {Chen, Chen and Xu, Enze and Yang, Defu and Yan, Chenggang and Wei, Tao and Chen, Hanning and Wei, Yong and Chen, Minghan},
  doi          = {10.1007/s00521-024-10775-1},
  journal      = {Neural Computing and Applications},
  month        = {2},
  number       = {5},
  pages        = {3287-3301},
  shortjournal = {Neural Comput. Appl.},
  title        = {Chemical environment adaptive learning for optical band gap prediction of doped graphitic carbon nitride nanosheets},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). White blood cell segmentation using U-net and its variants to improve leukemia diagnosis. <em>NCA</em>, <em>37</em>(5), 3265-3286. (<a href='https://doi.org/10.1007/s00521-024-10757-3'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Numerous computer-aided leukemia detection methods have been introduced to overcome the limitations of clinical diagnosis procedures. The precision of computer-aided leukemia detection highly depends on the accurate segmentation of white blood cells (WBCs) from the stained whole slide image (WSI). This paper proposes WBC segmentation from WSI using U-Net and its variants. The major contributions to this paper are as follows. First, WBC segmentation is proposed using U-Net, U-Net++ and three transfer learning-based U-Net models, namely U-Net-VGG16, U-Net-VGG19, and U-Net-ResNet. Second, a comprehensive and comparative experimental analysis of the proposed WBC segmentation approaches is presented using four evaluation parameters, namely Dice coefficient, Intersection over Union (IoU), precision, and recall. Third, WBC segmentation approaches are also evaluated using four loss functions such as binary cross-entropy, focal, Dice, and IoU to identify the most effective loss function for WBC segmentation. Finally, the trained models’ ability to perform WBC segmentation is validated to assess their practical applicability. The empirical results reveal that U-Net-VGG16 and U-Net-VGG19 achieve a high Dice coefficient of 90% and IoU of 83%. Besides, U-Net++ achieves high precision and recall of 98% and 70%, respectively. Although the results reveal that transfer learning-based U-Net models perform better, deployment of the trained model shows that the U-Net segments WBC more precisely than transfer learning-based U-Net models.},
  archive      = {J_NCA},
  author       = {Joshi, Vivek C. and Mehta, Mayuri A. and Kotecha, Ketan},
  doi          = {10.1007/s00521-024-10757-3},
  journal      = {Neural Computing and Applications},
  month        = {2},
  number       = {5},
  pages        = {3265-3286},
  shortjournal = {Neural Comput. Appl.},
  title        = {White blood cell segmentation using U-net and its variants to improve leukemia diagnosis},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A novel method based on hybrid deep learning with explainability for olive fruit pest forecasting. <em>NCA</em>, <em>37</em>(5), 3245-3264. (<a href='https://doi.org/10.1007/s00521-024-10731-z'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Predicting the occurrence of crop pests is becoming a crucial task in modern agriculture to facilitate farmers’ decision-making. One of the most significant pests is the olive fruit fly, a public concern because it causes damage that compromises oil quality, increasing acidity and altering its flavor. This paper proposes a hybrid deep learning model to predict the presence of olive flies in crops. This model is based on an autoencoder and an automated deep feed-forward neural network. First, the autoencoder neural network learns a representation of the data and then the automated deep feed-forward neural network automatically determines the best values for the hyperparameters in order to obtain the prediction of the number of flies caught in traps from the dataset generated by the autoencoder. On the other hand, farmers to trust the proposed deep learning models need these models to be explainable. Thus, explainable artificial intelligence techniques are applied to the produced models to interpret the results. Results using a dataset from different sources such as satellite image band data, vegetation indices, and meteorological variables are reported. The performance of the proposed model has been compared with classical benchmark algorithms and a deep learning model recently published in the literature. In addition, the comparison includes the automated deep feed-forward neural network individually to show how the autoencoder network improves the accuracy of predictions.},
  archive      = {J_NCA},
  author       = {Chacón-Maldonado, A. M. and Melgar-García, L. and Asencio-Cortés, G. and Troncoso, A.},
  doi          = {10.1007/s00521-024-10731-z},
  journal      = {Neural Computing and Applications},
  month        = {2},
  number       = {5},
  pages        = {3245-3264},
  shortjournal = {Neural Comput. Appl.},
  title        = {A novel method based on hybrid deep learning with explainability for olive fruit pest forecasting},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). GCINet: Global convolution interaction network with a pre-trained reversible normalization method for long-term time series forecasting. <em>NCA</em>, <em>37</em>(5), 3227-3244. (<a href='https://doi.org/10.1007/s00521-024-10692-3'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Long-term time series forecasting is crucial in various domains, such as weather forecasting and disease forecasting. With the rise of Transformer-based models, we have witnessed a significant improvement in long-term prediction accuracy, as these models use self-attention to capture long-term temporal features in the data during feature extraction. However, this approach has several drawbacks: Firstly, self-attention has quadratic complexity, and reducing its complexity often results in reduced prediction accuracy. Secondly, the distribution of time series data frequently changes over time, a phenomenon known as distribution shift, which is not taken into consideration in most Transformer-based models. This leads to decreased prediction accuracy as the distribution deviates from the one used for model training. Therefore, we have developed a new time series forecasting model, GCINet, which introduces a new mode of interaction based on a convolutional network. This captures both long-term and short-term temporal features while reducing complexity during feature extraction. Additionally, GCINet includes an Inverse Trained Normalization method to address distribution shift, bringing a fresh perspective to the design of time series forecasting models. Our experiments on long-term forecasting tasks demonstrate that GCINet outperforms other commonly used models.},
  archive      = {J_NCA},
  author       = {Fan, Jin and Yang, Baoshun and Sun, Danfeng and Liu, Jie and Chen, Qikai and Wang, Zhipeng and Wu, Jia},
  doi          = {10.1007/s00521-024-10692-3},
  journal      = {Neural Computing and Applications},
  month        = {2},
  number       = {5},
  pages        = {3227-3244},
  shortjournal = {Neural Comput. Appl.},
  title        = {GCINet: Global convolution interaction network with a pre-trained reversible normalization method for long-term time series forecasting},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Performance analysis of machine learning algorithms for hybrid power generation prediction. <em>NCA</em>, <em>37</em>(5), 3203-3225. (<a href='https://doi.org/10.1007/s00521-024-10805-y'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper aims to analyze the energy potentials of Çanakkale and Balıkesir provinces and evaluate the performance of renewable energy forecasting models. The first section presents a comprehensive analysis of wind and solar energy potentials. ANOVA analysis is used to evaluate the effects of meteorological variables on energy production. In the wind power analysis, air density and wind speed were found to be critical, emphasizing the importance of these factors in determining wind energy potential. In the solar panel power analysis, temperature, sunshine duration, solar radiation, and ambient temperature were critical. Regression analysis showed that most of the variability in turbine power is caused by wind speed. In the second section, based on the results of the previous analysis, an evaluation of the performance of the machine learning models is presented. Comparisons between the actual data and the predicted values are made using metrics such as coefficient of determination, mean absolute error, and root mean squared error. In addition, a custom prediction model is used to identify the features that affect the model's outputs. The third section evaluates the regional impacts. Using six different forecasting models, a regional performance analysis is performed for wind turbine power and solar panel power in Çanakkale and Balıkesir provinces. The analysis shows that high coefficients of determination are obtained for both provinces. As a result, a comparison of model performances is made in the fourth section. The XGBoost algorithm showed high accuracy on the training set and performed equally well with the other algorithms on the test set. The Random Forest model performed the best in terms of MAE. This inferential analysis provides important information for understanding the energy potential of Çanakkale and Balıkesir provinces and evaluating the performance of machine learning models. The paper contributes to regional energy planning and sheds light on practical applications for more efficient use of renewable energy resources.},
  archive      = {J_NCA},
  author       = {Sarıışık, Gencay and Öğütlü, Ahmet Sabri},
  doi          = {10.1007/s00521-024-10805-y},
  journal      = {Neural Computing and Applications},
  month        = {2},
  number       = {5},
  pages        = {3203-3225},
  shortjournal = {Neural Comput. Appl.},
  title        = {Performance analysis of machine learning algorithms for hybrid power generation prediction},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Supply chain management under VMI strategy using internet of things and smart contracts. <em>NCA</em>, <em>37</em>(5), 3167-3201. (<a href='https://doi.org/10.1007/s00521-024-10791-1'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Today, providing products in supply chains (SCs) with minimum delays and costs is a challenging issue which attracts the attention of many researchers. Additionally, as SC becomes more complex, its coordination is getting more difficult. Some products such as tire play a critical role in the safety of people, and a part of accidents occur due to defects of the tire. Releasing used products into the environment causes many environmental and health problems; that is why, governments make laws to prevent it. Vendor-managed inventory (VMI) is a strategy for coordinating partners in a SC in order to deliver required products to downstream echelons with the least delay. In this paper, we are going to show how to implement VMI in business-to-business and business-to-customer relations in a multi-echelon SC by using novel Industry 4 technologies including Internet of Things (IoT) and smart contracts. Since the implementation of the VMI strategy requires accurate and timely data, such as inventory levels, point-of-sale data, and maybe returned products data, we have recommended using IoT technology as a tool for receiving timely data. Furthermore, we have proposed a smart contract for implementing and executing the VMI rules and conventions in an automated system for increasing the clarity of operations and making trust among SC members. The addressed IoT-based system architecture is described along with the Ethereum blockchain-based smart contract structure. The smart contract was developed utilizing Solidity language in the Remix IDE environment. The contract is tested by using sample data and the results are reported.},
  archive      = {J_NCA},
  author       = {Bahrampour, Najmeh and Seifbarghy, Mehdi and Pasandideh, Seyed Hamid Reza},
  doi          = {10.1007/s00521-024-10791-1},
  journal      = {Neural Computing and Applications},
  month        = {2},
  number       = {5},
  pages        = {3167-3201},
  shortjournal = {Neural Comput. Appl.},
  title        = {Supply chain management under VMI strategy using internet of things and smart contracts},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Phrase-level emotion intensity detection of text using lexicon-based unit circle and pipelined neural networks approaches. <em>NCA</em>, <em>37</em>(5), 3151-3166. (<a href='https://doi.org/10.1007/s00521-024-10786-y'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Emotions have a significant impact on how people make decisions. Due to its potential applications in various fields, emotion intensity detection has attracted a lot of attention recently. Several methods have been proposed in the past using natural language processing to recognize emotions from the text. These methods include the keyword-based approach, the lexicon-based approach and the machine learning approaches. Nevertheless, there were a few limitations with the lexicon-based unit circle approach, as it focuses on detecting emotion intensity at the word-level. This research proposes an integrated model by considering the advantages of the lexicon-based unit circle and pipelined neural networks approaches. The model is trained on a large corpus of text data. The proposed approach aims to determine emotion intensity, analyzing text at the phrase-level, thereby achieving higher performance in terms of accuracy and F1-score compared to other models.},
  archive      = {J_NCA},
  author       = {Patil, Bhuvaneshwari Ajit and Vadlakunta, Vishnu Vardhan and Guruvu, Poojaa Pranathi and Mathew, Meenu and Prakash, Jay},
  doi          = {10.1007/s00521-024-10786-y},
  journal      = {Neural Computing and Applications},
  month        = {2},
  number       = {5},
  pages        = {3151-3166},
  shortjournal = {Neural Comput. Appl.},
  title        = {Phrase-level emotion intensity detection of text using lexicon-based unit circle and pipelined neural networks approaches},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Predictive VNF auto-scaling based on genetic programming. <em>NCA</em>, <em>37</em>(5), 3129-3150. (<a href='https://doi.org/10.1007/s00521-024-10763-5'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the continuous growth of cloud computing and virtualization technology, network function virtualization (NFV) techniques have been significantly enhanced. NFV has many advantages such as simplified services, providing more flexible services, and reducing network capital and operational costs. However, it also poses new challenges that need to be addressed. A challenging problem with NFV is resource management, since the resources required by each virtualized network function (VNF) change with dynamic traffic variations, requiring automatic scaling of VNF resources. Due to the resource consumption importance, it is essential to propose an efficient resource auto-scaling method in the NFV networks. Inadequate or excessive utilization of VNF resources can result in diminished performance of the entire service chain, thereby affecting network performance. Therefore, predicting VNF resource requirements is crucial for meeting traffic demands. VNF behavior in networks is complex and nonlinear, making it challenging to model. By incorporating machine learning methods into resource prediction models, network service performance can be improved by addressing this complexity. As a result, this paper introduces a new auto-scaling architecture and algorithm to tackle the predictive VNF problem. Within the proposed architecture, there is a predictive VNF auto-scaling engine that comprises two modules: a predictive task scheduler and a predictive VNF auto-scaler. Furthermore, a prediction engine with a VNF resource predictor module has been designed. In addition, the proposed algorithm called GPAS is presented in three phases, VNF resource prediction using genetic programming (GP) technique, task scheduling and decision-making, and auto-scaling execution. The GPAS method is simulated in the KSN framework, a network environment based on NFV/SDN. In the evaluation results, the GPAS method shows better performance in SLA violation rate, resource usage, and response time when compared to both ElasticSFC and Basic methods (without any auto-scaling algorithm). Furthermore, in terms of the resource prediction model, the GP method employing the ANN regressor has demonstrated superior outcomes compared to alternative methods that were tested. Considering the results, it can be concluded that resource prediction in task scheduling and decision-making in advance is an effective and efficient approach to run VNF auto-scaling.},
  archive      = {J_NCA},
  author       = {Nikbazm, Rojia and Ahmadi, Mahmood},
  doi          = {10.1007/s00521-024-10763-5},
  journal      = {Neural Computing and Applications},
  month        = {2},
  number       = {5},
  pages        = {3129-3150},
  shortjournal = {Neural Comput. Appl.},
  title        = {Predictive VNF auto-scaling based on genetic programming},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Self-distillation framework for improving fake speech detection in the domain variability scenario. <em>NCA</em>, <em>37</em>(5), 3111-3127. (<a href='https://doi.org/10.1007/s00521-024-10760-8'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Robust fake speech detection systems are crucial in an era where audio recordings can be easily altered or developed due to advancements in technology. The potential impact of this technology could be devastating due to its susceptibility to misuse. It can lead to political and economic instability, severe financial repercussions, the spread of misinformation, defamation, and security breaches in ASV systems and facilitate theft and fraud. Further, existing detection systems face a key issue; most state-of-the-art systems only provide results on the corpus they are trained on but fail in the domain variability scenario. The goal of this study is to enable a system to generalize across various domains, enhancing its reliability in real-world scenarios and thereby solidifying the authenticity of speech, making speech-based systems more dependable. All three audio spoofing scenarios are considered, logical access-based (LA) attacks including TTS and voice conversion (VC) techniques, spoofing attacks generated in real physical space, namely physical access (PA) attacks (replay, mimicry) and advanced deepfake technologies. The proposed system put forth in this study achieves commendable performance on three diverse test datasets. Notably, EER of 0.286, 0.337 and 0.371 is achieved on the In-The-Wild dataset with the proposed system implemented on ResNet, ECANet and SENet.},
  archive      = {J_NCA},
  author       = {Samhita, V. and Viju, Vaishnav and Bharathi, B.},
  doi          = {10.1007/s00521-024-10760-8},
  journal      = {Neural Computing and Applications},
  month        = {2},
  number       = {5},
  pages        = {3111-3127},
  shortjournal = {Neural Comput. Appl.},
  title        = {Self-distillation framework for improving fake speech detection in the domain variability scenario},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Advancing mental health detection in texts via multi-task learning with soft-parameter sharing transformers. <em>NCA</em>, <em>37</em>(5), 3077-3110. (<a href='https://doi.org/10.1007/s00521-024-10753-7'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, mental health issues have profoundly impacted individuals’ well-being, necessitating prompt identification and intervention. Existing approaches grapple with the complex nature of mental health, facing challenges like task interference, limited adaptability, and difficulty in capturing nuanced linguistic expressions indicative of various conditions. In response to these challenges, our research presents three novel models employing multi-task learning (MTL) to understand mental health behaviors comprehensively. These models encompass soft-parameter sharing-based long short-term memory with attention mechanism (SPS-LSTM-AM), SPS-based bidirectional gated neural networks with self-head attention mechanism (SPS-BiGRU-SAM), and SPS-based bidirectional neural network with multi-head attention mechanism (SPS-BNN-MHAM). Our models address diverse tasks, including detecting disorders such as bipolar disorder, insomnia, obsessive-compulsive disorder, and panic in psychiatric texts, alongside classifying suicide or non-suicide-related texts on social media as auxiliary tasks. Emotion detection in suicide notes, covering emotions of abuse, blame, and sorrow, serves as the main task. We observe significant performance enhancement in the primary task by incorporating auxiliary tasks. Advanced encoder-building techniques, including auto-regressive-based permutation and enhanced permutation language modeling, are recommended for effectively capturing mental health contexts’ subtleties, semantic nuances, and syntactic structures. We present the shared feature extractor called shared auto-regressive for language modeling (S-ARLM) to capture high-level representations that are useful across tasks. Additionally, we recommend soft-parameter sharing (SPS) subtypes-fully sharing, partial sharing, and independent layer-to minimize tight coupling and enhance adaptability. Our models exhibit outstanding performance across various datasets, achieving accuracies of 96.9%, 97.4%, and 98%, and F1 scores of 93.8%, 94%, and 94.6% for distinct mental health-related datasets, respectively. We conduct a thorough comparative analysis to evaluate the models’ applicability and effectiveness across diverse contexts and platforms, supported by ablation tests highlighting essential components and confirming their superiority over state-of-the-art models in the MTL context.},
  archive      = {J_NCA},
  author       = {Kodati, Dheeraj and Tene, Ramakrishnudu},
  doi          = {10.1007/s00521-024-10753-7},
  journal      = {Neural Computing and Applications},
  month        = {2},
  number       = {5},
  pages        = {3077-3110},
  shortjournal = {Neural Comput. Appl.},
  title        = {Advancing mental health detection in texts via multi-task learning with soft-parameter sharing transformers},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Silhouette coefficient-based weighting k-means algorithm. <em>NCA</em>, <em>37</em>(5), 3061-3075. (<a href='https://doi.org/10.1007/s00521-024-10706-0'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The classical k-means algorithm utilizes all features of the data equally for clustering. It fails to distinguish the roles played by different features in clustering, resulting in poor adaptability. In this paper, we propose a Silhouette coefficient-based weighting k-means algorithm (WKBSC) that automatically adjusts feature weights when clustering. In this algorithm, the optimal clustering is transformed into an optimization problem, which aims to maximize the Silhouette coefficient of the outputting clusters. The feature weights are updated during the iterations of k-means. Compared with eight clustering methods on ten datasets, our method achieves the highest score. We also insight into four datasets and reveal the factors that influence the performances of our algorithms. In summary, the experiment results verify that the proposed algorithm can automatically adjust feature weights to reflect the importance of features in clustering, and can be used for clustering complex real data.},
  archive      = {J_NCA},
  author       = {Lai, Huixia and Huang, Tao and Lu, BinLong and Zhang, Shi and Xiaog, Ruliang},
  doi          = {10.1007/s00521-024-10706-0},
  journal      = {Neural Computing and Applications},
  month        = {2},
  number       = {5},
  pages        = {3061-3075},
  shortjournal = {Neural Comput. Appl.},
  title        = {Silhouette coefficient-based weighting k-means algorithm},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Deep learning-based ovarian cyst classification and abnormality detection using convolutional neural networks. <em>NCA</em>, <em>37</em>(5), 3047-3059. (<a href='https://doi.org/10.1007/s00521-024-10810-1'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The goal of this study was to design a computer-aided diagnosis system using deep learning network for ovarian cyst classification and abnormality detection. An ovarian cyst is a sac filled with fluid or semisolid material that forms on or within one or both the ovaries. One in every 4–5 women in India suffers from different types of ovarian cysts (20–25% incidence). The doctor-population ratio of 0.62:1000 further worsens this situation for the most populous country on the planet. The main motivation for this work was to ease this situation by providing an automated system which can act as an aid to doctors in diagnosing ovarian cysts for different complications requiring immediate medical intervention. It can also be used as teaching aid for the gynecology/radiology students to identify the sonographic variations being exhibited by different ovarian cyst types. A new deep learning convolutional neural network dedicated for Ovarian cysts classification and abnormality detection, OvarianCystNet with 87 layers is designed using Deep Learning Toolbox of MATLAB R 2022b. Most of deep learning ovarian cyst classification systems are either using only pre-trained networks like VGG16 or inception V3. We have designed a dedicated deep learning network specifically for ovarian cyst classification. The classification results for OvarianCystNet show promising results and validate its use as tool for ovarian cyst diagnosis. A validation accuracy of 92.06% is achieved. It is 18.68% more accurate in comparison to VGG19, 7.65% more accurate in comparison with VGG16, 3.47% more accurate in comparison with MobileNetV2 and 1.49% more accurate in comparison with Resnet50. The training accuracy, validation loss, confusion matrix, receiver operating characteristics, and area under the curve is compared with four pre-trained deep learning networks namely MobilenetV2, Resnet50, VGG16 and VGG19. A dataset consisting of around 1319 images of ovarian cyst ultrasound images was collected from a leading obstetrics and gynecological hospital by several operators on Voluson-S8 a 4D ultrasound machine. Voluson-S8 is manufactured by general electric which is specifically used for obstetrics and gynecology related scans. All images in the dataset were manually labeled by an expert obstetrics and gynecology doctor with over 15 years of experience. Images were divided into five classes.},
  archive      = {J_NCA},
  author       = {Sood, Munish and Puthooran, Emjee and Jain, Nishant},
  doi          = {10.1007/s00521-024-10810-1},
  journal      = {Neural Computing and Applications},
  month        = {2},
  number       = {5},
  pages        = {3047-3059},
  shortjournal = {Neural Comput. Appl.},
  title        = {Deep learning-based ovarian cyst classification and abnormality detection using convolutional neural networks},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Segmentation, classification and interpretation of breast cancer medical images using human-in-the-loop machine learning. <em>NCA</em>, <em>37</em>(5), 3023-3045. (<a href='https://doi.org/10.1007/s00521-024-10799-7'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper explores the application of Human-in-the-Loop (HITL) strategies in the training of machine learning models in the medical domain. In this case, a doctor-in-the-loop approach is proposed to leverage human expertise in dealing with large and complex data. Specifically, the paper deals with the use of Whole Slide Imaging (WSI) for the analysis and prediction of the genomic subtype of breast cancer. Three different tasks were developed: segmentation of histopathological images, classification of these images regarding the genomic subtype of the cancer, and finally, interpretation of the machine learning results. The involvement of a pathologist helped us to develop a better segmentation model trying to group areas to make it more useful for further diagnosis. Because the classification models underperformed due to the complexity of the problem and insufficient data for certain cancer types, we focus our efforts in using the feedback from the pathologist to enhance model interpretability through a HITL hyperparameter optimization process.},
  archive      = {J_NCA},
  author       = {Vázquez-Lema, David and Mosqueira-Rey, Eduardo and Hernández-Pereira, Elena and Fernandez-Lozano, Carlos and Seara-Romera, Fernando and Pombo-Otero, Jorge},
  doi          = {10.1007/s00521-024-10799-7},
  journal      = {Neural Computing and Applications},
  month        = {2},
  number       = {5},
  pages        = {3023-3045},
  shortjournal = {Neural Comput. Appl.},
  title        = {Segmentation, classification and interpretation of breast cancer medical images using human-in-the-loop machine learning},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Comprehensive analysis study of techniques in different domains for turkish music genre classification task. <em>NCA</em>, <em>37</em>(5), 3005-3021. (<a href='https://doi.org/10.1007/s00521-024-10771-5'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Nowadays, models or algorithms are used in the analysis process as the amount of data increases. Depending on the sectors, techniques in domains such as NLP, image processing, and voice analysis can be used. In this study, analyses were applied in these domains to classify music genres on the Turkish music dataset and these domains were compared. To perform the first analysis, the voice characteristic features of the songs were extracted and the success of machine learning (ML) algorithms and the CNN model were analyzed. For the next analysis, spectrograms of the songs were extracted and Keras application models were trained with transfer learning. During these analyses, audio segmentation and feature reduction techniques were also performed on the songs to analyze them. The last analysis applied textual analysis with song lyrics to the NLP domain. After preprocessing, the vector representations of these lyrics were obtained and the success of ML algorithms and the CNN model was measured. At the same time, large language models were fine-tuned and the success of these models was analyzed. As a result of all analyses, it has been shown that the ML method with the application of audio segmentation and feature reduction for voice analysis is the most successful.},
  archive      = {J_NCA},
  author       = {Guven, Zekeriya Anil},
  doi          = {10.1007/s00521-024-10771-5},
  journal      = {Neural Computing and Applications},
  month        = {2},
  number       = {5},
  pages        = {3005-3021},
  shortjournal = {Neural Comput. Appl.},
  title        = {Comprehensive analysis study of techniques in different domains for turkish music genre classification task},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Design of an evolutionary optimization networks for transmission dynamics and control of bovine brucellosis in cattle. <em>NCA</em>, <em>37</em>(5), 2987-3003. (<a href='https://doi.org/10.1007/s00521-024-10743-9'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Animals worldwide are harmed by the bacterial illness brucellosis, which is dangerous, expensive to treat, and contagious. In this paper, we provide a numerical solution for studying bovine brucellosis in cattle using a neural network (NN). This method combines genetic algorithms (GA) with sequential quadratic programming (SQP) and a log-sigmoid activation function, i.e. NN-GASQP-LS. Several researchers have explored the brucellosis model in their research articles by using many conventional methods. The present work makes extensive use of hybrid recommended methods to address this problem, highlighting the innovative nature of the study. To examine the viability and effectiveness of NN-GASQP-LS, Adam’s technique is used. Theil’s inequality coefficients, root mean squared error, and mean of absolute deviation values have all been evaluated analytically. Statistical procedures are used to further confirm the convergence and accuracy of the NN-GASQP-LS technique.},
  archive      = {J_NCA},
  author       = {Shoaib, Muhammad and Kainat, Saba and Nisar, Kottakkaran Sooppy and Raja, Muhammad Asif Zahoor},
  doi          = {10.1007/s00521-024-10743-9},
  journal      = {Neural Computing and Applications},
  month        = {2},
  number       = {5},
  pages        = {2987-3003},
  shortjournal = {Neural Comput. Appl.},
  title        = {Design of an evolutionary optimization networks for transmission dynamics and control of bovine brucellosis in cattle},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). AC/DC power systems planning comprising voltage source converters using an enhanced symbiotic organisms search algorithm. <em>NCA</em>, <em>37</em>(5), 2945-2986. (<a href='https://doi.org/10.1007/s00521-024-10722-0'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The concept of distributed energy, where different energy sources are combined in remote locations, forms the basis of today's power systems overall energy production logic. Furthermore, advancements in power electronic infrastructures have emphasized their increased utilization within power systems. In particular, the transition from current source converters (CSC) technology to voltage source converters (VSC) technology has made it easier to integrate power grids with different characteristics into existing power systems. High voltage direct current (HVDC) transmission applications also play a significant role in this integration. In these increasingly complex power systems with various infrastructures and applications, maintaining a sustainable, secure, economical, and environmentally-friendly balance between supply and demand becomes more challenging using classical approaches. In this study, a metaheuristic algorithm is proposed for solving the power flow problems in hybrid AC/DC power systems that include VSC-based, Multi-Terminal HVDC grids. The proposed algorithm is an enhanced version of the symbiotic organisms search (SOS) algorithm and is named di-SOS (diversity improved SOS with Parazite RFDB) algorithm. To demonstrate the effectiveness of the developed algorithm, comparisons were made with SOS algorithm variants and 15 different metaheuristic algorithms found in the literature using various test functions. Nonparametric Wilcoxon signed-rank tests and Friedman tests were performed the compared algorithms and in the comparison between SOS algorithm variants, the di_1-SOS variant of the di_SOS algorithm performed the best with an algorithm score of 2.245. In the comparison with the other 15 metaheuristic algorithms, the di_1-SOS algorithm ranked first with a ranking score of 4.525, demonstrating its success in solving classical test functions. Finally, the algorithm was employed to address power flow problems concerns within hybrid AC/DC power systems, employing altered instances of the IEEE 14-bus and IEEE 30-bus test networks. The acquired outcomes substantiated the efficacy of the algorithm in strategic formulation of AC/DC power systems and in resolving intricate real-world engineering problems, characterized by nonlinearities and constraints.},
  archive      = {J_NCA},
  author       = {Battal, Onur and Güvenç, Uğur},
  doi          = {10.1007/s00521-024-10722-0},
  journal      = {Neural Computing and Applications},
  month        = {2},
  number       = {5},
  pages        = {2945-2986},
  shortjournal = {Neural Comput. Appl.},
  title        = {AC/DC power systems planning comprising voltage source converters using an enhanced symbiotic organisms search algorithm},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Hierarchical cluster-based IELM for financial distress prediction with imbalanced data. <em>NCA</em>, <em>37</em>(5), 2925-2943. (<a href='https://doi.org/10.1007/s00521-024-10716-y'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Financial distress (FD) occurs when external economic factors or internal financial challenges threaten the stability of a business, often leading to financial difficulties or bankruptcy. Predicting FD is critical, but existing methods suffer from limitations such as a narrow focus on input variables, inadequate emphasis on key financial indicators, and challenges in handling large, imbalanced datasets. Besides, most of the existing techniques often fail to minimize computational complexity while enhancing forecast accuracy, necessitating the development of more effective models. Recognizing these challenges, this work proposes the APCIELM model, a multistage approach specifically designed for FDP with imbalanced datasets. Davies–Bouldin index-based hierarchical K-means clustering approach is devised to group data samples, followed by a new strategic differentiation between minority and majority classes. The proposed Rotation Affinity Propagation Cluster-based hypothesis determines the necessity for oversampling within specific clusters based on data distribution characteristics. Finally, an incremental extreme learning machine (IELM) model is employed for FDP which optimizes computational efficiency by eliminating ineffective calculations while maintaining high prediction performance. The results demonstrate that the proposed multistage prediction model outperforms single-stage models when dealing with imbalanced data. The efficiency of APCIELM model is evaluated using different metrics, including accuracy, precision, recall, F-score, and time complexity. The comprehensive analysis reveals the superior performance of the APCIELM model over the existing methods.},
  archive      = {J_NCA},
  author       = {Ali, Amal Ibrahim Al and Sheeja Rani, S. and Pravija Raj, P. V. and Khedr, Ahmed M.},
  doi          = {10.1007/s00521-024-10716-y},
  journal      = {Neural Computing and Applications},
  month        = {2},
  number       = {5},
  pages        = {2925-2943},
  shortjournal = {Neural Comput. Appl.},
  title        = {Hierarchical cluster-based IELM for financial distress prediction with imbalanced data},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Hybrid nanofluid radiative flow across a permeable convective moving surface with heat generation: Numerical and statistical approach. <em>NCA</em>, <em>37</em>(5), 2911-2923. (<a href='https://doi.org/10.1007/s00521-024-10834-7'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This study introduces a novel computational technique aimed at enhancing fluid heat transfer capabilities through the integration of hybridized nanoparticles into a fluid matrix, resulting in a graphene–copper water-based hybrid nanofluid. The research focuses on modeling and solving the complex dynamics of radiative hybrid nanofluid flow across a permeable convective surface with simultaneous heat generation. Utilizing a similarity transformation, the model is simplified and subsequently solved using a MATLAB numerical solver. Dual solutions are identified, and their stability is confirmed through rigorous stability analysis. To optimize heat transfer enhancement, the study employs response surface methodology (RSM) to refine key parameters—specifically thermal radiation, heat generation, and the Biot number—with the goal of achieving maximum heat transfer efficiency. Findings indicate a notable increase in heat transfer efficiency when employing a 2% volume fraction of copper in the hybrid nanofluid compared to lower concentrations (1–1.5%). Optimal conditions for the skin friction coefficient and flow bifurcation delay are identified which demonstrates effective control over fluid dynamics. Additionally, strategic adjustments in heat generation and nanoparticle volume fractions lead to significant reductions in fluid temperature, thereby enhancing thermal management efficiency. This research significantly advances the understanding of the thermal performance of hybrid nanofluids under dynamic conditions and provides practical insights for optimizing heat transfer in industrial applications.},
  archive      = {J_NCA},
  author       = {Wahid, Nur Syahirah and Mustafa, Mohd Shafie and Arifin, Norihan Md and Khashi’ie, Najiyah Safwa and Pop, Ioan},
  doi          = {10.1007/s00521-024-10834-7},
  journal      = {Neural Computing and Applications},
  month        = {2},
  number       = {5},
  pages        = {2911-2923},
  shortjournal = {Neural Comput. Appl.},
  title        = {Hybrid nanofluid radiative flow across a permeable convective moving surface with heat generation: Numerical and statistical approach},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Hybrid fuzzy genetic algorithm for the integration of process planning and scheduling for distributed flexible job shop. <em>NCA</em>, <em>37</em>(4), 2775-2798. (<a href='https://doi.org/10.1007/s00521-024-10725-x'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The study explores the effectiveness of a hybrid fuzzy logic-based genetic algorithm model in addressing process planning and scheduling in a distributed flexible job shop. In this paper, the proposed hybrid fuzzy-GA framework presents a novel integrated mechanism of hybridization through integrating and synchronizing the main three function blocks (GA, fitness, and fuzzy logic) in order to achieve an adaptive control mechanism through dynamic adaptation that utilizes fuzzy inference to tune GA main parameters (i.e., mutation and crossover rates) dynamically and in real time within the evolution process, improving the algorithm’s ability to both explore and exploit the search space efficiently and effectively. Fuzzy logic assigns an output based on inserted input probability and human estimation assumptions. The genetic algorithm is a popular search technique used for solving optimization problems. The crossover and mutation rates, being key parameters in the genetic algorithm, are determined using fuzzy logic to improve overall effectiveness. The algorithm employs chromosome ∅s to minimize the makespan. In chromosome ∅s, the decisions are implicitly determined using heuristic rules that ensure load balancing among manufacturing resources. In this paper, the fuzzy genetic algorithm demonstrates its effectiveness to reduce the makespan by 6% and generate better results with a chance of 90% than standard genetic algorithms. Moreover, this study shows that there is 91% chance it yields outputs within 5% of the absolute minimum.},
  archive      = {J_NCA},
  author       = {Samhouri, Murad and Qareish, Sarah Z.},
  doi          = {10.1007/s00521-024-10725-x},
  journal      = {Neural Computing and Applications},
  month        = {2},
  number       = {4},
  pages        = {2775-2798},
  shortjournal = {Neural Comput. Appl.},
  title        = {Hybrid fuzzy genetic algorithm for the integration of process planning and scheduling for distributed flexible job shop},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). I-nema: A large-scale microscopic image dataset for nematode recognition. <em>NCA</em>, <em>37</em>(4), 2763-2773. (<a href='https://doi.org/10.1007/s00521-024-10687-0'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Nematode worms are one of the most abundant metazoan groups, occupying diverse ecological niches. Accurate recognition or identification of nematodes is of great importance for pest control, soil ecology, biogeography, habitat conservation, and climate change. Computer vision has witnessed a few successes in species recognition of nematodes; however, it is still in great demand. In this paper, we identify two main bottlenecks: (1) the lack of a publicly available microscopic-imaging dataset for diverse species of nematodes (especially the species only found in a natural environment) which requires considerable human resources in fieldwork and experts in taxonomy, and (2) the lack of a standard benchmark of state-of-the-art deep learning techniques on this dataset which demands the discipline background in computer science. With these in mind, we propose a large-scale microscopic image dataset consisting of 9215 images and 40 species (4 laboratories cultured and 36 naturally isolated species), which, to our knowledge, is the first time in the community. We further set up a species recognition benchmark by employing state-of-the-art deep learning networks on this dataset. We discuss the experimental results, compare the recognition accuracy of different networks, and show the challenges of our dataset. We will make our dataset publicly available.},
  archive      = {J_NCA},
  author       = {Lu, Shenglin and Fung, Sheldon and Wang, Yihao and Lu, Xuequan and Ouyang, Wanli and Qing, Xue and Li, Hongmei},
  doi          = {10.1007/s00521-024-10687-0},
  journal      = {Neural Computing and Applications},
  month        = {2},
  number       = {4},
  pages        = {2763-2773},
  shortjournal = {Neural Comput. Appl.},
  title        = {I-nema: A large-scale microscopic image dataset for nematode recognition},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Plantformer: Plant point cloud completion based on local–global feature aggregation and spatial context-aware transformer. <em>NCA</em>, <em>37</em>(4), 2747-2762. (<a href='https://doi.org/10.1007/s00521-024-10659-4'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Plant phenotypic analysis plays a crucial role in plant breeding and transgenic research. Three-dimensional (3D) point cloud is a powerful paradigm in plant phenotypic analysis, which can effectively represent 3D structure and alleviate occlusion-related issues. Moreover, it is imperative to reconstruct incomplete 3D point clouds of plants collected in complex planting scenarios. However, existing methods for plant point cloud completion mainly focus on the extraction of global features and exhibit limitations in effectively aggregating local and global features well, which fails to capture the intricate local geometric structures inherent in plant organs, thereby making it difficult to satisfy completion results. To address these challenges, we proposed a novel fine-grained point cloud completion method, namely, PlantFormer, to generate the complete point cloud from its partial observation. We propose an edge-convolution attention module to aggregate local–global features, which not only captures general geometric structures but also preserves local regional information. Furthermore, a spatial context-aware transformer was introduced to achieve a fine upsample effect on the plant point cloud. More specifically, due to the absence of high-quality datasets, we first conducted PlantCom3D, a plant point cloud completion dataset containing multiple species and growing environments. Extensive experiments demonstrate that our proposed model surpasses comparative models across various metrics. It achieves a $$CDL_1$$ of 2.879, with a notable improvement of 12.8 $$\%$$ compared to the previous optimal model.},
  archive      = {J_NCA},
  author       = {Li, Xiaomeng and Li, Fei and Qi, Yanyu and Li, Zhenbo},
  doi          = {10.1007/s00521-024-10659-4},
  journal      = {Neural Computing and Applications},
  month        = {2},
  number       = {4},
  pages        = {2747-2762},
  shortjournal = {Neural Comput. Appl.},
  title        = {Plantformer: Plant point cloud completion based on local–global feature aggregation and spatial context-aware transformer},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). TransTg: A new transformer model for predicting glass transition temperature of polymers from monomers’ molecular structures. <em>NCA</em>, <em>37</em>(4), 2733-2746. (<a href='https://doi.org/10.1007/s00521-024-10532-4'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The escalating demand for novel and multifunctional polymers with superior properties requires a deeper exploration of the chemical space, coupled with a well-established understanding of the relationship between their chemical structure and properties. The glass transition temperature (Tg), representing the temperature at which a polymer transitions from a solid to a soft and pliable state, is one important characteristic of polymers. Thus, predicting Tg values is crucial for discovering novel polymeric materials for diverse applications. However, experimental determination can be costly and time-consuming besides relying on specific experimental conditions that severely affect the measured Tg values. Recently, data-driven models based on machine learning have emerged as promising tools for predicting Tg values using molecular structure information. In this work, a novel SMILES-to-Tg transformer (TransTg) is developed as a new transformer-based deep learning model to predict the Tg values of polymers. Transformer models exploit the self-attention mechanism that captures long-range dependencies in sequential representations, which is suitable for modeling complex polymer structures. Our proposed transformer model achieves notable performance, with a mean absolute error (MAE) of 22.55 and a coefficient of determination (R2) of 0.849 on a held-out test set, outperforming the state-of-the-art results on using QSPR and machine learning approaches on monomers’ molecular structure. Besides their impressive performance, transformer-based models offer the advantage of result interpretation through their self-attention mechanism. Unlike black-box machine learning models, this mechanism highlights the most critical parts of the input sequence during the learning process, thereby aiding in result interpretation. By relating chemical language (SMILES), and chemical properties, there exists potential for developing language-to-property transformer models that directly link monomer structures to various polymer properties across multiple domains and tasks.},
  archive      = {J_NCA},
  author       = {Aleb, Nassima and Abu-Thabit, Nedal Y.},
  doi          = {10.1007/s00521-024-10532-4},
  journal      = {Neural Computing and Applications},
  month        = {2},
  number       = {4},
  pages        = {2733-2746},
  shortjournal = {Neural Comput. Appl.},
  title        = {TransTg: A new transformer model for predicting glass transition temperature of polymers from monomers’ molecular structures},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Dynamical system simulation with attention and recurrent neural networks. <em>NCA</em>, <em>37</em>(4), 2711-2731. (<a href='https://doi.org/10.1007/s00521-024-10732-y'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Accurate and efficient real-time simulation of nonlinear dynamic systems remains an important challenge in fields such as robotics, control systems and industrial processes, requiring innovative solutions for predictive modeling. In this work, we introduce a novel recurrent neural networks (RNN) architecture designed to simulate complex nonlinear dynamical systems. Our approach aims to predict system behavior at any time step and over any prediction horizon, using only the system’s initial state and external inputs. The proposed architecture combines RNN with multilayer perceptron and incorporates an attention mechanism to process both previous state estimates and external inputs. By training without teacher forcing, our model can iteratively estimate the system’s state over long prediction horizons. Experimental results on three public benchmarks show that our method outperforms other state-of-the-art solutions. We highlight the potential of our proposal for modeling and simulating nonlinear systems in real-world applications.},
  archive      = {J_NCA},
  author       = {Fañanás-Anaya, Javier and López-Nicolás, Gonzalo and Sagüés, Carlos},
  doi          = {10.1007/s00521-024-10732-y},
  journal      = {Neural Computing and Applications},
  month        = {2},
  number       = {4},
  pages        = {2711-2731},
  shortjournal = {Neural Comput. Appl.},
  title        = {Dynamical system simulation with attention and recurrent neural networks},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Real-time object segmentation for laparoscopic cholecystectomy using YOLOv8. <em>NCA</em>, <em>37</em>(4), 2697-2710. (<a href='https://doi.org/10.1007/s00521-024-10713-1'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Organ and tool detection and segmentation in real time during surgery have been significant challenges in the development of robotic surgery. Most existing detection methods are unsuitable for the surgical environment, where the lighting conditions, occlusions, and anatomical structures can vary significantly. This study presents an organ and surgical tool segmentation and detection algorithm using a manually annotated dataset based on YOLOv8 (You Only Look Once), a state-of-the-art object detection framework. The YOLOv8 deep learning neural network is trained to detect and segment organs and tools during laparoscopic cholecystectomy using a manually annotated dataset of frames taken from actual surgeries. After four experiments using combinations of small and extra-large model sizes and the original and a modified dataset, the resulting algorithm is evaluated and tested in real time on a new surgical video. The method shows it can provide real-time feedback to the surgeon by accurately locating and segmenting the target organs displayed in the surgical video. The method outperforms the baseline methods, with a “bounding box” mean average precision (mAP50) and precession (P) of (50.2%, 51.6%), (52.8%, 76.9%), (83.2%, 81.1%), and (86.3%, 85.7%) for the first, second, third, and fourth experiments, respectively, and a “masking segment” of mAP50 and precession of (50.5%, 51.8%), (54.3%, 76.1%), (82.6%, 80.4%), (86.0%, 85.4%) for the first, second, third, and fourth experiments, respectively. The best-performing model has a speed of around 13.1 ms per frame. This novel application could be a stepping stone in future work, such as developing an algorithm to display the results to the surgeon in a heads-up-display (HUD) to help navigate the scenes or even be implemented in robotic surgeries.},
  archive      = {J_NCA},
  author       = {Tashtoush, Amr and Wang, Yong and Khasawneh, Mohammad T. and Hader, Asma and Shazeeb, Mohammed Salman and Lindsay, Clifford Grant},
  doi          = {10.1007/s00521-024-10713-1},
  journal      = {Neural Computing and Applications},
  month        = {2},
  number       = {4},
  pages        = {2697-2710},
  shortjournal = {Neural Comput. Appl.},
  title        = {Real-time object segmentation for laparoscopic cholecystectomy using YOLOv8},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A real-time platform for spanish sign language interpretation. <em>NCA</em>, <em>37</em>(4), 2675-2696. (<a href='https://doi.org/10.1007/s00521-024-10776-0'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {More than 13% of the Spanish population suffers from hearing impairment. Despite the existence of several hearing aids and implants making sounds clearer and louder, there are some people unable to use them and, consequently, their only means of communication is Sign Language. However, this language is not widely known in the society. Consequently, those who are deaf or hard of hearing may be socially excluded and experience frustration as a result of the lack of communication. In this context, Sign Language Recognition and interpretation would help to break down the existing communication barriers and facilitate the creation of inclusive environments. With that aim, this paper presents a real-time platform to recognize and interpret finger-spelt words in Spanish Sign Language (Lengua de Signos Española). As finger spelling implies the recognition of each signed letter, a comparative analysis of different deep learning techniques to properly recognize the Spanish Sign Language alphabet has been carried out. For that, due to the lack of Spanish Sign Language datasets, the first step was to capture and build an image dataset representing its 30 letters. As there are static and in-motion letters, spatial and temporal analysis has been conducted by considering different kind of neural networks (Convolutional Neural Networks, Recurrent Neural Networks, and Vision Transformers). The experimental results highlight the good performance of the studied architectures, obtaining a maximum accuracy of 79.96% on previously unseen data. Finally, a real-time platform for the recognition and interpretation of finger-spelt words in Spanish Sign Language has been implemented making the communication possible.},
  archive      = {J_NCA},
  author       = {Morillas-Espejo, Francisco and Martinez-Martin, Ester},
  doi          = {10.1007/s00521-024-10776-0},
  journal      = {Neural Computing and Applications},
  month        = {2},
  number       = {4},
  pages        = {2675-2696},
  shortjournal = {Neural Comput. Appl.},
  title        = {A real-time platform for spanish sign language interpretation},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). PanicleDet: A deep learning-based model for detection of panicle stages in paddy. <em>NCA</em>, <em>37</em>(4), 2663-2673. (<a href='https://doi.org/10.1007/s00521-024-10746-6'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Rice is a staple food which is consumed by more than half of the world’s population. The paddy panicle, reproductive structure of rice plant directly impacts the yield, which plays an essential role in the food security of billions of people. The identification of various stages of the panicle development is crucial for farmers and researchers to optimize crop management and conduct scientific studies. While object detection techniques of deep learning have been widely used in agriculture for various purposes, the detection of paddy panicle stages has not been explored in the literature. This research presents a novel panicle detection model called “PanicleDet” based on an ensemble of base YOLOv5 and proposed extended YOLOv5 models. The proposed approach overcomes challenges associated with panicle detection, such as occlusion, object overlap, and varying object sizes. The model is trained on a dataset of paddy plants grown in controlled conditions, capturing images from different angles and timestamps before harvesting. The results demonstrate that “PanicleDet” achieves mean average precision (mAP) 93% and 77.9%, respectively, for intersection over union (IoU) threshold of 0.5 and 0.5 to 0.95, outperforming other SOTA detection models. It accurately identifies five critical panicle stages: boot leaf, heading, anthesis, grain filling, and grain maturity. The proposed model’s automation capabilities can aid farmers in making informed decisions regarding nutrient and water application, while researchers can conduct experiments related to crop duration and tolerance to environmental stress. The findings hold promise for enhancing rice productivity and addressing food security challenges in rice-dependent regions. By automating the detection of paddy panicle stages, this research contributes to precision agriculture practices and crop management strategies.},
  archive      = {J_NCA},
  author       = {Chaurasia, Himanshushekhar and Arora, Alka and Raju, Dhandapani and Marwaha, Sudeep and Chinnusamy, Viswanathan and Jain, Rajni and Haque, Md Ashraful},
  doi          = {10.1007/s00521-024-10746-6},
  journal      = {Neural Computing and Applications},
  month        = {2},
  number       = {4},
  pages        = {2663-2673},
  shortjournal = {Neural Comput. Appl.},
  title        = {PanicleDet: A deep learning-based model for detection of panicle stages in paddy},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A hybrid CNN-random forest model with landmark angles for real-time arabic sign language recognition. <em>NCA</em>, <em>37</em>(4), 2641-2662. (<a href='https://doi.org/10.1007/s00521-024-10729-7'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Sign language serves as a dynamic means of communication, facilitating expression and emotional connection among individuals who are deaf or hard of hearing and the broader community. However, persistent communication barriers impact various sectors, including healthcare and education. Sign Language Recognition (SLR) systems bridge this gap by leveraging artificial intelligence advancements and linguistic knowledge to classify and translate sign language gestures in real-time. Despite advancements in SLR for many languages, Arabic SLR remains underexplored, lacking diverse datasets and real-time evaluation. This study introduces a novel Arabic SLR system emphasizing real-time applicability. The system achieves competitive performance in static and real-time classification through a hybrid CNN-Random Forest model trained on 96,000 collected images, incorporating hand landmarks’ angles. The proposed model demonstrates superior performance in real-time settings, surpassing existing alternatives and achieving an impressive 99.95% accuracy on aggregated data. These results highlight the model’s effectiveness in addressing the unique challenges of real-time Arabic SLR.},
  archive      = {J_NCA},
  author       = {Boulesnane, Abdennour and Bellil, Lyna and Ghiri, Maissoun Ghouzlen},
  doi          = {10.1007/s00521-024-10729-7},
  journal      = {Neural Computing and Applications},
  month        = {2},
  number       = {4},
  pages        = {2641-2662},
  shortjournal = {Neural Comput. Appl.},
  title        = {A hybrid CNN-random forest model with landmark angles for real-time arabic sign language recognition},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A cosine similarity-based token subsampling method for vision transformer in cloud computing. <em>NCA</em>, <em>37</em>(4), 2627-2639. (<a href='https://doi.org/10.1007/s00521-024-10718-w'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deploying huge deep learning applications on resource-constrained edge devices is a challenging task. Cloud-based edge computing is a promising solution. Such as model partitioning, a portion of the deep learning model is deployed on the edge device; while, the remaining portion is executed by the cloud. Leveraging the computation power of edge devices, transmission latency is reduced, and bandwidth efficiency is increased. Recently, visual transformer models, supported by large datasets, have dominated in multiple vision tasks. However, model partitioning optimization methods for visual transformers are lacking. Therefore, the paper proposes a cosine similarity-based token subsampling method for visual transformer model partitioning to improve transmission efficiency. Tokens in the same class are subsampled and only the centroid tokens are uploaded. In the cloud, all tokens are reconstructed based on interpolation indexes. Three algorithm implementations are proposed and measured on PC, Jetson NANO and edge CPU Cortex-A53. The experimental results demonstrate that the recommended algorithm implementation can be executed with low-latency of 71.24 ms, and 35.65% transmitted data is reduced with an accuracy drop of 0.46%.},
  archive      = {J_NCA},
  author       = {Li, Qi and Kaneko, Hayata and Meng, Lin},
  doi          = {10.1007/s00521-024-10718-w},
  journal      = {Neural Computing and Applications},
  month        = {2},
  number       = {4},
  pages        = {2627-2639},
  shortjournal = {Neural Comput. Appl.},
  title        = {A cosine similarity-based token subsampling method for vision transformer in cloud computing},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Convolutional neural network for classifying the stages of the cell cycle. <em>NCA</em>, <em>37</em>(4), 2617-2626. (<a href='https://doi.org/10.1007/s00521-024-10709-x'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The cell cycle is a highly coordinated process that ensures the duplication and transmission of genetic information from one cell generation to the next. The detailed observation constitutes a fundamental starting point for diagnosing and preventing diseases such as cancer. Computational techniques, such as deep learning, offer a way to understand the cell cycle behavior. This study introduces a convolutional neural network model specifically designed to classify the stages of the cell cycle of a cancerous cell. Images from the Jurkat cell line were used to train the model. A rigorous evaluation was conducted to verify the effectiveness of the proposed model, comparing the results with other deep learning models that also use the same database. The results showed the high performance of the proposed model. The model demonstrated better alignment with the data characteristics, resulting in a more accurate classification of the cell cycle stages. This outcome underscores the distinctive ability of a convolutional neural network to identify patterns in the cell cycle more precisely than human perception, which, at times, may be susceptible to errors. Utilizing a convolutional neural network reduced complexity and heightened suitability, primarily owing to the available volume of data, which aligns seamlessly with the requisites of a moderately deep network. Furthermore, deploying a convolutional neural network is associated with reduced computational overhead and faster data processing.},
  archive      = {J_NCA},
  author       = {Duque-Vazquez, Edgar F. and Cepeda-Negrete, Jonathan and López-Meza, Joel E. and Saldaña-Robles, Noe and Sanchez-Yanez, Raul E.},
  doi          = {10.1007/s00521-024-10709-x},
  journal      = {Neural Computing and Applications},
  month        = {2},
  number       = {4},
  pages        = {2617-2626},
  shortjournal = {Neural Comput. Appl.},
  title        = {Convolutional neural network for classifying the stages of the cell cycle},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). AI-UNet: Attention information-based deep URL network for adult webpage classification. <em>NCA</em>, <em>37</em>(4), 2597-2615. (<a href='https://doi.org/10.1007/s00521-024-10408-7'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Internet provides us a plethora of knowledge and information across different domains. However, the internet also consists of mature contents like pornographic websites that has to be filtered to keep the internet kids friendly. There is a demand for automated adult webpage classifier which can handle the ever-growing World Wide Web efficiently and swiftly. In this research work, we propose a deep neural network architecture called as Attention Information-based URL Network (AI-UNet) for classifying the adult web sites without fetching the contents of the web page and can aid in quick decision making. The proposed AI-UNet is a two-stage classifier that uses special attention mechanisms to classify the given webpage as “Adult” or “Non-Adult” by leveraging the information present in an URL along with its web page title with a novel URL representation. A novel dense vector representation called as word-as-character embedding is proposed in this work. This embedding combines the advantages of both character and word level embedding with an effective pre-processing technique. To extract the highly relevant information present either in URL/title/both, an adaptive attention mechanism is added on top of the proposed recurrent convolutional neural network model. Our experimental evaluations show that the proposed AI-UNet model outperforms all other character and word embedding-based webpage classifiers including the recent URLNet model. Our proposed model achieves 89.92% accuracy over the benchmark ODP dataset. Furthermore, we evaluated the model with our own independent dataset called as real-time dataset and obtained an accuracy of 99.43% on this corpus. From the extensive experimental analysis, it is shown that there is a significant improvement over the existing methods for Adult Webpage classification.},
  archive      = {J_NCA},
  author       = {Rajalakshmi, Ratnavel and Raymann, Joel and Prabhu, Aneesh and Karthik, R. and Aravindan, Chandrabose},
  doi          = {10.1007/s00521-024-10408-7},
  journal      = {Neural Computing and Applications},
  month        = {2},
  number       = {4},
  pages        = {2597-2615},
  shortjournal = {Neural Comput. Appl.},
  title        = {AI-UNet: Attention information-based deep URL network for adult webpage classification},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A voice activity detection algorithm using deep learning in time–frequency domain. <em>NCA</em>, <em>37</em>(4), 2581-2595. (<a href='https://doi.org/10.1007/s00521-024-10795-x'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Voice activity detection (VAD) is an important component of signal processing that is critical for various applications, including speech recognition, speaker recognition, and speaker identification for example to eliminate different background noise signals. With the increasing use of deep learning techniques in speech-based applications, VAD has become more accurate and efficient. In this paper, a novel VAD method based on deep neural networks, specifically, a residual neural network (ResNet), a convolutional neural network with multilayer perceptron, and recurrent neural network components are proposed. The results of the proposed VAD are compared with other state-of-the-art methods, and its effectiveness is demonstrated in basic speech enhancement in the presence of various types of noise. The proposed VAD utilizes a combination of time–frequency features, such as log mel spectrogram representations, and the ResNet-32 as a flexible classifier method to detect speech/no-speech activities. The advantages of the proposed VAD method include its ability to adapt to different types of background noise, such as stationary, non-stationary, and periodic noises, and its flexibility in terms of selecting appropriate deep learning models for different applications. The results of the proposed VAD method show significant improvements compared to other methods, which demonstrates its effectiveness in speech enhancement in noisy environments. It is shown that the best performance is achieved by a hybrid of time–frequency features as log mel spectrogram representations and ResNet-32 as a flexible classifier method to detect speech/no-speech activities correctly.},
  archive      = {J_NCA},
  author       = {Mavaddati, Samira},
  doi          = {10.1007/s00521-024-10795-x},
  journal      = {Neural Computing and Applications},
  month        = {2},
  number       = {4},
  pages        = {2581-2595},
  shortjournal = {Neural Comput. Appl.},
  title        = {A voice activity detection algorithm using deep learning in time–frequency domain},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). An intelligent approach for autism spectrum disorder diagnosis and rehabilitation features identification. <em>NCA</em>, <em>37</em>(4), 2557-2580. (<a href='https://doi.org/10.1007/s00521-024-10770-6'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Autism spectrum disorder (ASD) affects 1 in 100 children globally. Early detection and intervention can enhance life quality for individuals diagnosed with ASD. This research utilizes the support vector machine-recursive feature elimination (SVM-RFE) method in its approach for ASD classification using the phenotypic and Automated Anatomical Labeling (AAL) Brain Atlas datasets of the Autism Brain Imaging Data Exchange preprocessed dataset. The functional connectivity matrix (FCM) is computed for the AAL data, generating 6670 features representing pair-wise brain region activity. The SVM-RFE feature selection method was applied five times to the FCM data, thus determining the optimal number of features to be 750 for the best performing support vector machine (SVM) model, corresponding to a dimensionality reduction of 88.76%. Pertinent phenotypic data features were manually selected and processed. Subsequently, five experiments were conducted, each representing a different combination of the features used for training and testing the linear SVM, deep neural networks, one-dimensional convolutional neural networks, and random forest machine learning models. These models are fine-tuned using grid search cross-validation (CV). The models are evaluated on various metrics using 5-fold CV. The most relevant brain regions from the optimal feature set are identified by ranking the SVM-RFE feature weights. The SVM-RFE approach achieved a state-of-the-art accuracy of 90.33% on the linear SVM model using the Data Processing Assistant for Resting-State Functional Magnetic Resonance Imaging pipeline. The SVM model’s ability to rank the features used based on their importance provides clarity into the factors contributing to the diagnosis. The thalamus right, rectus right, and temporal middle left AAL brain regions, among others, were identified as having the highest number of connections to other brain regions. These results highlight the importance of using traditional ML models for improved interpretability and provide insight into which features to target for ASD patient rehabilitation.},
  archive      = {J_NCA},
  author       = {Ghnemat, Rawan and Al-Madi, Nailah and Awad, Mohammad},
  doi          = {10.1007/s00521-024-10770-6},
  journal      = {Neural Computing and Applications},
  month        = {2},
  number       = {4},
  pages        = {2557-2580},
  shortjournal = {Neural Comput. Appl.},
  title        = {An intelligent approach for autism spectrum disorder diagnosis and rehabilitation features identification},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Machine learning-based models for predicting calibration cost of flowmeters in the oil industry. <em>NCA</em>, <em>37</em>(4), 2537-2556. (<a href='https://doi.org/10.1007/s00521-024-10748-4'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Accurate measurement of liquid volume in pipelines is a crucial and sensitive process in the petroleum industry. Flowmeter is a common device used to measure the volume of the liquid passing through the pipeline and to invoice clients. Improving flowmeter calibration preserves functionality, identifies issues, and ensures accurate market valuation and pricing. Estimating the cost of third-party calibration is a challenging task for decision-makers in the petroleum industry. Thus, this paper proposes machine-learning algorithms to predict and analyze the cost of flowmeter calibration in the petroleum industry. These techniques include artificial neural networks, SGDRegressor, XGBoost, AdaBoost, support vector machine, K-nearest neighbors, and random forest. Prior to developing the model, the factors impacting the cost calibration of the flowmeters are identified from the literature and finalized by oil and gas experts. In addition, the relationship between the input factors and the output is examined to ensure the quality of the data. The analysis indicated that the most crucial factors affecting the calibration cost are the flowmeter size, flange class, flowmeter type, calibration method, and calibration factor. Furthermore, the developed machine learning-based models are validated by using 153 new additional data. The results revealed that the random forest is the best technique for estimating calibration costs, with an accuracy of 99%, followed by AdaBoost with an accuracy of 97%, and artificial neural network with an accuracy of 96%. The proposed models provide superior accuracy and efficiency which will significantly contribute to estimating calibration costs, assisting decision-makers in establishing a budget for flowmeter calibration.},
  archive      = {J_NCA},
  author       = {Mohammed, Awsan and Javaid, Abdullah and Ghaithan, Ahmed and AlDhafer, Osamah and Al-shaibani, Maged S. and Alshibani, Adel},
  doi          = {10.1007/s00521-024-10748-4},
  journal      = {Neural Computing and Applications},
  month        = {2},
  number       = {4},
  pages        = {2537-2556},
  shortjournal = {Neural Comput. Appl.},
  title        = {Machine learning-based models for predicting calibration cost of flowmeters in the oil industry},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Optimization of grid-connected photovoltaic/wind/battery/supercapacitor systems using a hybrid artificial gorilla troops optimizer with a quadratic interpolation algorithm. <em>NCA</em>, <em>37</em>(4), 2497-2535. (<a href='https://doi.org/10.1007/s00521-024-10742-w'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A global transition toward renewable energy is essential for mitigating the environmental and economic challenges associated with fossil fuels. However, optimizing hybrid renewable energy systems (HRES) presents significant challenges, particularly in achieving a balance between efficiency and cost-effectiveness. This study introduces a novel optimization approach called the Quadratic Interpolation-enhanced Artificial Gorilla Troops Optimizer (QIGTO), which is specifically designed to address these challenges. Unlike existing methods, QIGTO improves convergence speed and solution accuracy, which are crucial for optimizing grid-connected HRES. The QIGTO algorithm was applied to a real-world scenario involving a grid-connected system comprising photovoltaic panels, wind turbines, batteries, and supercapacitors. QIGTO provides significant improvements over existing methods by increasing the renewable energy fraction to 78.54%, reducing the annual cost to $572369.93, and lowering the cost of energy to $0.23996/kWh. The results indicate significant improvements in the system’s renewable energy fraction, cost savings, and overall performance. These findings establish QIGTO as an effective tool for advancing sustainable energy solutions and tackling the complexities associated with hybrid energy systems. The results of this study underscore the importance of advanced optimization techniques in renewable energy research and provide a robust foundation for future studies aimed at optimizing HRES across various contexts.},
  archive      = {J_NCA},
  author       = {Güven, Aykut Fatih and Kamel, Salah and Hassan, Mohamed H.},
  doi          = {10.1007/s00521-024-10742-w},
  journal      = {Neural Computing and Applications},
  month        = {2},
  number       = {4},
  pages        = {2497-2535},
  shortjournal = {Neural Comput. Appl.},
  title        = {Optimization of grid-connected photovoltaic/wind/battery/supercapacitor systems using a hybrid artificial gorilla troops optimizer with a quadratic interpolation algorithm},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Utilizing convolutional neural networks and vision transformers for precise corn leaf disease identification. <em>NCA</em>, <em>37</em>(4), 2479-2496. (<a href='https://doi.org/10.1007/s00521-024-10769-z'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Corn is not only widely used in industry but also a crucial staple food. Early detection of corn leaf diseases is vital to prevent crop loss. Farmers and agricultural engineers often rely on computer-aided systems for early diagnosis of plant diseases. Among the various methods, deep learning stands out as the most popular and effective approach for detecting corn leaf diseases. In this study, we utilized cutting-edge Vision Transformer (ViT) models like MaxViT, DeiT3, MobileViT, and MViTv2, which have recently gained more traction compared to Convolutional Neural Networks (CNNs). Additionally, we incorporated well-known CNN architectures such as VGG, ResNet, DenseNet, and Xception to accurately diagnose corn leaf diseases. To enhance the models’ effectiveness, we employed image preprocessing, data augmentation techniques, transfer learning, and optimized parameters. Furthermore, we implemented a soft voting ensemble technique with an adaptive thresholding method to dynamically boost performance, leading to higher accuracy and balanced metrics in detecting corn diseases. Our approach was trained and evaluated on both the well-known PlantVillage dataset and the novel CD&S dataset. The results showed that four models from the MaxViT architecture, along with other deep learning models, achieved a high accuracy of 100% on the CD&S dataset’s test data, the highest performance recorded in the literature. On the PlantVillage dataset, the approach attained an impressive 99.83% accuracy, surpassing other studies. This proposed method offers an early and autonomous solution for diagnosing corn plant diseases in the agricultural field with high accuracy. This innovation highlights the potential of advanced ViT models to outperform traditional CNNs and improve crop disease detection.},
  archive      = {J_NCA},
  author       = {Pacal, Ishak and Işık, Gültekin},
  doi          = {10.1007/s00521-024-10769-z},
  journal      = {Neural Computing and Applications},
  month        = {2},
  number       = {4},
  pages        = {2479-2496},
  shortjournal = {Neural Comput. Appl.},
  title        = {Utilizing convolutional neural networks and vision transformers for precise corn leaf disease identification},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). TransNAS-TSAD: Harnessing transformers for multi-objective neural architecture search in time series anomaly detection. <em>NCA</em>, <em>37</em>(4), 2455-2477. (<a href='https://doi.org/10.1007/s00521-024-10759-1'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The surge in real-time data collection across various industries has underscored the need for advanced anomaly detection in both univariate and multivariate time series data. This paper introduces TransNAS-TSAD, a framework that synergizes the transformer architecture with neural architecture search (NAS), enhanced through NSGA-II algorithm optimization. This approach effectively tackles the complexities of time series data, balancing computational efficiency with detection accuracy. Our evaluation reveals that TransNAS-TSAD surpasses conventional anomaly detection models due to its tailored architectural adaptability and the efficient exploration of complex search spaces, leading to marked improvements in diverse data scenarios. We also introduce the Efficiency-accuracy-complexity score (EACS) as a composite metric that balances accuracy, computational efficiency, and model complexity, providing a comprehensive assessment of model performance. TransNAS-TSAD sets a new benchmark in time series anomaly detection, offering a versatile, efficient solution for complex real-world applications. This research highlights TransNAS-TSAD’s potential across a wide range of industry applications and paves the way for future developments in the field.},
  archive      = {J_NCA},
  author       = {Haq, Ijaz Ul and Lee, Byung Suk and Rizzo, Donna M.},
  doi          = {10.1007/s00521-024-10759-1},
  journal      = {Neural Computing and Applications},
  month        = {2},
  number       = {4},
  pages        = {2455-2477},
  shortjournal = {Neural Comput. Appl.},
  title        = {TransNAS-TSAD: Harnessing transformers for multi-objective neural architecture search in time series anomaly detection},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Enhancement of tea leaf diseases identification using modified SOTA models. <em>NCA</em>, <em>37</em>(4), 2435-2453. (<a href='https://doi.org/10.1007/s00521-024-10758-2'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The identification of tea leaf diseases holds considerable significance for preserving the health of tea plants and preventing losses in tea production. This study introduced a hybrid framework by combining modified state-of-the-art (SOTA) models with feature selection and Machine Learning (ML) classifiers for recognizing four types of tea leaf diseases. The investigation utilized SOTA models, namely VGG16, Xception, and ResNet152V2. These architectures underwent modification by adding extra layers, serving as feature extractors from tea leaves. The extracted features then underwent a feature selection process to identify the most relevant ones, which were subsequently employed in ML classifiers for predicting tea leaf diseases. The proposed method demonstrated outstanding performance with a 2-fold average accuracy of 99.5%, an Area Under the Curve (AUC) of 1.0, and a p value of 0.001.},
  archive      = {J_NCA},
  author       = {Panchbhai, Kamini G. and Lanjewar, Madhusudan G.},
  doi          = {10.1007/s00521-024-10758-2},
  journal      = {Neural Computing and Applications},
  month        = {2},
  number       = {4},
  pages        = {2435-2453},
  shortjournal = {Neural Comput. Appl.},
  title        = {Enhancement of tea leaf diseases identification using modified SOTA models},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). ResDense fusion: Enhancing schizophrenia disorder detection in EEG data through ensemble fusion of deep learning models. <em>NCA</em>, <em>37</em>(4), 2411-2433. (<a href='https://doi.org/10.1007/s00521-024-10701-5'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Schizophrenia, a complex and debilitating mental disorder, affects approximately 1% of the global population. Diagnosing schizophrenia is challenging due to its heterogeneous symptomatology and lack of objective biomarkers. Electroencephalography (EEG) has emerged as a promising modality for investigating the underlying neurophysiological mechanisms of schizophrenia. In this study, we introduce a novel deep learning model called ResDense Fusion, which leverages residual connections for feature extraction, tackling the issue of vanishing gradients in deep networks. Additionally, during classification, each layer within ResDense Fusion is connected to every other layer in a feed-forward manner within a dense block. ResDense Fusion model integrates the hierarchical feature extraction capabilities of ResNet with the dense feature reuse mechanism of DenseNet, aiming to capture both low-level and high-level representations from EEG signals. By combining these architectures, the model can effectively learn discriminative features relevant to schizophrenia pathology. We evaluated the performance of the ResDense Fusion model on a dataset comprising EEG recordings from schizophrenia patients and healthy controls. Implemented in Python and results demonstrate the efficacy of the proposed model, achieving an impressive accuracy of 96%. The findings of this study highlight the potential of deep learning approaches in harnessing EEG data for schizophrenia diagnosis. The ResDense Fusion model not only offers a powerful tool for detecting schizophrenia but also provides insights into neurobiological underpinnings of the disorder. Ultimately, the development of accurate and efficient diagnostic tools like the ResDense Fusion model has the potential to improve early detection, treatment planning, and patient outcomes in schizophrenia management.},
  archive      = {J_NCA},
  author       = {Senthil Kumar, S. and Venmathi, A. R. and Thangavel, Yuvaraja and Raja, L.},
  doi          = {10.1007/s00521-024-10701-5},
  journal      = {Neural Computing and Applications},
  month        = {2},
  number       = {4},
  pages        = {2411-2433},
  shortjournal = {Neural Comput. Appl.},
  title        = {ResDense fusion: Enhancing schizophrenia disorder detection in EEG data through ensemble fusion of deep learning models},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Application of natural language modeling techniques in natural gas segmentation in seismic reflection images. <em>NCA</em>, <em>37</em>(4), 2383-2409. (<a href='https://doi.org/10.1007/s00521-024-10557-9'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The most common indirect method the Oil and Gas industry uses to survey an area looking for hydrocarbon accumulations is based on the physical principle of seismic reflection. Geoscientists look for sudden signal intensity peaks which may indicate the accumulations. Most machine learning methods that automate this task using seismic reflection data are based on considering whole seismic lines as images. In this work, we propose a method to automate the segmentation of natural gas accumulations by taking into account the temporal nature of the data and turns reflection amplitudes into word-like objects, using a modified version of WordPiece tokenization, and a Robustly Optimized Bidirectional Encoder Representation from Transformers Pretraining Approach (RoBERTa) to segment each seismic trace that forms the image. As a post-processing step, we apply the mathematical morphology techniques of opening and closing to improve the initial segmentation. We also analyze the presence of a seismic imaging problem in the dataset and how it affects the resulting metrics depending on the dataset’s train-test split choice. Lastly, we compare the proposed method against two baseline models present in the literature. Experimental results show that the proposed method generalizes better than the baseline models and is more efficient to segment previously unseen gas accumulations, effectively decreasing the time between the seismic survey (data acquisition) and exploratory drilling phases. It also paves the way to use other methods from Natural Language Processing in geological research and time series tasks in other research areas.},
  archive      = {J_NCA},
  author       = {de Mello, Henrique Ribeiro and de Paiva, Anselmo Cardoso and Silva, Aristófanes Correa and Braz Junior, Geraldo and de Almeida, João Dallyson Sousa and Quintanilha, Darlan Bruno Pontes and Gattass, Marcelo},
  doi          = {10.1007/s00521-024-10557-9},
  journal      = {Neural Computing and Applications},
  month        = {2},
  number       = {4},
  pages        = {2383-2409},
  shortjournal = {Neural Comput. Appl.},
  title        = {Application of natural language modeling techniques in natural gas segmentation in seismic reflection images},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Food fraud detection in octopus mimus using hyperspectral imaging and machine learning techniques. <em>NCA</em>, <em>37</em>(4), 2369-2381. (<a href='https://doi.org/10.1007/s00521-024-10750-w'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Among the many seafood products susceptible to food fraud, octopus (Octopus mimus) is often replaced by giant squid (Dosidicus gigas) due to its higher market value and desirable nutritional and sensory properties. As a result, the development of rapid and noninvasive techniques for food quality assessment is of significant interest to the food industry. In this research, we evaluated the capability of two hyperspectral imaging systems to discriminate between octopus and giant squid meat. For the experimentation, samples of both species were acquired from a local port, and the arms were selected as the most similar parts between species. After cleaning and removing the skin, 300 cuts were extracted from each species and subdivided into three groups of 100, representing fresh, frozen, and cooked samples, respectively. Hyperspectral images were acquired in two spectral ranges: the visible and near-infrared (Vis-NIR, 400-1000 nm) and the near-infrared (NIR, 900-1700 nm) ranges. Mean spectral profiles were extracted and preprocessed with smoothing and normalization. An exploratory principal component analysis (PCA) was applied to the data. Subsequently, three classification models were implemented: linear discriminant analysis (LDA), support vector machine (SVM), and k-nearest neighbors (k-NN), with 30 repetitions and cross-validation applied for robustness. These models were created using the full spectrum and were then optimized. The results indicate that the highest accuracy was achieved using LDA classifiers, with smoothing and normalization as preprocessing steps. The accuracy reached 100% for the Vis-NIR range and 98.3% for the NIR range. After optimization, which involved selecting relevant wavelengths using feature selection algorithms, LDA classifiers again provided the best results. Accuracy improved to 99.9% for the Vis-NIR range and 94.8% for the NIR range, using only ten relevant wavelengths. These findings suggest that LDA-based classifiers are the most effective for distinguishing between octopus and giant squid meat. When optimization is required, feature selection using ReliefF or correlation-based feature subset selection algorithms is recommended. These results have important implications for the seafood industry, where accurate species identification is critical for ensuring quality control and food safety.},
  archive      = {J_NCA},
  author       = {Vera, William and Avila-George, Himer and Mogollón, Jorge and Chuquizuta, Tony and Castro, Wilson},
  doi          = {10.1007/s00521-024-10750-w},
  journal      = {Neural Computing and Applications},
  month        = {2},
  number       = {4},
  pages        = {2369-2381},
  shortjournal = {Neural Comput. Appl.},
  title        = {Food fraud detection in octopus mimus using hyperspectral imaging and machine learning techniques},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Neural network learning of black-scholes equation for option pricing. <em>NCA</em>, <em>37</em>(4), 2357-2368. (<a href='https://doi.org/10.1007/s00521-024-10761-7'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {One of the most discussed problems in the financial world is stock option pricing. The Black-Scholes equation is a parabolic partial differential equation which provides an option pricing model. The present work proposes an approach based on neural networks to solve the Black-Scholes equations. Real-world data from the stock options market were used as the initial boundary to solve the Black-Scholes equation. In particular, times series of call options prices of Brazilian companies Petrobras and Vale were employed. The results indicate that the network can learn to solve the Black-Scholes equation for a specific real-world stock options time series. The experimental results showed that the neural network option pricing based on the Black-Scholes equation solution can reach an option pricing forecasting more accurate than the traditional Black-Scholes analytical solutions. The experimental results making it possible to use this methodology to make short-term call option price forecasts in options markets.},
  archive      = {J_NCA},
  author       = {de Souza Santos, Daniel and Ferreira, Tiago A. E.},
  doi          = {10.1007/s00521-024-10761-7},
  journal      = {Neural Computing and Applications},
  month        = {2},
  number       = {4},
  pages        = {2357-2368},
  shortjournal = {Neural Comput. Appl.},
  title        = {Neural network learning of black-scholes equation for option pricing},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Vulnerable road users’ detection with bionic-corrected multi-fisheye images and safety warning for ART. <em>NCA</em>, <em>37</em>(4), 2327-2355. (<a href='https://doi.org/10.1007/s00521-024-10689-y'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Autonomous-rail Rapid Transit (ART) as a new form of road transportation faces the challenge of colliding with other road users. However, a complete and affordable environmental perception system has not yet been implemented to ART. This paper proposed a vulnerable road users detection and safety warning method for ART with four modules. The image acquisition module captures the surrounding environment using eight fisheye cameras distributed around the ART. The image correction module applies a bionic longitude and latitude correction algorithm to rectify fisheye images, generating images closer to normal vision. The target detection module applies a SOD-YOLOv5n lightweight detection model and is tested on the SODA10M dataset and our dataset. The mAP has increased by 7.9% and 3.6% compared to the original YOLOv5n directly, and by 4.4% and 4.0% compared to the latest YOLOv8n, respectively. Additionally, it integrates a monocular fisheye camera-based ranging algorithm, providing distance between ART and detected objects. The speed control module adopts a hierarchical speed control strategy based on the PID algorithm. By reading the speed and distance information, different accelerations are output for speed control. We confirmed the feasibility of the proposed method through experimentation and simulation, thereby ensuring safety of road users.},
  archive      = {J_NCA},
  author       = {Wang, Jirui and Han, Yongzhao and Tang, Hongjie and Liang, Fujian and Wu, Jiaoyi and Chen, Jiangfan and Zhang, Zutao},
  doi          = {10.1007/s00521-024-10689-y},
  journal      = {Neural Computing and Applications},
  month        = {2},
  number       = {4},
  pages        = {2327-2355},
  shortjournal = {Neural Comput. Appl.},
  title        = {Vulnerable road users’ detection with bionic-corrected multi-fisheye images and safety warning for ART},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Data-driven cooperative consensus control of nonlinear multiagent systems based on adaptive event-triggered strategies. <em>NCA</em>, <em>37</em>(4), 2313-2325. (<a href='https://doi.org/10.1007/s00521-024-10644-x'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper studies the synchronization problem of multi-agent systems (MASs) with completely unknown nonlinear dynamics. Firstly, the recurrent neural network is adopted to identify the completely unknown systems, and after sufficient training, the exact nonlinear MASs with model identification errors are obtained. Secondly, model identification error for each system is seen as the bounded disturbance, and then the robust leaderless consensus and leader-follower consensus problems with disturbances are considered, respectively. Then, to achieve the cooperative synchronization of MASs with disturbances, based on triggering mechanisms, two adaptive consensus protocols are constructed. The former solves the consensus issue without leader, and does not need continuous information transmission between neighbors. The latter achieves the consensus problem with one leader; the design of consensus protocol together with triggering mechanism has nothing to do with any global information of topology. Finally, the simulation results verify rationality of the obtained theoretical results.},
  archive      = {J_NCA},
  author       = {Dong, Xu and Zhang, Huaguang and Sun, Jiayue and Zhang, Juan},
  doi          = {10.1007/s00521-024-10644-x},
  journal      = {Neural Computing and Applications},
  month        = {2},
  number       = {4},
  pages        = {2313-2325},
  shortjournal = {Neural Comput. Appl.},
  title        = {Data-driven cooperative consensus control of nonlinear multiagent systems based on adaptive event-triggered strategies},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Machine visual perception from sim-to-real transfer learning for autonomous docking maneuvers. <em>NCA</em>, <em>37</em>(4), 2285-2312. (<a href='https://doi.org/10.1007/s00521-024-10543-1'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper presents a comprehensive approach to enhancing autonomous docking maneuvers through machine visual perception and sim-to-real transfer learning. By leveraging relative vectoring techniques, we aim to replicate the human ability to execute precise docking operations. Our study focuses on autonomous aerial refueling as a use case, demonstrating significant advancements in relative navigation and object detection. We introduce a novel method for aligning digital twins using fiducial targets and motion capture data, which facilitates accurate pose estimation from real-world imagery. Additionally, we develop cost-efficient annotation automation techniques for generating high-quality You Only Look Once training data. Experimental results indicate that our transfer learning methodologies enable accurate and reliable relative vectoring in real-world conditions, achieving error margins of less than 3 cm at contact (when vehicles are approximately 4 m from the camera) and maintaining performance at over 56 fps. The research findings underscore the potential of augmented reality and scene augmentation in improving model generalization and performance, bridging the gap between simulation and real-world applications. This work lays the groundwork for deploying autonomous docking systems in complex and dynamic environments, minimizing human intervention and enhancing operational efficiency.},
  archive      = {J_NCA},
  author       = {Worth, Derek and Choate, Jeffrey and Raettig, Ryan and Nykl, Scott and Taylor, Clark},
  doi          = {10.1007/s00521-024-10543-1},
  journal      = {Neural Computing and Applications},
  month        = {2},
  number       = {4},
  pages        = {2285-2312},
  shortjournal = {Neural Comput. Appl.},
  title        = {Machine visual perception from sim-to-real transfer learning for autonomous docking maneuvers},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Federated and transfer learning for cancer detection based on image analysis. <em>NCA</em>, <em>37</em>(4), 2239-2284. (<a href='https://doi.org/10.1007/s00521-024-10956-y'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This review highlights the efficacy of combining federated learning (FL) and transfer learning (TL) for cancer detection via image analysis. By integrating these techniques, research has shown improvements in diagnostic accuracy and efficiency. Specifically, the use of FL and TL has led to a measurable improvement in the precision of cancer diagnoses, with some studies reporting up to a 20% increase in accuracy compared to traditional methods. This synthesis of FL and TL optimizes distributed data usage while leveraging existing models to expedite learning and application in cancer detection tasks. A concrete assessment of the two methods, including their strengths and weaknesses, is presented. Moving on, their applications in cancer detection are discussed, including potential directions for the future. Finally, this article offers a thorough description of the functions of TL and FL in image-based cancer detection. The authors also make insightful suggestions for additional study in this rapidly developing area. The findings underscore the potential of these combined approaches to significantly advance medical imaging and cancer diagnosis, setting a promising direction for future research.},
  archive      = {J_NCA},
  author       = {Bechar, Amine and Medjoudj, Rafik and Elmir, Youssef and Himeur, Yassine and Amira, Abbes},
  doi          = {10.1007/s00521-024-10956-y},
  journal      = {Neural Computing and Applications},
  month        = {2},
  number       = {4},
  pages        = {2239-2284},
  shortjournal = {Neural Comput. Appl.},
  title        = {Federated and transfer learning for cancer detection based on image analysis},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Energy and QoS-aware virtual machine placement approach for IaaS cloud datacenter. <em>NCA</em>, <em>37</em>(4), 2211-2237. (<a href='https://doi.org/10.1007/s00521-024-10872-1'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Virtualization technology enables cloud providers to abstract, hide, and manage the underlying physical resources of cloud data centers in a flexible and scalable manner. It allows placing multiple independent virtual machines (VMs) on a single server in order to improve resource utilization and energy efficiency. However, determining the optimal VM placement is crucial as it directly impacts load balancing, energy consumption, and performance degradation within the data center. Furthermore, deciding on VM placement based on a single factor is usually insufficient to improve data center performance because many factors must be considered, and ignoring them may be too expensive. This paper improves a new multi-objective VM placement (MVMP) algorithm using a quantum particle swarm optimization (QPSO) technique. We call it QPSO-MOVMP, and its objective is to find the Pareto optimal solution for the VM placement problem by balancing different goals. This algorithm generates Pareto optimal solutions that save power by minimizing the number of running physical machines, avoid performance degradation by maintaining service level agreement (SLA), and improve load balancing by keeping server loads at optimal utilization. The experimental results show that QPSO-MOVMP had superior performance in terms of power consumption and performance degradation compared to three other multi-objective algorithms and three conventional single-objective algorithms. Simulation results show that the proposed QPSO-MOVMP achieves a consumption of 2.4 × 104 watts in power. Furthermore, it outperformed the others, achieving a minimum of 12% SLA breaches while experiencing a significant surge in requests from VMs. Moreover, the proposed model generated Pareto solutions that had a better distribution than those derived from a comparative method.},
  archive      = {J_NCA},
  author       = {Elsedimy, E. I. and Herajy, Mostafa and Abohashish, Sara M. M.},
  doi          = {10.1007/s00521-024-10872-1},
  journal      = {Neural Computing and Applications},
  month        = {2},
  number       = {4},
  pages        = {2211-2237},
  shortjournal = {Neural Comput. Appl.},
  title        = {Energy and QoS-aware virtual machine placement approach for IaaS cloud datacenter},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Comprehensive review of edge and contour detection: From traditional methods to recent advances. <em>NCA</em>, <em>37</em>(4), 2175-2209. (<a href='https://doi.org/10.1007/s00521-024-10936-2'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Edge and contour detection plays critical roles in computer vision and image processing, with extensive applications in advanced tasks including object recognition, shape matching, visual saliency, image segmentation, and inpainting. In recent decades, this field has attracted significant attention, leading to the development of numerous sophisticated methods that approximate human visual performance. Despite these advances, notable gaps remain. This review offers a comprehensive analysis of representative techniques, categorizing them into traditional and learning-based approaches, and examines their strengths and limitations to identify the underlying reasons for these gaps. Traditional methods are further divided into four sub-categories: local pattern, edge grouping, active contour, and bio-inspired techniques, with a specific emphasis on the promising potential of bio-inspired methods. Learning-based approaches, on the other hand, are classified into two types: classical learning, which typically relies on handcrafted features designed from empirical knowledge, and deep learning, which autonomously extracts features from large-scale datasets without human intervention. Additionally, benchmarks and evaluation metrics related to edge and contour detection are discussed, with potential issues identified within these frameworks. A quantitative assessment of representative methods is conducted across three popular benchmarks. Lastly, challenges and future prospects in edge and contour detection are explored, focusing on five key aspects: model architecture, learning strategies, feature extraction and fusion, method integration, and cross-domain applications. These considerations aim to bridge the gap with human visual perception. Overall, this work is expected to benefit researchers and advance progress in the field.},
  archive      = {J_NCA},
  author       = {Huang, Qinyuan and Huang, Jiaxiong},
  doi          = {10.1007/s00521-024-10936-2},
  journal      = {Neural Computing and Applications},
  month        = {2},
  number       = {4},
  pages        = {2175-2209},
  shortjournal = {Neural Comput. Appl.},
  title        = {Comprehensive review of edge and contour detection: From traditional methods to recent advances},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Revisiting recommender systems: An investigative survey. <em>NCA</em>, <em>37</em>(4), 2145-2173. (<a href='https://doi.org/10.1007/s00521-024-10828-5'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper provides a thorough review of recommendation methods from academic literature, offering a taxonomy that classifies recommender systems (RSs) into categories like collaborative filtering, content-based systems, and hybrid systems. It examines the effectiveness and challenges of these systems, such as filter bubbles, the "cold start" issue, and the reliance on collaborative filtering and content-based approaches. We trace the development of RSs, emphasizing the role of machine learning and deep learning models in overcoming these challenges and delivering more accurate, personalized, and context-aware recommendations. We also highlight the increasing significance of ethical considerations, including fairness, transparency, and trust, in the design of RSs. The paper presents a structured literature review, discussing various aspects of RSs, such as collaborative filtering, personalized recommender systems, and strategies to improve system robustness. It also points out the limitations of the existing approaches and suggests promising research directions for the future. In summary, this paper offers a comprehensive analysis of RSs, focusing on their evolution, challenges, and potential future improvements, particularly in enhancing accuracy, diversity, and ethical practices in recommendations.},
  archive      = {J_NCA},
  author       = {Ibrahim, Osman Ali Sadek and Younis, Eman M. G. and Mohamed, Ebtsam A. and Ismail, Walaa N.},
  doi          = {10.1007/s00521-024-10828-5},
  journal      = {Neural Computing and Applications},
  month        = {2},
  number       = {4},
  pages        = {2145-2173},
  shortjournal = {Neural Comput. Appl.},
  title        = {Revisiting recommender systems: An investigative survey},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Survey of software defect prediction features. <em>NCA</em>, <em>37</em>(4), 2113-2144. (<a href='https://doi.org/10.1007/s00521-024-10937-1'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Software defect prediction (SDP) is a technique that uses known software features and defect information to predict target software defects. It helps reduce software development costs, save development time, and improve software quality. The performance of SDP has always been the goal of this field. Many researchers have found that software features and their processing have a significant impact on the results of SDP. However, few reviews have summarized the features types, use, utility, processing and effect. So, we searched the six databases for SDP empirical papers. One hundred and forty-eight researches were obtained by using the inclusion and exclusion strategies. The information of feature types, processing methods, prediction schemes, modeling techniques, data sets, and performance evaluation metrics in these papers was collected and analyzed. We find that the rise of semantic and network features has broken the situation where only software metrics were used for SDP. These features demonstrate significant potential in prediction, especially semantic features. Deep learning is used increasingly in SDP, and deep learning feature extraction replaces the original feature selection based on machine learning. Although SDP performance continues to improve, issues such as data quality, feature stability, and scenes selection still deserve future research attention.},
  archive      = {J_NCA},
  author       = {Qiu, Shaoming and E, Bicong and He, Jingjie and Liu, Liangyu},
  doi          = {10.1007/s00521-024-10937-1},
  journal      = {Neural Computing and Applications},
  month        = {2},
  number       = {4},
  pages        = {2113-2144},
  shortjournal = {Neural Comput. Appl.},
  title        = {Survey of software defect prediction features},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). E-textiles in healthcare: A systematic literature review of wearable technologies for monitoring and enhancing human health. <em>NCA</em>, <em>37</em>(4), 2089-2111. (<a href='https://doi.org/10.1007/s00521-024-10947-z'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Integrating electronic textiles (E-Textiles) into healthcare presents significant advancements in patient monitoring and personalised care. This systematic literature review aims to assess the current state of E-Textile applications, focusing on their role in enhancing human health. Specifically, the review evaluates how E-Textiles contribute to real-time health monitoring, rehabilitation, and chronic disease management while identifying the challenges and opportunities for future implementation in healthcare systems. Following a systematic search of PubMed and IEEE Xplore, 48 studies were selected based on stringent inclusion criteria related to the design and functionality of E-Textiles for healthcare applications. These studies were analysed using the PRISMA framework, ensuring methodological rigour in selecting the most relevant literature. The review’s findings reveal that E-Textiles enable continuous, non-invasive monitoring of vital signs, improve patient engagement, and offer potential in remote healthcare delivery. Key advancements include sensor integration, IoT connectivity, and machine learning for health data analysis, which collectively enhance the personalisation and efficiency of medical interventions. However, challenges remain in areas such as cost, data privacy, and scalability within existing healthcare systems, particularly in resource-limited settings. Future applications of E-Textiles are expected to focus on expanding their use in personalised medicine, telehealth, and long-term patient care, promising a shift towards more accessible and efficient healthcare solutions. Hence, continued interdisciplinary research is essential to overcome current limitations and ensure the widespread adoption of this innovative technology.},
  archive      = {J_NCA},
  author       = {Wang, Chenjie and Fu, Lina and Ametefe, Divine Senanu and Wang, Suqi and John, Dah},
  doi          = {10.1007/s00521-024-10947-z},
  journal      = {Neural Computing and Applications},
  month        = {2},
  number       = {4},
  pages        = {2089-2111},
  shortjournal = {Neural Comput. Appl.},
  title        = {E-textiles in healthcare: A systematic literature review of wearable technologies for monitoring and enhancing human health},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Predicting and optimizing the mechanical properties of rejuvenated asphalt mix with RAP content. <em>NCA</em>, <em>37</em>(4), 2071-2087. (<a href='https://doi.org/10.1007/s00521-024-10553-z'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Reclaimed asphalt pavement (RAP) has grown in popularity in recent years due to its potential to blower costs and minimize negative effects on the environment. RAP incorporation, however, can also significantly influence the mechanical characteristics of asphalt mixtures, which can impact their general effectiveness and longevity. Due to their potential to improve the qualities of revitalized mixes with RAP, waste materials like waste engine oil (WEO) and waste cooking oil (WCO) have attracted interest for use in asphalt mixtures. This study focuses on predicting and optimizing the mechanical properties of revitalized asphalt mixtures using WCO and WEO along with RAP, particularly the modulus of resilience (MR) and indirect tensile strength (ITS). Classification and regression tree (CART) models were developed to forecast MR, ITS, and ITS loss% for asphalt mixes. It was found that the models could accurately predict the experimental data. With a WCO rejuvenator employed in less than or equal to 16.5% proportion, maximum MR and ITS were achieved. To get maximum MR, the asphalt content should not be more than 5.1%. On the other hand, WEO rejuvenator, asphalt content greater than 5.1%, and RAP content not greater than 45%, were used to achieve maximum durability (lowest ITS loss%). A 5% increase in the loss value is the result of choosing the design that provides the most strength. The study’s results encourage the adoption of environmentally friendly pavement building techniques by effectively reusing waste materials and improving the mechanical properties of revitalized asphalt mixtures.},
  archive      = {J_NCA},
  author       = {Islam, Md Kamrul and Gazder, Uneb and Al Mamun, Abdullah and Arifuzzaman, Md. and Al-Abdul Wahhab, Hamad Ibrahim and Rahman, Muhammad Muhitur},
  doi          = {10.1007/s00521-024-10553-z},
  journal      = {Neural Computing and Applications},
  month        = {2},
  number       = {4},
  pages        = {2071-2087},
  shortjournal = {Neural Comput. Appl.},
  title        = {Predicting and optimizing the mechanical properties of rejuvenated asphalt mix with RAP content},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A comprehensive systematic review of machine learning in the retail industry: Classifications, limitations, opportunities, and challenges. <em>NCA</em>, <em>37</em>(4), 2035-2070. (<a href='https://doi.org/10.1007/s00521-024-10869-w'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Machine learning has profoundly transformed various industries, notably revolutionizing the retail sector through diverse applications that significantly enhance operational efficiency and performance. This comprehensive review examines the state-of-the-art machine learning applications in the retail sector from 2019 to 2024, focusing on supervised learning, unsupervised learning, and ensemble methods. It aims to identify and categorize recent machine learning applications in retail, evaluate the performance of machine learning algorithms, and determine the most suitable algorithms for specific retail use cases. This review article examines 56 studies and identifies 20 unique machine learning applications within the retail sector. This review also discusses the challenges and opportunities of implementing machine learning in retail, offering valuable insights to guide future research and enhance retail performance and customer satisfaction. The findings highlight the strengths and limitations of different machine learning methods, providing insights into their practical applications and future potential.},
  archive      = {J_NCA},
  author       = {Hassan, Dler O. and Hassan, Bryar A.},
  doi          = {10.1007/s00521-024-10869-w},
  journal      = {Neural Computing and Applications},
  month        = {2},
  number       = {4},
  pages        = {2035-2070},
  shortjournal = {Neural Comput. Appl.},
  title        = {A comprehensive systematic review of machine learning in the retail industry: Classifications, limitations, opportunities, and challenges},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). The role of explainability and transparency in fostering trust in AI healthcare systems: A systematic literature review, open issues and potential solutions. <em>NCA</em>, <em>37</em>(4), 1999-2034. (<a href='https://doi.org/10.1007/s00521-024-10868-x'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The healthcare sector has advanced significantly as a result of the ability of artificial intelligence (AI) to solve cognitive problems that once required human intelligence. As artificial intelligence finds more applications in healthcare, trustworthiness must be guaranteed. Even while AI has the potential to improve healthcare, there are still challenging issues because it is yet to be widely adopted, especially when it comes to transparency. Concerns about comprehending the internal workings of AI models, possible biases, model robustness, and generalizability are raised by their opacity which makes them function like black boxes. A solution for worries over the transparency of AI algorithms is explainable AI. Explainable AI seeks to enhance AI explainability and analytical capabilities, particularly in vital industries like healthcare. Even though earlier research has examined several explainable AI-related topics, such as a lexicon, industry-specific overviews, and applications in the healthcare industry, a thorough analysis concentrating on the function of explainable AI in building trust in AI healthcare systems is required. In an effort to close this gap, a systematic literature review that adheres to PRISMA principles that analyze relevant papers that were published between 2015 and 2023 was done in this paper. To determine the critical role that explainable AI plays in fostering trust, this study examines widely utilized methodologies, machine learning and deep learning techniques, datasets, performance measures and validation procedures used in AI healthcare research. In addition, research issues and potential research directions are also discussed in this research. Thus, this systematic review provides a thorough summary of the present status of research on explainability and transparency in AI healthcare systems, thus illuminating crucial factors that affect user trust. The results are intended to assist researchers, policymakers and healthcare professionals in developing a more transparent, responsible and reliable AI system in the healthcare sector.},
  archive      = {J_NCA},
  author       = {Eke, Christopher Ifeanyi and Shuib, Liyana},
  doi          = {10.1007/s00521-024-10868-x},
  journal      = {Neural Computing and Applications},
  month        = {2},
  number       = {4},
  pages        = {1999-2034},
  shortjournal = {Neural Comput. Appl.},
  title        = {The role of explainability and transparency in fostering trust in AI healthcare systems: A systematic literature review, open issues and potential solutions},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Generalization potential of large language models. <em>NCA</em>, <em>37</em>(4), 1973-1997. (<a href='https://doi.org/10.1007/s00521-024-10827-6'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The rise of deep learning techniques and especially the advent of large language models (LLMs) intensified the discussions around possibilities that artificial intelligence with higher generalization capability entails. The range of opinions on the capabilities of LLMs is extremely broad: from equating language models with stochastic parrots to stating that they are already conscious. This paper represents an attempt to review LLM landscape in the context of their generalization capacity as an information theoretic property of those complex systems. We discuss the suggested theoretical explanations for generalization in LLMs and highlight possible mechanisms responsible for these generalization properties. Through an examination of existing literature and theoretical frameworks, we endeavor to provide insights into the mechanisms driving the generalization capacity of LLMs, thus contributing to a deeper understanding of their capabilities and limitations in natural language processing tasks.},
  archive      = {J_NCA},
  author       = {Budnikov, Mikhail and Bykova, Anna and Yamshchikov, Ivan P.},
  doi          = {10.1007/s00521-024-10827-6},
  journal      = {Neural Computing and Applications},
  month        = {2},
  number       = {4},
  pages        = {1973-1997},
  shortjournal = {Neural Comput. Appl.},
  title        = {Generalization potential of large language models},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A survey of recent machine learning techniques for stock prediction methodologies. <em>NCA</em>, <em>37</em>(4), 1951-1972. (<a href='https://doi.org/10.1007/s00521-024-10867-y'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The prime purpose of the research is to investigate stock price prediction techniques and their shortcomings concerning particular characteristics and performance measures. The research uses performance metric analysis, dataset analysis, and bibliographic analysis to determine the current state of recently published research on financial market prediction. The research examines how well machine learning models predict stock market performance, emphasizing how accuracy, precision, and recall are often used as performance measures. The researchers thoroughly analyzed 24 publications, detailing the data elements that were employed, such as historical datasets and technical indicators, and criticized related studies for frequently omitting the adjusted closing price. The research indicates that since Adj Close captures closing opinions from important market participants, it is essential for precise stock prediction. This research opens the door for further research into feature selection and how it affects prediction accuracy by illuminating how these machine learning models behave when other characteristics are added. Previous research has shown that machine learning methods such as long short-term memory and support vector machines are often used for stock price prediction with some data optimization. The performance metrics that were employed to assess the performance were also examined. The researchers have reported that rather than being regression-based, the most often utilized metrics are classification-based. Performance is also measured via other metrics, such as the Sharpe ratio and accumulated error. The findings will assist financial market researchers in developing creative concepts and selecting the most useful criteria from the data that have been provided.},
  archive      = {J_NCA},
  author       = {Vishwakarma, Vijay Kumar and Bhosale, Narayan P.},
  doi          = {10.1007/s00521-024-10867-y},
  journal      = {Neural Computing and Applications},
  month        = {2},
  number       = {4},
  pages        = {1951-1972},
  shortjournal = {Neural Comput. Appl.},
  title        = {A survey of recent machine learning techniques for stock prediction methodologies},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Deep learning-based EEG emotion recognition: A comprehensive review. <em>NCA</em>, <em>37</em>(4), 1919-1950. (<a href='https://doi.org/10.1007/s00521-024-10821-y'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Emotion recognition has been used in a wide range of different fields, such as human–computer interaction, safe driving, education and medical treatment. Compared with text, speech, expression and other physiological signals, electroencephalogram (EEG) signals can reflect an individual's emotion states more directly, objectively and accurately, and are less affected by the individual’s subjective consciousness. Following the continuous progress of EEG acquisition equipment over recent years, the acquisition of EEG signals has become increasingly convenient. Emotion recognition based on EEG signals has attracted more and more interest from a wide variety of researchers. This paper first reviews the basic theory of emotion recognition, including discrete and continuous emotion models, the development of acquisition equipment and the internal relationship between EEG signals and emotion recognition. A review of the application of deep learning algorithms to EEG emotion recognition is then presented, with a focus on the extraction of deep EEG features and emotion recognition by single deep learning models, attention-based deep learning models, hybrid deep learning models and domain adaptation-based deep learning models. Taking the public datasets SEED, DEAP, SEED-IV, DREAMER and MPED as examples, representative methods of subject-dependent and subject-independent experiments are quantitatively compared and analysed, and the significance of subject-independent research is indicated. Finally, aiming at the problems of dataset's application limitations, model optimisation and subject variability in existing research, the possible solutions to the corresponding problems are summarised, and the future development trends and prospects are proposed.},
  archive      = {J_NCA},
  author       = {Geng, Yuxiao and Shi, Shuo and Hao, Xiaoke},
  doi          = {10.1007/s00521-024-10821-y},
  journal      = {Neural Computing and Applications},
  month        = {2},
  number       = {4},
  pages        = {1919-1950},
  shortjournal = {Neural Comput. Appl.},
  title        = {Deep learning-based EEG emotion recognition: A comprehensive review},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Discrete-time double-integral zeroing neural dynamics for time-varying equality-constrained quadratic programming with application to manipulators. <em>NCA</em>, <em>37</em>(4), 1905-1918. (<a href='https://doi.org/10.1007/s00521-024-10708-y'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Neural dynamics remains a crucial field of interest for researchers, owing to its extensive applicability in addressing time-varying challenges across diverse domains. This study innovatively integrates discrete-time processing with neural dynamics principles, introducing a discrete-time double-integral zeroing neural dynamics (DTDIZND) method to address real-time-varying equality-constrained quadratic programming (ECQP) problems in diverse noisy environments. The DTDIZND model offers a robust solution. Theoretical analyses reveal that the DTDIZND model excels in real-time calculations with remarkable precision, effectively managing multiple noise sources. For comparative analysis, the existing discrete-time zeroing neural dynamics models are evaluated, addressing the same time-varying problems. Relative numerical experiments have been undertaken, further strengthening the evidence of the DTDIZND model’s efficiency and preponderance in managing diverse noise scenarios. In addition, the DTDIZND method is applied to robot manipulator motion planning, particularly in scenarios where diverse noise sources pose challenges. It is a promising tool for addressing time-varying challenges in various domains given its ability to handle real-time calculations with precision, coupled with its resilience against noise.},
  archive      = {J_NCA},
  author       = {Xiang, Qiuhong and Gong, Hongfang},
  doi          = {10.1007/s00521-024-10708-y},
  journal      = {Neural Computing and Applications},
  month        = {2},
  number       = {4},
  pages        = {1905-1918},
  shortjournal = {Neural Comput. Appl.},
  title        = {Discrete-time double-integral zeroing neural dynamics for time-varying equality-constrained quadratic programming with application to manipulators},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Computational intelligence approach to automated sentiment analysis of arabic tweets. <em>NCA</em>, <em>37</em>(4), 1889-1904. (<a href='https://doi.org/10.1007/s00521-024-10808-9'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper introduces the "Intelligent Analysis System for Arabic Tweets" (IASAT), an innovative framework designed to tackle the specific challenges of sentiment analysis in the Arabic language, particularly in educational contexts. IASAT was developed to analyze student sentiments from social media platforms like Twitter, addressing the unique complexities of Arabic, including its rich morphology, diverse dialects, and syntactic variations. The system employs advanced machine learning algorithms, allowing for more precise sentiment classification tailored to the intricacies of Arabic, making it particularly useful for understanding feedback in real time. IASAT achieved significant improvements over traditional methods, with accuracy rates averaging 82.4%, and reaching as high as 93% in some cases. Moreover, the system demonstrated a 12.8% reduction in computational latency, ensuring both efficiency and scalability for large-scale sentiment analysis tasks. These results highlight IASAT's potential as a powerful tool for educational institutions to analyze student feedback, offering actionable insights that can be used to improve educational services. IASAT represents a practical and scalable solution for the challenges of Arabic sentiment analysis, enabling more accurate and efficient understanding of student opinions on social.},
  archive      = {J_NCA},
  author       = {Eldakhly, Nabil M. and Zaher, Mahmoud A. and Hassan, Yahia B.},
  doi          = {10.1007/s00521-024-10808-9},
  journal      = {Neural Computing and Applications},
  month        = {2},
  number       = {4},
  pages        = {1889-1904},
  shortjournal = {Neural Comput. Appl.},
  title        = {Computational intelligence approach to automated sentiment analysis of arabic tweets},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Adaptive pseudo-label threshold for source-free domain adaptation. <em>NCA</em>, <em>37</em>(4), 1875-1887. (<a href='https://doi.org/10.1007/s00521-024-10697-y'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Source-free domain adaptation (SFDA) endeavors to utilize a source domain model for solving analogous tasks in a novel unlabeled domain, even when the source data remains inaccessible. However, existing SFDA methods ignore the class imbalance and unlabeled data noise, referred to as the class gap, which causes evident differences in the adapting difficulties of classes. To address the above limitations, we propose a novel threshold adaptive strategy (TAS) for SFDA. Specifically, to mitigate the negative effects of class imbalance, we design a Threshold Adapter that automatically modulates domain and class thresholds. Then, to counteract the unlabeled data noise, the Consistency Constraint Learner is employed to enhance the similarity between different augmentations of the same sample. In this way, the class gap can be efficiently spanned by adjusting the pseudo-label threshold and consistency constraints. Extensive experimental evaluations underscore the superiority of our method, positioning it as a robust baseline for future research in SFDA, and our approach extends beyond single-source scenarios to encompass multisource, multi-target, partial-set, and open-set benchmarks.},
  archive      = {J_NCA},
  author       = {Shao, Mingwen and Chen, Sijie and Wang, Fan and Zhang, Lixu},
  doi          = {10.1007/s00521-024-10697-y},
  journal      = {Neural Computing and Applications},
  month        = {2},
  number       = {4},
  pages        = {1875-1887},
  shortjournal = {Neural Comput. Appl.},
  title        = {Adaptive pseudo-label threshold for source-free domain adaptation},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Nature-inspired optimization techniques for cardiovascular disease detection: A comprehensive survey. <em>NCA</em>, <em>37</em>(4), 1839-1874. (<a href='https://doi.org/10.1007/s00521-024-10541-3'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {One of the most common illnesses that can shorten a person’s life span nowadays is heart disease. The early detection of aberrant heart diseases is crucial for identifying heart issues and preventing sudden cardiac death. Cardiovascular diseases, commonly referred to as heart diseases, are brought on by bad lifestyle choices including smoking, drinking alcohol, and eating a lot of fats, which can led to diabetes, hypertension, and other conditions. They include coronary heart disease and are brought on by conditions of the heart and blood arteries (heart attacks). The benefit of using optimization methods for complicated nonlinear situations is their adaptability and flexibility. This research provides a concise analysis of the most researched combination of various optimization techniques and machine learning techniques for cardiac disease prediction currently available in the literature along with the variations of several nature-inspired algorithms. These algorithms are also compared with regard to convergence, accuracy, feature reduction, and other aspects of nature-inspired optimization. The last 10 years’ worth of heart disease datasets are also thoroughly reviewed. This paper’s objective is to review various optimization methods like particle swarm optimization (PSO), ant colony optimization (ACO), artificial bee colony (ABC), spider monkey optimization (SMO), and many more nature-inspired optimization methods are discussed for effective cardiac disease diagnosis. The results of the investigation demonstrate that the accuracy rates for logistic regression with particle swarm optimization (PSO) is 98.33% and 65% accuracy, when k-closest neighbor (k-NN) is utilized with particle swarm optimization (PSO). These numbers are high when compared to the current decision tree, naive Bayes, and random forest approaches. According to that analysis, the Cleveland heart disease dataset is particularly well-liked by academics because it includes less missing values. It is clear from the thorough research and comparison that particle swarm optimization (PSO) will deliver superior results in comparison with other techniques currently being used for the detection of cardiac problems.},
  archive      = {J_NCA},
  author       = {Sharma, Siddhi Kumari and Goel, Lavika and Mittal, Namita},
  doi          = {10.1007/s00521-024-10541-3},
  journal      = {Neural Computing and Applications},
  month        = {2},
  number       = {4},
  pages        = {1839-1874},
  shortjournal = {Neural Comput. Appl.},
  title        = {Nature-inspired optimization techniques for cardiovascular disease detection: A comprehensive survey},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Intensity inhomogeneity correction in brain MRI: A systematic review of techniques, current trends and future challenges. <em>NCA</em>, <em>37</em>(4), 1821-1838. (<a href='https://doi.org/10.1007/s00521-024-10749-3'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Intensity inhomogeneity, a common artefact in brain magnetic resonance imaging, poses challenges in medical image analysis. Intensity inhomogeneity, also known as bias field, occurs in magnetic resonance images due to factors such as magnetic field non-uniformity, radiofrequency coil sensitivity, tissue properties, patient-related factors, scanner artefacts. It creates intensity non-uniformity inside the homogeneous tissue regions of the brain images. Thereby degrading the performance of diagnosis assessment. This systematic review proposes a first hand categorization of a range of methodologies for intensity inhomogeneity correction. In particular, an overview of retrospective techniques including the filtering methods, computational intelligence methods, fuzzy models, learning-based approaches, etc. is included. This paper also presents the emergence of learning-based techniques in developing the intensity inhomogeneity correction techniques. Additionally, major challenges, current trends, and future directions for research and development are discussed. Moreover, the characteristics of choosing a suitable method and the appropriate evaluation metric are elaborately presented. This paper may serve as a comprehensive resource for researchers, clinicians, and engineers interested in enhancing the quality and reliability of brain image analysis through effective intensity inhomogeneity correction techniques.},
  archive      = {J_NCA},
  author       = {Mishro, Pranaba K. and Agrawal, Sanjay and Panda, Rutuparna and Dora, Lingraj and Abraham, Ajith},
  doi          = {10.1007/s00521-024-10749-3},
  journal      = {Neural Computing and Applications},
  month        = {2},
  number       = {4},
  pages        = {1821-1838},
  shortjournal = {Neural Comput. Appl.},
  title        = {Intensity inhomogeneity correction in brain MRI: A systematic review of techniques, current trends and future challenges},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Advancements and challenges in fingerprint presentation attack detection: A systematic literature review. <em>NCA</em>, <em>37</em>(4), 1797-1819. (<a href='https://doi.org/10.1007/s00521-024-10423-8'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the rapidly evolving domain of biometric security, the significance of Fingerprint Presentation Attack Detection (FPAD) has become increasingly paramount, given the susceptibility of Automatic Fingerprint Identification System (AFIS) to advanced spoofing techniques. This systematic literature review (SLR), spanning from 2022 to the second quarter of 2024, delves into the intricate challenges and burgeoning opportunities within FPAD. It focuses on innovative methodologies for detecting presentation attacks, the prevalent challenges posed by spoof fabrications (including materials like silicone, gelatine, and latex), and the exploration of potential advancements in FPAD effectiveness. The comprehensive analysis, based on a rigorous review protocol, scrutinizes 40 seminal peer-reviewed articles from the IEEE Xplore and ScienceDirect databases. This exploration uncovers a diverse range of strategies in FPAD, including software-centric and hardware-assisted approaches, each bearing unique implications for security enhancement and user privacy considerations. A pivotal finding of this review is the identification of critical research gaps, particularly in the development of algorithms capable of universal detection, the system’s adaptability to novel spoofing materials, and the ethical management of biometric data. This review provides a contemporary assessment of the current state of FPAD and establishes a foundation for future research directions. It highlights the need for continuous innovation in response to the evolving sophistication of spoofing techniques and the imperative of maintaining a balance between robust security measures and user-centric design in biometric systems. This review underscores the dynamic interplay between technological advancements, the ingenuity of attackers, and the ongoing endeavour to achieve reliable, user-friendly, and ethically responsible biometric security solutions.},
  archive      = {J_NCA},
  author       = {Ametefe, Divine Senanu and Sarnin, Suzi Seroja and Ali, Darmawaty Mohd and Ametefe, George Dzorgbenya and John, Dah and Hussin, Norhayati},
  doi          = {10.1007/s00521-024-10423-8},
  journal      = {Neural Computing and Applications},
  month        = {2},
  number       = {4},
  pages        = {1797-1819},
  shortjournal = {Neural Comput. Appl.},
  title        = {Advancements and challenges in fingerprint presentation attack detection: A systematic literature review},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Learning dual aggregate features for face forgery detection. <em>NCA</em>, <em>37</em>(4), 1783-1795. (<a href='https://doi.org/10.1007/s00521-024-10700-6'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The rapid development of face forgery technologies brings security issues, and the importance of face forgery detection has increased. While some existing methods deliver satisfactory results when both training and testing occur on the same dataset, these methods struggle to generalize across unseen forgery datasets. Some works consider extracting unseen forgeries in terms of high-frequency information for judgment, but the lack of synergistic global considerations for such forgeries and features of the original RGB image tends to result in overfitting to specific local textures, making it difficult to further improve the generalization. In this work, to address this challenge, we propose a novel two-stream CNN-based face forgery detector. This detector synergistically combines RGB features with global high-frequency constrained forgery features, enhancing the effectiveness of face forgery detection. To this end, we design three components: 1) Multi-scale Aggregated Constrained Convolution (MACC) module. It creates modalities that both preserve comprehensive information and accentuate forgery traces. 2) Dual Spatial Aggregation Enhance (DSAE) module, which globally and synergistically aggregates and enhances features from both streams. 3) Dual Channel Enhance Aggregation (DCEA) module, which harmonizes the information across the two-stream channels based on high correlation between the streams and performs mutual enhancement. Our experimental results demonstrate that our method excels in face forgery detection, thereby achieving an AUC of $$99.63\%$$ on the FF++ dataset, surpassing the existing state-of-the-art two-stream network-based detection methods.},
  archive      = {J_NCA},
  author       = {Kou, Yuru and Jiang, Qian and Zhang, Jun and Jin, Xin and Wei, Ping and Miao, Shengfa and Chu, Xing},
  doi          = {10.1007/s00521-024-10700-6},
  journal      = {Neural Computing and Applications},
  month        = {2},
  number       = {4},
  pages        = {1783-1795},
  shortjournal = {Neural Comput. Appl.},
  title        = {Learning dual aggregate features for face forgery detection},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). TinyBERT for branch prediction in modern microprocessors. <em>NCA</em>, <em>37</em>(4), 1771-1782. (<a href='https://doi.org/10.1007/s00521-024-10535-1'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent progress has highlighted the crucial importance of branch prediction (BP) in optimizing computer performance, especially in reducing computational delays by preventing stalls in modern microprocessors (also known as CPUs). In this paper, we investigate the use of machine learning (ML) models to improve BP accuracy, focusing on the capabilities of transformer models based on their exceptional predictive and classification performance. Although existing studies have employed various ML methods for BP, their selected models are computationally expensive and impractical for such task. Hence, we present an advanced ML-based dynamic BP technique utilizing tiny bidirectional encoder representations from transformers (TinyBERT), notable for its efficiency, simplicity, and low resource utilization. This method not only streamlines the BP process but also offers a more effective alternative to conventional strategies. A key aspect of our approach is the application of local post hoc explanations, which provide deep insights into the model’s predictive actions. Our empirical findings reveal that this methodology secures a substantial 13% reduction in the rate of mispredictions compared to top predictors like TAGE-SC-L, across various multimedia and integer application benchmarks. These results underscore the potential of using compact transformers in establishing significant criteria for efficient and effective BP.},
  archive      = {J_NCA},
  author       = {Alajmi, Anwar and AlSarraf, Bashair and Abualhassan, Zainab and Fairouz, Abbas A. and Ahmad, Imtiaz},
  doi          = {10.1007/s00521-024-10535-1},
  journal      = {Neural Computing and Applications},
  month        = {2},
  number       = {4},
  pages        = {1771-1782},
  shortjournal = {Neural Comput. Appl.},
  title        = {TinyBERT for branch prediction in modern microprocessors},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Distributed data processing and task scheduling based on GPU parallel computing. <em>NCA</em>, <em>37</em>(4), 1757-1769. (<a href='https://doi.org/10.1007/s00521-024-10489-4'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Distributed data parallel (DDP) computing ensures data parallelism, enabling execution across several computers. A separate distributed data parallel computing instance should be created for each process that uses distributed data parallel computing. Task scheduling in parallel processing employs various methods and strategies to minimize the number of delayed jobs. Reliability in terms of a system’s capacity to distribute work over several machines and computers is improved by workload sharing, task migration, and automatic task replication; data transmission and reception are facilitated via the internet. The graphics processing unit (GPU) is a highly specialized electrical circuit. Compared to a traditional computer, a GPU parallel structure allows faster computation and greater effectiveness. GPUs are employed in parallel processing in new methods for DDP-GPUs with more processors, allowing a more significant workload to be handled in portions. The execution time of a program can be reduced by distributing various task areas among many processors. This study employs various multitasking scenarios, from simulated to real-world cases and from graphically heavy applications to parallel-processing workloads. The research compares the efficacy of several allocation algorithms, illuminating how best to divide GPU resources among multiple processes. By sorting through a vast volume of data more slowly, parallel computing saves time and money. Parallel and distributed computing concern the use of numerous computing resources to improve the results of a distributed and computationally expensive application. A single computer or a group of computers connected via a network can be used for computation. Our experimental findings show that the DPP-GPU implements task scheduling 95.3% better than the conventional GPU. The proposed model improves system reliability with an average execution time of 25.7 s. The time needed to run a program divided by the machine’s cost is approximately 95% when utilizing GPU execution ratio analysis to measure the parallel system’s efficacy. The high stability ratio between tasks helps us turn preferences into a distribution of probability by examining the study results. A task-based processing evaluation ratio of 98.45% allows the automatic execution of specified tasks if specific criteria are satisfied.},
  archive      = {J_NCA},
  author       = {Li, Jun},
  doi          = {10.1007/s00521-024-10489-4},
  journal      = {Neural Computing and Applications},
  month        = {2},
  number       = {4},
  pages        = {1757-1769},
  shortjournal = {Neural Comput. Appl.},
  title        = {Distributed data processing and task scheduling based on GPU parallel computing},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multimedia content recommendation algorithm based on behavior and knowledge feature embedding. <em>NCA</em>, <em>37</em>(4), 1741-1756. (<a href='https://doi.org/10.1007/s00521-024-09813-9'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As internet information technology continues to advance, individuals are increasingly encountering and managing a vast volume of data and information. A large and complex amount of information hinders the effective transmission of valuable information, making it difficult to find multimedia content of interest in the vastness of the internet. As the volume of multimedia content rapidly grows, personalized recommendation algorithms play a crucial role in matching relevant content to users. Knowledge graphs, due to their powerful organizational and relationship processing capabilities, are commonly applied in intelligent search engines and recommendation systems. This article focuses on the effective utilization of semantic association information in knowledge graphs for multimedia content recommendation. Two main areas of research are conducted. In this article, two novel approaches are presented. To begin with, the primary objective is to improve the learning of knowledge feature representation. This is achieved by introducing a model based on self-attention, which effectively captures the diverse significance of triplets in determining the semantics of entities. This leads to improved quality of knowledge feature representation, thereby serving as valuable auxiliary information for multimedia content recommendation systems. Secondly, the article addresses the integration of knowledge graphs in multimedia content recommendation applications. This paper proposes a content recommendation algorithm that integrates a combined embedding of behavior and knowledge features. By leveraging past preferences and utilizing the semantic structure of knowledge graphs, this algorithm provides a comprehensive exploration of user interests and hobbies. Ultimately, this conducts extensive experiments to assess the effectiveness and performance of the proposed algorithms. The results validate the feasibility and efficacy of these algorithms in enhancing multimedia content recommendation systems.},
  archive      = {J_NCA},
  author       = {Ji, Zhijun},
  doi          = {10.1007/s00521-024-09813-9},
  journal      = {Neural Computing and Applications},
  month        = {2},
  number       = {4},
  pages        = {1741-1756},
  shortjournal = {Neural Comput. Appl.},
  title        = {Multimedia content recommendation algorithm based on behavior and knowledge feature embedding},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A manipulator control method based on deep deterministic policy gradient with parameter noise. <em>NCA</em>, <em>37</em>(4), 1729-1739. (<a href='https://doi.org/10.1007/s00521-024-10492-9'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Focusing on the motion control problem of two link manipulator, three deep reinforcement learning models named the deep deterministic policy gradient (DDPG), asynchronous advantage actor-critic(A3C) and distributed proximal policy optimization (DPPO) are established for training according to the target setting, state variables and reward & punishment mechanism of the environment model. And then the motion control of two link manipulator is realized. After comparing and analyzing the three models, the traditional DDPG approach based on action noise converges faster and has a higher average reward compared to the other two algorithms. So, DDPG approach based on parameter noise is designed for further research to improve its applicability, so as to cut down the debugging time of the manipulator model and reach the goal smoothly. The experimental results indicate that the DDPG approach based on parameter noise can control the motion of two link manipulator effectively. The convergence speed of the control model is significantly promoted and the stability after convergence is improved. In comparison with the traditional control approach, the DDPG control approach based on parameter noise has higher efficiency and stronger applicability.},
  archive      = {J_NCA},
  author       = {Zhang, Haifei and Xu, Jian and Lei, Liting and Wu, Fang and Qian, Lanmei and Qiu, Jianlin},
  doi          = {10.1007/s00521-024-10492-9},
  journal      = {Neural Computing and Applications},
  month        = {2},
  number       = {4},
  pages        = {1729-1739},
  shortjournal = {Neural Comput. Appl.},
  title        = {A manipulator control method based on deep deterministic policy gradient with parameter noise},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Editorial on cyber security intelligence and analytics 2023. <em>NCA</em>, <em>37</em>(4), 1727. (<a href='https://doi.org/10.1007/s00521-024-10873-0'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_NCA},
  author       = {Xu, Zheng and Loyola-González, Octavio},
  doi          = {10.1007/s00521-024-10873-0},
  journal      = {Neural Computing and Applications},
  month        = {2},
  number       = {4},
  pages        = {1727},
  shortjournal = {Neural Comput. Appl.},
  title        = {Editorial on cyber security intelligence and analytics 2023},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Deep learning and supervised learning techniques for modeling and prediction of strength of ground granulated blast furnace slag based sustainable mortar. <em>NCA</em>, <em>37</em>(3), 1709-1726. (<a href='https://doi.org/10.1007/s00521-024-10736-8'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper presents an analysis and synthesis of deep learning and supervised learning techniques for predicting the flexural tensile strength and compressive strength of mortars containing ground granulated blast furnace slag (GGBFS). The goal is to save time and reduce the costs associated with experimental testing. Initially, mechanical tests were conducted experimentally, using GGBFS as a partial replacement for cement in the mortar, with replacement levels ranging from 0 wt% to 80 wt% in 1% intervals. The results indicated a nearly linear decrease in flexural tensile strength and a non-linear change in compressive strength with increasing GGBFS content. To address the limitations of traditional experimental methods and improve processes, deep learning and supervised learning approaches were explored and compared. Evaluation metrics such as Root Mean Squared Error (RMSE), Mean Absolute Error (MAE), Mean Squared Error (MSE), and R-squared demonstrated that the deep learning model achieved higher sensitivity and efficiency for both 7-day and 28-day flexural tensile strength, as well as 7-day and 28-day compressive strength. Quantitatively, the deep learning model outperformed traditional models for 28-day flexural tensile strength, with an RMSE of 0.6779, an R-squared value of 0.90, an MSE of 0.4596, and an MAE of 0.6045, and for 28-day compressive strength, with an RMSE of 0.6808, an R-squared value of 0.99, an MSE of 0.4636, and an MAE of 0.4891. These findings suggest that deep learning is a promising method for accurately modeling and predicting the mechanical properties of GGBFS-based mortars, particularly for the 28-day strengths.},
  archive      = {J_NCA},
  author       = {Gürsoy-Demir, Handan and Ozturk, Murat},
  doi          = {10.1007/s00521-024-10736-8},
  journal      = {Neural Computing and Applications},
  month        = {1},
  number       = {3},
  pages        = {1709-1726},
  shortjournal = {Neural Comput. Appl.},
  title        = {Deep learning and supervised learning techniques for modeling and prediction of strength of ground granulated blast furnace slag based sustainable mortar},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Enhancing knowledge-aware recommendation with a cross-view contrastive learning. <em>NCA</em>, <em>37</em>(3), 1693-1708. (<a href='https://doi.org/10.1007/s00521-024-10699-w'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Knowledge graph (KG) can effectively address the sparsity issue in recommendation systems. In recent years, graph neural networks (GNNs) have gained popularity in knowledge-aware recommendation (KGR) due to their powerful capability in modeling graph structures. However, existing GNN-based methods have the following limitations: (1) Primarily focusing on extracting collaborative signals between items from the user-item graph and neglecting the influence of preferences between different users. (2) The sparsity of user interaction data is a problem, and mining item knowledge associations on the KG with limited interaction data as a supervision signal will lead to limited performance improvement. In this paper, we propose an efficient cross-view contrastive learning method, KECL, to address the above challenges. Specifically, we first construct a user-user social graph based on the user-item graph to capture potential social connections among users. Then, we selectively utilize the entity information demanded by users and items to construct user-entity and item-entity graphs. Based on this, we design two contrastive loss tasks to perform contrastive learning on the above four graph views from both user and knowledge levels. This approach enables us to model the influence of preferences between users with social connections. It also allows us to efficiently mine item knowledge associations through a self-supervised paradigm, thereby learning high-quality node representations. Experimental results on three publicly available datasets demonstrate that our KECL outperforms state-of-the-art methods.},
  archive      = {J_NCA},
  author       = {Zhao, Ge and Zu, Shuaishuai and Yang, Zhisheng and Li, Li},
  doi          = {10.1007/s00521-024-10699-w},
  journal      = {Neural Computing and Applications},
  month        = {1},
  number       = {3},
  pages        = {1693-1708},
  shortjournal = {Neural Comput. Appl.},
  title        = {Enhancing knowledge-aware recommendation with a cross-view contrastive learning},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Ohabm-net: An enhanced attention-driven hybrid network for improved breast mass detection. <em>NCA</em>, <em>37</em>(3), 1673-1691. (<a href='https://doi.org/10.1007/s00521-024-10545-z'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Breast cancer begins in the breast tissues and can progressively spread to other parts of the body. Early detection is crucial, as it allows for timely treatment, potentially saving lives. Researchers have devised methods to detect cancer in its early stages. However, the majority of the approaches primarily utilize either attention-based deep models or handcrafted features-based models for providing local information. However, both these approaches lack the ability to provide crucial local information for precise tumor detection. Additionally, available breast cancer datasets are inherently imbalanced. To address these challenges, this paper presents the Optimized Hybrid Attention Breast Mass Network (OHABM-Net) for breast cancer detection. The proposed OHABM-Net uses a newly developed hybrid attention-based feature extraction network that combines attention-based deep features and handcrafted features using HOG which provides precise local information thereby enhancing overall performance of the system. Moreover, the proposed model incorporates the Borderline Synthetic Minority Over-sampling Technique (BSMOTE) to resolve the class imbalance problem. Furthermore, to improve the performance of the system, the proposed model incorporates BM3D denoising filter and hill climbing-based optimization method to fine-tune the feature extraction network, culminating in classification through SVM. The proposed OHABM-Net evaluates with the BUSI and UDIAT datasets, achieving average accuracies of 98.11% and 96.78%, respectively, which surpass the performance of existing models.},
  archive      = {J_NCA},
  author       = {Abhisheka, Barsha and Biswas, Saroj Kr. and Purkayastha, Biswajit},
  doi          = {10.1007/s00521-024-10545-z},
  journal      = {Neural Computing and Applications},
  month        = {1},
  number       = {3},
  pages        = {1673-1691},
  shortjournal = {Neural Comput. Appl.},
  title        = {Ohabm-net: An enhanced attention-driven hybrid network for improved breast mass detection},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A numerical magnitude aware multi-channel hierarchical encoding network for math word problem solving. <em>NCA</em>, <em>37</em>(3), 1651-1672. (<a href='https://doi.org/10.1007/s00521-024-10695-0'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Math word problem (MWP) represents a critical research area within reading comprehension, where accurate comprehension of math problem text is crucial for generating math expressions. However, current approaches still grapple with unresolved challenges in grasping the sensitivity of math problem text and delineating distinct roles across various clause types, and enhancing numerical representation. To address these challenges, this paper proposes a Numerical Magnitude Aware Multi-Channel Hierarchical Encoding Network (NMA-MHEA) for math expression generation. Firstly, NMA-MHEA implements a multi-channel hierarchical context encoding module to learn context representations at three different channels: intra-clause channel, inter-clause channel, and context-question interaction channel. NMA-MHEA constructs hierarchical constituent-dependency graphs for different levels of sentences and employs a Hierarchical Graph Attention Neural Network (HGAT) to learn syntactic and semantic information within these graphs at the intra-clause and inter-clause channels. NMA-MHEA then refines context clauses using question information at the context-question interaction channel. Secondly, NMA-MHEA designs a number encoding module to enhance the relative magnitude information among numerical values and type information of numerical values. Experimental results on two public benchmark datasets demonstrate that NMA-MHEA outperforms other state-of-the-art models.},
  archive      = {J_NCA},
  author       = {Xu, Junjie and Chen, Yuzhong and Xiao, Lingsheng and Liao, Hongmiao and Zhong, Jiayuan and Dong, Chen},
  doi          = {10.1007/s00521-024-10695-0},
  journal      = {Neural Computing and Applications},
  month        = {1},
  number       = {3},
  pages        = {1651-1672},
  shortjournal = {Neural Comput. Appl.},
  title        = {A numerical magnitude aware multi-channel hierarchical encoding network for math word problem solving},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Integrating a hybrid data processing strategy into an optimized light gradient boosting machine for photovoltaic power forecasting. <em>NCA</em>, <em>37</em>(3), 1633-1650. (<a href='https://doi.org/10.1007/s00521-024-10691-4'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As smart photovoltaic distribution networks and large-scale photovoltaic integration continue to evolve, accurate short-term photovoltaic power forecasting has become crucial for ensuring the safe, stable, and economic operation of the system. To address the challenges posed by multiple meteorological influencing factors and the volatility of photovoltaic power generation, this study proposes a hybrid prediction model that integrates an optimized light gradient boosting machine with a data processing strategy. Firstly, a mixed data processing strategy is adopted, which utilizes support vector machine to filter factors and reduce the dimensionality of the model. On this basis, Gaussian mixture model clustering is employed to partition different types of databases, resulting in different data feature libraries for light gradient boosting machine prediction models. Finally, the optimal hyperparameters of the model can be obtained through the gray wolf optimizer algorithm module for predictive evaluation. The proposed hybrid model is applied to perform prediction on a real-world dataset provided by the European Centre for Medium Range Weather Forecasts. Compared with 11 popular machine learning algorithms, our model demonstrates a decrease in mean absolute error, mean square error, and root mean square error on the test dataset by 12.35–70.53%, 24.58–91.69%, and 13.18–71.17%, respectively. Furthermore, the hybrid model also exhibits good interpretability and strong generalization performance.},
  archive      = {J_NCA},
  author       = {Zhang, Xiaoke and Deng, Qijun and Jia, Mengqi and Dai, Xiaoran and Gao, Xingran and Zhou, Hong},
  doi          = {10.1007/s00521-024-10691-4},
  journal      = {Neural Computing and Applications},
  month        = {1},
  number       = {3},
  pages        = {1633-1650},
  shortjournal = {Neural Comput. Appl.},
  title        = {Integrating a hybrid data processing strategy into an optimized light gradient boosting machine for photovoltaic power forecasting},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Adaptive torque estimation-based nonlinear h $$\infty $$ control of modular robot manipulators with uncertain environments. <em>NCA</em>, <em>37</em>(3), 1617-1631. (<a href='https://doi.org/10.1007/s00521-024-10643-y'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Modular robot manipulators (MRMs) based on harmonic drive (HD) transmissions perform various target tasks in unknown environments, and the main challenge is to overcome the uncertain noise and controller errors of systems generated by device vibrations while facing with unknown external disturbance. To address these challenges, a variational Bayesian (VB)-based extended Kalman filter (VBEKF) is developed as part of the MRM dynamic model to mitigate bias in torque estimation resulting from external interference within HD model joints. Furthermore, leveraging a two-player zero-sum game strategy, a robust adaptive dynamic programming (ADP) method based on single critic neural network (NN) is formulated to solve the Hamilton–Jacobi–Isaacs (HJI) equation in H $$\infty $$ control problem, ultimately yielding an approximate optimal solution for H $$\infty $$ control. The Lyapunov theory guarantees that the trajectory tracking error of a closed-loop MRM system remains ultimately uniformly bounded (UUB), even in the presence of unknown environmental disturbances. Ultimately, the experiment results are given to illustrate the advantages and effectiveness of the proposed method. The experimental results show that the proposed method reduces the control torque error by $$\sim $$ 20 $$\%$$ compared with the existing control methods.},
  archive      = {J_NCA},
  author       = {Dong, Bo and Wang, Yuge and An, Tianjiao and Cui, Yiming and Zhu, Xinye},
  doi          = {10.1007/s00521-024-10643-y},
  journal      = {Neural Computing and Applications},
  month        = {1},
  number       = {3},
  pages        = {1617-1631},
  shortjournal = {Neural Comput. Appl.},
  title        = {Adaptive torque estimation-based nonlinear h $$\infty $$ control of modular robot manipulators with uncertain environments},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Research on resource allocation methods for traditional chinese medicine services based on deep reinforcement learning. <em>NCA</em>, <em>37</em>(3), 1601-1616. (<a href='https://doi.org/10.1007/s00521-024-10579-3'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Chinese medicine resources are the crystallization of traditional Chinese culture, and more and more people are choosing Chinese medicine services for their health. To address the problems of pluralistic heterogeneity, waste of service resources, and lagging demand response in the resource allocation model for traditional Chinese medicine (TCM) services, a deep reinforcement learning-based resource allocation method for TCM services is proposed. To address the fragmentation of TCM service resources, this paper presents a TCM service resource association method based on improved spectral clustering and establishes a good resource-demand matching model. For the problem of TCM service resource allocation after resource association, we establish a TCM service resource allocation model and collaboratively solve the TCM service resource allocation problem via the deep reinforcement learning method. The results show that the proposed solution can accelerate the demand response of TCM service resources, effectively reduce the cost of TCM services for patients, improve the quality of TCM services, and satisfy the demand for TCM services for patients.},
  archive      = {J_NCA},
  author       = {Ma, Yuntao and Fang, Xiaolin and Qi, Jin and Sun, Yanfei},
  doi          = {10.1007/s00521-024-10579-3},
  journal      = {Neural Computing and Applications},
  month        = {1},
  number       = {3},
  pages        = {1601-1616},
  shortjournal = {Neural Comput. Appl.},
  title        = {Research on resource allocation methods for traditional chinese medicine services based on deep reinforcement learning},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Neural intellectual computing systems for the analysis of thermally stratified mixed convective micropolar liquid with the interaction of thermal diffusive nanofluid over a heated sheet. <em>NCA</em>, <em>37</em>(3), 1575-1599. (<a href='https://doi.org/10.1007/s00521-024-10515-5'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This study focuses on investigating entropy generation in a micropolar fluid flow over a transversely heated sheet. The governing equations of the specific model proposed in this article are transformed into a set of ordinary differential equations (ODEs) using similarity transformations, leading to a non-spatial arrangement. A BVP4C approach is then employed to solve this system of non-spatial ODEs. The Levenberg–Marquardt procedure, known for its effectiveness in artificial neural networks, is utilized to obtain a numerical solution for the entropy generation in micropolar liquid (EGML). This solution is achieved through regression plots, state transition measures, histogram representations, and mean squared errors. In this research, an EGML-based fluid flow problem is examined using an innovative application of an intelligent computer system that leverages neural structures and the Levenberg–Marquardt algorithm (NN-LMA). Through the BVP4C approach, data collection is conducted to facilitate the implementation of the NN-LMA. The EGML results obtained for various scenarios are evaluated using the exercise, acceptance, and trial processes of NN-LMA. A comparison is then made between these results and a reference dataset to validate the accuracy and efficiency of the prospective algorithm NN-LMA for investigating fluid problems associated with EGML. The study delves into how various non-dimensional factors influence flow patterns, temperature distributions, concentration profiles, and velocity profiles. State transition dynamics, regression analysis, mean square error calculations, and error histogram investigations are employed to verify the validity of the proposed NN-LMA method for solving the EGML. Overall, the results obtained through these analyses successfully demonstrate the efficacy and accuracy of the NN-LMA approach in addressing EGML-related fluid flow issues. The framework’s validity is demonstrated by the strong congruence of recommended results with reference solutions, and accuracy of $$1{0}^{-11}$$ to $$1{0}^{-10}$$ is also attained.},
  archive      = {J_NCA},
  author       = {Alghamdi, Metib and Zamir, Tayyab and Akbar, Noreen Sher and Muhammad, Taseer},
  doi          = {10.1007/s00521-024-10515-5},
  journal      = {Neural Computing and Applications},
  month        = {1},
  number       = {3},
  pages        = {1575-1599},
  shortjournal = {Neural Comput. Appl.},
  title        = {Neural intellectual computing systems for the analysis of thermally stratified mixed convective micropolar liquid with the interaction of thermal diffusive nanofluid over a heated sheet},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). OtoVision: Bridging machine learning and edge computing for effective and affordable ear disease diagnosis. <em>NCA</em>, <em>37</em>(3), 1565-1573. (<a href='https://doi.org/10.1007/s00521-024-10426-5'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The primary aim of this research is to tackle the issue of inaccurate and inconsistent ear disease detection, particularly in remote and under-resourced areas. Traditional diagnostic methods employed by general practitioners and otolaryngologists have shown limitations, underscoring the need for more reliable approaches in these challenging healthcare environments. In response to this issue, a digital otoscope powered by the Rockchip RK3566 processor and enhanced with machine learning capabilities has been developed. This device, using a novel camera system, captures high-resolution images of the patient's ear and applies image classification algorithms to identify and categorize various ear conditions in real time. The design of OtoVision emphasizes ease of use, affordability, and adaptability to different healthcare settings, aiming to make advanced diagnostic tools more accessible to underserved populations. Our testing and analysis reveal that OtoVision significantly enhances the accuracy of ear disease diagnosis. In controlled settings, the device achieved an accuracy rate of approximately 87% when connected to a desktop computer and 82% when operating on a standalone Rockchip single-board computer. These results indicate a substantial improvement over traditional diagnostic methods and demonstrate the potential of integrating machine learning technologies into medical diagnostics. OtoVision represents a step forward in the field of medical diagnostics, particularly for the detection of ear diseases in areas where specialist care is scarce. By leveraging machine learning and digital imaging, OtoVision offers a more accurate, accessible, and cost-effective solution compared to conventional methods. Ongoing development will focus on conducting extensive field testing.},
  archive      = {J_NCA},
  author       = {Yokananth, Rishit and Gosula, Varun},
  doi          = {10.1007/s00521-024-10426-5},
  journal      = {Neural Computing and Applications},
  month        = {1},
  number       = {3},
  pages        = {1565-1573},
  shortjournal = {Neural Comput. Appl.},
  title        = {OtoVision: Bridging machine learning and edge computing for effective and affordable ear disease diagnosis},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Bidirectional legendre memory unit: Bidirectional memory for person authentication combining voice and online signature. <em>NCA</em>, <em>37</em>(3), 1541-1563. (<a href='https://doi.org/10.1007/s00521-024-10717-x'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the increasing popularity of smart portable electronic gadgets, voice-based online person verification systems have become prevalent. However, these systems are susceptible to attacks where illegitimate individuals exploit the recorded voices of legitimate users, leading to false confirmations—spoofing attacks. To overcome this limitation, this article presents an innovative solution by combining speech and online handwritten signatures to mitigate the risks associated with spoofing attacks in voice-based authentication systems because a person has to be present in front of the system to produce an online handwritten signature. To accomplish this objective, this work proposes a novel bidirectional Legendre memory unit (BLMU), a type of recurrent neural network (RNN), for person authentication (verification) and recognition. The Legendre memory unit (LMU) is an innovative memory cell for RNNs that efficiently retains temporal/non-temporal sequential information over a long period with minimal resources. It achieves information orthogonalization by solving coupled ordinary differential equations (ODEs) and leveraging Legendre polynomials, ensuring effective data representation. The proposed framework for person authentication and recognition comprises seven convolution layers, four BLMU layers, two dense layers, and one output layer. The performance of the proposed BLMU-based deep learning framework has been evaluated on a self-generated/private dataset of combined feature matrix of voice signals and online handwritten signatures in the Devanagari script. To assess performance, experiments have also been conducted using various RNN architectures, such as LSTM, BLSTM, and ordinary differential equation recurrent neural network (ODE-RNN), to have a performance comparison with the proposed BLMU-based deep learning (DL) framework. The results demonstrate the superiority of the proposed BLMU-based DL framework in enhancing the accuracy of person verification systems, making it a promising solution for various real-world applications.},
  archive      = {J_NCA},
  author       = {Kumar, Rohitesh and Ghosh, Rajib},
  doi          = {10.1007/s00521-024-10717-x},
  journal      = {Neural Computing and Applications},
  month        = {1},
  number       = {3},
  pages        = {1541-1563},
  shortjournal = {Neural Comput. Appl.},
  title        = {Bidirectional legendre memory unit: Bidirectional memory for person authentication combining voice and online signature},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Incomplete data modeling based on alternate update of clustering and autoencoder for missing value imputation. <em>NCA</em>, <em>37</em>(3), 1523-1540. (<a href='https://doi.org/10.1007/s00521-024-10646-9'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Missing values exist widely in real-world datasets, which restrict the performance of data mining. In this paper, we propose a joint optimization framework to mine attribute associations and category structures in incomplete datasets, aiming to impute missing values with a full understanding of the data structure. Considering the differences in attribute correlations among different sample categories, we partition incomplete data into fuzzy subsets by fuzzy clustering. Within each subset, a tracking-removed autoencoder is constructed as a submodel to fit the regression relationships among attributes. Due to the mutual influence between fuzzy clustering and regression modeling, we further propose a missing value variable-based training scheme to iteratively optimize these two processes. Our proposed framework offers the advantage of decomposing the complex imputation task into simpler sub-tasks by fuzzy clustering where the attribute associations are more explicit. Moreover, the proposed training scheme activates the complementary nature of clustering and regression processes to reduce imputation errors. The experimental results on artificial and real datasets illustrate the effectiveness of our proposed framework.},
  archive      = {J_NCA},
  author       = {Lai, Xiaochen and Zhang, Zheng and Zhang, Liyong and Lu, Wei and Li, ZhuoHan},
  doi          = {10.1007/s00521-024-10646-9},
  journal      = {Neural Computing and Applications},
  month        = {1},
  number       = {3},
  pages        = {1523-1540},
  shortjournal = {Neural Comput. Appl.},
  title        = {Incomplete data modeling based on alternate update of clustering and autoencoder for missing value imputation},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Sparse attention is all you need for pre-training on tabular data. <em>NCA</em>, <em>37</em>(3), 1509-1522. (<a href='https://doi.org/10.1007/s00521-024-10698-x'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the world of data-driven decision-making, tabular data reigns supreme as the most prevalent and crucial format, especially in business contexts. However, data scarcity remains a recurring challenge. In this context, transfer learning has emerged as a potent solution. This study explores the untapped potential of transfer learning in the realm of tabular data analysis, with a focus on leveraging deep learning models—especially the Transformer model—that have garnered significant recognition. Our research investigates the intricacies of tabular data and illuminates the shortcomings of conventional attention mechanisms in the Transformer model when applied to such structured datasets. This highlights the pressing requirement need for specialized solutions tailored to tabular data. We introduce an innovative transfer learning method based on series of thoroughly designed experiments across diverse business domains. This approach harnesses Transformer-based models enhanced with optimized sparse attention mechanisms, offering a groundbreaking solution for tabular data analysis. Our findings reveal the remarkable effectiveness of enhancing the attention mechanism within the Transformer in transfer learning. Specifically, pre-training with sparse attention proves increasingly powerful as data volumes increase, resulting in superior performance on large datasets. Conversely, fine-tuning with full attention becomes more impactful when data availability decreases in downstream tasks, ensuring adaptability in situations with limited data. The empirical results presented in this study provide compelling evidence of the revolutionary potential of our approach. Our optimized sparse attention model emerges as a powerful tool for researchers and practitioners seeking highly effective solutions for tabular data tasks. As tabular data remain the backbone of business operations, our study promises to revolutionize data analysis in critical domains. This work bridges the gap between limited data availability and the requirement for effective analysis in business settings, marking a significant step forward in the field of tabular data analysis.},
  archive      = {J_NCA},
  author       = {Isomura, Tokimasa and Shimizu, Ryotaro and Goto, Masayuki},
  doi          = {10.1007/s00521-024-10698-x},
  journal      = {Neural Computing and Applications},
  month        = {1},
  number       = {3},
  pages        = {1509-1522},
  shortjournal = {Neural Comput. Appl.},
  title        = {Sparse attention is all you need for pre-training on tabular data},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multi-level multi-task representation learning with adaptive fusion for multimodal sentiment analysis. <em>NCA</em>, <em>37</em>(3), 1491-1508. (<a href='https://doi.org/10.1007/s00521-024-10678-1'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multimodal sentiment analysis is an active task in multimodal intelligence, which aims to compute the user’s sentiment tendency from multimedia data. Generally, each modality is a specific and necessary perspective to express human sentiment, providing complementary and consensus information unavailable in a single modality. Nevertheless, the heterogeneous multimedia data often contain inconsistent and conflicting sentiment semantics that limits the model performance. In this work, we propose a Multi-level Multi-task Representation Learning with Adaptive Fusion (MuReLAF) network to bridge the semantic gap among different modalities. Specifically, we design a modality adaptive fusion block to adjust modality contributions dynamically. Besides, we build a multi-level multimodal representations framework to obtain modality-specific and modality-shared semantics by the multi-task learning strategy, where modality-specific semantics contain complementary information and modality-shared semantics include consensus information. Extensive experiments are conducted on four publicly available datasets: MOSI, MOSEI, SIMS, and SIMSV2(s), demonstrating that our model exhibits superior or comparable performance to state-of-the-art models. The achieved accuracies are 86.28%, 86.07%, 84.46%, and 82.78%, respectively, showcasing improvements of 0.82%, 0.84%, 1.75%, and 1.83%. Further analyses also indicate the effectiveness of our model in sentiment analysis.},
  archive      = {J_NCA},
  author       = {Zhu, Chuanbo and Chen, Min and Li, Haomin and Zhang, Sheng and Liang, Han and Sun, Chao and Liu, Yifan and Chen, Jincai},
  doi          = {10.1007/s00521-024-10678-1},
  journal      = {Neural Computing and Applications},
  month        = {1},
  number       = {3},
  pages        = {1491-1508},
  shortjournal = {Neural Comput. Appl.},
  title        = {Multi-level multi-task representation learning with adaptive fusion for multimodal sentiment analysis},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Key-based data augmentation with curriculum learning for few-shot code search. <em>NCA</em>, <em>37</em>(3), 1475-1490. (<a href='https://doi.org/10.1007/s00521-024-10670-9'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Given a natural language query, code search aims to find matching code snippets from a codebase. Recent works are mainly designed for mainstream programming languages with large amounts of training data. However, code search is also needed for domain-specific programming languages, which have fewer training data, and it is a heavy burden to label a large amount of training data for each domain-specific language. To this end, we propose DAFCS, a data augmentation framework with curriculum learning for few-shot code search tasks. Specifically, we first collect unlabeled codes in the same programming language as the original codes, which can provide additional semantic signals to the original codes. Second, we employ an occlusion-based method to identify key statements in code fragments. Third, we design a set of new key-based augmentation operations for the original codes. Finally, we use curriculum learning to reasonably schedule augmented samples for training well-performing models. We conduct retrieval experiments on a public dataset and find that DAFCS surpasses state-of-the-art methods by 5.42% and 5.05% in the Solidity and SQL domain-specific languages, respectively. Our study shows that DAFCS, which adopts data augmentation and curriculum learning strategies, can achieve promising performance in few-shot code search tasks.},
  archive      = {J_NCA},
  author       = {Zhang, Fan and Peng, Manman and Wu, Qiang and Shen, Yuanyuan},
  doi          = {10.1007/s00521-024-10670-9},
  journal      = {Neural Computing and Applications},
  month        = {1},
  number       = {3},
  pages        = {1475-1490},
  shortjournal = {Neural Comput. Appl.},
  title        = {Key-based data augmentation with curriculum learning for few-shot code search},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Exploring the vulnerability of black-box adversarial attack on prompt-based learning in language models. <em>NCA</em>, <em>37</em>(3), 1457-1473. (<a href='https://doi.org/10.1007/s00521-024-10669-2'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Prompt-based learning has been proved to be an effective way in pre-trained language models (PLMs), especially in low-resource scenarios like few-shot settings. However, the trustworthiness of PLMs is of paramount significance and potential vulnerabilities have been shown in prompt-based templates that could mislead the predictions of language models, causing serious security concerns. In this paper, we will shed light on some vulnerabilities of PLMs, by proposing a prompt-based adversarial attack on manual templates in black-box scenarios. First of all, we design character-level and word-level heuristic approaches to break manual templates separately. Then we present a greedy algorithm for the attack based on the above heuristic destructive approaches and further combine it with negative words. Finally, we evaluate our approach with the classification tasks on three variants of BERT series models and eight datasets. And comprehensive experimental results justify the effectiveness of our approach in terms of attack success rate and attack speed. On average, it achieves an attack success rate of close to 90% and a query time of around 3000, which is significantly better than the compared baseline methods. Further experimental studies indicate that our proposed method also displays good capabilities in scenarios with varying shot counts, template lengths and query counts, exhibiting good generalizability.},
  archive      = {J_NCA},
  author       = {Tan, Zihao and Chen, Qingliang and Zhu, Wenbin and Huang, Yongjian and Liang, Chen},
  doi          = {10.1007/s00521-024-10669-2},
  journal      = {Neural Computing and Applications},
  month        = {1},
  number       = {3},
  pages        = {1457-1473},
  shortjournal = {Neural Comput. Appl.},
  title        = {Exploring the vulnerability of black-box adversarial attack on prompt-based learning in language models},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A feature enhancement network combining UNet and vision transformer for building change detection in high-resolution remote sensing images. <em>NCA</em>, <em>37</em>(3), 1429-1456. (<a href='https://doi.org/10.1007/s00521-024-10666-5'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Building change detection (CD) is significant for understanding ground changes and human activities. Deep learning has become the mainstream approach for building CD. However, the detection accuracy remains insufficient due to limitations in feature extraction. Therefore, this paper proposes a feature enhancement network, FENET-UEVTS, to improve the accuracy of building detection, which combines a UNet encoder and a vision transformer structure to detect building changes. It can enhance the ability to detect irregular buildings and distinguish changes between adjacent buildings in different locations. The model combines a deep convolutional network with a part of vision transformer structure, which has a robust feature extraction ability for various types of buildings. We design a spatial-channel attention mechanism module (SCAM) that takes into account both the spatial and channel dimensions to enhance the detection ability of small-scale buildings. We also develop a u-shaped residual module (USRM) and a strengthened feature extraction module (SFEM) to improve the feature extraction capability for buildings with different shapes and edge details. A self-attention feature fusion module (SAFFM) is proposed to facilitate the full convergence and integration of different feature information. The SAFFM can better distinguish buildings of various shapes and sizes to prevent false detection and missed detection. To minimize information loss, a cross-channel context semantic aggregation module (CCSAM) is designed to perform information aggregation in the channel dimension. To evaluate the performance of our model, we conducted numerous experiments on three CD datasets. The results demonstrate that our proposed model outperforms eight other state-of-the-art (SOTA) algorithms in F1-score, overall accuracy, and KAPPA coefficient, achieving up to 91.83 %, 87.65 %, and 93.29 % F1-score on three widely used public datasets, i.e., LEVIR-CD, WHU-CD, and CDD dataset.},
  archive      = {J_NCA},
  author       = {Sun, Yu and Zhao, Yujuan and Han, Xianwei and Gao, Wei and Hu, Yunliang and Zhang, Yimin},
  doi          = {10.1007/s00521-024-10666-5},
  journal      = {Neural Computing and Applications},
  month        = {1},
  number       = {3},
  pages        = {1429-1456},
  shortjournal = {Neural Comput. Appl.},
  title        = {A feature enhancement network combining UNet and vision transformer for building change detection in high-resolution remote sensing images},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Neural network quantization: Separate scaling of rows and columns in weight matrix. <em>NCA</em>, <em>37</em>(3), 1417-1428. (<a href='https://doi.org/10.1007/s00521-024-10657-6'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Quantization methods for neural networks use low-precision fixed-point representations of weight matrices and activation vectors to reduce memory requirements and increase inference speed. However, this reduction in the precision of the numerical representation also leads to a reduction in prediction accuracy. Existing methods aim to mitigate this effect either by approximating the fixed-point representation to the original values or by directly training the quantized network parameters. In this study, we propose a novel approach that involves separately scaling each row and column of the weight matrix before quantization. By considering the error distribution assumptions, we derive the expected error and develop an algorithm to determine the scaling ratio that minimizes this error. This approach effectively reduces the degradation of prediction accuracy in quantized networks. Furthermore, our method is applicable to various commonly used network structures and demonstrates strong performance.},
  archive      = {J_NCA},
  author       = {Wang, Xingyu and Hu, Yunhe and Yang, Zhouwang},
  doi          = {10.1007/s00521-024-10657-6},
  journal      = {Neural Computing and Applications},
  month        = {1},
  number       = {3},
  pages        = {1417-1428},
  shortjournal = {Neural Comput. Appl.},
  title        = {Neural network quantization: Separate scaling of rows and columns in weight matrix},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Generative adversarial learning for missing data imputation. <em>NCA</em>, <em>37</em>(3), 1403-1416. (<a href='https://doi.org/10.1007/s00521-024-10652-x'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Missing data widely exist in industrial problems and lead to difficulties in further modeling and analysis. Recently, a number of deep learning methods have been proposed for missing data imputation and have shown promising performance in various scenarios. Nevertheless, the inputs of imputation networks of these methods are usually incomplete data filled with zeros, which means the missing values always affect the output of the network and the network should be sufficiently large to have a strong denoising ability. Thus, these methods may not provide satisfactory imputation when the missing rate is high. In this work, we present a novel method called GANImputer for missing data imputation. The method is based on the generative adversarial network, and the input of the imputation network, namely a generator, is not incomplete data but a low-dimensional latent variable that can be optimized. The optimization process is composed of three stages. First, we optimize a generator via adversarial training. Second, the latent variable is optimized, while the generator is fixed. Finally, we fine-tune the generator and latent variable jointly. To analyze the theoretical mechanism of our GANImputer, we provide a generalization error bound with respect to missing not at random, which is practical and meaningful. Our method is tested on five diverse benchmark datasets and the Tennessee Eastman process and outperforms a few deep learning based imputation methods.},
  archive      = {J_NCA},
  author       = {Wang, Xinyang and Chen, Hongyu and Zhang, Jiayu and Fan, Jicong},
  doi          = {10.1007/s00521-024-10652-x},
  journal      = {Neural Computing and Applications},
  month        = {1},
  number       = {3},
  pages        = {1403-1416},
  shortjournal = {Neural Comput. Appl.},
  title        = {Generative adversarial learning for missing data imputation},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multivariable financial time series forecasting based on phase space reconstruction compensation. <em>NCA</em>, <em>37</em>(3), 1389-1402. (<a href='https://doi.org/10.1007/s00521-024-10650-z'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Accurate financial time series forecasting is an important challenge in the financial field due to varying levels of interaction among multiple financial time series, complicating the extraction of valid information from these variables. This study introduces an effective and efficient multivariable financial time series forecasting model based on phase space reconstruction compensation: Phase Space Reconstruction Compensation Long Short-Term Memory (PSR-LSTM). The PSR-LSTM model leverages the long short-term memory (LSTM) network to analyze short-term data behaviors, reconstructs multiple long-term variables by phase space reconstruction method, and utilizes multivariable trend attention to capture trend information in correlated variables. This trend information is finally used to correct the LSTM network’s predictive result. Experimental results demonstrate that the PSR-LSTM outperforms existing multivariable forecasting models by effectively mitigating noise interference while achieving optimal forecasting performance.},
  archive      = {J_NCA},
  author       = {Li, Jincheng and Zhou, Linli and Li, Xuefei and Wu, Di and Xiong, Jianqiao and Song, Liangtu},
  doi          = {10.1007/s00521-024-10650-z},
  journal      = {Neural Computing and Applications},
  month        = {1},
  number       = {3},
  pages        = {1389-1402},
  shortjournal = {Neural Comput. Appl.},
  title        = {Multivariable financial time series forecasting based on phase space reconstruction compensation},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Drift-grad-cam method for enhanced segmentation predictions without model retraining. <em>NCA</em>, <em>37</em>(3), 1375-1388. (<a href='https://doi.org/10.1007/s00521-024-10574-8'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Explanation methods are being used to understand model reasoning and decision-making. In this work, we introduce a novel point of view for these methods. We first apply Grad-CAM, initially proposed to explain image classification models, to a segmentation network. Then, we show that small negative gradients can be used to enhance model predictions in the case of under-pixel prediction without retraining. Instead of discarding negative gradients with ReLU as Grad-CAM does, we propose Drift-Grad-CAM with two heuristics methods of thresholding as a novel approach that leverages the informative potential hidden within negative gradients. Drift-Grad-CAM method applied to U-Net and DeepLabV3 model with a ResNet-50 backbone and on two datasets, results in an improvement of performance metrics, Dice and IoU scores, by up to 46% without retraining the model. It demonstrates that some small negative gradients are underestimated but valuable source of information for pixel prediction, and they should be considered as meaningful as positive gradients in future works.},
  archive      = {J_NCA},
  author       = {Lambert, Alexandre and Soni, Aakash and Soukane, Assia and Rabat, Arnaud and Ramdane Cherif, Amar},
  doi          = {10.1007/s00521-024-10574-8},
  journal      = {Neural Computing and Applications},
  month        = {1},
  number       = {3},
  pages        = {1375-1388},
  shortjournal = {Neural Comput. Appl.},
  title        = {Drift-grad-cam method for enhanced segmentation predictions without model retraining},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Integrating expert guidance with gradual moment approximation (GMAp)-enhanced transfer learning for improved pancreatic cancer classification. <em>NCA</em>, <em>37</em>(3), 1357-1373. (<a href='https://doi.org/10.1007/s00521-024-10521-7'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Despite significant research efforts, pancreatic cancer remains a formidable foe. To address the critical need for improved diagnostics, this study presents a novel approach that integrates expert guidance with computer-aided imaging for fine needle aspiration (FNA). A meticulously curated computed tomography (CT) dataset of ground truth images, focusing on key subregions of the pancreas, was established in collaboration with medical professionals. The images provided the training ground for a novel diagnostic model equipped with the gradual moment approximation (GMAp) optimization algorithm, designed to enhance the precision of cancer detection. By efficiently transferring knowledge from pre-trained models, the proposed model achieved remarkable accuracy (98.16%) in classifying CT images across distinct cancerous pancreatic subregions (head, body, and tail) and healthy pancreas. Extensive evaluations against diverse pre-trained models and benchmark medical databases: medical segmentation decathlon, clinical proteomic tumor analysis consortium pancreatic ductal adenocarcinoma, and pancreas-computed tomography proved the model's robustness and superior F1-scores compared to existing approaches. The experiment demonstrates that the deep learning-based 4-class classification outperforms state-of-the-art machine learning-based method by 3.66% in terms of accuracy. This efficiency, coupled with rigorous testing, paves the way for seamless integration into clinical workflows, potentially enabling earlier and more accurate pancreatic cancer diagnoses.},
  archive      = {J_NCA},
  author       = {Chhikara, Jasmine and Goel, Nidhi and Rathee, Neeru},
  doi          = {10.1007/s00521-024-10521-7},
  journal      = {Neural Computing and Applications},
  month        = {1},
  number       = {3},
  pages        = {1357-1373},
  shortjournal = {Neural Comput. Appl.},
  title        = {Integrating expert guidance with gradual moment approximation (GMAp)-enhanced transfer learning for improved pancreatic cancer classification},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A deep embedded clustering technique using dip test and unique neighbourhood set. <em>NCA</em>, <em>37</em>(3), 1345-1356. (<a href='https://doi.org/10.1007/s00521-024-10497-4'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, there has been a growing interest in deep learning-based clustering. A recently introduced technique called DipDECK has shown effective performance on large and high-dimensional datasets. DipDECK utilises Hartigan’s dip test, a statistical test, to merge small non-viable clusters. Notably, DipDECK was the first deep learning-based clustering technique to incorporate the dip test. However, the number of initial clusters of DipDECK is overestimated and the algorithm then randomly selects the initial seeds to produce the final clusters for a dataset. Therefore, in this paper, we presented a technique called UNSDipDECK , which is an improved version of DipDECK and does not require user input for datasets with an unknown number of clusters. UNSDipDECK produces high-quality initial seeds and the initial number of clusters through a deterministic process. UNSDipDECK uses the unique closest neighbourhood and unique neighbourhood set approaches to determine high-quality initial seeds for a dataset. In our study, we compared the performance of UNSDipDECK with fifteen baseline clustering techniques, including DipDECK, using NMI and ARI metrics. The experimental results indicate that UNSDipDECK outperforms the baseline techniques, including DipDECK. Additionally, we demonstrated that the initial seed selection process significantly contributes to UNSDipDECK ’s ability to produce high-quality clusters.},
  archive      = {J_NCA},
  author       = {Rahman, Md Anisur and Ang, Li-minn and Sun, Yuan and Seng, Kah Phooi},
  doi          = {10.1007/s00521-024-10497-4},
  journal      = {Neural Computing and Applications},
  month        = {1},
  number       = {3},
  pages        = {1345-1356},
  shortjournal = {Neural Comput. Appl.},
  title        = {A deep embedded clustering technique using dip test and unique neighbourhood set},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multitask multilayer-prediction model for predicting mechanical ventilation and the associated mortality rate. <em>NCA</em>, <em>37</em>(3), 1321-1343. (<a href='https://doi.org/10.1007/s00521-024-10468-9'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Mechanical ventilation (MV) is a crucial intervention in the intensive care unit (ICU) for severely ill patients. However, it can potentially contribute to lung damage due to the opening and closing of small airways and alveoli. This study aims to enhance the accuracy of mechanical ventilation prediction using a comprehensive dataset from the Medical Information Mart for Intensive Care (MIMIC-III). The data were extracted with three time frames, 6, 12, and 24 h. Then, 6 h left as a time gap and the ventilation as well as the mortality during the next 48 h. The proposed model consists of two layers: Layer 1 predicts ventilation and mortality in the ICU, while Layer 2 predicts the duration of ventilation. Classification techniques are applied to identify patients in need of ventilators, employing multilayer multitask long short-term memory (LSTM) models. Regression tasks use neural networks (multilayer perception). The optimum feature subset was obtained using particle swarm optimization (PSO). Additionally, this study examines the correlation between ventilation and mortality among patients with and without acute respiratory distress syndrome (ARDS). The findings of this research can enhance health-care outcomes and inform policymakers about resource allocation in overwhelmed health services. The best results were obtained when utilizing the first 24 h for prediction. The proposed MTL model achieved promising performance of 0.944, 0.923, 0.951, and 0.921 for the first task and 0.971, 0.961, 0.963, and 0.970 for the second task for accuracy, precision, recall, score, and AUC, respectively.},
  archive      = {J_NCA},
  author       = {El-Rashidy, Nora and Tarek, Zahraa and Elshewey, Ahmed M. and Shams, Mahmoud Y.},
  doi          = {10.1007/s00521-024-10468-9},
  journal      = {Neural Computing and Applications},
  month        = {1},
  number       = {3},
  pages        = {1321-1343},
  shortjournal = {Neural Comput. Appl.},
  title        = {Multitask multilayer-prediction model for predicting mechanical ventilation and the associated mortality rate},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). How do financial time series enhance the detection of news significance in market movements? a study using graph neural networks with heterogeneous representations. <em>NCA</em>, <em>37</em>(3), 1307-1319. (<a href='https://doi.org/10.1007/s00521-024-10418-5'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Forecasting trends in the financial market is a classic and challenging problem that attracts economists’ and computer scientists’ attention. This research area, characterized by its dynamic, chaotic, and nonlinear nature, is further complicated by the overarching influence of the efficient market hypothesis (EMH). The EMH posits that all available information, including historical prices and public news, is already reflected in current asset prices. It suggests that gaining consistent predictive advantages by leveraging such information is challenging. This paper evaluates different machine learning models to identify relevant news based on oscillations in a financial time series. Specifically, we explore the state-of-the-art in graph neural networks, which have the advantage of combining different representations of temporal series and textual data. As a result, we introduce three approaches to classify news as relevant or irrelevant and to model textual data and time series through graphs, taking into account the implications of the EMH. These approaches include text and time-series clusters with daily data, data occurring at perceptually important points in the time series, and data from moments when more than 70% of the news is classified as relevant. We find that similar to the challenge of using the news to enhance the prediction of financial series, the reverse is also true, highlighting the difficulty of identifying relevant news that potentially impacts commodity price fluctuations.},
  archive      = {J_NCA},
  author       = {Filho, Ivan J. Reis and Gôlo, Marcos P. S. and Marcacini, Ricardo M. and Rezende, Solange O.},
  doi          = {10.1007/s00521-024-10418-5},
  journal      = {Neural Computing and Applications},
  month        = {1},
  number       = {3},
  pages        = {1307-1319},
  shortjournal = {Neural Comput. Appl.},
  title        = {How do financial time series enhance the detection of news significance in market movements? a study using graph neural networks with heterogeneous representations},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Explainable ensemble deep learning-based model for brain tumor detection and classification. <em>NCA</em>, <em>37</em>(3), 1289-1306. (<a href='https://doi.org/10.1007/s00521-024-10401-0'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Brain tumors are very dangerous as they cause death. A lot of people die every year because of brain tumors. Therefore, accurate classification and detection in the early stages can help in recovery. Various deep learning techniques have achieved good results in brain tumor classification. The traditional deep learning methods and training the neural network from scratch are time-consuming and can last for weeks of training. Therefore, in this work, we proposed an ensemble approach depending on transfer learning that utilizes pre-trained models of DenseNet121 and InceptionV3 to detect three forms of brain tumors: meningioma, glioma, and pituitary. While developing the ensemble model, some changes were made to the architecture of pre-trained models by replacing their classifiers (fully connected and SoftMax layers) with a new classifier to adopt the recent task. In addition, gradient-weighted class activation maps (Grad-CAM) are an explainable model to verify results and achieve high confidence. The suggested model was validated using a publicly available dataset and achieved 99.02% accuracy, 98.75% precision, 98.98% recall, and a 98.86% F1 score. The suggested approach outperformed others in detecting and classifying brain tumor MRI data, and verifying results using the explainable model achieved a high degree of trust.},
  archive      = {J_NCA},
  author       = {Hosny, Khalid M. and Mohammed, Mahmoud A. and Salama, Rania A. and Elshewey, Ahmed M.},
  doi          = {10.1007/s00521-024-10401-0},
  journal      = {Neural Computing and Applications},
  month        = {1},
  number       = {3},
  pages        = {1289-1306},
  shortjournal = {Neural Comput. Appl.},
  title        = {Explainable ensemble deep learning-based model for brain tumor detection and classification},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A dual stream spatio-temporal deep network for micro-expression recognition using upper facial features. <em>NCA</em>, <em>37</em>(3), 1271-1287. (<a href='https://doi.org/10.1007/s00521-024-10374-0'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The performance of micro-expression recognition mechanisms has greatly improved with the use of deep learning approaches. Most studies have been shifting from the use of hand-designed methods to deep learning-based methods and have increasingly incorporated apex frames into the recognition process. Psychological studies have shown the benefits of only using the upper face for the detection of micro-expressions by humans. However, to the best of our knowledge, this study is the first to explore the effects of using solely upper facial features for computer-based micro-expression detection. To this end, we propose a novel spatio-temporal network to recognize micro-expressions from videos using only upper facial features. The proposed model takes the apex frame and extracts the spatial features using a 2D-CNN and a temporal window of 30 adjacent frames for the temporal feature extraction using a 3D-CNN. The two input streams in the model are in a parallel dual stream architecture. Our proposed method outperforms state-of-the-art methods in detecting micro-expressions in both cropped videos (in the CASMEII and SAMM datasets) and long videos (in the SMIC-E-HS dataset). We further evaluate our proposed method using a combined dataset. Our proposed method achieves a prediction accuracy of 0.832 for the combined dataset, which outperforms state-of-the-art methods. Results show that using only features from the upper face does not impede the detection of micro-expressions and that combining spatial and temporal analysis improves prediction accuracies.},
  archive      = {J_NCA},
  author       = {Matharaarachchi, Nikin and Fermi Pasha, Muhammad},
  doi          = {10.1007/s00521-024-10374-0},
  journal      = {Neural Computing and Applications},
  month        = {1},
  number       = {3},
  pages        = {1271-1287},
  shortjournal = {Neural Comput. Appl.},
  title        = {A dual stream spatio-temporal deep network for micro-expression recognition using upper facial features},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Efficient retinal artery/vein classification with dense color-invariant feature learning. <em>NCA</em>, <em>37</em>(3), 1255-1270. (<a href='https://doi.org/10.1007/s00521-024-10696-z'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Automatic classification of retinal arteries and veins (A/V) is crucial for assisting clinicians in diagnosing cardiovascular and eye diseases. Deep learning models have been widely employed for A/V classification, achieving remarkable performance. However, two primary challenges persist: vessel discontinuity and A/V confusion. To address the vessel discontinuity challenge, we have designed a multi-scale convolution block (MCB) that combines square and strip convolutions. This design enhances the segmentation of tiny vessels by capturing more discriminative vessel features, effectively resolving the issue of vessel discontinuity. Meanwhile, to tackle the A/V confusion challenge, we propose a dense color-invariant feature learning (DCIFL) method to improve the model’s robustness to color variations in retinal images. Specifically, DCIFL uses KL divergences to align the distributions of both latent representations and prediction maps of the raw input image and its color-transformed counterpart. By integrating MCB and DCIFL, we introduce EAV-Net, an efficient model for retinal A/V classification. The proposed method has been tested on three publicly available datasets, achieving accuracies of 96.59%, 97.28%, and 99.34% on the AV-DRIVE, WIDE, and HRF datasets, respectively. These results demonstrate the superiority of our approach in outperforming state-of-the-art methods. Our code will be released at https://github.com/guomugong/EAV-Net .},
  archive      = {J_NCA},
  author       = {Chen, Xiaojuan and Niu, Luyu and Guo, Song},
  doi          = {10.1007/s00521-024-10696-z},
  journal      = {Neural Computing and Applications},
  month        = {1},
  number       = {3},
  pages        = {1255-1270},
  shortjournal = {Neural Comput. Appl.},
  title        = {Efficient retinal artery/vein classification with dense color-invariant feature learning},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Occluded human pose estimation based on limb joint augmentation. <em>NCA</em>, <em>37</em>(3), 1241-1253. (<a href='https://doi.org/10.1007/s00521-024-10676-3'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Human pose estimation aims at locating the specific joints of humans from the images or videos. While existing deep learning-based methods have achieved high positioning accuracy, they often struggle with generalization in occlusion scenarios. In this paper, we propose an occluded human pose estimation framework based on limb joint augmentation to enhance the generalization ability of the pose estimation model on the occluded human bodies. Specifically, the occlusion blocks are at first employed to randomly cover the limb joints of the human bodies from the training images, imitating the scene where the objects or other people partially occlude the human body. Trained by the augmented samples, the pose estimation model is encouraged to accurately locate the occluded keypoints based on the visible ones. To further enhance the localization ability of the model, this paper constructs a dynamic structure loss function based on limb graphs to explore the distribution of occluded joints by evaluating the dependence between adjacent joints. Extensive experimental evaluations on two occluded datasets, OCHuman and CrowdPose, demonstrate significant performance improvements without additional computation cost during inference.},
  archive      = {J_NCA},
  author       = {Han, Gangtao and Song, Chunxiao and Wang, Song and Wang, Hao and Chen, Enqing and Wang, Guanghui},
  doi          = {10.1007/s00521-024-10676-3},
  journal      = {Neural Computing and Applications},
  month        = {1},
  number       = {3},
  pages        = {1241-1253},
  shortjournal = {Neural Comput. Appl.},
  title        = {Occluded human pose estimation based on limb joint augmentation},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Aggressive and robust low-level control and trajectory tracking for quadrotors with deep reinforcement learning. <em>NCA</em>, <em>37</em>(3), 1223-1240. (<a href='https://doi.org/10.1007/s00521-024-10675-4'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Executing accurate trajectory tracking tasks using a high-performance low-level controller is crucial for quadrotors to be applied in various scenarios, especially those involving uncertain disturbances. However, due to the uncertainties in disturbed environments, developing effective low-level controllers with traditional model-based control schemes is challenging. This paper presents an aggressive and robust reinforcement learning (RL)-based low-level control policy for quadrotors. The policy maps the observed quadrotor state directly to motor thrust commands, without requiring the quadrotor dynamics. Additionally, a trajectory generation pipeline is developed to improve the accuracy of trajectory tracking tasks based on differential flatness. With the learned low-level control policy, extensive simulations and real-world experiments are implemented to validate the performance of the policy. The results indicate that our RL-based low-level control policy outperforms traditional proportional–integral–derivative (PID) control methods and related learning-based policies in terms of accuracy and robustness, particularly in environments with uncertain disturbances. Furthermore, the proposed RL-based control policy exhibits an aggressive response in trajectory tracking, even when the speed of the desired trajectory is increased to 6 m/s. Moreover, the learned policy demonstrates strong vibration suppression capabilities and enables the quadrotor to recover to a hovering state from random initial conditions with shorter response time.},
  archive      = {J_NCA},
  author       = {Chen, Shiyu and Li, Yanjie and Lou, Yunjiang and Lin, Ke},
  doi          = {10.1007/s00521-024-10675-4},
  journal      = {Neural Computing and Applications},
  month        = {1},
  number       = {3},
  pages        = {1223-1240},
  shortjournal = {Neural Comput. Appl.},
  title        = {Aggressive and robust low-level control and trajectory tracking for quadrotors with deep reinforcement learning},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Robust 3D point clouds classification based on declarative defenders. <em>NCA</em>, <em>37</em>(3), 1209-1221. (<a href='https://doi.org/10.1007/s00521-024-10673-6'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {3D point cloud classification requires distinct models from 2D image classification due to the divergent characteristics of the respective input data. While 3D point clouds are unstructured and sparse, 2D images are structured and dense. Bridging the domain gap between these two data types is a non-trivial challenge to enable model interchangeability. Recent research using lattice point classifier (LPC) highlights the feasibility of cross-domain applicability. However, the lattice projection operation in LPC generates 2D images with disconnected projected pixels. In this paper, we explore three distinct algorithms for mapping 3D point clouds into 2D images. Through extensive experiments, we thoroughly examine and analyze their performance and defense mechanisms. Leveraging current large foundation models, we scrutinize the feature disparities between regular 2D images and projected 2D images. The proposed approaches demonstrate superior accuracy and robustness against adversarial attacks. The generative model-based mapping algorithms yield regular 2D images, further minimizing the domain gap from regular 2D classification tasks. The source code is available at https://github.com/KaidongLi/pytorch-LatticePointClassifier.git .},
  archive      = {J_NCA},
  author       = {Li, Kaidong and Zhang, Tianxiao and Zhong, Cuncong and Zhang, Ziming and Wang, Guanghui},
  doi          = {10.1007/s00521-024-10673-6},
  journal      = {Neural Computing and Applications},
  month        = {1},
  number       = {3},
  pages        = {1209-1221},
  shortjournal = {Neural Comput. Appl.},
  title        = {Robust 3D point clouds classification based on declarative defenders},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Robust encryption technique based on a block-lag-induced reactive substitution, fuzzy neural network, and memory-loss stochastic resonance. <em>NCA</em>, <em>37</em>(3), 1189-1207. (<a href='https://doi.org/10.1007/s00521-024-10665-6'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Encryption techniques are becoming more robust through innovative advancements, but cryptanalysis techniques are also progressing. Although encryption techniques have one step ahead, this balance may shift as literature reports more breaches in encryption techniques. Current encryption methods are founded on innovative concepts like chaos, randomness, DNA, and neural networks; however, these alone may not provide enough protection against modern cryptography analysis. To effectively thwart cryptanalysis tools, encryption techniques should include methods with a global state to increase nonlinearity and confusion of ciphertext. This paper presents a novel encryption technique that integrates principles from chaos, neural networks, cognitive theory, and deep bit manipulation. The proposed technique can detect variations in the key and plaintext, and impose significant changes to the ciphertext. Additionally, it employs a reactive substitution space that monitors changes in the plaintext and adjusts the processing model accordingly, resulting in a highly-confused output. Experimental results show that the proposed technique meets rigorous security standards while maintaining efficient execution times, outperforming state-of-the-art techniques.},
  archive      = {J_NCA},
  author       = {Al-Muhammed, Muhammed Jassem},
  doi          = {10.1007/s00521-024-10665-6},
  journal      = {Neural Computing and Applications},
  month        = {1},
  number       = {3},
  pages        = {1189-1207},
  shortjournal = {Neural Comput. Appl.},
  title        = {Robust encryption technique based on a block-lag-induced reactive substitution, fuzzy neural network, and memory-loss stochastic resonance},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Deep encodings vs. linguistic features in lexical complexity prediction. <em>NCA</em>, <em>37</em>(3), 1171-1187. (<a href='https://doi.org/10.1007/s00521-024-10662-9'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this work, we present a novel approach to lexical complexity prediction (LCP) that combines diverse linguistic features with encodings from deep neural networks. We explore the integration of 23 handcrafted linguistic features with embeddings from two well-known language models: BERT and XLM-RoBERTa. Our method concatenates these features before inputting them into various machine learning algorithms, including SVM, Random Forest, and fine-tuned transformer models. We evaluate our approach using two datasets: CompLex for English (a high-resource language) and CLexIS2 for Spanish (a relatively low-resource language in ), allowing us to study performance issues from a cross-lingual perspective. Our experiments involve different combinations of linguistic features with encodings from pretrained deep learning models, testing both token-based and sequence-related encodings. The results demonstrate the effectiveness of our hybrid approach. For the English CompLex corpus, our best model achieved a mean absolute error (MAE) of 0.0683, representing a 29.2% improvement over using linguistic features alone (MAE 0.0965). On the Spanish CLexIS2 corpus, we achieved an MAE of 0.1323, a 19.4. These findings show that handcrafted linguistic features play a fundamental role in achieving higher performance, particularly when combined with deep learning approaches. Our work suggests that hybrid approaches should be considered over full end-to-end solutions for LCP tasks, especially in multilingual contexts.},
  archive      = {J_NCA},
  author       = {Ortiz-Zambrano, Jenny A. and Espín-Riofrío, César H. and Montejo-Ráez, Arturo},
  doi          = {10.1007/s00521-024-10662-9},
  journal      = {Neural Computing and Applications},
  month        = {1},
  number       = {3},
  pages        = {1171-1187},
  shortjournal = {Neural Comput. Appl.},
  title        = {Deep encodings vs. linguistic features in lexical complexity prediction},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Qadg: Generating question–answer-distractors pairs for real examination. <em>NCA</em>, <em>37</em>(3), 1157-1170. (<a href='https://doi.org/10.1007/s00521-024-10658-5'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Reading comprehension question generation aims to generate questions from a given article, while distractor generation involves generating multiple distractors from a given article, question, and answer. Most existing research has mainly focused on one of the above tasks, with limited attention to the joint task of Question–Answer-Distractor (QAD) generation. While previous work has achieved success in the joint generation of answer-aware questions and distractors, applying these answer-aware approaches to practical applications in the education domain remains challenging. In this study, we propose a unified and high-performance Question–Answer-Distractors Generation model, named QADG. Our model comprises two components: Question–Answer Generation (QAG) and Distractor Generation (DG). This model is capable of generating Question–Answer pairs based on a given context and then generating distractors based on the context and QA pairs. To address the unconstrained nature of question-and-answer generation in QAG, we employ a key phrase extraction as reported by Willis (in: proceedings of the Sixth ACM Conference on Learning@ Scale, 2019) module to extract key phrases from the article. The extracted key phrases, as the constraints that can be used to match answers. To enhance the quality of distractors, we propose a novel ranking-rewriting mechanism. We employ a fine-tuned model to rank distractors and introduce a rewriting module to improve the quality of distractors. Furthermore, the Knowledge-Dependent-Answerability (KDA) as reported by Moon (Evaluating the knowledge dependency of questions, 2022) is used as a filter to ensure the answerability of the generated QAD pairs. Experiments on SQuAD and RACE datasets demonstrate that the proposed QADG exhibits superior performance, particularly in the DG phase. Additionally, human evaluations also confirm the effectiveness and educational relevance of our model.},
  archive      = {J_NCA},
  author       = {Zhou, Hao and Li, Li},
  doi          = {10.1007/s00521-024-10658-5},
  journal      = {Neural Computing and Applications},
  month        = {1},
  number       = {3},
  pages        = {1157-1170},
  shortjournal = {Neural Comput. Appl.},
  title        = {Qadg: Generating question–answer-distractors pairs for real examination},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A moment-based pooling approach in convolutional neural networks for breast cancer histopathology image classification. <em>NCA</em>, <em>37</em>(2), 1127-1156. (<a href='https://doi.org/10.1007/s00521-024-10406-9'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Automated analysis of breast cancer (BC) histopathology images is a challenging task due to the high resolution, multiple magnifications, color variations, the presence of image artifacts, and morphological variability in the tissue images, often resulting in misdiagnosis. We propose four CNN models using a novel Zernike moments ( $${ZM}_{s}$$ )-based pooling approach, which replaces the existing global average pooling for the effective classification, referred to as CZP-CNN, MZP-CNN, SPMZP-CNN, and SPCZP-CNN. We utilize the $${ZM}_{s}$$ -based pooling at three intermediate and last convolution layers. Of the four models, the two models, SPMZP-CNN and SPCZP-CNN, employ spatial pyramidal structures of feature maps to represent global and local characteristics of the feature maps. They effectively integrate the $${ZM}_{s}$$ of various orders derived from the pyramidal representation of a feature map, representing the global and local aspects of shape and fine morphological details. The ability of $${ZM}_{s}$$ is fully utilized to capture shape and morphological information available in the feature map using only a few low-order moments, which adds to the robustness of the proposed approach. Detailed experimental results on the publicly available BreaKHis dataset with four magnification levels and on the high-resolution BACH dataset demonstrate the proposed models’ effectiveness, robustness, and generalization ability. Among these proposed models, the complex $${ZM}_{s}$$ -based SPCZP-CNN model achieves an impressive mean classification accuracy of 94.16% on the BreaKHis dataset and overall best binary and multiclass classification accuracy of 96.25% and 93.75% on the BACH dataset.},
  archive      = {J_NCA},
  author       = {Kumar, Arvind and Singh, Chandan and Sachan, Manoj Kumar},
  doi          = {10.1007/s00521-024-10406-9},
  journal      = {Neural Computing and Applications},
  month        = {1},
  number       = {2},
  pages        = {1127-1156},
  shortjournal = {Neural Comput. Appl.},
  title        = {A moment-based pooling approach in convolutional neural networks for breast cancer histopathology image classification},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Deep attention for enhanced OCT image analysis in clinical retinal diagnosis. <em>NCA</em>, <em>37</em>(2), 1105-1125. (<a href='https://doi.org/10.1007/s00521-024-10450-5'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Retinal illnesses such as age-related macular degeneration (AMD) and diabetic maculopathy pose serious risks to vision in the developed world. The diagnosis and assessment of these disorders have undergone revolutionary change with the development of optical coherence tomography (OCT). This study proposes a novel method for improving clinical precision in retinal disease diagnosis by utilizing the strength of Attention-Based DenseNet, a deep learning architecture with attention processes. For model building and evaluation, a dataset of 84495 high-resolution OCT images divided into NORMAL, CNV, DME, and DRUSEN classes was used. Data augmentation techniques were employed to enhance the model's robustness. The Attention-Based DenseNet model achieved a validation accuracy of 0.9167 with a batch size of 32 and 50 training epochs. This discovery presents a promising route for more precise and speedy identification of retinal illnesses, ultimately enhancing patient care and outcomes in clinical settings by integrating cutting-edge technology with powerful neural network architectures.},
  archive      = {J_NCA},
  author       = {Talaat, Fatma M. and Ali, Ahmed Ali Ahmed and ElGendy, Raghda and ELShafie, Mohamed A.},
  doi          = {10.1007/s00521-024-10450-5},
  journal      = {Neural Computing and Applications},
  month        = {1},
  number       = {2},
  pages        = {1105-1125},
  shortjournal = {Neural Comput. Appl.},
  title        = {Deep attention for enhanced OCT image analysis in clinical retinal diagnosis},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Leveraging survival analysis in cost-aware deepnet for efficient hard drive failure prediction. <em>NCA</em>, <em>37</em>(2), 1089-1104. (<a href='https://doi.org/10.1007/s00521-024-10479-6'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Many real-world datasets, such as those used for failure and anomaly detection, are severely imbalanced, with a relatively small number of failed instances compared to the number of normal instances. To address these issues, this paper leverages the Backblaze hard disk drives (HDDs) data and makes several contributions to hard drive failure prediction. This research explores 1D convolutional neural networks (CNN) to utilize the sequential nature of hard drive sensor data. The performance of 1D CNN models is compared to traditional machine learning (ML) algorithms, such as the synthetic minority over-sampling technique (SMOTE) and weighted logistic regression (WLR), demonstrating superior results, suggesting the potential effectiveness of the proposed approaches. In addition to these efforts, this paper aims to provide a comprehensive understanding of hard drive longevity and the critical factors contributing to their eventual failure through survival analysis. The 1D CNN models employ weighted binary cross-entropy (WCE) loss and modified focal loss (MFL) functions to manage class imbalanced issues commonly observed in hard drive data. The findings suggest that 1D CNN models outperform traditional ML models, with regularization techniques like dropout and early stopping proving effective in controlling overfitting. Notably, the 1D CNN model with WCE loss demonstrated the best overall performance with a $$G_{mean}$$ of 0.692, successfully maximizing the FDR without increasing the FAR. In parallel, the research also employs Cox regression to identify key SMART parameters influencing drive failure. The high concordance index (c-index) of the Cox model (0.958) adds confidence to the insights derived. The research thus sets a solid groundwork for data center management strategies, with a future focus on practical implementation and evaluation of these findings.},
  archive      = {J_NCA},
  author       = {Ahmed, Jishan and Green II, Robert C.},
  doi          = {10.1007/s00521-024-10479-6},
  journal      = {Neural Computing and Applications},
  month        = {1},
  number       = {2},
  pages        = {1089-1104},
  shortjournal = {Neural Comput. Appl.},
  title        = {Leveraging survival analysis in cost-aware deepnet for efficient hard drive failure prediction},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). LSTM with shallow NNs for indoor temperature long-term predictions in refrigeration systems. <em>NCA</em>, <em>37</em>(2), 1067-1088. (<a href='https://doi.org/10.1007/s00521-024-10477-8'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Refrigeration systems account for a significant percentage of the energy consumption in buildings and industrial cold storage. Model-based control approaches, such as model predictive control (MPC), overcome classical control shortcomings and optimize energy consumption and expensive operating costs. However, they require an accurate long-term indoor temperature prediction model, and finding such a model is still challenging. This paper modifies the standard long short-term memory (LSTM) model used in the literature by adding shallow neural networks (NNs), named ShallowLSTM, that are able to learn different hidden representations from high-dimensional embeddings to improve the predictions in longer prediction horizons. Two versions of the ShallowLSTM with and without input gate of are presented. Experimental results on a 120 min-ahead prediction horizon show that the ShallowLSTM achieves better results than the LSTM model. Specifically, we found a model that achieves a mean absolute error of 0.0762 over the 0.2149 of the standard LSTM model, even with less parameters, thus providing empirical evidence of our proposal to be a candidate for improving long-term indoor temperature modeling and a candidate model for model-based control approaches.},
  archive      = {J_NCA},
  author       = {Machacuay, Javier and Manrique-Silupu, Jose and Ipanaqué, William},
  doi          = {10.1007/s00521-024-10477-8},
  journal      = {Neural Computing and Applications},
  month        = {1},
  number       = {2},
  pages        = {1067-1088},
  shortjournal = {Neural Comput. Appl.},
  title        = {LSTM with shallow NNs for indoor temperature long-term predictions in refrigeration systems},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Implicit sensing self-supervised learning based on graph multi-pretext tasks for traffic flow prediction. <em>NCA</em>, <em>37</em>(2), 1041-1066. (<a href='https://doi.org/10.1007/s00521-024-10461-2'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, spatio-temporal graph neural networks (GNNs) have successfully been used to improve traffic prediction by modeling intricate spatio-temporal dependencies in irregular traffic networks. However, these approaches may not capture the intrinsic properties of traffic data and can suffer from overfitting due to their local nature. This paper introduces the Implicit Sensing Self-Supervised learning model (ISSS), which leverages a multi-pretext task framework for traffic flow prediction. By transforming data into an alternative feature space, ISSS effectively captures both specific and general representations through self-supervised tasks, including contrastive learning and spatial jigsaw puzzles. This enhancement promotes a deeper understanding of traffic features, improved regularization, and more accurate representations. Comparative experiments on six datasets demonstrate the effectiveness of ISSS in learning general and discriminative features in both supervised and unsupervised modes. ISSS outperforms existing models, demonstrating its capabilities in improving traffic flow predictions while addressing challenges associated with local operations and overfitting. Comprehensive evaluations across various traffic prediction datasets, have established the validity of the proposed approach. Unsupervised learning scenarios have shown the improvements in RMSE for the METR-LA and PEMSBAY datasets of 0.39 and 0.35 for location-dependent and location-independent tasks, respectively. In supervised learning scenarios, for the same datasets, the improvements were 1.16 for location-dependent tasks and 0.55 for location-independent tasks.},
  archive      = {J_NCA},
  author       = {Sattarzadeh, Ali Reza and Pathirana, Pubudu Nishantha and Palaniswami, Marimuthu},
  doi          = {10.1007/s00521-024-10461-2},
  journal      = {Neural Computing and Applications},
  month        = {1},
  number       = {2},
  pages        = {1041-1066},
  shortjournal = {Neural Comput. Appl.},
  title        = {Implicit sensing self-supervised learning based on graph multi-pretext tasks for traffic flow prediction},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). BeGrading: Large language models for enhanced feedback in programming education. <em>NCA</em>, <em>37</em>(2), 1027-1040. (<a href='https://doi.org/10.1007/s00521-024-10449-y'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, large language models (LLMs) have gained significant traction across various domains, including education. This paper explores the application of LLMs in grading programming assignments. By leveraging data collected from existing programming assignments and their corresponding grades, we aim to develop a robust LLM-based grading system. We also incorporate augmented data representing various grading scenarios to enhance the model’s performance and ensure comprehensive coverage across all grading levels. Our approach involves training the LLM on this combined dataset to enable accurate and consistent evaluation of programming assignments. The proposed model, BeGrading, aims to reduce the grading burden on educators and provide timely and objective feedback to students. Compared to the Codestral model, our proposed model demonstrates an absolute difference rate of 19%, equivalent to $$\pm 0.95$$ out of 5. This is acceptable for using a small, fine-tuned model with optimized data. Additionally, the Codestral model compared to the dataset optimized score shows a difference of 15% equivalent to a margin of $$\pm 0.75$$ out of 5. Preliminary results demonstrate the potential of LLMs to perform grading tasks with a high degree of reliability, opening avenues for further research and practical applications in automated education systems.},
  archive      = {J_NCA},
  author       = {Yousef, Mina and Mohamed, Kareem and Medhat, Walaa and Mohamed, Ensaf Hussein and Khoriba, Ghada and Arafa, Tamer},
  doi          = {10.1007/s00521-024-10449-y},
  journal      = {Neural Computing and Applications},
  month        = {1},
  number       = {2},
  pages        = {1027-1040},
  shortjournal = {Neural Comput. Appl.},
  title        = {BeGrading: Large language models for enhanced feedback in programming education},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Neural acceleration of incomplete factorization preconditioning. <em>NCA</em>, <em>37</em>(2), 1009-1026. (<a href='https://doi.org/10.1007/s00521-024-10392-y'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The solution of a sparse system of linear equations is ubiquitous in scientific applications. Iterative methods, such as the preconditioned conjugate gradient (PCG) method and the generalized minimal residuals (GMRES) method, are normally chosen over direct methods due to memory and computational complexity constraints. However, the efficiency of these methods depends largely on the preconditioner utilized. The development of a preconditioner normally requires some insight into the sparse linear system and the desired trade-off between generating the preconditioner and the reduction in the number of iterations. Incomplete factorization is a popular black box method to generate these preconditioners. However, it may fail for several reasons, including numerical issues that require searching for adequate scaling, shifting, and fill-in while utilizing a difficult-to-parallelize algorithm. With a move toward heterogeneous computing, many sparse applications find GPUs that are optimized for dense tensor applications like training neural networks being underutilized. In this work, we demonstrate that a simple artificial neural network trained either at compile time or in parallel to the running application on a GPU can provide an incomplete $$LL^{T}$$ factorization that can be used as a preconditioner. This generated preconditioner is as good as or better than the ones found using multiple preconditioning techniques such as scaling and shifting in terms of reduction in number of iterations. Moreover, the generated method also works and never fails to produce a preconditioner that does not reduce the iteration count.},
  archive      = {J_NCA},
  author       = {Booth, Joshua Dennis and Sun, Hongyang and Garnett, Trevor},
  doi          = {10.1007/s00521-024-10392-y},
  journal      = {Neural Computing and Applications},
  month        = {1},
  number       = {2},
  pages        = {1009-1026},
  shortjournal = {Neural Comput. Appl.},
  title        = {Neural acceleration of incomplete factorization preconditioning},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A new bi-level deep human action representation structure based on the sequence of sub-actions. <em>NCA</em>, <em>37</em>(2), 985-1008. (<a href='https://doi.org/10.1007/s00521-024-10370-4'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Human action recognition is applicable in different domains. Previously proposed methods cannot appropriately consider the sequence of sub-actions. Herein, we propose a semantical action model based on the sequence of sub-actions. A technique is used to segment actions on the time axis based on body movements via an energy diagram. After dividing actions into sub-actions, a novel bi-level deep structure is used to extract their features. Then, the sequence of sub-action features is modeled by a deep network to create the action model. As extracted sub-actions have fewer variations in execution manner, their representation is more stable, and modeling their sequence would be an efficient model. Experimental results on UCF-YouTube, UCF-Sport, and Human Motion DataBase (HMDB) datasets indicate the sustainable performance of this method. Overall, the accuracy of the proposed method is 0.972 on average, while the value for the second-best method is 0.925.},
  archive      = {J_NCA},
  author       = {Akhlaghian Tab, Fardin and Ramezani, Mohsen and Afshoon, Hadi and Seyedi, Seyed Amjad and Moradyani, Atefeh},
  doi          = {10.1007/s00521-024-10370-4},
  journal      = {Neural Computing and Applications},
  month        = {1},
  number       = {2},
  pages        = {985-1008},
  shortjournal = {Neural Comput. Appl.},
  title        = {A new bi-level deep human action representation structure based on the sequence of sub-actions},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Imbalanced data learning using SMOTE and deep learning architecture with optimized features. <em>NCA</em>, <em>37</em>(2), 967-984. (<a href='https://doi.org/10.1007/s00521-024-10481-y'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Learning from a class-imbalanced data poses a challenging task in machine learning. Because the skewness in imbalanced data is inherent; therefore, the number of instances is not equal. The class with few samples is identified as the minority class, and a class with many instances is referred to as the majority class. Due to this, biased learning is caused by the machine learning models. When the dataset had more features, the classifier’s performance also suffered. A common method for choosing the crucial features from a dataset and enhancing the performance of a machine learning classifier is feature selection. The feature selection cannot be handled by oversampling approaches alone. Therefore, the purpose of the suggested strategy is to enhance deep learning classifier performance by feature selection-based oversampling. The three modules that make up the suggested methodology are deep 1D-convolutional neural network (DCNN) for classification, SMOTE for oversampling, and genetic algorithm for feature selection. High-dimensional datasets with class imbalance issues are considered by the proposed method. There are both small and large datasets used. In addition, the datasets with low and large imbalance ratios are considered. When compared to benchmark methods, the suggested solution performs well.},
  archive      = {J_NCA},
  author       = {Alex, Suja A.},
  doi          = {10.1007/s00521-024-10481-y},
  journal      = {Neural Computing and Applications},
  month        = {1},
  number       = {2},
  pages        = {967-984},
  shortjournal = {Neural Comput. Appl.},
  title        = {Imbalanced data learning using SMOTE and deep learning architecture with optimized features},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Unlocking the black box: An in-depth review on interpretability, explainability, and reliability in deep learning. <em>NCA</em>, <em>37</em>(2), 859-965. (<a href='https://doi.org/10.1007/s00521-024-10437-2'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep learning models have revolutionized numerous fields, yet their decision-making processes often remain opaque, earning them the characterization of “black-box” models due to their lack of transparency and comprehensibility. This opacity presents significant challenges to understanding the rationale behind their decisions, thereby impeding their interpretability, explainability, and reliability. This review examines 718 studies published between 2015 and 2024 in high-impact journals indexed in SCI, SCI-E, SSCI, and ESCI, providing a crucial reference for researchers investigating methodologies and techniques in related domains. In this exploration, we evaluate a wide array of interpretability and explainability (XAI) strategies, including visual and feature-based explanations, local approach-based techniques, and Bayesian methods. These strategies are assessed for their effectiveness and applicability using a comprehensive set of evaluation metrics. Moving beyond traditional analyses, we propose a novel taxonomy of XAI methods, addressing gaps in the literature and offering a structured classification that elucidates the roles and interactions of these methods. Moreover, we explore the intricate relationship between interpretability and explainability, examining potential conflicts and highlighting the necessity for interpretability in practical applications. Through detailed comparative analysis, we underscore the strengths and limitations of various XAI methods across different data types, ensuring a thorough understanding of their practical performance and real-world utility. The review also examines model robustness against adversarial attacks, emphasizing the critical importance of transparency, reliability, and ethical considerations in model development. A significant emphasis is placed on identifying and mitigating biases in deep learning systems, providing insights into future research directions that aim to enhance fairness and reduce bias. By thoroughly reviewing current challenges and emerging research directions, this article equips researchers with the knowledge and tools to advance the development of more transparent, fair, and reliable deep learning systems. Ultimately, this work aims to bridge existing literature gaps by offering a forward-looking perspective that fosters innovation and progress in the field. This comprehensive review not only illuminates the current state of XAI methodologies but also contributes to the broader understanding and enhancement of deep learning systems, ensuring their ethical and equitable application across various domains.},
  archive      = {J_NCA},
  author       = {ŞAHiN, Emrullah and Arslan, Naciye Nur and Özdemir, Durmuş},
  doi          = {10.1007/s00521-024-10437-2},
  journal      = {Neural Computing and Applications},
  month        = {1},
  number       = {2},
  pages        = {859-965},
  shortjournal = {Neural Comput. Appl.},
  title        = {Unlocking the black box: An in-depth review on interpretability, explainability, and reliability in deep learning},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Sign language interpretation using machine learning and artificial intelligence. <em>NCA</em>, <em>37</em>(2), 841-857. (<a href='https://doi.org/10.1007/s00521-024-10395-9'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Sign language is the only way for deaf and mute people to represent their needs and feelings. Most of non-deaf-mute people do not understand sign language, which leads to many difficulties for deaf-mutes' communication in their social life. Sign language interpretation systems and applications get a lot of attention in the recent years. In this paper, we review sign language recognition and interpretation studies based on machine learning, image processing, artificial intelligence, and animation tools. The two reverse processes for sign language interpretation are illustrated. This study discusses the recent research on sign language translation to text and speech with the help of hand gestures, facial expressions interpretation, and lip reading. Also, state of the art in speech to sign language translation is discussed. In addition, some of the popular and highly rated Android and Apple mobile applications that facilitate disabled people communication are presented. This paper clarifies and highlights the recent research and real used applications for deaf-mute people help. This paper tries to provide a link between research proposals and real applications. This link can help covering any gap or non-handled functionalities in the real used applications. Based on our study, we introduce a proposal involves set of functionalities/options that separately introduced and discussed by the recent research studies. These recent research directions should be integrated for achieving more real help. Also, a set of non-addressed research directions are suggested for future focus.},
  archive      = {J_NCA},
  author       = {Najib, Fatma M.},
  doi          = {10.1007/s00521-024-10395-9},
  journal      = {Neural Computing and Applications},
  month        = {1},
  number       = {2},
  pages        = {841-857},
  shortjournal = {Neural Comput. Appl.},
  title        = {Sign language interpretation using machine learning and artificial intelligence},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Artificial intelligence in music: Recent trends and challenges. <em>NCA</em>, <em>37</em>(2), 801-839. (<a href='https://doi.org/10.1007/s00521-024-10555-x'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Music has always been an essential aspect of human culture, and the methods for its creation and analysis have evolved alongside the advancement of computational capabilities. With the emergence of artificial intelligence (AI) and one of its major goals referring to mimicking human creativity, the interest in music-related research has increased significantly. This review examines current literature from renowned journals and top-tier conferences, published between 2017 and 2023, regarding the application of AI to music-related topics. The study proposes a division of AI-in-music research into three major categories: music classification, music generation and music recommendation. Each category is segmented into smaller thematic areas, with detailed analysis of their inter- and intra-similarities and differences. The second part of the study is devoted to the presentation of the AI methods employed, with specific attention given to deep neural networks—the prevailing approach in this domain, nowadays. In addition, real-life applications and copyright aspects of generated music are outlined. We believe that a detailed presentation of the field along with pointing out possible future challenges in the area will be of some value for both the established AI-in-music researchers, as well as the new scholars entering this fascinating field.},
  archive      = {J_NCA},
  author       = {Mycka, Jan and Mańdziuk, Jacek},
  doi          = {10.1007/s00521-024-10555-x},
  journal      = {Neural Computing and Applications},
  month        = {1},
  number       = {2},
  pages        = {801-839},
  shortjournal = {Neural Comput. Appl.},
  title        = {Artificial intelligence in music: Recent trends and challenges},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Using PSO and SA for optimizing the retardance in dextran-citrate coated ferrofluids. <em>NCA</em>, <em>37</em>(2), 785-800. (<a href='https://doi.org/10.1007/s00521-024-10041-4'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Double-layer coating of dextran and citrate (DC) on the Fe3O4 (magnetite) ferrofluids (FFs) has been conducted for biomedical applications such as hyperthermia and magnetic resonance imaging. The magnetic retardance of DC-coated FFs was measured, and the magnetic heating effect was investigated previously. An experiment was conducted using the uniform design method; we enabled the formula to fit with experimental data on retardance through the stepwise regression analysis. Two intelligent search methods, particle swarm optimization (PSO) and simulated annealing (SA), were used to find the maximum retardance. The optimized parametric combinations were decided as [0.0750, 75.7945, 0.3225, 0.6500] and [0.0750, 75.844, 0.323, 0.65], respectively, denoting the Fe3O4 concentration, the coating temperature, the mass of citrate and dextran. The corresponding maximum retardances were 119.6576° and 119.6558°. The PSO algorithm was more effective and accessible than the SA algorithm in optimizing retardance. As for the hybrid optimization selected for solving complex problems, such as PSO was used to find a near-global solution, and SA was then used for searching for a global solution, the parameter fine-tuning of SA affects the final result. A hybrid metaheuristic algorithm with the local gradient-based sequential quadratic programming (SQP) algorithm is used to find the global solution because of its effectiveness and convergence speed in research. Overall, we provide some two-level hybrid optimizations for the global exploration of the retardance of DC-coated FFs. The hybrid algorithms, including PSO-SA, PSO-SQP, or SA-SQP, allow us to explore a more accurate global solution with high performance.},
  archive      = {J_NCA},
  author       = {Lin, Jing-Fung and Sheu, Jer-Jia},
  doi          = {10.1007/s00521-024-10041-4},
  journal      = {Neural Computing and Applications},
  month        = {1},
  number       = {2},
  pages        = {785-800},
  shortjournal = {Neural Comput. Appl.},
  title        = {Using PSO and SA for optimizing the retardance in dextran-citrate coated ferrofluids},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Incremental class learning using variational autoencoders with similarity learning. <em>NCA</em>, <em>37</em>(2), 769-784. (<a href='https://doi.org/10.1007/s00521-023-08485-1'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Catastrophic forgetting in neural networks during incremental learning remains a challenging problem. Previous research investigated catastrophic forgetting in fully connected networks, with some earlier work exploring activation functions and learning algorithms. Applications of neural networks have been extended to include similarity learning. Understanding how similarity learning loss functions would be affected by catastrophic forgetting is of significant interest. Our research investigates catastrophic forgetting for four well-known similarity-based loss functions during incremental class learning. The loss functions are Angular, Contrastive, Center, and Triplet loss. Our results show that the catastrophic forgetting rate differs across loss functions on multiple datasets. The Angular loss was least affected, followed by Contrastive, Triplet loss, and Center loss with good mining techniques. We implemented three existing incremental learning techniques, iCaRL, EWC, and EBLL. We further proposed a novel technique using Variational Autoencoders (VAEs) to generate representation as exemplars passed through the network’s intermediate layers. Our method outperformed three existing state-of-the-art techniques. We show that one does not require stored images (exemplars) for incremental learning with similarity learning. The generated representations from VAEs help preserve regions of the embedding space used by prior knowledge so that new knowledge does not “overwrite” it.},
  archive      = {J_NCA},
  author       = {Huo, Jiahao and van Zyl, Terence L.},
  doi          = {10.1007/s00521-023-08485-1},
  journal      = {Neural Computing and Applications},
  month        = {1},
  number       = {2},
  pages        = {769-784},
  shortjournal = {Neural Comput. Appl.},
  title        = {Incremental class learning using variational autoencoders with similarity learning},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Neuroevolution gives rise to more focused information transfer compared to backpropagation in recurrent neural networks. <em>NCA</em>, <em>37</em>(2), 757-767. (<a href='https://doi.org/10.1007/s00521-022-08125-0'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Artificial neural networks (ANNs) are one of the most promising tools in the quest to develop general artificial intelligence. Their design was inspired by how neurons in natural brains connect and process, the only other substrate to harbor intelligence. Compared to biological brains that are sparsely connected and that form sparsely distributed representations, ANNs instead process information by connecting all nodes of one layer to all nodes of the next. In addition, modern ANNs are trained with backpropagation, while their natural counterparts have been optimized by natural evolution over eons. We study whether the training method influences how information propagates through the brain by measuring the transfer entropy, that is, the information that is transferred from one group of neurons to another. We find that while the distribution of connection weights in optimized networks is largely unaffected by the training method, neuroevolution leads to networks in which information transfer is significantly more focused on small groups of neurons (compared to those trained by backpropagation) while also being more robust to perturbations of the weights. We conclude that the specific attributes of a training method (local vs. global) can significantly affect how information is processed and relayed through the brain, even when the overall performance is similar.},
  archive      = {J_NCA},
  author       = {Hintze, Arend and Adami, Christoph},
  doi          = {10.1007/s00521-022-08125-0},
  journal      = {Neural Computing and Applications},
  month        = {1},
  number       = {2},
  pages        = {757-767},
  shortjournal = {Neural Comput. Appl.},
  title        = {Neuroevolution gives rise to more focused information transfer compared to backpropagation in recurrent neural networks},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Surrogate-assisted evolutionary multi-objective optimisation applied to a pressure swing adsorption system. <em>NCA</em>, <em>37</em>(2), 739-755. (<a href='https://doi.org/10.1007/s00521-022-07295-1'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The complexity of chemical plant systems (CPS) makes optimising their design and operation challenging tasks. This complexity also results in analytical and numerical simulation models of these systems having high computational costs. Research demonstrates the benefits of using machine learning models as surrogates or substitutes for these computationally expensive simulation models during CPS optimisation. This paper presents the results of our study, extending recent research into optimising chemical plant design and operation. The study explored the original surrogate-assisted genetic algorithms (SA-GA) in more complex variants of the plant design and operation optimisation problem. The more complex plant design variants include additional parallel and feedback components. The study also proposes a novel multivariate extension, surrogate-assisted NSGA (SA-NSGA), to the original univariate SA-GA algorithm. The study evaluated the SA-NSGA extension on the popular pressure swing adsorption (PSA) system. This paper outlines our extensive experimentation, comparing various meta-heuristic optimisation techniques and numerous machine learning models as surrogates. The results in both more complex plant design variants and the PSA case show the suitability of genetic algorithms combined with surrogate models as an optimisation framework for CPS design and operation in single and multi-objective scenarios. The analysis further confirms that combining a genetic algorithm framework with machine learning surrogate models as a substitute for long-running simulation models yields significant computational efficiency improvements, 1.7–1.84 times speedup for the increased complexity examples and a 2.7 times speedup for the pressure swing adsorption system. The discussion successfully concludes that surrogate-assisted evolutionary algorithms can be scaled to increasingly complex CPS with parallel and feedback components.},
  archive      = {J_NCA},
  author       = {Stander, Liezl and Woolway, Matthew and Van Zyl, Terence L.},
  doi          = {10.1007/s00521-022-07295-1},
  journal      = {Neural Computing and Applications},
  month        = {1},
  number       = {2},
  pages        = {739-755},
  shortjournal = {Neural Comput. Appl.},
  title        = {Surrogate-assisted evolutionary multi-objective optimisation applied to a pressure swing adsorption system},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). K-means-MIND: Comparing seeds without repeated k-means runs. <em>NCA</em>, <em>37</em>(2), 723-737. (<a href='https://doi.org/10.1007/s00521-022-07554-1'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A key drawback of the popular k-means clustering algorithm is its susceptibility to local minima. This problem is often addressed by performing repeated runs of the algorithm, and choosing the best run afterward. The approach is effective but computationally expensive: it multiplies the running time proportional to the number of repeats. We observe, in this paper, that repeated k-means runs is equivalent to comparing different candidate initializations using the k-means objective function obtained after running the algorithm fully, as the fitness function. This observation implies that if it were possible to compare the initializations ab initio without depending on the full algorithm for judgment, then there will be no need for repeats. Unfortunately, this phenomenon has not been studied in the literature, to our knowledge. We pursue the development of such a technique for comparing initializations directly. We choose as the "best", the initialization that possesses the largest minimum inter-center distance (MIND). This proposed technique also serves as a general technique for optimizing k-means seeding algorithms. We demonstrate the concept with MIND-optimized versions of two popular algorithms: k-means and k-means++. Experiments on real-world and benchmark synthetic datasets as well as mathematical analysis establish drastic efficiency gains when compared to repeated k-means. Furthermore, our approach significantly improves the accuracy of the standard versions of the algorithms, and it is easy to implement.},
  archive      = {J_NCA},
  author       = {Olukanmi, Peter and Nelwamondo, Fulufhelo and Marwala, Tshilidzi},
  doi          = {10.1007/s00521-022-07554-1},
  journal      = {Neural Computing and Applications},
  month        = {1},
  number       = {2},
  pages        = {723-737},
  shortjournal = {Neural Comput. Appl.},
  title        = {K-means-MIND: Comparing seeds without repeated k-means runs},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multivariate anomaly detection based on prediction intervals constructed using deep learning. <em>NCA</em>, <em>37</em>(2), 707-721. (<a href='https://doi.org/10.1007/s00521-021-06697-x'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {It has been shown that deep learning models can under certain circumstances outperform traditional statistical methods at forecasting. Furthermore, various techniques have been developed for quantifying the forecast uncertainty (prediction intervals). In this paper, we utilize prediction intervals constructed with the aid of artificial neural networks to detect anomalies in the multivariate setting. Challenges with existing deep learning-based anomaly detection approaches include (i) large sets of parameters that may be computationally intensive to tune, (ii) returning too many false positives rendering the techniques impractical for use, and (iii) requiring labeled datasets for training which are often not prevalent in real life. Our approach overcomes these challenges. We benchmark our approach against the oft-preferred well-established statistical models. We focus on three deep learning architectures, namely cascaded neural networks, reservoir computing, and long short-term memory recurrent neural networks. Our finding is deep learning outperforms (or at the very least is competitive to) the latter.},
  archive      = {J_NCA},
  author       = {Mathonsi, Thabang and Zyl, Terence L. van},
  doi          = {10.1007/s00521-021-06697-x},
  journal      = {Neural Computing and Applications},
  month        = {1},
  number       = {2},
  pages        = {707-721},
  shortjournal = {Neural Comput. Appl.},
  title        = {Multivariate anomaly detection based on prediction intervals constructed using deep learning},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Greedy search algorithm for partial quantization of convolutional neural networks inspired by submodular optimization. <em>NCA</em>, <em>37</em>(2), 695-705. (<a href='https://doi.org/10.1007/s00521-021-06752-7'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent results of studies have indicated that neural network quantization effects on inference accuracy vary among layers. Therefore, partial quantization and mixed precision quantization have been studied for neural network accelerators with multi-precision designs. However, these quantization methods typically require network training, which entails a high computational cost because of the exponentially increasing search space with respect to the number of layers N. However, an insufficient search leads to a significant degradation of inference accuracy. For partial quantization, this paper presents a greedy search algorithm that can derive practical combinations of quantization layers without re-training; notably, the proposed method exhibits particularly low computational complexity $$O(N^2)$$ . The proposed greedy search algorithm achieved 4.2 $$\times$$ model size compression with only 0.03% accuracy degradation in ResNet50 and 2.5 $$\times$$ compression with +0.015% accuracy gain in Xception. The computational cost of the greedy search algorithm was only 2.6 hours for a single V100 GPU in the case of MobileNetV2 quantization for ImageNet classification. Furthermore, we accelerated the proposed algorithm to computational complexity O(N) and achieved 4.15 $$\times$$ model size compression with only 0.072% accuracy degradation in ResNet50.},
  archive      = {J_NCA},
  author       = {Tsuji, Satoki and Yamada, Fuyuka and Kawaguchi, Hiroshi and Inoue, Atsuki and Sakai, Yasufumi},
  doi          = {10.1007/s00521-021-06752-7},
  journal      = {Neural Computing and Applications},
  month        = {1},
  number       = {2},
  pages        = {695-705},
  shortjournal = {Neural Comput. Appl.},
  title        = {Greedy search algorithm for partial quantization of convolutional neural networks inspired by submodular optimization},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Solving many-objective optimisation problems using partial dominance. <em>NCA</em>, <em>37</em>(2), 653-694. (<a href='https://doi.org/10.1007/s00521-023-09145-0'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Most optimisation problems have multiple, often conflicting, objectives. Due to the conflicting objectives, a single solution does not exist, and therefore, the goal of a multi-objective optimisation algorithm (MOA) is to find a set of optimal trade-off solutions. Pareto dominance is used to guide the search and compare the quality of two solutions of a multi-objective optimisation problem, where solutions equal in quality are referred to as being non-dominated. However, many-objective optimisation problems (MaOPs) have more than three objectives and the number of non-dominated solutions increases as the number of objectives increases. Therefore, Pareto dominance is no longer an effective approach to guide the search. Recently, a partial dominance approach has been proposed to address this problem. Preliminary results indicate that the partial dominance relation shows promise and scales well with increasing number of objectives. This article extends that study by incorporating the relation in another MOA, applying the relation at different frequencies and evaluating the performance of the relation against both the original MOAs and state-of-the-art algorithms. The results provide further evidence that the partial dominance relation is an efficient approach to solve MaOPs.},
  archive      = {J_NCA},
  author       = {Helbig, Mardé and Engelbrecht, Andries},
  doi          = {10.1007/s00521-023-09145-0},
  journal      = {Neural Computing and Applications},
  month        = {1},
  number       = {2},
  pages        = {653-694},
  shortjournal = {Neural Comput. Appl.},
  title        = {Solving many-objective optimisation problems using partial dominance},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Online signature verification using signature down-sampling and signer-dependent sampling frequency. <em>NCA</em>, <em>37</em>(2), 639-651. (<a href='https://doi.org/10.1007/s00521-021-06536-z'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Online signature verification considers signatures as time sequences of different measurements of the signing instrument. These signals are captured on digital devices and therefore consist of a discrete number of samples. To enrich or simplify this information, several verifiers employ resampling and interpolation as a preprocessing step to improve their results; however, their design decisions may be difficult to generalize. This study investigates the direct effect of the sampling rate of the input signals on the accuracy of online signature verification systems without using interpolation techniques and proposes a novel online signature verification system based on a signer-dependent sampling frequency. Twenty verifier configurations were created for five different public signature databases and a variety of popular preprocessing approaches and evaluated for 20–40 different sampling rates. Our results show that there is an optimal range for the sampling frequency and the number of sample points that minimizes the error rate of a verifier. A sampling frequency range of 15–50 Hz and a signature point count of 60–240 provided the best accuracies in our experiments. As expected, lower ranges showed inaccurate results; interestingly, however, higher frequencies often decreased the verification accuracy. The results show that one can achieve better or at least the same verification accuracies faster by down-sampling the online signatures before further processing. The proposed system achieved competitive results to state-of-the-art systems for different databases by using the optimal sampling frequency. We also studied the effect of choosing individual sampling frequencies for each signer and proposed a signature verification system based on signer-dependent sampling frequency. The proposed system was tested using 500 different verification methods and improved the accuracy in 92% of the test cases compared to the usage of the original frequency.},
  archive      = {J_NCA},
  author       = {Saleem, Mohammad and Kovari, Bence},
  doi          = {10.1007/s00521-021-06536-z},
  journal      = {Neural Computing and Applications},
  month        = {1},
  number       = {2},
  pages        = {639-651},
  shortjournal = {Neural Comput. Appl.},
  title        = {Online signature verification using signature down-sampling and signer-dependent sampling frequency},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Surrogate-assisted strategies: The parameterisation of an infectious disease agent-based model. <em>NCA</em>, <em>37</em>(2), 627-638. (<a href='https://doi.org/10.1007/s00521-022-07476-y'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Parameter calibration is a significant challenge in agent-based modelling and simulation (ABMS). An agent-based model’s (ABM) complexity grows as the number of parameters required to be calibrated increases. This parameter expansion leads to the ABMS equivalent of the "curse of dimensionality". In particular, infeasible computational requirements searching an infinite parameter space. We propose a more comprehensive and adaptive ABMS Framework. The new framework can effectively swap out parameterisation strategies and surrogate models to parameterise an infectious disease ABM. This framework allows us to evaluate different strategy-surrogate combinations’ performance in accuracy and efficiency (speedup). We show that we achieve better than parity in accuracy across the surrogate-assisted sampling strategies and the baselines. Also, we identify that the Metric Stochastic Response Surface strategy combined with the Support Vector Machine surrogate is the best overall in obtaining the actual synthetic parameters. Additionally, we show that DYnamic COOrdindate Search Using Response Surface Models with XGBoost as a surrogate attains one of the highest probabilities at 0.75 of approximating the cumulative synthetic daily infection data achieving a significant speedup of 2.9 with regards to our analysis. Lastly, we show in a real-world setting that DYCORS XGBoost and MSRS SVM can approximate the real-world cumulative daily infection distribution with 97.12% and 96.75% similarity, respectively.},
  archive      = {J_NCA},
  author       = {Perumal, Rylan and Zyl, Terence L van},
  doi          = {10.1007/s00521-022-07476-y},
  journal      = {Neural Computing and Applications},
  month        = {1},
  number       = {2},
  pages        = {627-638},
  shortjournal = {Neural Comput. Appl.},
  title        = {Surrogate-assisted strategies: The parameterisation of an infectious disease agent-based model},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). An investigation of IBM quantum computing device performance on combinatorial optimisation problems. <em>NCA</em>, <em>37</em>(2), 611-626. (<a href='https://doi.org/10.1007/s00521-022-07438-4'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The intractability of deterministic solutions in solving $$\mathcal {NP}$$ -Hard Combinatorial Optimisation Problems (COP) is well reported in the literature. One mechanism for overcoming this difficulty has been the use of efficient COP non-deterministic approaches. However, with the advent of quantum technology, these modern devices’ potential to overcome this tractability limitation requires exploration. This paper juxtaposes classical and quantum optimisation algorithms’ performance to solve two common COP, namely the Travelling Salesman Problem and the Quadratic Assignment Problem. Two accepted classical optimisation methods, Branch and Bound and Simulated Annealing, are compared to two quantum optimisation methods, Variational Quantum Eigensolver (VQE) algorithm and Quantum Approximate Optimisation Algorithm (QAOA). These algorithms are, respectively, executed on both classical devices and IBM’s suite of Noisy Intermediate-Scale Quantum (NISQ) devices. We have encoded the COP problems for the respective technologies and algorithms and provided the computational encodings for the NISQ devices. Our experimental results show that current classical devices significantly outperform the presently available NISQ devices, which both agree with and extend on those findings reported in the literature. Further, we introduce additional performance metrics to better compare the two approaches with respect to computational time, feasibility and solution quality. Our results show that the VQE performs better than QAOA with respect to these metrics, and we infer that this is due to the increased number of operations required. Additionally, we investigate the impact of a new set of basis gates on the quantum optimisation techniques and show they yield no notable improvement on obtained results. Finally, we highlight the present shortcomings of state-of-the-art NISQ IBM quantum devices and argue for continued future work on improving evolving devices.},
  archive      = {J_NCA},
  author       = {Khumalo, Maxine T. and Chieza, Hazel A. and Prag, Krupa and Woolway, Matthew},
  doi          = {10.1007/s00521-022-07438-4},
  journal      = {Neural Computing and Applications},
  month        = {1},
  number       = {2},
  pages        = {611-626},
  shortjournal = {Neural Comput. Appl.},
  title        = {An investigation of IBM quantum computing device performance on combinatorial optimisation problems},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Clustering-based adaptive data augmentation for class-imbalance in machine learning (CADA): Additive manufacturing use case. <em>NCA</em>, <em>37</em>(2), 597-610. (<a href='https://doi.org/10.1007/s00521-022-07347-6'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Large amount of data are generated from in-situ monitoring of additive manufacturing (AM) processes which is later used in prediction modelling for defect classification to speed up quality inspection of products. A high volume of this process data is defect-free (majority class) and a lower volume of this data has defects (minority class) which result in the class-imbalance issue. Using imbalanced datasets, classifiers often provide sub-optimal classification results, i.e. better performance on the majority class than the minority class. However, it is important for process engineers that models classify defects more accurately than the class with no defects since this is crucial for quality inspection. Hence, we address the class-imbalance issue in manufacturing process data to support in-situ quality control of additive manufactured components. For this, we propose cluster-based adaptive data augmentation (CADA) for oversampling to address the class-imbalance problem. Quantitative experiments are conducted to evaluate the performance of the proposed method and to compare with other selected oversampling methods using AM datasets from an aerospace industry and a publicly available casting manufacturing dataset. The results show that CADA outperformed random oversampling and the SMOTE method and is similar to random data augmentation and cluster-based oversampling. Furthermore, the results of the statistical significance test show that there is a significant difference between the studied methods. As such, the CADA method can be considered as an alternative method for oversampling to improve the performance of models on the minority class.},
  archive      = {J_NCA},
  author       = {Dasari, Siva Krishna and Cheddad, Abbas and Palmquist, Jonatan and Lundberg, Lars},
  doi          = {10.1007/s00521-022-07347-6},
  journal      = {Neural Computing and Applications},
  month        = {1},
  number       = {2},
  pages        = {597-610},
  shortjournal = {Neural Comput. Appl.},
  title        = {Clustering-based adaptive data augmentation for class-imbalance in machine learning (CADA): Additive manufacturing use case},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Chaotic flower pollination algorithm for scheduling tardiness-constrained flow shop with simultaneously loaded stations. <em>NCA</em>, <em>37</em>(2), 579-596. (<a href='https://doi.org/10.1007/s00521-022-08044-0'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper introduces a novel chaotic flower pollination algorithm (CFPA) to solve a tardiness-constrained flow-shop scheduling problem with simultaneously loaded stations. This industrial manufacturing problem is modeled from a filter basket production line in Germany and has been generally solved using standard deterministic algorithms. This research develops a metaheuristic approach based on the highly efficient flower pollination algorithm coupled with different chaos maps for stochasticity. The objective function targeted is the tardiness constraint of the due dates. Fifteen different experiments with thirty scenarios are generated to mimic industrial conditions. The results are compared with the genetic algorithm and with the four standard benchmark priority rule-based deterministic algorithms of First In First Out, Raghu and Rajendran, Shortest Processing Time and Slack. From the obtained results and analysis of the relative difference, percentage relative difference and t tests, CFPA was found to be significantly better performing than the deterministic heuristics and the GA algorithm.},
  archive      = {J_NCA},
  author       = {Davendra, Donald and Herrmann, Frank and Bialic-Davendra, Magdalena},
  doi          = {10.1007/s00521-022-08044-0},
  journal      = {Neural Computing and Applications},
  month        = {1},
  number       = {2},
  pages        = {579-596},
  shortjournal = {Neural Comput. Appl.},
  title        = {Chaotic flower pollination algorithm for scheduling tardiness-constrained flow shop with simultaneously loaded stations},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Character-level inclusive transformer architecture for information gain in low resource code-mixed language. <em>NCA</em>, <em>37</em>(2), 559-577. (<a href='https://doi.org/10.1007/s00521-022-06983-2'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The use of code-mixed languages in social media platforms is very common to communicate in an informal way and has immense importance in a multilingual society, like India. Implementing various NLP tasks on code-mixed language for machine comprehension and NLP applications is the need of the hour. The implementation of complex learning models is difficult due to the scarcity of available code-mixed resources. Designing more effective architectures to perform learning from low resource dataset along with transfer learning settings are the possible solutions. We propose an improvised transformer network (Character Inclusion Transformer) that utilizes and learns character-level information available in the words of code-mixed sentences. The proposed model improves the performance of the transformer model when trained from scratch using low resource code-mixed datasets. We also propose two more architecture settings, useful for transfer learning strategy using the mBERT pre-trained model. Three basic word-level tagging NLP tasks, i.e., NER, POS Tagging, and Language Identification (LID) are considered in the paper where Language Identification is specific to code-mixed language. Six separate datasets, namely IIITH NER, LID FIRE, LID ICON, LID UD, POS ICON, POS UD, have been tested, and results are reported using weighted and macro-average while evaluating precision, recall and F1 score},
  archive      = {J_NCA},
  author       = {Bhowmick, Rajat Subhra and Ganguli, Isha and Sil, Jaya},
  doi          = {10.1007/s00521-022-06983-2},
  journal      = {Neural Computing and Applications},
  month        = {1},
  number       = {2},
  pages        = {559-577},
  shortjournal = {Neural Comput. Appl.},
  title        = {Character-level inclusive transformer architecture for information gain in low resource code-mixed language},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Mobile games success and failure: Mining the hidden factors. <em>NCA</em>, <em>37</em>(2), 543-557. (<a href='https://doi.org/10.1007/s00521-022-07154-z'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Predicting the success of a mobile game is a prime issue in game industry. Thousands of games are being released each day. However, a few of them succeed while the majority fail. Toward the goal of investigating the potential correlation between the success of a mobile game and its specific attributes, this work was conducted. More than 17 thousand games were considered for that reason. We show that IAPs (In-App Purchases), genre, number of supported languages, developer profile, and release month have a clear effect on the success of a mobile game. We also develop a novel success score reflecting multiple objectives. Furthermore, we show that game icons with certain visual characteristics tend to be associated with more rating counts. We employ different machine learning models to predict a novel success score metric of a mobile game given its attributes. The trained models were able to predict this score, as well as the expected rating average and rating count for a mobile game with 70% accuracy.},
  archive      = {J_NCA},
  author       = {Kerim, Abdulrahman and Genç, Burkay},
  doi          = {10.1007/s00521-022-07154-z},
  journal      = {Neural Computing and Applications},
  month        = {1},
  number       = {2},
  pages        = {543-557},
  shortjournal = {Neural Comput. Appl.},
  title        = {Mobile games success and failure: Mining the hidden factors},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). S-DFP: Shifted dynamic fixed point for quantized deep neural network training. <em>NCA</em>, <em>37</em>(2), 535-542. (<a href='https://doi.org/10.1007/s00521-021-06821-x'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent advances in deep neural networks have achieved higher accuracy with more complex models. Nevertheless, they require much longer training time. To reduce the training time, training methods using quantized weight, activation, and gradient have been proposed. Neural network calculation by integer format improves the energy efficiency of hardware for deep learning models. Therefore, training methods for deep neural networks with fixed point format have been proposed. However, the narrow data representation range of the fixed point format degrades neural network accuracy. In this work, we propose a new fixed point format named shifted dynamic fixed point (S-DFP) to prevent accuracy degradation in quantized neural networks training. S-DFP can change the data representation range of dynamic fixed point format by adding bias to the exponent. We evaluated the effectiveness of S-DFP for quantized neural network training on the ImageNet task using ResNet-34, ResNet-50, ResNet-101 and ResNet-152. For example, the accuracy of quantized ResNet-152 is improved from 76.6% with conventional 8-bit DFP to 77.6% with 8-bit S-DFP.},
  archive      = {J_NCA},
  author       = {Sakai, Yasufumi and Tamiya, Yutaka},
  doi          = {10.1007/s00521-021-06821-x},
  journal      = {Neural Computing and Applications},
  month        = {1},
  number       = {2},
  pages        = {535-542},
  shortjournal = {Neural Comput. Appl.},
  title        = {S-DFP: Shifted dynamic fixed point for quantized deep neural network training},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Extractive single document summarization using multi-objective modified cat swarm optimization approach: ESDS-MCSO. <em>NCA</em>, <em>37</em>(2), 519-534. (<a href='https://doi.org/10.1007/s00521-021-06337-4'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As the world is progressing faster, to compete with the demand, the need for proficient computing technology has increased, resulting in huge volumes of data. Consequently, the extraction of relevant information from such a massive volume of data in a short time becomes challenging. Hence, automatic text summarization (TS) has emerged as an efficient solution to this problem. In the current study, the automatic TS problem is formulated as a multi-objective optimization problem, and to mitigate this problem, the modified cat swarm optimization (MCSO) strategy is employed. In this work, the population is represented as a collection of feasible individuals where the summary length limit is considered as a constraint that determines the feasibility of an individual. Here, each individual is shaped by randomly selecting some of the sentences encoded in the binary form. Furthermore, two objective functions, namely “coverage and informativeness” and “anti-redundancy,” are used to evaluate each individual’s fitness. Also, to update the position of an individual, genetic and bit manipulating operators and the best cat memory pool have been incorporated into the system. Finally, from the generated non-dominated optimal solutions, the best solution is selected based on the ROUGE score for the summary generation process. The system’s performance is evaluated using ROUGE-1 and ROUGE-2 measures on two standard summarization datasets, namely DUC-2001 and DUC-2002, which revealed that the proposed approach achieved a noticeable improvement in ROUGE scores compared to many state-of-the-art methods mentioned in this paper. The system is also evaluated using the generational distance, CPU processing time, and cohesion, reflecting that the obtained summaries are readable, concise, and relevant being fast converging.},
  archive      = {J_NCA},
  author       = {Debnath, Dipanwita and Das, Ranjita and Pakray, Partha},
  doi          = {10.1007/s00521-021-06337-4},
  journal      = {Neural Computing and Applications},
  month        = {1},
  number       = {2},
  pages        = {519-534},
  shortjournal = {Neural Comput. Appl.},
  title        = {Extractive single document summarization using multi-objective modified cat swarm optimization approach: ESDS-MCSO},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Editorial: 2020 india international congress on computational intelligence. <em>NCA</em>, <em>37</em>(2), 515-517. (<a href='https://doi.org/10.1007/s00521-024-10879-8'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_NCA},
  author       = {Deb, Suash and Wong, Ka-Chun and Hanne, Thomas},
  doi          = {10.1007/s00521-024-10879-8},
  journal      = {Neural Computing and Applications},
  month        = {1},
  number       = {2},
  pages        = {515-517},
  shortjournal = {Neural Comput. Appl.},
  title        = {Editorial: 2020 india international congress on computational intelligence},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Retraction note: Utilizing fog computing and explainable deep learning techniques for gestational diabetes prediction. <em>NCA</em>, <em>37</em>(1), 513. (<a href='https://doi.org/10.1007/s00521-024-10773-3'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_NCA},
  author       = {El-Rashidy, Nora and ElSayed, Nesma E. and El-Ghamry, Amir and Talaat, Fatma M.},
  doi          = {10.1007/s00521-024-10773-3},
  journal      = {Neural Computing and Applications},
  month        = {1},
  number       = {1},
  pages        = {513},
  shortjournal = {Neural Comput. Appl.},
  title        = {Retraction note: Utilizing fog computing and explainable deep learning techniques for gestational diabetes prediction},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Real-time bidding with multi-agent reinforcement learning in multi-channel display advertising. <em>NCA</em>, <em>37</em>(1), 499-511. (<a href='https://doi.org/10.1007/s00521-024-10649-6'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Real-time bidding is the main way to display advertisements in the current e-commerce market. To maximize the revenue and investment reporting (ROI) brought by advertising, the platform should not only meet the needs of various brands for accurate audiences but also fully adapt to the complex and volatile e-commerce environment. However, the existing research still stays on the single-sequence real-time auction, without paying attention to the new business environment of multi-channel simultaneous auction. In this paper, we use multi-agent reinforcement learning to build an intelligent bidding optimization model, and design and implement MCMAB algorithm (multi-channel multi-agent bidding algorithm) based on MADDPG algorithm (multi-agent deep deterministic policy gradient algorithm). First of all, for special business backgrounds, we designed a specific loss function and used a reward model which pretrained with the preference model pretraining (PMP) to capture user preferences. Secondly, in order to solve the problem of PV data dimension confusion in a multi-channel environment, we propose a new data preprocessing scheme, which not only effectively solves the problem, but also improves the convergence rate of the algorithm. On this basis, our model for offline bidding needs to use a lot of offline data for training and testing. So we propose a new simulation data generation algorithm, and improve the algorithm structure of MCMAB algorithm concerning the TD3 algorithm idea, so that it can adapt to the offline environment. Finally, different experiments on different data sets verify the effectiveness of the method.},
  archive      = {J_NCA},
  author       = {Chen, Chen and Wang, Gao and Liu, Baoyu and Song, Siyao and Mao, Keming and Yu, Shiyu and Liu, Jingyu},
  doi          = {10.1007/s00521-024-10649-6},
  journal      = {Neural Computing and Applications},
  month        = {1},
  number       = {1},
  pages        = {499-511},
  shortjournal = {Neural Comput. Appl.},
  title        = {Real-time bidding with multi-agent reinforcement learning in multi-channel display advertising},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A metadata-aware detection model for fake restaurant reviews based on multimodal fusion. <em>NCA</em>, <em>37</em>(1), 475-498. (<a href='https://doi.org/10.1007/s00521-024-10647-8'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Nowadays, online reviews have a significant impact on customers who purchase products or evaluate services, as well as merchants who do business. Unfortunately, there are a lot of fake reviews included in them, which badly affect consumers’ decision-making. Fake review detection has been a significant focus of many researchers in the last few years. Existing approaches mostly focus on the review text, the behavior of reviewers, and the relationship between review entities, while ignoring the abundant metadata in reviews. In this research, we construct the first publicly available dataset of fake restaurant review detection with rich metadata. Through detailed analysis of our dataset, we extract 30 metadata features and text features to help improve the detection performance of models, including 10 brand-new features as well as 9 redefined features. Moreover, a novel metadata-aware model for fake restaurant review detection is proposed. In this model, review text is fed to a transformer model and a convolutional neural network based on pre-training strategies, and the extracted features are fed to a multi-layer perceptron with an attention mechanism. Such a structure enables an effective combination of text and rich metadata. Experimental results show that our proposed model has good stability and robustness and achieves an accuracy of 93.12%, outperforming the baselines by 1.73% in generalization ability test. The fake review dataset and innovative framework proposed in this study can provide ideas and solutions for future research.},
  archive      = {J_NCA},
  author       = {Jian, Yifei and Chen, Xinyu and Wang, Xiaoda and Liu, Ying and Chen, Xingshu and Lan, Xiao and Wang, Wenxian and Wang, Haizhou},
  doi          = {10.1007/s00521-024-10647-8},
  journal      = {Neural Computing and Applications},
  month        = {1},
  number       = {1},
  pages        = {475-498},
  shortjournal = {Neural Comput. Appl.},
  title        = {A metadata-aware detection model for fake restaurant reviews based on multimodal fusion},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multitasking, an EEG experiment: Comparative analysis of cognitive workload during demanding stimuli presentation. <em>NCA</em>, <em>37</em>(1), 457-473. (<a href='https://doi.org/10.1007/s00521-024-10628-x'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {There are two perspectives regarding the human brain’s ability to multitask: concurrent multitasking where multiple tasks are performed simultaneously, and sequential multitasking with switching between tasks. This study investigates the human brain’s ability to “multitask” with multiple demanding stimuli of approximately equal concentration, from an electrophysiological perspective different from that of stimuli which don’t require full attention or exhibit impulsive multitasking responses. This study investigates the P3 component which has been experimentally proven to be associated with mental workload through information processing and cognitive function in visual and auditory tasks, where in the multitasking domain the greater attention elicited, the larger P3 waves are produced. This experiment compares the amplitude of the P3 component of individual stimulus presentation to that of multitasking trials, taking note of the brain workload. This study investigates how the average wave amplitude in a multitasking ERP experiment will be compared to the grand average when performing the two tasks individually with respect to the P3 component. Twenty undergraduate students at Barrett, the Honors College at Arizona State University (10 males and 10 females, with a mean age of 18.75 years, SD = 1.517) right-handed, with normal or corrected visual acuity, English as first language, and no evidence of neurological compromise voluntarily participated in the study. One hundred percent of participants underwent sequential multitasking in the presence of two demanding stimuli in the electrophysiological data, behavioral data, and subjective reported data. In this study, these findings indicate that the presence of additional demanding stimuli causes the workload of the brain to decrease as attention deviates in a bottleneck process to the multiple requisitions for focus, indicated by a reduced P3 voltage amplitude with the multitasking stimuli when compared to the independent. This study illustrates the feasible replication of P3 cognitive workload results for demanding stimuli, not only impulsive-response experiments, to suggest the brain’s tendency to undergo sequential multitasking when faced with multiple demanding stimuli. In brief, this study demonstrates that when higher cognitive processing is required to interpret and respond to the stimuli, the human brain results to sequential multitasking (task switching, not concurrent multitasking) in the face of more challenging problems with each stimulus requiring a higher level of focus, workload, and attention.},
  archive      = {J_NCA},
  author       = {Neill, Ryan},
  doi          = {10.1007/s00521-024-10628-x},
  journal      = {Neural Computing and Applications},
  month        = {1},
  number       = {1},
  pages        = {457-473},
  shortjournal = {Neural Comput. Appl.},
  title        = {Multitasking, an EEG experiment: Comparative analysis of cognitive workload during demanding stimuli presentation},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Image-based rice leaf disease detection using CNN and generative adversarial network. <em>NCA</em>, <em>37</em>(1), 439-456. (<a href='https://doi.org/10.1007/s00521-024-10572-w'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Rice is a major crop and staple food for more than half of the world’s population and plays a vital role in ensuring food security as well as the global economy pests and diseases pose a threat to the production of rice and have a substantial impact on the yield and quality of the crop. In recent times, deep learning methods have gained prominence in predicting rice leaf diseases. Despite the increasing use of these methods, there are notable limitations in existing approaches. These include a scarcity of extensive and diverse collections of leaf disease images, lower accuracy rates, higher time complexity, and challenges in real-time leaf disease detection. To address the limitations, we explicitly investigate various data augmentation approaches using different generative adversarial networks (GANs) for rice leaf disease detection. Along with the GAN model, advanced CNN-based classifiers have been applied to classify the images with improving data augmentation. Our approach involves employing various GANs to generate high-quality synthetic images. This strategy aims to tackle the challenges posed by limited and imbalanced datasets in the identification of leaf diseases. The key benefit of incorporating GANs in leaf disease detection lies in their ability to create synthetic images, effectively augmenting the dataset’s size, enhancing diversity, and reducing the risk of overfitting. For dataset augmentation, we used three distinct GAN architectures—namely simple GAN, CycleGAN, and DCGAN. Our experiments demonstrated that models utilizing the GAN-augmented dataset generally outperformed those relying on the non-augmented dataset. Notably, the CycleGAN architecture exhibited the most favorable outcomes, with the MobileNet model achieving an accuracy of 98.54%. These findings underscore the significant potential of GAN models in improving the performance of detection models for rice leaf diseases, suggesting their promising role in the future research within this domain.},
  archive      = {J_NCA},
  author       = {Ramadan, Syed Taha Yeasin and Islam, Md Shafiqul and Sakib, Tanjim and Sharmin, Nusrat and Rahman, Md. Mokhlesur and Rahman, Md. Mahbubur},
  doi          = {10.1007/s00521-024-10572-w},
  journal      = {Neural Computing and Applications},
  month        = {1},
  number       = {1},
  pages        = {439-456},
  shortjournal = {Neural Comput. Appl.},
  title        = {Image-based rice leaf disease detection using CNN and generative adversarial network},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Representation ensemble learning applied to facial expression recognition. <em>NCA</em>, <em>37</em>(1), 417-438. (<a href='https://doi.org/10.1007/s00521-024-10556-w'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This work introduces the representation ensemble learning algorithm, a novel approach for generating diverse unsupervised representations rooted in the principles of self-taught learning. The ensemble comprises convolutional autoencoders (CAEs) learned in an unsupervised manner, fostering diversity via a loss function designed to penalize similar CAEs’ latent representations. We employ support vector machines, bagging, and random forest as primary classification methods for the final classification step. Additionally, we incorporate KnoraU, a well-established technique used to dynamically select competent classifiers based on a test sample. We evaluate various fusion strategies, including sum, product, and stacking, to comprehensively assess the ensemble’s performance. A robust experimental protocol considering the facial expression recognition problem shows that the proposed approach based on self-taught learning surpasses the accuracy of fine-tuned convolutional neural network (CNN) models. In terms of accuracy, the proposed method is up to 9.9 and 6.3 percentage points better than the CNN-based models fine-tuned for JAFFE and CK+ datasets, respectively.},
  archive      = {J_NCA},
  author       = {Delazeri, Bruna Rossetto and Hochuli, Andre Gustavo and Barddal, Jean Paul and Koerich, Alessandro Lameiras and Britto, Alceu de Souza},
  doi          = {10.1007/s00521-024-10556-w},
  journal      = {Neural Computing and Applications},
  month        = {1},
  number       = {1},
  pages        = {417-438},
  shortjournal = {Neural Comput. Appl.},
  title        = {Representation ensemble learning applied to facial expression recognition},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Topology-adaptive bayesian optimization for deep ring echo state networks in speech emotion recognition. <em>NCA</em>, <em>37</em>(1), 399-416. (<a href='https://doi.org/10.1007/s00521-024-10519-1'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Speech emotion recognition is a difficult task that is gaining attention in a variety of domains, including psychology, human–computer interaction, and speech processing. To recognize speech emotions, machine learning techniques such as convolutional neural networks and long short-term memory networks have been successfully used. However, these models often fail to capture the long-term temporal dynamics of emotional speech. In this work, data augmentation was performed on all datasets. The generated result was used to extract the discriminant features. Then we apply a deep echo state network with a ring topology. The Bayesian optimization (BO) algorithm was applied to the spectral radius of the reservoir matrix. Our approach reveals term-weighted and unweighted accuracies of (75.45%, 74.79%), (86.89%, 86.14%), (98.99%, 97.62%), and (62%, 61.24%) on EMO-DB, SAVEE, RAVDESS, and TESS datasets. The findings demonstrate that by using BO, we are able to determine an optimal spectral radius. Therefore, this approach balances the minimal complexity of reservoir design and high emotion recognition accuracy. Notably, we demonstrate that a simple, deterministically constructed cycle reservoir within DRESN is comparable to other methodologies.},
  archive      = {J_NCA},
  author       = {Soltani, Rebh and Benmohamed, Emna and Ltifi, Hela},
  doi          = {10.1007/s00521-024-10519-1},
  journal      = {Neural Computing and Applications},
  month        = {1},
  number       = {1},
  pages        = {399-416},
  shortjournal = {Neural Comput. Appl.},
  title        = {Topology-adaptive bayesian optimization for deep ring echo state networks in speech emotion recognition},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). New LMI-based criteria for robust finite-time stability of singular large-scale neural networks with interacted delays. <em>NCA</em>, <em>37</em>(1), 387-398. (<a href='https://doi.org/10.1007/s00521-024-10498-3'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper studies robust finite-time stability for a class of singular large-scale neural networks. The innovation in this study lies in the exploration of interconnected delays, bounded disturbances and Lipschitzian activation functions. The objective is to find sufficient conditions for the robust finite-time stability of linear singular large-scale delay systems. More precisely, based on the singular value theory and Lyapunov–Krasovskii function method, we propose new LMI-based conditions for the robust finite-time stability of such systems. Additionally, some new technical lemmas utilizing the Schur and estimated integral inequality are presented to get less conservative conditions. The conditions are presented in terms of tractable linear matrix inequalities (LMIs), which can be solved efficiently by an LMI toolbox algorithm. An numerical example is provided to demonstrate the effectiveness and validity of the proposed method.},
  archive      = {J_NCA},
  author       = {Niamsup, P. and Huong, P. T. and Phat, V. N.},
  doi          = {10.1007/s00521-024-10498-3},
  journal      = {Neural Computing and Applications},
  month        = {1},
  number       = {1},
  pages        = {387-398},
  shortjournal = {Neural Comput. Appl.},
  title        = {New LMI-based criteria for robust finite-time stability of singular large-scale neural networks with interacted delays},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Enhanced honey badger algorithm based on nonlinear adaptive weight and golden sine operator. <em>NCA</em>, <em>37</em>(1), 367-386. (<a href='https://doi.org/10.1007/s00521-024-10484-9'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The honey badger algorithm (HBA) is a swarm intelligence algorithm that imitates honey badgers’ intelligent foraging techniques. HBA diversifies and intensifies the search space by simulating digging and honey-finding strategies. However, HBA suffers from slow convergence speed, imbalanced diversification, and intensification problems. Therefore, we developed the nonlinear adaptive weight and the golden sine operator-based enhanced HBA (NGS-eHBA). The newly added nonlinear adaptive weight explores the search space adaptively, balancing its diversification and intensification. Next, we incorporate the improved golden sine operator to establish a sine route that accelerates the global convergence speed during the search. We compare NGS-eHBA with recent optimization algorithms using well-known benchmark functions for performance evaluation, and statistical analyses show that it outperforms other algorithms. We also use the NGS-eHBA algorithm to resolve engineering design problems, where it outperforms other algorithms noticeably.},
  archive      = {J_NCA},
  author       = {Majumdar, Parijata and Mitra, Sanjoy},
  doi          = {10.1007/s00521-024-10484-9},
  journal      = {Neural Computing and Applications},
  month        = {1},
  number       = {1},
  pages        = {367-386},
  shortjournal = {Neural Comput. Appl.},
  title        = {Enhanced honey badger algorithm based on nonlinear adaptive weight and golden sine operator},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). NLP-based clinical text classification and sentiment analyses of complex medical transcripts using transformer model and machine learning classifiers. <em>NCA</em>, <em>37</em>(1), 341-366. (<a href='https://doi.org/10.1007/s00521-024-10482-x'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A foundation model in software design could be the fundamental framework or structure that a system is constructed around. This could comprise building blocks, libraries, or approaches to design that offer a strong foundation for creating more sophisticated software systems. A proposed software foundation framework illustrates the transformation of traditional models to potential advanced models. As a part of this objective, machine learning classifiers and pre-trained BERT and LSTM models are employed on a complex transcript dataset for context analysis and comparative of both models metrics is done to ML classifiers. The LSTM, a type of RNN, outperformed the ML classifiers in performing text classification from complex medical transcriptions. The accuracy achieved by LSTM is 0.94 with a precision of 0.87, and F1-score value is 0.90. SVM has an accuracy of 0.65 and a F1-score value of 0.64, whereas the convolutional neural network has an accuracy of 0.66 and F1-score value is 0.65. The metrics shows that LSTM, BERT models perform better to other ML classifiers and ensemblers in text classification of medical transcripts.},
  archive      = {J_NCA},
  author       = {Guleria, Pratiyush},
  doi          = {10.1007/s00521-024-10482-x},
  journal      = {Neural Computing and Applications},
  month        = {1},
  number       = {1},
  pages        = {341-366},
  shortjournal = {Neural Comput. Appl.},
  title        = {NLP-based clinical text classification and sentiment analyses of complex medical transcripts using transformer model and machine learning classifiers},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). SimSANet: A simple sequential attention-aided deep neural network for vehicle make and model recognition. <em>NCA</em>, <em>37</em>(1), 319-339. (<a href='https://doi.org/10.1007/s00521-024-10480-z'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Vehicle make and model recognition (VMMR) is a pivotal task for developing automatic vehicle recognition systems. In recent decades, this field has attracted significant attention from the computer vision and artificial intelligence communities. Previous research heavily emphasized improving recognition by focusing on implementing different types of attention mechanisms. The attention mechanism has demonstrated its effectiveness in considering features and uncovering distinctive local and global intricacies. However, one significant issue with this approach is that it increases complexity, which leads to a costly and pointless computational burden. To this end, we introduce a deep neural network model, called simple sequential attention network, which concurrently blends a sequential multi-kernel approach to achieve a trade-off between complexity and performance. This method reduces the computational load and adopts a faster approach while efficiently capturing essential information from feature maps of different scales, from local to global. To demonstrate the effectiveness of the proposed method, we conduct experiments on a variety of publicly accessible VMMR datasets, including Stanford Cars, Comprehensive Cars (CompCars), Comprehensive Cars Surveillance Nature (CompCarsSV), and the Vehicle Images dataset. The suggested approach performs better in the vehicle make and model recognition task than the most advanced models. With 94.47% accuracy on Stanford Cars, 98.34% on CompCars, 99.20% on CompCarsSV, and 97.20% on the Vehicle Images dataset, our model achieves state-of-the-art performance. The implementation details with the source code can be found at: https://github.com/JUVCSE/SIMSANET .},
  archive      = {J_NCA},
  author       = {Gayen, Soumyajit and Maity, Sourajit and Singh, Pawan Kumar and Sarkar, Ram},
  doi          = {10.1007/s00521-024-10480-z},
  journal      = {Neural Computing and Applications},
  month        = {1},
  number       = {1},
  pages        = {319-339},
  shortjournal = {Neural Comput. Appl.},
  title        = {SimSANet: A simple sequential attention-aided deep neural network for vehicle make and model recognition},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Procedure segmentation in videos with bayesian neural ODE model (BNODE). <em>NCA</em>, <em>37</em>(1), 303-317. (<a href='https://doi.org/10.1007/s00521-024-10467-w'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Video event localization, the task of accurately identifying and localizing events within a video, poses a significant challenge due to the complex nature of video data. The abundance of information embedded in videos requires sophisticated models to effectively capture and process the relevant features for precise event localization. Traditional video event localization methods based on recurrent neural networks (RNNs) face challenges due to their limitations in handling long-range dependencies and sensitivity to initial network parameters. As an alternative, neural ordinary differential equations (neural ODEs) offer distinct advantages over RNNs. However, it is imperative to note that neural ODEs are inherently deterministic, rendering them less effective in modeling the inherent uncertainty in real-world videos. This paper proposes Bayesian-based neural ordinary differential equations (Bayesian neural ODEs—BNODEs) as an alternative approach for video event localization. BNODEs based on variational inference techniques are not scalable to large models, as they require complex optimization, instead, we propose Bayesian neural ODE based on Monte Carlo dropout. This method offers several advantages over both RNNs and standard neural ODEs, including the ability to capture extended context or temporal dependencies, being less sensitive to the initial weights of the network parameters, improved modeling of inherent uncertainty, and producing competitive results with less trainable parameters. The proposed encoder–decoder model powered by BNODEs achieves state-of-the-art results on the YouCook2 dataset, outperforming existing methods by a significant margin including those relying on convolution neural networks (CNNs) and bimodal inputs.},
  archive      = {J_NCA},
  author       = {Artham, Sainithin and Shaikh, Soharab Hossain},
  doi          = {10.1007/s00521-024-10467-w},
  journal      = {Neural Computing and Applications},
  month        = {1},
  number       = {1},
  pages        = {303-317},
  shortjournal = {Neural Comput. Appl.},
  title        = {Procedure segmentation in videos with bayesian neural ODE model (BNODE)},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Determination of sample size and design of final product control plan based on product types. <em>NCA</em>, <em>37</em>(1), 283-302. (<a href='https://doi.org/10.1007/s00521-024-10459-w'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Companies carry out the control process, including quality, with similar parameters, regardless of product type. Product-based classification must be made in final/completed product control, and the improving parameters must also be determined. Sending faulty products to the end user is an external failure cost and difficult to compensate. This study aims to design a new control process to prevent companies from sending faulty products to customers. First, criteria to improve product quality indicators were determined, and criterion weights were found using the analytic hierarchy process. Then, the Technique for Order Preference by Similarity to Ideal was used to rank the products. Finally, according to the importance levels of the products, the type of sampling plan, inspection levels, sample size, and acceptance numbers were determined using the acceptance sampling method. The most important criterion for the company is quality, with approximately 60% weight, followed by cost, with 20% weight, and demand and product complexity, with equal weight of 10% each. When we compared the existing and new control numbers, it was seen that 14% of the products had a higher control number than the new control number, and 80% needed to increase the control number above 100%. In the remaining 6%, it was determined that the existing and new control numbers were the same.},
  archive      = {J_NCA},
  author       = {Arinç, Aylin and Çetinyokuş, Tahsin and İfraz, Metin},
  doi          = {10.1007/s00521-024-10459-w},
  journal      = {Neural Computing and Applications},
  month        = {1},
  number       = {1},
  pages        = {283-302},
  shortjournal = {Neural Comput. Appl.},
  title        = {Determination of sample size and design of final product control plan based on product types},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Automated face recognition using deep learning technique and center symmetric multivariant local binary pattern. <em>NCA</em>, <em>37</em>(1), 263-281. (<a href='https://doi.org/10.1007/s00521-024-10447-0'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Researchers have recently created several deep learning strategies for various tasks, and facial recognition has made remarkable progress in employing these techniques. Face recognition is a noncontact, nonobligatory, acceptable, and harmonious biometric recognition method with a promising national and social security future. The purpose of this paper is to improve the existing face recognition algorithm, investigate extensive data-driven face recognition methods, and propose a unique automated face recognition methodology based on generative adversarial networks (GANs) and the center symmetric multivariable local binary pattern (CS-MLBP). To begin, this paper employs the center symmetric multivariant local binary pattern (CS-MLBP) algorithm to extract the texture features of the face, addressing the issue that C2DPCA (column-based two-dimensional principle component analysis) does an excellent job of removing the global characteristics of the face but struggles to process the local features of the face under large samples. The extracted texture features are combined with the international features retrieved using C2DPCA to generate a multifeatured face. The proposed method, GAN-CS-MLBP, syndicates the power of GAN with the robustness of CS-MLBP, resulting in an accurate and efficient face recognition system. Deep learning algorithms, mainly neural networks, automatically extract discriminative properties from facial images. The learned features capture low-level information and high-level meanings, permitting the model to distinguish among dissimilar persons more successfully. To assess the proposed technique’s GAN-CS-MLBP performance, extensive experiments are performed on benchmark face recognition datasets such as LFW, YTF, and CASIA-WebFace. Giving to the findings, our method exceeds state-of-the-art facial recognition systems in terms of recognition accuracy and resilience. The proposed automatic face recognition system GAN-CS-MLBP provides a solid basis for accurate and efficient face recognition, paving the way for biometric breakthroughs and growing the security and ease of many applications.},
  archive      = {J_NCA},
  author       = {Sekhar, J. C. and Josephson, P. Joel and Chinnasamy, A. and Maheswari, M. and Sankar, S. and Kalangi, Ruth Ramya},
  doi          = {10.1007/s00521-024-10447-0},
  journal      = {Neural Computing and Applications},
  month        = {1},
  number       = {1},
  pages        = {263-281},
  shortjournal = {Neural Comput. Appl.},
  title        = {Automated face recognition using deep learning technique and center symmetric multivariant local binary pattern},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Scalable reinforcement learning-based neural architecture search. <em>NCA</em>, <em>37</em>(1), 231-261. (<a href='https://doi.org/10.1007/s00521-024-10445-2'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We assess the feasibility of a reusable neural architecture search agent aimed at amortizing the initial time-investment in building a good search strategy. We do this through the use of Reinforcement Learning, where an agent learns to iteratively select the best way to modify a given neural network architecture. This is achieved using a transformer-based agent design trained using the Ape-X algorithm. We consider both the NAS-Bench-101 and NAS-Bench-301 settings, and compare against various known strong baselines, such as local search and random search. While achieving competitive performance on both benchmarks, the amount of training required for the much larger NAS-Bench-301 is only marginally greater than NAS-Bench-101, illustrating the strong scaling properties of our agent. Our agent is able to achieve strong performance, but the choice of values for certain parameters are crucial to ensuring the succesful training of the agent. We provide some guidance for the selection of appropriate values for hyperparameters through a detailed description of our experimental setup and several ablation studies.},
  archive      = {J_NCA},
  author       = {Cassimon, Amber and Mercelis, Siegfried and Mets, Kevin},
  doi          = {10.1007/s00521-024-10445-2},
  journal      = {Neural Computing and Applications},
  month        = {1},
  number       = {1},
  pages        = {231-261},
  shortjournal = {Neural Comput. Appl.},
  title        = {Scalable reinforcement learning-based neural architecture search},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). LeXNet++: Layer-wise eXplainable ResUNet++ framework for segmentation of colorectal polyp cancer images. <em>NCA</em>, <em>37</em>(1), 213-229. (<a href='https://doi.org/10.1007/s00521-024-10441-6'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Colorectal polyps are benign lesions that develop in the colon and can progress to cancer if left untreated. Clinical observations from medical images are often preferred over computational results due to the lack of trust in the machine learning models, thereby posing serious challenge for the explainability of the results. In order to computationally diagnose colorectal polyps from cancerous images and explain the results, we propose a Layer-wise eXplainable ResUNet++ (LeXNet++) framework for segmentation of the cancerous images, followed by layer-wise explanation of the results. We utilize a publicly accessible dataset that contains of 612 raw images with a resolution of $$256\times 256\times 3$$ and an additional 612 clinically annotated and labeled images with a resolution of $$256\times 256\times 1$$ , which includes the infected region. The LeXNet++ framework comprises of three components—encoder, decoder and the bridge. The encoder and the decoder components each comprise of four layers. Each of the four layers in the encoder and the decoder comprises of 14 and 11 internal sub-layers, respectively. Among the sub-layers of the encoder and the decoder, there are three $$3\times 3$$ convolutional layers with an additional $$3\times 3$$ convolution-transpose layer in the decoder. The output of each of the sub-layers has been explained through heatmap generation after each iteration which have been further explained. The encoder and the decoder are connected by the bridge which comprises of three sub-layers. The results obtained from these three sub-layers have also been explained to inculcate trust in the findings. In this study, we have used three models to segment the images, namely UNet, ResUNet, and proposed LeXNet++. LeXNet++ exhibited the best result among the three models in terms of performance; hence, only LeXNet++ was explained layer-wise. Apart from explanation of the results fetched in this study, the performance of the proposed explainable model has been observed to be 2% greater than the existing polyp segmentation proposals, both with and without explanations. The source code is available at https://github.com/Surajitd492/Polyp-Segmentation- .},
  archive      = {J_NCA},
  author       = {Das, Surajit and Khan, Soumya Suvra and Sengupta, Diganta and Debashis, De},
  doi          = {10.1007/s00521-024-10441-6},
  journal      = {Neural Computing and Applications},
  month        = {1},
  number       = {1},
  pages        = {213-229},
  shortjournal = {Neural Comput. Appl.},
  title        = {LeXNet++: Layer-wise eXplainable ResUNet++ framework for segmentation of colorectal polyp cancer images},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). BGRU-MTRA: Bilinear GRU networks with multi-path temporal residual attention for suspicious activity recognition. <em>NCA</em>, <em>37</em>(1), 185-212. (<a href='https://doi.org/10.1007/s00521-024-10416-7'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Suspicious activity recognition (SAR) is an active research field in computer vision and image processing due to the rapid demand for intelligent video surveillance systems. However, current automated systems focus too much on the temporal dynamics of events in videos and ignore the importance of spatial dynamics. The existing methods for SAR are complex and demand substantial resources. Hence, this paper proposes a novel trade-off architecture for SAR integrating the strengths of the multi-path temporal residual attention (MTRA) module with the bilinear-gated recurrent unit (BGRU) module. The MTRA module combines spatial feature extraction (SFE) with temporal residual attention network (TRAN) blocks for improving SAR by enhancing resilience to variations in object sizes, viewpoints, and motion patterns. It selects relevant action features, addresses vanishing gradient issues, and reduces spatial dimension. The BGRU module preserves spatial features, improving the model’s ability to recognize fine-grained features and complex temporal patterns for effectively recognizing suspicious activities. The BGRU-MTRA system yields recognition accuracy of 93.28%, 98.97%, 99.86%, and 99.66%, and 48.42% on the benchmark hybrid crime action (HCA), UT interaction (UTI), CAVIER, hockey fight (HF), and the most challenging UCF-crime datasets, respectively. Through comprehensive experiments, it is clear that the proposed method outperforms state-of-the-art methods over benchmark datasets with reduced parameters.},
  archive      = {J_NCA},
  author       = {Pandey, Ajeet and Kumar, Piyush},
  doi          = {10.1007/s00521-024-10416-7},
  journal      = {Neural Computing and Applications},
  month        = {1},
  number       = {1},
  pages        = {185-212},
  shortjournal = {Neural Comput. Appl.},
  title        = {BGRU-MTRA: Bilinear GRU networks with multi-path temporal residual attention for suspicious activity recognition},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). An efficient claim management assurance system using EPC contract based on improved monarch butterfly optimization models. <em>NCA</em>, <em>37</em>(1), 169-184. (<a href='https://doi.org/10.1007/s00521-024-10414-9'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Engineering Procurement Construction (EPC) contract systems are widely employed in the construction industry. Among the prevalent issues in this sector, cash flow problems frequently lead to decreased productivity and efficiency. To address these challenges, a claim management system is developed based on the Improved Monarch Butterfly Optimization Algorithm (IMBOA) and the principles of EPC. Conventional construction models typically optimize only a single objective, such as time, cost, or delay, which may not effectively enhance overall performance. This study aims to develop a claim management system based on IMBOA and EPC principles to optimize multiple objectives, focusing on minimizing project costs and time delays while ensuring high-quality results. The basic methodology of this research involves integrating EPC and claim management principles with the IMBOA algorithm to create an efficient, high-quality system. This process starts with a comprehensive literature review on EPC, claims, MBOA, and related algorithms. Common disputes and claims in the construction industry are examined, and critical factors influencing these claims are identified. The Monarch Butterfly Optimization Algorithm (MBOA) and its improved version (IMBOA) are explored for their application in optimizing project performance. A case study in China's coal mining industry evaluates the effectiveness of the EPC approach, demonstrating that it minimizes time delays and costs. The IMBOA approach proposed in this study has the potential to mitigate 23% of risks and avoid 32% of risks associated with the action plan of China's coal mining industry. Furthermore, comparative analysis with other optimization models indicates that the developed IMBOA model yields superior results, reducing overall project time by 15% and cost by 18%.},
  archive      = {J_NCA},
  author       = {Mukilan, K. and Rameshbabu, C. and Baranitharan, B. and Muthusamy, Suresh and Ramamoorthi, Ponarun and Sadasivuni, Kishor Kumar and Oflaz, Kamil and Khan, Anish},
  doi          = {10.1007/s00521-024-10414-9},
  journal      = {Neural Computing and Applications},
  month        = {1},
  number       = {1},
  pages        = {169-184},
  shortjournal = {Neural Comput. Appl.},
  title        = {An efficient claim management assurance system using EPC contract based on improved monarch butterfly optimization models},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Gen-CNN: A framework for the automatic generation of CNNs for image classification. <em>NCA</em>, <em>37</em>(1), 149-168. (<a href='https://doi.org/10.1007/s00521-024-10398-6'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Convolutional neural networks (CNNs) have become widely adopted for computer vision tasks. However, the vast amount of design choices and the complex interactions among their hyperparameters, which ultimately influence the model’s performance, impede their accessibility to users who are not experts in machine learning (ML). To address this challenge, we present AutoML as a solution, leveraging hyperparameter optimization (HPO) for effective parameter selection. Particularly good at handling non-convex, non-differentiable optimization tasks, genetic algorithms are easy to implement and parallelize, making them well suited for deep learning applications. In this context, we introduce Gen-CNN, an AutoML framework based on a genetic algorithm that generates CNN models for image classification. Our framework incorporates transfer learning and operates in a low-compute regime to accelerate the hyperparameter optimization phase. We test Gen-CNN on four datasets, including Sign Language Digits for convergence assessment and KVASIR-v2, ISIC-2019, and BreakHis for performance evaluation. Our results prove that Gen-CNN automatically generates CNN models with classification performance comparable to state-of-the-art custom models already published in the literature. Moreover, in the recommended testing regime for heuristic optimization techniques, we surpassed other HPO algorithms by achieving better mean categorical accuracy. Gen-CNN code is available at—omitted for anonymous review.},
  archive      = {J_NCA},
  author       = {García-Aguirre, Rogelio and Navarro-López, Eva María and Torres-Treviño, Luis},
  doi          = {10.1007/s00521-024-10398-6},
  journal      = {Neural Computing and Applications},
  month        = {1},
  number       = {1},
  pages        = {149-168},
  shortjournal = {Neural Comput. Appl.},
  title        = {Gen-CNN: A framework for the automatic generation of CNNs for image classification},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). OSL-ABE: An optimal secure and lightweight attribute-based encryption method for blockchain-enabled IoT-based healthcare systems. <em>NCA</em>, <em>37</em>(1), 123-148. (<a href='https://doi.org/10.1007/s00521-024-10388-8'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A secure IoT system can provide end-to-end encryption, secure communication protocols, mechanisms for access control, and intrusion detection and prevention systems to protect against various cyber threats. This paper presents a novel method to guarantee the secured and privateness of information in healthcare system that is Internet of Things based. This proposed method, called optimal secure and lightweight attribute-based encryption (OSL-ABE), is designed to provide an efficient and effective solution for secure data storage in the blockchain. By leveraging ABE, this method enables fine-grained access control over data, ensuring that only authorized users with specific attributes can access the data. Current ABE systems, including those used in IoT applications, often neglect privacy protection, during the key generation phase which is a significant threat to confidentiality of industrial secrets. Therefore, an optimal key generation method is proposed using modified sandpiper optimization (MSO) algorithm to ensure privacy preservation. In addition, it is incorporated with an enhanced gravitational search (EGS) algorithm in this proposed method to facilitate secure key revocations. It uses a single short broadcast message, which helps to maintain the IoT systems with confidentiality and integrity. Based on the simulation results, NPCR and UACI results exhibit 99.76% and 34%, respectively, and the entropy value is also obtained with a safe value of 8 which can withstand brute force attacks; it is found that the OSL-ABE method provides a comprehensive and effective solution for ensuring data security, preserving privacy, and facilitating secure key revocations in blockchain-enabled IoT-based healthcare systems.},
  archive      = {J_NCA},
  author       = {Vinnarasi, A. Preethi and Dayana, R.},
  doi          = {10.1007/s00521-024-10388-8},
  journal      = {Neural Computing and Applications},
  month        = {1},
  number       = {1},
  pages        = {123-148},
  shortjournal = {Neural Comput. Appl.},
  title        = {OSL-ABE: An optimal secure and lightweight attribute-based encryption method for blockchain-enabled IoT-based healthcare systems},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). The FCM-guided deep learning model for low-frequency oscillation damping for electric power networks. <em>NCA</em>, <em>37</em>(1), 105-121. (<a href='https://doi.org/10.1007/s00521-024-10377-x'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Machine learning (ML) techniques have gained substantial attention in many aspects of contemporary life during the last several years. As part of the digital revolution, the electricity industry is one of the leaders in implementing such appealing and effective technology for various applications. In general, low-frequency oscillations (LFO) are nonthreatening but slow-burning issues that, if not addressed appropriately, might lead to complete network collapse. Due to the significance of prominent ML family members in improving LFO damping in electric power system (EPS) networks, the applicability of the fuzzy c-means (FCM) clustering-based deep learning (DL) technique is modeled in this paper for two typical EPS networks by predicting the critical parameters of the power system stabilizers (PSS). The first network is a single-machine infinite bus (SMIB) network where the synchronous generator is equipped with a PSS. On the other hand, a unified power flow controller (UPFC) coordinated PSS is connected at one terminal of the synchronous generator of the second network. The clustering of the datasets obtained through the whale optimization algorithm (WOA) is performed based on the calculated silhouette values for both power networks. Then, several statistical performance indices (SPI) are evaluated to validate the robustness of the training and testing procedure of the DL method for the prepared data clusters using the FCM clustering technique. The efficacy of the proposed FCM-DL strategy in enhancing LFO damping for the two test networks is assessed based on standard analytical and time-domain analysis. Therefore, the minimum damping ratio (MDR), eigenvalue, rotor angle, and angular frequency with respect to time are simulated and analyzed. The article also includes a comparison of the findings of previous studies to illustrate the potential of the proposed FCM-DL strategy in improving EPS stability by damping out undesirable LFOs. It is worth noting that the developed FCM-DL models can predict the candidate parameters with a coefficient of determination (R2) value of more than 0.9974. During the implementation phase, the proposed strategy achieves competitive MDR, for instance, more than 0.50 and 0.74 MDR for the first and second networks, respectively.},
  archive      = {J_NCA},
  author       = {Shafiullah, Md},
  doi          = {10.1007/s00521-024-10377-x},
  journal      = {Neural Computing and Applications},
  month        = {1},
  number       = {1},
  pages        = {105-121},
  shortjournal = {Neural Comput. Appl.},
  title        = {The FCM-guided deep learning model for low-frequency oscillation damping for electric power networks},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Deep learning detection method for large language models-generated scientific content. <em>NCA</em>, <em>37</em>(1), 91-104. (<a href='https://doi.org/10.1007/s00521-024-10538-y'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Large Language Models (LLMs), such as GPT-3 and BERT, reshape how textual content is written and communicated. These models can generate indistinguishable scientific content from human-authored work, raising concerns within the scientific community. Mitigating the risk of LLM-facilitated research fabrication and disseminating falsified data and results requires robust safeguards to maintain the integrity of scientific publications. This research paper presents a novel ChatGPT-generated scientific text detection method, AI-Catcher. AI-Catcher integrates two deep learning models, multilayer perceptron (MLP) and convolutional neural networks (CNN). The MLP learns the feature representations of the linguistic and statistical features. The CNN extracts high-level representations of the sequential patterns from the textual content. The proposed model fuses hidden patterns derived from MLP and CNN. AI-Catcher is a multimodal model trained using linguistic and statistical features and textual content. In addition, a new ChatGPT-Generated scientific text dataset, AIGTxt, has been collected to enhance AI-generated text detection tools. AIGTxt contains 3000 records collected from published academic articles across ten domains and divided into three classes: Human-written, ChatGPT-generated, and Mixed text. Several experiments are conducted to evaluate the performance of AI-Catcher. The comparative results demonstrate the capability of AI-Catcher to distinguish between human-written and ChatGPT-generated scientific text more accurately than alternative methods. On average, AI-Catcher improved accuracy by 37.4%.},
  archive      = {J_NCA},
  author       = {Alhijawi, Bushra and Jarrar, Rawan and AbuAlRub, Aseel and Bader, Arwa},
  doi          = {10.1007/s00521-024-10538-y},
  journal      = {Neural Computing and Applications},
  month        = {1},
  number       = {1},
  pages        = {91-104},
  shortjournal = {Neural Comput. Appl.},
  title        = {Deep learning detection method for large language models-generated scientific content},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Groupers and moray eels (GME) optimization: A nature-inspired metaheuristic algorithm for solving complex engineering problems. <em>NCA</em>, <em>37</em>(1), 63-90. (<a href='https://doi.org/10.1007/s00521-024-10384-y'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As engineering technology advances and the number of complex engineering problems increases, there is a growing need to expand the abundance of swarm intelligence algorithms and enhance their performance. It is crucial to develop, assess, and hybridize new powerful algorithms that can be used to deal with optimization issues in different fields. This paper proposes a novel nature-inspired algorithm, namely the Groupers and Moray Eels (GME) optimization algorithm, for solving various optimization problems. GME mimics the associative hunting between groupers and moray eels. Many species, including chimpanzees and lions, have shown cooperation during hunting. Cooperative hunting among animals of different species, which is called associative hunting, is extremely rare. Groupers and moray eels have complementary hunting approaches. Cooperation is thus mutually beneficial because it increases the likelihood of both species successfully capturing prey. The two predators have complementary hunting methods when they work together, and an associated hunt creates a multi-predator attack that is difficult to evade. This example of hunting differs from that of groups of animals of the same species due to the high level of coordination among the two species. GME consists of four phases: primary search, pair association, encircling or extended search, and attacking and catching. The behavior characteristics are mathematically represented to allow for an adequate balance between GME exploitation and exploration. Experimental results indicate that the GME outperforms competing algorithms in terms of accuracy, execution time, convergence rate, and the ability to locate all or the majority of local or global optima.},
  archive      = {J_NCA},
  author       = {Mansour, Nehal A. and Saraya, M. Sabry and Saleh, Ahmed I.},
  doi          = {10.1007/s00521-024-10384-y},
  journal      = {Neural Computing and Applications},
  month        = {1},
  number       = {1},
  pages        = {63-90},
  shortjournal = {Neural Comput. Appl.},
  title        = {Groupers and moray eels (GME) optimization: A nature-inspired metaheuristic algorithm for solving complex engineering problems},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Toward an enhanced automatic medical report generator based on large transformer models. <em>NCA</em>, <em>37</em>(1), 43-62. (<a href='https://doi.org/10.1007/s00521-024-10382-0'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Medical imaging interpretation plays a vital role in primary health care, and with increasing workloads, the integration of artificial intelligence to automate this task can be useful in assisting doctors in their daily work. In the present study, we develop a novel neural architecture based on Transformer models called the enhanced transformer based-medical image interpretation (ETB-MII) to generate medical reports. The structure of the model aims to demonstrate that an implementation that relies solely on attention models, supported by effective data augmentation, can achieve competitive and state-of-the-art (SOTA) performance. To properly assess the effectiveness of our approach, we compare the medical reports generated by ETB-MII against SOTA methods considering the IU X-ray and MIMIC-CXR datasets. Commonly, the metrics evaluated in said collections are BLEU, METEOR, ROUGE-L, and CIDEr. ETB-MII achieves competitive results across BLEU and ROUGE-L metrics while also attaining SOTA performance in the CIDEr score. A comparative evaluation between metrics revealed that CIDEr effectively penalizes medical reports with unclear wording, misspellings, and poor semantic structure. Therefore, we consider CIDEr the most appropriate metric for evaluating medical reports. In addition, a computational complexity analysis reveals that our approach has lower computational demands compared to a baseline SOTA model. This efficiency is critical for supporting decision-making processes in patient care.},
  archive      = {J_NCA},
  author       = {Prieto-Ordaz, Olanda and Ramirez-Alonso, Graciela and Montes-y-Gomez, Manuel and Lopez-Santillan, Roberto},
  doi          = {10.1007/s00521-024-10382-0},
  journal      = {Neural Computing and Applications},
  month        = {1},
  number       = {1},
  pages        = {43-62},
  shortjournal = {Neural Comput. Appl.},
  title        = {Toward an enhanced automatic medical report generator based on large transformer models},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Colon cancer classification and detection by novel CMNV2 model and methods of deep learning. <em>NCA</em>, <em>37</em>(1), 25-41. (<a href='https://doi.org/10.1007/s00521-024-10563-x'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Colon cancer is the leading cause of death among cancers. Colon cancer, commonly referred as colorectal cancer (CRC), has two common names. Adenomas are a sequence of benign stages that colon cancer undergoes after starting in a healthy intestinal cell. Reducing the possibility of treatment failures and consequences requiring early identification of cancer, our study started with pre-processing digital histopathological images using transfer learning and deep learning (DL) models. A total of 12 models were evaluated and compared in this research, including standard methods like MobileNet, NASNetMobile, DenseNet121, DenseNet169, InceptionV3, DenseNet201, ResNet152V2, EfficientNetB0, InceptionResNetV2, Xception, EfficientNetV2M and our proposed model. The proposed hybrid model was developed by combining the convolutional architecture for fast feature extraction (CAFFE) framework with the modified MobileNetV2 (MMNV2) framework named as 'CAFFE+MMNV2 (CMNV2).' To increase performance, classification, detection and prediction accuracy in colon cancer, this study presents a novel CMNV2 model created by an additional 5-layer pre-trained model for feature extraction from images using DL, deep neural network (DNN). The suggested CMNV2 model outperformed the remaining 11 existing methods for colon cancer classification and detection with a 0.001 learning rate. Feature extraction from histopathological images to classify and detect colon adenocarcinoma (COAD) and colon benign tissue (COBT) is implemented by a Python model using a dataset consisting of 10,000 images. The suggested model significantly outperformed other existing methods, obtaining 99.95% accuracy and other high-performance criteria including 100% recall, 99.90% precision, 99.95% f1-score and 0.05% error rate, when using fewer parameters.},
  archive      = {J_NCA},
  author       = {Kumar, B. Anil and Misra, Neeraj Kumar},
  doi          = {10.1007/s00521-024-10563-x},
  journal      = {Neural Computing and Applications},
  month        = {1},
  number       = {1},
  pages        = {25-41},
  shortjournal = {Neural Comput. Appl.},
  title        = {Colon cancer classification and detection by novel CMNV2 model and methods of deep learning},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Revolutionizing cardiovascular health: Integrating deep learning techniques for predictive analysis of personal key indicators in heart disease. <em>NCA</em>, <em>37</em>(1), 1-24. (<a href='https://doi.org/10.1007/s00521-024-10453-2'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cardiovascular diseases (CVDs) remain a global burden, highlighting the need for innovative approaches for early detection and intervention. This study investigates the potential of deep learning, specifically convolutional neural networks (CNNs), to improve the prediction of heart disease risk using key personal health markers. Our approach revolutionizes traditional healthcare predictive modeling by integrating CNNs, which excel at uncovering subtle patterns and hidden interactions among various health indicators such as blood pressure, cholesterol levels, and lifestyle factors. To achieve this, we leverage advanced neural network architectures. The model utilizes embedding layers to transform categorical data into numerical representations, convolutional layers to extract spatial features, and dense layers to model complex interactions and predict CVD risk. Regularization techniques like dropout and batch normalization, along with hyperparameter optimization, enhance model generalizability and performance. Rigorous validation against conventional methods demonstrates the model’s superiority, with a significantly higher R2 value of 0.994. This achievement underscores the model’s potential as a valuable tool for clinicians in CVD prevention and management. The study also emphasizes the need for interpretability in deep learning models and addresses ethical considerations to ensure responsible implementation in clinical practice.},
  archive      = {J_NCA},
  author       = {Talaat, Fatma M.},
  doi          = {10.1007/s00521-024-10453-2},
  journal      = {Neural Computing and Applications},
  month        = {1},
  number       = {1},
  pages        = {1-24},
  shortjournal = {Neural Comput. Appl.},
  title        = {Revolutionizing cardiovascular health: Integrating deep learning techniques for predictive analysis of personal key indicators in heart disease},
  volume       = {37},
  year         = {2025},
}
</textarea>
</details></li>
</ul>

</body>
</html>
