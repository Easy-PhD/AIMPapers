<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>SAC</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="sac">SAC - 15</h2>
<ul>
<li><details>
<summary>
(2026). Importance sampling for rare event tracking within the ensemble kalman filtering framework. <em>SAC</em>, <em>36</em>(1), 1--27. (<a href='https://doi.org/10.1007/s11222-025-10736-1'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this work we employ importance sampling (IS) techniques to track a small over-threshold probability of a running maximum associated with the solution of a stochastic differential equation (SDE) within the framework of ensemble Kalman filtering (EnKF). The proposed method acts as a post-processing step applied to the EnKF output: it uses the ensemble at a given observation time to estimate the probability of a rare event occurring before the next observation, without altering the EnKF itself. Between two observation times of the EnKF, we propose to use IS with respect to the initial condition of the SDE, IS with respect to the Wiener process via a stochastic optimal control formulation, and combined IS with respect to both initial condition and Wiener process. Both IS strategies require the approximation of the solution of Kolmogorov Backward equation (KBE) with boundary conditions. In multidimensional settings, we employ a Markovian projection dimension reduction technique to obtain an approximation of the solution of the KBE by just solving a one dimensional PDE. The proposed ideas are tested on three illustrative examples: Double Well SDE, Langevin dynamics and noisy Charney-deVore model, and showcase a significant variance reduction compared to the standard Monte Carlo method and another sampling-based IS technique, namely, multilevel cross entropy.},
  archive      = {J_SAC},
  author       = {Rached, Nadhir Ben and Schwerin, Erik von and Shaimerdenova, Gaukhar and Tempone, Raúl},
  doi          = {10.1007/s11222-025-10736-1},
  journal      = {Statistics and Computing},
  month        = {2},
  number       = {1},
  pages        = {1--27},
  shortjournal = {Stat. Comput.},
  title        = {Importance sampling for rare event tracking within the ensemble kalman filtering framework},
  volume       = {36},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Penalized robust estimating equation and variable selection in a partially linear single-index varying-coefficient model. <em>SAC</em>, <em>36</em>(1), 1--21. (<a href='https://doi.org/10.1007/s11222-025-10754-z'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper focuses on the variable selection and estimation for a partially linear single-index varying-coefficient model. A novel fusing $$L_1$$ and exponential integral penalty and a new loss function are proposed to construct the penalized robust estimating equation (PREE) for variable selection and estimations of the regression parameters. The consistency of the variable selection procedure and the oracle property of the regularized estimators are proved under some regularity conditions. The bias correction technique is employed in PREE to avoid undersmoothing of the coefficient functions. An iteration algorithm is proposed for estimating the regression parameters. The finite sample performance of our method is validated through simulation studies, and a real data analysis further confirms the validity of the proposed method.},
  archive      = {J_SAC},
  author       = {Li, Gaorong and Xue, Liugen and Zhang, Riquan},
  doi          = {10.1007/s11222-025-10754-z},
  journal      = {Statistics and Computing},
  month        = {2},
  number       = {1},
  pages        = {1--21},
  shortjournal = {Stat. Comput.},
  title        = {Penalized robust estimating equation and variable selection in a partially linear single-index varying-coefficient model},
  volume       = {36},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Uncertain model selection criteria for mixture modeling. <em>SAC</em>, <em>36</em>(1), 1--18. (<a href='https://doi.org/10.1007/s11222-025-10759-8'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Model selection criteria constitute one of the most critical issues in data clustering via mixture modeling. In this framework, criteria based on penalized log-likelihood (e.g. AIC and BIC) are usually adopted and one single model is finally selected. In this paper, an approach taking into account uncertainty in model selection is proposed in the maximum likelihood framework yielding a conditional probability distribution on the number of components, given a sample. The assessment of the number of population components, which are referred to as underlying populations, is also proposed based on employing a non-parametric bootstrap sampling of the observed data sample. Finally, a novel clustering approach relying on the developed concepts is presented. The proposal is illustrated on the ground of a numerical study based on both simulated and real data.},
  archive      = {J_SAC},
  author       = {Melnykov, Volodymyr and Ingrassia, Salvatore},
  doi          = {10.1007/s11222-025-10759-8},
  journal      = {Statistics and Computing},
  month        = {2},
  number       = {1},
  pages        = {1--18},
  shortjournal = {Stat. Comput.},
  title        = {Uncertain model selection criteria for mixture modeling},
  volume       = {36},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Goodness of fit in relational event models. <em>SAC</em>, <em>36</em>(1), 1--21. (<a href='https://doi.org/10.1007/s11222-025-10751-2'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A relational event process involves temporally ordered interactions between actors, where past network configurations may influence future ones. The relational event model (REM) can be used to identify the drivers of the underlying network dynamics. Despite the rapid development of REMs over the past 15 years, an ongoing area of research revolves around techniques for evaluating their goodness-of-fit, especially when they incorporate time-varying and random effects. Current methodologies often rely on comparing observed and simulated events using specific statistics, but this can be computationally intensive. We introduce a versatile framework for testing the goodness-of-fit of REMs using weighted martingale residuals. Our focus is on a Kolmogorov-Smirnov type test and its multivariate extensions designed to assess if covariates accurately model the dynamics. A simulation study is performed to assess the test’s power and coverage. Furthermore, we apply the method to a sociological study involving 57,791 emails sent by 159 employees of a Polish manufacturing company in 2010. The method is implemented using the R package mgcv.},
  archive      = {J_SAC},
  author       = {Boschi, Martina and Wit, Ernst C.},
  doi          = {10.1007/s11222-025-10751-2},
  journal      = {Statistics and Computing},
  month        = {2},
  number       = {1},
  pages        = {1--21},
  shortjournal = {Stat. Comput.},
  title        = {Goodness of fit in relational event models},
  volume       = {36},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Inverse probability weighting estimation under ultrahigh-dimensional error-prone covariates and misclassified treatments. <em>SAC</em>, <em>36</em>(1), 1--18. (<a href='https://doi.org/10.1007/s11222-025-10755-y'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Estimating the average treatment effect (ATE) is one of the fundamental problems in causal inference, as it quantifies the causal impact of a treatment on an outcome of interest. A widely used approach for ATE estimation is the inverse probability weighting method based on propensity scores. However, in practice, datasets often exhibit ultrahigh-dimensional covariates and measurement errors, which can lead to biased or unreliable ATE estimators if not properly addressed. In this paper, we focus on a challenging setting where both covariates and treatments may be measured with error, and the potential outcomes may exhibit nonlinear dependence on the covariates. To tackle these challenges and obtain a more accurate estimator of the ATE, we propose a novel method named FATE, which integrates feature screening, adaptive lasso, treatment adjustment, and error correction for covariates. The proposed feature screening procedure is based on measurement error adjusted data and is designed to accommodate a wide variety of outcome distributions. Furthermore, under appropriate corrections for both treatment misclassification and covariates measurement error, we construct a consistent estimator of the propensity score that accounts for possible collinearity. This leads to a reliable ATE estimator. Theoretical guarantees, including consistency and asymptotic properties, are established. Extensive numerical studies demonstrate that the proposed FATE method performs well and consistently outperforms several competing approaches.},
  archive      = {J_SAC},
  author       = {Chen, Li-Pang},
  doi          = {10.1007/s11222-025-10755-y},
  journal      = {Statistics and Computing},
  month        = {2},
  number       = {1},
  pages        = {1--18},
  shortjournal = {Stat. Comput.},
  title        = {Inverse probability weighting estimation under ultrahigh-dimensional error-prone covariates and misclassified treatments},
  volume       = {36},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Model-based clustering of time-dependent observations with common structural changes. <em>SAC</em>, <em>36</em>(1), 1--15. (<a href='https://doi.org/10.1007/s11222-025-10756-x'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a novel model-based clustering approach for samples of time series. We assume as a unique commonality that two observations belong to the same group if structural changes in their behaviors happen at the same time. We resort to a latent representation of structural changes in each time series, based on random orders, to induce ties among different observations. Such an approach results in a general modeling strategy and can be combined with many time-dependent models already known in the literature. Our studies have been motivated by an epidemiological problem. Specifically, we want to provide clusters of different countries of the European Union where two countries belong to the same cluster if the spreading processes of the COVID-19 virus show structural changes at the same time.},
  archive      = {J_SAC},
  author       = {Corradin, Riccardo and Danese, Luca and KhudaBukhsh, Wasiur R. and Ongaro, Andrea},
  doi          = {10.1007/s11222-025-10756-x},
  journal      = {Statistics and Computing},
  month        = {2},
  number       = {1},
  pages        = {1--15},
  shortjournal = {Stat. Comput.},
  title        = {Model-based clustering of time-dependent observations with common structural changes},
  volume       = {36},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Wrapped gaussian process functional regression model for batch data on riemannian manifolds. <em>SAC</em>, <em>36</em>(1), 1--23. (<a href='https://doi.org/10.1007/s11222-025-10758-9'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Regression is an essential and fundamental methodology in statistical analysis. The majority of the literature focuses on linear and non-linear regression in the context of Euclidean space. However, regression models in non-Euclidean spaces deserve more attention due to the collection of increasing volumes of manifold-valued data. In this context, this paper proposes a concurrent functional regression model for batch data on Riemannian manifolds by estimating both the mean structure and the covariance structure simultaneously. The response variable is assumed to follow a wrapped Gaussian distribution. Nonlinear relationships between manifold-valued response variables and multiple Euclidean covariates can be captured by this model in which the covariates can be functional and/or scalar. The performance of our model has been tested on both simulated and real data, showing that it is an effective and efficient tool to perform functional data regression on Riemannian manifolds.},
  archive      = {J_SAC},
  author       = {Liu, Jinzhao and Liu, Chao and Shi, Jian Qing and Nye, Tom},
  doi          = {10.1007/s11222-025-10758-9},
  journal      = {Statistics and Computing},
  month        = {2},
  number       = {1},
  pages        = {1--23},
  shortjournal = {Stat. Comput.},
  title        = {Wrapped gaussian process functional regression model for batch data on riemannian manifolds},
  volume       = {36},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Fast sampling and model selection for bayesian mixture models. <em>SAC</em>, <em>36</em>(1), 1--15. (<a href='https://doi.org/10.1007/s11222-025-10753-0'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We study Bayesian estimation of mixture models and argue in favor of fitting the marginal posterior distribution over component assignments directly, rather than Gibbs sampling from the joint posterior on components and parameters as is conventional. Some previous authors have found the former approach to have slow mixing, but we show that, implemented correctly, it can achieve excellent performance. In particular, we describe a new Monte Carlo algorithm for sampling from the marginal posterior of a general integrable mixture that makes use of rejection-free sampling from the prior over component assignments to achieve excellent mixing times in typical applications, outperforming standard Gibbs sampling, in some cases by a wide margin. We demonstrate the approach with a selection of applications to Gaussian and Poisson models and latent class analysis.},
  archive      = {J_SAC},
  author       = {Newman, M. E. J.},
  doi          = {10.1007/s11222-025-10753-0},
  journal      = {Statistics and Computing},
  month        = {2},
  number       = {1},
  pages        = {1--15},
  shortjournal = {Stat. Comput.},
  title        = {Fast sampling and model selection for bayesian mixture models},
  volume       = {36},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Bootstrap tests for almost goodness-of-fit. <em>SAC</em>, <em>36</em>(1), 1--19. (<a href='https://doi.org/10.1007/s11222-025-10762-z'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We introduce the almost goodness-of-fit test, a procedure to assess whether a (parametric) model provides a good representation of the probability distribution generating the observed sample. Specifically, given a distribution function F and a parametric family $$\mathcal {G}=\{ G(\varvec{\theta }): \varvec{\theta } \in \Theta \}$$ , we consider the testing problem $$ H_0: \Vert F - G(\varvec{\theta }_F) \Vert _p \ge \epsilon \quad \text {vs} \quad H_1: \Vert F - G(\varvec{\theta }_F) \Vert _p < \epsilon , $$ where $$\epsilon >0$$ is a margin of error and $$G(\varvec{\theta }_F)$$ denotes a representative of F within the parametric class. The approximate model is determined via an M-estimator of the parameters. The methodology also quantifies the percentage improvement of the proposed model relative to a non-informative (constant) benchmark. The test statistic is the $$\textrm{L}^p$$ -distance between the empirical distribution function and that of the estimated model. We present two consistent, easy-to-implement, and flexible bootstrap schemes to carry out the test. The performance of the proposal is illustrated through simulation studies and analysis and real-data applications.},
  archive      = {J_SAC},
  author       = {Baíllo, Amparo and Cárcamo, Javier},
  doi          = {10.1007/s11222-025-10762-z},
  journal      = {Statistics and Computing},
  month        = {2},
  number       = {1},
  pages        = {1--19},
  shortjournal = {Stat. Comput.},
  title        = {Bootstrap tests for almost goodness-of-fit},
  volume       = {36},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). A model identification and selection method for varying coefficient EV models with missing responses. <em>SAC</em>, <em>36</em>(1), 1--17. (<a href='https://doi.org/10.1007/s11222-025-10765-w'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we propose a model identification and selection method for varying coefficient errors-in-variables (EV) models with missing responses, termed the imputation-based bias-corrected double-penalized estimating equation (ibbcDPEE) method. The proposed method does not need to assume in advance whether the regression coefficients in models are constants or varying coefficients. First, it utilizes B-spline basis functions to approximate the nonparametric regression coefficients. Subsequently, the bias-corrected double-penalized estimating equation (bcDPEE) is constructed based on the observed responses, while accounting for the bias in the unobserved covariates. The missing responses are then imputed via the kernel estimation technique. Lastly, the ibbcDPEE is constructed to do model identification and selection simultaneously. Under some regularity conditions, the proposed method can consistently identify and select varying coefficients and nonzero constant coefficients. Moreover, the estimators of the varying coefficients achieve the optimal convergence rate of nonparametric function estimation. The finite sample performance of the proposed method is evaluated through simulation studies and a real data analysis.},
  archive      = {J_SAC},
  author       = {Li, Fanqun and Wu, Houwu and Feng, Sanying and Fan, Yan and Zhao, Mingtao},
  doi          = {10.1007/s11222-025-10765-w},
  journal      = {Statistics and Computing},
  month        = {2},
  number       = {1},
  pages        = {1--17},
  shortjournal = {Stat. Comput.},
  title        = {A model identification and selection method for varying coefficient EV models with missing responses},
  volume       = {36},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Practical considerations for implementing the isolation forest EWMA control chart in phase II process monitoring. <em>SAC</em>, <em>36</em>(1), 1--18. (<a href='https://doi.org/10.1007/s11222-025-10766-9'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Statistical monitoring methodologies are constantly evolving to cope with various data complexities, non-standard situations and related questions that often arise in modern day applications. These questions, for example, relate to the speed of data collection, multi-dimensionality and distributional assumptions. In this context, researchers in the Statistical Process Monitoring (SPM) literature have recently considered Statistical Learning (SL) techniques as viable means to define models under more practical assumptions, and thus to allow setting up control charts for monitoring process stability in a variety of situations. However, a rigorous investigation of some of the key issues related to the implementation of the SL based control charts, supporting on-line (Phase II) SPM, has been lacking and is much needed. As a first step in this direction, here we consider a control chart based on the Isolation Forest (IF), an unsupervised SL technique running an ensemble of decision trees, which has recently been extended to the SPM area. We examine key implementation issues related to the selection of a proper Phase I reference sample, as it strongly influences the model trained to construct the control chart and its statistical performance properties. Our results show that correctly running an IF control chart is a challenging task, needing careful attention by the practitioners. In particular, a huge Phase I sample size and a careful check of the reference sample stability are required to maintain the in-control performance of the control chart at the anticipated target level, and to prevent a significant deterioration of the chart’s shift detection capability.},
  archive      = {J_SAC},
  author       = {Perdikis, Theodoros and Celano, Giovanni and Chakraborti, Subhabrata},
  doi          = {10.1007/s11222-025-10766-9},
  journal      = {Statistics and Computing},
  month        = {2},
  number       = {1},
  pages        = {1--18},
  shortjournal = {Stat. Comput.},
  title        = {Practical considerations for implementing the isolation forest EWMA control chart in phase II process monitoring},
  volume       = {36},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Best-subset instrumental variable selection method using mixed integer optimization with applications to health-related quality of life and education–wage analyses. <em>SAC</em>, <em>36</em>(1), 1--22. (<a href='https://doi.org/10.1007/s11222-025-10760-1'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The classical best-subset selection method has been demonstrated to be nondeterministic polynomial-time-hard and thus presents computational challenges. This problem can now be solved via advanced mixed integer optimization (MIO) algorithms for linear regression. We extend this methodology to linear instrumental variable (IV) regression and propose the best-subset instrumental variable (BSIV) method incorporating the MIO procedure. Classical IV estimation methods assume that IVs must not directly impact the outcome variable and should remain uncorrelated with nonmeasured variables. However, in practice, IVs are likely to be invalid, and existing methods can lead to a large bias relative to standard errors in certain situations. The proposed BSIV estimator is robust in estimating causal effects in the presence of unknown IV validity. We demonstrate that the BSIV using MIO algorithms outperforms two-stage least squares, Lasso-type IVs, and two-sample analysis (median and mode estimators) through Monte Carlo simulations in terms of bias and relative efficiency. We analyze two datasets involving the health-related quality of life index and proximity and the education–wage relationship to demonstrate the utility of the proposed method.},
  archive      = {J_SAC},
  author       = {Qasim, Muhammad and Månsson, Kristofer and Balakrishnan, Narayanaswamy},
  doi          = {10.1007/s11222-025-10760-1},
  journal      = {Statistics and Computing},
  month        = {2},
  number       = {1},
  pages        = {1--22},
  shortjournal = {Stat. Comput.},
  title        = {Best-subset instrumental variable selection method using mixed integer optimization with applications to health-related quality of life and education–wage analyses},
  volume       = {36},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Adaptive reduced multilevel splitting. <em>SAC</em>, <em>36</em>(1), 1--25. (<a href='https://doi.org/10.1007/s11222-025-10724-5'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper considers the classical problem of sampling with Monte Carlo methods a target rare event distribution defined by a score function that is very expensive to compute. We assume we can build an approximate surrogate score using evaluations of the true score with error bounds. This work proposes a fully adaptive algorithm to sequentially sample surrogate rare event distributions with increasing target levels. An essential contribution consists in sampling at each iteration the surrogate rare event at a critical level corresponding to a specific cost. This cost is related to importance sampling of the target. The critical level is calculated solely from the reduced score and its error bound. From a practical point of view, sampling the proposal sequence is performed by extending the framework of the popular adaptive multilevel splitting algorithm to the use of score approximations. Numerical experiments evaluate the proposed importance sampling algorithm in terms of computational complexity versus squared error. In particular, we investigate the performance of the algorithm when simulating rare events related to the solution of a parametric PDE, which is approximated by a reduced basis.},
  archive      = {J_SAC},
  author       = {Cérou, Frédéric and Héas, Patrick and Rousset, Mathias},
  doi          = {10.1007/s11222-025-10724-5},
  journal      = {Statistics and Computing},
  month        = {2},
  number       = {1},
  pages        = {1--25},
  shortjournal = {Stat. Comput.},
  title        = {Adaptive reduced multilevel splitting},
  volume       = {36},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Distributed learning for high-dimensional factor augmented regression models. <em>SAC</em>, <em>36</em>(1), 1--18. (<a href='https://doi.org/10.1007/s11222-025-10761-0'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Factor Augmented Regression Model (FARM) is recently developed as a powerful tool to deal with high-dimensional correlated data. However, there is still lack of suitable methods for the FARM in a distributed system, which is becoming very common nowadays. In this article, we investigate the distributed learning of the FARM for high-dimensional data. A direct divide-and-conquer estimator by averaging all local penalized estimators is biased. We then in this paper develop two debiasing procedures for the FARM. The first approach still follows the divide-and-conquer strategy by averaging all local debiased estimators. The second approach takes a one-step estimation strategy, which reduces computational cost largely. Finally, a hard thresholding method is considered to ensure the sparsity of the proposed debiased lasso estimators. We establish the convergence rates and asymptotic normality of the proposed estimators. Through simulations and real-data experiments, we validate the finite-sample behavior of our estimators and provide practical guidelines for method selection in heterogeneous environments.},
  archive      = {J_SAC},
  author       = {Qi, Zhen and Guo, Xu and Qi, Haobo},
  doi          = {10.1007/s11222-025-10761-0},
  journal      = {Statistics and Computing},
  month        = {2},
  number       = {1},
  pages        = {1--18},
  shortjournal = {Stat. Comput.},
  title        = {Distributed learning for high-dimensional factor augmented regression models},
  volume       = {36},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). BOB: Bayesian optimized bootstrap for approximate posterior sampling in gaussian mixture models. <em>SAC</em>, <em>36</em>(1), 1--33. (<a href='https://doi.org/10.1007/s11222-025-10763-y'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The posterior distribution of a Gaussian mixture model (GMM) provides a natural framework to infer the model parameters or make predictions about a population of interest. That said, sampling from the posterior distribution of GMMs via standard Markov chain Monte Carlo (MCMC) imposes several computational challenges, which have slowed down the adoption of a broader full Bayesian implementation of these models. A growing body of literature has introduced the weighted likelihood bootstrap and the weighted Bayesian bootstrap as alternatives to MCMC sampling. The core idea of these methods is to repeatedly compute maximum a posteriori (MAP) estimates from many randomly weighted posterior densities. These MAP estimates then can be treated as approximate posterior draws. Nonetheless, a central question remains unanswered: How to select the random weights under arbitrary sample sizes. We, therefore, introduce the Bayesian optimized bootstrap (BOB), a computational method to automatically tune these random weights by minimizing, through Bayesian optimization, a black-box and noisy version of the reverse Kullback–Leibler (KL) divergence between the Bayesian posterior and an approximate posterior obtained via random weighting. Our proposed method outperforms competing approaches in recovering the Bayesian posterior, while retaining key asymptotic properties from established methodologies. BOB’s performance is demonstrated through extensive simulations, along with real-world data analyses.},
  archive      = {J_SAC},
  author       = {Marin, Santiago and Loong, Bronwyn and Westveld, Anton H.},
  doi          = {10.1007/s11222-025-10763-y},
  journal      = {Statistics and Computing},
  month        = {2},
  number       = {1},
  pages        = {1--33},
  shortjournal = {Stat. Comput.},
  title        = {BOB: Bayesian optimized bootstrap for approximate posterior sampling in gaussian mixture models},
  volume       = {36},
  year         = {2026},
}
</textarea>
</details></li>
</ul>

</body>
</html>
