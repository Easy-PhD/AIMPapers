<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>SAC</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="sac">SAC - 219</h2>
<ul>
<li><details>
<summary>
(2025). Improving the prediction accuracy of statistical models: A new hierarchical clustering approach. <em>SAC</em>, <em>35</em>(6), 1-23. (<a href='https://doi.org/10.1007/s11222-025-10683-x'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Statisticians and machine learning practitioners frequently encounter datasets originated from multiple populations but containing the same type of measurements. In such cases, predictive analytics is typically carried out by either fitting a separate model to each dataset independently or by merging the datasets and fitting a single model to the combined data. These approaches overlook the potential existence of multiple groups of datasets associated with different underlying models, and, therefore, fail to exploit the inherent similarity between datasets to improve predictions. A third alternative is to perform pairwise comparisons between the populations before fitting the models. However, this is not always feasible, can become a very challenging task with complex models, and often does not rely on predictive accuracy. To address these issues, we propose a clustering approach designed to improve predictions in general databases. The method is based on a novel type of objective function that represents the total by-group prediction error. The clustering problem is solved using a hierarchical-type algorithm of agglomerative nature that automatically obtains the resulting clustering partition in a fully data-driven manner. An additional advantage of this procedure is that the number of clusters is treated as a variable in the minimization problem, allowing it to be determined naturally in a way that optimizes the predictive accuracy of the underlying models. Furthermore, the technique is versatile and can be used with any type of model for both regression, and classification tasks. Several simulation experiments and two real-world applications involving housing prices demonstrate that the procedure outperforms benchmark approaches in terms of predictive accuracy.},
  archive      = {J_SAC},
  author       = {López-Oriona, Ángel and Sun, Ying and Vilar, José A.},
  doi          = {10.1007/s11222-025-10683-x},
  journal      = {Statistics and Computing},
  month        = {12},
  number       = {6},
  pages        = {1-23},
  shortjournal = {Stat. Comput.},
  title        = {Improving the prediction accuracy of statistical models: A new hierarchical clustering approach},
  volume       = {35},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Information-theoretic criteria for optimizing designs of individually randomized stepped-wedge clinical trials. <em>SAC</em>, <em>35</em>(6), 1-14. (<a href='https://doi.org/10.1007/s11222-025-10690-y'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Clinical trials are essential for advancing medical knowledge and improving health care, with Randomized Clinical Trials (RCTs) considered the gold standard for minimizing bias and generating reliable evidence on treatment efficacy and safety. Stepped-wedge individual RCTs, which randomize participants into sequences transitioning from control to intervention at staggered time points, are increasingly adopted. To improve their design, we propose an information-theoretic framework based on D– and A–optimality criteria for participant allocation to sequences. Our approach leverages semidefinite programming for automated computation and is applicable across a range of settings, varying in: (i) number of sequences, (ii) attrition rates, (iii) optimality criteria, (iv) error correlation structures, and (v) multi-objective designs using the $$\epsilon $$ -constraint method.},
  archive      = {J_SAC},
  author       = {Duarte, Belmiro P. M. and Atkinson, Anthony C. and Moerbeek, Mirjam},
  doi          = {10.1007/s11222-025-10690-y},
  journal      = {Statistics and Computing},
  month        = {12},
  number       = {6},
  pages        = {1-14},
  shortjournal = {Stat. Comput.},
  title        = {Information-theoretic criteria for optimizing designs of individually randomized stepped-wedge clinical trials},
  volume       = {35},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Fast post-process bayesian inference with variational sparse bayesian quadrature. <em>SAC</em>, <em>35</em>(6), 1-21. (<a href='https://doi.org/10.1007/s11222-025-10695-7'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In applied Bayesian inference scenarios, users may have access to a large number of pre-existing model evaluations, for example from maximum-a-posteriori (MAP) optimization runs. However, traditional approximate inference techniques make little to no use of this available information. We propose the framework of post-process Bayesian inference as a means to obtain a quick posterior approximation from existing target density evaluations, with no further model calls. Within this framework, we introduce Variational Sparse Bayesian Quadrature (vsbq), a method for post-process approximate inference for models with black-box and potentially noisy likelihoods. vsbq reuses existing target density evaluations to build a sparse Gaussian process (GP) surrogate model of the log posterior density function. Subsequently, we leverage sparse-GP Bayesian quadrature combined with variational inference to achieve fast approximate posterior inference over the surrogate. We validate our method on challenging synthetic scenarios and real-world applications from computational neuroscience. The experiments show that vsbq builds high-quality posterior approximations by post-processing existing optimization traces, with no further model evaluations.},
  archive      = {J_SAC},
  author       = {Li, Chengkun and Clarté, Grégoire and Jørgensen, Martin and Acerbi, Luigi},
  doi          = {10.1007/s11222-025-10695-7},
  journal      = {Statistics and Computing},
  month        = {12},
  number       = {6},
  pages        = {1-21},
  shortjournal = {Stat. Comput.},
  title        = {Fast post-process bayesian inference with variational sparse bayesian quadrature},
  volume       = {35},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Tests for simultaneous ordered alternatives in a two-way ANOVA with interaction. <em>SAC</em>, <em>35</em>(6), 1-23. (<a href='https://doi.org/10.1007/s11222-025-10706-7'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In many experimental designs, it is known a priori that the mean effects of the factors follow a monotone ordering. In this article, the problem of testing the homogeneity of the effects of both the factors against the alternative of their simultaneous monotone ordering is studied for a two-way crossed heteroscedastic ANOVA model. An intersection type test based on likelihood ratios of two sub-hypotheses and two multiple comparison tests are developed. Algorithms are proposed for the implementation of these tests using a parametric bootstrap approach and the asymptotic accuracy of this approach is also established. Extensive simulations are carried out to study the efficacy of these tests in controlling the type-I error rates and achieving a good power performance. It is shown that the proposed tests achieve the nominal size values regardless of the dimension of the design, number of replications in each cell and level of heterogeneity of error variances. Further, they are seen to have very good powers indicating consistency of tests. The proposed tests are further examined for their robustness under deviation from normality. An ‘R’ package is developed and shared on the open platform ‘GitHub’ for easy usage by practitioners. Finally, the applicability of our proposed test procedures is illustrated using two real data sets on mortality rates.},
  archive      = {J_SAC},
  author       = {Dey, Raju and Kumar, Somesh},
  doi          = {10.1007/s11222-025-10706-7},
  journal      = {Statistics and Computing},
  month        = {12},
  number       = {6},
  pages        = {1-23},
  shortjournal = {Stat. Comput.},
  title        = {Tests for simultaneous ordered alternatives in a two-way ANOVA with interaction},
  volume       = {35},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Quantile-based fitting for graph signals. <em>SAC</em>, <em>35</em>(6), 1-19. (<a href='https://doi.org/10.1007/s11222-025-10689-5'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The development of monitoring tools has led to an emerging demand for analyzing data residing on graphs, referred to as graph signals. In this study, we propose a quantile-based fitting method for graph signals, which can be applicable to graph signals with a wide range of distributions. Unlike traditional data fitting methods, such as smoothing splines or quantile smoothing splines in Euclidean space, the proposed method is designed for the graph domain, considering the inherent structure of graphs. In contrast to prevalent graph signal fitting methods that rely on optimization problems with $$L_2$$ -norm fidelity, the proposed method provides robust fits for graph signals in the presence of outliers. More importantly, it identifies various distributional structures of graph signals beyond the mean feature. We further investigate the theoretical properties of the proposed solution, including its existence and uniqueness. Through a comprehensive simulation study and real data analysis, we demonstrate the promising performance of the proposed method.},
  archive      = {J_SAC},
  author       = {Kim, Kyusoon and Oh, Hee-Seok},
  doi          = {10.1007/s11222-025-10689-5},
  journal      = {Statistics and Computing},
  month        = {12},
  number       = {6},
  pages        = {1-19},
  shortjournal = {Stat. Comput.},
  title        = {Quantile-based fitting for graph signals},
  volume       = {35},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Numerical approaches to finite-horizon optimal stopping problems for the shiryaev process. <em>SAC</em>, <em>35</em>(6), 1-22. (<a href='https://doi.org/10.1007/s11222-025-10696-6'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In a finite-horizon optimal stopping problem the optimal stopping time is typically given by the first moment at which a sufficient statistic, namely a process containing all the relevant information on the problem, exceeds an unknown time-dependent boundary. This boundary often turns out to be the solution of a highly nonlinear integral equation involving the transition density of the sufficient statistic. When this density cannot be computed directly or easily, standard methods for solving the integral equation must be modified. This situation arises in sequential detection problems and in the pricing of certain derivative securities, where the corresponding sufficient statistics follow the so called Shiryaev process. In this context, we analyze and implement three distinct numerical methods for solving the integral equations characterizing the associated optimal stopping boundaries: two of them rely on solutions to partial differential equations, while the third is based on approximating the distribution of the sufficient statistic using a log-normal distribution. We demonstrate that these approaches return accurate results and are generally efficient.},
  archive      = {J_SAC},
  author       = {Buonaguidi, B.},
  doi          = {10.1007/s11222-025-10696-6},
  journal      = {Statistics and Computing},
  month        = {12},
  number       = {6},
  pages        = {1-22},
  shortjournal = {Stat. Comput.},
  title        = {Numerical approaches to finite-horizon optimal stopping problems for the shiryaev process},
  volume       = {35},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Hierarchical mixtures of latent trait analyzers with concomitant variables for multivariate binary data. <em>SAC</em>, <em>35</em>(6), 1-20. (<a href='https://doi.org/10.1007/s11222-025-10697-5'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Analyzing three-way data is challenging due to complex dependencies between observations, which must be accounted for to ensure reliable results. We focus on hierarchical, multivariate, binary data organized in a three-way data structure, where rows correspond to first-level units, columns to variables, and layers to second-level units within which the first-level units are nested. In this framework, model-based clustering methods can be effectively employed for dimensionality reduction purposes, facilitating a clear understanding of the phenomenon under investigation. In this work, we propose a novel modeling tool for a hierarchical clustering of first- and second-level units. We extend the Mixture of Latent Trait Analyzers (MLTA) with concomitant variables by letting prior component probabilities depend also on second-level-specific random effects. Parameter estimation is performed by means of a double EM algorithm based on a variational approximation of the model log-likelihood function, along with a nonparametric maximum likelihood estimation of the second-level-specific random effect distribution. This latter approach allows to estimate a discrete distribution which directly provides a clustering of second-level units. Within (conditional on) each of such clusters, first-level units are partitioned thanks to the MLTA specification. The proposal is applied to data from the European Social Survey to partition countries (second-level units) according to the baseline attitude of their residents (first-level units) toward digital technologies (variables). Within these clusters, residents are partitioned on the basis of their attitude toward specific digital skills. The influence of socio-economic factors on the identification of digitalization profiles is also taken into consideration via a concomitant variable approach.},
  archive      = {J_SAC},
  author       = {Failli, Dalila and Marino, Maria Francesca and Arpino, Bruno},
  doi          = {10.1007/s11222-025-10697-5},
  journal      = {Statistics and Computing},
  month        = {12},
  number       = {6},
  pages        = {1-20},
  shortjournal = {Stat. Comput.},
  title        = {Hierarchical mixtures of latent trait analyzers with concomitant variables for multivariate binary data},
  volume       = {35},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Exact computation of angular halfspace depth. <em>SAC</em>, <em>35</em>(6), 1-29. (<a href='https://doi.org/10.1007/s11222-025-10700-z'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The angular halfspace depth ( $$ahD$$ ) was, already in 1987, the first depth function proposed for the nonparametric analysis of directional data. Mainly due to its presumed high computational cost and lack of efficient computational algorithms, it was never widely used in directional data analysis. We address the problem of the exact computation of $$ahD$$ in any dimension d. We proceed in two steps: (i) We express $$ahD$$ as a generalized (Euclidean) halfspace depth in dimension $$d-1$$ , using a projection approach. That allows us to develop fast exact computational algorithms for $$ahD$$ in dimensions $$d=1, 2, 3$$ . (ii) In spaces of dimension 3]]d 3 we design an inductive procedure that reduces the dimensionality d in the computation of $$ahD$$ , until the algorithms for $$d \le 3$$ can be used. Using our advances we develop a family of powerful algorithms for the computation of $$ahD$$ in any dimension d. Our procedures are implemented efficiently in C++ with an interface in R. A detailed analysis of the complexity of the novel algorithms is performed. Surprisingly, we show that computing $$ahD$$ of multiple points with respect to the same dataset is substantially faster than the same task for the classical (Euclidean) halfspace depth.},
  archive      = {J_SAC},
  author       = {Dyckerhoff, Rainer and Nagy, Stanislav},
  doi          = {10.1007/s11222-025-10700-z},
  journal      = {Statistics and Computing},
  month        = {12},
  number       = {6},
  pages        = {1-29},
  shortjournal = {Stat. Comput.},
  title        = {Exact computation of angular halfspace depth},
  volume       = {35},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Testing mean independence with functional covariate. <em>SAC</em>, <em>35</em>(6), 1-21. (<a href='https://doi.org/10.1007/s11222-025-10705-8'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a new nonparametric conditional mean independence test for a scalar response and a functional covariate. The test statistics are built from continuous functionals over a residual marked empirical process indexed by a randomly projected functional covariate, which is less sensitive to tuning parameters and circumvents the curse of dimensionality. The asymptotic properties of the proposed test statistics under the null and the fixed alternative are established. We also show that our proposed test is able to detect a broad class of local alternatives converging to the null at the parametric rate. Due to the non-pivotal limiting null distribution, we use an easy-to-implement multiplier bootstrap procedure to estimate the critical values. Monte Carlo simulation studies demonstrate that our test outperforms other tests available in the literature due to its higher power and computational efficiency. The proposed test is further illustrated by analyzing the Tecator data set.},
  archive      = {J_SAC},
  author       = {Feng, Yongzhen and Li, Jie and Lu, Haokun and Song, Xiaojun},
  doi          = {10.1007/s11222-025-10705-8},
  journal      = {Statistics and Computing},
  month        = {12},
  number       = {6},
  pages        = {1-21},
  shortjournal = {Stat. Comput.},
  title        = {Testing mean independence with functional covariate},
  volume       = {35},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Random sampling of contingency tables and partitions: Two practical examples of the burnside process. <em>SAC</em>, <em>35</em>(6), 1-20. (<a href='https://doi.org/10.1007/s11222-025-10708-5'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper gives new, efficient algorithms for approximate uniform sampling of contingency tables and integer partitions. The algorithms use the Burnside process, a general algorithm for sampling a uniform orbit of a finite group acting on a finite set. We show that a technique called ‘lumping’ can be used to derive efficient implementations of the Burnside process. For both contingency tables and partitions, the lumped processes have far lower per step complexity than the original Markov chains. We also define a second Markov chain for partitions called the reflected Burnside process. The reflected Burnside process maintains the computational advantages of the lumped process but empirically converges to the uniform distribution much more rapidly. By using the reflected Burnside process we can easily sample uniform partitions of size $$10^{10}$$ .},
  archive      = {J_SAC},
  author       = {Diaconis, Persi and Howes, Michael},
  doi          = {10.1007/s11222-025-10708-5},
  journal      = {Statistics and Computing},
  month        = {12},
  number       = {6},
  pages        = {1-20},
  shortjournal = {Stat. Comput.},
  title        = {Random sampling of contingency tables and partitions: Two practical examples of the burnside process},
  volume       = {35},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Outcome-guided spike-and-slab lasso biclustering: A novel approach for enhancing biclustering techniques for gene expression analysis. <em>SAC</em>, <em>35</em>(6), 1-24. (<a href='https://doi.org/10.1007/s11222-025-10709-4'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Biclustering has gained interest in gene expression data analysis due to its ability to identify groups of samples that exhibit similar behaviour in specific subsets of genes (or vice versa), in contrast to traditional clustering methods that classify samples based on all genes. Despite advances, biclustering remains a challenging problem, even with cutting-edge methodologies. This paper introduces an extension of the recently proposed Spike-and-Slab Lasso Biclustering (SSLB) algorithm, termed Outcome-Guided SSLB (OG-SSLB), aimed at enhancing the identification of biclusters in gene expression analysis. Our proposed approach integrates disease outcomes into the biclustering framework through Bayesian profile regression. By leveraging additional clinical information, OG-SSLB improves the interpretability and relevance of the resulting biclusters. Comprehensive simulations and numerical experiments demonstrate that OG-SSLB achieves superior performance, with improved accuracy in estimating the number of clusters and higher consensus scores compared to the original SSLB method. Furthermore, OG-SSLB effectively identifies meaningful patterns and associations between gene expression profiles and disease states. These promising results demonstrate the effectiveness of OG-SSLB in advancing biclustering techniques, providing a powerful tool for uncovering biologically relevant insights. The OGSSLB software can be found as an R/C++ package at https://github.com/luisvargasmieles/OGSSLB .},
  archive      = {J_SAC},
  author       = {Vargas-Mieles, Luis A. and Kirk, Paul D. W. and Wallace, Chris},
  doi          = {10.1007/s11222-025-10709-4},
  journal      = {Statistics and Computing},
  month        = {12},
  number       = {6},
  pages        = {1-24},
  shortjournal = {Stat. Comput.},
  title        = {Outcome-guided spike-and-slab lasso biclustering: A novel approach for enhancing biclustering techniques for gene expression analysis},
  volume       = {35},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Distributed estimation and inference for high-dimensional confounded models. <em>SAC</em>, <em>35</em>(6), 1-19. (<a href='https://doi.org/10.1007/s11222-025-10710-x'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The hidden confounded model has been widely applied in many fields. Without adjusting for the hidden confounders, the estimators from the standard methods of high-dimensional models could be biased, potentially leading to spurious scientific discoveries. Meanwhile, distributed computation has attracted wide attention in modern statistical learning. Based on high-dimensional confounded models, this paper proposes a deconfounding and debiasing approach for distributed computing, aiming to obtain accurate estimation by reducing the confounding effect and bias. Two different distributed methods are applied: one is the straightforward divide-and-conquer (DC) method and the other is the communication-efficient surrogate likelihood (CSL) method. The former is easy to use in practice, while the latter uses surrogate loss to achieve better performance than the former through multiple iterations. The estimation accuracy and asymptotic theories for both DC and CSL estimators are established. Extensive simulation experiments verify the good performance of the two estimators and two real data applications are also presented to illustrate their validity and feasibility.},
  archive      = {J_SAC},
  author       = {Liu, Jin and Fei, Yuxin and Ma, Wei and Wang, Lei},
  doi          = {10.1007/s11222-025-10710-x},
  journal      = {Statistics and Computing},
  month        = {12},
  number       = {6},
  pages        = {1-19},
  shortjournal = {Stat. Comput.},
  title        = {Distributed estimation and inference for high-dimensional confounded models},
  volume       = {35},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Bayesian additive tree ensembles for composite quantile regressions. <em>SAC</em>, <em>35</em>(6), 1-15. (<a href='https://doi.org/10.1007/s11222-025-10711-w'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we introduce a novel approach that integrates Bayesian additive regression trees (BART) with the composite quantile regression (CQR) framework, creating a robust method for modeling complex relationships between predictors and outcomes under various error distributions. Unlike traditional quantile regression, which focuses on specific quantile levels, our proposed method, composite quantile BART, offers greater flexibility in capturing the entire conditional distribution of the response variable. By leveraging the strengths of BART and CQR, the proposed method provides enhanced predictive performance, especially in the presence of heavy-tailed errors and non-linear covariate effects. Numerical studies confirm that the proposed composite quantile BART method generally outperforms classical BART, quantile BART, and composite quantile linear regression models in terms of RMSE, especially under heavy-tailed or contaminated error distributions. Notably, under contaminated normal errors, it reduces RMSE by approximately 17% compared to composite quantile regression, and by 27% compared to classical BART.},
  archive      = {J_SAC},
  author       = {Lim, Yaeji and Lu, Ruijin and Ville, Madeleine St. and Chen, Zhen},
  doi          = {10.1007/s11222-025-10711-w},
  journal      = {Statistics and Computing},
  month        = {12},
  number       = {6},
  pages        = {1-15},
  shortjournal = {Stat. Comput.},
  title        = {Bayesian additive tree ensembles for composite quantile regressions},
  volume       = {35},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Simulation based bayesian optimization. <em>SAC</em>, <em>35</em>(6), 1-13. (<a href='https://doi.org/10.1007/s11222-025-10715-6'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Bayesian Optimization (BO) is a powerful method for optimizing black-box functions by combining prior knowledge with ongoing function evaluations. BO constructs a probabilistic surrogate model of the objective function given the covariates, which is in turn used to inform the selection of future evaluation points through an acquisition function. For smooth continuous search spaces, Gaussian Processes (GPs) are commonly used as the surrogate model as they offer analytical access to posterior predictive distributions, thus facilitating the computation and optimization of acquisition functions. However, in complex scenarios involving optimization over categorical or mixed covariate spaces, GPs may not be ideal. This paper introduces Simulation Based Bayesian Optimization (SBBO) as a novel approach to optimizing acquisition functions that only requires sampling-based access to posterior predictive distributions. SBBO allows the use of surrogate probabilistic models tailored for combinatorial spaces with discrete variables. Any Bayesian model in which posterior inference is carried out through Markov chain Monte Carlo can be selected as the surrogate model in SBBO. We demonstrate empirically the effectiveness of SBBO using various choices of surrogate models in applications involving combinatorial optimization.},
  archive      = {J_SAC},
  author       = {Naveiro, Roi and Tang, Becky},
  doi          = {10.1007/s11222-025-10715-6},
  journal      = {Statistics and Computing},
  month        = {12},
  number       = {6},
  pages        = {1-13},
  shortjournal = {Stat. Comput.},
  title        = {Simulation based bayesian optimization},
  volume       = {35},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Heterogeneous analysis of longitudinal profiles using adaptive banded precision matrices via penalized fusion. <em>SAC</em>, <em>35</em>(6), 1-14. (<a href='https://doi.org/10.1007/s11222-025-10716-5'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Clustering profiles of longitudinal data is a prevalent method employed for analyzing heterogeneity patterns among individuals, aiming to identify clusters based on diverse patterns of the mean progression trajectories. The correlation structure plays a crucial role in longitudinal data analysis, as accurate modeling of this structure enhances the estimation efficiency of mean trajectories. In this paper, we assume that subjects are sampled from a Gaussian mixture distribution, and incorporate regularized bandable precision matrix structure information for each subgroup. In order to identify the latent group structure, we employ concave penalty functions to estimate the pairwise differences of the model parameters derived from finite mixture models. The model parameters and cluster labels are estimated simultaneously using the Expectation-Maximization (EM) algorithm in conjunction with the Alternating Direction Method of Multipliers (ADMM) algorithm. We establish computational convergence and provide a statistical guarantee through demonstrating the asymptotic rate. Numerical studies and real data analysis show improved clustering results and excellent accuracy performance.},
  archive      = {J_SAC},
  author       = {Liang, Chunhui and Ma, Wenqing},
  doi          = {10.1007/s11222-025-10716-5},
  journal      = {Statistics and Computing},
  month        = {12},
  number       = {6},
  pages        = {1-14},
  shortjournal = {Stat. Comput.},
  title        = {Heterogeneous analysis of longitudinal profiles using adaptive banded precision matrices via penalized fusion},
  volume       = {35},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Tensor train approximation of multivariate gaussian density by scaling and squaring. <em>SAC</em>, <em>35</em>(6), 1-12. (<a href='https://doi.org/10.1007/s11222-025-10707-6'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Tensor train decomposition is a promising tool for dealing with high dimensional arrays. Point mass filters utilise such arrays for representing probability density functions of the state. Proofs of concept of the application of the low rank decomposition have been provided in the literature. However, the application requires to design parameters, such as tensor train ranks. Since the parameters dictating the computational requirements are derived from the data according to more abstract hyper-parameters such as precision, an analysis of benchmark examples is needed for allocating resources. This paper studies the ranks in the case of Gaussian densities. The influence of correlation and the effect of rounding are discussed first. Efficiency of the density representation used by standard point mass filters is considered next. Aspects of series expansion of the Gaussian density evaluated over array are considered for the tensor train format. The growth of the ranks is illustrated on a four-dimensional example. An observation of the growth for a multi-dimensional case is made last. The lessons learned are valuable for designing efficient point mass filters. Namely, they show that at least the naive implementations of tensor decomposition methods do not break the curse of dimensionality.},
  archive      = {J_SAC},
  author       = {Ajgl, Jiří and Straka, Ondřej},
  doi          = {10.1007/s11222-025-10707-6},
  journal      = {Statistics and Computing},
  month        = {12},
  number       = {6},
  pages        = {1-12},
  shortjournal = {Stat. Comput.},
  title        = {Tensor train approximation of multivariate gaussian density by scaling and squaring},
  volume       = {35},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A new resampling method for meta gaussian distributions. <em>SAC</em>, <em>35</em>(6), 1-14. (<a href='https://doi.org/10.1007/s11222-025-10717-4'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Meta Gaussian distributions, also known as multivariate Gaussian copula models, are a type of statistical distribution that is particularly useful in modeling dependencies among variables. The key advantage of meta Gaussian distributions is their flexibility - they can capture a wide range of dependency structures, making them a powerful tool for statistical modeling. Discrete approximations of continuous multivariate distributions, such as meta Gaussian distributions, which are of significant importance, are widely utilized across numerous disciplines. This paper introduces a new resampling method based on mean square error representative points (MSE-RPs) to construct accurate approximations for meta Gaussian distributions, thereby enhancing precision in statistical analysis. We carry out a systematic examination of the structural patterns and characteristics of MSE-RPs of these distributions. From a theoretical perspective, we analyze the invariance properties in copula-based association measures by leveraging group theory. This allows us to identify more stable invariants that are suitable for complex dependency structures. Through a simulation study, we demonstrate that MSE-RPs achieve significantly higher estimation accuracy for mean vectors and correlation matrices compared to Monte Carlo (MC) and Quasi-Monte Carlo (QMC) methods. Furthermore, MSE-RPs offer faster computation relative to the QMC method based on generalized good lattice points (GGLP) sets. Finally, we illustrate the practical advantages of our approach through empirical analysis on real-world datasets.},
  archive      = {J_SAC},
  author       = {He, Pingan and Fang, Kai-Tai and He, Ping and Ye, Huajun},
  doi          = {10.1007/s11222-025-10717-4},
  journal      = {Statistics and Computing},
  month        = {12},
  number       = {6},
  pages        = {1-14},
  shortjournal = {Stat. Comput.},
  title        = {A new resampling method for meta gaussian distributions},
  volume       = {35},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Predictive subgroup logistic regression for classification with unobserved heterogeneity. <em>SAC</em>, <em>35</em>(6), 1-23. (<a href='https://doi.org/10.1007/s11222-025-10712-9'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Unobserved heterogeneity refers to the variation among subjects that is not accounted for by the observed features used in a model. Its presence poses a substantial challenge to statistical modeling. This study introduces the Predictive Subgroup Logistic Regression (PSLR) model, which extends the conventional logistic regression and is specifically designed to address unobserved heterogeneity in classification problems. The PSLR model incorporates subject-specific intercepts in the log odds, fitted through a penalized likelihood approach with a concave pairwise fusion penalty. A novel two-step procedure is developed to facilitate the out-of-sample predictions for new subjects whose subgroup membership labels are unknown. This procedure allows the PSLR model to perform both inferential and predictive tasks. Through extensive simulation studies and an empirical application to a customer churn dataset in the telecommunications industry, the PSLR model not only demonstrates great performance in various aggregate accuracy metrics but also achieves a balanced effectiveness in sensitivity and specificity.},
  archive      = {J_SAC},
  author       = {Chen, Kun and Huang, Rui and Tong, Zhiwei},
  doi          = {10.1007/s11222-025-10712-9},
  journal      = {Statistics and Computing},
  month        = {12},
  number       = {6},
  pages        = {1-23},
  shortjournal = {Stat. Comput.},
  title        = {Predictive subgroup logistic regression for classification with unobserved heterogeneity},
  volume       = {35},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Gradient learning of symmetric positive-definite matrix regression. <em>SAC</em>, <em>35</em>(6), 1-16. (<a href='https://doi.org/10.1007/s11222-025-10714-7'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Non-Euclidean data are nowadays frequently encountered due to the advance in data-collection techniques. Under the Tikhonov regularization framework, this paper focuses on the gradient learning in a regression setting where the response is a symmetric positive-definite (SPD) matrix and the predictor is a Euclidean vector. We endow the SPD manifold with the Log-Euclidean metric to transform our model on the manifold to the Euclidean space and calculate the gradients by solving a linear system under the assumption that the gradient function resides in a reproducing kernel Hilbert space. We further simplify our algorithm and reduce the dimension of the linear system by singular value decomposition. Theoretical properties about the approximation error of the reducing-matrix-size algorithm and the error bound of gradient estimation are investigated as well. In numerical experiments, we show the validity of our SPD gradient learning algorithm in variable selection and sufficient dimension reduction. A real-world dataset about New York taxi networks is studied to illustrate the applicability of our algorithm.},
  archive      = {J_SAC},
  author       = {Chen, Baiyu and Fu, Xiaoyi and Li, Yunchen and Wang, Xiaozhou and Yu, Zhou},
  doi          = {10.1007/s11222-025-10714-7},
  journal      = {Statistics and Computing},
  month        = {12},
  number       = {6},
  pages        = {1-16},
  shortjournal = {Stat. Comput.},
  title        = {Gradient learning of symmetric positive-definite matrix regression},
  volume       = {35},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Enhancing proportional hazards mixture cure models with transfer learning for interval-censored data. <em>SAC</em>, <em>35</em>(6), 1-18. (<a href='https://doi.org/10.1007/s11222-025-10721-8'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Motivated by a breast cancer study, we consider the analysis of interval-censored failure time data in the presence of a cure fraction. Although a great deal of literature has been established for the analysis of interval-censored data with cure fractions, there is no established method that adequately handles the limited sample size issue. Corresponding to this, we propose a transfer learning approach under the proportional hazards mixture cure models for interval-censored data with the aim to transfer the information from the informative auxiliary samples in a larger cohort to improve the performance of the target regression analysis. To assess the proposed method, an extensive simulation study is performed and suggests that it works well in practical situations. Furthermore, we apply the method to the breast cancer study with the focus on Black women by leveraging the data of other racial women and obtain the improved results.},
  archive      = {J_SAC},
  author       = {Lou, Yichen and Sun, Jianguo and Wang, Peijie and Zhao, Shishun},
  doi          = {10.1007/s11222-025-10721-8},
  journal      = {Statistics and Computing},
  month        = {12},
  number       = {6},
  pages        = {1-18},
  shortjournal = {Stat. Comput.},
  title        = {Enhancing proportional hazards mixture cure models with transfer learning for interval-censored data},
  volume       = {35},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Robust mean-shift clustering based on impartial trimming. <em>SAC</em>, <em>35</em>(6), 1-19. (<a href='https://doi.org/10.1007/s11222-025-10718-3'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A robust version of the mean-shift algorithm is developed to cope with the presence of contaminated data when targeting a clustering problem by means of a modal approach. The goal is to protect nonparametric density-based clustering from the deleterious effect of outliers. Their occurrence affects the analysis mainly because outliers lead to the detection of spurious modes and groups. Therefore, the proposed methodology aims to recover the underlying clustered data configuration, while detecting and discarding outliers. A strategy to select the level of trimming is discussed. The finite sample behaviour of the proposed method is investigated by Monte Carlo numerical studies and empirical applications.},
  archive      = {J_SAC},
  author       = {Greco, Luca and Menardi, Giovanna},
  doi          = {10.1007/s11222-025-10718-3},
  journal      = {Statistics and Computing},
  month        = {12},
  number       = {6},
  pages        = {1-19},
  shortjournal = {Stat. Comput.},
  title        = {Robust mean-shift clustering based on impartial trimming},
  volume       = {35},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multi-network assisted clustering using a grouped factor model. <em>SAC</em>, <em>35</em>(6), 1-13. (<a href='https://doi.org/10.1007/s11222-025-10719-2'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This study addresses the clustering task of large-scale panel data with assistance from multi-network information under the grouped factor model. In many real-world clustering tasks, multiple networks can be observed for the same set of cross-sectional units based on different types of interactions. Different networks are different portraits of latent group memberships, which inspired us to utilize multi-network information to improve the clustering accuracy and stability. Therefore, we propose a multi-network-assisted clustering method that encourages coherence between the clustering results and the weighted multi-network in a penalized manner. We also developed a flexible weight learning strategy to identify the clustering capacity differences of multiple networks. A computationally efficient algorithm with random initialization was developed to implement penalized estimation. Thorough simulation studies demonstrate that the proposed method is more promising than existing competitors, even with misleading network information. Finally, application to the daily returns of stocks traded on the Shanghai and Shenzhen stock exchanges demonstrates the effectiveness and efficiency of the new method. Supplementary materials for this article are available online.},
  archive      = {J_SAC},
  author       = {Liang, Wanwan and Wu, Ben and Fan, Xinyan and Zhang, Bo},
  doi          = {10.1007/s11222-025-10719-2},
  journal      = {Statistics and Computing},
  month        = {12},
  number       = {6},
  pages        = {1-13},
  shortjournal = {Stat. Comput.},
  title        = {Multi-network assisted clustering using a grouped factor model},
  volume       = {35},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Integrating quantile regression for multi-source subgroup identification. <em>SAC</em>, <em>35</em>(6), 1-13. (<a href='https://doi.org/10.1007/s11222-025-10713-8'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Data heterogeneity and the multi-source nature of modern datasets present significant challenges for statistical methodologies. Motivated by the Stimulant Reduction Intervention using Dosed Exercise (STRIDE) study, we analyze primary outcomes from two complementary data sources to assess heterogeneity in treatment effects between experimental and control groups at medium and high quantiles. To address these challenges, we propose a novel approach that integrates quantile regression with weighted quantile loss and joint pairwise fusion penalties, enabling joint subgroup identification across data sources. Our method distinguishes between homogeneous and heterogeneous effects using a center regularization term and can detect sources lacking group structures. Theoretically, we establish weak Oracle properties, ensuring consistent estimation of group structures. Computationally, we employ the alternating direction method of multipliers (ADMM) and mitigate the burden of pairwise fusion through a k-nearest neighbors trimming method. The effectiveness of our approach is demonstrated through numerical simulations and an application to the STRIDE study.},
  archive      = {J_SAC},
  author       = {Wu, Jiaqi and Zhang, Weiping},
  doi          = {10.1007/s11222-025-10713-8},
  journal      = {Statistics and Computing},
  month        = {12},
  number       = {6},
  pages        = {1-13},
  shortjournal = {Stat. Comput.},
  title        = {Integrating quantile regression for multi-source subgroup identification},
  volume       = {35},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Finite mixtures of multivariate poisson-log normal factor analyzers for clustering count data. <em>SAC</em>, <em>35</em>(6), 1-31. (<a href='https://doi.org/10.1007/s11222-025-10720-9'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A mixture of multivariate Poisson-log normal factor analyzers is introduced by imposing constraints on the covariance matrix, which results in flexible models for clustering purposes. In particular, a class of eight parsimonious mixture models based on the mixtures of factor analyzers is introduced. The variational Gaussian approximation is used for parameter estimation, and information criteria are used for model selection. The proposed models are explored in the context of clustering discrete data arising from RNA sequencing studies. Using real and simulated data, the models are shown to give favourable clustering performance. The GitHub R package for this work is available at https://github.com/anjalisilva/mixMPLNFA and is released under the open-source MIT license.},
  archive      = {J_SAC},
  author       = {Payne, Andrea and Silva, Anjali and Rothstein, Steven J and McNicholas, Paul D. and Subedi, Sanjeena},
  doi          = {10.1007/s11222-025-10720-9},
  journal      = {Statistics and Computing},
  month        = {12},
  number       = {6},
  pages        = {1-31},
  shortjournal = {Stat. Comput.},
  title        = {Finite mixtures of multivariate poisson-log normal factor analyzers for clustering count data},
  volume       = {35},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Randomized spectral clustering for large-scale multi-layer networks. <em>SAC</em>, <em>35</em>(6), 1-20. (<a href='https://doi.org/10.1007/s11222-025-10723-6'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Large-scale multi-layer networks with large numbers of nodes, edges, and layers arise across various domains, which poses a great computational challenge for downstream analysis. In this paper, we develop an efficient randomized spectral clustering algorithm for community detection in multi-layer networks. We first utilize the random sampling strategy to sparsify the adjacency matrix of each layer. Then we use the random projection strategy to accelerate the eigen-decomposition of the sum of squared sparsified adjacency matrices of all layers. The communities are finally obtained via the k-means of the eigenvectors. The algorithm not only has low time complexity but also saves storage space. Theoretically, we study the misclassification error rate of the proposed algorithm under the multi-layer stochastic block model, which shows that the randomization does not deteriorate the error bound under certain conditions. Numerical studies on multi-layer networks with millions of nodes show the superior efficiency of the proposed algorithm, which achieves clustering results rapidly. We develop a new R package, MLRclust, which makes the proposed methods available for both simulated and real multi-layer networks.},
  archive      = {J_SAC},
  author       = {Su, Wenqing and Guo, Xiao and Chang, Xiangyu and Yang, Ying},
  doi          = {10.1007/s11222-025-10723-6},
  journal      = {Statistics and Computing},
  month        = {12},
  number       = {6},
  pages        = {1-20},
  shortjournal = {Stat. Comput.},
  title        = {Randomized spectral clustering for large-scale multi-layer networks},
  volume       = {35},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Enhancing quantile function estimation with beta-kernel smoothing. <em>SAC</em>, <em>35</em>(6), 1-21. (<a href='https://doi.org/10.1007/s11222-025-10725-4'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper introduces a class of nonparametric quantile function estimators based on Beta kernel smoothing. We conduct a rigorous investigation into their large-sample properties, including asymptotic normality and mean squared equivalence to existing methods. Through extensive simulation studies, we demonstrate that the proposed Beta kernel estimators perform comparably to or outperform traditional empirical and symmetric location-scale kernel-based quantile estimators. Additionally, we provide two real-world applications to illustrate the practical effectiveness of our approach. The results suggest that Beta kernel smoothing offers a flexible and efficient alternative for quantile function estimation, particularly in cases where classical methods exhibit inefficiencies.},
  archive      = {J_SAC},
  author       = {Li, Juan and Yu, Ping and Shi, Jianhong and Song, Weixing},
  doi          = {10.1007/s11222-025-10725-4},
  journal      = {Statistics and Computing},
  month        = {12},
  number       = {6},
  pages        = {1-21},
  shortjournal = {Stat. Comput.},
  title        = {Enhancing quantile function estimation with beta-kernel smoothing},
  volume       = {35},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Bayesian additive weighted composite quantile regression. <em>SAC</em>, <em>35</em>(6), 1-25. (<a href='https://doi.org/10.1007/s11222-025-10726-3'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we propose a nonparametric Bayesian estimation and model selection method for additive models based on weighted composite quantile regression. This method identifies the unknown smooth functions of the additive model as linear, nonlinear, or zero components by setting a multiplicative parameterized spike-slab prior distribution, which solves the problem of selecting predictive variable components in partially linear additive models when prior information is insufficient. In addition, it further generalizes the composite quantile regression to additive models by combining information from multiple quantiles. A Bayesian hierarchical model is established based on the mixed representation of the asymmetric Laplace distribution, and the posterior distributions of all unknown parameters are sampled by Markov chain Monte Carlo (MCMC). Finally, in the simulation and real data analysis, the variable selection results, root mean square error and other indicators are used to further prove that the method is more competitive than the existing methods.},
  archive      = {J_SAC},
  author       = {Ji, Yonggang and Wang, Mian and Zhou, Maoyuan},
  doi          = {10.1007/s11222-025-10726-3},
  journal      = {Statistics and Computing},
  month        = {12},
  number       = {6},
  pages        = {1-25},
  shortjournal = {Stat. Comput.},
  title        = {Bayesian additive weighted composite quantile regression},
  volume       = {35},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Adaptive stratified monte carlo using decision trees. <em>SAC</em>, <em>35</em>(6), 1-12. (<a href='https://doi.org/10.1007/s11222-025-10731-6'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {It has been known for a long time that stratification is one possible strategy to obtain higher convergence rates for the Monte Carlo estimation of integrals over the hypercube $$[0, 1]^s$$ of dimension s. However, stratified estimators such as Haber’s are not practical as s grows, as they require $$\mathcal {O}(k^s)$$ evaluations for some $$k\ge 2$$ . We propose an adaptive stratification strategy, where the strata are derived from a decision tree applied to a preliminary sample. We show that this strategy leads to higher convergence rates, that is the corresponding estimators converge at rate $$\mathcal {O}(N^{-1/2-r})$$ for some $$r>0$$ for certain classes of functions. Empirically, we show through numerical experiments that the method may improve on standard Monte Carlo even when s is large.},
  archive      = {J_SAC},
  author       = {Chopin, Nicolas and Wang, Hejin and Gerber, Mathieu},
  doi          = {10.1007/s11222-025-10731-6},
  journal      = {Statistics and Computing},
  month        = {12},
  number       = {6},
  pages        = {1-12},
  shortjournal = {Stat. Comput.},
  title        = {Adaptive stratified monte carlo using decision trees},
  volume       = {35},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A hybrid variational bayesian approach for spatial random effects structural equation modeling. <em>SAC</em>, <em>35</em>(6), 1-18. (<a href='https://doi.org/10.1007/s11222-025-10730-7'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, structural equation modeling (SEM) has been widely applied in fields such as education, psychology, and environmental science. However, most studies overlook the spatial dependencies within the data, and there is limited research on SEM for spatial data. Spatial random effects (SRE) models, which flexibly capture spatial variation through a series of spatial basis functions (e.g., multiresolution wavelet basis functions), have become a powerful tool for spatial data analysis. This study extends the traditional SEM by incorporating SRE, resulting in a spatial random effects structural equation model (SRE-SEM) for modeling complex environmental spatial data. The model is fitted using a hybrid variational Bayes algorithm, with some model parameters estimated via the standard mean-field variational Bayes approach. To address the intractable posterior and the dimensionality of latent variables, the Metropolis-Hastings algorithm is employed to sample from the exact conditional posterior distribution of the latent variables. Additionally, a fixed-form variational Bayes approach is used to estimate the matrix on which the spatial covariance matrix depends. Simulation studies and case analyses demonstrate that the proposed model effectively captures the structure of spatial data, while the introduced estimation algorithms significantly improve computational efficiency. This study provides a robust and efficient framework for spatial data analysis, offering a promising solution for modeling complex environmental and socio-economic phenomena.},
  archive      = {J_SAC},
  author       = {Wu, Ying and Zhu, Hongyu and Zhang, Jiwei and Lu, Jing},
  doi          = {10.1007/s11222-025-10730-7},
  journal      = {Statistics and Computing},
  month        = {12},
  number       = {6},
  pages        = {1-18},
  shortjournal = {Stat. Comput.},
  title        = {A hybrid variational bayesian approach for spatial random effects structural equation modeling},
  volume       = {35},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Spilt conformal prediction with missing response. <em>SAC</em>, <em>35</em>(6), 1-14. (<a href='https://doi.org/10.1007/s11222-025-10722-7'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Conformal prediction provides a distribution-free framework for constructing interval estimates of response variables with guaranteed coverage probabilities. This study introduces a missing weighted split conformalized quantile regression (M-WSCQR) method to address challenges associated with missing response variables, assuming a Missing at Random (MAR) mechanism. M-WSCQR employs covariate shift adjustment to account for the MAR mechanism, ensuring robust and reliable predictions with specified coverage guarantees. The proposed method exhibits double robustness and scalability, making it adaptable to a broad range of problem settings by modifying the weighting function or quantile regression model. Simulation studies conducted under varying missing rates, as well as both homoscedastic and heteroscedastic conditions, confirm the effectiveness and robustness of M-WSCQR. Additionally, its practical utility is demonstrated through analyses of two real-world datasets, highlighting its applicability in diverse inference contexts. In contrast to traditional methods that assume independent and identically distributed observations, M-WSCQR integrates information from both observed and missing groups. This approach accommodates covariate shifts arising from the MAR mechanism and differences in covariate distributions between observed and missing responses. By incorporating these considerations, M-WSCQR achieves enhanced robustness and versatility, offering a valuable solution for addressing missing data challenges in statistical modeling.},
  archive      = {J_SAC},
  author       = {Cao, Zhimiao and Zhang, Ce and Lian, Beibei and Jiang, Bei and Kong, Linglong and Yan, Xiaodong},
  doi          = {10.1007/s11222-025-10722-7},
  journal      = {Statistics and Computing},
  month        = {12},
  number       = {6},
  pages        = {1-14},
  shortjournal = {Stat. Comput.},
  title        = {Spilt conformal prediction with missing response},
  volume       = {35},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Measuring dependence between functional data via projection hilbert-schmidt covariance. <em>SAC</em>, <em>35</em>(6), 1-16. (<a href='https://doi.org/10.1007/s11222-025-10727-2'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Hilbert-Schmidt Independence Criteria is a well-known method for quantifying the dependence between two random vectors. However, it suffers from the curse of dimensionality. In this paper, we introduce a novel nonparametric independence test specifically designed for two functional random variables X and Y. The test is based on a new dependence metric, the so-called Projection Hilbert-Schmidt Covariance (PHSC), which efficiently characterizes the dependence of the random variables and improves upon the Hilbert-Schmidt Independence Criterion. The projection Hilbert-Schmidt covariance exhibits several favorable properties. It equals zero if and only if X and Y are independent, provided that the employed kernel is characteristic. It can be applied to random elements without finite moments when the employed kernel is bounded. It admits an U-statistic estimator, which facilitates the construction of our test. We construct a test based on the estimator and provide a theoretical critical value depending on the sample size and the bandwidths. Our analysis theoretically demonstrates the probabilities of the test committing two types of errors respectively, and proves its consistency under the local alternative hypothesis. Simulations and real data analysis show that the test based on the projection Hilbert-Schmidt covariance outperforms other competing tests for functional data.},
  archive      = {J_SAC},
  author       = {Tian, Zhentao and Wang, Darong and Zhang, Zhongzhan},
  doi          = {10.1007/s11222-025-10727-2},
  journal      = {Statistics and Computing},
  month        = {12},
  number       = {6},
  pages        = {1-16},
  shortjournal = {Stat. Comput.},
  title        = {Measuring dependence between functional data via projection hilbert-schmidt covariance},
  volume       = {35},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Significativity indices for agreement values. <em>SAC</em>, <em>35</em>(6), 1-18. (<a href='https://doi.org/10.1007/s11222-025-10728-1'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Agreement measures, such as Cohen’s kappa or intraclass correlation, gauge the matching between two or more classifiers. They are used in a wide range of contexts from medicine, where they evaluate the effectiveness of medical treatments and clinical trials, to artificial intelligence, where they can quantify the approximation due to the reduction of a classifier. The consistency of different classifiers to a gold standard can be compared simply by using the order induced by their agreement measure with respect to the gold standard itself. Nevertheless, labelling an approach as good or bad exclusively by using the value of an agreement measure requires a scale or a significativity index. Some quality scales have been proposed in the literature for Cohen’s kappa, but they are mainly naïve, and their boundaries are arbitrary. We propose a general approach to evaluate the significance of any agreement value between two classifiers as the probability of randomly choosing a confusion matrix, built over the same data set, with a lower agreement value. This measure, named significativity, gauges the relevance of the observed agreement value rather than replacing the agreement measure used to calculate it. This work introduces two significativity indices: one dealing with finite data sets, the other one handling classification probability distributions. This manuscript also addresses the computational challenges of evaluating such indices and proposes some efficient algorithms for their evaluation.},
  archive      = {J_SAC},
  author       = {Casagrande, Alberto and Fabris, Francesco and Girometti, Rossano and Pagliarini, Roberto},
  doi          = {10.1007/s11222-025-10728-1},
  journal      = {Statistics and Computing},
  month        = {12},
  number       = {6},
  pages        = {1-18},
  shortjournal = {Stat. Comput.},
  title        = {Significativity indices for agreement values},
  volume       = {35},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Zero-inflation in the multivariate poisson lognormal family. <em>SAC</em>, <em>35</em>(6), 1-22. (<a href='https://doi.org/10.1007/s11222-025-10729-0'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Analyzing high-dimensional count data is a challenge and statistical model-based approaches provide an adequate and efficient framework that preserves explainability. The (multivariate) Poisson-Log-Normal (PLN) model is one such model: it assumes count data are driven by an underlying structured latent Gaussian variable, so that the dependencies between counts solely stems from the latent dependencies. However PLN doesn’t account for zero-inflation, a feature frequently observed in real-world datasets. Here we introduce the Zero-Inflated PLN (ZIPLN) model, adding a multivariate zero-inflated component to the model, as an additional Bernoulli latent variable. The Zero-Inflation can be fixed, site-specific, feature-specific or depends on covariates. We estimate model parameters using variational inference that scales up to datasets with a few thousands variables and compare two approximations: (i) independent Gaussian and Bernoulli variational distributions or (ii) Gaussian variational distribution conditioned on the Bernoulli one. The method is assessed on synthetic data and the efficiency of ZIPLN is established even when zero-inflation concerns up to $$90\%$$ of the observed counts. We then apply both ZIPLN and PLN to a cow microbiome dataset, containing $$90.6\%$$ of zeroes. Accounting for zero-inflation significantly increases log-likelihood and reduces dispersion in the latent space, thus leading to improved group discrimination.},
  archive      = {J_SAC},
  author       = {Batardière, Bastien and Chiquet, Julien and Gindraud, François and Mariadassou, Mahendra},
  doi          = {10.1007/s11222-025-10729-0},
  journal      = {Statistics and Computing},
  month        = {12},
  number       = {6},
  pages        = {1-22},
  shortjournal = {Stat. Comput.},
  title        = {Zero-inflation in the multivariate poisson lognormal family},
  volume       = {35},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Statistical inference for matrix-vector linear regression without debiasing under kronecker covariance structure. <em>SAC</em>, <em>35</em>(6), 1-28. (<a href='https://doi.org/10.1007/s11222-025-10732-5'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we consider estimation and statistical inference for a mixed matrix-vector linear regression model by relaxing the independency entries restriction to a general Kronecker structure. A two-step estimation approach based on decorrelated score and rotation is proposed without additional debiasing procedure. In the first step, to deal with the high-dimensional nuisance matrix estimator, we construct an unbiased estimator for the vector parameter based on the decorrelated score function. In the second step, to achieve an unbiased and asymptotically normality estimator for the matrix estimator, we employ the sample-splitting and rotation-unbiasedness techniques. The Cramér-Rao lower bound of the proposed estimator for any linear function of matrix coefficient is attained. Simulation studies and an empirical analysis of Beijing air quality dataset demonstrate the superior performance of our proposed estimators.},
  archive      = {J_SAC},
  author       = {Ke, Baofang and Zhao, Weihua and Wang, Lei},
  doi          = {10.1007/s11222-025-10732-5},
  journal      = {Statistics and Computing},
  month        = {12},
  number       = {6},
  pages        = {1-28},
  shortjournal = {Stat. Comput.},
  title        = {Statistical inference for matrix-vector linear regression without debiasing under kronecker covariance structure},
  volume       = {35},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A multiscale method for data collected from network edges via the line graph. <em>SAC</em>, <em>35</em>(6), 1-28. (<a href='https://doi.org/10.1007/s11222-025-10733-4'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Data collected over networks can be modelled as noisy observations of an unknown function over the nodes of a graph or network structure, fully described by its nodes and their connections, the edges. In this context, function estimation has been proposed in the literature and typically makes use of the network topology such as relative node arrangement, often using given or artificially constructed node Euclidean coordinates. However, networks that arise in fields such as hydrology (for example, river networks) present features that challenge these established modelling setups since the target function may naturally live on edges (e.g., river flow) and/or the node-oriented modelling uses noisy edge data as weights. This work tackles these challenges and develops a novel lifting scheme along with its associated (second) generation wavelets that permit data decomposition across the network edges. The transform, which we refer to under the acronym LG-LOCAAT, makes use of a line graph construction that first maps the data in the line graph domain. We thoroughly investigate the proposed algorithm’s properties and illustrate its performance versus existing methodologies. We conclude with an application pertaining to hydrology that involves the denoising of a water quality index over the England river network, backed up by a simulation study for a river flow dataset.},
  archive      = {J_SAC},
  author       = {Cao, Dingjia and Knight, Marina I. and Nason, Guy P.},
  doi          = {10.1007/s11222-025-10733-4},
  journal      = {Statistics and Computing},
  month        = {12},
  number       = {6},
  pages        = {1-28},
  shortjournal = {Stat. Comput.},
  title        = {A multiscale method for data collected from network edges via the line graph},
  volume       = {35},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Heterogeneity-aware debiased machine learning for high-dimensional partially linear models. <em>SAC</em>, <em>35</em>(6), 1-24. (<a href='https://doi.org/10.1007/s11222-025-10737-0'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In practice, distributed data are often collected from different locations/times/populations /environments with non-negligible heterogeneity. In this paper, we consider inherently distributed data follow heterogeneous partially linear models with high-dimensional covariates, where each site involves a common parameter vector and site-specific nuisance functions. Three distributed estimators based on debiased machine learning are proposed to account for heterogeneity. The closed forms and asymptotic normal distributions of the proposed estimators have been explored and compared. Moreover, these estimators can be computed easily by transmitting some statistics from the local sites to the central site. The finite-sample performance is demonstrated through simulation studies and an application to Beijing multi-site air quality dataset is also provided.},
  archive      = {J_SAC},
  author       = {Wu, Yining and Wang, Lei and Lian, Heng},
  doi          = {10.1007/s11222-025-10737-0},
  journal      = {Statistics and Computing},
  month        = {12},
  number       = {6},
  pages        = {1-24},
  shortjournal = {Stat. Comput.},
  title        = {Heterogeneity-aware debiased machine learning for high-dimensional partially linear models},
  volume       = {35},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Adaptive generalized P-splines for functional data: A statistical framework via blockwise GSVD. <em>SAC</em>, <em>35</em>(6), 1-16. (<a href='https://doi.org/10.1007/s11222-025-10734-3'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this work, we introduce a novel approach for functional data approximation based on generalized P-splines with non-uniform and adaptively placed knots. The key innovation of our proposal is the integration of a conditioning-aware strategy for selecting both the number and positions of the knots, as well as the regularization parameters. By reformulating the Tikhonov regularization problem, we propose a computationally efficient criterion that controls model complexity while ensuring numerical stability. The resulting approximation framework not only improves the fit across the entire functional domain but also maintains compactness and robustness. Extensive numerical experiments conducted on both synthetic and real-world datasets demonstrate that our approach significantly outperforms traditional free knot and smoothing spline methods in terms of approximation error and conditioning.},
  archive      = {J_SAC},
  author       = {Magistris, Anna De and Romano, Elvira and Campagna, Rosanna},
  doi          = {10.1007/s11222-025-10734-3},
  journal      = {Statistics and Computing},
  month        = {12},
  number       = {6},
  pages        = {1-16},
  shortjournal = {Stat. Comput.},
  title        = {Adaptive generalized P-splines for functional data: A statistical framework via blockwise GSVD},
  volume       = {35},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Post-transfer learning statistical inference in high-dimensional regression. <em>SAC</em>, <em>35</em>(6), 1-23. (<a href='https://doi.org/10.1007/s11222-025-10738-z'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Transfer learning (TL) for high-dimensional regression (HDR) is an important problem in machine learning, particularly when dealing with limited sample size in the target task. However, there currently lacks a method to quantify the statistical significance of the relationship between features and the response in TL-HDR settings. In this paper, we introduce a novel statistical inference framework for assessing the reliability of feature selection in TL-HDR, called PTL-SI (Post-TL Statistical Inference). The core contribution of PTL-SI is its ability to provide valid p-values to features selected in TL-HDR, thereby rigorously controlling the false positive rate (FPR) at desired significance level $$\alpha $$ (e.g., 0.05). Furthermore, we enhance statistical power by incorporating a strategic divide-and-conquer approach into our framework. We demonstrate the validity and effectiveness of the proposed PTL-SI through extensive experiments on both synthetic and real-world high-dimensional datasets, confirming its theoretical properties and utility in testing the reliability of feature selection in TL scenarios.},
  archive      = {J_SAC},
  author       = {Tam, Nguyen Vu Khai and My, Cao Huyen and Le Duy, Vo Nguyen},
  doi          = {10.1007/s11222-025-10738-z},
  journal      = {Statistics and Computing},
  month        = {12},
  number       = {6},
  pages        = {1-23},
  shortjournal = {Stat. Comput.},
  title        = {Post-transfer learning statistical inference in high-dimensional regression},
  volume       = {35},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). On semi-parametric progressive censoring. <em>SAC</em>, <em>35</em>(6), 1-15. (<a href='https://doi.org/10.1007/s11222-025-10740-5'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article concerns classical and Bayesian inference of the unknown parameters of a semi-parametric proportional hazard’s model when the data are progressively censored. Wang et al. (2010) considered this problem for specific parametric proportional hazard’s model with Weibull, Lomax and Gompertz baseline distributions. The aim of this paper is not to assume any specific parametric family of distributions. Instead, it is assumed that the baseline distribution has a piecewise constant hazard function with a given number of cut points. It becomes more flexible than any specific parametric class of distribution functions. The maximum likelihood estimators of the unknown parameters can be obtained in closed form. Order restricted maximum likelihood estimators have also been proposed. A very flexible Dirichlet gamma priors has been assumed on the unknown parameters. Based on these priors, the Bayes estimates, ordered restricted Bayes estimates, and the associated credible intervals are also provided. Simulation experiments have been performed to illustrate the effectiveness of the proposed method. In practice, the cut points may not be known. The choice of the cut points is an important issue. We have discussed the choice of the cut points based on a non-homogeneous Poisson process model. Finally, two data sets are analyzed for illustrative purposes.},
  archive      = {J_SAC},
  author       = {Prajapati, Deepak and Kundu, Debasis},
  doi          = {10.1007/s11222-025-10740-5},
  journal      = {Statistics and Computing},
  month        = {12},
  number       = {6},
  pages        = {1-15},
  shortjournal = {Stat. Comput.},
  title        = {On semi-parametric progressive censoring},
  volume       = {35},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Two-metric variable scaled splitting algorithm for nonsmooth nonconvex sparsity-penalized quantile regression. <em>SAC</em>, <em>35</em>(6), 1-25. (<a href='https://doi.org/10.1007/s11222-025-10742-3'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The inherent nonsmoothness of quantile regression problems creates significant computational difficulties. These challenges are further exacerbated when an additional nonconvex and nonsmooth sparsity penalty is incorporated into the model. By leveraging the Moreau envelope method to smooth out inherent nonsmooth components, we derive a stepwise smooth approximation scheme tailored for nonsmooth nonconvex sparsity-penalized quantile regression. However, this smoothing process introduces severe ill-conditioning into the problem formulation. To resolve this issue and nonconvexity due to the folded concave penalty, we propose an innovative two-metric variable scaled splitting algorithm that synergizes second-order optimization with first-order information from nonconvex penalties. The algorithm comprises a forward step that uses a quasi-Newton strategy to modify the descent direction and a backward step that employs an adaptive weight-metric matrix for the construction of the proximal operator associated with the nonconvex penalties. Theoretical analysis demonstrates the global convergence properties of the algorithm. Numerical experiments show that the proposed method outperforms widely used language packages R in solving penalized quantile regression problems, achieving superior computational efficiency and enhanced variable selection capabilities.},
  archive      = {J_SAC},
  author       = {Yang, Fan and Wang, Shangfei and Shen, Zhengwei and Bao, Wenwen},
  doi          = {10.1007/s11222-025-10742-3},
  journal      = {Statistics and Computing},
  month        = {12},
  number       = {6},
  pages        = {1-25},
  shortjournal = {Stat. Comput.},
  title        = {Two-metric variable scaled splitting algorithm for nonsmooth nonconvex sparsity-penalized quantile regression},
  volume       = {35},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Hausdorff correlation for interval-valued random objects. <em>SAC</em>, <em>35</em>(6), 1-13. (<a href='https://doi.org/10.1007/s11222-025-10743-2'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Analyzing the correlation between interval-valued data presents an essential yet challenging problem in modern statistical research due to the lack of basic geometric and algebraic structures. Existing methods are often limited by their reliance on algebraic formulations or assumptions about the underlying distribution of true values within intervals. Moreover, they primarily focus on simple midpoint-range interval representations, restricting their applicability to more complex interval structures, e.g., when the interval contains multiple segments. To address these limitations, we introduce the Fréchet framework into the interval metric space equipped with the Hausdorff distance, extending the notions of Fréchet mean and proposing a more general and straightforward interval dependency measure, called Hausdorff correlation. The proposed method offers a strong geometric interpretation, revealing the relationship between random intervals and their Hausdorff mean, while also accommodating a broader range of interval forms. From a theoretical perspective, we establish the foundational properties of the proposed framework, proving the existence and uniqueness of the Hausdorff mean. Empirical evaluations on both synthetic and real-world datasets demonstrate the distinctiveness and effectiveness of Hausdorff correlation and its superior performance in feature selection compared to existing methods. In particular, a real-world Wearable Watch Dataset analysis shows the Hausdorff correlation successfully captures the relationship between multi-segment sleep intervals and physiological indicators, where existing methods fail to provide meaningful estimates.},
  archive      = {J_SAC},
  author       = {Kang, Xinlai and Ouyang, Xiaxue and Liang, Haoxian and Meng, Cheng},
  doi          = {10.1007/s11222-025-10743-2},
  journal      = {Statistics and Computing},
  month        = {12},
  number       = {6},
  pages        = {1-13},
  shortjournal = {Stat. Comput.},
  title        = {Hausdorff correlation for interval-valued random objects},
  volume       = {35},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). An algorithm aiming at unimodal density-based clustering using gaussian mixture models. <em>SAC</em>, <em>35</em>(6), 1-27. (<a href='https://doi.org/10.1007/s11222-025-10659-x'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We study the problem of unimodal density-based clustering based on Gaussian mixture models. In the proposed approach, clusters are thought of as regions of high probability density separated from other regions of low probability density, encouraging the creation of unimodal clusters. The problem of estimation of unimodal Gaussian mixtures has been solved only in the univariate case, while in this work we try to provide a solution for the multivariate setting. The unimodal density-based clustering works in two stages. First, a new merging algorithm based on the density definition of a cluster is used. This algorithm identifies which components should be merged in order to obtain a number of clusters less than or equal to the initial number of mixture components, on the basis of density similarities. Second, a penalized likelihood approach is adopted to induce unimodality in the merged set of components. We evaluate the performance of both methods on the basis of simulated samples and empirical applications.},
  archive      = {J_SAC},
  author       = {Tancini, Daniele and Scrucca, Luca and Bartolucci, Francesco},
  doi          = {10.1007/s11222-025-10659-x},
  journal      = {Statistics and Computing},
  month        = {12},
  number       = {6},
  pages        = {1-27},
  shortjournal = {Stat. Comput.},
  title        = {An algorithm aiming at unimodal density-based clustering using gaussian mixture models},
  volume       = {35},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). The effective number of parameters in kernel density estimation. <em>SAC</em>, <em>35</em>(6), 1-21. (<a href='https://doi.org/10.1007/s11222-025-10744-1'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We devise a new formula for measuring the effective degrees of freedom (EDoF) in kernel density estimation (KDE). Starting from the orthogonal polynomial sequence (OPS) expansion for the ratio of the empirical to the oracle density, we show how convolution with the kernel leads to a new OPS with respect to which one may express the resulting KDE. The expansion coefficients of the two OPS systems can then be related via a kernel sensitivity matrix, which leads to a natural oracle definition of EDoF through the trace operator. Asymptotic properties of the (empirical) plug-in EDoF are worked out through influence functions, and connections with other empirical EDoFs are established. Minimization of Kullback-Leibler divergence is investigated as an alternative to integrated squared error based bandwidth selection rules, yielding a new normal scale rule. The methodology, which arises from a proper oracle formulation and is not restricted to convolution kernels, suggests the possibility of a new bandwidth selection rule based on an information criterion such as AIC.},
  archive      = {J_SAC},
  author       = {Guglielmini, Sofia and Volobouev, Igor and Trindade, A. Alexandre},
  doi          = {10.1007/s11222-025-10744-1},
  journal      = {Statistics and Computing},
  month        = {12},
  number       = {6},
  pages        = {1-21},
  shortjournal = {Stat. Comput.},
  title        = {The effective number of parameters in kernel density estimation},
  volume       = {35},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Parallel selected inversion for space-time gaussian markov random fields. <em>SAC</em>, <em>35</em>(6), 1-16. (<a href='https://doi.org/10.1007/s11222-025-10747-y'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Performing Bayesian inference on large spatio-temporal models requires extracting inverse elements of large sparse precision matrices for marginal variances, as well as estimating model hyperparameters. Although direct matrix factorizations can be used for the inversion, such methods fail to scale well for distributed problems when run on large computing clusters. On the contrary, Krylov subspace methods for the selected inversion have been gaining traction. We propose a parallel hybrid approach based on domain decomposition, which extends the Rao-Blackwellized Monte Carlo estimator for distributed precision matrices. Our approach exploits the strength of Krylov subspace methods as global solvers and efficiency of direct factorizations as base case solvers to compute the marginal variances and the derivatives required for hyperparameter estimation using a divide-and-conquer strategy. By introducing subdomain overlaps, one can achieve greater accuracy at an increased computational effort with little to no additional communication. We demonstrate the speed improvements and efficient hyperparameter inference on both simulated models and a massive US daily temperature data.},
  archive      = {J_SAC},
  author       = {Zhumekenov, Abylay and Krainski, Elias T. and Rue, Håvard},
  doi          = {10.1007/s11222-025-10747-y},
  journal      = {Statistics and Computing},
  month        = {12},
  number       = {6},
  pages        = {1-16},
  shortjournal = {Stat. Comput.},
  title        = {Parallel selected inversion for space-time gaussian markov random fields},
  volume       = {35},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Bayesian parameter estimation for partially observed McKean-vlasov diffusions using multilevel markov chain monte carlo. <em>SAC</em>, <em>35</em>(6), 1-15. (<a href='https://doi.org/10.1007/s11222-025-10749-w'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article we consider Bayesian estimation of static parameters for a class of partially observed McKean-Vlasov diffusion processes with discrete-time observations over a fixed time interval. This problem features several obstacles to its solution, which include that the posterior density is numerically intractable in continuous-time, even if the transition probabilities are available and even when one uses a time-discretization, the posterior still cannot be used by adopting well-known computational methods such as Markov chain Monte Carlo (MCMC). In this paper, we provide a solution to this problem by using new MCMC algorithms which can solve the afore-mentioned issues. This MCMC algorithm is extended to use multilevel Monte Carlo (MLMC) methods. We prove convergence bounds on our parameter estimators and show that the MLMC-based MCMC algorithm reduces the computational cost to achieve a mean square error versus ordinary MCMC by an order of magnitude. We numerically illustrate our results on two models.},
  archive      = {J_SAC},
  author       = {JASRA, AJAY and WU, AMIN},
  doi          = {10.1007/s11222-025-10749-w},
  journal      = {Statistics and Computing},
  month        = {12},
  number       = {6},
  pages        = {1-15},
  shortjournal = {Stat. Comput.},
  title        = {Bayesian parameter estimation for partially observed McKean-vlasov diffusions using multilevel markov chain monte carlo},
  volume       = {35},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Ensemble prediction via covariate-dependent stacking. <em>SAC</em>, <em>35</em>(6), 1-14. (<a href='https://doi.org/10.1007/s11222-025-10739-y'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This study proposes a novel approach to ensemble prediction, called “covariate-dependent stacking” (CDST). Unlike traditional stacking and model averaging methods, CDST allows model weights to vary flexibly as a function of covariates, thereby enhancing predictive performance in complex scenarios. We formulate the covariate-dependent weights through combinations of basis functions and estimate them by optimizing cross-validation. To analyze the theoretical properties, we establish an oracle inequality regarding the expected loss to be minimized for estimating model weights. Through comprehensive simulation studies and an application to large-scale land price prediction, we demonstrate that the CDST consistently outperforms conventional model averaging methods, particularly on datasets where base models fail to capture the underlying complexity. Our findings suggest that the CDST is especially valuable for, but not limited to, spatio-temporal prediction problems, offering a powerful tool for researchers and practitioners across a wide spectrum of data analysis fields.},
  archive      = {J_SAC},
  author       = {Wakayama, Tomoya and Sugasawa, Shonosuke},
  doi          = {10.1007/s11222-025-10739-y},
  journal      = {Statistics and Computing},
  month        = {12},
  number       = {6},
  pages        = {1-14},
  shortjournal = {Stat. Comput.},
  title        = {Ensemble prediction via covariate-dependent stacking},
  volume       = {35},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A non-asymptotic error analysis for parallel monte carlo estimation from many short markov chains. <em>SAC</em>, <em>35</em>(6), 1-17. (<a href='https://doi.org/10.1007/s11222-025-10741-4'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Single-chain Markov chain Monte Carlo simulates realizations from a Markov chain to estimate expectations with the empirical average. The single-chain simulation is generally of considerable length and restricts many advantages of modern parallel computation. This paper constructs a novel many-short-chains Monte Carlo (MSC) estimator by averaging over multiple independent sums from Markov chains of a guaranteed short length. The computational advantage is the independent Markov chain simulations can be fast and may be run in parallel, but require a well-designed initial distribution constructed from importance sampling. Alternatively, the MSC estimator introduced here is a method to improve estimation properties in importance sampling by additionally simulating a Markov chain. A non-asymptotic error analysis is developed for the MSC estimator under both geometric and multiplicative drift conditions on the Markov chain that allows a theory for estimation of highly irregular and unbounded functions. Empirical performance is illustrated on an autoregressive process and the Pólya-Gamma Gibbs sampler for Bayesian logistic regression to predict cardiovascular disease.},
  archive      = {J_SAC},
  author       = {Brown, Austin},
  doi          = {10.1007/s11222-025-10741-4},
  journal      = {Statistics and Computing},
  month        = {12},
  number       = {6},
  pages        = {1-17},
  shortjournal = {Stat. Comput.},
  title        = {A non-asymptotic error analysis for parallel monte carlo estimation from many short markov chains},
  volume       = {35},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Efficient likelihood-based temporal changepoint detection in spatio-temporal processes. <em>SAC</em>, <em>35</em>(6), 1-16. (<a href='https://doi.org/10.1007/s11222-025-10745-0'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The rapid advancements of scalable methodologies have opened new avenues for analyzing complex spatio-temporal data, which is crucial in understanding dynamic environmental phenomena. This paper introduces a likelihood-based methodology for detecting abrupt changes in time in spatio-temporal processes, a field where traditional time series methods fall short. Unlike recent approaches, we do not make the unrealistic assumption that data is independent across changepoints. Instead, we use a recently proposed family of covariance models that allows nonstationarity in time, and we propose a Markov approximation to reduce the computational burden of calculating likelihoods under this model. We apply our method to two years of daily wind speed data from various synoptic weather stations in Ireland, identifying a significant changepoint on July 24, 2021, which aligns with a major shift in weather patterns. This application not only demonstrates the method’s utility in handling spatio-temporal datasets but also showcases its potential in broader environmental and climatic studies, offering a scalable solution for analyzing changing patterns in spatial data over time.},
  archive      = {J_SAC},
  author       = {Agarwal, Gaurav and Eckley, Idris A. and Fearnhead, Paul},
  doi          = {10.1007/s11222-025-10745-0},
  journal      = {Statistics and Computing},
  month        = {12},
  number       = {6},
  pages        = {1-16},
  shortjournal = {Stat. Comput.},
  title        = {Efficient likelihood-based temporal changepoint detection in spatio-temporal processes},
  volume       = {35},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Finding interpretable data pockets in tabular data. <em>SAC</em>, <em>35</em>(6), 1-21. (<a href='https://doi.org/10.1007/s11222-025-10746-z'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper develops a bump hunting method for discrete-valued tabular data where each bump is modeled by a rectangular region of the input data space so that its rule-based description admits a simple logical interpretation that can be used to make informed decisions. This method is designed to work with labeled data where each input feature has a separate and distinct meaning that may or may not be related to the output, and the goal is to find feature subsets that are related to the output, rectangular regions within these subset feature spaces, and pockets of data within these rectangular regions that simultaneously obey five properties: each rectangle is described by a small subset of input features, the pocket data occupies a local region of the subset-feature space (i.e. the input samples are all similar to one another in the reduced feature space), the input/output relationship for the pocket data is nearly pure (i.e. nearly all output values belong to a designated target set), the number of pocket data samples in each rectangle is both statistically significant and large enough to have relevant meaning for the end application, and the overlap between rectangles is minimal. In contrast to state-of-the-art methods that use decision trees or the PRIM algorithm, this new method is better at distinguishing multiple closely spaced bumps, better at representing non-rectangular shaped bumps that are formed by co-linear features, better at controlling the extent of the rectangles (to provide a simpler interpretation), and more robust against overfitting and the inclusion of spurious features that have no or little relation to the output.},
  archive      = {J_SAC},
  author       = {Ojha, Tushar and Hush, Don},
  doi          = {10.1007/s11222-025-10746-z},
  journal      = {Statistics and Computing},
  month        = {12},
  number       = {6},
  pages        = {1-21},
  shortjournal = {Stat. Comput.},
  title        = {Finding interpretable data pockets in tabular data},
  volume       = {35},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Approximate bayesian computation sequential monte carlo via random forests. <em>SAC</em>, <em>35</em>(6), 1-16. (<a href='https://doi.org/10.1007/s11222-025-10748-x'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Approximate Bayesian Computation (ABC) is a popular inference method when likelihoods are hard to come by. Practical bottlenecks of ABC applications include selecting statistics that summarize the data without losing too much information or introducing uncertainty, and choosing distance functions and tolerance thresholds that balance accuracy and computational efficiency. Recent studies have shown that ABC methods using random forest (RF) methodology perform well while circumventing many of ABC’s drawbacks. However, RF construction is computationally expensive for large numbers of trees and model simulations, and there can be high uncertainty in the posterior if the prior distribution is uninformative. Here we further adapt random forests to the ABC setting in two ways. The first exploits distributional random forests to provide a direct method for inferring the joint posterior distribution of parameters of interest, while the second describes a sequential Monte Carlo approach which updates the prior distribution iteratively to focus on the most likely regions in the parameter space. We show that the new methods can accurately infer posterior distributions for a wide range of deterministic and stochastic models in different scientific areas.},
  archive      = {J_SAC},
  author       = {Dinh, Khanh N. and Liu, Cécile and Xiang, Zijin and Liu, Zhihan and Tavaré, Simon},
  doi          = {10.1007/s11222-025-10748-x},
  journal      = {Statistics and Computing},
  month        = {12},
  number       = {6},
  pages        = {1-16},
  shortjournal = {Stat. Comput.},
  title        = {Approximate bayesian computation sequential monte carlo via random forests},
  volume       = {35},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Current tendencies in free-running oscillators: A review. <em>SAC</em>, <em>35</em>(6), 1-20. (<a href='https://doi.org/10.1007/s11222-025-10750-3'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The use of random numbers, or at least sequences with particular stochastic properties, is crucial in several disciplines. The literature classifies these sequences into two main classes: true random numbers and pseudorandom numbers. The process of generating each category requires different methodologies adapted to their intrinsic properties. This study focuses on the generation of true random numbers, specifically by exploiting free-running oscillators as a source of entropy. A comprehensive analysis of different generation techniques is presented, with special emphasis on the validation and verification procedures used to assess the statistical robustness and unpredictability of the generated sequences.},
  archive      = {J_SAC},
  author       = {Almaraz Luengo, Elena},
  doi          = {10.1007/s11222-025-10750-3},
  journal      = {Statistics and Computing},
  month        = {12},
  number       = {6},
  pages        = {1-20},
  shortjournal = {Stat. Comput.},
  title        = {Current tendencies in free-running oscillators: A review},
  volume       = {35},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Distance-based CUSUM statistics for high dimensional change points. <em>SAC</em>, <em>35</em>(6), 1-19. (<a href='https://doi.org/10.1007/s11222-025-10752-1'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We introduce distance-based CUSUM statistics for detecting change points in high dimensional data streams. Unlike the standard CUSUM statistic which can mainly detect linear changes such as shifts in the mean of observations, the distance-based CUSUM statistics are constructed based on pairwise dissimilarity distances between observations and hence capable of detecting more general types of change points including linear and non-linear changes in a data stream, such as changes in the mean, variance, correlation, or other changes in the shape of distribution over time. Moreover, the distance-based CUSUM method is particularly useful for high dimensional low sample size (HDLSS) data in which the number of observations is very small but the dimension is very large. Detecting change points in such high dimensional data is an understudied problem. We study the properties of our proposed distance-based CUSUM statistic and use it to develop a non-parametric test to determine statistical significance of the estimated change point locations. Our approach does not require normality or any other distribution for the data. We provide theoretical guarantees for our method and demonstrate its empirical performance in comparison with some of the recent methods via extensive simulation studies and two real data applications. We provide an R package called distCUSUM for implementation of the proposed method.},
  archive      = {J_SAC},
  author       = {Zhang, Lupeng and Drikvandi, Reza},
  doi          = {10.1007/s11222-025-10752-1},
  journal      = {Statistics and Computing},
  month        = {12},
  number       = {6},
  pages        = {1-19},
  shortjournal = {Stat. Comput.},
  title        = {Distance-based CUSUM statistics for high dimensional change points},
  volume       = {35},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Rank test of independence based on adaptive hellinger distance. <em>SAC</em>, <em>35</em>(6), 1-22. (<a href='https://doi.org/10.1007/s11222-025-10757-w'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a novel independence measure for two random vectors, termed the Adaptive Hellinger Distance (AHD). This measure employs an innovative variant of the Hellinger distance (HD) and integrates methodological insights from the HHG test (introduced in Heller et al. (2012)). The proposed AHD test can be regarded as a crucial extension of the HHG test, as it shares a similar analytical framework while addressing certain limitations. Specifically, like the HHG test, it can detect dependence between two random vectors in any dimension, regardless of whether they are continuous or discrete. The rank test based on AHD performs exceptionally well and demonstrates robustness in applications. Furthermore, we have developed two tests utilizing p-value combination methods, named AHD-Simes and AHD-CCT. These methods are designed to integrate the results of tests conducted on four bins, ultimately enhancing overall performance. Simulation studies and real data analysis simultaneously illustrate the power and the utility of the proposed method, which is highly competitive especially when dealing with random vectors that are heavy-tailed or have outliers. We applied the proposed method to analyze two real datasets with different data types. Additionally, a new R package named AHD is developed to implement the proposed method.},
  archive      = {J_SAC},
  author       = {Guo, Wenwen and Shang, Shilin and Wu, Jiujing},
  doi          = {10.1007/s11222-025-10757-w},
  journal      = {Statistics and Computing},
  month        = {12},
  number       = {6},
  pages        = {1-22},
  shortjournal = {Stat. Comput.},
  title        = {Rank test of independence based on adaptive hellinger distance},
  volume       = {35},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Identifying collapsible sets in directed graphical models via inducing paths. <em>SAC</em>, <em>35</em>(5), 1-13. (<a href='https://doi.org/10.1007/s11222-025-10653-3'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A graphical model is said to be collapsible onto a subset if the implied model for the marginal distributions over the subset is equal to the model given by the related induced subgraph. In this paper, we extend the concepts of t-removable and c-removable vertices, proposed by Xie and Geng (2009), to t-removable and c-removable sets in terms of inducing paths. We show that, under certain conditions, these extensions are equivalent, respectively, to model collapsibility and estimate collapsibility for directed graphical models. We provide polynomial time algorithms with respect to the number of variables to determine whether a given set of variables is removable. The code for this study is available at https://github.com/Jamyang-D/removable-sets-check .},
  archive      = {J_SAC},
  author       = {Deng, Yuxin and Sun, Yi and Liu, Huaxiong},
  doi          = {10.1007/s11222-025-10653-3},
  journal      = {Statistics and Computing},
  month        = {10},
  number       = {5},
  pages        = {1-13},
  shortjournal = {Stat. Comput.},
  title        = {Identifying collapsible sets in directed graphical models via inducing paths},
  volume       = {35},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Information preservation with wasserstein autoencoders: Generation consistency and adversarial robustness. <em>SAC</em>, <em>35</em>(5), 1-27. (<a href='https://doi.org/10.1007/s11222-025-10657-z'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Amongst the numerous variants Variational Autoencoder (VAE) has inspired, the Wasserstein Autoencoder (WAE) stands out due to its heightened generative quality and intriguing theoretical properties. WAEs consist of an encoding and a decoding network— forming a bottleneck— with the prime objective of generating new samples resembling the ones it was catered to. In the process, they aim to achieve a target latent representation of the encoded data. Our work offers a comprehensive theoretical understanding of the machinery behind WAEs. From a statistical viewpoint, we pose the problem as concurrent density estimation tasks based on neural network-induced transformations. This allows us to establish deterministic upper bounds on the realized errors WAEs commit, supported by simulations on real and synthetic data sets. We also analyze the propagation of these stochastic errors in the presence of adversaries. As a result, both the large sample properties of the reconstructed distribution and the resilience of WAE models are explored.},
  archive      = {J_SAC},
  author       = {Chakrabarty, Anish and Basu, Arkaprabha and Das, Swagatam},
  doi          = {10.1007/s11222-025-10657-z},
  journal      = {Statistics and Computing},
  month        = {10},
  number       = {5},
  pages        = {1-27},
  shortjournal = {Stat. Comput.},
  title        = {Information preservation with wasserstein autoencoders: Generation consistency and adversarial robustness},
  volume       = {35},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Feature splitting parallel algorithm for dantzig selectors. <em>SAC</em>, <em>35</em>(5), 1-26. (<a href='https://doi.org/10.1007/s11222-025-10658-y'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Dantzig selector is a widely used and effective method for variable selection in ultra-high-dimensional data. Feature splitting is an efficient processing technique that involves dividing these ultra-high-dimensional variable datasets into manageable subsets that can be stored and processed more easily on a single machine. This paper proposes a variable splitting parallel algorithm for solving both convex and nonconvex Dantzig selectors based on the proximal point algorithm. The primary advantage of our parallel algorithm, compared to existing parallel approaches, is the significantly reduced number of iteration variables, which greatly enhances computational efficiency and accelerates the convergence speed of the algorithm. Furthermore, we show that our solution remains unchanged regardless of how the data is partitioned, a property referred to as partition-insensitive. In theory, we use a concise proof framework to demonstrate that the algorithm exhibits linear convergence. Numerical experiments indicate that our algorithm performs competitively in both parallel and nonparallel environments. The R package for implementing the proposed algorithm can be obtained at https://github.com/xfwu1016/PPADS .},
  archive      = {J_SAC},
  author       = {Wu, Xiaofei and Chao, Yue and Liang, Rongmei and Tang, Shi and Zhang, Zhimin},
  doi          = {10.1007/s11222-025-10658-y},
  journal      = {Statistics and Computing},
  month        = {10},
  number       = {5},
  pages        = {1-26},
  shortjournal = {Stat. Comput.},
  title        = {Feature splitting parallel algorithm for dantzig selectors},
  volume       = {35},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Fast bayesian inference in a class of sparse linear mixed effects models. <em>SAC</em>, <em>35</em>(5), 1-18. (<a href='https://doi.org/10.1007/s11222-025-10628-4'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Linear mixed effects models are widely used in statistical modelling. We consider a mixed effects model with Bayesian variable selection in the random effects using spike-and-slab priors and develop a optimisation-based inference schemes that can be applied to large data sets. An EM algorithm is proposed for the model with normal errors where the posterior distribution of the variable inclusion parameters is approximated using an Occam’s window approach. Placing this approach within a variational Bayes scheme allows the algorithm to be extended to the model with skew-t errors. The performance of the algorithm is evaluated in a simulation study and applied to a longitudinal model for elite athlete performance in 100 metres track sprinting and weightlifting.},
  archive      = {J_SAC},
  author       = {Spyropoulou, Maria-Zafeiria and Hopker, James G. and Griffin, Jim E.},
  doi          = {10.1007/s11222-025-10628-4},
  journal      = {Statistics and Computing},
  month        = {10},
  number       = {5},
  pages        = {1-18},
  shortjournal = {Stat. Comput.},
  title        = {Fast bayesian inference in a class of sparse linear mixed effects models},
  volume       = {35},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Data-adaptive structural change-point detection via isolation. <em>SAC</em>, <em>35</em>(5), 1-33. (<a href='https://doi.org/10.1007/s11222-025-10643-5'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, a new data-adaptive method, called DAIS (Data Adaptive ISolation), is introduced for the estimation of the number and the location of change-points in a given data sequence. The proposed method can detect changes in various different signal structures; we focus on the examples of piecewise-constant and continuous, piecewise-linear signals. The novelty of the proposed algorithm comes from the data-adaptive nature of the methodology. At each step, and for the data under consideration, we search for the most prominent change-point in a targeted neighborhood of the data sequence that contains this change-point with high probability. Using a suitably chosen contrast function, the change-point will then get detected after being isolated in an interval. The isolation feature enhances estimation accuracy, while the data-adaptive nature of DAIS is advantageous regarding, mainly, computational complexity. The methodology can be applied to both univariate and multivariate signals. The simulation results presented indicate that DAIS is at least as accurate as state-of-the-art competitors and in many cases significantly less computationally expensive.},
  archive      = {J_SAC},
  author       = {Anastasiou, Andreas and Loizidou, Sophia},
  doi          = {10.1007/s11222-025-10643-5},
  journal      = {Statistics and Computing},
  month        = {10},
  number       = {5},
  pages        = {1-33},
  shortjournal = {Stat. Comput.},
  title        = {Data-adaptive structural change-point detection via isolation},
  volume       = {35},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). DGP-LVM: Derivative gaussian process latent variable models. <em>SAC</em>, <em>35</em>(5), 1-27. (<a href='https://doi.org/10.1007/s11222-025-10644-4'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We develop a framework for derivative Gaussian process latent variable models (DGP-LVMs) that can handle multi-dimensional output data using modified derivative covariance functions. The modifications account for complexities in the underlying data generating process such as scaled derivatives, varying information across multiple output dimensions as well as interactions between outputs. Further, our framework provides uncertainty estimates for each latent variable samples using Bayesian inference. Through extensive simulations, we demonstrate that latent variable estimation accuracy can be drastically increased by including derivative information due to our proposed covariance function modifications. The developments are motivated by a concrete biological research problem involving the estimation of the unobserved cellular ordering from single-cell RNA (scRNA) sequencing data for gene expression and its corresponding derivative information known as RNA velocity. Since the RNA velocity is only an estimate of the exact derivative information, the derivative covariance functions need to account for potential scale differences. In a real-world case study, we illustrate the application of DGP-LVMs to such scRNA sequencing data. While motivated by this biological problem, our framework is generally applicable to all kinds of latent variable estimation problems involving derivative information irrespective of the field of study.},
  archive      = {J_SAC},
  author       = {Mukherjee, Soham and Claassen, Manfred and Bürkner, Paul-Christian},
  doi          = {10.1007/s11222-025-10644-4},
  journal      = {Statistics and Computing},
  month        = {10},
  number       = {5},
  pages        = {1-27},
  shortjournal = {Stat. Comput.},
  title        = {DGP-LVM: Derivative gaussian process latent variable models},
  volume       = {35},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Bayesian shared parameter joint models for heterogeneous populations. <em>SAC</em>, <em>35</em>(5), 1-17. (<a href='https://doi.org/10.1007/s11222-025-10647-1'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Joint models (JMs) for longitudinal and time-to-event data are an important class of biostatistical models in health and medical research. When the study population consists of heterogeneous subgroups, standard JMs may be inadequate, leading to misleading results or loss of information. Joint latent class models (JLCMs) and their variants have been proposed to incorporate latent class structures into JMs. JLCMs are useful for identifying latent subgroups, uncovering deeper insights into relationships between the outcomes, and improving prediction performance. We consider the problem of Bayesian inference for the generic form of JLCMs, which poses significant computational challenges due to the complex nature of the posterior distribution. We propose a new Bayesian inference framework to tackle these challenges. Our approach leverages state-of-the-art Markov chain Monte Carlo techniques and parallel computing for parameter estimation and model selection regarding the number of latent classes. Through a simulation study, we demonstrate the feasibility and superiority of our proposed method over the existing approach. Additionally, we provide practical guidance on model and prior specification, which has received little attention, to facilitate the implementation of such complex models. We illustrate our method using data from the PAQUID prospective cohort study, where the outcomes of interest include a longitudinal measurement of cognitive performance and time to dementia diagnosis. Our analysis provides deeper insights into the latent class characteristics underlying the study population.},
  archive      = {J_SAC},
  author       = {Chen, Sida and Alvares, Danilo and Palma, Marco and Barrett, Jessica K.},
  doi          = {10.1007/s11222-025-10647-1},
  journal      = {Statistics and Computing},
  month        = {10},
  number       = {5},
  pages        = {1-17},
  shortjournal = {Stat. Comput.},
  title        = {Bayesian shared parameter joint models for heterogeneous populations},
  volume       = {35},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Unbalanced longitudinal data clustering with a copula kernel mixture model. <em>SAC</em>, <em>35</em>(5), 1-26. (<a href='https://doi.org/10.1007/s11222-025-10650-6'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Unbalanced longitudinal data appears commonly in practice, for example in cases where measurements are collected at different time points for different subjects and can therefore be sparse and/or irregularly sampled. Treating such data as functional enables smooth curve estimation and better handling of missing or irregularly spaced observations. Therefore, a Gaussian copula kernel mixture model (CKMM), based on functional data analysis, is proposed for clustering unbalanced multivariate longitudinal data. In this model, subject-specific warping matrices are included to account for irregularly spaced observations. A regularized functional eigen-decomposition is employed to estimate the copula correlation parameters, ensuring the smoothing procedure is integrated into clustering. Additionally, a functional gradient descent algorithm is implemented as an alternative to kernel density estimation to reduce computational complexity. An expectation-maximization-like algorithm is proposed to estimate marginal distributions, copula parameters, eigenfunctions, and eigenvalues in the CKMM. The performance of the CKMM is demonstrated through a simulation study and a data application. The proposed model exhibits superior performance compared to k-means with dynamic time warping, the growth mixture model, and functional high-dimensional data clustering.},
  archive      = {J_SAC},
  author       = {Zhang, Xi and Murphy, Orla A. and McNicholas, Paul D.},
  doi          = {10.1007/s11222-025-10650-6},
  journal      = {Statistics and Computing},
  month        = {10},
  number       = {5},
  pages        = {1-26},
  shortjournal = {Stat. Comput.},
  title        = {Unbalanced longitudinal data clustering with a copula kernel mixture model},
  volume       = {35},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Annealing strategies for variance reduction in balance heuristic estimators. <em>SAC</em>, <em>35</em>(5), 1-18. (<a href='https://doi.org/10.1007/s11222-025-10651-5'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The computation of normalisation constants, or marginal likelihoods, is essential in Bayesian inference since they provide information about model adequacy and facilitate model comparison. While importance sampling offers unbiased estimates, its naive implementations often suffer from high variance in complex models. Our work makes the following contributions to multiple importance sampling methodology. First, we demonstrate that the balance heuristic estimator with stochastically selected proposals maintains statistical efficiency while reducing computational costs compared to Rao-Blackwellised alternatives. Second, we introduce a novel extended-space representation for the balance heuristic that enables the incorporation of annealing steps, which is essential for variance reduction. Third, we propose a resampling scheme tailored to this extended-space framework that mitigates the curse of dimensionality while preserving unbiasedness. We validate our framework through numerical experiments, demonstrating its efficiency and robustness in complex inference problems.},
  archive      = {J_SAC},
  author       = {Medina-Aguayo, Felipe J. and Everitt, Richard G.},
  doi          = {10.1007/s11222-025-10651-5},
  journal      = {Statistics and Computing},
  month        = {10},
  number       = {5},
  pages        = {1-18},
  shortjournal = {Stat. Comput.},
  title        = {Annealing strategies for variance reduction in balance heuristic estimators},
  volume       = {35},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Sampling from conditional distributions of simplified vines. <em>SAC</em>, <em>35</em>(5), 1-30. (<a href='https://doi.org/10.1007/s11222-025-10652-4'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Simplified vine copulas are flexible tools over standard multivariate distributions for modeling and understanding different dependence properties in high-dimensional data. Their conditional distributions are of utmost importance, from statistical learning to graphical models. However, the conditional densities of vine copulas and, thus, vine distributions cannot be obtained in closed form without integration for all possible sets of conditioning variables. We propose a Markov chain Monte Carlo based approach of using Hamiltonian Monte Carlo to sample from any conditional distribution of arbitrarily specified simplified vine copulas and thus vine distributions. We show its accuracy through simulation studies and analyze data of multiple maize traits such as flowering times, plant height, and vigor. Use cases from predicting traits to estimating conditional Kendall’s tau are presented.},
  archive      = {J_SAC},
  author       = {Hanebeck, Ariane and Şahin, Özge and Havlíčková, Petra and Czado, Claudia},
  doi          = {10.1007/s11222-025-10652-4},
  journal      = {Statistics and Computing},
  month        = {10},
  number       = {5},
  pages        = {1-30},
  shortjournal = {Stat. Comput.},
  title        = {Sampling from conditional distributions of simplified vines},
  volume       = {35},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Bayesian model selection for analyzing predictor-dependent directional data. <em>SAC</em>, <em>35</em>(5), 1-16. (<a href='https://doi.org/10.1007/s11222-025-10655-1'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The need for models for directional data is increasing, driven primarily by the necessity of analyzing peak hours in 24-hour services. Motivated by the need to analyze demand data for a 24-hour bike rental service in Seoul and the factors influencing demand fluctuations across distinct hours, we develop a Bayesian nonparametric density regression modeling framework for the case of a circular response and linear covariates, allowing model selection. Our proposal is based on a linear dependent Dirichlet process mixture of projected normal distributions, accommodating asymmetrical and multimodal shapes, in conjunction with discrete spike-and-slab priors, to enable model selection. A further advantage of our approach is that it enables model averaging, thereby properly accounting for model uncertainty. The simulation study shows that, across various scenarios, our model (i) successfully recovers the true functional form of the conditional density and (ii) selects the correct model, with accuracy improving as the sample size increases. The application of our method suggests that weather conditions significantly impact bike demand. The approach also allows us to predict peak rental times, revealing that, for instance, on a typical summer day, bike demand decreases between 8 am and 4 pm, while in winter, it drops during the early morning.},
  archive      = {J_SAC},
  author       = {Guevara, Ingrid and Inacio, Vanda and Gutiérrez, Luis},
  doi          = {10.1007/s11222-025-10655-1},
  journal      = {Statistics and Computing},
  month        = {10},
  number       = {5},
  pages        = {1-16},
  shortjournal = {Stat. Comput.},
  title        = {Bayesian model selection for analyzing predictor-dependent directional data},
  volume       = {35},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Spectral clustering on aggregated multilayer networks with covariates. <em>SAC</em>, <em>35</em>(5), 1-15. (<a href='https://doi.org/10.1007/s11222-025-10661-3'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The community detection problem on multilayer networks have drawn much interest. When the nodal covariates are also present, few work has been done to integrate information from both sources. To leverage the multilayer networks and the covariates, we propose two new algorithms: the spectral clustering on aggregated networks with covariates (SCANC), and the spectral clustering on aggregated Laplacian with covariates (SCALC). These two algorithms are easy to implement, computationally fast, and feature a data-driven approach for tuning parameter selection. We establish theoretical guarantees for both methods under the Multilayer Stochastic Blockmodel with Covariates (MSBM-C), demonstrating their consistency in recovering community structure. Our analysis reveals that increasing the number of layers, incorporating covariate information, and enhancing network density all contribute to improved clustering accuracy. Notably, SCANC is most effective when all layers exhibit similar assortativity, whereas SCALC performs better when both assortative and disassortative layers are present. On the simulation studies and a primary school contact data analysis, our method outperforms other methods. Our results highlight the advantages of spectral-based aggregation techniques in leveraging both network structure and nodal attributes for robust community detection.},
  archive      = {J_SAC},
  author       = {Zhao, Da and Wang, Wanjie and Li, Jialiang},
  doi          = {10.1007/s11222-025-10661-3},
  journal      = {Statistics and Computing},
  month        = {10},
  number       = {5},
  pages        = {1-15},
  shortjournal = {Stat. Comput.},
  title        = {Spectral clustering on aggregated multilayer networks with covariates},
  volume       = {35},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Wasserstein k-centers clustering for distributional data. <em>SAC</em>, <em>35</em>(5), 1-17. (<a href='https://doi.org/10.1007/s11222-025-10662-2'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We develop a novel clustering method for distributional data, where each data point is regarded as a probability distribution on the real line. For distributional data, it has been challenging to develop a clustering method that utilizes modes of variation of the data because the space of probability distributions lacks a vector space structure, preventing the application of existing methods devised for functional data. Our clustering method for distributional data takes account of the differences in both means and modes of variation of clusters, in the spirit of the k-centers clustering approach proposed for functional data. Specifically, we consider the space of distributions equipped with the Wasserstein metric and define geodesic modes of variation of distributional data using the notion of geodesic principal component analysis. Then, we utilize geodesic modes of clusters to predict the cluster membership of each distribution. We theoretically show the validity of the proposed clustering criterion by studying the probability of correct membership. Through a simulation study and real data application, we demonstrate that the proposed distributional clustering method can improve the quality of the cluster compared to conventional clustering algorithms.},
  archive      = {J_SAC},
  author       = {Okano, Ryo and Imaizumi, Masaaki},
  doi          = {10.1007/s11222-025-10662-2},
  journal      = {Statistics and Computing},
  month        = {10},
  number       = {5},
  pages        = {1-17},
  shortjournal = {Stat. Comput.},
  title        = {Wasserstein k-centers clustering for distributional data},
  volume       = {35},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Sparse estimation and inference for prediction-powered semi-supervised linear regression. <em>SAC</em>, <em>35</em>(5), 1-14. (<a href='https://doi.org/10.1007/s11222-025-10663-1'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Semi-supervised learning has become increasingly prevalent in recent years. In the semi-supervised setting, most of the data are unlabeled since the acquisition of high-quality labels requires expensive scientific measurements and/or laborious human labeling. Hence, it is common to employ some black-box machine learning methods as predictive models to generate outcomes on unlabeled data for subsequent statistical inference. In this paper, we consider sparse regression in semi-supervised setting with prediction assistance. Although the predictions may be imperfect and/or noisy, the empirical risk based on a rectified loss function, is unbiased with respect to the population risk, thereby yielding a relatively safe imputation strategy. Under some regularity conditions, the near-optimal statistical rate is established. It is interesting to derive the prediction error bound on the unlabeled data, rather than the labeled data. More importantly, the asymptotic normality and confidence intervals are also studied via debiasing strategy. With mild conditions on predictive models, it is shown that our proposed method, integrating unlabeled data for combined analysis, is asymptotically more efficient than supervised sparse regression. Both numerical evidence and real-world data analysis demonstrate the effectiveness of our method.},
  archive      = {J_SAC},
  author       = {Song, Zihao and Liu, Jicai and Wang, Lei and Lian, Heng and Zhao, Weihua},
  doi          = {10.1007/s11222-025-10663-1},
  journal      = {Statistics and Computing},
  month        = {10},
  number       = {5},
  pages        = {1-14},
  shortjournal = {Stat. Comput.},
  title        = {Sparse estimation and inference for prediction-powered semi-supervised linear regression},
  volume       = {35},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Detecting structural shifts and estimating single change-points in interval-based time series. <em>SAC</em>, <em>35</em>(5), 1-21. (<a href='https://doi.org/10.1007/s11222-025-10666-y'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper addresses the open problem of conducting single change-point analysis for interval-valued time series data using the maximum likelihood estimation (MLE) framework. Motivated by financial time series, we analyze data that includes daily opening (O), up (U), low (L), and closing (C) values, rather than just a closing value as traditionally used. To tackle this, we propose a fundamental model based on stochastic differential equations, which also serves as a transformation of other widely used models, such as the log-transformed geometric Brownian motion model. We derive the joint distribution for these interval-valued observations using the reflection principle and Girsanov’s theorem. The MLE is obtained by optimizing the log-likelihood function through first and second-order derivative calculations, utilizing the Newton-Raphson algorithm. We further propose a novel parametric bootstrap method to compute confidence intervals, addressing challenges related to temporal dependency and interval-based data relationships. The performance of the model is evaluated through extensive simulations and real data analysis using S&P500 returns during the 2022 Russo-Ukrainian War. The results demonstrate that the proposed OULC model consistently outperforms the traditional OC model, offering more accurate and reliable change-point detection and parameter estimates.},
  archive      = {J_SAC},
  author       = {Sun, Li-Hsien and Huang, Zong-Yuan and Chiu, Chi-Yang and Ning, Ning},
  doi          = {10.1007/s11222-025-10666-y},
  journal      = {Statistics and Computing},
  month        = {10},
  number       = {5},
  pages        = {1-21},
  shortjournal = {Stat. Comput.},
  title        = {Detecting structural shifts and estimating single change-points in interval-based time series},
  volume       = {35},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Pseudo-R2D2 prior for high-dimensional ordinal regression. <em>SAC</em>, <em>35</em>(5), 1-13. (<a href='https://doi.org/10.1007/s11222-025-10667-x'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Ordinal regression with a high-dimensional covariate space has many important application areas including gene expression studies. The lack of an intrinsic numeric value associated with ordinal responses, however, makes methods based on continuous data, like linear regression, inappropriate. In this work, we extend the R2D2 prior framework to the high-dimensional ordinal setting. Since the $$R^2$$ definition used in the original R2D2 prior relies on means and variances, it cannot be used for ordinal regression as these two quantities are not suitable for such data. Instead, by simulating data and using McFadden’s coefficient-of-determination ( $$R^2_M$$ ), we show that a generalized inverse Gaussian prior distribution on the global variance parameter approximately induces a beta prior distribution on $$R^2_M$$ . The proposed prior can be implemented in Stan and an $$\texttt {R}$$ package is also developed. Our method demonstrates excellent coefficient estimation and variable selection properties on simulated data, and yields accurate predictions when applied to a liver tissue gene expression dataset.},
  archive      = {J_SAC},
  author       = {Yanchenko, Eric},
  doi          = {10.1007/s11222-025-10667-x},
  journal      = {Statistics and Computing},
  month        = {10},
  number       = {5},
  pages        = {1-13},
  shortjournal = {Stat. Comput.},
  title        = {Pseudo-R2D2 prior for high-dimensional ordinal regression},
  volume       = {35},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Variational bayesian inference for models with nuisance parameters and an intractable likelihood. <em>SAC</em>, <em>35</em>(5), 1-14. (<a href='https://doi.org/10.1007/s11222-025-10654-2'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A primary challenge in Bayesian analysis lies in computing the posterior distribution of model parameters, especially for models with a large number of parameters or intractable likelihoods. Often, the focus is on a subset of parameters, with the remainder regarded as nuisance parameters introduced for computational convenience. This complexity necessitates refined computational methods. Variational Bayesian inference (VB) has emerged as a powerful solution, enhancing computational efficiency by recasting inference as an optimization problem within a family of tractable distributions. However, common VB techniques sometimes fall short, especially for models with nuisance parameters or intractable likelihoods. After identifying characteristics of suboptimal VB methods, we build upon the Hybrid Variational Bayes (HVB) approach introduced by Loaiza-Maya et al. (2022) and develop an extended and unified HVB framework designed to achieve more precise Bayesian inference in such scenarios. Through theoretical exploration and a series of illustrative examples, our approach demonstrates notable improvements over traditional VB methods.},
  archive      = {J_SAC},
  author       = {Tseng, Y. H. Paco and Tran, Minh-Ngoc and Kohn, Robert},
  doi          = {10.1007/s11222-025-10654-2},
  journal      = {Statistics and Computing},
  month        = {10},
  number       = {5},
  pages        = {1-14},
  shortjournal = {Stat. Comput.},
  title        = {Variational bayesian inference for models with nuisance parameters and an intractable likelihood},
  volume       = {35},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Optimal response-free poisson subsampling for generalized linear models in increasing dimension. <em>SAC</em>, <em>35</em>(5), 1-27. (<a href='https://doi.org/10.1007/s11222-025-10660-4'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Morden data with considerable size and large dimension are widely encountered in practice. Moreover, the responses of massive data are sometimes hard to obtain due to high cost and privacy protection. For generalized linear models (GLMs) in increasing dimension, this paper investigates optimal response-free Poisson subsampling. We first propose an inverse probability weighted subsample estimator and theoretical results under unconditional framework are established. The optimal response-free probabilities are derived through L-optimality criterion for increasing dimension, and a two-step algorithm is considered to meet practical needs. To further enhance the estimation efficiency, a more efficient unweighted estimator is constructed based on the optimal subsample. The satisfactory performance of our proposed subsample estimators are illustrated by both simulation studies and two real world applications.},
  archive      = {J_SAC},
  author       = {Shan, Junhao and Wang, Lei},
  doi          = {10.1007/s11222-025-10660-4},
  journal      = {Statistics and Computing},
  month        = {10},
  number       = {5},
  pages        = {1-27},
  shortjournal = {Stat. Comput.},
  title        = {Optimal response-free poisson subsampling for generalized linear models in increasing dimension},
  volume       = {35},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Expert-elicitation method for non-parametric joint priors using normalizing flows. <em>SAC</em>, <em>35</em>(5), 1-33. (<a href='https://doi.org/10.1007/s11222-025-10665-z'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose an expert-elicitation method for learning non-parametric joint prior distributions using normalizing flows. Normalizing flows are a class of generative models that enable exact, single-step density evaluation and can capture complex density functions through specialized deep neural networks. Building on our previously introduced simulation-based framework, we adapt and extend the methodology to accommodate non-parametric joint priors. Our framework thus supports the development of elicitation methods for learning both parametric and non-parametric priors, as well as independent or joint priors for model parameters. To evaluate the performance of the proposed method, we perform four simulation studies and present an evaluation pipeline that incorporates diagnostics and additional evaluation tools to support decision-making at each stage of the elicitation process.},
  archive      = {J_SAC},
  author       = {Bockting, Florence and Radev, Stefan T. and Bürkner, Paul-Christian},
  doi          = {10.1007/s11222-025-10665-z},
  journal      = {Statistics and Computing},
  month        = {10},
  number       = {5},
  pages        = {1-33},
  shortjournal = {Stat. Comput.},
  title        = {Expert-elicitation method for non-parametric joint priors using normalizing flows},
  volume       = {35},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A nonparametric test of independence between the second-order structure of point patterns and covariates. <em>SAC</em>, <em>35</em>(5), 1-15. (<a href='https://doi.org/10.1007/s11222-025-10669-9'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We introduce a novel nonparametric method for testing the hypothesis of independence between a marked point process with functional marks and a covariate. In a special case, when the functional marks are constructed using the local indicator of spatial association (LISA) functions, it results in testing the second-order intensity reweighted stationarity (SOIRS) hypothesis against a specific alternative that is determined by a chosen covariate. SOIRS is often used as a convenient assumption in analysing point patterns, but a general formal test of the SOIRS hypothesis is not available in the literature. Our simulation experiments indicate that the proposed test performs well both in size and power. The practical application of the proposed methodology is demonstrated using a point pattern of fish locations in a water reservoir, accompanied by four covariates.},
  archive      = {J_SAC},
  author       = {González, Jonatan A. and Dvořák, Jiří and Mrkvička, Tomáš and Mateu, Jorge},
  doi          = {10.1007/s11222-025-10669-9},
  journal      = {Statistics and Computing},
  month        = {10},
  number       = {5},
  pages        = {1-15},
  shortjournal = {Stat. Comput.},
  title        = {A nonparametric test of independence between the second-order structure of point patterns and covariates},
  volume       = {35},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Parallel ADMM algorithm with gaussian back substitution for high-dimensional quantile regression and classification. <em>SAC</em>, <em>35</em>(5), 1-26. (<a href='https://doi.org/10.1007/s11222-025-10631-9'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the field of high-dimensional data analysis, modeling methods based on quantile loss function are highly regarded due to their ability to provide a comprehensive statistical perspective and effective handling of heterogeneous data. In recent years, many studies have focused on using the parallel alternating direction method of multipliers (P-ADMM) to solve high-dimensional quantile regression and classification problems. One efficient strategy is to reformulate the quantile loss function by introducing slack variables. However, this reformulation introduces a theoretical challenge: even when the regularization term is convex, the convergence of the algorithm cannot be guaranteed. To address this challenge, this paper proposes the Gaussian Back-Substitution strategy, which requires only a simple and effective correction step that can be easily integrated into existing parallel algorithm frameworks, achieving a linear convergence rate. Furthermore, this paper extends the parallel algorithm to handle some novel quantile loss classification models. Numerical simulations demonstrate that the proposed modified P-ADMM algorithm exhibits excellent performance in terms of reliability and efficiency. The R package for implementing the proposed algorithms can be obtained at https://github.com/xfwu1016/QPADMslackGB .},
  archive      = {J_SAC},
  author       = {Wu, Xiaofei and Guo, Dingzi and Liang, Rongmei and Zhang, Zhimin},
  doi          = {10.1007/s11222-025-10631-9},
  journal      = {Statistics and Computing},
  month        = {10},
  number       = {5},
  pages        = {1-26},
  shortjournal = {Stat. Comput.},
  title        = {Parallel ADMM algorithm with gaussian back substitution for high-dimensional quantile regression and classification},
  volume       = {35},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). S-SIRUS: An explainability algorithm for spatial regression random forest. <em>SAC</em>, <em>35</em>(5), 1-16. (<a href='https://doi.org/10.1007/s11222-025-10656-0'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Random Forest (RF) is a widely used machine learning algorithm known for its flexibility, user-friendliness, and high predictive performance across various domains. However, it is non-interpretable. This can limit its usefulness in applied sciences, where understanding the relationships between predictors and response variable is crucial from a decision-making perspective. In the literature, several methods have been proposed to explain RF, but none of them addresses the challenge of explaining RF in the context of spatially dependent data. Therefore, this work aims to explain regression RF in the case of spatially dependent data by extracting a compact and simple list of rules from an RF that explicitly takes into account the spatial correlation, i.e. RF-GLS. In this respect, we propose S-SIRUS, a spatial extension of SIRUS, the latter being a well-established regression rule algorithm able to extract a stable and short list of rules from the classical regression RF algorithm. To our knowledge, S-SIRUS is the only explainability tool proposed to open an RF-GLS, which, in turn, is the only random forest algorithm in the literature that accounts for spatial correlation internally in the algorithm. A simulation study was conducted to evaluate the explainability capability of the proposed S-SIRUS, by considering different levels of spatial dependence among the data. The results suggest that S-SIRUS exhibits a higher test predictive accuracy than SIRUS when spatial correlation is present. We encourage the use of SIRUS in the absence of spatial correlation and recommend adopting S-SIRUS when such correlation is present.},
  archive      = {J_SAC},
  author       = {Patelli, Luca and Golini, Natalia and Ignaccolo, Rosaria and Cameletti, Michela},
  doi          = {10.1007/s11222-025-10656-0},
  journal      = {Statistics and Computing},
  month        = {10},
  number       = {5},
  pages        = {1-16},
  shortjournal = {Stat. Comput.},
  title        = {S-SIRUS: An explainability algorithm for spatial regression random forest},
  volume       = {35},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Tree-based variational inference for poisson log-normal models. <em>SAC</em>, <em>35</em>(5), 1-35. (<a href='https://doi.org/10.1007/s11222-025-10668-w'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {When studying ecosystems, hierarchical trees are often used to organize entities based on proximity criteria, such as the taxonomy in microbiology, social classes in geography, or product types in retail businesses, offering valuable insights into entity relationships. Despite their significance, current count-data models do not leverage this structured information. In particular, the widely used Poisson log-normal (PLN) model, known for its ability to model interactions between entities from count data, lacks the possibility to incorporate such hierarchical tree structures, limiting its applicability in domains characterized by such complexities. To address this matter, we introduce the PLN-Tree model as an extension of the PLN model, specifically designed for modeling hierarchical count data. By integrating structured variational inference techniques, we propose an adapted training procedure and establish identifiability results, enhancing both theoretical foundations and practical interpretability. Experiments on synthetic datasets and human gut microbiome data highlight generative improvements when using PLN-Tree, demonstrating the practical interest of knowledge graphs like the taxonomy in microbiome modeling. Additionally, we present a proof-of-concept implication of the identifiability results by illustrating the practical benefits of using identifiable features for classification tasks, showcasing the versatility of the framework.},
  archive      = {J_SAC},
  author       = {Chaussard, Alexandre and Bonnet, Anna and Gassiat, Elisabeth and Le Corff, Sylvain},
  doi          = {10.1007/s11222-025-10668-w},
  journal      = {Statistics and Computing},
  month        = {10},
  number       = {5},
  pages        = {1-35},
  shortjournal = {Stat. Comput.},
  title        = {Tree-based variational inference for poisson log-normal models},
  volume       = {35},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Subsampling for big data linear models with measurement errors. <em>SAC</em>, <em>35</em>(5), 1-16. (<a href='https://doi.org/10.1007/s11222-025-10670-2'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Subsampling algorithms for various parametric regression models with massive data have been extensively investigated in recent years. However, all existing studies on subsampling heavily rely on clean massive data. In practical applications, the observed covariates may suffer from inaccuracies due to measurement errors. To address the challenge of large datasets with measurement errors, this study explores two subsampling algorithms based on the corrected likelihood approach: the optimal subsampling algorithm utilizing inverse probability weighting and the perturbation subsampling algorithm employing random weighting with a perfectly known distribution. Theoretical properties for both algorithms are provided. Numerical simulations and two real-data examples demonstrate the effectiveness of these proposed methods compared to other existing algorithms.},
  archive      = {J_SAC},
  author       = {Ju, Jiangshan and Liu, Min-Qian and Wang, Mingqiu and Zhao, Shengli},
  doi          = {10.1007/s11222-025-10670-2},
  journal      = {Statistics and Computing},
  month        = {10},
  number       = {5},
  pages        = {1-16},
  shortjournal = {Stat. Comput.},
  title        = {Subsampling for big data linear models with measurement errors},
  volume       = {35},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Estimating expectile-optimal treatment regimes. <em>SAC</em>, <em>35</em>(5), 1-16. (<a href='https://doi.org/10.1007/s11222-025-10672-0'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {It is a key issue for determining the optimal treatment regime or a sequence of treatment regimes based on individual characteristics in precision medical research. Most existing studies on estimating optimal treatment regimes focus on maximizing the average return on the potential outcomes of interest. However, this approach fails to capture the potential heterogeneity of observations and can not provide a complete characterization of the data. Expectiles, derived from an asymmetric quadratic loss function, encompass the mean and serve as a valuable tool for describing the distribution of outcomes. Motivated by these advantages of expectiles, we propose novel estimators for both static and dynamic expectile-optimal treatment regimes. Due to the differentiability of the loss function, it can bring computational advantages and facilitates theoretical analysis. The asymptotic properties of the proposed estimators are derived using empirical process theory. Their good finite sample performances are demonstrated through simulations and a real data from the HIV patients.},
  archive      = {J_SAC},
  author       = {Fan, Caiyun and Li, Siru and Xue, Minwei and Zhang, Feipeng},
  doi          = {10.1007/s11222-025-10672-0},
  journal      = {Statistics and Computing},
  month        = {10},
  number       = {5},
  pages        = {1-16},
  shortjournal = {Stat. Comput.},
  title        = {Estimating expectile-optimal treatment regimes},
  volume       = {35},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Causal segment regression with multiple thresholds. <em>SAC</em>, <em>35</em>(5), 1-12. (<a href='https://doi.org/10.1007/s11222-025-10673-z'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We introduce a novel multi-threshold change-plane structural equation model designed to accommodate varying causal effects across different change planes. In our proposed model, the number of thresholds, their locations, and the coefficients of change planes are all left unspecified. This model serves as a non-trivial extension of the multi-threshold change-point structural equation model. We first use a group selection technique to detect the number of thresholds, and then adopt an iterative procedure to refine the estimation of threshold locations and other model parameters. The Wald ratio method is applied to estimate causal effects for identified subgroups. Furthermore, we establish the consistency of estimated parameters and the asymptotic distribution of the estimated segmented causal effects. Finally, the performance of the proposed methodology is demonstrated by simulations and a real economic data example.},
  archive      = {J_SAC},
  author       = {Wang, Jingli},
  doi          = {10.1007/s11222-025-10673-z},
  journal      = {Statistics and Computing},
  month        = {10},
  number       = {5},
  pages        = {1-12},
  shortjournal = {Stat. Comput.},
  title        = {Causal segment regression with multiple thresholds},
  volume       = {35},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A neural network integrated accelerated failure time-based mixture cure model. <em>SAC</em>, <em>35</em>(5), 1-14. (<a href='https://doi.org/10.1007/s11222-025-10674-y'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The mixture cure rate model (MCM) is commonly used for analyzing survival data with a cured subgroup. While the prevailing approach to modeling the probability of cure involves a generalized linear model using a known parametric link function, such as the logit link function, it has limitations in capturing the complex effects of covariates on cure probability. This paper introduces a novel MCM employing a neural network-based classifier for cure probability and an accelerated failure time structure for the survival distribution of uncured patients. An expectation maximization algorithm is developed for parameter estimation. Simulation results demonstrate the superior performance of the proposed model in capturing non-linear classification boundaries compared to logit-based and spline-based MCMs, as well as other machine learning algorithms. This enhances the accuracy and precision of cured probability estimates, improving predictive accuracy. The proposed model and estimation method are applied to survival data on leukemia cancer patients, showcasing their effectiveness.},
  archive      = {J_SAC},
  author       = {Aselisewine, Wisdom and Pal, Suvra},
  doi          = {10.1007/s11222-025-10674-y},
  journal      = {Statistics and Computing},
  month        = {10},
  number       = {5},
  pages        = {1-14},
  shortjournal = {Stat. Comput.},
  title        = {A neural network integrated accelerated failure time-based mixture cure model},
  volume       = {35},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Robust feature screening via grothendieck’s correlation with FDR control. <em>SAC</em>, <em>35</em>(5), 1-17. (<a href='https://doi.org/10.1007/s11222-025-10677-9'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we propose a novel model-free feature screening procedure via Grothendieck’s correlation for ultrahigh-dimensional data. Since Grothendieck’s correlation does not need moment restrictions on random vectors and the influence function of Grothendieck’s covariance is bounded, the proposed feature screening method is robust to heavy-tailed distributions or outliers. Furthermore, we select the threshold by using the reflection via data splitting method to control the false discovery rate at a pre-specified level. Meanwhile, the proposed algorithm for screening active features is highly efficient due to the low complexity of estimating the Grothendieck’s correlation. In addition, the proposed feature screening procedure enjoys both the sure screening property and FDR control simultaneously under certain technical conditions. Finally, we illustrate the satisfactory finite sample performance of our feature screening method via extensive numerical simulations and a real dataset analysis.},
  archive      = {J_SAC},
  author       = {Jiang, Yunlu and Huang, Xiaowen and Tan, Jing and Jiang, Ruizhe},
  doi          = {10.1007/s11222-025-10677-9},
  journal      = {Statistics and Computing},
  month        = {10},
  number       = {5},
  pages        = {1-17},
  shortjournal = {Stat. Comput.},
  title        = {Robust feature screening via grothendieck’s correlation with FDR control},
  volume       = {35},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Subgroup analysis of differential networks with latent variables. <em>SAC</em>, <em>35</em>(5), 1-20. (<a href='https://doi.org/10.1007/s11222-025-10681-z'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Differential network analysis serves as a crucial tool in capturing variations in network rewiring patterns across different biological conditions. Real-world observational data often contain subgroup structures with differing statistical properties, potentially leading to heterogeneity among differential networks. However, existing graphical model-based heterogeneity analysis methods are designed for subgroup networks rather than differential networks. Moreover, these methods require sparsity within each network, which is ineffective for dense networks, especially when observed variables are confounded by latent (unobserved) variables. In this article, we focus on estimating differential networks between an unlabeled heterogeneous group and a baseline group, and develop subgroup analysis from the perspective of differential networks. By imposing a sparse plus low-rank structure on the baseline network and sparsity on differential networks, which characterize and balance the influence of latent variables, the proposed method allows for effective estimation of sparse differential networks and non-sparse subgroup networks. We develop an efficient computational algorithm for this purpose. Simulation studies demonstrate the competitive performance of the proposed approach over closely related alternatives. An application to a set of data on Non-Small Cell Lung Cancer (NSCLC) further confirms the utility of the methodology.},
  archive      = {J_SAC},
  author       = {Li, Linxi and Ma, Shuangge and Zhang, Qingzhao},
  doi          = {10.1007/s11222-025-10681-z},
  journal      = {Statistics and Computing},
  month        = {10},
  number       = {5},
  pages        = {1-20},
  shortjournal = {Stat. Comput.},
  title        = {Subgroup analysis of differential networks with latent variables},
  volume       = {35},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Double debiased estimation and inference for longitudinal generalized linear models with hidden confounders. <em>SAC</em>, <em>35</em>(5), 1-15. (<a href='https://doi.org/10.1007/s11222-025-10645-3'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The hidden confounding model has been widely applied in many fields. Without adjusting for the hidden confounders, the estimators from the standard methods of high-dimensional models could be biased. In this paper, we focus the estimation and statistical inference on a preconceived low-dimensional parameter of main interest for high-dimensional longitudinal generalized linear models with hidden confounders. To handle the impact of the hidden confounders and high-dimensional nuisance covariates in longitudinal data simultaneously, a two-stage deconfounded and debiased (DD) estimator is proposed, which achieves the double debiased estimation. In the first stage, we impute the hidden confounders by high-dimensional factor model analysis techniques and maximum likelihood estimation. In the second stage, we construct the quasi decorrelated score function and obtain the DD estimator by the two-step generalized method of moment approach. Theoretically, the error bounds and the asymptotic normality of our proposed estimator are established. Our method shows good finite sample performance in simulation studies. An application to the Beijing Air Quality Dataset is also presented.},
  archive      = {J_SAC},
  author       = {Fei, Yuxin and Liu, Jin and Wang, Lei},
  doi          = {10.1007/s11222-025-10645-3},
  journal      = {Statistics and Computing},
  month        = {10},
  number       = {5},
  pages        = {1-15},
  shortjournal = {Stat. Comput.},
  title        = {Double debiased estimation and inference for longitudinal generalized linear models with hidden confounders},
  volume       = {35},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Persistent sampling: Enhancing the efficiency of sequential monte carlo. <em>SAC</em>, <em>35</em>(5), 1-22. (<a href='https://doi.org/10.1007/s11222-025-10682-y'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Sequential Monte Carlo (SMC) samplers are powerful tools for Bayesian inference but suffer from high computational costs due to their reliance on large particle ensembles for accurate estimates. We introduce persistent sampling (PS), an extension of SMC that systematically retains and reuses particles from all prior iterations to construct a growing, weighted ensemble. By leveraging multiple importance sampling and resampling from a mixture of historical distributions, PS mitigates the need for excessively large particle counts, directly addressing key limitations of SMC such as particle impoverishment and mode collapse. Crucially, PS achieves this without additional likelihood evaluations-weights for persistent particles are computed using cached likelihood values. This framework not only yields more accurate posterior approximations but also produces marginal likelihood estimates with significantly lower variance, enhancing reliability in model comparison. Furthermore, the persistent ensemble enables efficient adaptation of transition kernels by leveraging a larger, decorrelated particle pool. Experiments on high-dimensional Gaussian mixtures, hierarchical models, and non-convex targets demonstrate that PS consistently outperforms standard SMC and related variants, including recycled and waste-free SMC, achieving substantial reductions in mean squared error for posterior expectations and evidence estimates, all at reduced computational cost. PS thus establishes itself as a robust, scalable, and efficient alternative for complex Bayesian inference tasks.},
  archive      = {J_SAC},
  author       = {Karamanis, Minas and Seljak, Uroš},
  doi          = {10.1007/s11222-025-10682-y},
  journal      = {Statistics and Computing},
  month        = {10},
  number       = {5},
  pages        = {1-22},
  shortjournal = {Stat. Comput.},
  title        = {Persistent sampling: Enhancing the efficiency of sequential monte carlo},
  volume       = {35},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Nonparametric empirical bayes prediction in mixed models. <em>SAC</em>, <em>35</em>(5), 1-25. (<a href='https://doi.org/10.1007/s11222-025-10686-8'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Mixed models are classical tools in statistics for modeling repeated data on subjects, such as data on patients or firms collected over time. They extend conventional linear models to include latent parameters, called random effects, that capture between-subject variation and accommodate dependence within the repeated measurements of a subject. Traditionally, predictions in mixed models are conducted by assuming that the random effects have a zero mean Normal distribution, which leads to the Best Linear Unbiased Predictor (BLUP) of the random effects in these models. However, such a distributional assumption on the random effects is restrictive and may lead to inefficient predictions, especially when the true random effect distribution is far from Normal. In this article, we develop a framework, EBPred, for empirical Bayes prediction in mixed models. The predictions from EBPred rely on the Best Predictor of the random effects, which are constructed without any parametric assumption on the distribution of the random effects and offer a natural extension to the BLUP when the true random effect distribution is not Normal. An extensive simulation study demonstrates the superior prediction performance of EBPred relative to extant approaches across many settings. Extensions to dynamic panel data and cross-classified random effect models are discussed. The method is illustrated on an application involving the prediction of bank stock returns.},
  archive      = {J_SAC},
  author       = {Banerjee, Trambak and Sharma, Padma},
  doi          = {10.1007/s11222-025-10686-8},
  journal      = {Statistics and Computing},
  month        = {10},
  number       = {5},
  pages        = {1-25},
  shortjournal = {Stat. Comput.},
  title        = {Nonparametric empirical bayes prediction in mixed models},
  volume       = {35},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). From sparse to dense functional data: Phase transitions from a simultaneous inference perspective. <em>SAC</em>, <em>35</em>(5), 1-19. (<a href='https://doi.org/10.1007/s11222-025-10671-1'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We aim to develop unified tools for simultaneous inference regarding the mean function of functional data, covering situations from sparse to dense observations. We first establish a unified Gaussian approximation applicable across arbitrary sampling schemes, which facilitates the construction of simultaneous confidence bands for mean functions using the B-spline estimator. Subsequently, we explore the conditions leading to phase transitions by decomposing the asymptotic variance of the approximating Gaussian processes. These conditions are determined by the relationship between the number of knots, the sample sizes, and the number of observations per trajectory. As an extension, we also consider the orthogonal series estimator, highlighting how phase transitions are affected by the sup-norm, the approximation power, and other features of the employed orthonormal basis. Our extensive simulations strongly support our theoretical findings and further illustrate the variability of the asymptotic distribution via the decomposition of asymptotic variance we obtain. The developed method is further applied to body fat data and traffic data.},
  archive      = {J_SAC},
  author       = {Cai, Leheng and Hu, Qirui},
  doi          = {10.1007/s11222-025-10671-1},
  journal      = {Statistics and Computing},
  month        = {10},
  number       = {5},
  pages        = {1-19},
  shortjournal = {Stat. Comput.},
  title        = {From sparse to dense functional data: Phase transitions from a simultaneous inference perspective},
  volume       = {35},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Confidence intervals and one-sample-based hypothesis testing for cronbach’s $$\alpha $$ coefficient for interval-valued data. <em>SAC</em>, <em>35</em>(5), 1-21. (<a href='https://doi.org/10.1007/s11222-025-10676-w'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The definition of the well-known Cronbach $$\alpha $$ coefficient has been recently extended to measure the internal consistency of interval-valued scale-based items in questionnaires. Although some of its main properties have already been studied, no inferential technique has yet been proposed to analyze its population value. In this paper, large sample theory and bootstrap procedures are used to obtain one-sample-based confidence intervals and perform hypothesis testing for the population Cronbach $$\alpha $$ coefficient for interval-valued data. The asymptotic correctness and consistency of the suggested methods are theoretically derived by analyzing the limit distribution of the sample and bootstrap analogue estimators of such extended reliability index. In addition, some simulation studies are conducted to empirically investigate the performance of the proposed confidence intervals and hypothesis tests, which is also illustrated through a real-life example.},
  archive      = {J_SAC},
  author       = {García-García, José and Lubiano, María Asunción},
  doi          = {10.1007/s11222-025-10676-w},
  journal      = {Statistics and Computing},
  month        = {10},
  number       = {5},
  pages        = {1-21},
  shortjournal = {Stat. Comput.},
  title        = {Confidence intervals and one-sample-based hypothesis testing for cronbach’s $$\alpha $$ coefficient for interval-valued data},
  volume       = {35},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Communication-efficient transfer learning for adaptive huber regression. <em>SAC</em>, <em>35</em>(5), 1-15. (<a href='https://doi.org/10.1007/s11222-025-10685-9'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Transfer learning aims to improve the performance of a target model by leveraging data from related source populations. In this paper, we consider the data are inherently distributed and focus on distributed transfer learning in the presence of heavy-tailed and/or asymmetric errors. When the transferable sources are known, based on adaptive Huber regression (AHR) to achieve the bias-robustness tradeoff, a two-step communication-efficient transfer learning for AHR (CTrans-AHR) estimator is proposed to deal with the distributed data. To correct the biases caused by the Lasso penalty, a debiased Lasso CTrans-AHR estimator is constructed and its asymptotic distribution is also studied. When the transferable sources are unknown, a data-driven transferable source detection algorithm is proposed with the theoretical guarantee. Simulation results show that our detection algorithm can consistently select all transferable sources with probability approaching one. Compared to the transfer learning estimator using one site alone or the distributed estimator based solely on target data, our CTrans-AHR estimator achieves the lowest $$\ell _\infty $$ and $$\ell _2$$ estimation errors under various error distributions. The accurate coverage probabilities demonstrate the effectiveness of our proposed debiased Lasso CTrans-AHR estimator. An application to the DVL1 gene expression of brain tissues in the Genotype-Tissue Expression dataset is also presented.},
  archive      = {J_SAC},
  author       = {Yang, Jichen and Wang, Lei and Lian, Heng},
  doi          = {10.1007/s11222-025-10685-9},
  journal      = {Statistics and Computing},
  month        = {10},
  number       = {5},
  pages        = {1-15},
  shortjournal = {Stat. Comput.},
  title        = {Communication-efficient transfer learning for adaptive huber regression},
  volume       = {35},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Estimation of the additive hazards model based on interval-censored failure time data with random change point. <em>SAC</em>, <em>35</em>(5), 1-11. (<a href='https://doi.org/10.1007/s11222-025-10675-x'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The additive hazards model is one of the most commonly used models for regression analysis of failure time data and a great deal of literature has been established for its estimation under various situations. In this paper, we consider the situation where only interval-censored data are available and also there exists a random change point, which occurs quite often on cancer studies among others and for which there does not seem to exist an established estimation approach. For the situation, we propose a sieve maximum likelihood estimation procedure with the use of Bernstein polynomials, and the asymptotic properties of the resulting estimators are established. In addition, a simulation study is conducted to assess the empirical performance of the proposed approach and suggests that it works well in practical situations. The proposed methodology is applied to a set of real data on child mortality that motivated this study.},
  archive      = {J_SAC},
  author       = {Du, Mingyue and Lou, Yichen and Sun, Jianguo},
  doi          = {10.1007/s11222-025-10675-x},
  journal      = {Statistics and Computing},
  month        = {10},
  number       = {5},
  pages        = {1-11},
  shortjournal = {Stat. Comput.},
  title        = {Estimation of the additive hazards model based on interval-censored failure time data with random change point},
  volume       = {35},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). DeepJAM: Joint alignment of multivariate quasi-periodic functional data using deep learning. <em>SAC</em>, <em>35</em>(5), 1-16. (<a href='https://doi.org/10.1007/s11222-025-10678-8'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The joint alignment of multivariate functional data plays an important role in various fields such as signal processing, neuroscience and medicine, including the statistical analysis of data from wearable devices. Traditional methods often ignore the phase variability and instead focus on the variability in the observed amplitude. We present a novel method for joint alignment of multivariate quasi-periodic functions using deep neural networks, decomposing, but retaining all the information in the data by preserving both phase and amplitude variability. Our proposed neural network uses a special activation of the output that builds on the unit simplex transformation, and we utilize a loss function based on the Fisher-Rao metric to train our model. Furthermore, our method is unsupervised and can provide an optimal common template function as well as subject-specific templates. We demonstrate our method on two simulated datasets and one real example, comprising data from 12-lead 10s electrocardiogram recordings.},
  archive      = {J_SAC},
  author       = {Pham, Vi Thanh and Nielsen, Jonas Bille and Kofoed, Klaus Fuglsang and Kühl, Jørgen Tobias and Jensen, Andreas Kryger},
  doi          = {10.1007/s11222-025-10678-8},
  journal      = {Statistics and Computing},
  month        = {10},
  number       = {5},
  pages        = {1-16},
  shortjournal = {Stat. Comput.},
  title        = {DeepJAM: Joint alignment of multivariate quasi-periodic functional data using deep learning},
  volume       = {35},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). The deep latent position block model for block clustering and latent representation of nodes in networks. <em>SAC</em>, <em>35</em>(5), 1-24. (<a href='https://doi.org/10.1007/s11222-025-10679-7'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The current surge in data has led to a significant increase in the size of networks used to model relationships between different objects represented as nodes. Therefore, summarising network information is a crucial task that can be conducted using node clustering methods. Additionally, to ensure interpretable results, it is essential to employ relevant visualisation techniques to depict the network. To tackle both issues, we propose a new methodology called the deep latent position block model (Deep LPBM). This simultaneously provides a network visualisation coherent with block modelling, allowing a clustering more general than community detection methods, as well as a continuous representation of nodes in a latent space given by partial membership vectors. Deep LPBM is based on a variational autoencoder strategy, relying on a graph convolutional network, with a specifically designed decoder. The inference involves the construction of an approximation of the marginal likelihood of Deep LPBM through the expected lower bound (ELBO). A gradient-descent algorithm based on Monte-Carlo approximations is used to optimize the ELBO with respect to its parameters. To select the number of clusters, we compare three model selection criteria. A node clustering benchmark comprising positional community detection as well as model methods is conducted. We also compare the quality of Deep LPBM node partial membership estimation with other methodologies. We conclude with an analysis of the French political blogosphere network and a comparison with another methodology to illustrate the novelty provided by Deep LPBM results.},
  archive      = {J_SAC},
  author       = {Boutin, Rémi and Latouche, Pierre and Bouveyron, Charles},
  doi          = {10.1007/s11222-025-10679-7},
  journal      = {Statistics and Computing},
  month        = {10},
  number       = {5},
  pages        = {1-24},
  shortjournal = {Stat. Comput.},
  title        = {The deep latent position block model for block clustering and latent representation of nodes in networks},
  volume       = {35},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Penalty learning for optimal partitioning using multilayer perceptron. <em>SAC</em>, <em>35</em>(5), 1-10. (<a href='https://doi.org/10.1007/s11222-025-10680-0'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Changepoint detection is a technique used to identify significant shifts in sequences and is widely used in fields such as finance, genomics, and medicine. To identify the changepoints, dynamic programming (DP) algorithms, particularly Optimal Partitioning (OP) family, are widely used. To control the changepoints count, these algorithms use a fixed penalty to penalize the changepoints presence. To predict the optimal value of that penalty, existing methods used simple models such as linear or tree-based, which may limit predictive performance. To address this issue, this study proposes using a multilayer perceptron (MLP) with a ReLU activation function to predict the penalty. The proposed model generates continuous predictions – as opposed to the stepwise ones in tree-based models – and handles non-linearity better than linear models. Experiments on large benchmark genomic datasets demonstrate that the proposed model improves accuracy and F1 score compared to existing models.},
  archive      = {J_SAC},
  author       = {Nguyen, Tung L and Hocking, Toby Dylan},
  doi          = {10.1007/s11222-025-10680-0},
  journal      = {Statistics and Computing},
  month        = {10},
  number       = {5},
  pages        = {1-10},
  shortjournal = {Stat. Comput.},
  title        = {Penalty learning for optimal partitioning using multilayer perceptron},
  volume       = {35},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Personalized federated learning on large-scale association networks. <em>SAC</em>, <em>35</em>(5), 1-17. (<a href='https://doi.org/10.1007/s11222-025-10684-w'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Federated learning (FL) has rapidly gained traction for its capacity to facilitate collaborative model training across decentralized data sources while maintaining data privacy. Addressing the challenges stemming from intrinsic heterogeneity in local data distributions, personalization has become an essential aspect in advancing FL techniques. In this study, we introduce a new personalized federated learning (PFL) framework for recovering large-scale response-predictor association networks, named sparse clustered association learning (SCALE). This approach subtly incorporates sparse orthogonal factor learning and fusion penalization, enabling the joint exploration of latent relationships and structured clustering. To operationalize SCALE, we develop the PerFL-LSAN algorithm, tailored specifically for large-scale association networks recovery within federated settings. Theoretical analyses confirm the convergence guarantee of the PerFL-LSAN algorithm and the statistical consistency of the proposed estimator. We further validate the practical effectiveness of the new method through multiple simulation experiments and a real-world application on the Communities and Crime dataset.},
  archive      = {J_SAC},
  author       = {Li, Letian and Zhou, Runlin and Zheng, Zemin},
  doi          = {10.1007/s11222-025-10684-w},
  journal      = {Statistics and Computing},
  month        = {10},
  number       = {5},
  pages        = {1-17},
  shortjournal = {Stat. Comput.},
  title        = {Personalized federated learning on large-scale association networks},
  volume       = {35},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Robust and efficient estimation for the block-basu bivariate exponential distribution. <em>SAC</em>, <em>35</em>(5), 1-21. (<a href='https://doi.org/10.1007/s11222-025-10687-7'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Block-Basu bivariate exponential (BBBE) distribution, see Block and Basu (1974), is one of the most popular variants of an absolutely continuous bivariate distribution. The traditional maximum likelihood estimation method is highly efficient, yet the maximum likelihood estimators (MLEs) can be significantly affected by the presence of a few outliers in the data. This article focuses on introducing a robust estimation procedure in the presence of outliers. We explore the implementation of the minimum density power divergence estimator (MDPDE) for robust and efficient parameter estimation of the BBBE distribution. The MDPDE is indexed by a single tuning parameter that controls the trade-off between robustness and efficiency. We show that the MDPDE for the BBBE model has a bounded influence function, that ensures the robustness of MDPDE. We compare the asymptotic covariance matrix of MDPDE with the Fisher information matrix. We study the asymptotic relative efficiency of MDPDE. We discussed about the data-driven selection of optimal tuning parameter. The simulation results indicate that the MDPDE is more robust and efficient than MLE for the BBBE distribution when outliers are present in a data set. A real data set has been analyzed for illustration purposes.},
  archive      = {J_SAC},
  author       = {Kumar, Sanjay and Kundu, Debasis and Mitra, Sharmishtha},
  doi          = {10.1007/s11222-025-10687-7},
  journal      = {Statistics and Computing},
  month        = {10},
  number       = {5},
  pages        = {1-21},
  shortjournal = {Stat. Comput.},
  title        = {Robust and efficient estimation for the block-basu bivariate exponential distribution},
  volume       = {35},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Spatial autoregressive model for interval-valued data and applications. <em>SAC</em>, <em>35</em>(5), 1-23. (<a href='https://doi.org/10.1007/s11222-025-10688-6'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Interval-valued data, characterized by intrinsic measurement imprecision, uncertainty, and variability, are common in real-world applications. This study introduces a novel spatial autoregressive model tailored for interval-valued data, unifying and generalizing several existing frameworks. To address the limitations of interval representations, we develop a joint quasi-maximum likelihood estimation method that holistically incorporates complete interval information through both center and radius parameters. Crucially, we introduce a novel $$L_2$$ -type distance metric to quantify interval variance, which systematically captures richer intra-interval information compared to classical Euclidean interval distance metric. The asymptotic properties of the estimators under regularity conditions are established, ensuring statistical robustness. Numerical experiments on synthetic datasets demonstrate the superiority of the proposed method over conventional approaches in prediction accuracy and information retention. Empirical validation on real spatial interval datasets-urban house price domain-confirms the efficiency of the parameter estimation framework and the operational viability of the proposed model.},
  archive      = {J_SAC},
  author       = {Zhang, Jinjin and Li, Kun and Ji, Aibing},
  doi          = {10.1007/s11222-025-10688-6},
  journal      = {Statistics and Computing},
  month        = {10},
  number       = {5},
  pages        = {1-23},
  shortjournal = {Stat. Comput.},
  title        = {Spatial autoregressive model for interval-valued data and applications},
  volume       = {35},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Network-assisted semi-supervised logistic regression. <em>SAC</em>, <em>35</em>(5), 1-12. (<a href='https://doi.org/10.1007/s11222-025-10691-x'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A notable issue in the semi-supervised logistic regression estimation stems from the scarcity of available labeled data. To solve this problem, we propose a novel approach to enhance the estimation of high-dimensional logistic regression by leveraging network information. By formulating the total effect of covariates on network connections and establishing the relationship between class labels and network connections, we deduce that the regression coefficients fall into a low-dimensional subspace under mild conditions, transforming the high-dimensional estimation problem into a low-dimensional one. Particularly, we construct efficient estimators of parameters without the sparsity assumption, allowing the dimension to be larger than the sample size of labeled data. Theoretically, it is shown that the proposed estimators enjoy a faster convergence rate than existing methods using labeled data only, for both dense and sparse networks. Additionally, we extend the idea to the network with community structure, in which both the parameter estimation and community detection are simultaneously explored. Extensive simulation studies and real data analysis confirm the advantages of the proposed method.},
  archive      = {J_SAC},
  author       = {Zheng, Shengbin and Wang, Zhonghan and Zhao, Junlong},
  doi          = {10.1007/s11222-025-10691-x},
  journal      = {Statistics and Computing},
  month        = {10},
  number       = {5},
  pages        = {1-12},
  shortjournal = {Stat. Comput.},
  title        = {Network-assisted semi-supervised logistic regression},
  volume       = {35},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Distributed estimation and algorithm for distributed outcome dependent subsampling in generalized linear regression with large-scale data. <em>SAC</em>, <em>35</em>(5), 1-18. (<a href='https://doi.org/10.1007/s11222-025-10692-w'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To reduce the computational and storage pressure in processing distributed large-scale data, we study a distributed outcome dependent subsampling strategy by conducting a cost-effective subsampling procedure on each machine. We develop a likelihood-based distributed estimation method for the generalized linear regression and obtain the asymptotic properties of the proposed estimator. By iteratively combining an aggregated gradient based method and an accelerated gradient update between the local machines and a central machine, we establish a distributed algorithm to implement the computation of the proposed estimator. The proposed method can process large-scale data by multiple machines in parallel, reduce distributed subsample sizes and meanwhile maintain statistical efficiency. We illustrate and evaluate the proposed method through simulation studies and real data applications.},
  archive      = {J_SAC},
  author       = {Yin, Jie and Ding, Jieli and Yang, Changming},
  doi          = {10.1007/s11222-025-10692-w},
  journal      = {Statistics and Computing},
  month        = {10},
  number       = {5},
  pages        = {1-18},
  shortjournal = {Stat. Comput.},
  title        = {Distributed estimation and algorithm for distributed outcome dependent subsampling in generalized linear regression with large-scale data},
  volume       = {35},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Disclosure risk assessment with bayesian non-parametric hierarchical modelling. <em>SAC</em>, <em>35</em>(5), 1-14. (<a href='https://doi.org/10.1007/s11222-025-10693-9'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Micro and survey datasets often contain private information about individuals, like their health status, income, or political preferences. Previous studies have shown that, even after data anonymization, a malicious intruder could still be able to identify individuals in the dataset by matching their variables to external information. Disclosure risk measures are statistical measures meant to quantify how big such a risk is for a specific dataset. One of the most common measures is the number of sample unique values that are also population unique. Mixed membership models can provide very accurate estimates of this measure. A limitation of this approach is that the number of extreme profiles has to be chosen by the modeller. In this article, we propose a non-parametric version of the model, based on the Hierarchical Dirichlet Process (HDP). The proposed approach does not require any tuning parameter or model selection step and provides accurate estimates of the disclosure risk measure, even with samples as small as 1 $$\%$$ of the population size. Moreover, a data augmentation scheme to address the presence of structural zeros is presented. The proposed methodology is tested on a real dataset from the New York microdata.},
  archive      = {J_SAC},
  author       = {Battiston, Marco and Rimella, Lorenzo},
  doi          = {10.1007/s11222-025-10693-9},
  journal      = {Statistics and Computing},
  month        = {10},
  number       = {5},
  pages        = {1-14},
  shortjournal = {Stat. Comput.},
  title        = {Disclosure risk assessment with bayesian non-parametric hierarchical modelling},
  volume       = {35},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Distributed empirical likelihood inference with or without byzantine failures. <em>SAC</em>, <em>35</em>(5), 1-20. (<a href='https://doi.org/10.1007/s11222-025-10699-3'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Empirical likelihood is a very important nonparametric approach that is of wide application. However, it is hard and even infeasible to calculate the empirical log-likelihood ratio statistic with massive data. The main challenge is the calculation of the Lagrange multiplier. This motivates us to develop a distributed empirical likelihood method by calculating the Lagrange multiplier in a multi-round distributed manner. It is shown that the distributed empirical log-likelihood ratio statistic is asymptotically standard chi-squared under some mild conditions. The proposed algorithm is communication-efficient and achieves the desired accuracy in a few rounds. Further, the distributed empirical likelihood method is extended to the case of Byzantine failures. A machine selection algorithm is developed to identify the worker machines without Byzantine failures such that the distributed empirical likelihood method can be applied. The proposed methods are evaluated by numerical simulations and illustrated with an analysis of airline on-time performance study and a surface climate analysis of Yangtze River Economic Belt.},
  archive      = {J_SAC},
  author       = {Wang, Qihua and Du, Jinye and Sheng, Ying},
  doi          = {10.1007/s11222-025-10699-3},
  journal      = {Statistics and Computing},
  month        = {10},
  number       = {5},
  pages        = {1-20},
  shortjournal = {Stat. Comput.},
  title        = {Distributed empirical likelihood inference with or without byzantine failures},
  volume       = {35},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). RSFclust: Robust sparse clustering of functional data using quantile curves. <em>SAC</em>, <em>35</em>(5), 1-18. (<a href='https://doi.org/10.1007/s11222-025-10694-8'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we propose a novel method for clustering functional data that significantly extends the distribution scope of functional data. Most existing methods are developed for a specific distribution structure of functional data and, thus, provide good results only when the distribution assumptions are met. For example, mean-based methods perform poorly when the error distribution is asymmetric. In contrast, methods based on asymmetric measures perform relatively poorly compared to mean-based methods in the presence of symmetric errors. In addition, most methods based on the basis expansion approach use only one curve representing the centrality of data as a clustering input, limiting their ability to handle complex distributional structures. To address this limitation, we consider multiple types of curves, including mean and quantile curves, and use their functional principal component scores as input variables for clustering, which can better reflect the distribution of data. Furthermore, we apply the concept of sparse clustering to multiple types of curves, resulting in good clustering performance if at least one curve can divide data into several subgroups well. Results from numerical experiments, including real data analysis, empirically verify that the proposed approach consistently achieves superior clustering performance across various distributional settings.},
  archive      = {J_SAC},
  author       = {Lee, Chihoon and Oh, Hee-Seok and Kim, Joonpyo},
  doi          = {10.1007/s11222-025-10694-8},
  journal      = {Statistics and Computing},
  month        = {10},
  number       = {5},
  pages        = {1-18},
  shortjournal = {Stat. Comput.},
  title        = {RSFclust: Robust sparse clustering of functional data using quantile curves},
  volume       = {35},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Optimal F-score matching for bipartite record linkage. <em>SAC</em>, <em>35</em>(5), 1-11. (<a href='https://doi.org/10.1007/s11222-025-10701-y'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Probabilistic record linkage is often used to match records from two files, in particular when the variables common to both files comprise identifiers measured with occasional errors like names and demographic variables. We consider bipartite record linkage settings in which there are no duplicates within the files, but some entities appear in both files. In this setting, the analyst desires a point estimate of the linkage structure that matches each record to at most one record from the other file. We introduce an estimator that maximizes the expected F-score for the linkage structure. We target the approach for record linkage methods that produce either (an approximate) posterior distribution of the unknown linkage structure or probabilities of matches for record pairs. Using simulations and applications with genuine data, we illustrate that the F-score estimators have desirable properties.},
  archive      = {J_SAC},
  author       = {Bai, Eric A. and Binette, Olivier and Reiter, Jerome P.},
  doi          = {10.1007/s11222-025-10701-y},
  journal      = {Statistics and Computing},
  month        = {10},
  number       = {5},
  pages        = {1-11},
  shortjournal = {Stat. Comput.},
  title        = {Optimal F-score matching for bipartite record linkage},
  volume       = {35},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Bias-enhanced support detection and root finding approach. <em>SAC</em>, <em>35</em>(5), 1-21. (<a href='https://doi.org/10.1007/s11222-025-10702-x'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we focus on parameter estimation for sparse, correlated, high-dimensional linear regression models, and propose a framework to approximate a class of the oracle biased estimators. We term this approach the bias-enhanced support detection and root finding approach (BESDAR). The BESDAR algorithm significantly reduces estimation mean square error while improving predictive accuracy, and simultaneously lowers computational costs and iteration requirements. Under some certain conditions, we demonstrate that the BESDAR algorithm achieves sharp error bounds and obtain an optimal order within finite iterations. If the target signal is above the detectable level, the BESDAR algorithm will achieve the oracle biased estimator with high probability. Furthermore, we derive some classical biased estimators that satisfy our framework, such as the Ridge estimator, the Liu estimator. To overcome the over-compression of Ridge estimator and Liu estimator, we further propose the adaptive Liu estimator and adaptive Ridge estimator. Simulations and real data show that our proposed BESDAR algorithm outperforms some existing methods for variable selection when dealing with correlated data.},
  archive      = {J_SAC},
  author       = {Ming, Hao and Yang, Hu},
  doi          = {10.1007/s11222-025-10702-x},
  journal      = {Statistics and Computing},
  month        = {10},
  number       = {5},
  pages        = {1-21},
  shortjournal = {Stat. Comput.},
  title        = {Bias-enhanced support detection and root finding approach},
  volume       = {35},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Data-driven uncertainty quantification for constrained stochastic differential equations and application to solar photovoltaic power forecast data. <em>SAC</em>, <em>35</em>(5), 1-22. (<a href='https://doi.org/10.1007/s11222-025-10698-4'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this work, we extend the data-driven Itô stochastic differential equation (SDE) framework for the pathwise assessment of short-term forecast errors to account for the time-dependent upper bound that naturally constrains the observable historical data and forecast. We propose a new nonlinear and time-inhomogeneous SDE model with a Jacobi-type diffusion term for the observable phenomenon of interest, simultaneously driven by the forecast and the constraining upper bound. We rigorously demonstrate the existence and uniqueness of a strong solution to the SDE model by imposing a condition for the time-varying mean-reversion parameter appearing in the drift term. After normalization, the original forecast function is thresholded to keep such time-varying mean-reversion parameters bounded. Thus, for any finite time interval, the paths of the forecast error process almost surely do not reach the time-dependent boundaries. The SDE model parameter calibration procedure is applied to user-selected approximations of the likelihood function. Another novel contribution is estimating the unknown transition density of the forecast error process with a tailored kernel smoothing technique without and with a control variate method, coupling an adequate SDE to the original one. We provide the theoretical study about how to choose the optimal bandwidth. As a case study, we fit the model to the 2019 photovoltaic (PV) solar power daily production and forecast data in Uruguay, computing the daily maximum solar PV production estimation. Two statistical versions of the constrained SDE model are fit, with the beta and truncated normal distributions as surrogates for the transition density function of the forecast error process. Empirical results include simulations of the normalized solar PV power production and pathwise confidence bands generated with the desired coverage probability through an indirect inference method. An objective comparison of optimal parametric points associated with the two selected statistical approximations is provided by applying our innovative kernel smoothing estimation technique of the transition function of the forecast error process. As a byproduct, we created a procedure providing a reliable criterion for choosing an adequate density proxy candidate that better fits the actual data at a low time cost. The methodology employs a thorough pathwise assessment of the forecast error uncertainty in situations where time-dependent boundaries and available forecasts drive the model specifications.},
  archive      = {J_SAC},
  author       = {Ben Chaabane, Khaoula and Kebaier, Ahmed and Scavino, Marco and Tempone, Raúl},
  doi          = {10.1007/s11222-025-10698-4},
  journal      = {Statistics and Computing},
  month        = {10},
  number       = {5},
  pages        = {1-22},
  shortjournal = {Stat. Comput.},
  title        = {Data-driven uncertainty quantification for constrained stochastic differential equations and application to solar photovoltaic power forecast data},
  volume       = {35},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A bayesian model for co-clustering ordinal data with informative missing entries. <em>SAC</em>, <em>35</em>(5), 1-13. (<a href='https://doi.org/10.1007/s11222-025-10703-w'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Several approaches have been proposed in the literature for clustering multivariate ordinal data. These methods typically treat missing values as absent information, rather than recognizing them as valuable for profiling population characteristics. To address this gap, we introduce a Bayesian nonparametric model for co-clustering multivariate ordinal data that treats censored observations as informative, rather than merely missing. We demonstrate that this offers a significant improvement in understanding the underlying structure of the data. Our model exploits the flexibility of two independent Dirichlet processes, allowing us to infer potentially distinct subpopulations that characterize the latent structure of both subjects and variables. The ordinal nature of the data is addressed by introducing latent variables, while a matrix factorization specification is adopted to handle the high dimensionality of the data in a parsimonious way. The conjugate structure of the model enables an explicit derivation of the full conditional distributions of all the random variables in the model, which facilitates seamless posterior inference using a Gibbs sampling algorithm. We demonstrate the method’s performance through simulations and by analyzing politician and movie ratings data.},
  archive      = {J_SAC},
  author       = {Giampino, Alice and Canale, Antonio and Nipoti, Bernardo},
  doi          = {10.1007/s11222-025-10703-w},
  journal      = {Statistics and Computing},
  month        = {10},
  number       = {5},
  pages        = {1-13},
  shortjournal = {Stat. Comput.},
  title        = {A bayesian model for co-clustering ordinal data with informative missing entries},
  volume       = {35},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Estimation of ratios of normalizing constants using stochastic approximation: The SARIS algorithm. <em>SAC</em>, <em>35</em>(5), 1-20. (<a href='https://doi.org/10.1007/s11222-025-10664-0'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Computing ratios of normalizing constants plays an important role in statistical modeling. Two important examples are hypothesis testing in latent variables models, and model comparison in Bayesian statistics. In both examples, the likelihood ratio and the Bayes factor are defined as the ratio of the normalizing constants of posterior distributions. We propose in this article a new methodology that estimates this ratio using stochastic approximation principle. Our estimator is consistent and asymptotically Gaussian. Its asymptotic variance is smaller than the one of the popular optimal bridge sampling estimator. Furthermore, it is much more robust to little overlap between the two unnormalized distributions considered. Thanks to its online definition, our procedure can be integrated in an estimation process in latent variables models, reducing the computational effort. The performances of the estimator are illustrated through a simulation study and compared to two other estimators: the ratio importance sampling and the optimal bridge sampling estimators.},
  archive      = {J_SAC},
  author       = {Guédon, Tom and Baey, Charlotte and Kuhn, Estelle},
  doi          = {10.1007/s11222-025-10664-0},
  journal      = {Statistics and Computing},
  month        = {10},
  number       = {5},
  pages        = {1-20},
  shortjournal = {Stat. Comput.},
  title        = {Estimation of ratios of normalizing constants using stochastic approximation: The SARIS algorithm},
  volume       = {35},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Individualized treatment rules based on adaptive transfer-dragonnet. <em>SAC</em>, <em>35</em>(5), 1-19. (<a href='https://doi.org/10.1007/s11222-025-10704-9'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper focuses on the inference of individualized treatment rules(ITRs) in personalized medicine, particularly addressing the challenges of covariate distribution differences and external validity encountered in real-world applications. To overcome these issues, we propose two frameworks, Transfer-Dragonnet(TDN) and Adaptive Transfer-Dragonnet(ATDN), which integrate neural networks with transfer learning. TDN corrects covariate distribution differences by estimating participation probabilities and incorporating them into a weighted learning framework. We then provide a detailed discussion of the asymptotic properties of the relevant estimator, ensuring the theoretical validity of TDN. Building upon this, ATDN further improves stability by directly optimizing transfer weights, reducing the impact of extreme propensity scores and better handling covariate shifts. Through simulations, we use mean squared error(MSE) to assess estimation accuracy and find that ATDN performs best, especially under significant covariate shifts. Furthermore, in the real-world application of the Tennessee Student/Teacher Achievement Ratio(STAR) project, ATDN derived an optimized small-class assignment strategy based on math scores, demonstrating its practical effectiveness.},
  archive      = {J_SAC},
  author       = {Huang, Zhonghe and Song, Yujia and Zhang, Qi},
  doi          = {10.1007/s11222-025-10704-9},
  journal      = {Statistics and Computing},
  month        = {10},
  number       = {5},
  pages        = {1-19},
  shortjournal = {Stat. Comput.},
  title        = {Individualized treatment rules based on adaptive transfer-dragonnet},
  volume       = {35},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). An improved bisection-type algorithm for control chart calibration. <em>SAC</em>, <em>35</em>(4), 1-13. (<a href='https://doi.org/10.1007/s11222-025-10609-7'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Calculation of the control limits is a critical step when designing control charts in statistical process control. Traditional control chart designs require the control limits to be computed so that a characteristic of the in-control run length distribution, such as the mean or median, equals a pre-determined value. When the complexity of the in-control process distribution hinders analytical methods, Monte Carlo approaches can be used to find the appropriate control limits. Among these methods, the classical bisection searching algorithm is widely used. However, a major drawback of this method is the requirement of an initial range of values for the search. Furthermore, it is computationally very demanding when multiple control charts are used simultaneously. In this paper, we present a modified bisection searching algorithm to enhance the computational efficiency. The new method eliminates the initial specification of a range for searching. Additionally, an efficient generalization of this approach is proposed to handle the multi-chart setting. Numerical results confirm that our method offers an efficient and reliable way to compute the control limits, in comparison with the conventional bisection searching algorithm and the algorithm based on stochastic approximations. A Julia computer code implementing the proposed method is provided in the supplemental materials.},
  archive      = {J_SAC},
  author       = {Zago, Daniele and Capizzi, Giovanna and Qiu, Peihua},
  doi          = {10.1007/s11222-025-10609-7},
  journal      = {Statistics and Computing},
  month        = {8},
  number       = {4},
  pages        = {1-13},
  shortjournal = {Stat. Comput.},
  title        = {An improved bisection-type algorithm for control chart calibration},
  volume       = {35},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Wiener-type integral approximation for sampling distributions of irregularly spaced spatial data. <em>SAC</em>, <em>35</em>(4), 1-19. (<a href='https://doi.org/10.1007/s11222-025-10616-8'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Resampling approaches to approximate the sampling distribution of the statistic constructed from irregularly spaced data are still far from well-developed. We propose a novel approach, the Wiener-type integral approximation (WIA), as a complement to the existing approaches. It uses a Wiener-type integral (WI) to approximate the sampling distribution of a statistic. Meanwhile, the variance of the WI is consistent to that of the statistic. The construction of the WI involves integrating a localized version of the statistic with respect to standard Gaussian white noise, offering a general class of resampling estimators based on irregularly spaced spatial data. The proposed WIA imitates the second-order dependence of multiple statistics very well, enabling it to approximate the sampling distribution of multivariate statistics that can achieve the joint asymptotic normality. In this paper, we demonstrate the applicability of WIA to various important statistics, including the sample mean, sample variance, auto-covariance estimator, and discrete Fourier transform. Moreover, through simulation studies, the finite sample performance of the WIA is investigated, in comparison with some competitive approaches.},
  archive      = {J_SAC},
  author       = {Zhang, Shibin},
  doi          = {10.1007/s11222-025-10616-8},
  journal      = {Statistics and Computing},
  month        = {8},
  number       = {4},
  pages        = {1-19},
  shortjournal = {Stat. Comput.},
  title        = {Wiener-type integral approximation for sampling distributions of irregularly spaced spatial data},
  volume       = {35},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Gradient boosting for generalised additive mixed models. <em>SAC</em>, <em>35</em>(4), 1-20. (<a href='https://doi.org/10.1007/s11222-025-10612-y'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Generalised additive mixed models are a common tool for modelling of grouped or longitudinal data where random effects are incorporated into the model in order to account for within-group or inter-individual correlations. As an alternative to established penalised maximum likelihood approaches, several different types of boosting routines have been developed to make more demanding data situations manageable. However, when estimating mixed models with component-wise gradient boosting, random and fixed effects compete within the variable selection mechanism. This can result in irregular selection properties and biased parameter estimates, particularly when covariates are constant within clusters. Moreover, while researchers typically are more interested in the covariance structure of random effects than in the effects themselves, current gradient boosting implementations focus solely on estimating the random effects. To overcome these drawbacks we propose a novel gradient boosting scheme for generalized additive mixed models. This novel approach is implemented as an R-package mermboost, seamlessly wrapped around the established mboost framework, maintaining its flexibility while enhancing functionality. The improved performance of the new framework is shown via an extensive simulation study and real world applications.},
  archive      = {J_SAC},
  author       = {Knieper, Lars and Hothorn, Torsten and Bergherr, Elisabeth and Griesbach, Colin},
  doi          = {10.1007/s11222-025-10612-y},
  journal      = {Statistics and Computing},
  month        = {8},
  number       = {4},
  pages        = {1-20},
  shortjournal = {Stat. Comput.},
  title        = {Gradient boosting for generalised additive mixed models},
  volume       = {35},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Learning from data’s evolution in normal equations: The DECK-principle and equations’ adjustment. <em>SAC</em>, <em>35</em>(4), 1-7. (<a href='https://doi.org/10.1007/s11222-025-10613-x'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {An algorithmic framework is proposed to improve stochastic equations obtained with principle/method, $$\mathcal{P}.$$ The framework is motivated by the way neurons are processing information. In each neuron, Data Evolves Creating Knowledge (DECK) transmitted to the next neuron for processing. In stochastic equations, DECK is observed when new information, i.e. obtained solutions, replace parameters and the remaining, evolved equations cannot be derived with $$\mathcal{P},$$ deviate from zero and need adjustment. In statistical inference, the DECK principle was not allowed by Fisher due to the unique distribution used in MLE. However, equations’ adjustment with an evolved distribution due to DECK improved maximum likelihood and method of moments estimates, that are either biased or inconsistent. The Neyman–Scott (Econometrica 16(1):1–32, 1948) problem for the MLE of the common variance, $$\psi ,$$ for m normal models which holds also for the moments’ estimate for all models, was not solved so far with Deep Learning, and is due to neglecting DECK and the adjustment of the $$\psi $$ -equation and not the increasing number of the m means. The evolution of static moments and likelihood equations seen as layers, will allow statisticians to see the parallel with the improved layers of equations in Deep Learning, and should be used in other stochastic optimization problems.},
  archive      = {J_SAC},
  author       = {Yatracos, Yannis G.},
  doi          = {10.1007/s11222-025-10613-x},
  journal      = {Statistics and Computing},
  month        = {8},
  number       = {4},
  pages        = {1-7},
  shortjournal = {Stat. Comput.},
  title        = {Learning from data’s evolution in normal equations: The DECK-principle and equations’ adjustment},
  volume       = {35},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). On bayesian wavelet shrinkage estimation of nonparametric regression models with stationary correlated noise. <em>SAC</em>, <em>35</em>(4), 1-14. (<a href='https://doi.org/10.1007/s11222-025-10618-6'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This work proposes a Bayesian rule based on the mixture of a point mass function at zero and the logistic distribution to perform wavelet shrinkage in nonparametric regression models with stationary errors (with short or long-memory behavior). The proposal is assessed through Monte Carlo experiments and illustrated with real data. Simulation studies indicate that the precision of the estimates decreases as the amount of correlation increases. However, given a sample size and error correlated noise, the performance of the rule is almost the same while the signal-to-noise ratio decreases, compared to the performance of the rule under independent and identically distributed errors. Further, we find that the performance of the proposal is better than the standard soft thresholding rule with universal policy in most of the considered underlying functions, sample sizes and signal-to-noise ratios scenarios.},
  archive      = {J_SAC},
  author       = {dos S. Sousa, Alex Rodrigo and Zevallos, Mauricio},
  doi          = {10.1007/s11222-025-10618-6},
  journal      = {Statistics and Computing},
  month        = {8},
  number       = {4},
  pages        = {1-14},
  shortjournal = {Stat. Comput.},
  title        = {On bayesian wavelet shrinkage estimation of nonparametric regression models with stationary correlated noise},
  volume       = {35},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Nonlinear principal component analysis for two-dimensional functional data using neural networks. <em>SAC</em>, <em>35</em>(4), 1-12. (<a href='https://doi.org/10.1007/s11222-025-10619-5'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Principal component analysis for two-dimensional functional data is a crucial issue in functional data analysis. Most existing methods rely on Karhunen-Loève expansion, which assumes linear structure of the data and may become inefficient when data deviates from the linear assumption. To address this issue, we propose a novel neural network model for conducting principal component analysis on two-dimensional functional data with nonlinear structures. Compared with conventional neural network, our model incorporates weight and bias functions to maintain the characteristics of functional data. Our proposed model eliminates the need to compute the high-cost four-dimensional covariance function. Additionally, we explore the universal approximation property of the proposed model and conduct a simulation study to evaluate its performance. The proposed method is also applied to a real-world dataset to further demonstrate its superiority.},
  archive      = {J_SAC},
  author       = {Chen, Xinran and Zhang, Jingxiao and Zhong, Rou},
  doi          = {10.1007/s11222-025-10619-5},
  journal      = {Statistics and Computing},
  month        = {8},
  number       = {4},
  pages        = {1-12},
  shortjournal = {Stat. Comput.},
  title        = {Nonlinear principal component analysis for two-dimensional functional data using neural networks},
  volume       = {35},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Two-level D- and A-optimal main-effects designs with run sizes one and two more than a multiple of four. <em>SAC</em>, <em>35</em>(4), 1-15. (<a href='https://doi.org/10.1007/s11222-025-10604-y'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {For run sizes that are a multiple of four, the literature offers many two-level designs that are D- and A-optimal for the main-effects model and minimize the aliasing between main effects and interaction effects and among interaction effects. For run sizes that are not a multiple of four, no conclusive results are known. In this paper, we propose two algorithms that generate all non-isomorphic D- and A-optimal main-effects designs for run sizes that are one and two more than a multiple of four. We enumerate all such designs for run sizes up to 18, report the numbers of designs we obtained, and identify those that minimize the aliasing between main effects and interaction effects and among interaction effects. Finally, we compare the minimally aliased designs we found with benchmark designs from the literature.},
  archive      = {J_SAC},
  author       = {Hameed, Mohammed Saif Ismail and Núñez Ares, José and Schoen, Eric D. and Goos, Peter},
  doi          = {10.1007/s11222-025-10604-y},
  journal      = {Statistics and Computing},
  month        = {8},
  number       = {4},
  pages        = {1-15},
  shortjournal = {Stat. Comput.},
  title        = {Two-level D- and A-optimal main-effects designs with run sizes one and two more than a multiple of four},
  volume       = {35},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Distributed empirical likelihood inference with privacy guarantees. <em>SAC</em>, <em>35</em>(4), 1-27. (<a href='https://doi.org/10.1007/s11222-025-10614-w'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the rapid advancement in information technology, data analysis has become increasingly vital in various fields. Balancing the utility of data while protecting individual privacy has become a hot topic for both academic research and practical applications. As a technology that can provide strict privacy guarantees, differential privacy has attracted widespread attention in recent years. In this paper, we study statistical inference for differentially private data based on empirical likelihood. Specifically, we develop two novel privacy-preserving-based statistical inference methods, including differentially private distributed empirical likelihood and balanced augmented differentially private distributed empirical likelihood. Under some mild conditions, the asymptotic properties of the proposed methods are derived. We also illustrate the finite sample performance of the proposed approaches via simulation studies and real data analysis.},
  archive      = {J_SAC},
  author       = {Liu, Qianqian and Li, Zhouping},
  doi          = {10.1007/s11222-025-10614-w},
  journal      = {Statistics and Computing},
  month        = {8},
  number       = {4},
  pages        = {1-27},
  shortjournal = {Stat. Comput.},
  title        = {Distributed empirical likelihood inference with privacy guarantees},
  volume       = {35},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Graph sub-sampling for divide-and-conquer algorithms in large networks. <em>SAC</em>, <em>35</em>(4), 1-17. (<a href='https://doi.org/10.1007/s11222-025-10620-y'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As networks continue to increase in size, current methods must be capable of handling large numbers of nodes and edges in order to be practically relevant. Instead of working directly with the entire (large) network, analyzing sub-networks has become a popular approach. Due to a network’s inherent inter-connectedness, however, sub-sampling is not a trivial task. While this problem has gained popularity in recent years, it has not received sufficient attention from the statistics community. In this work, we provide a thorough comparison of seven graph sub-sampling algorithms by applying them to divide-and-conquer algorithms for community structure and core-periphery (CP) structure. After discussing the various algorithms and sub-sampling routines, we derive theoretical results for the mis-classification rate of the divide-and-conquer algorithm for CP structure under various sub-sampling schemes. We then perform extensive experiments on both simulated and real-world data to compare the various methods. For the community detection task, we found that sampling nodes uniformly at random yields the best performance, but that sometimes the base algorithm applied to the entire network yields better results both in terms of identification and computational time. For CP structure on the other hand, there was no single winner, but algorithms which sampled core nodes at a higher rate consistently outperformed other sampling routines, e.g., random edge sampling and random walk sampling. Unlike community detection, the CP divide-and-conquer algorithm tends to yield better identification results while also being faster than the base algorithm. The varying performance of the sampling algorithms on different tasks demonstrates the importance of carefully selecting a sub-sampling routine for the specific application.},
  archive      = {J_SAC},
  author       = {Yanchenko, Eric},
  doi          = {10.1007/s11222-025-10620-y},
  journal      = {Statistics and Computing},
  month        = {8},
  number       = {4},
  pages        = {1-17},
  shortjournal = {Stat. Comput.},
  title        = {Graph sub-sampling for divide-and-conquer algorithms in large networks},
  volume       = {35},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Estimation of two-layer gaussian mixture model for streaming longitudinal data in bayesian framework. <em>SAC</em>, <em>35</em>(4), 1-16. (<a href='https://doi.org/10.1007/s11222-025-10621-x'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the development of big data techniques, the difficulty that streaming longitudinal data lack theoretical support in modeling has become increasingly prominent. This paper proposes a two-layer Gaussian mixture model within a Bayesian framework to theoretically fit such data by characterizing their nature, such as being correlated, time-varying, and velocitous. Under the assumptions of first-lag autocorrelation and dependency, the conditional posterior probability, posterior distributions of the parameters, and posterior expectations are estimated for the purpose of statistical inference. These theoretical results are effectively validated through simulations under different hyper-parameter values, model estimation structures, as well as in a real data where real-time classification is conducted for heart disease diagnosis using heart sound signals. The model performance is also explored in cases where assumptions are violated in the simulation. The results show that the estimated theoretical statistical inference is precise under the assumptions. Specifically, the proposed model structure is more effective in characterizing the data compared with structures that do not consider their autocorrelation. Data with different location parameters are classified with high accuracy compared with those with differences only in scale parameters. The model performance is generally guaranteed under appropriate noise level. In contrast, the parameter estimation performance is suboptimal when the stated assumptions are violated by the data.},
  archive      = {J_SAC},
  author       = {Zhao, Xin and Nie, Xiaokai},
  doi          = {10.1007/s11222-025-10621-x},
  journal      = {Statistics and Computing},
  month        = {8},
  number       = {4},
  pages        = {1-16},
  shortjournal = {Stat. Comput.},
  title        = {Estimation of two-layer gaussian mixture model for streaming longitudinal data in bayesian framework},
  volume       = {35},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). High order expansion method for kuiper’s $$V_n$$ statistic in goodness-of-fit test. <em>SAC</em>, <em>35</em>(4), 1-19. (<a href='https://doi.org/10.1007/s11222-025-10623-9'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Kuiper’s $$V_n$$ statistic, a measure for comparing the difference of ideal distribution and empirical distribution, is of great significance in the goodness-of-fit test. However, Kuiper’s formulae for computing the cumulative distribution function, false positive probability, and the upper tail quantile of $$V_n$$ cannot be applied to the case of small sample capacity n since the approximation error is $$\mathcal {O}\left( n^{-1}\right) $$ . In this work, our contributions lie in three perspectives: firstly the approximation error is reduced to $$\mathcal {O}\left( n^{-(k+1)/2}\right) $$ where k is the expansion order with the high order expansion for the exponent of the differential operator; secondly, a novel high order formula with approximation error $$\mathcal {O}\left( n^{-3}\right) $$ is obtained by massive calculations; thirdly, the fixed-point algorithms are designed for solving the Kuiper pair of critical values and upper tail quantiles based on the novel formula. The high order expansion method for Kuiper’s $$V_n$$ statistic is applicable for various applications where there are more than five samples of data. The principles, algorithms, and code for the high order expansion method are attractive for the goodness-of-fit test.},
  archive      = {J_SAC},
  author       = {Zhang, Hong-Yan and Feng, Zhi-Qiang and Liu, Haoting and Lin, Rui-Jia and Zhou, Yu},
  doi          = {10.1007/s11222-025-10623-9},
  journal      = {Statistics and Computing},
  month        = {8},
  number       = {4},
  pages        = {1-19},
  shortjournal = {Stat. Comput.},
  title        = {High order expansion method for kuiper’s $$V_n$$ statistic in goodness-of-fit test},
  volume       = {35},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A sparse PAC-bayesian approach for high-dimensional quantile prediction. <em>SAC</em>, <em>35</em>(4), 1-21. (<a href='https://doi.org/10.1007/s11222-025-10617-7'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Quantile regression, a robust method for estimating conditional quantiles, has advanced significantly in fields such as econometrics, statistics and machine learning. In high-dimensional settings, where the number of covariates exceeds sample size, penalized methods like lasso have been developed to address sparsity challenges. Bayesian methods, initially connected to quantile regression via the asymmetric Laplace likelihood, have also evolved, though issues with posterior variance have led to new approaches, including pseudo/score likelihoods. This paper presents a novel probabilistic machine learning approach for high-dimensional quantile prediction. It uses a pseudo-Bayesian framework with a scaled Student-t prior and Langevin Monte Carlo for efficient computation. The method demonstrates strong theoretical guarantees, through PAC-Bayes bounds, that establish non-asymptotic oracle inequalities, showing minimax-optimal prediction error and adaptability to unknown sparsity. Its effectiveness is validated through simulations and real-world data, where it performs competitively against established frequentist and Bayesian techniques.},
  archive      = {J_SAC},
  author       = {Mai, The Tien},
  doi          = {10.1007/s11222-025-10617-7},
  journal      = {Statistics and Computing},
  month        = {8},
  number       = {4},
  pages        = {1-21},
  shortjournal = {Stat. Comput.},
  title        = {A sparse PAC-bayesian approach for high-dimensional quantile prediction},
  volume       = {35},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Mixture cure semiparametric additive hazard models under partly interval censoring — A penalized likelihood approach. <em>SAC</em>, <em>35</em>(4), 1-18. (<a href='https://doi.org/10.1007/s11222-025-10622-w'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Survival analysis can sometimes involve individuals who will not experience the event of interest, forming what is known as the “cured group”. Identifying such individuals is not always possible beforehand, as they provide only right-censored data. Ignoring the presence of the cured group can introduce bias in the final model. This paper presents a method for estimating a semiparametric additive hazards model that accounts for the cured fraction. Unlike regression coefficients in a hazard ratio model, those in an additive hazard model measure hazard differences. The proposed method uses a primal-dual interior point algorithm to obtain constrained maximum penalized likelihood estimates of the model parameters, including the regression coefficients and the baseline hazard, subject to certain non-negativity constraints.},
  archive      = {J_SAC},
  author       = {Li, Jinqing and Ma, Jun},
  doi          = {10.1007/s11222-025-10622-w},
  journal      = {Statistics and Computing},
  month        = {8},
  number       = {4},
  pages        = {1-18},
  shortjournal = {Stat. Comput.},
  title        = {Mixture cure semiparametric additive hazard models under partly interval censoring — A penalized likelihood approach},
  volume       = {35},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). On sufficient dimension reduction methods based on a graphical model with non-concave penalty. <em>SAC</em>, <em>35</em>(4), 1-30. (<a href='https://doi.org/10.1007/s11222-025-10625-7'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Ultra high-dimensional datasets, which refer to scenarios where the number of covariates grows at an exponential rate relative to the sample size, are frequently encountered in modern data analysis across fields such as genomics, finance, and social sciences. These datasets pose significant challenges due to the large number of variables relative to the number of observations, potentially resulting in issues such as multicollinearity, overfitting, and computational difficulties. Traditional sufficient dimension reduction (SDR) methods struggle with these challenges, making it necessary to develop new approaches. To address these limitations, we introduce a graphical model-based SDR method that incorporates a smoothly clipped absolute deviation (SCAD) penalty. This method effectively reduces dimensionality while managing sparsity in the dataset. Additionally, we extend directional regression for high-dimensional data by integrating them with graphical LASSO, which enhances the model’s ability to estimate sparse precision matrices. This combined approach not only mitigates computational infeasibility in estimating covariance matrices but also helps avoid overfitting, making it particularly suitable for high-dimensional contexts. Through extensive simulation studies and real-world data analyses, we validate the robustness and effectiveness of our proposed methods. Moreover, we provide a theoretical framework that discusses the convergence rate of these methods, offering insights into their performance under various conditions. Finally, we outline potential avenues for future research, including exploring alternative penalty functions and expanding the applicability of these methods to other types of data structures.},
  archive      = {J_SAC},
  author       = {Park, Yujin and Kim, Kyongwon and Yoo, Jae Keun},
  doi          = {10.1007/s11222-025-10625-7},
  journal      = {Statistics and Computing},
  month        = {8},
  number       = {4},
  pages        = {1-30},
  shortjournal = {Stat. Comput.},
  title        = {On sufficient dimension reduction methods based on a graphical model with non-concave penalty},
  volume       = {35},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Estimating the number of true null hypotheses based on change point of observed p values. <em>SAC</em>, <em>35</em>(4), 1-18. (<a href='https://doi.org/10.1007/s11222-025-10632-8'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The estimation of the number of null hypotheses is closely related to error rate control process such as the Family-Wise Error Rate (FWER) and the False Discovery Rate (FDR), in multiple hypothesis testing problems. Several methods have been proposed to address this challenge, including the p value graph type methods. In this paper, we introduce a novel p value graph type method for estimating the number of true null hypotheses. Our method constructs a mean-change point model using the differences of the adjacent order p values, and applies the CUSUM statistic to detect potential change point. Based on the change point detection result, the number of true null hypotheses is estimated. Various simulation studies and a real data application are conducted with comparison to other existing methods. The numerical results demonstrate that the new estimating method is robust and achieves superior performance across various scenarios. It is versatile and can handle diverse settings, including cases where all hypotheses are null, all are alternative, or only a subset are null hypotheses.},
  archive      = {J_SAC},
  author       = {Jin, Lishuai and Fang, Hongyan and Yang, Wenzhi},
  doi          = {10.1007/s11222-025-10632-8},
  journal      = {Statistics and Computing},
  month        = {8},
  number       = {4},
  pages        = {1-18},
  shortjournal = {Stat. Comput.},
  title        = {Estimating the number of true null hypotheses based on change point of observed p values},
  volume       = {35},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Nonlinear functional sufficient dimension reduction via principal fitted components. <em>SAC</em>, <em>35</em>(4), 1-19. (<a href='https://doi.org/10.1007/s11222-025-10633-7'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we propose a novel functional nonlinear sufficient dimension reduction method based on the principal fitted component model. Our approach extends the concept of principal fitted components to functional data, covering the case where both the predictors and responses are functions. We consider a general framework in which the predictor and response can each be viewed as elements of potentially infinite dimensional Hilbert spaces. This includes the important scalar on function and function on function cases as special instances. We generalize a nonlinear principal fitted component model within the framework of reproducing kernel Hilbert space, leveraging the nested Hilbert spaces theory to characterize nonlinear structures in functional data. The first space accommodates functions of random curves and the second space captures their nonlinear relationships. To establish the theoretical validity of our approach, we develop an asymptotic theory that characterizes the convergence behavior of the proposed estimator under mild regularity conditions. Extensive simulation studies demonstrate that our method outperforms existing functional sufficient dimension reduction methods, particularly in scenarios with complex nonlinear dependencies. The effectiveness of the proposed method is further validated through real data analysis.},
  archive      = {J_SAC},
  author       = {Kim, Minjee and Park, Yujin and Kim, Kyongwon and Yoo, Jae Keun},
  doi          = {10.1007/s11222-025-10633-7},
  journal      = {Statistics and Computing},
  month        = {8},
  number       = {4},
  pages        = {1-19},
  shortjournal = {Stat. Comput.},
  title        = {Nonlinear functional sufficient dimension reduction via principal fitted components},
  volume       = {35},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Extended fiducial inference for individual treatment effects via deep neural networks. <em>SAC</em>, <em>35</em>(4), 1-16. (<a href='https://doi.org/10.1007/s11222-025-10624-8'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Individual treatment effect estimation has gained significant attention in recent data science literature. This work introduces the Double Neural Network (Double-NN) method to address this problem within the framework of extended fiducial inference (EFI). In the proposed method, deep neural networks are used to model the treatment and control effect functions, while an additional neural network is employed to estimate their parameters. The universal approximation capability of deep neural networks ensures the broad applicability of this method. Numerical results highlight the superior performance of the proposed Double-NN method compared to the conformal quantile regression (CQR) method in individual treatment effect estimation. From the perspective of statistical inference, this work advances the theory and methodology for statistical inference of large models. Specifically, it is theoretically proven that the proposed method permits the model size to increase with the sample size n at a rate of $$O(n^{\zeta })$$ for some $$0 \le \zeta <1$$ , while still maintaining proper quantification of uncertainty in the model parameters. This result marks a significant improvement compared to the range $$0\le \zeta < \frac{1}{2}$$ required by the classical central limit theorem. Furthermore, this work provides a rigorous framework for quantifying the uncertainty of deep neural networks under the neural scaling law, representing a substantial contribution to the statistical understanding of large-scale neural network models.},
  archive      = {J_SAC},
  author       = {Kim, Sehwan and Liang, Faming},
  doi          = {10.1007/s11222-025-10624-8},
  journal      = {Statistics and Computing},
  month        = {8},
  number       = {4},
  pages        = {1-16},
  shortjournal = {Stat. Comput.},
  title        = {Extended fiducial inference for individual treatment effects via deep neural networks},
  volume       = {35},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multivariate two-sample tests via random projection. <em>SAC</em>, <em>35</em>(4), 1-21. (<a href='https://doi.org/10.1007/s11222-025-10627-5'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The two-sample test problem is a fundamental problem in statistical inference that attempts to test whether two probability measures are different based on corresponding samples. Consequently, many statistical methods have been proposed when random vectors are multivariate or even high-dimensional. For this problem, we introduce a randomly projected maximum mean discrepancy (MMD) in a reproducing kernel Hilbert space to characterize the distance between the distributions of two random vectors. The multivariate random vectors are projected onto univariate random variables and projected MMD statistics are constructed. The collection of projected MMD indexed by the unit sphere, and hence we treat it as the U-process. Theorems include the asymptotic theory of test statistics under the null hypothesis and the alternative hypothesis. Combining continuous mapping theory with projected MMD statistics, a class of test statistics is proposed, which includes the Cramér-von Mises and Kolmogorov-Smirnov methods as special cases. Since the limit null distribution of the test statistic depends on the data generation process, we apply the permutation test procedure to determine a critical value. Furthermore, the empirical size and power of the test statistics are evaluated by numerical simulations. Finally, we illustrate our method by applying it to real data sets with two-sample problem.},
  archive      = {J_SAC},
  author       = {Shi, Xiangyu and Du, Jiang and Xie, Tianfa},
  doi          = {10.1007/s11222-025-10627-5},
  journal      = {Statistics and Computing},
  month        = {8},
  number       = {4},
  pages        = {1-21},
  shortjournal = {Stat. Comput.},
  title        = {Multivariate two-sample tests via random projection},
  volume       = {35},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). An online updating approach for estimating and testing mediation effects with big data streams. <em>SAC</em>, <em>35</em>(4), 1-17. (<a href='https://doi.org/10.1007/s11222-025-10636-4'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The use of mediation analysis has become increasingly popular across various research fields in recent years. The primary objective of mediation analysis is to examine the indirect effects along the pathways from exposure to outcome. Meanwhile, the advent of data collection technology has sparked a surge of interest in the burgeoning field of big data analysis, where mediation analysis of streaming data sets has recently garnered significant attention. The enormity of the data, however, results in an augmented computational burden. The present study proposes an online updating approach to address this issue, aiming to estimate and test mediation effects in the context of linear and logistic mediation models with massive data streams. The proposed algorithm significantly enhances the computational efficiency of Sobel test, adjusted Sobel test, joint significance test, and adjusted joint significance test. This study also investigates the adjusted Sobel-type confidence interval for mediation effect within the framework of streaming data. We conduct a substantial number of simulations to evaluate the performance of the proposed method. Two real-world examples are employed to showcase the practical applicability of this approach.},
  archive      = {J_SAC},
  author       = {Bai, Xueyan and Zhang, Haixiang},
  doi          = {10.1007/s11222-025-10636-4},
  journal      = {Statistics and Computing},
  month        = {8},
  number       = {4},
  pages        = {1-17},
  shortjournal = {Stat. Comput.},
  title        = {An online updating approach for estimating and testing mediation effects with big data streams},
  volume       = {35},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Two-armed bandit bootstrap for model-free equivalent rank test. <em>SAC</em>, <em>35</em>(4), 1-18. (<a href='https://doi.org/10.1007/s11222-025-10634-6'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Two-sample test is used to identify significant differences between two samples, such as differences in their expectations, variances or overall distribution shapes. This paper focuses on a specific problem: testing whether the difference between $$P(X > Y)$$ and $$P(X < Y)$$ exceeds a predefined equivalence margin, through a novel Equivalent Rank Test. Traditional methods based on asymptotic normality are limited by their reliance on a narrow subset of information, failing to fully exploit the potential of finite samples. To address this, we propose a sequential bootstrap sampling approach inspired by a two-armed bandit (TAB) process, which adaptively constructs test statistics. By minimizing bias and variance simultaneously, this adaptive strategy achieves higher power compared to classical fixed-sampling methods, particularly in small or unbalanced samples. Theoretical analysis and extensive simulations confirm the superior performance of the proposed method. Furthermore, an empirical analysis of students’ sleep data highlights both the practical applicability and the advantage of the proposed approach in real-world scenarios.},
  archive      = {J_SAC},
  author       = {Xu, Shuoxun and Yan, Xiaodong and Zhang, Zhaoang},
  doi          = {10.1007/s11222-025-10634-6},
  journal      = {Statistics and Computing},
  month        = {8},
  number       = {4},
  pages        = {1-18},
  shortjournal = {Stat. Comput.},
  title        = {Two-armed bandit bootstrap for model-free equivalent rank test},
  volume       = {35},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Uncertainty quantification for linear inverse problems with besov prior: A randomize-then-optimize method. <em>SAC</em>, <em>35</em>(4), 1-16. (<a href='https://doi.org/10.1007/s11222-025-10638-2'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this work, we investigate the use of Besov priors in the context of Bayesian inverse problems. The solution to Bayesian inverse problems is the posterior distribution which naturally enables us to interpret the uncertainties. Besov priors are discretization invariant and can promote sparsity in terms of wavelet coefficients. We propose the randomize-then-optimize method to draw samples from the posterior distribution with Besov priors under a general parameter setting and estimate the modes of the posterior distribution. The performance of the proposed method is studied through numerical experiments of a 1D inpainting problem, a 1D deconvolution problem, and a 2D computed tomography problem. Further, we discuss the influence of the choice of the Besov parameters and the wavelet basis in detail, and we compare the proposed method with the state-of-the-art methods. The numerical results suggest that the proposed method is an effective tool for sampling the posterior distribution equipped with general Besov priors.},
  archive      = {J_SAC},
  author       = {Horst, Andreas and Maboudi Afkham, Babak and Dong, Yiqiu and Lemvig, Jakob},
  doi          = {10.1007/s11222-025-10638-2},
  journal      = {Statistics and Computing},
  month        = {8},
  number       = {4},
  pages        = {1-16},
  shortjournal = {Stat. Comput.},
  title        = {Uncertainty quantification for linear inverse problems with besov prior: A randomize-then-optimize method},
  volume       = {35},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Mean-restricted matrix-variate normals with an application to clustering. <em>SAC</em>, <em>35</em>(4), 1-20. (<a href='https://doi.org/10.1007/s11222-025-10641-7'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This work introduces different approaches to model mean structures in matrix-variate normals, addressing the over-parameterization issue commonly encountered when exploiting these distributions in the context of model-based clustering. The methodology employs parsimonious parameterizations of the means using additive, interaction and/or polynomial terms to reveal the intricate multivariate interdependence underlying the data’s mean structure. Identifiability issues related to the proposed parameterizations are discussed and expressions to compute maximum likelihood estimates of the parameters of the resulting mean-restricted matrix normal are derived. In order to exploit the proposed parameterizations in a model-based clustering setting, finite mixtures of mean-restricted matrix normals are considered. Integrating structured covariance matrices, the approach maintains model flexibility without succumbing to overfitting. An Expectation-Maximization (EM) algorithm is developed to estimate all model parameters. Through a comprehensive simulation study and a real-world example on climate data, the efficacy of the proposed solutions in capturing complex data relationships is demonstrated, offering significant improvements over traditional methods.},
  archive      = {J_SAC},
  author       = {Berrettini, Marco and Galimberti, Giuliano and Viroli, Cinzia},
  doi          = {10.1007/s11222-025-10641-7},
  journal      = {Statistics and Computing},
  month        = {8},
  number       = {4},
  pages        = {1-20},
  shortjournal = {Stat. Comput.},
  title        = {Mean-restricted matrix-variate normals with an application to clustering},
  volume       = {35},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Local polynomial $$L_p$$ -norm regression. <em>SAC</em>, <em>35</em>(4), 1-13. (<a href='https://doi.org/10.1007/s11222-025-10635-5'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The local least squares estimator for a regression curve cannot provide optimal results when non-Gaussian noise is present. Both theoretical and empirical evidence suggests that residuals often exhibit distributional properties different from those of a normal distribution, making it worthwhile to consider estimation based on other norms. It is suggested that $$L_p$$ -norm estimators be used to minimize the residuals when these exhibit non-normal kurtosis. In this paper, we propose a local polynomial $$L_p$$ -norm regression that replaces weighted least squares estimation with weighted $$L_p$$ -norm estimation for fitting the polynomial locally. We also introduce a new method for estimating the parameter p from the residuals, enhancing the adaptability of the approach. Through numerical and theoretical investigation, we demonstrate our method’s superiority over local least squares in one-dimensional data and show promising outcomes for higher dimensions, specifically in 2D.},
  archive      = {J_SAC},
  author       = {Tazik, Ladan and Stafford, James and Braun, W. John},
  doi          = {10.1007/s11222-025-10635-5},
  journal      = {Statistics and Computing},
  month        = {8},
  number       = {4},
  pages        = {1-13},
  shortjournal = {Stat. Comput.},
  title        = {Local polynomial $$L_p$$ -norm regression},
  volume       = {35},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Quantile least squares: A flexible approach for robust estimation and validation of location-scale families. <em>SAC</em>, <em>35</em>(4), 1-21. (<a href='https://doi.org/10.1007/s11222-025-10626-6'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, the problem of robust estimation and validation of location-scale families is revisited. The proposed methods exploit the joint asymptotic normality of sample quantiles (of i.i.d. random variables) to construct the ordinary and generalized least squares estimators of location and scale parameters. These quantile least squares (QLS) estimators are easy to compute because they have explicit expressions, their robustness is achieved by excluding extreme quantiles from the least-squares estimation, and efficiency is boosted by using as many non-extreme quantiles as practically relevant. The influence functions of the QLS estimators are specified and plotted for several location-scale families. They closely resemble the shapes of some well-known influence functions yet those shapes emerge automatically (i.e., do not need to be specified). The joint asymptotic normality of the proposed estimators is established and their finite-sample properties are explored using simulations. Also, computational costs of these estimators, as well as those of MLE, are evaluated for sample sizes $$n = 10^6, 10^7, 10^8, 10^9$$ . For model validation, two goodness-of-fit tests are constructed and their performance is studied using simulations and real data. In particular, for the daily stock returns of Google over the years 2020-2023, both tests strongly support the logistic distribution assumption and reject other bell-shaped competitors.},
  archive      = {J_SAC},
  author       = {Adjieteh, Mohammed and Brazauskas, Vytaras},
  doi          = {10.1007/s11222-025-10626-6},
  journal      = {Statistics and Computing},
  month        = {8},
  number       = {4},
  pages        = {1-21},
  shortjournal = {Stat. Comput.},
  title        = {Quantile least squares: A flexible approach for robust estimation and validation of location-scale families},
  volume       = {35},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Penalized quadratic inference functions estimation for fixed effects partially linear single index spatial error model. <em>SAC</em>, <em>35</em>(4), 1-20. (<a href='https://doi.org/10.1007/s11222-025-10630-w'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper focuses on studying fixed effects partially linear single index spatial error model with correlated structure within individuals. By combining spline basis function approximation, variable transformation, penalty function and quadratic inference function, penalized quadratic inference functions estimators for unknowns are constructed. By appropriately selecting tuning parameters, we derive that the parametric estimators satisfy consistency and asymptotic normality, and the nonparametric estimator has the optimal convergence rate. Numerical simulation implies the estimates have excellent small sample performance. The proposed model is applied to analyze the driving forces of China’s provincial digital economy development.},
  archive      = {J_SAC},
  author       = {Li, Fen and Chen, Hao},
  doi          = {10.1007/s11222-025-10630-w},
  journal      = {Statistics and Computing},
  month        = {8},
  number       = {4},
  pages        = {1-20},
  shortjournal = {Stat. Comput.},
  title        = {Penalized quadratic inference functions estimation for fixed effects partially linear single index spatial error model},
  volume       = {35},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Numerical generalized randomized HMC processes for restricted domains. <em>SAC</em>, <em>35</em>(4), 1-25. (<a href='https://doi.org/10.1007/s11222-025-10637-3'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a generic approach for numerically efficient simulation from analytically intractable distributions with constrained support. Our approach relies upon Generalized Randomized Hamiltonian Monte Carlo (GRHMC) processes and combines these with a randomized transition kernel that appropriately adjusts the Hamiltonian flow at the boundary of the constrained domain, ensuring that it remains within the domain. The numerical implementation of this constrained GRHMC process exploits the sparsity of the randomized transition kernel and the specific structure of the constraints so that the proposed approach is numerically accurate, computationally fast and operational even in high-dimensional applications. We illustrate this approach with posterior distributions of several Bayesian models with challenging parameter domain constraints in applications to real-word data sets. Building on the capability of GRHMC processes to efficiently explore otherwise challenging and high-dimensional posteriors, the proposed method expands the set of Bayesian models that can be analyzed by using the standard Markov-Chain Monte-Carlo (MCMC) methodology, As such, it can advance the development and use of Bayesian models with useful constrained priors, which are difficult to handle with existing methods. The article is accompanied by an R-package ( https://github.com/torekleppe/pdmphmc ), which allows for automatically implementing GRHMC processes for arbitrary target distributions and domain constraints.},
  archive      = {J_SAC},
  author       = {Kleppe, Tore Selland and Liesenfeld, Roman},
  doi          = {10.1007/s11222-025-10637-3},
  journal      = {Statistics and Computing},
  month        = {8},
  number       = {4},
  pages        = {1-25},
  shortjournal = {Stat. Comput.},
  title        = {Numerical generalized randomized HMC processes for restricted domains},
  volume       = {35},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Sequential bayesian registration for functional data. <em>SAC</em>, <em>35</em>(4), 1-20. (<a href='https://doi.org/10.1007/s11222-025-10640-8'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In many modern applications, discretely-observed data may be naturally understood as a set of functions. Functional data often exhibit two confounded sources of variability: amplitude (y-axis) and phase (x-axis). The extraction of amplitude and phase, a process known as registration, is essential in exploring the underlying structure of functional data in a variety of areas, from environmental monitoring to medical imaging. Critically, such data are often gathered sequentially with new functional observations arriving over time. Despite this, existing registration procedures do not sequentially update inference based on the new data, requiring model refitting. To address these challenges, we introduce a Bayesian framework for sequential registration of functional data, which updates statistical inference as new sets of functions are assimilated. This Bayesian model-based sequential learning approach utilizes sequential Monte Carlo sampling to recursively update the alignment of observed functions while accounting for associated uncertainty. Distributed computing significantly reduces computational cost relative to refitting the model using an iterative method such as Markov chain Monte Carlo on the full data. Simulation studies and comparisons reveal that the proposed approach performs well even when the target posterior distribution has a challenging structure. We apply the proposed method to three real datasets: (1) functions of annual drought intensity near Kaweah River in California, (2) annual sea surface salinity functions near Null Island, and (3) a sequence of repeated patterns in electrocardiogram signals.},
  archive      = {J_SAC},
  author       = {Kim, Yoonji and Chkrebtii, Oksana A. and Kurtek, Sebastian A.},
  doi          = {10.1007/s11222-025-10640-8},
  journal      = {Statistics and Computing},
  month        = {8},
  number       = {4},
  pages        = {1-20},
  shortjournal = {Stat. Comput.},
  title        = {Sequential bayesian registration for functional data},
  volume       = {35},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Bayesian quantile regression model with linear inequality constraints. <em>SAC</em>, <em>35</em>(4), 1-24. (<a href='https://doi.org/10.1007/s11222-025-10646-2'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Linear inequality constraints enhance the precision of parameter estimation by constraining parameters to a more confined space. This paper introduces a Bayesian quantile regression model with linear inequality constraints. To rigorously enforce these inequality constraints within the Bayesian framework, we adopt truncated prior distributions. Leveraging asymmetric Laplace distributions, we propose two novel Gibbs sampling methods based on truncated normal and truncated Laplace prior distributions, which involve sampling from truncated normal and generalized inverse Gaussian distributions, respectively. Our approach extends the constraints from solely inequality-based to a comprehensive framework incorporating both equality and inequality constraints. Simulation studies demonstrate that the proposed method outperforms traditional quantile regression without constraints across various error distributions. Furthermore, we apply this method to analyze non-performing loan ratios and corn yield data, showcasing its practical applicability.},
  archive      = {J_SAC},
  author       = {Yu, Jialei and Dai, Jun and Xu, Min and Ullah, Sami},
  doi          = {10.1007/s11222-025-10646-2},
  journal      = {Statistics and Computing},
  month        = {8},
  number       = {4},
  pages        = {1-24},
  shortjournal = {Stat. Comput.},
  title        = {Bayesian quantile regression model with linear inequality constraints},
  volume       = {35},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Uncertainty quantification in bayesian reduced-rank sparse regressions. <em>SAC</em>, <em>35</em>(4), 1-19. (<a href='https://doi.org/10.1007/s11222-025-10629-3'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Reduced-rank regression recognises the possibility of a rank-deficient matrix of coefficients. We propose a novel Bayesian model for estimating the rank of the coefficient matrix, which obviates the need for post-processing steps and allows for uncertainty quantification. Our method employs a mixture prior on the regression coefficient matrix along with a global-local shrinkage prior on its low-rank decomposition. Then, we rely on the Signal Adaptive Variable Selector to perform sparsification and define two novel tools: the Posterior Inclusion Probability uncertainty index and the Relevance Index. The validity of the method is assessed in a simulation study, and then its advantages and usefulness are shown in real-data applications on the chemical composition of tobacco and on the photometry of galaxies.},
  archive      = {J_SAC},
  author       = {Pintado, Maria F. and Iacopini, Matteo and Rossini, Luca and Shestopaloff, Alexander Y.},
  doi          = {10.1007/s11222-025-10629-3},
  journal      = {Statistics and Computing},
  month        = {8},
  number       = {4},
  pages        = {1-19},
  shortjournal = {Stat. Comput.},
  title        = {Uncertainty quantification in bayesian reduced-rank sparse regressions},
  volume       = {35},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). The second–derivative lower–bound function (SeLF) algorithm and three acceleration techniques for maximization with strongly stable convergence. <em>SAC</em>, <em>35</em>(4), 1-17. (<a href='https://doi.org/10.1007/s11222-025-10639-1'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper proposes a new maximization method, called as the second–derivative lower–bound function (SeLF) algorithm, which is a general principle for iteratively calculating the maximum likelihood estimate (MLE) $$\hat{\theta }$$ of the parameter $$\theta $$ in a one–dimensional target function $$\ell (\theta )$$ (usually, the log-likelihood or marginal log-likelihood for multi-parameter cases), and its each iteration consists of two steps: A second–derivative lower–bound function step (SeLF-step) and a maximization step (M-step). The SeLF-step finds a function $$b(\theta )$$ satisfying $$\ell ''(\theta )\geqslant b(\theta )$$ and constructs a surrogate function $$Q(\theta |\theta ^{(t)})$$ [whose form depends on $$\theta ^{(t)}$$ being the t-th iteration of $$\hat{\theta }$$ ] minorizing $$\ell (\theta )$$ at $$\theta =\theta ^{(t)}$$ . The M-step calculates the maximizer $$\theta ^{(t+1)}$$ of the $$Q(\theta |\theta ^{(t)})$$ function, which is equivalent to solving the equation $$\ell '(\theta ) + \int _{\theta ^{(t)}}^{\theta } b(z) \,\text {d}z =0$$ to obtain its explicit solution $$\theta ^{(t+1)}$$ . The SeLF algorithm holds two major advantages: (i) it strongly stably converges to the MLE $$\hat{\theta }$$ , in contrast to the weakly stable convergence of minorization–maximization (MM) algorithms; and (ii) it converges regardless of initial values, in contrast to Newton’s method. The key for designing the SeLF algorithm is to find a function $$b(\theta )$$ satisfying $$\ell ''(\theta )\geqslant b(\theta )$$ for all $$\theta $$ in the domain such that an explicit solution to the equation $$\ell '(\theta ) + \int _{\theta ^{(t)}}^{\theta } b(z) \,\text { d}z \;=\; 0$$ is available. Furthermore, we develop three acceleration techniques, namely optimal SeLF, sub-optimal SeLF, and fast–SeLF algorithms, for the SeLF algorithm, resulting in a weakly stable convergence. Various applications in statistics of the proposed SeLF algorithm and three accelerated versions are introduced. The Dirichlet distribution illustrates the potential generalization of the SeLF algorithm to the multi-dimensional case. We study the convergence rates of these algorithms, accompanied by numerical experiments and comparisons.},
  archive      = {J_SAC},
  author       = {Tian, Guo-Liang and Zhou, Hua and Lange, Kenneth and Li, Xun-Jian},
  doi          = {10.1007/s11222-025-10639-1},
  journal      = {Statistics and Computing},
  month        = {8},
  number       = {4},
  pages        = {1-17},
  shortjournal = {Stat. Comput.},
  title        = {The second–derivative lower–bound function (SeLF) algorithm and three acceleration techniques for maximization with strongly stable convergence},
  volume       = {35},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Gradient-free gibbs sampler for shallow-wide bayesian neural networks. <em>SAC</em>, <em>35</em>(4), 1-22. (<a href='https://doi.org/10.1007/s11222-025-10642-6'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Hamiltonian Monte Carlo [HMC] is widely regarded as the de facto standard for posterior sampling-based inference [SBI] of Bayesian neural networks [BNNs]. Iterative gradient computations required in HMC to generate proposals can become prohibitively expensive, particularly for BNNs with numerous latent nodes and thus high-dimensional posterior distributions. We consider a gradient-free approach, leveraging a generalized auxiliary model that admits tractable full conditional distributions to devise a Metropolis-within-Gibbs sampler with asymmetric proposals that do not require gradients. Through simulation studies with shallow-wide BNNs, we demonstrate that the proposed sampler produces posterior samples of superior or comparable quality to HMC in key aspects such as nonlinearity approximation, out-of-distribution uncertainty quantification, acceptance rates, and effective sample sizes, while being gradient-free and thus significantly cost-efficient, making it a practical alternative for BNN posterior inference.},
  archive      = {J_SAC},
  author       = {Han, Geonhee},
  doi          = {10.1007/s11222-025-10642-6},
  journal      = {Statistics and Computing},
  month        = {8},
  number       = {4},
  pages        = {1-22},
  shortjournal = {Stat. Comput.},
  title        = {Gradient-free gibbs sampler for shallow-wide bayesian neural networks},
  volume       = {35},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Nudging state-space models for bayesian filtering under misspecified dynamics. <em>SAC</em>, <em>35</em>(4), 1-25. (<a href='https://doi.org/10.1007/s11222-025-10648-0'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Nudging is a popular algorithmic strategy in numerical filtering to deal with the problem of inference in high-dimensional dynamical systems. We demonstrate in this paper that general nudging techniques can also tackle another crucial statistical problem in filtering, namely the misspecification of the transition kernel. Specifically, we rely on the formulation of nudging as a general operation increasing the likelihood and prove analytically that, when applied carefully, nudging techniques implicitly define state-space models that have higher marginal likelihoods for a given (fixed) sequence of observations. This provides a theoretical justification of nudging techniques as data-informed algorithmic modifications of state-space models to obtain robust models under misspecified dynamics. To demonstrate the use of nudging, we provide numerical experiments on linear Gaussian state-space models and a stochastic Lorenz 63 model with misspecified dynamics and show that nudging offers a robust filtering strategy for these cases.},
  archive      = {J_SAC},
  author       = {González, Fabián and Akyildiz, O. Deniz and Crisan, Dan and Míguez, Joaquín},
  doi          = {10.1007/s11222-025-10648-0},
  journal      = {Statistics and Computing},
  month        = {8},
  number       = {4},
  pages        = {1-25},
  shortjournal = {Stat. Comput.},
  title        = {Nudging state-space models for bayesian filtering under misspecified dynamics},
  volume       = {35},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Sparse high-dimensional linear mixed modeling with a partitioned empirical bayes ECM algorithm. <em>SAC</em>, <em>35</em>(4), 1-15. (<a href='https://doi.org/10.1007/s11222-025-10649-z'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {High-dimensional longitudinal data is increasingly used in a wide range of scientific studies. To properly account for dependence between longitudinal observations, statistical methods for high-dimensional linear mixed models (LMMs) have been developed. However, few packages implementing these high-dimensional LMMs are available in the statistical software R. Additionally, some packages suffer from scalability issues. This work presents an efficient and accurate Bayesian framework for high-dimensional LMMs. We use empirical Bayes estimators of hyperparameters for increased flexibility and an Expectation-Conditional-Minimization (ECM) algorithm for computationally efficient maximum a posteriori (MAP) estimation of parameters. The novelty of the approach lies in its partitioning and parameter expansion as well as its fast and scalable computation. We illustrate Linear Mixed Modeling with PaRtitiOned empirical Bayes ECM (LMM-PROBE) in simulation studies evaluating fixed and random effects estimation along with computation time. A real-world example is provided using data from a study of lupus in children with 15,424 genetic and clinical predictors. Whereas it is computationally prohibitive to fit other LMMs to this data, LMM-PROBE successfully identifies genes and clinical factors associated with a new lupus biomarker and predicts it over time. Supplementary materials are available online.},
  archive      = {J_SAC},
  author       = {Zgodic, Anja and Bai, Ray and Zhang, Jiajia and Olejua, Peter and McLain, Alexander C.},
  doi          = {10.1007/s11222-025-10649-z},
  journal      = {Statistics and Computing},
  month        = {8},
  number       = {4},
  pages        = {1-15},
  shortjournal = {Stat. Comput.},
  title        = {Sparse high-dimensional linear mixed modeling with a partitioned empirical bayes ECM algorithm},
  volume       = {35},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Simulation based composite likelihood. <em>SAC</em>, <em>35</em>(3), 1-20. (<a href='https://doi.org/10.1007/s11222-025-10584-z'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Inference for high-dimensional hidden Markov models is challenging due to the exponential-in-dimension computational cost of calculating the likelihood. To address this issue, we introduce an innovative composite likelihood approach called “Simulation Based Composite Likelihood” (SimBa-CL). With SimBa-CL, we approximate the likelihood by the product of its marginals, which we estimate using Monte Carlo sampling. In a similar vein to approximate Bayesian computation (ABC), SimBa-CL requires multiple simulations from the model, but, in contrast to ABC, it provides a likelihood approximation that guides the optimization of the parameters. Leveraging automatic differentiation libraries, it is simple to calculate gradients and Hessians to not only speed up optimization but also to build approximate confidence sets. We present extensive empirical results which validate our theory and demonstrate its advantage over SMC, and apply SimBa-CL to real-world Aphtovirus data.},
  archive      = {J_SAC},
  author       = {Rimella, Lorenzo and Jewell, Chris and Fearnhead, Paul},
  doi          = {10.1007/s11222-025-10584-z},
  journal      = {Statistics and Computing},
  month        = {6},
  number       = {3},
  pages        = {1-20},
  shortjournal = {Stat. Comput.},
  title        = {Simulation based composite likelihood},
  volume       = {35},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Exact score and information matrix for panel hidden semi-markov models. <em>SAC</em>, <em>35</em>(3), 1-12. (<a href='https://doi.org/10.1007/s11222-025-10585-y'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We describe a general multivariate hidden semi-Markov model for time series and panel data. The model entails multiple response variables arising from exponential families conditionally on covariates and a discrete time-varying latent variable. The latter is modeled through transition probabilities and sojourn time distributions. We derive efficient forward recursions to exactly compute the score and information matrix. In a simulation study, we show the validity of our inferential approach for parameter and standard error estimation. The approach is also illustrated on an original real data example on sales of four arm types from member countries of the North Atlantic Treaty Organization to non-member countries in the period 2002–2022.},
  archive      = {J_SAC},
  author       = {Farcomeni, Alessio},
  doi          = {10.1007/s11222-025-10585-y},
  journal      = {Statistics and Computing},
  month        = {6},
  number       = {3},
  pages        = {1-12},
  shortjournal = {Stat. Comput.},
  title        = {Exact score and information matrix for panel hidden semi-markov models},
  volume       = {35},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). On approximations of subordinators in $$L^p$$ and the simulation of tempered stable distributions. <em>SAC</em>, <em>35</em>(3), 1-16. (<a href='https://doi.org/10.1007/s11222-025-10586-x'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Subordinators are infinitely divisible distributions on the positive half-line. They are often used as mixing distributions in Poisson mixtures. We show that appropriately scaled Poisson mixtures can approximate the mixing subordinator and we derive a rate of convergence in $$L^p$$ for each $$p\in [1,\infty ]$$ . This includes the Kolmogorov and Wasserstein metrics as special cases. As an application, we develop an approach for approximate simulation of the underlying subordinator. In the interest of generality, we present our results in the context of more general mixtures, specifically those that can be represented as differences of randomly stopped Lévy processes. Particular focus is given to the case where the subordinator belongs to the class of tempered stable distributions.},
  archive      = {J_SAC},
  author       = {Grabchak, Michael and Saba, Sina},
  doi          = {10.1007/s11222-025-10586-x},
  journal      = {Statistics and Computing},
  month        = {6},
  number       = {3},
  pages        = {1-16},
  shortjournal = {Stat. Comput.},
  title        = {On approximations of subordinators in $$L^p$$ and the simulation of tempered stable distributions},
  volume       = {35},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Hyperparameter optimization for randomized algorithms: A case study on random features. <em>SAC</em>, <em>35</em>(3), 1-28. (<a href='https://doi.org/10.1007/s11222-025-10587-w'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Randomized algorithms exploit stochasticity to reduce computational complexity. One important example is random feature regression (RFR) that accelerates Gaussian process regression (GPR). RFR approximates an unknown function with a random neural network whose hidden weights and biases are sampled from a probability distribution. Only the final output layer is fit to data. In randomized algorithms like RFR, the hyperparameters that characterize the sampling distribution greatly impact performance, yet are not directly accessible from samples. This makes optimization of hyperparameters via standard (gradient-based) optimization tools inapplicable. Inspired by Bayesian ideas from GPR, this paper introduces a random objective function that is tailored for hyperparameter tuning of vector-valued random features. The objective is minimized with ensemble Kalman inversion (EKI). EKI is a gradient-free particle-based optimizer that is scalable to high-dimensions and robust to randomness in objective functions. A numerical study showcases the new black-box methodology to learn hyperparameter distributions in several problems that are sensitive to the hyperparameter selection: two global sensitivity analyses, integrating a chaotic dynamical system, and solving a Bayesian inverse problem from atmospheric dynamics. The success of the proposed EKI-based algorithm for RFR suggests its potential for automated optimization of hyperparameters arising in other randomized algorithms.},
  archive      = {J_SAC},
  author       = {Dunbar, Oliver R. A. and Nelsen, Nicholas H. and Mutic, Maya},
  doi          = {10.1007/s11222-025-10587-w},
  journal      = {Statistics and Computing},
  month        = {6},
  number       = {3},
  pages        = {1-28},
  shortjournal = {Stat. Comput.},
  title        = {Hyperparameter optimization for randomized algorithms: A case study on random features},
  volume       = {35},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Robust $$\ell _{2,0}$$ -penalized rank regression for high-dimensional group selection. <em>SAC</em>, <em>35</em>(3), 1-16. (<a href='https://doi.org/10.1007/s11222-025-10588-9'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Sparse group selection is the process of selecting a small part of nonoverlapping groups to achieve the good interpretability and prediction on the response, and it has recently seen increasing applications in machine learning, image processing and bio-medical fields. However, developing robust and efficient algorithms for group selection remains a challenging research topic due to the computational complexity and potential outliers in high-dimensional settings. Motivated by the superior performance of rank-based methodology, we design a fast and efficient algorithm based on the $$\ell _{2,0}$$ penalty to achieve the goal of robust group selection for a given size of active groups s. This new algorithm can iteratively detect the active groups and exclude the irrelevant ones. When s is not less than $$s^*$$ (the true size of active groups), we theoretically prove that the proposed algorithm covers the true subset of active groups with high probability and the estimation error of the solution sequence generated by our algorithm decays to the optimal error bound in a few iterations. Moreover, coupled with the group Bayesian information criterion, an adaptive algorithm is further introduced to determine the optimal s. Theoretically, without any prior knowledge of $$s^*$$ , the proposed adaptive algorithm is able to exactly identify the true subset of active groups with probability approaching to one. Finally, extensive simulation examples show that our method outperforms existing competitors, resulting in significant improvements in terms of efficiency and accuracy of group selection and parametric estimation. The Bardet-Biedl syndrome gene expression data set is also analyzed to illustrate the application of our proposed method.},
  archive      = {J_SAC},
  author       = {Lv, Jing and Guo, Chaohui},
  doi          = {10.1007/s11222-025-10588-9},
  journal      = {Statistics and Computing},
  month        = {6},
  number       = {3},
  pages        = {1-16},
  shortjournal = {Stat. Comput.},
  title        = {Robust $$\ell _{2,0}$$ -penalized rank regression for high-dimensional group selection},
  volume       = {35},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Frequentist model averaging under a linear exponential loss. <em>SAC</em>, <em>35</em>(3), 1-27. (<a href='https://doi.org/10.1007/s11222-025-10589-8'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper introduces a new model averaging approach to consider uncertainty in model specification using an asymmetric loss, linear exponential (LINEX) loss function. We are motivated by the existing model-averaging prediction analysis studies being based on symmetric loss functions, which cannot meet practical situations where different weights are needed for over-prediction and under-prediction. The existing approaches cannot be used for the asymmetric loss. The proposed model averaging estimator established via the LINEX model averaging (LMA) criterion is shown to be optimal in achieving the lowest possible LINEX loss. We demonstrate the superiority of the LMA method and its effectiveness in movie forecasting and bitcoin volatility forecasting applications. Compared to other methods, the LMA estimator effectively reduces asymmetric loss and performs reasonably well even in the case of symmetric loss.},
  archive      = {J_SAC},
  author       = {Li, Xinmin and Liang, Hua and Liu, Huihang and Tong, Tingting and Xie, Tian},
  doi          = {10.1007/s11222-025-10589-8},
  journal      = {Statistics and Computing},
  month        = {6},
  number       = {3},
  pages        = {1-27},
  shortjournal = {Stat. Comput.},
  title        = {Frequentist model averaging under a linear exponential loss},
  volume       = {35},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Group inference for high-dimensional mediation models. <em>SAC</em>, <em>35</em>(3), 1-13. (<a href='https://doi.org/10.1007/s11222-025-10591-0'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Mediation analysis serves as a foundational statistical approach to comprehending the impact of exposure on an outcome. In this study, we investigate group inference for high-dimensional mediation models, examining mediators within a specified group either jointly or individually, allowing the number of mediators within the group to diverge. For both situations, we construct suitable test statistics and establish their asymptotic distributions. Extensive numerical studies demonstrate the superiority of our proposed methods over recent representative approaches. Our procedure can control the type I error well and exhibit the highest power. We also apply our methods to analyse how Deoxyribonucleic acid (DNA) methylation operates in the regulation of human stress reactivity impacted by childhood trauma. We have pinpointed seven key biological process groups, with the top five significant groups-axon development, neuron projection regeneration, positive regulation of the Mitogen-activated protein kinases (MAPK) cascade, regulation of neuron projection development, and axonogenesis-playing a collective role in nurturing nerve cell growth, development, and signal transmission.},
  archive      = {J_SAC},
  author       = {Yu, Ke and Guo, Xu and Luo, Shan},
  doi          = {10.1007/s11222-025-10591-0},
  journal      = {Statistics and Computing},
  month        = {6},
  number       = {3},
  pages        = {1-13},
  shortjournal = {Stat. Comput.},
  title        = {Group inference for high-dimensional mediation models},
  volume       = {35},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Variational bayesian analysis for joint models of longitudinal and failure time data with interval censoring. <em>SAC</em>, <em>35</em>(3), 1-23. (<a href='https://doi.org/10.1007/s11222-025-10592-z'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Alzheimer’s Disease (AD) progression is marked by a gradual decline in cognitive function, with significant events often occurring within uncertain intervals. To comprehensively understand AD, it is essential to jointly model longitudinal cognitive assessments and interval-censored survival data. However, current methodologies have certain limitations when applied to joint models. Maximum Likelihood Estimation often neglects parameter and model uncertainty, while Bayesian methods permit uncertainty quantification but rely on traditional Markov Chain Monte Carlo algorithms, which suffer from slow convergence and high memory demands. To address these challenges, we propose variational Bayesian methods as a more computationally efficient and scalable alternative. Specifically, we focus on two approaches: the Non-Conjugate Variational Message Passing method and the Non-Conjugate Variational Laplace Approximation method. These techniques effectively approximate complex posterior distributions while minimizing the excessive computational demands typically associated with traditional Bayesian techniques. Additionally, we introduce a variational Bayesian framework for local influence analysis and outlier detection, utilizing sparse priors to enhance the model’s robustness against data anomalies. Through simulation studies and an application to the Alzheimer’s Disease Neuroimaging Initiative dataset, we demonstrate the effectiveness of our variational Bayesian joint modeling approach. Our results underscore the advantages of these methods in terms of computational efficiency and scalability, making them well-suited for analyzing complex longitudinal and interval-censored data in AD research.},
  archive      = {J_SAC},
  author       = {Li, Huiqiong and Luo, Lu and Liu, Wenting and Wang, Min and Tang, Niansheng},
  doi          = {10.1007/s11222-025-10592-z},
  journal      = {Statistics and Computing},
  month        = {6},
  number       = {3},
  pages        = {1-23},
  shortjournal = {Stat. Comput.},
  title        = {Variational bayesian analysis for joint models of longitudinal and failure time data with interval censoring},
  volume       = {35},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A multifacet hierarchical sentiment-topic model with application to multi-brand online review analysis. <em>SAC</em>, <em>35</em>(3), 1-18. (<a href='https://doi.org/10.1007/s11222-025-10593-y'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-brand analysis based on review comments and ratings is a commonly used strategy to compare different brands in marketing. It can help consumers make more informed decisions and help marketers understand their brand’s position in the market. In this work, we propose a multifacet hierarchical sentiment-topic model (MH-STM) to detect brand-associated sentiment polarities towards multiple comparative aspects from online customer reviews. The proposed method is built on a unified generative framework that explains review words with a hierarchical brand-associated topic model and the overall polarity score with a regression model on the empirical topic distribution. Moreover, a novel hierarchical Pólya urn (HPU) scheme is proposed to enhance the topic-word association among topic hierarchy, such that the general topics shared by all brands are separated effectively from the unique topics specific to individual brands. The performance of the proposed method is evaluated on both synthetic data and two real-world review corpora. Experimental studies demonstrate that the proposed method can be effective in detecting reasonable topic hierarchy and deriving accurate brand-associated rankings on multi-aspects.},
  archive      = {J_SAC},
  author       = {Liang, Qiao and Deng, Xinwei},
  doi          = {10.1007/s11222-025-10593-y},
  journal      = {Statistics and Computing},
  month        = {6},
  number       = {3},
  pages        = {1-18},
  shortjournal = {Stat. Comput.},
  title        = {A multifacet hierarchical sentiment-topic model with application to multi-brand online review analysis},
  volume       = {35},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Air-HOLP: Adaptive regularized feature screening for high dimensional correlated data. <em>SAC</em>, <em>35</em>(3), 1-11. (<a href='https://doi.org/10.1007/s11222-025-10599-6'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Handling high-dimensional datasets presents substantial computational challenges, particularly when the number of features far exceeds the number of observations and when features are highly correlated. A modern approach to mitigate these issues is feature screening. In this work, the High-dimensional Ordinary Least-squares Projection (HOLP) feature screening method is advanced by employing adaptive ridge regularization. The impact of the ridge tuning parameter on the Ridge-HOLP method is examined and Adaptive iterative ridge-HOLP (Air-HOLP) is proposed, a data-adaptive advance to Ridge-HOLP where the ridge-regularization tuning parameter is selected iteratively and optimally for better feature screening performance. The proposed method addresses the challenges of tuning parameter selection in high dimensions by offering a computationally efficient and stable alternative to traditional methods like bootstrapping and cross-validation. Air-HOLP is evaluated using simulated data and a prostate cancer genetic dataset. The empirical results demonstrate that Air-HOLP has improved performance over a large range of simulation settings. We provide R codes implementing the Air-HOLP feature screening method and integrating it into existing feature screening methods that utilize the HOLP formula.},
  archive      = {J_SAC},
  author       = {Joudah, Ibrahim and Muller, Samuel and Zhu, Houying},
  doi          = {10.1007/s11222-025-10599-6},
  journal      = {Statistics and Computing},
  month        = {6},
  number       = {3},
  pages        = {1-11},
  shortjournal = {Stat. Comput.},
  title        = {Air-HOLP: Adaptive regularized feature screening for high dimensional correlated data},
  volume       = {35},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Metropolis-adjusted interacting particle sampling. <em>SAC</em>, <em>35</em>(3), 1-31. (<a href='https://doi.org/10.1007/s11222-025-10595-w'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, various interacting particle samplers have been developed to sample from complex target distributions, such as those found in Bayesian inverse problems. These samplers are motivated by the mean-field limit perspective and implemented as ensembles of particles that move in the product state space according to coupled stochastic differential equations. The ensemble approximation and numerical time stepping used to simulate these systems can introduce bias and affect the invariance of the particle system with respect to the target distribution. To correct for this, we investigate the use of a Metropolization step, similar to the Metropolis-adjusted Langevin algorithm. We examine Metropolization of either the whole ensemble or smaller subsets of the ensemble, and prove basic convergence of the resulting ensemble Markov chain to the target distribution. Our numerical results demonstrate the benefits of this correction in numerical examples for popular interacting particle samplers such as ALDI, CBS, and stochastic SVGD.},
  archive      = {J_SAC},
  author       = {Sprungk, Björn and Weissmann, Simon and Zech, Jakob},
  doi          = {10.1007/s11222-025-10595-w},
  journal      = {Statistics and Computing},
  month        = {6},
  number       = {3},
  pages        = {1-31},
  shortjournal = {Stat. Comput.},
  title        = {Metropolis-adjusted interacting particle sampling},
  volume       = {35},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Monotonic warpings for additive and deep gaussian processes. <em>SAC</em>, <em>35</em>(3), 1-22. (<a href='https://doi.org/10.1007/s11222-025-10598-7'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Gaussian processes (GPs) are canonical as surrogates for computer experiments because they enjoy a degree of analytic tractability. But that breaks when the response surface is constrained, say to be monotonic. Here, we provide a “mono-GP” construction for a single input that is highly efficient even though the calculations are non-analytic. Key ingredients include transformation of a reference process and elliptical slice sampling. We then show how mono-GP may be deployed effectively in two ways. One is additive, extending monotonicity to more inputs; the other is as a prior on injective latent warping variables in a deep Gaussian process for (non-monotonic, multi-input) nonstationary surrogate modeling. We provide illustrative and benchmarking examples throughout, showing that our methods yield improved performance over the state-of-the-art on examples from those two classes of problems.},
  archive      = {J_SAC},
  author       = {Barnett, Steven D. and Beesley, Lauren J. and Booth, Annie S. and Gramacy, Robert B. and Osthus, Dave},
  doi          = {10.1007/s11222-025-10598-7},
  journal      = {Statistics and Computing},
  month        = {6},
  number       = {3},
  pages        = {1-22},
  shortjournal = {Stat. Comput.},
  title        = {Monotonic warpings for additive and deep gaussian processes},
  volume       = {35},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Uncertainty quantification and propagation in surrogate-based bayesian inference. <em>SAC</em>, <em>35</em>(3), 1-28. (<a href='https://doi.org/10.1007/s11222-025-10597-8'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Surrogate models are statistical or conceptual approximations for more complex simulation models. In this context, it is crucial to propagate the uncertainty induced by limited simulation budget and surrogate approximation error to predictions, inference, and subsequent decision-relevant quantities. However, quantifying and then propagating the uncertainty of surrogates is usually limited to special analytic cases or is otherwise computationally very expensive. In this paper, we propose a framework enabling a scalable, Bayesian approach to surrogate modeling with thorough uncertainty quantification, propagation, and validation. Specifically, we present three methods for Bayesian inference with surrogate models given measurement data. This is a task where the propagation of surrogate uncertainty is especially relevant, because failing to account for it may lead to biased and/or overconfident estimates of the parameters of interest. We showcase our approach in three detailed case studies for linear and nonlinear real-world modeling scenarios. Uncertainty propagation in surrogate models enables more reliable and safe approximation of expensive simulators and will therefore be useful in various fields of applications.},
  archive      = {J_SAC},
  author       = {Reiser, Philipp and Aguilar, Javier Enrique and Guthke, Anneli and Bürkner, Paul-Christian},
  doi          = {10.1007/s11222-025-10597-8},
  journal      = {Statistics and Computing},
  month        = {6},
  number       = {3},
  pages        = {1-28},
  shortjournal = {Stat. Comput.},
  title        = {Uncertainty quantification and propagation in surrogate-based bayesian inference},
  volume       = {35},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Variational bayes inference for simultaneous autoregressive models with missing data. <em>SAC</em>, <em>35</em>(3), 1-16. (<a href='https://doi.org/10.1007/s11222-025-10590-1'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The simultaneous autoregressive (SAR) models are often used to analyse spatially correlated data. Markov chain Monte Carlo is one of the most widely used Bayesian methods for estimating the SAR models, but it has significant limitations when it comes to handling missing data in the response variable due to its high computational cost. Variational Bayes (VB) approximation offers an alternative solution to this problem. Two VB-based algorithms employing Gaussian variational approximation with factor covariance structure are presented, joint VB (JVB) and hybrid VB (HVB), suitable for both missing at random and not at random inference. While the JVB method inaccurately estimates the posterior distributions of some SAR parameters and missing values, the standard HVB algorithm struggles to make accurate inferences when dealing with a large number of missing values. Our modified versions of HVB enable accurate inference within a reasonable computational time, thus improving its performance. The performance of the VB methods is evaluated using simulated and real datasets. While we demonstrate the method using SAR models, the approach has broad applicability to various models with missing data.},
  archive      = {J_SAC},
  author       = {Wijayawardhana, Anjana and Gunawan, David and Suesse, Thomas},
  doi          = {10.1007/s11222-025-10590-1},
  journal      = {Statistics and Computing},
  month        = {6},
  number       = {3},
  pages        = {1-16},
  shortjournal = {Stat. Comput.},
  title        = {Variational bayes inference for simultaneous autoregressive models with missing data},
  volume       = {35},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Estimation and model selection for finite mixtures of tukey’s g- &-h distributions. <em>SAC</em>, <em>35</em>(3), 1-18. (<a href='https://doi.org/10.1007/s11222-025-10596-9'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A finite mixture of distributions is a popular statistical model, which is especially meaningful when the population of interest may include distinct subpopulations. This work is motivated by analysis of protein expression levels quantified using immunofluorescence immunohistochemistry assays of human tissues. The distributions of cellular protein expression levels in a tissue often exhibit multimodality, skewness and heavy tails, but there is a substantial variability between distributions in different tissues from different subjects, while some of these mixture distributions include components consistent with the assumption of a normal distribution. To accommodate such diversity, we propose a mixture of 4-parameter Tukey’s g- &-h distributions for fitting finite mixtures with both Gaussian and non-Gaussian components. Tukey’s g- &-h distribution is a flexible model that allows variable degree of skewness and kurtosis in mixture components, including normal distribution as a particular case. Since the likelihood of the Tukey’s g- &-h mixtures does not have a closed analytical form, we propose a quantile least Mahalanobis distance (QLMD) estimator for parameters of such mixtures. QLMD is an indirect estimator minimizing the Mahalanobis distance between the sample and model-based quantiles, and its asymptotic properties follow from the general theory of indirect estimation. We have developed a stepwise algorithm to select a parsimonious Tukey’s g- &-h mixture model and implemented all proposed methods in the R package QuantileGH available on CRAN. A simulation study was conducted to evaluate performance of the Tukey’s g- &-h mixtures and compare to performance of mixtures of skew-normal or skew-t distributions. The Tukey’s g- &-h mixtures were applied to model cellular expressions of Cyclin D1 protein in breast cancer tissues, and resulting parameter estimates evaluated as predictors of progression-free survival.},
  archive      = {J_SAC},
  author       = {Zhan, Tingting and Yi, Misung and Peck, Amy R. and Rui, Hallgeir and Chervoneva, Inna},
  doi          = {10.1007/s11222-025-10596-9},
  journal      = {Statistics and Computing},
  month        = {6},
  number       = {3},
  pages        = {1-18},
  shortjournal = {Stat. Comput.},
  title        = {Estimation and model selection for finite mixtures of tukey’s g- &-h distributions},
  volume       = {35},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A new p-value based multiple testing procedure for generalized linear models. <em>SAC</em>, <em>35</em>(3), 1-10. (<a href='https://doi.org/10.1007/s11222-025-10600-2'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This study introduces a novel p-value-based multiple testing approach tailored for generalized linear models. Despite the crucial role of generalized linear models in statistics, existing methodologies face obstacles arising from the heterogeneous variance of response variables and complex dependencies among estimated parameters. Our aim is to address the challenge of controlling the false discovery rate (FDR) amidst arbitrarily dependent test statistics. Through the development of efficient computational algorithms, we present a versatile statistical framework for multiple testing. The proposed framework accommodates a range of tools developed for constructing a new model matrix in regression-type analysis, including random row permutations and Model-X knockoffs. We devise efficient computing techniques to solve the encountered non-trivial quadratic matrix equations, enabling the construction of paired p-values suitable for the two-step multiple testing procedure proposed by Sarkar and Tang (Biometrika 109(4): 1149–1155, 2022). Theoretical analysis affirms the properties of our approach, demonstrating its capability to control the FDR at a given level. Empirical evaluations further substantiate its promising performance across diverse simulation settings.},
  archive      = {J_SAC},
  author       = {Rilling, Joseph and Tang, Cheng Yong},
  doi          = {10.1007/s11222-025-10600-2},
  journal      = {Statistics and Computing},
  month        = {6},
  number       = {3},
  pages        = {1-10},
  shortjournal = {Stat. Comput.},
  title        = {A new p-value based multiple testing procedure for generalized linear models},
  volume       = {35},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Bayesian design for sampling anomalous spatio-temporal data. <em>SAC</em>, <em>35</em>(3), 1-19. (<a href='https://doi.org/10.1007/s11222-025-10594-x'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Data collected from arrays of sensors are essential for informed decision-making in various systems. However, the presence of anomalies can compromise the accuracy and reliability of insights drawn from the collected data or information obtained via statistical analysis. This study aims to develop a robust Bayesian optimal experimental design framework with anomaly detection methods for high-quality data collection. We introduce a general framework that involves anomaly generation, detection and error scoring when searching for an optimal design. This method is demonstrated using two comprehensive simulated case studies: the first study uses a spatial dataset, and the second uses a spatio-temporal river network dataset. As a baseline approach, we employed a commonly used prediction-based utility function based on minimising errors. Results illustrate the trade-off between predictive accuracy and anomaly detection performance for our method under various design scenarios. An optimal design robust to anomalies ensures the collection and analysis of more trustworthy data, playing a crucial role in understanding the dynamics of complex systems such as the environment, therefore enabling informed decisions in monitoring, management, and response.},
  archive      = {J_SAC},
  author       = {Buchhorn, Katie and Mengersen, Kerrie and Santos-Fernandez, Edgar and McGree, James},
  doi          = {10.1007/s11222-025-10594-x},
  journal      = {Statistics and Computing},
  month        = {6},
  number       = {3},
  pages        = {1-19},
  shortjournal = {Stat. Comput.},
  title        = {Bayesian design for sampling anomalous spatio-temporal data},
  volume       = {35},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Bayesian analysis of doubly semiparametric mixture cure models with interval-censored data. <em>SAC</em>, <em>35</em>(3), 1-19. (<a href='https://doi.org/10.1007/s11222-025-10601-1'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Interval-censored data are commonly encountered in medical studies, where the occurrence of a disease can only be observed within specific time intervals or during periodic examinations. In the presence of individuals being cured or never experiencing the disease, a mixture cure model is often assumed for regression analysis accounting for the mixture of cured and uncured individuals in the study population. In this model, the Cox proportional hazards model is typically specified as a latency component for the event time and logistic regression as an incidence component for the probability of uncured. Challenges appear in the analysis when some covariates are time-related. It is unrealistic to assume linear covariate effects on a known transformation of cure probability or the hazard ratio of uncured individuals, as is commonly done. We propose a doubly semiparametric mixture cure model for interval-censored data, providing more flexibility by allowing linear and nonlinear effects of covariates in both the incidence and latency parts. We develop a computationally feasible Bayesian estimation procedure, incorporating a two-stage data augmentation with Poisson latent variables to deal with interval-censored data and splines for modelling the nonlinear terms in the model. We evaluate the finite sample performance of the proposed method via extensive simulations and demonstrate its utility through analysis of data from a hypobaric decompression sickness study.},
  archive      = {J_SAC},
  author       = {Liu, Xiaoyu and Xiang, Liming},
  doi          = {10.1007/s11222-025-10601-1},
  journal      = {Statistics and Computing},
  month        = {6},
  number       = {3},
  pages        = {1-19},
  shortjournal = {Stat. Comput.},
  title        = {Bayesian analysis of doubly semiparametric mixture cure models with interval-censored data},
  volume       = {35},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Sparse and debiased lasso estimation and statistical inference for long time series via divide-and-conquer. <em>SAC</em>, <em>35</em>(3), 1-16. (<a href='https://doi.org/10.1007/s11222-025-10602-0'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To tackle long time series with high-dimensional covariates and dependent non-Gaussian errors, we consider the divide-and-conquer strategy and develop a class of sparse and debiased Lasso estimators. To alleviate the serial correlation in long time series data, we sequentially split the long time series into several subseries and apply a generalized penalized least squares (GLS) method for linear regression models in each subseries allowing stationary covariates and AR(q) error processes. To make accurate statistical inference, we further propose a sparse and debiased estimator and investigate its asymptotic properties. By constructing a pseudo-response variable using a squared loss transformation, the proposed GLS method is extended to a unified M-estimation framework including Huber and quantile regression models to reduce computational burden. Extensive simulations validate theoretical properties and demonstrate that our proposed estimators have better performance than some existing methods. The proposed estimators are applied to Beijing Air Quality Data and NIFTY 50 Index Data to illustrate their validity and feasibility.},
  archive      = {J_SAC},
  author       = {Liu, Jin and Ma, Wei and Wang, Lei and Lian, Heng},
  doi          = {10.1007/s11222-025-10602-0},
  journal      = {Statistics and Computing},
  month        = {6},
  number       = {3},
  pages        = {1-16},
  shortjournal = {Stat. Comput.},
  title        = {Sparse and debiased lasso estimation and statistical inference for long time series via divide-and-conquer},
  volume       = {35},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Debiased transfer learning estimation and inference for multinomial regression. <em>SAC</em>, <em>35</em>(3), 1-30. (<a href='https://doi.org/10.1007/s11222-025-10607-9'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Transfer learning has gained considerable attention for improving the performance of high-dimensional linear and generalized linear models by leveraging source data. However, few studies have explored transfer learning in multinomial regression (MR) for multi-class classification problems. In this paper, we propose a two-step MR transfer learning estimator when the transferable sources are known and establish its error bounds. When the target and source datasets are close, these bounds can be improved over the MR estimator using only target data under mild conditions. To address the bias introduced by the Lasso penalty, we develop a unified debiasing framework based on KKT conditions, establishing the asymptotic normality for the construction of confidence intervals and hypothesis tests. For practical implementation, a transferable source detection algorithm with theoretical guarantees is proposed. Numerical studies and an application to Genotype-Tissue Expression data demonstrate the effectiveness of our proposed methods.},
  archive      = {J_SAC},
  author       = {Yang, Jichen and Wang, Lei and Lian, Heng},
  doi          = {10.1007/s11222-025-10607-9},
  journal      = {Statistics and Computing},
  month        = {6},
  number       = {3},
  pages        = {1-30},
  shortjournal = {Stat. Comput.},
  title        = {Debiased transfer learning estimation and inference for multinomial regression},
  volume       = {35},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Online bayesian changepoint detection for network poisson processes with community structure. <em>SAC</em>, <em>35</em>(3), 1-29. (<a href='https://doi.org/10.1007/s11222-025-10606-w'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Network point processes often exhibit latent structure that govern the behaviour of the sub-processes. It is not always reasonable to assume that this latent structure is static, and detecting when and how this driving structure changes is often of interest. In this paper, we introduce a novel online methodology for detecting changes within the latent structure of a network point process. We focus on block-homogeneous Poisson processes, where latent node memberships determine the rates of the edge processes. We propose a scalable variational procedure which can be applied on large networks in an online fashion via a Bayesian forgetting factor applied to sequential variational approximations to the posterior distribution. The proposed framework is tested on simulated and real-world data, and it rapidly and accurately detects changes to the latent edge process rates, and to the latent node group memberships, both in an online manner. In particular, in an application on the Santander Cycles bike-sharing network in central London, we detect changes within the network related to holiday periods and lockdown restrictions between 2019 and 2020.},
  archive      = {J_SAC},
  author       = {Corneck, Joshua and Cohen, Edward A. K. and Martin, James S. and Sanna Passino, Francesco},
  doi          = {10.1007/s11222-025-10606-w},
  journal      = {Statistics and Computing},
  month        = {6},
  number       = {3},
  pages        = {1-29},
  shortjournal = {Stat. Comput.},
  title        = {Online bayesian changepoint detection for network poisson processes with community structure},
  volume       = {35},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Fast bayesian variable screening using correlation thresholds. <em>SAC</em>, <em>35</em>(3), 1-20. (<a href='https://doi.org/10.1007/s11222-025-10608-8'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a fast Bayesian variable screening method for Normal regression models using thresholds on Pearson and partial correlation coefficients. Although the proposed method is based on the computation of correlation coefficients, it is derived using purely Bayesian arguments obtained from thresholds on Bayes factors and posterior model odds. The proposed method can be used to screen out the “non-important" covariates and reduce the size of the model space even in cases when the number of covariates is larger than the sample size. Then, on the reduced model space, obtained from the proposed approach, more accurate, traditional, computer-intensive, Bayesian variable selection methods can be implemented, if needed. We focus on the use of g-priors where Bayes factors can be obtained analytically and the corresponding correlation threshold computations are exact. Nevertheless, the approach is general and can be easily extended to any prior setup. The proposed method is illustrated using simulated examples.},
  archive      = {J_SAC},
  author       = {Paroli, Roberta and Fouskakis, Dimitris and Ntzoufras, Ioannis},
  doi          = {10.1007/s11222-025-10608-8},
  journal      = {Statistics and Computing},
  month        = {6},
  number       = {3},
  pages        = {1-20},
  shortjournal = {Stat. Comput.},
  title        = {Fast bayesian variable screening using correlation thresholds},
  volume       = {35},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Fast gibbs sampling for the local-seasonal-global trend bayesian exponential smoothing model. <em>SAC</em>, <em>35</em>(3), 1-22. (<a href='https://doi.org/10.1007/s11222-025-10603-z'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In Smyl et al. [Local and global trend Bayesian exponential smoothing models. International Journal of Forecasting, 2025.], a generalised exponential smoothing model was proposed that is able to capture strong trends and volatility in time series. This method achieved state-of-the-art performance in many forecasting tasks, but its fitting procedure, which is based on the NUTS sampler, is very computationally expensive. In this work, we propose several modifications to the original model, as well as a bespoke Gibbs sampler for posterior exploration; these changes improve sampling time by an order of magnitude, thus rendering the model much more practically relevant. The new model, and sampler, are evaluated on the M3 dataset and are shown to be competitive, or superior, in terms of accuracy to the original method, while being substantially faster to run.},
  archive      = {J_SAC},
  author       = {Long, Xueying and Schmidt, Daniel F. and Bergmeir, Christoph and Smyl, Slawek},
  doi          = {10.1007/s11222-025-10603-z},
  journal      = {Statistics and Computing},
  month        = {6},
  number       = {3},
  pages        = {1-22},
  shortjournal = {Stat. Comput.},
  title        = {Fast gibbs sampling for the local-seasonal-global trend bayesian exponential smoothing model},
  volume       = {35},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Semiparametric efficient estimation of genetic relatedness with machine learning methods. <em>SAC</em>, <em>35</em>(3), 1-15. (<a href='https://doi.org/10.1007/s11222-025-10605-x'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we propose semiparametric efficient estimators of genetic relatedness between two traits in a model-free framework. Most existing methods require specifying certain parametric models involving the traits and genetic variants. However, the bias due to model misspecification may yield misleading statistical results. Moreover, the semiparametric efficient bounds for estimators of genetic relatedness are still lacking. In this paper, we develop semiparametric efficient estimators with machine learning methods and construct valid confidence intervals for two important measures of genetic relatedness: genetic covariance and genetic correlation, allowing both continuous and discrete responses. Based on the derived efficient influence functions of genetic relatedness, we propose a consistent estimator of the genetic covariance as long as one of the genetic values is consistently estimated. The data of two traits may be collected from the same group or different groups of individuals. To validate our approach, we conduct various numerical studies to illustrate the introduced estimation procedures. Additionally, we apply our proposed methodologies to analyze data from the Carworth Farms White mice genome-wide association study.},
  archive      = {J_SAC},
  author       = {Guo, Xu and Shi, Hongwei and Yang, Weichao and Qian, Yiyuan and Zhou, Niwen},
  doi          = {10.1007/s11222-025-10605-x},
  journal      = {Statistics and Computing},
  month        = {6},
  number       = {3},
  pages        = {1-15},
  shortjournal = {Stat. Comput.},
  title        = {Semiparametric efficient estimation of genetic relatedness with machine learning methods},
  volume       = {35},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Correction: PCA-uCPD: An ensemble method for multiple change-point detection in moderately high-dimensional data. <em>SAC</em>, <em>35</em>(3), 1. (<a href='https://doi.org/10.1007/s11222-025-10615-9'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_SAC},
  author       = {Qin, Shanshan and Tan, Zhenni and Wei, Dongwei and Wu, Yuehua},
  doi          = {10.1007/s11222-025-10615-9},
  journal      = {Statistics and Computing},
  month        = {6},
  number       = {3},
  pages        = {1},
  shortjournal = {Stat. Comput.},
  title        = {Correction: PCA-uCPD: An ensemble method for multiple change-point detection in moderately high-dimensional data},
  volume       = {35},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Penalized principal component analysis using smoothing. <em>SAC</em>, <em>35</em>(3), 1-15. (<a href='https://doi.org/10.1007/s11222-025-10610-0'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Principal components computed via principal component analysis (PCA) are traditionally used to reduce dimensionality in genomic data or to correct for population stratification. In this paper, we explore the penalized eigenvalue problem (PEP) which reformulates the computation of the first eigenvector as an optimization problem and adds an $$L_1$$ penalty constraint to enforce sparseness of the solution. The contribution of our article is threefold. First, we extend PEP by applying smoothing to the original LASSO-type $$L_1$$ penalty. This allows one to compute analytical gradients which enable faster and more efficient minimization of the objective function associated with the optimization problem. Second, we demonstrate how higher order eigenvectors can be calculated with PEP using established results from singular value decomposition (SVD). Third, we present four experimental studies to demonstrate the usefulness of the smoothed penalized eigenvectors. Using data from the 1000 Genomes Project dataset, we empirically demonstrate that our proposed smoothed PEP allows one to increase numerical stability and obtain meaningful eigenvectors. We also employ the penalized eigenvector approach in two additional real data applications (computation of a polygenic risk score and clustering), demonstrating that exchanging the penalized eigenvectors for their smoothed counterparts can increase prediction accuracy in polygenic risk scores and enhance discernibility of clusterings. Moreover, we compare our proposed smoothed PEP to seven state-of-the-art algorithms for sparse PCA and evaluate the accuracy of the obtained eigenvectors, their support recovery, and their runtime.},
  archive      = {J_SAC},
  author       = {Hurwitz, Rebecca and Hahn, Georg},
  doi          = {10.1007/s11222-025-10610-0},
  journal      = {Statistics and Computing},
  month        = {6},
  number       = {3},
  pages        = {1-15},
  shortjournal = {Stat. Comput.},
  title        = {Penalized principal component analysis using smoothing},
  volume       = {35},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Efficient estimation for varying coefficient modal regression. <em>SAC</em>, <em>35</em>(3), 1-19. (<a href='https://doi.org/10.1007/s11222-025-10611-z'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Modal regression uses the conditional mode to explain the dependent variable and has good robustness when the dataset contains outliers or the error distribution is heavy-tailed or asymmetric. For the varying coefficient modal regression model, some scholars have proposed local modal estimation for coefficient functions using the local polynomial method. Since the same bandwidth is applied for each coefficient function, when the smoothness of coefficient functions is different, the efficiency of the local estimation method will decrease. This paper proposes global estimation methods for the varying coefficient modal regression model, which allow different bandwidths to be used for each coefficient function, thus improving the estimation accuracy, especially when the smoothness of the coefficient functions is significantly different. The asymptotic properties of the proposed estimators are proved in this paper. The numerical simulation shows that the global methods perform better than the local method, and a real data analysis is also provided.},
  archive      = {J_SAC},
  author       = {Cheng, Ruonan and Zhou, Xiuqing},
  doi          = {10.1007/s11222-025-10611-z},
  journal      = {Statistics and Computing},
  month        = {6},
  number       = {3},
  pages        = {1-19},
  shortjournal = {Stat. Comput.},
  title        = {Efficient estimation for varying coefficient modal regression},
  volume       = {35},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Penalized empirical likelihood estimation and EM algorithms for closed-population capture–recapture models. <em>SAC</em>, <em>35</em>(2), 1-15. (<a href='https://doi.org/10.1007/s11222-024-10557-8'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Capture–recapture experiments are widely used to estimate the abundance of a finite population. Based on capture–recapture data, the empirical likelihood (EL) method has been shown to outperform the conventional conditional likelihood (CL) method. However, the current literature on EL abundance estimation ignores behavioral effects, and the EL estimates may not be stable, especially when the capture probability is low. We make three contributions in this paper. First, we extend the EL method to capture–recapture models that account for behavioral effects. Second, to overcome the instability of the EL method, we propose a penalized EL (PEL) estimation method that penalizes large abundance values. We then investigate the asymptotics of the maximum PEL estimator and the PEL ratio statistic. Third, we develop standard expectation–maximization (EM) algorithms for PEL to improve its practical performance. The EM algorithm is also applicable to EL and CL with slight modifications. Our simulation and a real-world data analysis demonstrate that the PEL method successfully overcomes the instability of the EL method and the proposed EM algorithm produces more reliable results than existing optimization algorithms.},
  archive      = {J_SAC},
  author       = {Liu, Yang and Li, Pengfei and Liu, Yukun},
  doi          = {10.1007/s11222-024-10557-8},
  journal      = {Statistics and Computing},
  month        = {4},
  number       = {2},
  pages        = {1-15},
  shortjournal = {Stat. Comput.},
  title        = {Penalized empirical likelihood estimation and EM algorithms for closed-population capture–recapture models},
  volume       = {35},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Optimal distributed subsampling under heterogeneity. <em>SAC</em>, <em>35</em>(2), 1-20. (<a href='https://doi.org/10.1007/s11222-024-10558-7'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Distributed subsampling approaches have been proposed to process massive data in a distributed computing environment, where subsamples are taken from each site and then analyzed collectively to address statistical problems when the full data is not available. In this paper, we consider that each site involves a common parameter and site-specific nuisance parameters and then formulate a unified framework of optimal distributed subsampling under heterogeneity for general optimization problems with convex loss functions that could be nonsmooth. By establishing the consistency and asymptotic normality of the distributed subsample estimators for the common parameter of interest, we derive the optimal subsampling probabilities and allocation sizes under the A- and L-optimality criteria. A two-step algorithm is proposed for practical implementation and the asymptotic properties of the resultant estimator are established. For nonsmooth loss functions, an alternating direction method of multipliers method and a random perturbation procedure are proposed to obtain the subsample estimator and estimate the covariance matrices for statistical inference, respectively. The finite-sample performance of linear regression, logistic regression and quantile regression models is demonstrated through simulation studies and an application to the National Longitudinal Survey of Youth Dataset is also provided.},
  archive      = {J_SAC},
  author       = {Shao, Yujing and Wang, Lei and Lian, Heng},
  doi          = {10.1007/s11222-024-10558-7},
  journal      = {Statistics and Computing},
  month        = {4},
  number       = {2},
  pages        = {1-20},
  shortjournal = {Stat. Comput.},
  title        = {Optimal distributed subsampling under heterogeneity},
  volume       = {35},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). PCA-uCPD: An ensemble method for multiple change-point detection in moderately high-dimensional data. <em>SAC</em>, <em>35</em>(2), 1-17. (<a href='https://doi.org/10.1007/s11222-024-10553-y'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Change-point detection (CPD) receives extensive studies due to its wide applications in various fields. However, CPD remains a challenging problem for complex data with medium or high dimensions, high correlations, outliers or heavy-tailed distribution. This article proposes an integrated change-point detection method called PCA-uCPD, which utilizes principal components analysis (PCA) to project the original data series into uncorrelated principal components (PCs). Subsequently, we apply existing univariate change-point detection methods to the mapped PCs, followed by a proposed refining technique to obtain the ultimate change-point estimates for the original data sequences. The proposed method admits a flexible architecture that is thus capable of dealing with complex data. Theoretical justifications have been provided to guarantee the feasibility of the proposed methods. Moreover, we conduct simulations to assess performance across various data-generating scenarios. The efficacy of PCA-uCPD is further demonstrated through applications in both genetic and financial datasets.},
  archive      = {J_SAC},
  author       = {Qin, Shanshan and Tan, Zhenni and Wei, Weidong and Wu, Yuehua},
  doi          = {10.1007/s11222-024-10553-y},
  journal      = {Statistics and Computing},
  month        = {4},
  number       = {2},
  pages        = {1-17},
  shortjournal = {Stat. Comput.},
  title        = {PCA-uCPD: An ensemble method for multiple change-point detection in moderately high-dimensional data},
  volume       = {35},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Power enhancing probability subsampling using side information. <em>SAC</em>, <em>35</em>(2), 1-16. (<a href='https://doi.org/10.1007/s11222-024-10556-9'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we study the subsampling technique for hypothesis testing in generalized linear models with large-scale datasets, focusing on testing simple null hypotheses against composite linear alternatives. We propose a subsample-based test statistic and show that it converges to non-central chi-square distributions under Pitman’s local alternatives. The optimal subsampling distribution that maximizes power requires iterative calculations on the full data, which is computationally infeasible. Furthermore, it depends on the true parameter, which cannot be consistently estimated under Pitman’s local alternatives. We maximize a lower bound of the non-central parameter to define the power enhancing probability and utilize side information under the alternative to replace the true parameter. Extensive simulations and an application to a real dataset on flight delays and cancellations show that the proposed method offers a computationally viable solution for hypothesis testing in the realm of big data.},
  archive      = {J_SAC},
  author       = {Gao, Junzhuo and Wang, Lei and Wang, Haiying},
  doi          = {10.1007/s11222-024-10556-9},
  journal      = {Statistics and Computing},
  month        = {4},
  number       = {2},
  pages        = {1-16},
  shortjournal = {Stat. Comput.},
  title        = {Power enhancing probability subsampling using side information},
  volume       = {35},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Inference issue in multiscale geographically and temporally weighted regression. <em>SAC</em>, <em>35</em>(2), 1-22. (<a href='https://doi.org/10.1007/s11222-024-10559-6'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Geographically and temporally weighted regression (GTWR) models assume that all of the varying coefficients operate at a same spatiotemporal scale, which reduces the flexibility of local regression models. Multiscale geographically and temporally weighted regression (MGTWR) models increase flexibility, enhance interpretability, and provide more comprehensive information by allowing regression coefficients to vary across different spatiotemporal scales. However, a limitation of the MGTWR models is that, to date, statistical inference regarding the local coefficient estimates has not been feasible. Formally, “hat matrix”, which projects the observed response variable vector onto the fitting response variable, was not available in the MGTWR model. This paper settles this limitation by reformulating the GTWR model into a Generalized Additive Model, extending this framework to the MGTWR model and then deriving standard deviations for the local coefficient estimates in the MGTWR model. In addition, we also obtain the number of effective parameters for the overall fit of the MGTWR model and for each covariate within the model. This statistic is crucial for comparing the goodness of fit between MGTWR, GTWR and classical global regression models, as well as for adjusting multiple tests. Simulation studies and a real-world example demonstrate these advances to the MGTWR framework.},
  archive      = {J_SAC},
  author       = {Hong, Zhimin and Wang, Ruoxuan and Wang, Zhiwen and Du, Wala},
  doi          = {10.1007/s11222-024-10559-6},
  journal      = {Statistics and Computing},
  month        = {4},
  number       = {2},
  pages        = {1-22},
  shortjournal = {Stat. Comput.},
  title        = {Inference issue in multiscale geographically and temporally weighted regression},
  volume       = {35},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Constrained least squares simplicial-simplicial regression. <em>SAC</em>, <em>35</em>(2), 1-16. (<a href='https://doi.org/10.1007/s11222-024-10560-z'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Simplicial-simplicial regression refers to the regression setting where both the responses and predictor variables lie within the simplex space, i.e. they are compositional. For this setting, constrained least squares, where the regression coefficients themselves lie within the simplex, is proposed. The model is transformation-free but the adoption of a power transformation is straightforward, it can treat more than one compositional datasets as predictors and offers the possibility of weights among the simplicial predictors. Among the model’s advantages are its ability to treat zeros in a natural way and a highly computationally efficient algorithm to estimate its coefficients. Resampling based hypothesis testing procedures are employed regarding inference, such as linear independence, and equality of the regression coefficients to some pre-specified values. The strategy behind the formulation of the new model implemented is related to an existing methodology, that is of the same spirit, showcasing how other similar models can be employed as well. Finally, the performance of the proposed technique and its comparison to the existing methodology takes place using simulation studies and real data examples.},
  archive      = {J_SAC},
  author       = {Tsagris, Michail},
  doi          = {10.1007/s11222-024-10560-z},
  journal      = {Statistics and Computing},
  month        = {4},
  number       = {2},
  pages        = {1-16},
  shortjournal = {Stat. Comput.},
  title        = {Constrained least squares simplicial-simplicial regression},
  volume       = {35},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Empirical investigations of boosting with pseudo-outcome imputation for missing responses. <em>SAC</em>, <em>35</em>(2), 1-18. (<a href='https://doi.org/10.1007/s11222-024-10561-y'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Boosting techniques have gained increasing interest in both machine learning and statistical research. However, many of these methods are primarily designed for complete datasets, which limits their applicability to handle incomplete data such as missing observations. In this paper, we propose the pseudo-outcome strategy to account for missingness effects and describe a functional gradient descent algorithm. Numerical studies demonstrate the satisfactory performance of the proposed method in finite sample settings. Furthermore, we apply the proposed method to analyze the KLIPS Data.},
  archive      = {J_SAC},
  author       = {Bian, Yuan and Yi, Grace Y. and He, Wenqing},
  doi          = {10.1007/s11222-024-10561-y},
  journal      = {Statistics and Computing},
  month        = {4},
  number       = {2},
  pages        = {1-18},
  shortjournal = {Stat. Comput.},
  title        = {Empirical investigations of boosting with pseudo-outcome imputation for missing responses},
  volume       = {35},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). An analysis of the modality and flexibility of the inverse stereographic normal distribution. <em>SAC</em>, <em>35</em>(2), 1-12. (<a href='https://doi.org/10.1007/s11222-025-10563-4'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Circular data arises in various fields including robotics, biology, geology and material sciences. Modelling such data requires flexible distribution families on the hypertorus. Common choices are the von Mises and the wrapped normal distributions. In this work we investigate the inverse stereographic normal distribution as an alternative distribution family both from a theoretical and applied perspective. We first generalize unimodality results to arbitrary dimensions by proving that the inverse stereographic normal distribution is unimodal if and only if all eigenvalues of the covariance matrix are less than or equal to 0.5. We then analyze the flexibility by fitting mixtures of shifted inverse stereographic normal distributions via gradient descent to several examples of toroidal data.},
  archive      = {J_SAC},
  author       = {Hinz, Florian B. and Mahmoud, Amr H. and Lill, Markus A.},
  doi          = {10.1007/s11222-025-10563-4},
  journal      = {Statistics and Computing},
  month        = {4},
  number       = {2},
  pages        = {1-12},
  shortjournal = {Stat. Comput.},
  title        = {An analysis of the modality and flexibility of the inverse stereographic normal distribution},
  volume       = {35},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Asymptotic post-selection inference for regularized graphical models. <em>SAC</em>, <em>35</em>(2), 1-30. (<a href='https://doi.org/10.1007/s11222-025-10564-3'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Asymptotically valid inference is obtained for graphical model edge parameters after selection using the same data set as the one used for inference. We consider Gaussian and (trans)elliptical graphical models, where the edge selection and consequent sparse estimation is operated by applying the $$\ell _1$$ , elastic net, smoothly clipped absolute deviation, or minimax concave penalty to an appropriately regular loss function. The polyhedral lemma is used to carry out conditional inference which is asymptotically valid in the (possibly wrong) selected graphical model. Simulation studies show how the method yields valid inference in a variety of finite-sample settings.},
  archive      = {J_SAC},
  author       = {Guglielmini, Sofia and Claeskens, Gerda},
  doi          = {10.1007/s11222-025-10564-3},
  journal      = {Statistics and Computing},
  month        = {4},
  number       = {2},
  pages        = {1-30},
  shortjournal = {Stat. Comput.},
  title        = {Asymptotic post-selection inference for regularized graphical models},
  volume       = {35},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Efficient modeling of quasi-periodic data with seasonal gaussian process. <em>SAC</em>, <em>35</em>(2), 1-18. (<a href='https://doi.org/10.1007/s11222-025-10565-2'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Quasi-periodicity refers to a pattern in a function where it appears periodic at a certain frequency but exhibits evolving amplitudes over time. This is often the case in practical settings such as the modeling of case counts of infectious disease or the population dynamics of species over time. In this paper, we consider a class of Gaussian processes, called seasonal Gaussian Processes (sGP), for model-based inference of such quasi-periodic behavior. We illustrate that the exact sGP can be efficiently fitted using its state space representation for equally spaced time points. However, for large datasets with irregular spacing, the exact approach becomes computationally inefficient and unstable. To address this, we develop a continuous finite dimensional approximation for sGP using the seasonal B-spline (sB-spline) basis constructed by damping B-splines with sinusoidal functions. We prove the covariance convergence rate of the proposed approximation to the true sGP as the number of basis functions increases, and show its superior approximation quality through numerical studies. We also provide a unified and interpretable way to define priors for the sGP, based on the notion of predictive standard deviation. Finally, we implement the proposed inference method on several real data examples to illustrate its practical usage.},
  archive      = {J_SAC},
  author       = {Zhang, Ziang and Brown, Patrick and Stafford, Jamie},
  doi          = {10.1007/s11222-025-10565-2},
  journal      = {Statistics and Computing},
  month        = {4},
  number       = {2},
  pages        = {1-18},
  shortjournal = {Stat. Comput.},
  title        = {Efficient modeling of quasi-periodic data with seasonal gaussian process},
  volume       = {35},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Fuzzy K-expectiles clustering. <em>SAC</em>, <em>35</em>(2), 1-19. (<a href='https://doi.org/10.1007/s11222-025-10566-1'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper the Fuzzy K-expectiles clustering model is proposed. The model takes into account the asymmetry inherent in the data distribution, extending its applicability to a broader spectrum of data than the Fuzzy K-means. To achieve this, the Fuzzy K-expectiles clustering model introduces the cluster centroid expectile, and assigns data points based on expectile distances. An adaptive asymmetry parameter is specified for each variable and for each cluster The performance of the adaptive Fuzzy K-expectiles model is compared with other clustering models suggested in the literature. To show the performances of the proposed model three simulation studies and three applications to real datasets are presented.},
  archive      = {J_SAC},
  author       = {D’Urso, Pierpaolo and De Giovanni, Livia and Federico, Lorenzo and Vitale, Vincenzina},
  doi          = {10.1007/s11222-025-10566-1},
  journal      = {Statistics and Computing},
  month        = {4},
  number       = {2},
  pages        = {1-19},
  shortjournal = {Stat. Comput.},
  title        = {Fuzzy K-expectiles clustering},
  volume       = {35},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Geographically weighted quantile regression for count data. <em>SAC</em>, <em>35</em>(2), 1-18. (<a href='https://doi.org/10.1007/s11222-025-10568-z'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, a methodological framework known as geographically weighted quantile regression (GWQR) has emerged for spatial data analysis. This framework offers the abilities to simultaneously explore spatial heterogeneity or nonstationarity in regression relationships and to estimate various conditional quantile functions. However, the current configuration of GWQR is limited to the analysis of continuous dependent variables. Discrete count data are observed in many disciplines. Whenever modeling such outcomes is necessary, the conventional GWQR approach is inadequate and fails to provide comprehensive insights into count data. To address this issue, this study aims to extend the GWQR framework originally designed for continuous dependent variables to accommodate count outcomes. We introduce an approach called geographically weighted count quantile regression (GWCQR), wherein the model specification is based on the smoothing of count responses through a jittering procedure. A semiparametric counterpart that allows for the inclusion of both spatially varying and invariant coefficients is also discussed. Finally, the proposed techniques are applied to a dataset of dengue fever in Taiwan as an empirical illustration.},
  archive      = {J_SAC},
  author       = {Chen, Vivian Yi-Ju and Wang, Shi-Ting},
  doi          = {10.1007/s11222-025-10568-z},
  journal      = {Statistics and Computing},
  month        = {4},
  number       = {2},
  pages        = {1-18},
  shortjournal = {Stat. Comput.},
  title        = {Geographically weighted quantile regression for count data},
  volume       = {35},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Anisotropic multidimensional smoothing using bayesian tensor product P-splines. <em>SAC</em>, <em>35</em>(2), 1-14. (<a href='https://doi.org/10.1007/s11222-025-10569-y'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We introduce a highly efficient fully Bayesian approach for anisotropic multidimensional smoothing. The main challenge in this context is the Markov chain Monte Carlo (MCMC) update of the smoothing parameters as their full conditional posterior comprises a pseudo-determinant that appears to be intractable at first sight. As a consequence, existing implementations are computationally feasible only for the estimation of two-dimensional tensor product smooths, which is, however, too restrictive for many applications. In this paper, we break this barrier and derive closed-form expressions for the log-pseudo-determinant and its first and second order partial derivatives. These expressions are valid for arbitrary dimension and very fast to evaluate, which allows us to set up an efficient MCMC sampler with derivative-based Metropolis–Hastings (MH) updates for the smoothing parameters. We derive simple formulas for low-dimensional slices and averages to facilitate visualization and investigate hyperprior sensitivity. We show that our new approach outperforms previous suggestions in the literature in terms of accuracy, scalability and computational cost and demonstrate its applicability through an illustrating temperature data example from spatio-temporal statistics.},
  archive      = {J_SAC},
  author       = {Bach, Paul and Klein, Nadja},
  doi          = {10.1007/s11222-025-10569-y},
  journal      = {Statistics and Computing},
  month        = {4},
  number       = {2},
  pages        = {1-14},
  shortjournal = {Stat. Comput.},
  title        = {Anisotropic multidimensional smoothing using bayesian tensor product P-splines},
  volume       = {35},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Logit unfolding choice models for binary data. <em>SAC</em>, <em>35</em>(2), 1-18. (<a href='https://doi.org/10.1007/s11222-025-10570-5'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Discrete choice models with non-monotonic response functions are important in many areas of application, especially political sciences and marketing. This paper describes a novel unfolding model for binary data that allows for heavy-tailed shocks to the underlying utilities. One of our key contributions is a Markov chain Monte Carlo algorithm that requires little or no parameter tuning, fully explores the support of the posterior distribution, and can be used to fit various extensions of our core model that involve (Bayesian) hypothesis testing on the latent construct. Our empirical evaluations of the model and the associated algorithm suggest that they provide better complexity-adjusted fit to voting data from the United States House of Representatives.},
  archive      = {J_SAC},
  author       = {Lei, Rayleigh and Rodriguez, Abel},
  doi          = {10.1007/s11222-025-10570-5},
  journal      = {Statistics and Computing},
  month        = {4},
  number       = {2},
  pages        = {1-18},
  shortjournal = {Stat. Comput.},
  title        = {Logit unfolding choice models for binary data},
  volume       = {35},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Particle gibbs for likelihood-free inference of stochastic volatility models. <em>SAC</em>, <em>35</em>(2), 1-15. (<a href='https://doi.org/10.1007/s11222-025-10571-4'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Stochastic volatility models (SVMs) are widely used in finance and econometrics for analyzing and interpreting volatility. Real financial data are often observed to have heavy tails, which violate a Gaussian assumption and may be better modeled using the stable distribution. However, the intractable density of the stable distribution hinders the use of common computational methods such as Markov chain Monte Carlo (MCMC) for parameter inference of SVMs. In this paper, we propose a new particle Gibbs sampler as a strategy to handle SVMs with intractable likelihoods in the approximate Bayesian computation (ABC) setting. The proposed sampler incorporates a conditional auxiliary particle filter, which can help mitigate the weight degeneracy often encountered when using ABC. Simulation studies demonstrate the efficacy of our sampler for inferring SVM parameters when compared to existing particle Gibbs samplers based on the conditional bootstrap filter, and for inferring both SVM and stable distribution parameters when compared to existing particle MCMC samplers. As a real data application, we apply the proposed sampler for fitting an SVM to S&P 500 Index time-series data during the 2008–2009 financial crisis.},
  archive      = {J_SAC},
  author       = {Hou, Zhaoran and Wong, Samuel W. K.},
  doi          = {10.1007/s11222-025-10571-4},
  journal      = {Statistics and Computing},
  month        = {4},
  number       = {2},
  pages        = {1-15},
  shortjournal = {Stat. Comput.},
  title        = {Particle gibbs for likelihood-free inference of stochastic volatility models},
  volume       = {35},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Real-time inference for smoothing quantile regression on streaming datasets with heterogeneity detection. <em>SAC</em>, <em>35</em>(2), 1-40. (<a href='https://doi.org/10.1007/s11222-025-10572-3'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Streaming data is generated at high speed and in large quantities over time, and it calls for online learning to deal with it. In this paper, a new online updating method is established for smoothing quantile regression to make inferences in real-time. The renewable estimators are updated only by the current dataset and summary statistics of historical datasets. This method is adapted to the streaming datasets containing small samples. Theoretically, it is proved that renewable estimators have consistency and asymptotic normality and equivalence to pooled offline estimators based on all datasets. The dynamic bandwidth selection is applied to estimate the asymptotic covariance matrix in an online manner, which is theoretically highly asymptotically efficient. In particular, the renewable estimator provides asymptotic confidence intervals that are asymptotically smaller than those generated by existing methods, thereby improving the accuracy of interval estimation. Additionally, our approach addresses the common assumption of homogeneous models by accommodating non-parametric heterogeneity and detecting and removing anomalous data batches through an online screening process. Meanwhile, numerical simulations verify the theoretical results and outcomes on real datasets illustrate that our method is adapted to real streaming data situations.},
  archive      = {J_SAC},
  author       = {Wang, Yidan and Zhang, Lingyun and Gai, Yujie},
  doi          = {10.1007/s11222-025-10572-3},
  journal      = {Statistics and Computing},
  month        = {4},
  number       = {2},
  pages        = {1-40},
  shortjournal = {Stat. Comput.},
  title        = {Real-time inference for smoothing quantile regression on streaming datasets with heterogeneity detection},
  volume       = {35},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Exploratory analysis of dynamic networks using latent functions. <em>SAC</em>, <em>35</em>(2), 1-18. (<a href='https://doi.org/10.1007/s11222-025-10573-2'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Dynamic networks, which capture the evolving interactions among entities, have flourished in various scientific fields. We propose a framework for exploratory analysis of these dynamic networks by representing them as latent functions. This framework comprises several visualization tools based on functional data analysis, specifically tailored for addressing typical tasks such as community detection, central node identification, and change point discovery. Besides, we develop a computationally efficient algorithm to obtain the latent functions. Through comprehensive simulation studies conducted under commonly investigated settings, we demonstrate the effectiveness of these tools. Furthermore, we apply the proposed tools to three representative and intriguing real-world networks, yielding enlightening discoveries. An R package for implementing the proposed methods, along with supplementary materials for this article, is available online.},
  archive      = {J_SAC},
  author       = {Shi, Haosheng and Dai, Wenlin},
  doi          = {10.1007/s11222-025-10573-2},
  journal      = {Statistics and Computing},
  month        = {4},
  number       = {2},
  pages        = {1-18},
  shortjournal = {Stat. Comput.},
  title        = {Exploratory analysis of dynamic networks using latent functions},
  volume       = {35},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Statistical inference and goodness-of-fit test in functional data via error distribution function. <em>SAC</em>, <em>35</em>(2), 1-16. (<a href='https://doi.org/10.1007/s11222-025-10574-1'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A kernel distribution estimator (KDE) is proposed for the error distribution in the functional data, which is computed from the residuals of the B-spline trajectories over all the measurements. The maximal stochastic process between the KDE and the error distribution is shown to converge to a Gaussian process with mean zero and specified covariance function under some mild conditions. Thus, a simultaneous confidence band (SCB) is constructed for the error distribution based on the KDE in the dense functional data. The proposed SCB is applicable in not only the independent functional data but also the functional time series. In addition, the symmetric test is proposed for the error distribution, as well as a goodness-of-fit test for mean function by using the bootstrap method. Simulation studies examine the finite sample performance of the SCB and show the bootstrap method performs well in numerical studies. The proposed theory is illustrated by the electroencephalogram (EEG) functional data.},
  archive      = {J_SAC},
  author       = {Zhong, Chen},
  doi          = {10.1007/s11222-025-10574-1},
  journal      = {Statistics and Computing},
  month        = {4},
  number       = {2},
  pages        = {1-16},
  shortjournal = {Stat. Comput.},
  title        = {Statistical inference and goodness-of-fit test in functional data via error distribution function},
  volume       = {35},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). An EM algorithm for fitting matrix-variate normal distributions on interval-censored and missing data. <em>SAC</em>, <em>35</em>(2), 1-11. (<a href='https://doi.org/10.1007/s11222-025-10575-0'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Matrix-variate distributions are powerful tools for modeling three-way datasets that often arise in longitudinal and multidimensional spatio-temporal studies. However, observations in these datasets can be missing or subject to some detection limits because of the restriction of the experimental apparatus. Here, we develop an efficient EM-type algorithm for maximum likelihood estimation of parameters, in the context of interval-censored and/or missing data, utilizing the matrix-variate normal distribution. This algorithm provides closed-form expressions that rely on truncated moments, offering a reliable approach to parameter estimation under these conditions. Results obtained from the analysis of both simulated data and real case studies concerning water quality monitoring are reported to demonstrate the effectiveness of the proposed method.},
  archive      = {J_SAC},
  author       = {Lachos, Victor H. and Tomarchio, Salvatore D. and Punzo, Antonio and Ingrassia, Salvatore},
  doi          = {10.1007/s11222-025-10575-0},
  journal      = {Statistics and Computing},
  month        = {4},
  number       = {2},
  pages        = {1-11},
  shortjournal = {Stat. Comput.},
  title        = {An EM algorithm for fitting matrix-variate normal distributions on interval-censored and missing data},
  volume       = {35},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Testing common degree-correction parameters of multilayer networks. <em>SAC</em>, <em>35</em>(2), 1-22. (<a href='https://doi.org/10.1007/s11222-025-10576-z'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Graph (or network) is a mathematical structure that has been widely used to model relational data. As real-world systems get more complex, multilayer (or multiple) networks are employed to represent diverse patterns of relationships among the objects in the systems. One active research problem in multilayer networks analysis is to study the common properties of the networks. In this paper, we study whether multilayer networks share the same degree-correction parameters, which is a special case of the widely studied common invariant subspace problem. We first attempt to answer this question by means of hypothesis testing. The null hypothesis states that the multilayer networks share the same degree-correction parameters, and under the alternative hypothesis, there exist at least two networks that have different degree-correction parameters. We propose a weighted degree difference test, derive the limiting distribution of the test statistic and provide an analytical analysis of the power. Simulation study shows that the proposed test has satisfactory performance, and a real data application is provided.},
  archive      = {J_SAC},
  author       = {Yuan, Mingao and Yao, Qianqian},
  doi          = {10.1007/s11222-025-10576-z},
  journal      = {Statistics and Computing},
  month        = {4},
  number       = {2},
  pages        = {1-22},
  shortjournal = {Stat. Comput.},
  title        = {Testing common degree-correction parameters of multilayer networks},
  volume       = {35},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Online structural break detection in financial durations. <em>SAC</em>, <em>35</em>(2), 1-19. (<a href='https://doi.org/10.1007/s11222-025-10577-y'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Durations between events of interest such as intra-day transactions of assets can reflect the volatility of asset prices in financial markets. The diverse dynamics of these intervals, which we refer to as financial durations, offer valuable insights into market behavior for investors. Inspection of streaming price data for structural breaks and timely and accurate detection of transitions between different duration patterns within a trading day enables practitioners to update parameters of suitable duration models. In this article, an innovative Ensemble Penalized Estimating Function (E-PEF) approach is proposed to effectively detect change points in the logarithmic autoregressive conditional duration models for financial durations. As a quasi-score-based online detection approach, this methodology leverages Mahalanobis distances and the block bootstrap sampling method to compute critical values for finite sample time series. The online structural break detection rule is informed by comparing observed quasi-scores in the monitoring period with pre-calculated critical values from training data in an ensemble manner. Extensive simulations demonstrate that the E-PEF method has fast structural break detection performance, while effectively controlling the probability of false detection. In the real data application, we applied our method to identify structural breaks for four assets, explored their relationships with summarized changes in volatility patterns, and noted several considerations for practitioners in the financial market.},
  archive      = {J_SAC},
  author       = {Wang, Yanzhao and Zhang, Yaohua and Zou, Jian and Ravishanker, Nalini},
  doi          = {10.1007/s11222-025-10577-y},
  journal      = {Statistics and Computing},
  month        = {4},
  number       = {2},
  pages        = {1-19},
  shortjournal = {Stat. Comput.},
  title        = {Online structural break detection in financial durations},
  volume       = {35},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Function-on-function regression models with nonlinear dynamic effect and linear concurrent effect. <em>SAC</em>, <em>35</em>(2), 1-15. (<a href='https://doi.org/10.1007/s11222-025-10578-x'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a function-on-function regression model that predicts a functional response by both a nonlinear dynamic effect of a functional predictor and a linear concurrent effect of another functional predictor. The nonlinear dynamic effect is characterized by taking an integral of a time-dependent two-dimensional smooth surface and the linear concurrent effect is modeled through a time-varying coefficient. The model structure combines the flexibility of nonlinear modeling with the interpretability of the linear concurrent effect. To approximate the two-dimensional surface, we use tensor product basis expansions, and for the time-varying coefficient in the concurrent effect, we employ B-spline expansions. The expansion parameters for each effect are estimated iteratively to account for the mutual dependencies between these two estimated effects. Each iteration of parameter estimation involves solving a penalized least squares problem. We establish the asymptotic properties of our estimator. The numerical performance of the proposed method is illustrated by simulation studies and two real data applications.},
  archive      = {J_SAC},
  author       = {Jia, Shifan and Shi, Haolun and Guan, Tianyu},
  doi          = {10.1007/s11222-025-10578-x},
  journal      = {Statistics and Computing},
  month        = {4},
  number       = {2},
  pages        = {1-15},
  shortjournal = {Stat. Comput.},
  title        = {Function-on-function regression models with nonlinear dynamic effect and linear concurrent effect},
  volume       = {35},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Robust bayesian functional principal component analysis. <em>SAC</em>, <em>35</em>(2), 1-25. (<a href='https://doi.org/10.1007/s11222-025-10580-3'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We develop a robust Bayesian functional principal component analysis (RB-FPCA) method that utilizes the skew elliptical class of distributions to model functional data, which are observed over a continuous domain. This approach effectively captures the primary sources of variation among curves, even in the presence of outliers, and provides a more robust and accurate estimation of the covariance function and principal components. The proposed method can also handle sparse functional data, where only a few observations per curve are available. We employ annealed sequential Monte Carlo for posterior inference, which offers several advantages over conventional Markov chain Monte Carlo algorithms. To evaluate the performance of our proposed model, we conduct simulation studies, comparing it with well-known frequentist and conventional Bayesian methods. The results show that our method outperforms existing approaches in the presence of outliers and performs competitively in outlier-free datasets. Finally, we demonstrate the effectiveness of our method by applying it to environmental and biological data to identify outlying functional observations. The implementation of our proposed method and applications are available at https://github.com/SFU-Stat-ML/RBFPCA .},
  archive      = {J_SAC},
  author       = {Zhang, Jiarui and Cao, Jiguo and Wang, Liangliang},
  doi          = {10.1007/s11222-025-10580-3},
  journal      = {Statistics and Computing},
  month        = {4},
  number       = {2},
  pages        = {1-25},
  shortjournal = {Stat. Comput.},
  title        = {Robust bayesian functional principal component analysis},
  volume       = {35},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Inference on diffusion processes related to a general growth model. <em>SAC</em>, <em>35</em>(2), 1-19. (<a href='https://doi.org/10.1007/s11222-025-10562-5'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper considers two stochastic diffusion processes associated with a general growth curve that includes a wide family of growth phenomena. The resulting processes are lognormal and Gaussian, and for them inference is addressed by means of the maximum likelihood method. The complexity of the resulting system of equations requires the use of metaheuristic techniques. The limitation of the parameter space, typically required by all metaheuristic techniques, is also provided by means of a suitable strategy. Several simulation studies are performed to evaluate to goodness of the proposed methodology, and an application to real data is described.},
  archive      = {J_SAC},
  author       = {Albano, Giuseppina and Barrera, Antonio and Giorno, Virginia and Torres-Ruiz, Francisco},
  doi          = {10.1007/s11222-025-10562-5},
  journal      = {Statistics and Computing},
  month        = {4},
  number       = {2},
  pages        = {1-19},
  shortjournal = {Stat. Comput.},
  title        = {Inference on diffusion processes related to a general growth model},
  volume       = {35},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Density regression via dirichlet process mixtures of normal structured additive regression models. <em>SAC</em>, <em>35</em>(2), 1-20. (<a href='https://doi.org/10.1007/s11222-025-10567-0'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Within Bayesian nonparametrics, dependent Dirichlet process mixture models provide a flexible approach for conducting inference about the conditional density function. However, several formulations of this class make either restrictive modelling assumptions or involve intricate algorithms for posterior inference. We propose a flexible and computationally convenient approach for density regression based on a single-weights dependent Dirichlet process mixture of normal distributions model for univariate continuous responses. We assume an additive structure for the mean of each mixture component and incorporate the effects of continuous covariates through smooth functions. The key components of our modelling approach are penalised B-splines and their bivariate tensor product extension. Our method also seamlessly accommodates categorical covariates, linear effects of continuous covariates, varying coefficient terms, and random effects, which is why we refer our model as a Dirichlet process mixture of normal structured additive regression models. A notable feature of our method is the simplicity of posterior simulation using Gibbs sampling, as closed-form full conditional distributions for all model parameters are available. Results from a simulation study demonstrate that our approach successfully recovers the true conditional densities and other regression functionals in challenging scenarios. Applications to three real datasets further underpin the broad applicability of our method. An R package, DDPstar, implementing the proposed method is provided.},
  archive      = {J_SAC},
  author       = {Rodríguez-Álvarez, María Xosé and Inácio, Vanda and Klein, Nadja},
  doi          = {10.1007/s11222-025-10567-0},
  journal      = {Statistics and Computing},
  month        = {4},
  number       = {2},
  pages        = {1-20},
  shortjournal = {Stat. Comput.},
  title        = {Density regression via dirichlet process mixtures of normal structured additive regression models},
  volume       = {35},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multilevel latent class models for cross-classified categorical data: Model definition and estimation through stochastic EM. <em>SAC</em>, <em>35</em>(2), 1-15. (<a href='https://doi.org/10.1007/s11222-025-10579-w'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present an extension of the multilevel latent class model for dealing with multilevel cross-classified categorical data. Cross-classified data structures arise when observations are simultaneously nested within two or more groups, for example, children nested within both schools and neighborhoods. More specifically, we propose extending the standard hierarchical latent class model, which contains mixture components at two levels, say for children and schools, by including a separate set of mixture components for each of the higher-level crossed classifications, say for schools and neighborhoods. Because of the complex dependency structure arising from the cross-classified nature of the data, it is no longer possible to obtain maximum likelihood estimates of the model parameters, for example, using the EM algorithm. As a solution to the estimation problem, we propose an approximate estimation approach using a stochastic version of the EM algorithm. The performance of this approach, which resembles Gibbs sampling, was investigated through a set of simulation studies. Moreover, the application of the new model is illustrated using an Italian dataset on the quality of university experience at degree programme level, with degree programmes nested in both universities and fields of study.},
  archive      = {J_SAC},
  author       = {Columbu, S. and Piras, N. and Vermunt, J. K.},
  doi          = {10.1007/s11222-025-10579-w},
  journal      = {Statistics and Computing},
  month        = {4},
  number       = {2},
  pages        = {1-15},
  shortjournal = {Stat. Comput.},
  title        = {Multilevel latent class models for cross-classified categorical data: Model definition and estimation through stochastic EM},
  volume       = {35},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Control charts for monitoring weibull quantile under generalized hybrid and progressive censoring schemes. <em>SAC</em>, <em>35</em>(2), 1-20. (<a href='https://doi.org/10.1007/s11222-025-10581-2'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we propose control charts to monitor quantiles of Weibull distribution for Type-I generalized hybrid censoring scheme and Type-II progressive censoring scheme. Parametric bootstrap method is employed to derive the control limits. Monte Carlo simulations are carried out to assess the in-control and out-of-control performance of the proposed charts. The phase-I analysis evaluates the performance of the charts through average run lengths for various combinations of quantile, false-alarm rate, sample size, and censoring parameters. Chart performance is thoroughly investigated in the phase-II analysis for several choices of shifts in the scale and shape parameters of Weibull distribution along with different censoring schemes. The charts for both censoring schemes have been demonstrated to be highly effective in detecting out-of-control signals, both in terms of magnitude and speed. The proposed charts are illustrated through applications in reliability and clinical practices. While both charts show similar performance in terms of speed, the chart with the optimal Type-II progressive censoring scheme outperforms the one with the Type-I generalized hybrid censoring scheme in terms of magnitude in both examples.},
  archive      = {J_SAC},
  author       = {Modok, Bidhan and Chowdhury, Shovan and Kundu, Amarjit},
  doi          = {10.1007/s11222-025-10581-2},
  journal      = {Statistics and Computing},
  month        = {4},
  number       = {2},
  pages        = {1-20},
  shortjournal = {Stat. Comput.},
  title        = {Control charts for monitoring weibull quantile under generalized hybrid and progressive censoring schemes},
  volume       = {35},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Using prior-data conflict to tune bayesian regularized regression models. <em>SAC</em>, <em>35</em>(2), 1-19. (<a href='https://doi.org/10.1007/s11222-025-10582-1'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In high-dimensional regression models, variable selection becomes challenging from a computational and theoretical perspective. Bayesian regularized regression via shrinkage priors like the Laplace or spike-and-slab prior are effective methods for variable selection in $$p>n$$ scenarios provided the shrinkage priors are configured adequately. We propose an empirical Bayes configuration using checks for prior-data conflict: tests that assess whether there is disagreement in parameter information provided by the prior and data. We apply our proposed method to the Bayesian LASSO and spike-and-slab shrinkage priors in the linear regression model and assess the variable selection performance of our prior configurations through a high-dimensional simulation study. Additionally, we apply our method to proteomic data collected from patients admitted to the Albany Medical Center in Albany NY in April of 2020 with COVID-like respiratory issues. Simulation results suggest our proposed configurations may outperform competing models when the true regression effects are small.},
  archive      = {J_SAC},
  author       = {Biziaev, Timofei and Kopciuk, Karen and Chekouo, Thierry},
  doi          = {10.1007/s11222-025-10582-1},
  journal      = {Statistics and Computing},
  month        = {4},
  number       = {2},
  pages        = {1-19},
  shortjournal = {Stat. Comput.},
  title        = {Using prior-data conflict to tune bayesian regularized regression models},
  volume       = {35},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Directional data analysis: Spherical cauchy or poisson kernel-based distribution?. <em>SAC</em>, <em>35</em>(2), 1-21. (<a href='https://doi.org/10.1007/s11222-025-10583-0'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In 2020, two novel distributions for the analysis of directional data were introduced: the spherical Cauchy distribution and the Poisson kernel-based distribution. This paper provides a detailed exploration of both distributions within various analytical frameworks. To enhance the practical utility of these distributions, alternative parametrizations that offer advantages in numerical stability and parameter estimation are presented, such as implementation of the Newton–Raphson algorithm for parameter estimation, while facilitating a more efficient and simplified approach in the regression framework. Additionally, a two-sample location test based on the log-likelihood ratio test is introduced. This test is designed to assess whether the location parameters of two populations can be assumed equal. The maximum likelihood discriminant analysis framework is developed for classification purposes, and finally, the problem of clustering directional data is addressed, by fitting finite mixtures of Spherical Cauchy or Poisson kernel-based distributions. Empirical validation is conducted through comprehensive simulation studies and real data applications, wherein the performance of the spherical Cauchy and Poisson kernel-based distributions is systematically compared.},
  archive      = {J_SAC},
  author       = {Tsagris, Michail and Papastamoulis, Panagiotis and Kato, Shogo},
  doi          = {10.1007/s11222-025-10583-0},
  journal      = {Statistics and Computing},
  month        = {4},
  number       = {2},
  pages        = {1-21},
  shortjournal = {Stat. Comput.},
  title        = {Directional data analysis: Spherical cauchy or poisson kernel-based distribution?},
  volume       = {35},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multilevel importance sampling for rare events associated with the McKean–Vlasov equation. <em>SAC</em>, <em>35</em>(1), 1-21. (<a href='https://doi.org/10.1007/s11222-024-10508-3'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This work combines multilevel Monte Carlo with importance sampling to estimate rare-event quantities that can be expressed as the expectation of a Lipschitz observable of the solution to a broad class of McKean–Vlasov stochastic differential equations. We extend the double loop Monte Carlo (DLMC) estimator introduced in this context in Ben Rached et al. (Stat Comput, 2024. https://doi.org/10.1007/s11222-024-10497-3 ) to the multilevel setting. We formulate a novel multilevel DLMC estimator and perform a comprehensive cost-error analysis yielding new and improved complexity results. Crucially, we devise an antithetic sampler to estimate level differences guaranteeing reduced computational complexity for the multilevel DLMC estimator compared with the single-level DLMC estimator. To address rare events, we apply the importance sampling scheme, obtained via stochastic optimal control in Ben Rached et al. (2024), over all levels of the multilevel DLMC estimator. Combining importance sampling and multilevel DLMC reduces computational complexity by one order and drastically reduces the associated constant compared to the single-level DLMC estimator without importance sampling. We illustrate the effectiveness of the proposed multilevel DLMC estimator on the Kuramoto model from statistical physics with Lipschitz observables, confirming the reduced complexity from $${\mathcal {O}(\textrm{TOL}_{\textrm{r}}^{-4})}$$ for the single-level DLMC estimator to $${\mathcal {O}(\textrm{TOL}_{\textrm{r}}^{-3})}$$ while providing a feasible estimate of rare-event quantities up to prescribed relative error tolerance $$\textrm{TOL}_{\textrm{r}}$$ .},
  archive      = {J_SAC},
  author       = {Ben Rached, Nadhir and Haji-Ali, Abdul-Lateef and Subbiah Pillai, Shyam Mohan and Tempone, Raúl},
  doi          = {10.1007/s11222-024-10508-3},
  journal      = {Statistics and Computing},
  month        = {2},
  number       = {1},
  pages        = {1-21},
  shortjournal = {Stat. Comput.},
  title        = {Multilevel importance sampling for rare events associated with the McKean–Vlasov equation},
  volume       = {35},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Bootstrap estimation of the proportion of outliers in robust regression. <em>SAC</em>, <em>35</em>(1), 1-14. (<a href='https://doi.org/10.1007/s11222-024-10526-1'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper presents a nonparametric bootstrap method for estimating the proportions of inliers and outliers in robust regression models. Our approach is based on the concept of stability, providing robustness against distributional assumptions and eliminating the need for pre-specified confidence levels. Through numerical experiments, we demonstrate that this method yields more accurate and stable estimates than existing alternatives. Additionally, the generated instability paths offer a valuable graphical tool for understanding the inlier and outlier distributions within the data. The method naturally extends to generalized linear models, where we find that variance-stabilizing transformations produce residuals that are well-suited for outlier detection. Applications to two real-world datasets further illustrate the practical utility of our approach in identifying outliers.},
  archive      = {J_SAC},
  author       = {Heng, Qiang and Lange, Kenneth},
  doi          = {10.1007/s11222-024-10526-1},
  journal      = {Statistics and Computing},
  month        = {2},
  number       = {1},
  pages        = {1-14},
  shortjournal = {Stat. Comput.},
  title        = {Bootstrap estimation of the proportion of outliers in robust regression},
  volume       = {35},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A boosting framework for positive-unlabeled learning. <em>SAC</em>, <em>35</em>(1), 1-22. (<a href='https://doi.org/10.1007/s11222-024-10529-y'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Positive-unlabeled (PU) learning deals with binary classification problems where only positive and unlabeled data are available. In this paper, we introduce a novel boosting framework, Adaptive PU (AdaPU), for learning from PU data. AdaPU builds an ensemble of weak classifiers using weak learners tailored to PU data. We propose two main approaches for learning the weak classifiers: a direct loss minimization approach that learns weak classifiers to greedily minimize PU-data-based estimates of the exponential loss, specifically, the unbiased PU estimate and the non-negative PU estimate; and a constrained loss minimization approach that learns weak classifiers to greedily minimize the unbiased PU estimate of the exponential loss, subject to regularization constraints. The direct loss minimization approach, while natural and simple, often yields weak learners prone to overfitting or leads to computationally expensive algorithms. On the other hand, the constrained loss minimization approach can effectively alleviate overfitting and allow the design of efficient weak learners. In particular, we propose a tailored weak learner for the simple class of decision stumps, or one-level decision trees, which interestingly demonstrates strong performance in comparison to various other weak classifiers. Furthermore, we provide several theoretical results on the performance of AdaPU. We performed extensive experiments to evaluate the variants of AdaPU and various baseline algorithms. Our results demonstrate the effectiveness of the constrained loss minimization approach.},
  archive      = {J_SAC},
  author       = {Zhao, Yawen and Zhang, Mingzhe and Zhang, Chenhao and Chen, Weitong and Ye, Nan and Xu, Miao},
  doi          = {10.1007/s11222-024-10529-y},
  journal      = {Statistics and Computing},
  month        = {2},
  number       = {1},
  pages        = {1-22},
  shortjournal = {Stat. Comput.},
  title        = {A boosting framework for positive-unlabeled learning},
  volume       = {35},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Exact gradient evaluation for adaptive quadrature approximate marginal likelihood in mixed models for grouped data. <em>SAC</em>, <em>35</em>(1), 1-18. (<a href='https://doi.org/10.1007/s11222-024-10536-z'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A method is introduced for approximate marginal likelihood inference via adaptive Gaussian quadrature in mixed models with a single grouping factor. The core technical contribution is an algorithm for computing the exact gradient of the approximate log-marginal likelihood. This leads to efficient maximum likelihood via quasi-Newton optimization that is demonstrated to be faster than existing approaches based on finite-differenced gradients or derivative-free optimization. The method is specialized to Bernoulli mixed models with multivariate, correlated Gaussian random effects; here computations are performed using an inverse log-Cholesky parameterization of the Gaussian density that involves no matrix decomposition during model fitting, while Wald confidence intervals are provided for variance parameters on the original scale. Simulations give evidence of these intervals attaining nominal coverage if enough quadrature points are used, for data comprised of a large number of very small groups exhibiting large between-group heterogeneity. The Laplace approximation is well-known to give especially poor coverage and high bias for data comprised of a large number of small groups. Adaptive quadrature mitigates this, and the methods in this paper improve the computational feasibility of this more accurate method. All results may be reproduced using code available at https://github.com/awstringer1/aghmm-paper-code .},
  archive      = {J_SAC},
  author       = {Stringer, Alex},
  doi          = {10.1007/s11222-024-10536-z},
  journal      = {Statistics and Computing},
  month        = {2},
  number       = {1},
  pages        = {1-18},
  shortjournal = {Stat. Comput.},
  title        = {Exact gradient evaluation for adaptive quadrature approximate marginal likelihood in mixed models for grouped data},
  volume       = {35},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Huber-energy measure quantization. <em>SAC</em>, <em>35</em>(1), 1-22. (<a href='https://doi.org/10.1007/s11222-024-10540-3'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We describe a measure quantization procedure i.e., an algorithm which finds the best approximation of a target probability law (and more generally signed finite variation measure) by a sum of Q Dirac masses (Q being the quantization parameter). The procedure is implemented by minimizing the statistical distance between the original measure and its quantized version; the distance is built from a negative definite kernel and, if necessary, can be computed on the fly and feed to a stochastic optimization algorithm (such as SGD, Adam, ...). We investigate theoretically the fundamental questions of existence of the optimal measure quantizer and identify what are the required kernel properties that guarantee suitable behavior. We propose two best linear unbiased (BLUE) estimators for the squared statistical distance and use them in an unbiased procedure, called HEMQ, to find the optimal quantization. We test HEMQ on several databases: multi-dimensional Gaussian mixtures, Wiener space cubature, Italian wine cultivars and the MNIST image database. The results indicate that the HEMQ algorithm is robust and versatile and, for the class of Huber-energy kernels, matches the expected intuitive behavior.},
  archive      = {J_SAC},
  author       = {Turinici, Gabriel},
  doi          = {10.1007/s11222-024-10540-3},
  journal      = {Statistics and Computing},
  month        = {2},
  number       = {1},
  pages        = {1-22},
  shortjournal = {Stat. Comput.},
  title        = {Huber-energy measure quantization},
  volume       = {35},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Estimating a signal subspace in the presence of impulsive noise. <em>SAC</em>, <em>35</em>(1), 1-16. (<a href='https://doi.org/10.1007/s11222-024-10528-z'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider the problem of estimating a signal subspace in the presence of interference that contaminates some proportion of the received observations. Our emphasis is on detecting the contaminated observations so that the signal subspace can be estimated with the contaminated observations discarded. To this end, we employ a signal model which explicitly includes an interference term that is distinct from environmental noise. To detect when the interference term is nonzero, we estimate the interference term using an optimization problem with a sparsity-inducing group SLOPE penalty which accounts for simultaneous sparsity across all channels of the multichannel signal. We propose an iterative algorithm which efficiently computes the observations estimated to contain interference. Theoretical support for the accuracy of our interference estimator is provided by bounding its false discovery rate, the expected proportion of uncontaminated observations among those estimated to be contaminated. Finally, we demonstrate the empirical performance of our contributions in a number of simulated experiments.},
  archive      = {J_SAC},
  author       = {Bassett, Robert L. and Oh, Micah Y.},
  doi          = {10.1007/s11222-024-10528-z},
  journal      = {Statistics and Computing},
  month        = {2},
  number       = {1},
  pages        = {1-16},
  shortjournal = {Stat. Comput.},
  title        = {Estimating a signal subspace in the presence of impulsive noise},
  volume       = {35},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Large-scale constrained gaussian processes for shape-restricted function estimation. <em>SAC</em>, <em>35</em>(1), 1-16. (<a href='https://doi.org/10.1007/s11222-024-10541-2'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we revisit the problem of Bayesian shape-restricted function estimation. The finite-dimensional Gaussian process (GP) approximation proposed by Maatouk and Bay (Math Geosci 49(5): 557–582, 2017) is considered, which admits an equivalent formulation of the shape constraints in terms of basis coefficients. This approximation satisfies a wide variety of shape constraints everywhere, whether applied alone, in combination, or sequentially. We propose a new, efficient, and fast algorithm for sampling from a large Gaussian vector extracted from a stationary GP. The proposed approach significantly improves the novel circulant embedding technique proposed by Ray et al. (Stat Comput 30(4): 839–853, 2020) for efficiently sampling from the resulting posterior constrained distribution. The main idea of the algorithm developed in the present paper is to divide the input domain into smaller subdomains and apply a cross-correlated technique to address the correlation structure in the entire domain. As the number of subdomains increases, the computational complexity is drastically reduced. The developed algorithm is accurate and efficient, as demonstrated through comparisons with competing approaches. The performance of the proposed approach has been evaluated within the context of Bayesian shape-restricted function estimation.},
  archive      = {J_SAC},
  author       = {Maatouk, Hassan and Rullière, Didier and Bay, Xavier},
  doi          = {10.1007/s11222-024-10541-2},
  journal      = {Statistics and Computing},
  month        = {2},
  number       = {1},
  pages        = {1-16},
  shortjournal = {Stat. Comput.},
  title        = {Large-scale constrained gaussian processes for shape-restricted function estimation},
  volume       = {35},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Improving power by conditioning on less in post-selection inference for changepoints. <em>SAC</em>, <em>35</em>(1), 1-23. (<a href='https://doi.org/10.1007/s11222-024-10542-1'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Post-selection inference has recently been proposed as a way of quantifying uncertainty about detected changepoints. The idea is to run a changepoint detection algorithm, and then re-use the same data to perform a test for a change near each of the detected changes. By defining the p-value for the test appropriately, so that it is conditional on the information used to choose the test, this approach will produce valid p-values. We show how to improve the power of these procedures by conditioning on less information. This gives rise to an ideal post-selection p-value that is intractable but can be approximated by Monte Carlo. We show that for any Monte Carlo sample size, this procedure produces valid p-values, and empirically that noticeable increase in power is possible with only very modest Monte Carlo sample sizes. Our procedure is easy to implement given existing post-selection inference methods, as we just need to generate perturbations of the data set and re-apply the post-selection method to each of these. On genomic data consisting of human GC content, our procedure increases the number of significant changepoints that are detected when compared to the method of Jewell et al. (J R Stat Soc Ser B 84(4):1082-1104, 2022).},
  archive      = {J_SAC},
  author       = {Carrington, Rachel and Fearnhead, Paul},
  doi          = {10.1007/s11222-024-10542-1},
  journal      = {Statistics and Computing},
  month        = {2},
  number       = {1},
  pages        = {1-23},
  shortjournal = {Stat. Comput.},
  title        = {Improving power by conditioning on less in post-selection inference for changepoints},
  volume       = {35},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Gaussian measures conditioned on nonlinear observations: Consistency, MAP estimators, and simulation. <em>SAC</em>, <em>35</em>(1), 1-23. (<a href='https://doi.org/10.1007/s11222-024-10535-0'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The article presents a systematic study of the problem of conditioning a Gaussian random variable $$\xi $$ on nonlinear observations of the form $$F \circ {\varvec{\phi }}(\xi )$$ where $${\varvec{\phi }}: \mathcal {X}\rightarrow \mathbb {R}^N$$ is a bounded linear operator and F is nonlinear. Such problems arise in the context of Bayesian inference and recent machine learning-inspired PDE solvers. We give a representer theorem for the conditioned random variable $$\xi \mid F\circ {\varvec{\phi }}(\xi )$$ , stating that it decomposes as the sum of an infinite-dimensional Gaussian (which is identified analytically) as well as a finite-dimensional non-Gaussian measure. We also introduce a novel notion of the mode of a conditional measure by taking the limit of the natural relaxation of the problem, to which we can apply the existing notion of maximum a posteriori estimators of posterior measures. Finally, we introduce a variant of the Laplace approximation for the efficient simulation of the aforementioned conditioned Gaussian random variables towards uncertainty quantification.},
  archive      = {J_SAC},
  author       = {Chen, Yifan and Hosseini, Bamdad and Owhadi, Houman and Stuart, Andrew M.},
  doi          = {10.1007/s11222-024-10535-0},
  journal      = {Statistics and Computing},
  month        = {2},
  number       = {1},
  pages        = {1-23},
  shortjournal = {Stat. Comput.},
  title        = {Gaussian measures conditioned on nonlinear observations: Consistency, MAP estimators, and simulation},
  volume       = {35},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). FunBIalign: A hierachical algorithm for functional motif discovery based on mean squared residue scores. <em>SAC</em>, <em>35</em>(1), 1-15. (<a href='https://doi.org/10.1007/s11222-024-10537-y'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Motif discovery is gaining increasing attention in the domain of functional data analysis. Functional motifs are typical “shapes” or “patterns” that recur multiple times in different portions of a single curve and/or in misaligned portions of multiple curves. In this paper, we define functional motifs using an additive model and we propose funBIalign for their discovery and evaluation. Inspired by clustering and biclustering techniques, funBIalign is a multi-step procedure which uses agglomerative hierarchical clustering with complete linkage and a functional distance based on mean squared residue scores to discover functional motifs, both in a single curve (e.g., time series) and in a set of curves. We assess its performance and compare it to other recent methods through extensive simulations. Moreover, we use funBIalign for discovering motifs in two real-data case studies; one on food price inflation and one on temperature changes.},
  archive      = {J_SAC},
  author       = {Di Iorio, Jacopo and Cremona, Marzia A. and Chiaromonte, Francesca},
  doi          = {10.1007/s11222-024-10537-y},
  journal      = {Statistics and Computing},
  month        = {2},
  number       = {1},
  pages        = {1-15},
  shortjournal = {Stat. Comput.},
  title        = {FunBIalign: A hierachical algorithm for functional motif discovery based on mean squared residue scores},
  volume       = {35},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Simulating signed mixtures. <em>SAC</em>, <em>35</em>(1), 1-21. (<a href='https://doi.org/10.1007/s11222-024-10539-w'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Simulating mixtures of distributions with both positive and negative (signed) weights proves a challenge as standard simulation algorithms prove inefficient in handling the negative weights. In particular, the natural representation of mixture random variates as being associated with latent component indicators is no longer available. We propose an exact accept–reject algorithm for the general case of finite signed mixtures that relies on optimally pairing positive and negative components and designing a stratified sampling scheme on these pairs. We analyze the performances of our approach, relative to the inverse cdf approach, since the cdf of the targeted distribution remains available for signed mixtures of common distributions.},
  archive      = {J_SAC},
  author       = {Robert, Christian P. and Stoehr, Julien},
  doi          = {10.1007/s11222-024-10539-w},
  journal      = {Statistics and Computing},
  month        = {2},
  number       = {1},
  pages        = {1-21},
  shortjournal = {Stat. Comput.},
  title        = {Simulating signed mixtures},
  volume       = {35},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Online robust estimation and bootstrap inference for function-on-scalar regression. <em>SAC</em>, <em>35</em>(1), 1-20. (<a href='https://doi.org/10.1007/s11222-024-10538-x'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a novel and robust online function-on-scalar regression technique via geometric median to learn associations between functional responses and scalar covariates based on massive or streaming datasets. The online estimation procedure, developed using the average stochastic gradient descent algorithm, offers an efficient and cost-effective method for analyzing sequentially augmented datasets, eliminating the need to store large volumes of data in memory. We establish the almost sure consistency, $$L^p$$ convergence, and asymptotic normality of the online estimator. To enable efficient and fast inference of the parameters of interest, including the derivation of confidence intervals, we also develop an innovative two-step online bootstrap procedure to approximate the limiting error distribution of the robust online estimator. Numerical studies under a variety of scenarios demonstrate the effectiveness and efficiency of the proposed online learning method. A real application analyzing PM $$_{2.5}$$ air-quality data is also included to exemplify the proposed online approach.},
  archive      = {J_SAC},
  author       = {Cheng, Guanghui and Hu, Wenjuan and Lin, Ruitao and Wang, Chen},
  doi          = {10.1007/s11222-024-10538-x},
  journal      = {Statistics and Computing},
  month        = {2},
  number       = {1},
  pages        = {1-20},
  shortjournal = {Stat. Comput.},
  title        = {Online robust estimation and bootstrap inference for function-on-scalar regression},
  volume       = {35},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Probabilistic time integration for semi-explicit PDAEs. <em>SAC</em>, <em>35</em>(1), 1-13. (<a href='https://doi.org/10.1007/s11222-024-10543-0'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper deals with the application of probabilistic time integration methods to semi-explicit partial differential–algebraic equations of parabolic type and its semi-discrete counterparts, namely semi-explicit differential–algebraic equations of index 2. The proposed methods iteratively construct a probability distribution over the solution of deterministic problems, enhancing the information obtained from the numerical simulation. Within this paper, we examine the efficacy of the randomized versions of the implicit Euler method, the midpoint scheme, and exponential integrators of first and second order. By demonstrating the consistency and convergence properties of these solvers, we illustrate their utility in capturing the sensitivity of the solution to numerical errors. Our analysis establishes the theoretical validity of randomized time integration for constrained systems and offers insights into the calibration of probabilistic integrators for practical applications.},
  archive      = {J_SAC},
  author       = {Altmann, R. and Moradi, A.},
  doi          = {10.1007/s11222-024-10543-0},
  journal      = {Statistics and Computing},
  month        = {2},
  number       = {1},
  pages        = {1-13},
  shortjournal = {Stat. Comput.},
  title        = {Probabilistic time integration for semi-explicit PDAEs},
  volume       = {35},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Laplace-based strategies for bayesian optimal experimental design with nuisance uncertainty. <em>SAC</em>, <em>35</em>(1), 1-22. (<a href='https://doi.org/10.1007/s11222-024-10544-z'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Finding the optimal design of experiments in the Bayesian setting typically requires estimation and optimization of the expected information gain functional. This functional consists of one outer and one inner integral, separated by the logarithm function applied to the inner integral. When the mathematical model of the experiment contains uncertainty about the parameters of interest and nuisance uncertainty, (i.e., uncertainty about parameters that affect the model but are not themselves of interest to the experimenter), two inner integrals must be estimated. Thus, the already considerable computational effort required to determine good approximations of the expected information gain is increased further. The Laplace approximation has been applied successfully in the context of experimental design in various ways, and we propose two novel estimators featuring the Laplace approximation to alleviate the computational burden of both inner integrals considerably. The first estimator applies Laplace’s method followed by a Laplace approximation, introducing a bias. The second estimator uses two Laplace approximations as importance sampling measures for Monte Carlo approximations of the inner integrals. Both estimators use Monte Carlo approximation for the remaining outer integral estimation. We provide four numerical examples demonstrating the applicability and effectiveness of our proposed estimators.},
  archive      = {J_SAC},
  author       = {Bartuska, Arved and Espath, Luis and Tempone, Raúl},
  doi          = {10.1007/s11222-024-10544-z},
  journal      = {Statistics and Computing},
  month        = {2},
  number       = {1},
  pages        = {1-22},
  shortjournal = {Stat. Comput.},
  title        = {Laplace-based strategies for bayesian optimal experimental design with nuisance uncertainty},
  volume       = {35},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). INLA $$^+$$: Approximate bayesian inference for non-sparse models using HPC. <em>SAC</em>, <em>35</em>(1), 1-23. (<a href='https://doi.org/10.1007/s11222-024-10545-y'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The integrated nested Laplace approximations (INLA) method has become a widely utilized tool for researchers and practitioners seeking to perform approximate Bayesian inference across various fields of application. To address the growing demand for incorporating more complex models and enhancing the method’s capabilities, this paper introduces a novel framework, INLA $$^+$$ , that leverages dense matrices for performing approximate Bayesian inference based on INLA, across multiple computing nodes using high-performance computing (HPC). When dealing with non-sparse precision or covariance matrices, this new approach scales better compared to the current INLA method, capitalizing on the computational power offered by multiprocessors in shared and distributed memory architectures available in contemporary computing resources and specialized dense matrix algebra. To validate the efficacy of this approach, we conduct a simulation study where INLA is compared with INLA $$^+$$ , whereafter it is applied to analyze cancer mortality data in Spain with a three-way spatio-temporal interaction model.},
  archive      = {J_SAC},
  author       = {Abdul Fattah, Esmail and Van Niekerk, Janet and Rue, Håvard},
  doi          = {10.1007/s11222-024-10545-y},
  journal      = {Statistics and Computing},
  month        = {2},
  number       = {1},
  pages        = {1-23},
  shortjournal = {Stat. Comput.},
  title        = {INLA $$^+$$: Approximate bayesian inference for non-sparse models using HPC},
  volume       = {35},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Optimal subsampling for generalized additive models on large-scale datasets. <em>SAC</em>, <em>35</em>(1), 1-17. (<a href='https://doi.org/10.1007/s11222-024-10546-x'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the age of big data, the efficient analysis of vast datasets is paramount, yet hindered by computational limitations such as memory constraints and processing duration. To tackle these obstacles, conventional approaches often resort to parallel and distributed computing methodologies. In this study, we present an innovative statistical approach that exploits an optimized subsampling technique tailored for generalized additive models (GAM). Our approach harnesses the versatile modeling capabilities of GAM while alleviating computational burdens and enhancing the precision of parameter estimation. Through simulations and a practical application, we illustrate the efficacy of our method. Furthermore, we provide theoretical support by establishing convergence assurances and elucidating the asymptotic properties of our estimators. Our findings indicate that our approach surpasses uniform sampling in accuracy, while significantly reducing computational time in comparison to utilizing complete large-scale datasets.},
  archive      = {J_SAC},
  author       = {Li, Lili and Liu, Bingfan and Liu, Xiaodi and Shi, Haolun and Cao, Jiguo},
  doi          = {10.1007/s11222-024-10546-x},
  journal      = {Statistics and Computing},
  month        = {2},
  number       = {1},
  pages        = {1-17},
  shortjournal = {Stat. Comput.},
  title        = {Optimal subsampling for generalized additive models on large-scale datasets},
  volume       = {35},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Random perturbation subsampling for rank regression with massive data. <em>SAC</em>, <em>35</em>(1), 1-19. (<a href='https://doi.org/10.1007/s11222-024-10548-9'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Rank regression plays a fundamental role in statistical data analysis due to its robustness and high efficiency, and has been widely used in various scientific fields. However, when the data size is huge or massive, but the computing resource is limited, it can lead to an unacceptably computational cost for rank regression estimation. To handle this issue, this paper applies a random perturbation subsampling method to rank regression models. Specifically, we develop two repeatedly random perturbation subsampling algorithms to find the estimation of parameters. Two different weighting strategies with product weights and additive weights are examined in the objective function. Differing from the existing optimal and Poisson subsampling methods, our methods do not require the explicit calculation of subsampling probabilities for all data points, in which some unknown parameters often need to be estimated, thus making the implementation of our methods easier. Theoretically, statistical justifications are further provided for the proposed estimators including consistency and asymptotic normality. Extensive simulation studies and an empirical application are carried out to illustrate the effectiveness of the proposed methods.},
  archive      = {J_SAC},
  author       = {He, Sijin and Xia, Xiaochao},
  doi          = {10.1007/s11222-024-10548-9},
  journal      = {Statistics and Computing},
  month        = {2},
  number       = {1},
  pages        = {1-19},
  shortjournal = {Stat. Comput.},
  title        = {Random perturbation subsampling for rank regression with massive data},
  volume       = {35},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multivariate zero-inflated INGARCH models: Bayesian inference and composite likelihood approach. <em>SAC</em>, <em>35</em>(1), 1-19. (<a href='https://doi.org/10.1007/s11222-024-10549-8'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we propose a framework for modeling multivariate count time series data that accommodates zero-inflated components. Our approach is based on a novel class of bivariate distributions (ZMP $$_2$$ ) which is formulated via the mixed Poisson representation and a four-point mixture. In the ZMP $$_2$$ , shared latent effects from some exponential family distribution act on the Poisson rates in order to introduce dependency. Bivariate zero-inflation is then posed via the mixture approach by Chin-Shang et al. (Technometrics 41:29–38, 1999), where excess of zeros can arise in different ways. This can be with respect to either one of the marginals, or simultaneous for both components. The ZMP $$_2$$ class admits a flexible zero-inflation behaviour and a tractable hierarchical structure achieved with augmentation of the data to the mixture components. It is placed in the time series context by adopting a bivariate integer-valued GARCH (INGARCH) structure, becoming the first model in this class capable to handle zero inflation. The ZMP $$_2$$ is then extended to dimensions higher than two via a composite likelihood approach, which is a proxy for an underlying p-variate ( $$p>2$$ ) distribution. The latter is constructed by the product of ZMP $$_2$$ INGARCH terms, and provides a trade-off between complexity and information. Inferential procedures are carefully outlined from a Bayesian perspective via Markov Chain Monte Carlo. We demonstrate that an efficient Metropolis-within-Gibbs sampler can be constructed availing of the model’s hierarchical structure and conjugate priors. Our proposal is explored in a real data application concerning the weekly cases of measles reported in three states of Germany. Comparison to other bivariate INGARCH models is pursed and demonstrates that the ZMP INGARCH substantially improves the analysis of measles infections.},
  archive      = {J_SAC},
  author       = {Piancastelli, Luiza S. C. and Silva, Rodrigo B.},
  doi          = {10.1007/s11222-024-10549-8},
  journal      = {Statistics and Computing},
  month        = {2},
  number       = {1},
  pages        = {1-19},
  shortjournal = {Stat. Comput.},
  title        = {Multivariate zero-inflated INGARCH models: Bayesian inference and composite likelihood approach},
  volume       = {35},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). K-fold cross-validation based frequentist model averaging for linear models with nonignorable missing responses. <em>SAC</em>, <em>35</em>(1), 1-19. (<a href='https://doi.org/10.1007/s11222-024-10554-x'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A frequentist model averaging method based on K-fold cross-validation is proposed for linear models with nonignorable missing responses. When all the candidate models are misspecified, the asymptotic optimality of the proposed method is established in the sense that it can asymptotically achieve the minimum squared loss. If the set of the candidate models contains the correct ones, the proposed method is also shown to be selection consistent in the sense that the sum of the model averaging weights assigned to the correct candidate models asymptotically converges to one. Numerical simulations were conducted to demonstrate the performance of the proposed method, and as an illustration the proposed method was applied to analyze a diabetes dataset.},
  archive      = {J_SAC},
  author       = {Liang, Zhongqi and Cai, Li and Wang, Suojin and Wang, Qihua},
  doi          = {10.1007/s11222-024-10554-x},
  journal      = {Statistics and Computing},
  month        = {2},
  number       = {1},
  pages        = {1-19},
  shortjournal = {Stat. Comput.},
  title        = {K-fold cross-validation based frequentist model averaging for linear models with nonignorable missing responses},
  volume       = {35},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Robust and efficient sparse learning over networks: A decentralized surrogate composite quantile regression approach. <em>SAC</em>, <em>35</em>(1), 1-16. (<a href='https://doi.org/10.1007/s11222-024-10547-w'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Decentralized distributed learning has recently attracted considerable interest due to its advantages in system stability, data privacy, and efficient communication and computation. Despite these benefits, the approach encounters three significant challenges: arbitrary noise, high dimensionality, and data heterogeneity. To tackle the first two challenges, we propose integrating composite quantile regression with an $$\ell _1$$ penalty. This method introduces a doubly nonsmooth objective function, presenting new difficulties for both algorithmic and theoretical development. Traditional optimization algorithms typically demonstrate a slow sublinear convergence rate in such scenarios. To speed the convergence rate, we introduce an innovative local smoothing technique that effectively overcomes nonsmoothness, allowing our algorithm to achieve a rapid linear convergence rate with simple implementation. Additionally, this technique addresses the third challenge by managing heterogeneous covariates and noise across various nodes. From a theoretical perspective, we provide statistical guarantees for estimation accuracy and support recovery. Specifically, the proposed estimator achieves a near-oracle rate without imposing stringent requirements on the number of nodes. Extensive experiments and a real data application confirm the efficacy of our method.},
  archive      = {J_SAC},
  author       = {Qiao, Nan and Chen, Canyi and Zhu, Zhengtian},
  doi          = {10.1007/s11222-024-10547-w},
  journal      = {Statistics and Computing},
  month        = {2},
  number       = {1},
  pages        = {1-16},
  shortjournal = {Stat. Comput.},
  title        = {Robust and efficient sparse learning over networks: A decentralized surrogate composite quantile regression approach},
  volume       = {35},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multi-task learning via robust regularized clustering with non-convex group penalties. <em>SAC</em>, <em>35</em>(1), 1-27. (<a href='https://doi.org/10.1007/s11222-024-10550-1'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-task learning (MTL) aims to improve estimation and prediction performance by sharing common information among related tasks. One natural assumption in MTL is that tasks are classified into clusters based on their characteristics. However, existing MTL methods based on this assumption often ignore outlier tasks that have large task-specific components or no relation to other tasks. To address this issue, we propose a novel MTL method called Multi-Task Learning via Robust Regularized Clustering (MTLRRC). MTLRRC incorporates robust regularization terms inspired by robust convex clustering, which is further extended to handle non-convex and group-sparse penalties. The extension allows MTLRRC to simultaneously perform robust task clustering and outlier task detection. The connection between the extended robust clustering and the multivariate M-estimator is also established. This provides an interpretation of the robustness of MTLRRC against outlier tasks. An efficient algorithm based on a modified alternating direction method of multipliers is developed for the estimation of the parameters. The effectiveness of MTLRRC is demonstrated through simulation studies and application to real data.},
  archive      = {J_SAC},
  author       = {Okazaki, Akira and Kawano, Shuichi},
  doi          = {10.1007/s11222-024-10550-1},
  journal      = {Statistics and Computing},
  month        = {2},
  number       = {1},
  pages        = {1-27},
  shortjournal = {Stat. Comput.},
  title        = {Multi-task learning via robust regularized clustering with non-convex group penalties},
  volume       = {35},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A neural network-based adaptive cut-off approach to normality testing for dependent data. <em>SAC</em>, <em>35</em>(1), 1-15. (<a href='https://doi.org/10.1007/s11222-024-10551-0'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {There is a wide availability of methods for testing normality under the assumption of independent and identically distributed data. When data are dependent in space and/or time, however, assessing and testing the marginal behavior is considerably more challenging, as the marginal behavior is impacted by the degree of dependence, which typically leads to an inflation in Type I error rates. We propose a new approach to assess normality for dependent data by non-linearly incorporating existing statistics from normality tests as well as sample moments such as skewness and kurtosis through a neural network with adaptive cut-offs by which the Type I error inflation issue is fixed. We calibrate (deep) neural networks by simulated normal and non-normal data with a wide range of dependence structures and we determine the probability of rejecting the null hypothesis. We compare several approaches for normality tests and demonstrate the superiority of our method in terms of statistical power through an extensive simulation study. A real world application to global temperature data further demonstrates how the degree of spatio-temporal aggregation affects the marginal normality in the data.},
  archive      = {J_SAC},
  author       = {Kim, Minwoo and Genton, Marc G. and Huser, Raphaël and Castruccio, Stefano},
  doi          = {10.1007/s11222-024-10551-0},
  journal      = {Statistics and Computing},
  month        = {2},
  number       = {1},
  pages        = {1-15},
  shortjournal = {Stat. Comput.},
  title        = {A neural network-based adaptive cut-off approach to normality testing for dependent data},
  volume       = {35},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A limit formula and recursive algorithm for multivariate normal tail probability. <em>SAC</em>, <em>35</em>(1), 1-18. (<a href='https://doi.org/10.1007/s11222-024-10552-z'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This work develops a formula for the large threshold limit of multivariate Normal tail probability when at least one of the normalised thresholds grows indefinitely. Derived using integration by parts, the formula expresses the tail probability in terms of conditional probabilities involving one less variate, thereby reducing the problem dimension by 1. The formula is asymptotic to Ruben’s formula under Salvage’s condition. It satisfies Plackett’s identity exactly or approximately, depending on the correlation parameter being differentiated. A recursive algorithm is proposed that allows the tail probability limit to be calculated in terms of univariate Normal probabilities only. The algorithm shows promise in numerical examples to offer a semi-analytical approximation under non-asymptotic situations to within an order of magnitude. The number of univariate Normal probability evaluations is at least n!, however, and in this sense the algorithm suffers from the curse of dimension.},
  archive      = {J_SAC},
  author       = {Au, Siu-Kui},
  doi          = {10.1007/s11222-024-10552-z},
  journal      = {Statistics and Computing},
  month        = {2},
  number       = {1},
  pages        = {1-18},
  shortjournal = {Stat. Comput.},
  title        = {A limit formula and recursive algorithm for multivariate normal tail probability},
  volume       = {35},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). The clustered mallows model. <em>SAC</em>, <em>35</em>(1), 1-21. (<a href='https://doi.org/10.1007/s11222-024-10555-w'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Rankings represent preferences that arise from situations where assessors arrange items, for example, in decreasing order of utility. Orderings of the item set are permutations ( $$\pi $$ ) that reflect strict preferences. However, strict preference relations can be unrealistic for real data. Common traits among items can justify equal ranks and there can also be different importance attribution to decisions that form $$\pi $$ . In large item sets, assessors might prioritise certain items, rank others low, and express indifference towards the remaining. Rank aggregation may involve decisive judgments in some parts and ambiguity in others. In this paper, we extend the famous Mallows (Biometrika 44:114–130, 1957) model (MM) to accommodate item indifference. Grouping similar items motivates the proposed Clustered Mallows Model (CMM), a MM counterpart for tied ranks with ties learned from the data. The CMM provides the flexibility to combine strictness and indifferences, describing rank collections as ordered clusters. CMM Bayesian inference is a doubly-intractable problem since the normalised model is unavailable. We overcome this with a version of the exchange algorithm (Murray et al. in Proceedings of the 22nd annual conference on uncertainty in artificial intelligence (UAI-06), 2006) and provide a pseudo-likelihood approximation as a computationally cheaper alternative. Analysis of two real-world ranking datasets is presented, showcasing the practical application of the CMM and highlighting scenarios where it offers advantages over alternative models.},
  archive      = {J_SAC},
  author       = {Piancastelli, Luiza S. C. and Friel, Nial},
  doi          = {10.1007/s11222-024-10555-w},
  journal      = {Statistics and Computing},
  month        = {2},
  number       = {1},
  pages        = {1-21},
  shortjournal = {Stat. Comput.},
  title        = {The clustered mallows model},
  volume       = {35},
  year         = {2025},
}
</textarea>
</details></li>
</ul>

</body>
</html>
