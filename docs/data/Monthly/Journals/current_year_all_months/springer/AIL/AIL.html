<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>AIL</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="ail">AIL - 19</h2>
<ul>
<li><details>
<summary>
(2025). Legal sentence boundary detection using hybrid deep learning and statistical models. <em>AIL</em>, <em>33</em>(2), 519-549. (<a href='https://doi.org/10.1007/s10506-024-09394-x'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Sentence boundary detection (SBD) represents an important first step in natural language processing since accurately identifying sentence boundaries significantly impacts downstream applications. Nevertheless, detecting sentence boundaries within legal texts poses a unique and challenging problem due to their distinct structural and linguistic features. Our approach utilizes deep learning models to leverage delimiter and surrounding context information as input, enabling precise detection of sentence boundaries in English legal texts. We evaluate various deep learning models, including domain-specific transformer models like LegalBERT and CaseLawBERT. To assess the efficacy of our deep learning models, we compare them with a state-of-the-art domain-specific statistical conditional random field (CRF) model. After considering model size, F1-score, and inference time, we identify the Convolutional Neural Network Model (CNN) as the top-performing deep learning model. To further enhance performance, we integrate the features of the CNN model into the subsequent CRF model, creating a hybrid architecture that combines the strengths of both models. Our experiments demonstrate that the hybrid model outperforms the baseline model, achieving a 4% improvement in the F1-score. Additional experiments showcase the superiority of the hybrid model over SBD open-source libraries when confronted with an out-of-domain test set. These findings underscore the importance of efficient SBD in legal texts and emphasize the advantages of employing deep learning models and hybrid architectures to achieve optimal performance.},
  archive      = {J_AIL},
  author       = {Sheik, Reshma and Ganta, Sneha Rao and Nirmala, S. Jaya},
  doi          = {10.1007/s10506-024-09394-x},
  journal      = {Artificial Intelligence and Law},
  month        = {6},
  number       = {2},
  pages        = {519-549},
  shortjournal = {Artif. Intell. Law},
  title        = {Legal sentence boundary detection using hybrid deep learning and statistical models},
  volume       = {33},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A comparative user study of human predictions in algorithm-supported recidivism risk assessment. <em>AIL</em>, <em>33</em>(2), 471-517. (<a href='https://doi.org/10.1007/s10506-024-09393-y'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we study the effects of using an algorithm-based risk assessment instrument (RAI) to support the prediction of risk of violent recidivism upon release. The instrument we used is a machine learning version of RiskCanvi used by the Justice Department of Catalonia, Spain. It was hypothesized that people can improve their performance on defining the risk of recidivism when assisted with a RAI. Also, that professionals can perform better than non-experts on the domain. Participants had to predict whether a person who has been released from prison will commit a new crime leading to re-incarceration, within the next two years. This user study is done with (1) general participants from diverse backgrounds recruited through a crowdsourcing platform, (2) targeted participants who are students and practitioners of data science, criminology, or social work and professionals who work with RisCanvi. We also run focus groups with participants of the targeted study, including people who use RisCanvi in a professional capacity, to interpret the quantitative results. Among other findings, we observe that algorithmic support systematically leads to more accurate predictions from all participants, but that statistically significant gains are only seen in the performance of targeted participants with respect to that of crowdsourced participants. Among other comments, professional participants indicate that they would not foresee using a fully-automated system in criminal risk assessment, but do consider it valuable for training, standardization, and to fine-tune or double-check their predictions on particularly difficult cases. We found that the revised prediction by using a RAI increases the performance of all groups, while professionals show a better performance in general. And, a RAI can be considered for extending professional capacities and skills along their careers.},
  archive      = {J_AIL},
  author       = {Portela, Manuel and Castillo, Carlos and Tolan, Songül and Karimi-Haghighi, Marzieh and Pueyo, Antonio Andres},
  doi          = {10.1007/s10506-024-09393-y},
  journal      = {Artificial Intelligence and Law},
  month        = {6},
  number       = {2},
  pages        = {471-517},
  shortjournal = {Artif. Intell. Law},
  title        = {A comparative user study of human predictions in algorithm-supported recidivism risk assessment},
  volume       = {33},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Agents preserving privacy on intelligent transportation systems according to EU law. <em>AIL</em>, <em>33</em>(2), 437-470. (<a href='https://doi.org/10.1007/s10506-024-09391-0'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Intelligent Transportation Systems are expected to automate how parking slots are booked by trucks. The intrinsic dynamic nature of this problem, the need of explanations and the inclusion of private data justify an agent-based solution. Agents solving this problem act with a Believe Desire Intentions reasoning, and are implemented with JASON. Privacy of trucks becomes protected sharing a list of parkings ordered by preference. Furthermore, the process of assigning parking slots takes into account legal requirements on breaks and driving time limits. Finally, the agent simulations use the distances, the number of trucks and parkings corresponding to the proportions of the current European Union data. The performance of the proposed solution is tested in these simulations with three different distances against an alternative with complete knowledge. The difference in efficiency, the number of illegal breaks and the traveled distances are measured in them. Comparing the results, we can conclude that the nonprivate alternative is slightly better in performance while both alternatives do not produce illegal breaks. In this way the simulations show that the proposed privacy protection does not impose a relevant handicap in efficiency.},
  archive      = {J_AIL},
  author       = {Carbo, Javier and Pedraza, Juanita and Molina, Jose M.},
  doi          = {10.1007/s10506-024-09391-0},
  journal      = {Artificial Intelligence and Law},
  month        = {6},
  number       = {2},
  pages        = {437-470},
  shortjournal = {Artif. Intell. Law},
  title        = {Agents preserving privacy on intelligent transportation systems according to EU law},
  volume       = {33},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). The challenge of open-texture in law. <em>AIL</em>, <em>33</em>(2), 405-435. (<a href='https://doi.org/10.1007/s10506-024-09390-1'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {An important challenge when creating automatically processable laws concerns open-textured terms. The ability to measure open-texture can assist in determining the feasibility of encoding regulation and where additional legal information is required to properly assess a legal issue or dispute. In this article, we propose a novel conceptualisation of open-texture with the aim of determining the extent of open-textured terms in legal documents. We conceptualise open-texture as a lever whose state is impacted by three types of forces: internal forces (the words within the text themselves), external forces (the resources brought to challenge the definition of words), and lateral forces (the merit of such challenges). We tested part of this conceptualisation with 26 participants by investigating agreement in paired annotators. Five key findings emerged. First, agreement on which words are open-texture within a legal text is possible and statistically significant. Second, agreement is even high at an average inter-rater reliability of 0.7 (Cohen’s kappa). Third, when there is agreement on the words, agreement on the Open-Texture Value is high. Fourth, there is a dependence between the Open-Texture Value and reasons invoked behind open-texture. Fifth, involving only four annotators can yield similar results compared to involving twenty more when it comes to only flagging clauses containing open-texture. We conclude the article by discussing limitations of our experiment and which remaining questions in real life cases are still outstanding.},
  archive      = {J_AIL},
  author       = {Guitton, Clement and Tamò-Larrieux, Aurelia and Mayer, Simon and van Dijck, Gijs},
  doi          = {10.1007/s10506-024-09390-1},
  journal      = {Artificial Intelligence and Law},
  month        = {6},
  number       = {2},
  pages        = {405-435},
  shortjournal = {Artif. Intell. Law},
  title        = {The challenge of open-texture in law},
  volume       = {33},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Code is law: How COMPAS affects the way the judiciary handles the risk of recidivism. <em>AIL</em>, <em>33</em>(2), 383-404. (<a href='https://doi.org/10.1007/s10506-024-09389-8'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Judges in multiple US states, such as New York, Pennsylvania, Wisconsin, California, and Florida, receive a prediction of defendants’ recidivism risk, generated by the COMPAS algorithm. If judges act on these predictions, they implicitly delegate normative decisions to proprietary software, even beyond the previously documented race and age biases. Using the ProPublica dataset, we demonstrate that COMPAS predictions favor jailing over release. COMPAS is biased against defendants. We show that this bias can largely be removed. Our proposed correction increases overall accuracy, and attenuates anti-black and anti-young bias. However, it also slightly increases the risk that defendants are released who commit a new crime before tried. We argue that this normative decision should not be buried in the code. The tradeoff between the interests of innocent defendants and of future victims should not only be made transparent. The algorithm should be changed such that the legislator and the courts do make this choice.},
  archive      = {J_AIL},
  author       = {Engel, Christoph and Linhardt, Lorenz and Schubert, Marcel},
  doi          = {10.1007/s10506-024-09389-8},
  journal      = {Artificial Intelligence and Law},
  month        = {6},
  number       = {2},
  pages        = {383-404},
  shortjournal = {Artif. Intell. Law},
  title        = {Code is law: How COMPAS affects the way the judiciary handles the risk of recidivism},
  volume       = {33},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Combining prompt-based language models and weak supervision for labeling named entity recognition on legal documents. <em>AIL</em>, <em>33</em>(2), 361-381. (<a href='https://doi.org/10.1007/s10506-023-09388-1'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Named entity recognition (NER) is a very relevant task for text information retrieval in natural language processing (NLP) problems. Most recent state-of-the-art NER methods require humans to annotate and provide useful data for model training. However, using human power to identify, circumscribe and label entities manually can be very expensive in terms of time, money, and effort. This paper investigates the use of prompt-based language models (OpenAI’s GPT-3) and weak supervision in the legal domain. We apply both strategies as alternative approaches to the traditional human-based annotation method, relying on computer power instead human effort for labeling, and subsequently compare model performance between computer and human-generated data. We also introduce combinations of all three mentioned methods (prompt-based, weak supervision, and human annotation), aiming to find ways to maintain high model efficiency and low annotation costs. We showed that, despite human labeling still maintaining better overall performance results, the alternative strategies and their combinations presented themselves as valid options, displaying positive results and similar model scores at lower costs. Final results demonstrate preservation of human-trained models scores averaging 74.0% for GPT-3, 95.6% for weak supervision, 90.7% for GPT + weak supervision combination, and 83.9% for GPT + 30% human-labeling combination.},
  archive      = {J_AIL},
  author       = {Oliveira, Vitor and Nogueira, Gabriel and Faleiros, Thiago and Marcacini, Ricardo},
  doi          = {10.1007/s10506-023-09388-1},
  journal      = {Artificial Intelligence and Law},
  month        = {6},
  number       = {2},
  pages        = {361-381},
  shortjournal = {Artif. Intell. Law},
  title        = {Combining prompt-based language models and weak supervision for labeling named entity recognition on legal documents},
  volume       = {33},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). DiscoLQA: Zero-shot discourse-based legal question answering on european legislation. <em>AIL</em>, <em>33</em>(2), 323-359. (<a href='https://doi.org/10.1007/s10506-023-09387-2'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The structures of discourse used by legal and ordinary languages share differences that foster technical issues when applying or fine-tuning general-purpose language models for open-domain question answering on legal resources. For example, longer sentences may be preferred in European laws (i.e., Brussels I bis Regulation EU 1215/2012) to reduce potential ambiguities and improve comprehensibility, distracting a language model trained on ordinary English. In this article, we investigate some mechanisms to isolate and capture the discursive patterns of legalese in order to perform zero-shot question answering, i.e., without training on legal documents. Specifically, we use pre-trained open-domain answer retrieval systems and study what happens when changing the type of information to consider for retrieval. Indeed, by selecting only the important parts of discourse (e.g., elementary units of discourse, EDU for short, or abstract representations of meaning, AMR for short), we should be able to help the answer retriever identify the elements of interest. Hence, with this paper, we publish Q4EU, a new evaluation dataset that includes more than 70 questions and 200 answers on 6 different European norms, and study what happens to a baseline system when only EDUs or AMRs are used during information retrieval. Our results show that the versions using EDUs are overall the best, leading to state-of-the-art F1, precision, NDCG and MRR scores.},
  archive      = {J_AIL},
  author       = {Sovrano, Francesco and Palmirani, Monica and Sapienza, Salvatore and Pistone, Vittoria},
  doi          = {10.1007/s10506-023-09387-2},
  journal      = {Artificial Intelligence and Law},
  month        = {6},
  number       = {2},
  pages        = {323-359},
  shortjournal = {Artif. Intell. Law},
  title        = {DiscoLQA: Zero-shot discourse-based legal question answering on european legislation},
  volume       = {33},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). To test or not to test? a question of rational decision making in forensic biology. <em>AIL</em>, <em>33</em>(2), 293-322. (<a href='https://doi.org/10.1007/s10506-023-09386-3'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {How can the forensic scientist rationally justify performing a sequence of tests and analyses in a particular case? When is it worth performing a test or analysis on an item? Currently, there is a large void in logical frameworks for making rational decisions in forensic science. The aim of this paper is to fill this void by presenting a step-by-step guide on how to apply Bayesian decision theory to routine decision problems encountered by forensic scientists on performing or not performing a particular laboratory test or analysis. A decision-theoretic framework, composed of actions, states of nature, and utilities, models this problem, and an influence diagram translates its notions into a probabilistic graphical network. Within this framework, the expected value of information (EVOI) for the submission of an item to a particular test or analysis addresses the above questions. The development of a classical case example on whether to perform presumptive tests for blood before submitting the item for a DNA analysis illustrates the use of this model for source level questions in forensic biology (i.e., questions that ask whether a crime stain consisting of a particular body fluid comes from a particular person). We show how to construct an influence diagram for this example, and how sensitivity analyses lead to an optimal analytical sequence. The key idea is to show that such a Bayesian decisional approach provides a coherent framework for justifying the optimal analytical sequence for a particular case in forensic science.},
  archive      = {J_AIL},
  author       = {Gittelson, Simone and Taroni, Franco},
  doi          = {10.1007/s10506-023-09386-3},
  journal      = {Artificial Intelligence and Law},
  month        = {6},
  number       = {2},
  pages        = {293-322},
  shortjournal = {Artif. Intell. Law},
  title        = {To test or not to test? a question of rational decision making in forensic biology},
  volume       = {33},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Enhancing legal judgment summarization with integrated semantic and structural information. <em>AIL</em>, <em>33</em>(2), 271-292. (<a href='https://doi.org/10.1007/s10506-023-09381-8'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Legal Judgment Summarization (LJS) can highly summarize legal judgment documents, improving judicial work efficiency in case retrieval and other occasions. Legal judgment documents are usually lengthy; however, most existing LJS methods are directly based on general text summarization models, which cannot handle long texts effectively. Additionally, due to the complex structural characteristics of legal judgment documents, some information may be lost by applying only one single kind of summarization model. To address these issues, we propose an integrated summarization method which leverages both semantic and structural information to improve the quality of LJS. Specifically, legal judgment documents are firstly segmented into three relatively short parts according to their specific structure. We propose an extractive summarization model named BSLT and an abstractive summarization model named LPGN by adopting Lawformer as the encoder. Lawformer is a new pre-trained language model for long legal documents, which specializes in capturing long-distance dependency and modeling legal semantic features. Then, we adopt different models to summarize the corresponding part regarding its structural characteristics. Finally, the obtained summaries are integrated to generate a high-quality summary involving semantic and structural information. We conduct comparative experiments to evaluate the performance of our model. The results show that our model outperforms the baseline model LEAD-3 by 14.78% on the mean ROUGE score, which demonstrates our method is effective in LJS and is prospected to be applied to assist other tasks in legal artificial intelligence.},
  archive      = {J_AIL},
  author       = {Dan, Jingpei and Hu, Weixuan and Wang, Yuming},
  doi          = {10.1007/s10506-023-09381-8},
  journal      = {Artificial Intelligence and Law},
  month        = {6},
  number       = {2},
  pages        = {271-292},
  shortjournal = {Artif. Intell. Law},
  title        = {Enhancing legal judgment summarization with integrated semantic and structural information},
  volume       = {33},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). AI, law and beyond. a transdisciplinary ecosystem for the future of AI & law. <em>AIL</em>, <em>33</em>(1), 253-270. (<a href='https://doi.org/10.1007/s10506-024-09404-y'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We live in exciting times for AI and Law: technical developments are moving at a breakneck pace, and at the same time, the call for more robust AI governance and regulation grows stronger. How should we as an AI & Law community navigate these dramatic developments and claims? In this Presidential Address, I present my ideas for a way forward: researching, developing and evaluating real AI systems for the legal field with researchers from AI, Law and beyond. I will demonstrate how we at the Netherlands National Police Lab AI are developing responsible AI by combining insights from different disciplines, and how this connects to the future of our field.},
  archive      = {J_AIL},
  author       = {Bex, Floris J.},
  doi          = {10.1007/s10506-024-09404-y},
  journal      = {Artificial Intelligence and Law},
  month        = {3},
  number       = {1},
  pages        = {253-270},
  shortjournal = {Artif. Intell. Law},
  title        = {AI, law and beyond. a transdisciplinary ecosystem for the future of AI & law},
  volume       = {33},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Automating petition classification in brazil’s legal system: A two-step deep learning approach. <em>AIL</em>, <em>33</em>(1), 227-251. (<a href='https://doi.org/10.1007/s10506-023-09385-4'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Automated classification of legal documents has been the subject of extensive research in recent years. However, this is still a challenging task for long documents, since it is difficult for a model to identify the most relevant information for classification. In this paper, we propose a two-stage supervised learning approach for the classification of petitions, a type of legal document that requests a court order. The proposed approach is based on a word-level encoder–decoder Seq2Seq deep neural network, such as a Bidirectional Long Short-Term Memory (BiLSTM) or a Bidirectional Encoder Representations from Transformers (BERT) model, and a document-level Support Vector Machine classifier. To address the challenges posed by the lengthy legal documents, the approach introduces a human-in-the-loop approach, whose task is to localize and tag relevant segments of text in the word-level training part, which dramatically reduces the dimension of the document classifier input vector. We performed experiments to validate our approach using a real-world dataset comprised of 270 intermediate petitions, which were carefully annotated by specialists from the 15th civil unit of the State of Alagoas, Brazil. Our results revealed that both BiLSTM and BERT-Convolutional Neural Networks variants achieved an accuracy of up to 95.49%, and also outperformed baseline classifiers based on the Term Frequency–Inverse Document Frequency test vectorizer. The proposed approach is currently being utilized to automate the aforementioned justice unit, thereby increasing its efficiency in handling repetitive tasks.},
  archive      = {J_AIL},
  author       = {Costa, Yuri D. R. and Oliveira, Hugo and Nogueira, Valério and Massa, Lucas and Yang, Xu and Barbosa, Adriano and Oliveira, Krerley and Vieira, Thales},
  doi          = {10.1007/s10506-023-09385-4},
  journal      = {Artificial Intelligence and Law},
  month        = {3},
  number       = {1},
  pages        = {227-251},
  shortjournal = {Artif. Intell. Law},
  title        = {Automating petition classification in brazil’s legal system: A two-step deep learning approach},
  volume       = {33},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Toward representing interpretation in factor-based models of precedent. <em>AIL</em>, <em>33</em>(1), 199-226. (<a href='https://doi.org/10.1007/s10506-023-09384-5'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article discusses the desirability and feasibility of modeling precedents with multiple interpretations within factor-based models of precedential constraint. The main idea is that allowing multiple reasonable interpretations of cases and modeling precedential constraint as a function of what all reasonable interpretations compel may be advantageous. The article explains the potential benefits of extending the models in this way with a focus on incorporating a theory of vertical precedent in U.S. federal appellate courts. It also considers the costs of extending the models in this way, such as the significant increase in the functional size of the case base and the need to provide some kind of ordering on interpretations to select a “best” interpretation. Finally, the article suggests partially incorporating multiple interpretations of dimensions as a realistic starting point for incorporating interpretations generally, and shows how doing so can help address difficulties with dimensions. The conclusion remarks on the use of interpretations to deal with inconsistent precedents.},
  archive      = {J_AIL},
  author       = {Rigoni, Adam},
  doi          = {10.1007/s10506-023-09384-5},
  journal      = {Artificial Intelligence and Law},
  month        = {3},
  number       = {1},
  pages        = {199-226},
  shortjournal = {Artif. Intell. Law},
  title        = {Toward representing interpretation in factor-based models of precedent},
  volume       = {33},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Decision support for detecting sensitive text in government records. <em>AIL</em>, <em>33</em>(1), 171-197. (<a href='https://doi.org/10.1007/s10506-023-09383-6'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Freedom of information laws promote transparency by permitting individuals and organizations to obtain government documents. However, exemptions from disclosure are necessary to protect privacy and to permit government officials to deliberate freely. Deliberative language is often the most challenging and burdensome exemption to detect, leading to high processing costs and delays in responding to open-records requests. This paper describes a novel deliberative-language detection model trained on a new annotated training set. The deliberative-language detection model is a component of a decision-support system for open-records requests under the US Freedom of Information Act, the FOIA Assistant, that ingests documents responsive to an open-records requests, suggests passages likely to be subject to deliberative language, privacy, or other exemptions, and assists analysts in rapidly redacting suggested passages. The tool’s interface is based on extensive human-factors and usability studies with analysts and is currently in operational testing by multiple US federal agencies.},
  archive      = {J_AIL},
  author       = {Branting, Karl and Brown, Bradford and Giannella, Chris and Guilder, James Van and Harrold, Jeff and Howell, Sarah and Baron, Jason R.},
  doi          = {10.1007/s10506-023-09383-6},
  journal      = {Artificial Intelligence and Law},
  month        = {3},
  number       = {1},
  pages        = {171-197},
  shortjournal = {Artif. Intell. Law},
  title        = {Decision support for detecting sensitive text in government records},
  volume       = {33},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Correction to: Reasoning with inconsistent precedents. <em>AIL</em>, <em>33</em>(1), 167-170. (<a href='https://doi.org/10.1007/s10506-024-09392-z'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_AIL},
  author       = {Canavotto, Ilaria},
  doi          = {10.1007/s10506-024-09392-z},
  journal      = {Artificial Intelligence and Law},
  month        = {3},
  number       = {1},
  pages        = {167-170},
  shortjournal = {Artif. Intell. Law},
  title        = {Correction to: Reasoning with inconsistent precedents},
  volume       = {33},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Reasoning with inconsistent precedents. <em>AIL</em>, <em>33</em>(1), 137-166. (<a href='https://doi.org/10.1007/s10506-023-09382-7'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Computational models of legal precedent-based reasoning developed in AI and Law are typically based on the simplifying assumption that the background set of precedent cases is consistent. Besides being unrealistic in the legal domain, this assumption is problematic for recent promising applications of these models to the development of explainable AI methods. In this paper I explore a model of legal precedent-based reasoning that, unlike existing models, does not rely on the assumption that the background set of precedent cases is consistent. The model is a generalization of the reason model of precedential constraint. I first show that the model supports an interesting deontic logic, where consistent obligations can be derived from inconsistent case bases. I then provide an explanation of this surprising result by proposing a reformulation of the model in terms of cases that support a new potential decision and cases that conflict with it. Finally, I show that the reformulation of the model allows us to verify that inconsistent case bases do not make verification that a decision is permissible substantially more complex than consistent case bases and to introduce intuitive criteria to compare different permissible decisions.},
  archive      = {J_AIL},
  author       = {Canavotto, Ilaria},
  doi          = {10.1007/s10506-023-09382-7},
  journal      = {Artificial Intelligence and Law},
  month        = {3},
  number       = {1},
  pages        = {137-166},
  shortjournal = {Artif. Intell. Law},
  title        = {Reasoning with inconsistent precedents},
  volume       = {33},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A neural network to identify requests, decisions, and arguments in court rulings on custody. <em>AIL</em>, <em>33</em>(1), 101-135. (<a href='https://doi.org/10.1007/s10506-023-09380-9'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Court rulings are among the most important documents in all legal systems. This article describes a study in which natural language processing is used for the automatic characterization of Spanish judgments that deal with the physical custody (joint or individual) of minors. The model was trained to identify a set of elements: the type of custody requested by the plaintiff, the type of custody decided on by the court, and eight of the most commonly used arguments in this type of judgment. Two jurists independently annotated more than 3000 judgments, which were used to train a model based on transformers. The main difficulties encountered in this task were the complexity of the judicial language and the need to work with appellate court rulings that have a more complicated structure than decisions at first instance. For the complete court rulings, the F1 score of the inter-annotator agreement ranged from 0.60 to 0.86 and the Kappa index from 0.33 to 0.73. The F1 score of the agreement between the model and the annotators ranged from 0.66 to 0.93 and the Kappa index from 0.57 to 0.80. These results in which the model performance exceeds even the inter-annotator agreement show the high ability of transformers to identify abstract entities in legal texts.},
  archive      = {J_AIL},
  author       = {Muñoz-Soro, José Félix and del Hoyo Alonso, Rafael and Montañes, Rosa and Lacueva, Francisco},
  doi          = {10.1007/s10506-023-09380-9},
  journal      = {Artificial Intelligence and Law},
  month        = {3},
  number       = {1},
  pages        = {101-135},
  shortjournal = {Artif. Intell. Law},
  title        = {A neural network to identify requests, decisions, and arguments in court rulings on custody},
  volume       = {33},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Natural language processing for legal document review: Categorising deontic modalities in contracts. <em>AIL</em>, <em>33</em>(1), 79-100. (<a href='https://doi.org/10.1007/s10506-023-09379-2'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The contract review process can be a costly and time-consuming task for lawyers and clients alike, requiring significant effort to identify and evaluate the legal implications of individual clauses. To address this challenge, we propose the use of natural language processing techniques, specifically text classification based on deontic tags, to streamline the process. Our research question is whether natural language processing techniques, specifically dense vector embeddings, can help semi-automate the contract review process and reduce time and costs for legal professionals reviewing deontic modalities in contracts. In this study, we create a domain-specific dataset and train both baseline and neural network models for contract sentence classification. This approach offers a more efficient and cost-effective solution for contract review, mimicking the work of a lawyer. Our approach achieves an accuracy of 0.90, showcasing its effectiveness in identifying and evaluating individual contract sentences.},
  archive      = {J_AIL},
  author       = {Graham, S. Georgette and Soltani, Hamidreza and Isiaq, Olufemi},
  doi          = {10.1007/s10506-023-09379-2},
  journal      = {Artificial Intelligence and Law},
  month        = {3},
  number       = {1},
  pages        = {79-100},
  shortjournal = {Artif. Intell. Law},
  title        = {Natural language processing for legal document review: Categorising deontic modalities in contracts},
  volume       = {33},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A large scale benchmark for session-based recommendations on the legal domain. <em>AIL</em>, <em>33</em>(1), 43-78. (<a href='https://doi.org/10.1007/s10506-023-09378-3'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The proliferation of legal documents in various formats and their dispersion across multiple courts present a significant challenge for users seeking precise matches to their information requirements. Despite notable advancements in legal information retrieval systems, research into legal recommender systems remains limited. A plausible factor contributing to this scarcity could be the absence of extensive publicly accessible datasets or benchmarks. While a few studies have emerged in this field, a comprehensive analysis of the distinct attributes of legal data that influence the design of effective legal recommenders is notably absent in the current literature. This paper addresses this gap by initially amassing a comprehensive session-based dataset from Jusbrasil, one of Brazil’s largest online legal platforms. Subsequently, we scrutinize and discourse key facets of legal session-based recommendation data, including session duration, types of recommendable legal artifacts, coverage, and popularity. Furthermore, we introduce the first session-based recommendation benchmark tailored to the legal domain, shedding light on the performance and constraints of several renowned session-based recommendation approaches. These evaluations are based on real-world data sourced from Jusbrasil.},
  archive      = {J_AIL},
  author       = {Domingues, Marcos Aurélio and de Moura, Edleno Silva and Marinho, Leandro Balby and da Silva, Altigran},
  doi          = {10.1007/s10506-023-09378-3},
  journal      = {Artificial Intelligence and Law},
  month        = {3},
  number       = {1},
  pages        = {43-78},
  shortjournal = {Artif. Intell. Law},
  title        = {A large scale benchmark for session-based recommendations on the legal domain},
  volume       = {33},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Integrating legal event and context information for chinese similar case analysis. <em>AIL</em>, <em>33</em>(1), 1-42. (<a href='https://doi.org/10.1007/s10506-023-09377-4'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Similar case analysis (SCA) is an essential topic in legal artificial intelligence, serving as a reference for legal professionals. Most existing works treat SCA as a traditional text classification task and ignore some important legal elements that affect the verdict and case similarity, like legal events, and thus are easily misled by semantic structure. To address this issue, we propose a Legal Event-Context Model named LECM to improve the accuracy and interpretability of SCA based on Chinese legal corpus. The event-context integration mechanism, which is an essential component of the LECM, is proposed to integrate the legal event and context information based on the attention mechanism, enabling legal events to be associated with their corresponding relevant contexts. We introduce an event detection module to obtain the legal event information, which is pre-trained on a legal event detection dataset to avoid labeling events manually. We conduct extensive experiments on two SCA tasks, i.e., similar case matching (SCM) and similar case retrieval (SCR). Compared with baseline models, LECM is validated by about 13% and 11% average improvement in terms of mean average precision and accuracy respectively, for SCR and SCM tasks. These results indicate that LECM effectively utilizes event-context knowledge to enhance SCA performance and its potential application in various legal document analysis tasks.},
  archive      = {J_AIL},
  author       = {Dan, Jingpei and Xu, Lanlin and Wang, Yuming},
  doi          = {10.1007/s10506-023-09377-4},
  journal      = {Artificial Intelligence and Law},
  month        = {3},
  number       = {1},
  pages        = {1-42},
  shortjournal = {Artif. Intell. Law},
  title        = {Integrating legal event and context information for chinese similar case analysis},
  volume       = {33},
  year         = {2025},
}
</textarea>
</details></li>
</ul>

</body>
</html>
