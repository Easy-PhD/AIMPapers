<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>CC</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="cc">CC - 155</h2>
<ul>
<li><details>
<summary>
(2025). Enhanced scene-specific saliency prediction incorporating scene information. <em>CC</em>, <em>17</em>(6), 1-19. (<a href='https://doi.org/10.1007/s12559-025-10506-1'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Saliency prediction (SP) estimates human gaze fixation in scenes, guided by visual attention mechanisms. While deep learning approaches have made substantial progress in SP, many overlook the inherent scene information presented in images. Further exploration is needed to generalize SP models across broader scene types and investigate how SP models behave differently across scenes, thereby advancing our understanding of scene knowledge’s impact on SP performance and human visual attention. This study introduces a scene-specific SP framework that incorporates scene labels predicted by a transferred CLIP-based classifier with multi-layer perceptron enhancement, trained on the CAT2000 dataset comprising 20 scene types. The scene classifier achieves high accuracy (averaging 88%) for 20-scene classification, providing reliable labels for subsequent scene-specific SP. In the SP phase, we employ transfer learning with DINet, training separate SP models for each scene type and a general SP model trained on images from all 20 scene types for comparison. Results show that the scene-specific SP framework consistently outperforms the general model across most scenes, with average improvements of 0.36% in AUC, 6.07% in NSS, and 4.66% in CC. Higher NSS and CC gains indicate that the scene-specific models effectively capture more accurate saliency position and distribution information from each specific scene that aligns with human gaze patterns. Moreover, the proposed framework is model-agnostic, ensuring compatibility with various SP models. Our findings highlight the cognitive importance of incorporating prior scene knowledge for precise SP and deepen the understanding of visual attention mechanisms across diverse environments.},
  archive      = {J_CC},
  author       = {Han, Shuning and Sun, Zhe and Michikawa, Takashi and F. Caiafa, Cesar and Solé-Casals, Jordi and Yokota, Hideo},
  doi          = {10.1007/s12559-025-10506-1},
  journal      = {Cognitive Computation},
  month        = {12},
  number       = {6},
  pages        = {1-19},
  shortjournal = {Cogn. Comput.},
  title        = {Enhanced scene-specific saliency prediction incorporating scene information},
  volume       = {17},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Early detection of cardiovascular disease using AdaBoost convolutional random arithmetic trigonometric algorithm. <em>CC</em>, <em>17</em>(6), 1-21. (<a href='https://doi.org/10.1007/s12559-025-10511-4'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {At present, health prediction has become greatly significant in the medical domain. Healthcare prediction enhances diagnostic accuracy and aids in public health as well as medical management. Predictive analytics using big data allows researchers to develop different predictive models for forecasting health conditions and improving clinical outcomes. In recent years, cardiovascular disease has been considered to be the deadliest disease worldwide. Numerous research works have used both Machine Learning and Deep Learning approaches to attain superior decision-making processes. In this article, we propose a novel predictive framework named AdaBoost Convolutional-based Random Arithmetic Trigonometric algorithm for disease prediction. The proposed algorithm is designed for large datasets, offering probabilistic classification with feature independence. It is trained using cardiac disease data obtained from the Kaggle website. The dataset is utilized to accurately identify and classify both healthy and unhealthy cases. Also, it is suitable to validate with big data features and efficiency is estimated by validating with the determined dataset. Finally, the experimental evaluation is made in predicting the health condition of diverse patients. The results show that the proposed algorithm achieved a 98.85% accuracy rate, a precision of 98.21%, a recall of 98.76%, and an F1-score of 98.48%, enabling early disease detection and the prediction of patients’ future health.},
  archive      = {J_CC},
  author       = {Chidambaram, S. and Ramesh, S. and Geetha, S. and Cyril, C. Pretty Diana},
  doi          = {10.1007/s12559-025-10511-4},
  journal      = {Cognitive Computation},
  month        = {12},
  number       = {6},
  pages        = {1-21},
  shortjournal = {Cogn. Comput.},
  title        = {Early detection of cardiovascular disease using AdaBoost convolutional random arithmetic trigonometric algorithm},
  volume       = {17},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). PPHN: Deep sequential learning architecture for parkinson’s disease prediction using amino acid descriptors from protein sequences. <em>CC</em>, <em>17</em>(6), 1-30. (<a href='https://doi.org/10.1007/s12559-025-10515-0'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Parkinson’s disease (PD) is a slow-progressing neurological disorder that usually appears in elderly individuals, although it develops much earlier. While many researchers have focused on detecting PD based on symptoms, only limited research has explored the use of protein sequences for early detection. In this work, we propose a deep sequential learning model, Parkinson’s Protein Hybrid Net (PPHN), which leverages long short-term memory (LSTM) and gated recurrent unit (GRU) to detect PD from protein amino acid sequences. The Parkinson’s and healthy protein sequences were collected from the NCBI and UniProt databases. The relevant features of the amino acid sequences were extracted using three primary descriptors, viz., amino acid composition (AAC), dipeptide composition (DPC), and tripeptide composition (TPC). To capture more comprehensive sequence-level information, composite descriptors (viz., AAC-DPC-TPC, AAC-DPC, AAC-TPC, and DPC-TPC) were further constructed by combining these individual feature sets. Additionally, SHapley Additive exPlanations (SHAP) analysis was incorporated to interpret the model’s predictions and identify the most influential features contributing to classification. The proposed method is found to produce promising results for most of the feature descriptors, achieving the highest accuracy of 98.73% with a precision of 0.9890, a recall of 0.9850, and an $$F_1$$ score of 0.9900 (for AAC-DPC-TPC feature descriptor compared with other counterpart methods). The results of the paired t-tests and the Wilcoxon signed-rank tests also confirm the statistical significance of the better results obtained by the proposed model versus other compared methods in most cases. Confidence interval (CI) tests performed at 90%, 95%, and 99% confidence levels demonstrate lower error rates and smaller error bounds achieved by the proposed method than those of the other methods for most of the feature descriptors. Therefore, the proposed method may serve as an effective computational tool for the early detection of PD from protein sequences.},
  archive      = {J_CC},
  author       = {Baruah, Bornali and Kumar, Ansuman and Halder, Anindya},
  doi          = {10.1007/s12559-025-10515-0},
  journal      = {Cognitive Computation},
  month        = {12},
  number       = {6},
  pages        = {1-30},
  shortjournal = {Cogn. Comput.},
  title        = {PPHN: Deep sequential learning architecture for parkinson’s disease prediction using amino acid descriptors from protein sequences},
  volume       = {17},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Smartphone selection with picture fuzzy modified combined compromise solutions. <em>CC</em>, <em>17</em>(5), 1-32. (<a href='https://doi.org/10.1007/s12559-025-10492-4'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The selection of smartphones is a complex decision-making process influenced by multiple criteria such as technical specifications, brand reputation, price, battery capacity, internal storage, and user preferences. The ambiguity and uncertainty around phenomena are handled by fuzzy sets. Picture fuzzy (PF) sets are an extension of fuzzy sets used for managing uncertainty in more complex situations when fuzzy sets are unable to produce reliable results. This paper outlines a framework in PF environment to solve the problem of selection of smartphone based on different factors. A new PF knowledge measure is proposed to measure the amount of knowledge linked to PF sets, and its reliability is tested using some numerical examples. A new PF accuracy measure is proposed based on the suggested knowledge measure and is used to find the pattern similarity of unknown patterns with given pattern. A new score function is proposed to compare the PF numbers, which can get around the drawbacks of existing score functions. A modified combined compromise solution (CoCoSo) technique is provided to select the best smartphone by using suggested scoring function and accuracy measure. Lastly, to prove the effectiveness of the suggested method, a comparative analysis carries out with the other existing methods.},
  archive      = {J_CC},
  author       = {Garg, Manish and Kumar, Satish},
  doi          = {10.1007/s12559-025-10492-4},
  journal      = {Cognitive Computation},
  month        = {10},
  number       = {5},
  pages        = {1-32},
  shortjournal = {Cogn. Comput.},
  title        = {Smartphone selection with picture fuzzy modified combined compromise solutions},
  volume       = {17},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Unveiling task difficulty in a visual pattern memory task: A comprehensive analysis of factors influencing performance. <em>CC</em>, <em>17</em>(5), 1-15. (<a href='https://doi.org/10.1007/s12559-025-10494-2'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Traditional assessments of visual pattern memory difficulty rely predominantly on the number of items, overlooking other potential influential factors. This study aimed to identify and analyze several variables affecting performance in a visual pattern memory task. We conducted a computer-based visual pattern memory experiment with 20 participants. The paradigm systematically manipulated both the quantity and spatial arrangement of target stimuli while controlling for eye movements. Performance data were analyzed using generalized linear mixed models (GLMMs) and linear mixed models (LMMs) to identify significant predictors of task difficulty. Performance was influenced by the number and spatial relationships of target and non-target stimuli, their distance from the display center, and participants’ selection strategies. Dense stimulus patterns facilitated better recall than distributed configurations, suggesting an advantage of chunking strategies. Additionally, a left-side selection bias emerged, which was affected by target distribution. This study highlights the complexity of difficulty scaling in a visual pattern memory task. It demonstrates that incorporating multiple quantified aspects of visual stimuli, including spatial configuration, target distribution, and behavioral strategies, is essential for developing adaptive cognitive training programs that rely on spatial information processing.},
  archive      = {J_CC},
  author       = {Mortazavi, Fatemeh and Vahabie, Abdol-Hossein and Ahmadabadi, Majid Nili and Moradi, Hadi},
  doi          = {10.1007/s12559-025-10494-2},
  journal      = {Cognitive Computation},
  month        = {10},
  number       = {5},
  pages        = {1-15},
  shortjournal = {Cogn. Comput.},
  title        = {Unveiling task difficulty in a visual pattern memory task: A comprehensive analysis of factors influencing performance},
  volume       = {17},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Knowledge enhanced and incongruity perceiving network for multimodal sarcasm detection. <em>CC</em>, <em>17</em>(5), 1-16. (<a href='https://doi.org/10.1007/s12559-025-10499-x'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Sarcasm is a complex linguistic act in which the literal meaning is opposite to the true attitude. Multimodal sarcasm detection (MSD) aims to identify whether a given multimodal data sample is sarcastic. Although existing methods have achieved impressive success, they ignored sarcastic information in text and potential clues in images and failed to fully explore cross-modal feature representation and inconsistency between modalities. To solve the above problems, we propose a knowledge enhanced and incongruity perceiving network (KEIPN) for MSD. We design a text knowledge pretraining module that includes RoBERTa models enhanced with biased sentiment knowledge and sarcasm knowledge. Next, we develop an image knowledge amalgamation module to integrate different types of knowledge into visual model, enhancing its visual understanding capability. Pretrained language and visual models can extract better feature representations. Then, we propose a cross-modal knowledge distillation module to model the relationship between text and images and achieve adaptive weighted fusion. Finally, we design an incongruity perception module to capture the inconsistency between images and text and weight the loss using keyless attention mechanisms. Extensive experiments on widely used datasets MMSD and MMSD2.0 demonstrate the superiority of our model over state-of-the-art (SOTA) methods.},
  archive      = {J_CC},
  author       = {Liu, Mingqi and Li, Zhixin},
  doi          = {10.1007/s12559-025-10499-x},
  journal      = {Cognitive Computation},
  month        = {10},
  number       = {5},
  pages        = {1-16},
  shortjournal = {Cogn. Comput.},
  title        = {Knowledge enhanced and incongruity perceiving network for multimodal sarcasm detection},
  volume       = {17},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). TDGAT: Triple-dimensional graph attention networks for exploring the optimal perspective for aspect-based sentiment analysis. <em>CC</em>, <em>17</em>(5), 1-15. (<a href='https://doi.org/10.1007/s12559-025-10497-z'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Aspect-based sentiment analysis (ABSA) is an attractive and challenging fine-grained subtask in the natural language processing (NLP) community. In prior works, the relevant research has achieved significant progress in leveraging external knowledge to strengthen the sentiment representation in ABSA. However, the prior simple utilization of external knowledge hurts the model’s comprehension of the diversity of sentiment. Therefore, this paper proposed a novel ABSA approach to achieve multi-dimensional understanding of sentiment, namely triple-dimensional graph attention networks (TDGAT). Preliminarily, a task-oriented prompt template is designed to evoke the pre-trained language model (PLM) generation ability, which would provide the essential semantic features for sentiment understanding. To avoid the errors caused by the single-dimension external knowledge, TDGAT constructs triple-dimensional sentiment graphs depending on valence, arousal, and dominance, respectively, which enhance the model’s sentiment comprehension ability greatly. Besides, this work also leverages GATs to evacuate the sentiment relations through exploiting the constructed graphs, which would largely assist to learn an optimal ABSA model. Eventually, to uncover the optimal sentiment exploration perspective, extensive experiments are conducted on five public and available benchmark datasets, and the related results show the proposed TDGAT outperforms the-state-of-art baselines with different sentiment perspectives.},
  archive      = {J_CC},
  author       = {Shi, Xuefeng and Ding, Weiping and Hu, Min and Kang, Xin and Ren, Fuji},
  doi          = {10.1007/s12559-025-10497-z},
  journal      = {Cognitive Computation},
  month        = {10},
  number       = {5},
  pages        = {1-15},
  shortjournal = {Cogn. Comput.},
  title        = {TDGAT: Triple-dimensional graph attention networks for exploring the optimal perspective for aspect-based sentiment analysis},
  volume       = {17},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). An optimized feature selection approach for multi-view ensemble learning in sentiment analysis of user reviews. <em>CC</em>, <em>17</em>(5), 1-21. (<a href='https://doi.org/10.1007/s12559-025-10496-0'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Sentiment classification is a discipline of artificial intelligence that classifies evaluations as positive, neutral, or negative based on the emotional opinion detected through natural language processing. A key challenge in sentiment classification is high dimensionality, as it often leads to reduced model performance due to redundant or irrelevant features. Removing such features, a common practice in machine learning and data processing, helps create an optimal feature space by reducing the number of input variables. This study proposes a novel multi-view ensemble learning model that integrates ElasticNetCV-based feature selection across multiple text representations—namely, Term Frequency-Inverse Document Frequency (TF-IDF), DistillBERT, and Global Vectors for Word Representation (GloVe). To our knowledge, this is the first systematic evaluation of ElasticNetCV applied to both traditional and contextual embeddings within a unified model. The selected features are used to train a multi-view soft voting ensemble model incorporating Random Forest (RF), Logistic Regression (LR), and Artificial Neural Networks (ANN) as base classifiers. Experiments on Yelp, Amazon, and IMDB datasets demonstrate notable F1-scores of 97.1%, 91.2%, and 95.4%, respectively, highlighting the efficiency of the proposed approach.},
  archive      = {J_CC},
  author       = {Demirci, Fatih and Garip, Zeynep and Ekinci, Ekin},
  doi          = {10.1007/s12559-025-10496-0},
  journal      = {Cognitive Computation},
  month        = {10},
  number       = {5},
  pages        = {1-21},
  shortjournal = {Cogn. Comput.},
  title        = {An optimized feature selection approach for multi-view ensemble learning in sentiment analysis of user reviews},
  volume       = {17},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Customer-perceived value and social media analytics: How supplier evaluation can benefit from aspect-based sentiment analysis and fuzzy inference. <em>CC</em>, <em>17</em>(5), 1-28. (<a href='https://doi.org/10.1007/s12559-025-10495-1'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Understanding customer-perceived value is essential for driving strategic supplier development and operational excellence in modern supply chains. As customer sentiments increasingly manifest in social media, natural language processing (NLP) techniques carry the potential of extracting actionable insights from unstructured data. This study proposes a novel decision-making model that combines aspect-based sentiment analysis (ABSA) with fuzzy inference systems (FIS) to support supplier evaluation based on customer value perception. Bridging symbolic and sub-symbolic AI, the model quantifies sentiment polarity, subjectivity, and aspect relevance from social media content, integrating this information into a multi-stage fuzzy logic framework for large-scale group decision-making (LSGDM). Unlike conventional supplier evaluation methods, this approach operationalizes concept-level affective information by fusing customer sentiment information with supply chain operational data. An illustrative application in the smartphone industry demonstrates the model’s ability to analyze social media data from the X platform and generate quantitative indicators reflecting customer value perception. The model effectively incorporates these insights into supplier evaluation, highlighting suppliers’ strengths and areas for improvement. The results show that sentiment-informed supplier assessment enables more responsive and customer-aligned development strategies. The proposed approach highlights the benefits of integrating sentic computing principles into supply chain analytics, showing the model’s capability to capture customer perceptions and make them a driver for continuous improvement initiatives in supplier development.},
  archive      = {J_CC},
  author       = {Zanon, Lucas Gabriel and Arantes, Rafael Ferro Munhoz and Calache, Lucas Daniel Del Rosso and Martins, Roberto Antonio and Carpinetti, Luiz Cesar Ribeiro},
  doi          = {10.1007/s12559-025-10495-1},
  journal      = {Cognitive Computation},
  month        = {10},
  number       = {5},
  pages        = {1-28},
  shortjournal = {Cogn. Comput.},
  title        = {Customer-perceived value and social media analytics: How supplier evaluation can benefit from aspect-based sentiment analysis and fuzzy inference},
  volume       = {17},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Exploring artificial intelligence tutor teammate adaptability to harness discovery curiosity and promote learning in the context of interactive molecular dynamics. <em>CC</em>, <em>17</em>(5), 1-24. (<a href='https://doi.org/10.1007/s12559-025-10498-y'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This study examines the impact of an artificial intelligence tutor teammate (AI) on student curiosity-driven engagement and learning effectiveness during interactive molecular dynamics (IMD) tasks on the Visual Molecular Dynamics platform. It explores the role of the AI’s curiosity triggering and response behaviors in stimulating and sustaining student curiosity, affecting the frequency and complexity of student-initiated questions. The study further assesses how AI interventions shape student engagement, foster discovery curiosity, and enhance team performance within the IMD learning environment. Using a Wizard-of-Oz paradigm, a human experimenter dynamically adjusts the AI tutor teammate’s behavior through a large language model. By employing a mixed-methods exploratory design, a total of 11 high school students participated in four IMD tasks that involved molecular visualization and calculations, which increased in complexity over a 60 min. Team performance was evaluated through real-time observation and recordings, whereas team communication was measured by question complexity, and AI’s curiosity-triggering and response behaviors. Cross recurrence quantification analysis (CRQA) metrics reflected structural alignment in coordination and were linked to communication behaviors. High-performing teams exhibited superior task completion, deeper understanding, and increased engagement. Advanced questions were associated with AI curiosity triggering, indicating heightened engagement and cognitive complexity. CRQA metrics highlighted dynamic synchronization in student-AI interactions, emphasizing structured yet adaptive engagement to promote curiosity. These proof-of-concept findings suggest that the AI’s dual role as a teammate and educator indicates its capacity to provide adaptive feedback, sustaining engagement, and epistemic curiosity. Future research will refine AI strategies by integrating multimodal data to enhance curiosity-driven learning.},
  archive      = {J_CC},
  author       = {Demir, Mustafa and Miratsky, Jacob and Mishra, Punya and Nguyen, Jonathan and Chan, Chun Kit and Singharoy, Abhishek},
  doi          = {10.1007/s12559-025-10498-y},
  journal      = {Cognitive Computation},
  month        = {10},
  number       = {5},
  pages        = {1-24},
  shortjournal = {Cogn. Comput.},
  title        = {Exploring artificial intelligence tutor teammate adaptability to harness discovery curiosity and promote learning in the context of interactive molecular dynamics},
  volume       = {17},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Event-triggered optimized control for nonlinear multiagent systems via reinforcement learning strategy. <em>CC</em>, <em>17</em>(5), 1-9. (<a href='https://doi.org/10.1007/s12559-025-10502-5'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper focuses on the event-triggered optimal control issue for a class of nonlinear strict-feedback multiagent system, which embraces optimization as a fundamental design principle for high-order system control. The reinforcement learning (RL) strategy based on the identifier-actor-critic structure, in the process of optimized backstepping design, is adopted to optimize control performance. Besides that, a novel event-triggered mechanism, which utilizes both on the latest sampled state and a non-negative threshold, is implemented to enhance the efficiency of resource utilization. The computing cost is reduced, and the communication resources are saved by reducing the number of sampling times and the amount of data transmission, thus improving the system efficiency. It is proved that the closed-loop system is semi-globally uniformly ultimately bounded (SGUUB) in probability and the Zeno behavior is prevented through the analysis of Lyapunov stability theory. The conduction of two simulation examples is as the final step to substantiate the efficacy of the designed method.},
  archive      = {J_CC},
  author       = {Meng, Luyu and Wang, Xin and Wang, Ziming},
  doi          = {10.1007/s12559-025-10502-5},
  journal      = {Cognitive Computation},
  month        = {10},
  number       = {5},
  pages        = {1-9},
  shortjournal = {Cogn. Comput.},
  title        = {Event-triggered optimized control for nonlinear multiagent systems via reinforcement learning strategy},
  volume       = {17},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Reinforcement learning for human-AI collaboration: Challenges, mechanisms, and methods. <em>CC</em>, <em>17</em>(5), 1-16. (<a href='https://doi.org/10.1007/s12559-025-10500-7'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Reinforcement Learning (RL) has significantly advanced the research topic of human-Artificial-Intelligence (AI) collaboration, offering powerful methods for improving the interaction between humans and AI systems across a range of applications. Despite the rapid growth of research on this topic, a review systematizing the challenges, mechanisms, and methods specifically tailored for human-AI collaboration is still lacking in the literature. This review focuses on RL-based human-AI collaboration, which can be structured into the following four parts: (1) comprehensively summarizing and analyzing the main challenges in human-AI collaboration, including aligning AI systems with human behaviors, generalizing to new human collaborators, ensuring robustness in dynamic environments, and performing coordination among multiple agents; (2) discussing the core mechanisms to address these challenges, including behavior characterization, intention understanding, and multi-agent coordination, and presenting a human-centric viewpoint on these mechanisms; (3) exploring the methods containing each mechanism, and investigating their effectiveness in enhancing human-AI collaboration; (4) comparing the representative methods from the perspectives of scalability, adaptability, and interpretability, and identifying the current challenges haunting this topic and proposing the potential directions for the prospective research on it. We expect this work will provide valuable insights into the academic advancements and inspire the technological breakthroughs in RL-based human-AI collaboration in the near future.},
  archive      = {J_CC},
  author       = {Li, Wei and Liu, Hongming and Huang, Kaizhu and Hussain, Amir},
  doi          = {10.1007/s12559-025-10500-7},
  journal      = {Cognitive Computation},
  month        = {10},
  number       = {5},
  pages        = {1-16},
  shortjournal = {Cogn. Comput.},
  title        = {Reinforcement learning for human-AI collaboration: Challenges, mechanisms, and methods},
  volume       = {17},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multiple local-global correlation-based deep neural network with selective kernel attention for bearing fault diagnosis. <em>CC</em>, <em>17</em>(5), 1-17. (<a href='https://doi.org/10.1007/s12559-025-10508-z'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Few-shot learning-based deep neural networks have emerged as a promising solution for bearing fault diagnosis, particularly under limited labeled data conditions. However, existing approaches often rely on a single linear comparison module at the end of the feature extraction pipeline and commonly utilize pre-trained models from other domains, which might not generalize well to specific datasets such as those involving bearing faults. In this study, we propose a novel few-shot learning model, namely the Multiple Local-Global Correlation-based Deep Neural Network (MLGCN), for bearing fault diagnosis. The model is designed by simultaneously learning nonlinear data distributions and embedding representations in an end-to-end fashion. Inspired by human perceptual mechanisms that focus attention on task-relevant information, we introduce a Selective Kernel Attention Feature Extractor module to dynamically capture diverse and critical signal features, which is especially effective in limited data conditions. In addition, we present the Multiple Dual Local-Global Cross Attention (Dual LGCA) module to leverage the full feature hierarchy for enhanced similarity learning, enabling hierarchical similarity reasoning—akin to human comparison strategies that integrate multi-scale contextual cues. The proposed model is assessed using two popular bearing fault benchmarks, including the Case Western Reserve University (CWRU) and Paderborn University (PU) datasets, in distinct experiments to assess its accuracy under limited data and its generalization to real bearing damages. Our code will be released at https://github.com/ZQuang2202/MLGCN-FewshotBearingFault .},
  archive      = {J_CC},
  author       = {Nguyen, Van-Quang and Tran, Thi-Thao and Vu, Manh-Hung and Pham, Van-Truong and Lo, Men-Tzung},
  doi          = {10.1007/s12559-025-10508-z},
  journal      = {Cognitive Computation},
  month        = {10},
  number       = {5},
  pages        = {1-17},
  shortjournal = {Cogn. Comput.},
  title        = {Multiple local-global correlation-based deep neural network with selective kernel attention for bearing fault diagnosis},
  volume       = {17},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). An EfficientNet-b0 framework for nitrogen stress identification in sustainable precision farming plants. <em>CC</em>, <em>17</em>(5), 1-21. (<a href='https://doi.org/10.1007/s12559-025-10504-3'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Nitrogen deficiency is a critical stress factor in plants, significantly impacting crop yields in precision agriculture. Early detection of nutrient imbalances, such as inadequate nitrogen levels, is essential for optimizing plant growth and achieving high agricultural productivity. Nitrogen plays a vital role in various plant processes, including the synthesis of chlorophyll, proteins, amino acids, and nucleic acids, all of which are crucial for proper plant development. Deficiency in nitrogen can lead to observable changes in plant morphology, such as reduced leaf number, discoloration, and stunted growth. Recent advancements in imaging technologies have facilitated the development of computer vision-based plant phenomics, enabling rapid, non-invasive, and automated detection of plant stress. In this study, we propose an automated deep learning (DL)-based phenotyping approach to detect and classify nitrogen deficiency in plant leaf images. The method employs EfficientNet-B0, a convolutional neural network (CNN) architecture, to identify and categorize stress symptoms associated with nitrogen insufficiency. The performance of the proposed model was evaluated on two datasets, using key metrics including accuracy, precision, recall, and F1 score. The results show that the EfficientNet-B0 model achieves an accuracy of 97.9%, outperforming the baseline model, which reached an accuracy of 93.2%. These results demonstrate the effectiveness of the proposed deep learning model in accurately detecting nitrogen deficiency in plants, offering a promising tool for precision agriculture applications.},
  archive      = {J_CC},
  author       = {Babu, Surya and Thawait, Kartikey and Gopi, Arepalli Peda and Srinivasarao, Ulligaddala and Naik, K. Jairam},
  doi          = {10.1007/s12559-025-10504-3},
  journal      = {Cognitive Computation},
  month        = {10},
  number       = {5},
  pages        = {1-21},
  shortjournal = {Cogn. Comput.},
  title        = {An EfficientNet-b0 framework for nitrogen stress identification in sustainable precision farming plants},
  volume       = {17},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Explainable deep learning system for custom report generation in breast cancer histology. <em>CC</em>, <em>17</em>(5), 1-13. (<a href='https://doi.org/10.1007/s12559-025-10510-5'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Breast cancer is the most lethal type of cancer among women, one of the causes can be due to the lack of professionals to evaluate the results of medical images in time (Sharafaddini et al. Multimed Tools Appl, 1–112 2024). This problem is even greater in developing countries. In recent years, diagnostic tools based on artificial intelligence techniques have been developed to improve diagnosis time and results. In this work, we present a system that analyzes histopathological images obtained from breast tissue biopsies to design a classification system that distinguishes between benign and malignant tissue. To demonstrate that the proposed work is robust, multiple alternatives and combinations are studied to obtain the best cases. Finally, we compare the proposed approach with previous works. Furthermore, the developed system integrates explainable artificial intelligence techniques to produce a report to the physician, including a heat map with the areas the system has determined to be essential for classification.},
  archive      = {J_CC},
  author       = {Anguita-Molina, Miguel Ángel and Civit-Masot, Javier and Muñoz-Saavedra, Luis and Polo-Rodríguez, Aurora and Domínguez-Morales, Manuel},
  doi          = {10.1007/s12559-025-10510-5},
  journal      = {Cognitive Computation},
  month        = {10},
  number       = {5},
  pages        = {1-13},
  shortjournal = {Cogn. Comput.},
  title        = {Explainable deep learning system for custom report generation in breast cancer histology},
  volume       = {17},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Product color emotional design based on interdependent networks and cascading failure. <em>CC</em>, <em>17</em>(5), 1-23. (<a href='https://doi.org/10.1007/s12559-025-10507-0'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Product color emotional design (PCED) systems are characterized by a power-law distribution; this is manifested as a small number of colors constituting a large number of emotional connections of users, and the majority of colors constituting a small number of emotional connections of users. This will lead to the neglect of the potential connections between colors, thus causing the inability to generate a product color design scheme based on color interactions. Consequently, the reliability of the design scheme ultimately undermines the reliability of the design scheme. For this reason, this paper proposes a PCED method based on interdependent networks (INs) and cascading failure (CF). First, a web crawler, word processing, clustering, and complex network analysis are utilized to construct the product color emotion dataset. Then, based on this dataset, the INs of color emotion are constructed, and the initial failure node in the color emotion network is determined based on a variety of important node judgment methods. Finally, the color scheme that meets the user’s emotional needs is generated via the CF-based product color scheme generation rules. The PCED of a high-speed rail business seat is provided as an example to verify the validity of the proposed method.},
  archive      = {J_CC},
  author       = {Ding, Man and Ju, Yixian and Liu, Zhengwen and Zhao, Fanghua and Cho, Jounghyung},
  doi          = {10.1007/s12559-025-10507-0},
  journal      = {Cognitive Computation},
  month        = {10},
  number       = {5},
  pages        = {1-23},
  shortjournal = {Cogn. Comput.},
  title        = {Product color emotional design based on interdependent networks and cascading failure},
  volume       = {17},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Blockchain-enhanced deep learning framework for secure patient data management in IoT-enabled healthcare systems. <em>CC</em>, <em>17</em>(5), 1-19. (<a href='https://doi.org/10.1007/s12559-025-10503-4'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The healthcare industry has witnessed significant advancements with the adoption of smart wearable devices, enabling remote patient monitoring and treatment. However, these technologies introduce critical security risks, including session hijacking, data manipulation, and spoofing attacks, which can compromise patient privacy and safety. This study proposes a secure framework integrating a ridgelet neural network optimized with the addax optimization algorithm (AOA) to detect and mitigate threats in wearable healthcare systems. The proposed three-layered framework consists of (1) an analytics layer, which utilizes a ridgelet neural network for classifying wearable device data as malicious or non-malicious, (2) a blockchain layer, ensuring the integrity and secure storage of verified patient data, and (3) a user layer, facilitating authorized access to healthcare providers. The framework demonstrates superior performance in detecting malicious data, achieving an accuracy ranging from 98.5 to 99.5%. The blockchain layer ensures data transparency, immutability, and security, significantly reducing the risks of cyber threats. The proposed approach enhances the security of wearable healthcare technology by effectively classifying threats and safeguarding patient data, offering a trusted and resilient health data management solution.},
  archive      = {J_CC},
  author       = {Srivastava, Vivek and Raj, Gaurav and Gupta, Shruti and Kaur, Kamaljit},
  doi          = {10.1007/s12559-025-10503-4},
  journal      = {Cognitive Computation},
  month        = {10},
  number       = {5},
  pages        = {1-19},
  shortjournal = {Cogn. Comput.},
  title        = {Blockchain-enhanced deep learning framework for secure patient data management in IoT-enabled healthcare systems},
  volume       = {17},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Longitudinal prediction of mental health outcomes in vulnerable youth using machine learning. <em>CC</em>, <em>17</em>(5), 1-19. (<a href='https://doi.org/10.1007/s12559-025-10509-y'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Mental illnesses affect almost 15% of the world’s population, with half of the cases emerging before age 14. Improved methods for predicting mental distress among adolescents, particularly in vulnerable populations, are needed. This study utilized traditional machine learning techniques to predict mental health status at age 17. We assessed the correlates of mental health outcomes in a sample of 632 adolescents with general mental distress (i.e., total difficulties score of 17 or higher) at age 11, who participated in the UK Millennium Cohort Study. Predictors measured at ages 11 and 14 were included in the analysis. Mental health status at age 17 was best predicted using a Balanced Random Forest model (AUC 0.75). Explainability techniques enabled the identification of several critical factors, such as school environment, emotional distress, sleep patterns, patience, and social network at ages 11 or 14, which were able to differentiate participants with poor or good mental health outcomes at age 17. Individuals experiencing persistent mental distress between the ages 11 and 17 were most likely to suffer from unhappiness and academic struggles. Our results point to potentially modifiable factors associated with the progression of mental distress in adolescents at high risk. These factors could pave the way for improved early intervention and preventive strategies for vulnerable young people during adolescence.},
  archive      = {J_CC},
  author       = {Pujadas, Esmeralda Ruiz and Díaz-Caneja, Covadonga M. and Stevanovic, Dejan and Quintero, Marta Ferrer and Martín-Isla, Carlos and Hernández-González, Jerónimo and Atehortúa, Angelica and Lazrak, Noussair and Pries, Lotta and Delespaul, Philippe and Camacho, Marina and Gülöksüz, Sinan and Rutten, Bart P. F. and Lekadir, Karim},
  doi          = {10.1007/s12559-025-10509-y},
  journal      = {Cognitive Computation},
  month        = {10},
  number       = {5},
  pages        = {1-19},
  shortjournal = {Cogn. Comput.},
  title        = {Longitudinal prediction of mental health outcomes in vulnerable youth using machine learning},
  volume       = {17},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Parallel hybrid CNN-KELM and attention-guided pyramid transformer networks for efficient fruit image classification and segmentation. <em>CC</em>, <em>17</em>(4), 1-27. (<a href='https://doi.org/10.1007/s12559-025-10460-y'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Fruit recognition systems are rapidly growing areas in computer vision for segmentation and classification. The pioneering fruit recognition systems are insufficient for accurately and quantitatively analyzing fruit segmentation and classification, and their computational and generalization efficiencies are also not optimal. In this paper, a novel segmentation approach called Attention-Guided Pyramid Transformer Network (Atn-PTNet) and a classification mechanism termed Fine-tuned Hybrid Parallel Convolutional Neural Network-Kernel Extreme Learning Machine (Ft-HPCNN-KELM) are employed to optimize the segmentation and categorization of fruit images in a fruit recognition system. In the segmentation process, transformer self-attention and feature pyramid attention are incorporated to develop a self-aware attention mechanism that effectively learns extensive and valuable contextual data between encoding characteristics, thereby enhancing the accuracy of image segmentation and the stability of feature maps. Additionally, the Progressive Enhancement Module applies two distinct convolutional procedures, multi-scale dilated convolution, and gated convolution, to produce the gate maps, achieving more detailed data and a higher receptive field. Further extra multi-scale skip connections across decoder blocks are employed to combine features that are upsampled with various semantic scales. Finally, the segmentation quality of fruit images is improved by the proposed Atn-PTNet, which successfully reduces information loss caused by bilinear upsampling. Two distinct kernel functions, global and local, are fused to create a hybrid kernel or hybrid KELM, indicating the classification process to achieve high learning and generalization capacity. Most discriminative information is extracted using parallel CNN and fed into a hybrid KELM for fruit classification. The hyper-parameters of the parallel CNN and hybrid KELM are optimized using the PSO algorithm. Overall, Atn-PTNet achieves 98.86% accuracy, precision of 98.32%, and MAE of 0.855, significantly superior to existing approaches. The Ft-HPCNN-KELM system attains 98.91% accuracy, 97.76% precision, and a Matthews correlation coefficient of 0.931, demonstrating that the proposed fruit recognition system delivers excellent results compared to pioneering mechanisms. The proposed method outperforms both segmentation and classification in all evaluation metrics, highlighting its effectiveness and reliability in accurately recognizing and segmenting various fruit types.},
  archive      = {J_CC},
  author       = {S., Gracia Nissi and Gladston, Angelin and H, Khanna Nehemiah},
  doi          = {10.1007/s12559-025-10460-y},
  journal      = {Cognitive Computation},
  month        = {8},
  number       = {4},
  pages        = {1-27},
  shortjournal = {Cogn. Comput.},
  title        = {Parallel hybrid CNN-KELM and attention-guided pyramid transformer networks for efficient fruit image classification and segmentation},
  volume       = {17},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Intelligent robotic path planning based on SPIDI-lion optimization algorithm using deep spectral LSTM-gated convolution neural network. <em>CC</em>, <em>17</em>(4), 1-21. (<a href='https://doi.org/10.1007/s12559-025-10464-8'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Robotic intelligence makes an automatic seeing environment to ravel source to target location to do automation. However, traversing path planning is a complex problem due to region coverage, obstacles, and object reasoning, which are many problems in making the decisions for optimal path planning. With billions of connected objects, managing and controlling them for large distributed networks is complex. Most traditional methods are nondependent on path planning because they take the known node point relationship of neighbor distance-based routes. The lack of behavioral analysis of feature limits leads to inadequate decision-making on the best path. To resolve this problem, we propose a novel Deep Spectral LSTM-gated convolution neural network that is used to attain a path planning decision to improve accuracy. Initially, the graph object margin is created using Relative Spectral Neighboring Graph Margin (RSNGP), and Lattice Object Convex Defense Weight (LOCDW) is used to point to an object reference. Then, the Average Moving Index Rate (AMIR) finds the mean rate of path planning to mean patterns. To the importance of the feature limits using SPIDI-Lion Optimization Algorithm (SPIDI-LOA): This finds the feature dependencies of object finding in the path to get the significance of feature relevance pattern frequency to support decision-making. Finally, LSTM-gated CNN (LSTM-GCNN) neural decisions are made by logical choices to create a linear model using an LSTM-gated unit. The proposed system achieves high performance in finding the right path to avoid congestion collision and making the best decision to choose the path compared to the other systems. This achieves the accuracy of 96% of the result on low levels of absolute error rate and high precision, as well as f1-measure accuracy compared to existing methods.},
  archive      = {J_CC},
  author       = {Selvaraj, Dinesh and A.P, Senthil Kumar},
  doi          = {10.1007/s12559-025-10464-8},
  journal      = {Cognitive Computation},
  month        = {8},
  number       = {4},
  pages        = {1-21},
  shortjournal = {Cogn. Comput.},
  title        = {Intelligent robotic path planning based on SPIDI-lion optimization algorithm using deep spectral LSTM-gated convolution neural network},
  volume       = {17},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Handwriting-based gender classification using robotic and machine learning models. <em>CC</em>, <em>17</em>(4), 1-17. (<a href='https://doi.org/10.1007/s12559-025-10478-2'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Handwriting analysis provides insights into motor control and cognitive processes, with potential differences arising from biological gender and neurological conditions such as Parkinson’s disease (PD). Investigating these differences can lead to improved understanding of motor and cognitive functions. This study introduces a novel methodology that integrates robotic features to estimate gender from handwriting. Kinematic and dynamic features are estimated by simulating handwriting with a robotic model. Linear predictive coding (LPC) and singular spectrum analysis (SSA) are applied to the kinematic and dynamic sequences. Machine learning algorithms are used to classify handwriting as male or female. Handwriting samples from healthy individuals (BiosecurID database) and PD patients (PaHaW dataset) were analyzed. The proposed method demonstrates state-of-the-art performance in gender classification, revealing significant differences between healthy and unhealthy individuals. The robotic-based approach successfully mimics arm movements during writing, highlighting distinct motor patterns associated with gender and health status. This research advances the understanding of gender-based differences in motor and cognitive function, particularly in populations with neurological conditions. The integration of robotic features and machine learning provides a promising pathway for future investigations in handwriting analysis, gender classification, and neurodegenerative disease diagnosis.},
  archive      = {J_CC},
  author       = {Aleman, Belen Esther and Diaz, Moises and Ferrer, Miguel A. and Quintana, Jose Juan and Faundez-Zanuy, Marcos},
  doi          = {10.1007/s12559-025-10478-2},
  journal      = {Cognitive Computation},
  month        = {8},
  number       = {4},
  pages        = {1-17},
  shortjournal = {Cogn. Comput.},
  title        = {Handwriting-based gender classification using robotic and machine learning models},
  volume       = {17},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Systematic evaluation of memory retrieval in a neuromorphic model of the hippocampus. <em>CC</em>, <em>17</em>(4), 1-19. (<a href='https://doi.org/10.1007/s12559-025-10476-4'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Theoretical studies on memory capacity in artificial neural networks have shown the number of storable memories scale with the number of neurons and synapses in the network. As the memory capacity limit is reached, then stored memories interfere, and recall performance is reduced. A well-established neuromorphic microcircuit model was employed to systematically evaluate its recall performance as a function of stored patterns, interference, contextual information, network size, and engram cells when specific synaptic connections in the network were strengthened. The model consisted of multi-compartmental Hodgkin-Huxley-based excitatory (pyramidal) cells and two types of inhibitory neurons (bistratified cell, oriens lacunosum-moleculare (OLM) cell) firing at specific phases of a theta oscillation imposed by an external inhibitory signal targeting only the inhibitory cells in the network. Inhibitory cells inhibited specific compartments of the network’s excitatory cells. Two excitatory inputs (sensory and contextual inputs) targeted dendritic compartments of cells in the network and caused cells to fire. Simulation results showed that out of six model variants tested strengthening of excitatory synapses in proximal but not basal dendrites of bistratified cells inhibiting pyramidal cells (model 1) made recall perfect. Strengthening of inhibitory synapses in pyramidal cells (model 2) made recall worst. Decreasing the number of engram cells coding for a memory pattern improved recall in a pathway-dependent way. However, increases in network size had a small effect on improving memory recall and so did increases in stored patterns. Interference between stored patterns had a detrimental effect on recall, which was reversible as the number of engram cells decreased. Changes in contextual information made recall worse confirming previous evidence that more familiar context facilitates memory retrieval.},
  archive      = {J_CC},
  author       = {Andreakos, Nikolaos and Yue, Shigang and Cutsuridis, Vassilis},
  doi          = {10.1007/s12559-025-10476-4},
  journal      = {Cognitive Computation},
  month        = {8},
  number       = {4},
  pages        = {1-19},
  shortjournal = {Cogn. Comput.},
  title        = {Systematic evaluation of memory retrieval in a neuromorphic model of the hippocampus},
  volume       = {17},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Prioritizing measures to mitigate the impact of cultural differences on semiconductor supply chain localization: A neutrosophic collaborative intelligence approach. <em>CC</em>, <em>17</em>(4), 1-21. (<a href='https://doi.org/10.1007/s12559-025-10479-1'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Wafer foundries face cultural differences when building localized wafer fabs and have proposed multiple measures to mitigate the impact of cultural differences. However, given the limited budget, time, and other resources, it is critical to prioritize these measures. Due to the lack of experience and references, how to prioritize criteria and evaluate the performances of measures remains indeterminate. Decision makers also need a highly flexible decision-making process to respond to changing local conditions. This issue is urgent and important for foundries, but there is a clear gap between theory and practice. To fill this gap, this study proposes a neutrosophic collaborative intelligence (NCI) approach, in which experts’ judgments are more reasonably aggregated and the indeterminacy of experts’ judgments is retained until the last step, thereby making a flexible decision, which is distinct from previous studies. In the NCI approach, each expert uses the calibrated neutrosophic geometric mean to derive the neutrosophic weights of criteria for prioritizing mitigating measures. The neutrosophic technique for order of preference by similarity to the ideal solution is then used to evaluate the suitability of each measure. Finally, the neutrosophic weighted intersection operator is devised to aggregate the evaluation results by all experts. A systematic procedure is also established to prioritize these measures. The NCI approach has been applied to a real case. According to the experimental results, two out of the three experts emphasized legal resolvability as the most decisive factor behind why some mitigating measures were more favorable than others. The most suitable measure for mitigating the impact of cultural differences was accelerating yield learning through early mass production, while the least suitable measure was designing a staggered shift system. Other conclusions were indeterminate because experts’ more uncertain judgments had a greater impact on the two measures to be compared than their more certain judgments, which can be resolved by experts modifying their judgments.},
  archive      = {J_CC},
  author       = {Chiu, Min-Chi and Chen, Tin-Chih Toly},
  doi          = {10.1007/s12559-025-10479-1},
  journal      = {Cognitive Computation},
  month        = {8},
  number       = {4},
  pages        = {1-21},
  shortjournal = {Cogn. Comput.},
  title        = {Prioritizing measures to mitigate the impact of cultural differences on semiconductor supply chain localization: A neutrosophic collaborative intelligence approach},
  volume       = {17},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Heterogeneous semantic projection optimization for thematic depth in homogeneous graph. <em>CC</em>, <em>17</em>(4), 1-15. (<a href='https://doi.org/10.1007/s12559-025-10480-8'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Analyzing complex bibliographic networks is essential for understanding research trends and collaborations. Heterogeneous graph neural networks (GNNs) are adept at processing diverse information but are resource-intensive. Conversely, homogeneous GNNs are more efficient but struggle to accurately represent different types of information, making deep thematic representation difficult. To address this, we propose heterogeneous semantic projection optimization (HSPO), a novel approach to enhance thematic depth within homogeneous GNNs. Our method projects rich semantic information from heterogeneous bibliographic networks onto homogeneous networks, enabling more detailed and efficient analysis. We included an evaluation step to verify the method’s effectiveness in predicting future research areas based on the analyzed bibliographic network. Experimental results demonstrated that our method increased the F1-score for single-category recommendations by an average of 10.33% compared to existing GNNs. Additionally, our method significantly improved performance by reducing training time by 59.10% compared to the heterogeneous graph transformer (HGT). These findings confirm the effectiveness of the HSPO approach in optimizing thematic depth while maintaining computational efficiency.},
  archive      = {J_CC},
  author       = {Jung, NamGyu and Panizo-LLedot, Angel and Camacho, David and Choi, Chang},
  doi          = {10.1007/s12559-025-10480-8},
  journal      = {Cognitive Computation},
  month        = {8},
  number       = {4},
  pages        = {1-15},
  shortjournal = {Cogn. Comput.},
  title        = {Heterogeneous semantic projection optimization for thematic depth in homogeneous graph},
  volume       = {17},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). SAD-GAN: A novel secure anomaly detection framework for enhancing the resilience of cyber-physical systems. <em>CC</em>, <em>17</em>(4), 1-19. (<a href='https://doi.org/10.1007/s12559-025-10483-5'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cyber-physical systems occupy a significant portion of the critical infrastructure market, but their prominence has raised concerns due to their susceptibility to certain anomalies. The typical approaches tend to be ineffective to flexible and complex conditions of CPS environments. To address these issues, this paper presents SAD-GAN—self-adaptive deep generative adversarial network—framework that aimed at improving real-time detection of anomalies. SAD-GAN follows a GAN framework with generator (G), which is trained to generate normal behavior of the system, and discriminator (D) which is trained to distinguish normal and artificial data patterns. The anomalies are detected based on a dual-scoring mechanism which consists of reconstruction error and discriminator confidence and are multiplied by the two adjustable constants, 2 and 3. These coefficients determine the relative adjustment of action of each of the scores of the final anomaly detection process and are pumped dynamically with the verification turnover to guarantee credible detection with the changes in the progress of the system. This mechanism enables SAD-GAN to learn and adjust at run time with no need of manual reconfiguration. It was tested against benchmark CPS datasets (SWaT and WADI) and proved to be better performing than conventional models, e.g., Isolation Forest and static GANs. SAD-GAN has an accuracy of 97.2, and the false positive was under 2% and identified significant changes in the time of detection and flexibility. These findings validate the efficiency of SAD-GAN to find minute and changing anomalies without a high number of false alarms. The suggested method, in general, provides a flexible, smart, and adaptable algorithm of robust anomaly detection in contemporary CPS systems.},
  archive      = {J_CC},
  author       = {Bhutani, Monica and Dalal, Surjeet and Alhussein, Musaed and Lilhore, Umesh Kumar and Aurangzeb, Khursheed and Hussain, Amir},
  doi          = {10.1007/s12559-025-10483-5},
  journal      = {Cognitive Computation},
  month        = {8},
  number       = {4},
  pages        = {1-19},
  shortjournal = {Cogn. Comput.},
  title        = {SAD-GAN: A novel secure anomaly detection framework for enhancing the resilience of cyber-physical systems},
  volume       = {17},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multi-stage alzheimer’s disease classification using squeeze excitation light channel network with a random lyrebird: Achieving high sensitivity and precision in early detection. <em>CC</em>, <em>17</em>(4), 1-27. (<a href='https://doi.org/10.1007/s12559-025-10485-3'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Alzheimer’s disease poses a substantial challenge due to the desperate need for enhancing early detection methods for this debilitating neurodegenerative condition. Present diagnostic methodologies frequently depend on subjective clinical evaluations and invasive procedures, resulting in delays in diagnosis and commencement of treatment. Therefore, this paper proposes a structured novel Squeeze Excitation Light Channel Network with a Random Lyrebird algorithm for Alzheimer’s sisease classification comprising six fundamental stages, each integral to the overall process. Initially, the focus is on data collection to gather pertinent medical images. Subsequently, in the data preparation stage, rigorous preprocessing techniques are applied to ensure data quality and consistency. In the third stage, augmentation techniques are then employed to enrich the dataset to bolster model robustness and enhance generalization capabilities. The fourth stage employs cross-validation to systematically evaluate and refine the classification model across multiple data subsets. Following this, the fifth stage utilizes a modified variation autoencoder for feature extraction, optimizing the representation of critical Alzheimer’s disease-related features. Finally, the sixth stage enables effective Alzheimer’s disease detection by applying a novel proposed algorithm. It aims to advance Alzheimer’s disease diagnosis and prognosis, leveraging cutting-edge techniques to enhance accuracy and reliability in clinical settings. From the analysis, the proposed model demonstrates superior efficiency with an accuracy rate of 98.45%. These results underscore the proposed model’s efficiency in accurately diagnosing Alzheimer’s disease compared to existing methods, highlighting its potential for clinical application and patient care.},
  archive      = {J_CC},
  author       = {Alsowail, Rakan A. and Al-Shehari, Taher},
  doi          = {10.1007/s12559-025-10485-3},
  journal      = {Cognitive Computation},
  month        = {8},
  number       = {4},
  pages        = {1-27},
  shortjournal = {Cogn. Comput.},
  title        = {Multi-stage alzheimer’s disease classification using squeeze excitation light channel network with a random lyrebird: Achieving high sensitivity and precision in early detection},
  volume       = {17},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Quantum-inspired multi-objective seahorse optimizer for predictive maintenance analysis using numerical association rule mining. <em>CC</em>, <em>17</em>(4), 1-45. (<a href='https://doi.org/10.1007/s12559-025-10486-2'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this study, we developed a revolutionary algorithm called Quantum-inspired Multi-Objective Seahorse Optimizer (MOQSHO) for tackling Numerical Association Rule Mining (NARM) problem, which is a particular case of Association Rule Mining (ARM). The challenge inherent in NARM can be approached along three distinct axes: distribution, discretization, and optimization. Traditional single-objective SHO, a bio-inspired metaheuristic optimization method, has shown competitive performance in many complex problems. Still, it needs to tackle multiple objectives of the NARM problem and tends to be stuck in local optima. We propose an optimization-based MOQSHO algorithm to overcome SHO deficiencies by integrating the quantum mechanics in traditional SHO and enhancing its ability to simultaneously handle multiple objectives of the NARM problem. Quantum mechanics helps to improve traditional SHO algorithms’ exploitation and exploration abilities. To evaluate the performance of the MOQSHO algorithm, we undertook a series of tests, using measures such as generational distance (GD), inverse generational distance (IGD), hypervolume (HV), spacing, spread, and $$\Delta _p$$ . This experimental evaluation conclusively demonstrates that the MOQSHO method generates significantly superior results. In the next step, we validated the performance of the MOQSHO on the real-world biparty multi-objective UAV path planning (BP-UAVPP) problem, where it showed good results over the baseline multi-objective techniques. Finally, we extended our investigation by applying the MOQSHO algorithm to the NARM problem and comparing it with other innovative and successful algorithms. The results of this comparison reveal that the proposed MOQSHO algorithm significantly outperforms its counterparts, demonstrating its exceptional efficiency and relevance in the context of numerical association rule mining.},
  archive      = {J_CC},
  author       = {Yacoubi, Salma and Manita, Ghaith and Chhabra, Amit and Oliva, Diego and Korbaa, Ouajdi},
  doi          = {10.1007/s12559-025-10486-2},
  journal      = {Cognitive Computation},
  month        = {8},
  number       = {4},
  pages        = {1-45},
  shortjournal = {Cogn. Comput.},
  title        = {Quantum-inspired multi-objective seahorse optimizer for predictive maintenance analysis using numerical association rule mining},
  volume       = {17},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Human stress level detection using hybrid cascaded neuro-fuzzy SpinalNet. <em>CC</em>, <em>17</em>(4), 1-22. (<a href='https://doi.org/10.1007/s12559-025-10471-9'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the current century, most human beings are distressed by stress. The stress impacts the mental and physical health of people and causes a high influence on human health. The stress affects the daily activities of human life like academics and work. Moreover, stress causes illnesses like anxiety, headaches, heart disease, and depression. Hence, it is essential to avoid these kinds of negative issues due to the high level of stress. The earlier stage of stress detection is necessary to reduce the stress level and provide proper treatment to patients. In this paper, the hybrid deep learning (DL) model called cascaded neuro-fuzzy SpinalNet (CNFSNet)–based human stress level detection model is proposed. The min–max normalization–enabled data normalization is the initial process, in which the data is normalized. Moreover, the essential features from the data are augmented to enhance the data sizes. The fuzzy local information cluster means (FLICM) is employed for feature clustering. At last, the detection of stress levels is performed using the CNFSNet. The accuracy, precision, and recall metrics are used to estimate the CNFSNet-based stress level detection model, with the outcomes of 0.9012, 0.906, and 0.902 achieved.},
  archive      = {J_CC},
  author       = {Lakshmi, P. and G., Manoj Kumar and P., Smitha Vas and P. S, Baiju and Chidambaranathan, Senthilnathan},
  doi          = {10.1007/s12559-025-10471-9},
  journal      = {Cognitive Computation},
  month        = {8},
  number       = {4},
  pages        = {1-22},
  shortjournal = {Cogn. Comput.},
  title        = {Human stress level detection using hybrid cascaded neuro-fuzzy SpinalNet},
  volume       = {17},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). SwinTSER: An improved bilingual speech emotion recognition using shift window transformer. <em>CC</em>, <em>17</em>(4), 1-14. (<a href='https://doi.org/10.1007/s12559-025-10484-4'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Emotion recognition from human speech occupies a significant position in Human-Computer Interaction, especially with the recent advancements in Artificial Intelligence and Robotic computing. As the level of interactivity of man–machine increases, intuitive responses that are emotionally based have attracted a lot of research into emotion recognition from speech signals. However, with various machine learning models littering the literature, cross-language efficient speech emotion recognition with extracted features inherent in speech signals with state-of-the-art deep learning techniques, is still posing a serious challenge. In this paper, we proposed a deep learning transformer network based on a shift window for speech emotion recognition using speech corpus from two different languages. Shift Window Transformer (SWT) is based on a hierarchical transformer architecture designed for natural language tasks and has recently become a novel model in computer vision and image processing tasks. The input feature to the model, Mel spectrogram, is extracted from two public speech datasets: Toronto English Emotion Speech (TEES) and EMOVO. Our proposed transformer model achieved a promising result of 98.3%, 64%, and 66% recognition accuracy on TESS, EMOVO, and TESS_EMOVO (hybrid bi-lingual) datasets, respectively, after extensive experiments and parameter optimization. Our performance evaluation revealed that the proposed model yielded an improved result in the recognition of six different emotions from human auditory speech compared to others found in the literature. The study explores the performance of the SWT architecture on cross-language speech emotion recognition and informs future robust and adaptive model development.},
  archive      = {J_CC},
  author       = {Akinpelu, Samson and Viriri, Serestina and Haroon Yousaf, Muhammad},
  doi          = {10.1007/s12559-025-10484-4},
  journal      = {Cognitive Computation},
  month        = {8},
  number       = {4},
  pages        = {1-14},
  shortjournal = {Cogn. Comput.},
  title        = {SwinTSER: An improved bilingual speech emotion recognition using shift window transformer},
  volume       = {17},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Symbolic faces and artificial minds: Evaluating artificial intelligence recognition of punctuation-based face expressions using ChatGPT, claude, and DeepSeek models. <em>CC</em>, <em>17</em>(4), 1-15. (<a href='https://doi.org/10.1007/s12559-025-10487-1'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Many scientists suspect that AI can replace humans in most situations, while others think that AI cannot intuitively replace humans. Emoticons, punctuation-based symbols representing emotions, serve as the focus of this study which investigates the abilities of ChatGPT, Claude, and DeepSeek to identify and interpret facial emotions conveyed through symbolic emoticons. A total of 34 emoticons, ranging from common to rare punctuation combinations, were presented to each model using the prompt: “What do you think:) is?”, without additional context. Model responses were evaluated based on recognition accuracy, emotional interpretation, and explanation coherence. The results showed that all three models successfully identified the majority of emoticons as face expressions, with DeepSeek demonstrating the highest accuracy, followed by ChatGPT, and Claude. Distinct interpretive styles were observed: ChatGPT gave concise, function-oriented responses; Claude offered culturally rich interpretations; and DeepSeek adopted a socially and psychologically analytical tone. Our findings suggest that while language models can recognize faces and basic emotions comparably to humans, they show limitations in detecting nuanced emotional states. We conclude that AI may contribute meaningfully to the study of emotional intelligence and its applications in sentiment analysis, human–computer interaction, and digital communication.},
  archive      = {J_CC},
  author       = {Akdeniz, Gülsüm and Kul, Halil and Demirci, Harun},
  doi          = {10.1007/s12559-025-10487-1},
  journal      = {Cognitive Computation},
  month        = {8},
  number       = {4},
  pages        = {1-15},
  shortjournal = {Cogn. Comput.},
  title        = {Symbolic faces and artificial minds: Evaluating artificial intelligence recognition of punctuation-based face expressions using ChatGPT, claude, and DeepSeek models},
  volume       = {17},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). New approach to crop disease classification and data security in smart agriculture networks. <em>CC</em>, <em>17</em>(4), 1-29. (<a href='https://doi.org/10.1007/s12559-025-10482-6'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Agriculture is the backbone of the economy which plays a significant role in the economic life cycle. The traditional secure authentication-based methods are affected by challenging tasks such as low reliability, security deployment, and high monitoring costs. To overcome these challenges, the Smart and Secured Agriculture Crop Management Network model is proposed in this research article. This proposed architecture is created based on different layers such as crop, edge, blockchain, network, and application layers. The crop images are collected from the agricultural farm in the crop layer and the edge layer is used to collect the crop images for further processes. These collected crop images are preprocessed to enhance image quality by implementing imputation, normalization, and data-cleaning processes. The features present in these preprocessed images are extracted by using the MobileNetV2 model that enhances computation complexity. The Kohonen Learning Dense Attention-based transformer model is proposed to categorize the different kinds of crop diseases. The blockchain is applied for securing stored data and the smart contract is introduced in this blockchain layer for storing data in an Interplanetary File System that handles high data storage costs. 6G network interface is used as a network layer and the application layer accelerates scientific discovery and crop productivity by minimizing environmental impacts. To validate the system, classification-based analysis was conducted to assess disease detection accuracy, latency, and processing delays, while security-based analysis evaluated the robustness of blockchain integration, including metrics such as security level and node communication time. The experimental evaluation of the proposed model attained performance values of 98.86%, 0.1 ms, 28 s, 95.9%, and 51 s from accuracy, average delay, latency, security level, and node communication time respectively. These simulation findings show that our proposed framework attained superior results compared to previous works in smart and secured crop management systems.},
  archive      = {J_CC},
  author       = {R, Meenakshiammal and R, Bharathi and P, Krishna Kumar},
  doi          = {10.1007/s12559-025-10482-6},
  journal      = {Cognitive Computation},
  month        = {8},
  number       = {4},
  pages        = {1-29},
  shortjournal = {Cogn. Comput.},
  title        = {New approach to crop disease classification and data security in smart agriculture networks},
  volume       = {17},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Test-time adaptation of a multi-class object localization and size estimation framework for smart agriculture applications. <em>CC</em>, <em>17</em>(4), 1-20. (<a href='https://doi.org/10.1007/s12559-025-10488-0'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Smart agriculture brings massive amounts of real-time images generated via modern information and communication technology. Promptly providing accurate estimates of fruit/vegetable information, such as location, quantity, and size, is worth studying. Therefore, we focus on exploring a deep learning-based backbone model for heatmap regression to capture the yield information. This singular and lightweight architecture effectively addresses the unified challenge of object counting, location detection, and size estimation for fruits/vegetables. However, when dealing with real-world applications, the data distribution shift would happen in response to the collection of new data. Moreover, some unseen fruits/vegetables often appear during the training process. All of these give rise to the open set recognition (OSR) problem. In such an OSR environment, a test-time domain adaptation approach based on deep learning is proposed for multi-class object localization and size estimation. This is the first attempt at unsupervised domain adaptation for heatmap regression tasks. Furthermore, to overcome the drawback of lacking a public dataset, a new benchmark dataset (including synthetic and real image data) has been created and collected to train, test, and evaluate our approach. Extensive experimental evaluations prove that our approach can achieve accurate predictions in the OSR setting within a single epoch of test-time optimization without altering the training process.},
  archive      = {J_CC},
  author       = {Liu, Zixu and Wu, Qinhao and Chai, Yuan and Yu, Huan},
  doi          = {10.1007/s12559-025-10488-0},
  journal      = {Cognitive Computation},
  month        = {8},
  number       = {4},
  pages        = {1-20},
  shortjournal = {Cogn. Comput.},
  title        = {Test-time adaptation of a multi-class object localization and size estimation framework for smart agriculture applications},
  volume       = {17},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Hierarchical osprey residual attentional soft capsule citrus model (HORASC2M) for citrus plant disease classification. <em>CC</em>, <em>17</em>(4), 1-22. (<a href='https://doi.org/10.1007/s12559-025-10489-z'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Citrus is one crop with significant added value on a global scale. Early disease diagnosis is crucial for citrus fruits, as well as for all agricultural goods, depending on market needs and potential financial losses. Thus, it is essential to use technology methods to identify citrus illnesses in their early stages, as well as physically damaged citrus fruits. Deep learning techniques have been used to address the challenge of diagnosing diseases of citrus fruits and leaves, as they have lately demonstrated promising results in a number of artificial intelligence applications. This study aims to discover and classify blackspot, canker, greening, and healthy categories, which are frequently encountered in diverse locations. First, many images from the citrus leaves are pre-processed for this purpose. An enhanced Weighted Nuclear Norm Minimization denoising model is used to remove noise during this stage. A special architecture based on an attention-based capsule network is then created. To enhance the ability of the proposed classification, the parameters are fine-tuned using the Extended Osprey Optimization (EOO) algorithm. Using the PlantifyDr dataset, the proposed model achieves 99.17% accuracy, highlighting deep learning’s potential to improve citrus plant disease prediction and classification accuracy.},
  archive      = {J_CC},
  author       = {Patil, Varsha Santosh and Bora, Rina Kamalkumar},
  doi          = {10.1007/s12559-025-10489-z},
  journal      = {Cognitive Computation},
  month        = {8},
  number       = {4},
  pages        = {1-22},
  shortjournal = {Cogn. Comput.},
  title        = {Hierarchical osprey residual attentional soft capsule citrus model (HORASC2M) for citrus plant disease classification},
  volume       = {17},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multimodal framework for automatic behavior analysis of children with autism during ADOS-2. <em>CC</em>, <em>17</em>(4), 1-17. (<a href='https://doi.org/10.1007/s12559-025-10481-7'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The rising prevalence of autism spectrum disorder, coupled with limited professional resources, highlights the urgency of developing efficient diagnostic tools. While standardized assessments exist, identifying subtle communication deficits, especially during multimodal interactions, remains time-consuming and prone to human error. To address this, we propose an automated behavior analysis framework that aims to support clinicians by accurately detecting both verbal and non-verbal communication markers. Specifically, we put forth a composite artificial intelligence framework that integrates various deep learning algorithms to analyze information from body and hand poses, object detection, tracking and manipulation, and speech. By combining these features with a rule-based system, we can identify events within the Autism Diagnostic Observation Schedule second edition, Construction Task, where participants initiate requests. These requests can be verbal, non-verbal or a combination of both resulting in multimodal interactions. Building on our prior work, this paper introduces a smart glass technology component, integrating gaze and blinking analysis, which are challenging for clinicians to monitor, given the multi-task nature of their role. These additions enable the detection of eye contact, a crucial social cue. Our approach allows us to recognize gestures, identify hand object manipulations, detect eye contact, and understand the natural language in clinician-participant interactions. We achieve 94% and 73% F-1 score, on verbal and non-verbal request detection, respectively, which may improve, as deep learning advances.},
  archive      = {J_CC},
  author       = {Dos Santos Melício, Bruno Carlos and Karaköse, Kaan and Fodor, Ádám and Xiang, Linyun and Varga, Viktor and Soorya, Latha and Dillon, Emily and Kun, Péter and Sárkány, András and Chetouani, Mohamed and Fenech, Kristian and Lőrincz, András},
  doi          = {10.1007/s12559-025-10481-7},
  journal      = {Cognitive Computation},
  month        = {8},
  number       = {4},
  pages        = {1-17},
  shortjournal = {Cogn. Comput.},
  title        = {Multimodal framework for automatic behavior analysis of children with autism during ADOS-2},
  volume       = {17},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Prescribed-time-based adaptive optimal control for nonlinear systems with error constraint. <em>CC</em>, <em>17</em>(4), 1-17. (<a href='https://doi.org/10.1007/s12559-025-10490-6'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper concentrates on the problem of adaptive optimal prescribed-time prescribed performance control based on the reinforcement learning algorithm in a pioneering way for nonlinear systems with error constraints. After developing the prescribed-time performance function and the transformation error function, together with the barrier function, a neural-based adaptive transformation error-constrained optimal controller is derived by employing the fuzzy approximation and the optimal backstepping technique. Compared with the traditional prescribed performance control, the constructed performance function is predetermined based on the prescribed time that remains independent of initial conditions and controller design parameters. Moreover, the transformation error constraint can be handled in the control structure through a barrier function, which restricts the transformation error while further enhancing the tracking error convergence performance. Under the proposed controller, it is proven that all signals within the closed-loop system remain bounded, and the transformation error constraint is achieved. This means that the output tracking error converges to a prescribed, arbitrarily small region within a prescribed time interval, while the convergence time is not influenced by the initial state. Simulation results on the numerical example and the single-link robot manipulator system demonstrate the merits of the proposed control scheme.},
  archive      = {J_CC},
  author       = {Qin, Yan and Liu, Yang and Pan, Yingnan and Cao, Liang and Lin, Guohuai},
  doi          = {10.1007/s12559-025-10490-6},
  journal      = {Cognitive Computation},
  month        = {8},
  number       = {4},
  pages        = {1-17},
  shortjournal = {Cogn. Comput.},
  title        = {Prescribed-time-based adaptive optimal control for nonlinear systems with error constraint},
  volume       = {17},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). The effect of toxin on a diffusive zooplankton-phytoplankton model. <em>CC</em>, <em>17</em>(4), 1-20. (<a href='https://doi.org/10.1007/s12559-025-10493-3'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Toxicants increasingly influence phytoplankton and zooplankton dynamics in marine ecosystems, driving spatial and temporal patterns critical to understanding plankton blooms and toxin distributions. This study investigates a reaction-diffusion model of phytoplankton-zooplankton interactions with a toxicant-taxis term, where zooplankton migrate away from high toxin concentrations, addressing the ecological problem of toxicant-induced population instability. We prove the existence of globally bounded solutions, ensuring the model’s biological relevance by preventing unrealistic population blow-up. Linear stability analysis determines the stability of the coexistence equilibrium, identifying critical thresholds for steady-state and Hopf bifurcations via the toxicant-taxis coefficient. Bifurcation analysis reveals conditions for spatial pattern formation and oscillatory dynamics, validated by numerical simulations on a one-dimensional domain. These findings underscore the role of toxicant-taxis in destabilizing uniform plankton distributions, offering insights into spatial heterogeneity in marine environments.},
  archive      = {J_CC},
  author       = {Wang, Yi and Zhang, Xuebing and Moussaoui, Ali},
  doi          = {10.1007/s12559-025-10493-3},
  journal      = {Cognitive Computation},
  month        = {8},
  number       = {4},
  pages        = {1-20},
  shortjournal = {Cogn. Comput.},
  title        = {The effect of toxin on a diffusive zooplankton-phytoplankton model},
  volume       = {17},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Advanced decision-making strategies in life 3.0 through q-spherical fuzzy rough dombi aggregation operators. <em>CC</em>, <em>17</em>(3), 1-38. (<a href='https://doi.org/10.1007/s12559-025-10434-0'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the era of Life 3.0, as envisioned by Max Tegmark, artificial intelligence has evolved to a point where it can autonomously improve and redesign its capabilities. This transformation necessitates sophisticated decision-making frameworks capable of handling ambiguity, uncertainty, and complex human values. This paper aims to develop a robust decision-making model for advanced artificial intelligence systems, particularly suitable for scenarios involving conflicting objectives and ethical considerations, such as artificial intelligence governance. To achieve this, we propose integrating q-spherical fuzzy sets, rough set theory, and Dombi geometric operators. q-Spherical fuzzy sets provide a nuanced representation of uncertainty, while rough set theory addresses vagueness. Dombi geometric operators facilitate effective information aggregation. This study introduces two new aggregation operators: q-spherical fuzzy rough Dombi weighted geometric and q-spherical fuzzy rough Dombi ordered weighted geometric. These operators are applied in multiple attribute decision-making scenarios using q-spherical fuzzy rough data to generate insightful results. Through detailed theoretical analysis and illustrative case studies, we demonstrate how this novel approach can improve the accuracy, reliability, and ethical alignment of artificial intelligence decisions, contributing to a more human-centric artificial intelligence in the age of Life 3.0. Comparative and sensitivity analyses further validate the effectiveness and robustness of our proposed approach. This work offers new perspectives on gathering and understanding q-spherical fuzzy rough data, expanding the current knowledge base and providing a more flexible, sensitive, and long-lasting solution to the challenges of artificial intelligence and human coexistence.},
  archive      = {J_CC},
  author       = {Azim, Ahmad Bin and Alballa, Tmader and Alhabeeb, Somayah Abdualziz and Albely, Maryam Sulaiman and Khalifa, Hamiden Abd El-Wahed},
  doi          = {10.1007/s12559-025-10434-0},
  journal      = {Cognitive Computation},
  month        = {6},
  number       = {3},
  pages        = {1-38},
  shortjournal = {Cogn. Comput.},
  title        = {Advanced decision-making strategies in life 3.0 through q-spherical fuzzy rough dombi aggregation operators},
  volume       = {17},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Topologically consistent prototype network for incomplete multimodal learning. <em>CC</em>, <em>17</em>(3), 1-18. (<a href='https://doi.org/10.1007/s12559-025-10448-8'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multimodal learning aims to utilize the complementary information of multiple data modalities to improve the generalization performance. However, incomplete observations of multimodal data often hinder the learning of complementary information due to missing modalities. While many approaches have been proposed to address the incompleteness of test data, few methods can flexibly handle arbitrary missing modalities in both training and testing phases. In this paper, we propose a general framework called topologically consistent prototype network (TCPN) for effective and flexible solution of multimodal learning with arbitrarily missing modalities, particularly in scenarios involving two or more distinct modalities. The proposed framework establishes cross-modal associations based on the fundamental assumption of topological consistency, which posits that geometric relationships between samples and prototypes are preserved across modalities. Additionally, it introduces prototype networks to facilitate missing modal inference by employing a linear combination of modal prototypes and topological weights. For classification, multiple classifiers based on prototype learning are constructed and trained in a multi-task framework to improve feature representation and classification accuracy. Experimental results on multiple multimodal datasets demonstrate the superiority of the proposed framework over existing methods, particularly in small sample learning and imbalanced learning scenarios.},
  archive      = {J_CC},
  author       = {Wang, Yang and Zhang, Xu-Yao and Liu, Cheng-Lin},
  doi          = {10.1007/s12559-025-10448-8},
  journal      = {Cognitive Computation},
  month        = {6},
  number       = {3},
  pages        = {1-18},
  shortjournal = {Cogn. Comput.},
  title        = {Topologically consistent prototype network for incomplete multimodal learning},
  volume       = {17},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Free-head gaze estimation with deep learning. <em>CC</em>, <em>17</em>(3), 1-11. (<a href='https://doi.org/10.1007/s12559-025-10452-y'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Eye tracking is an essential tool for studying human mental activity and behavior. Appearance-based eye tracking has broad application prospects in actual unconstrained scenes, especially in the field of human-computer interaction. However, existing gaze estimation methods are less robust when the head pose changes. In order to improve the accuracy and real-time performance of eye tracking on the device, we propose a novel multi-input free head pose eye-tracking network model with the name FreeGazeNet, which uses binocular images and face images as inputs to estimate the gaze and the head posture, and designs a new loss function to adapt to model training. We use this method to evaluate the eye-tracking data sets MPIIFaceGaze and GazeCapture, and compared with the latest existing methods, the accuracy is improved by 3.4% and 10.0%, respectively. In addition, we also propose a simple post-processing method for individual gaze calibration to obtain more accurate estimates for different users. The experimental results prove that our proposed method can obtain higher accuracy in gaze estimation and has a greater value in practical applications.},
  archive      = {J_CC},
  author       = {Duan, Feng and Song, Zhuochao and Chen, Xuyi and Yin, Shifan and Sun, Zhe and Liang, Bing and Guo, Yisha},
  doi          = {10.1007/s12559-025-10452-y},
  journal      = {Cognitive Computation},
  month        = {6},
  number       = {3},
  pages        = {1-11},
  shortjournal = {Cogn. Comput.},
  title        = {Free-head gaze estimation with deep learning},
  volume       = {17},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Residual movement metrology network and transformer-based framework for evaluating movement quality. <em>CC</em>, <em>17</em>(3), 1-12. (<a href='https://doi.org/10.1007/s12559-025-10453-x'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {An effective rehabilitation program can significantly hasten the recovery of patients. It promotes the metabolism of damaged tissues and aids in the seamless integration of morphology, function, and structure. With the emergence of sophisticated computer vision technologies, rehabilitation training has become more feasible. Depth imaging devices or traditional video sensors can collect movement data, which can then be automatically evaluated to provide quantitative feedback. The movement quality assessment (MQA) problem has been a subject of prior research. This paper presents a novel methodology that includes three innovative skeletal data augmentation techniques and an effective scoring model, named the Residual Movement Metric Network (R2MN). The data generated through these proposed techniques led to a substantial improvement in results. Defining MQA scores is crucial for solving the MQA problem. The proposed scoring model can easily assess the quality of an action and provide relevant metrics. Compared to the existing scoring system, the proposed model shows an 11.4% improvement in prediction results. Finally, a new transformer-based MQA architecture is put forward. In contrast to existing methods, the prediction of movement quality scores on the UI—PRMD dataset is enhanced by 34%, and on the KIMORE, dataset is enhanced by 34.5%.},
  archive      = {J_CC},
  author       = {Meng, WeiQing and Chen, Peng and Zhang, Jun and Xia, Yi and Wang, Bing},
  doi          = {10.1007/s12559-025-10453-x},
  journal      = {Cognitive Computation},
  month        = {6},
  number       = {3},
  pages        = {1-12},
  shortjournal = {Cogn. Comput.},
  title        = {Residual movement metrology network and transformer-based framework for evaluating movement quality},
  volume       = {17},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Joint structural balance feature representation on graph convolution networks for link prediction. <em>CC</em>, <em>17</em>(3), 1-22. (<a href='https://doi.org/10.1007/s12559-025-10454-w'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Link prediction identifies missing or future connections in networks. Existing methods often neglect varying neighbor contributions and lack mechanisms to reclassify networks based on these differences. To address these issues, a link prediction algorithm by jointing structural balance feature representation on graph convolution network (GCN) is proposed. GCN intersects with cognitive computing, which simulates human cognitive functions such as learning and decision-making. Specifically, the algorithm begins by calculating node similarities based on their attribute features. It then applies interactive importance filtering within the neighbor set to partition a typical network into positive and negative subnetworks, effectively creating a specialized signed network. Building upon this, the algorithm maintains a balanced structure in the divided signed network while employing signed GCN to extract both positive and negative neighborhood features of nodes. Concurrently, it filters out weak interaction node features. Subsequently, node feature representations are transformed into corresponding edge feature representations. Finally, these edge features are processed through a multi-layer perceptron to yield sign prediction results. The algorithm achieves strong performance on real datasets (e.g., citation/social networks), particularly in preserving structural balance and modeling neighbor contributions. This work bridges structural balance theory and GCNs, enhancing link prediction accuracy with interpretable neighbor interaction analysis. Future directions include scalability improvements for larger networks.},
  archive      = {J_CC},
  author       = {He, Meixia and Chen, Jianrui and Wang, Zhihui and Shao, Zhongshi},
  doi          = {10.1007/s12559-025-10454-w},
  journal      = {Cognitive Computation},
  month        = {6},
  number       = {3},
  pages        = {1-22},
  shortjournal = {Cogn. Comput.},
  title        = {Joint structural balance feature representation on graph convolution networks for link prediction},
  volume       = {17},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multigate long short-term memory-based stress detection from multimodal signal. <em>CC</em>, <em>17</em>(3), 1-25. (<a href='https://doi.org/10.1007/s12559-025-10444-y'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Stress is a pensive problem across the globe, and this issue has enormous impacts on mental and physical health. Owing to long-term exposure to stress, serious health problems may arise. Therefore, its timely detection is supportive for controlling stressful periods. Accordingly, various multimodal (MM) signal-based approaches are expansively explored, as stress seriously impacts the structure and functioning of the brain. These techniques developed for detecting stress require enhancement regarding reliability and prediction accuracy. Therefore, this research presents a new model termed multigate long short-term memory (LSTM) for stress detection utilizing MM signal. Initially, MM signals are obtained to process stress detection. The considered MM signals are acquired from four sensors namely a temperature sensor, electrodermal activity (EDA) sensor, photoplethysmograph (PPG) sensor, and average heart rate (HR) as well as a 3-axis accelerometer sensor. After that, features from four sensors are individually extracted to obtain output- 1, output- 2, output- 3, and output- 4. Lastly, stress detection is performed utilizing Multigate-LSTM by taking four feature-extracted outputs. In addition, Multigate-LSTM obtained minimum values of mean absolute percentage error (MAPE) of about 0.172, mean square error (MSE) of about 0.082, and root mean square error (RMSE) of about 0.286.},
  archive      = {J_CC},
  author       = {M., Meenalakshmi and K., Valarmathi},
  doi          = {10.1007/s12559-025-10444-y},
  journal      = {Cognitive Computation},
  month        = {6},
  number       = {3},
  pages        = {1-25},
  shortjournal = {Cogn. Comput.},
  title        = {Multigate long short-term memory-based stress detection from multimodal signal},
  volume       = {17},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Enhancing skin disease diagnosis: Light GBM-DMS algorithm for accurate image classification. <em>CC</em>, <em>17</em>(3), 1-18. (<a href='https://doi.org/10.1007/s12559-025-10450-0'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Globally, skin diseases are emerging as the most common health problem. It initiates depressive disorder, and it also causes physical health distress. It rarely led to skin cancer in extreme cases. Diagnosing skin disease is subjective and time-consuming when it is performed by health experts manually. Hence, automatic skin disease prediction is required by both dermatologists and patients to enhance and accelerate treatment plans. Even though many machine-learning techniques are available to detect skin diseases, the timely prediction and accurate classification of skin lesions are still in demand. Hence, the Discrete Mycorrhizal Search Algorithm is used to optimize the novel LightGBM to meet this objective. Firstly, intrusions like noise elimination and hair removal are performed and then the image resizing takes place by utilizing the preprocessing pipelines. Subsequent to data pre-processing, the prime features are extracted using the convolutional self-attention based L2 regularization, thus minimizing the data complexity. At last, the extracted features are shifted to the final categorization phase which is done by the novel approach for effective classification of skin diseases. Two skin disease classification images, including datasets, the Skin Lesions Dermatoscopic Image dataset and Skin Cancer MNIST HAM10000, are selected to validate the classification performance. Comprehensive analyses are performed with significant measures that showed the proposed model attained better performances of 98.6%, 0.95, and 10.43 s from accuracy, intersection of union, and execution time, respectively. The stability and robustness under different environmental conditions for clinical applications in skin disease detection and classification tasks are analyzed for making decisions in real-time scenarios.},
  archive      = {J_CC},
  author       = {Kalpana, B. and Senthilselvi, A. and Muruganandam, S. and Kumar, S. Vinodh},
  doi          = {10.1007/s12559-025-10450-0},
  journal      = {Cognitive Computation},
  month        = {6},
  number       = {3},
  pages        = {1-18},
  shortjournal = {Cogn. Comput.},
  title        = {Enhancing skin disease diagnosis: Light GBM-DMS algorithm for accurate image classification},
  volume       = {17},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). OBoctNet: Enhancing ophthalmic biomarker detection through active learning and explainable AI in radiological analysis. <em>CC</em>, <em>17</em>(3), 1-18. (<a href='https://doi.org/10.1007/s12559-025-10451-z'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Optical coherence tomography (OCT) scanning is crucial for the diagnosis of widespread ophthalmic diseases. Traditionally, experts manually identify diseases and biomarkers from OCT scans. Recently, modern medical imaging practices have increasingly utilized deep learning techniques to speed up and improve diagnostic accuracy in ophthalmology. However, obtaining accurately labeled datasets is a significant challenge in medical imaging due to the expertise required for precise annotation by trained professionals. This article presents a novel method, named OBoctNet, with a new two-stage training strategy to improve the identification of ophthalmic biomarkers using OCT scans from the OLIVES dataset, which contains only 12% labeled data. This approach leverages a robust methodology that effectively uses labeled and unlabeled data to enhance biomarker identification accuracy, achieving a cumulative performance increase of 23% across 50% of the biomarkers when compared to the previous studies. To better identify biomarkers, the OBoctNet employs an active learning strategy that uses unlabeled data and dynamically ensembles models based on their performance within each experimental setup. Additionally, the usage of Gradient weighted Class Activation Mapping (GradCAM) helps identify regions of interest associated with relevant biomarkers, enhancing interpretability and transparency for potential clinical adoption.},
  archive      = {J_CC},
  author       = {Acharja, Samya and Hasan, Md. Zahid and Chamok, Farjana Haque and Fahim, Kayes Uddin and Shuva, Taslima Ferdaus and Bulbul, Abdullah Al-Mamun and Khan, Risala Tasin and Rahman, Md. Tanvir and Kaiser, M. Shamim and Mahmud, Mufti and Moni, Mohammad Ali},
  doi          = {10.1007/s12559-025-10451-z},
  journal      = {Cognitive Computation},
  month        = {6},
  number       = {3},
  pages        = {1-18},
  shortjournal = {Cogn. Comput.},
  title        = {OBoctNet: Enhancing ophthalmic biomarker detection through active learning and explainable AI in radiological analysis},
  volume       = {17},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). HSBNN: A high-scalable bayesian neural networks accelerator based on field programmable gate arrays (FPGA). <em>CC</em>, <em>17</em>(3), 1-14. (<a href='https://doi.org/10.1007/s12559-025-10455-9'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Traditional artificial neural networks have inherent overfitting problems and tend to produce overly confident predictions due to their reliance on point estimation methods. In contrast, Bayesian theory offers a probabilistic framework that replaces point estimation with probability distributions, effectively addressing issues of overconfidence. The brain is also believed working under the Bayesian rules, the neural networks of which evaluate the precision of prior knowledge and incoming evidence, achieving the balance of weight updating to the most reliable information sources [1]. By integrating Bayesian principles with artificial neural networks, the bio-inspired Bayesian Neural Networks (BNNs) can generate predictions accompanied by confidence evaluations, enhancing their practical applicability. To further improve the computational efficiency of BNNs and enable scalable deployment on edge devices, we propose a High-Scalable Bayesian Neural Network (HSBNN) accelerator based on field-programmable gate arrays (FPGAs) with multiple optimizations. A resource-saving Gaussian random number generator (RS-GRNG) optimized for FPGAs shows high efficiency, which seamlessly extends to support parallel sampling of weight distributions, enabling reliable confidence probability evaluations. Furthermore, the parameterization of BNN architectures with configuration files and employment of a layer-by-layer computing mode ensure that different BNNs can be accelerated without reprogramming the FPGA, offering excellent scalability. The entire system, implemented with the OpenCL heterogeneous computing library, leverages parallel processing units and pipeline channels to achieve high acceleration performance and efficient data transfer. The experiment results demonstrate that the system achieves a data processing throughput of 1.002 milliseconds per image, exceeding CPU performance by 1000-fold and GPU performance by nearly 500-fold.},
  archive      = {J_CC},
  author       = {Liu, Yinghao and Zhang, Hao and Sun, Zhe and Duan, Feng and Ma, Yuan and Lu, Wenyi and Caiafa, Cesar F. and Solé-Casals, Jordi},
  doi          = {10.1007/s12559-025-10455-9},
  journal      = {Cognitive Computation},
  month        = {6},
  number       = {3},
  pages        = {1-14},
  shortjournal = {Cogn. Comput.},
  title        = {HSBNN: A high-scalable bayesian neural networks accelerator based on field programmable gate arrays (FPGA)},
  volume       = {17},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Decision-making-based solar panel selection: Sugeno-weber operators and fermatean fuzzy distance measure with AROMAN methodology. <em>CC</em>, <em>17</em>(3), 1-28. (<a href='https://doi.org/10.1007/s12559-025-10456-8'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Solar energy provides substantial benefits as compared to the fossil fuels as being renewable, reducing greenhouse gas emissions, and providing decentralized power systems. The government of India is implementing several policy measures to increase the production of solar energy with a strategic importance on solar power to fulfil the growing electricity requirements. Choosing a suitable solar panel with longer assurances is an important and critical issue that ensures longevity and reliability. It is a significant concern that requires to be thoroughly analyzed for producing solar power efficiently by means of several key dimensions viz. mechanical, financial, electrical, and customer characteristics. This study aims to develop a hybrid multi-criteria group decision-making framework for addressing the solar panel selection problem, including 5 solar panel options over 12 criteria from 4 aspects. The proposed approach incorporates the Sugeno-Weber-weighted aggregation operators, standard deviation (SD)-based model, rank sum (RS) model, and the alternative ranking order method accounting for two-step normalization (AROMAN) under the context of Fermatean fuzzy sets. To illustrate the effectiveness of introduced approach, it is employed to a case study of solar panel selection problem, followed by sensitivity and comparative analyzes. The results show that the “Monocrystalline PERC solar panel” has the maximum preference among the others in the process of solar panel selection problem. This study provides practical insights for policymakers by offering a systematic technique for evaluating the solar panels based on different criteria and uncertain information.},
  archive      = {J_CC},
  author       = {Rani, Pratibha and Mishra, Arunodaya Raj and Pamucar, Dragan and Alshamrani, Ahmad M. and Alrasheedi, Adel Fahad and Simic, Vladimir},
  doi          = {10.1007/s12559-025-10456-8},
  journal      = {Cognitive Computation},
  month        = {6},
  number       = {3},
  pages        = {1-28},
  shortjournal = {Cogn. Comput.},
  title        = {Decision-making-based solar panel selection: Sugeno-weber operators and fermatean fuzzy distance measure with AROMAN methodology},
  volume       = {17},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Peeping into the future: Understanding and combating generative AI-based fake news. <em>CC</em>, <em>17</em>(3), 1-33. (<a href='https://doi.org/10.1007/s12559-025-10457-7'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The widespread dissemination of fake news in the digital age, accelerated by generative artificial intelligence (GAI), presents a significant challenge to the integrity of information in our interconnected world. This review paper comprehensively analyzes the critical concerns surrounding GAI-generated fake news, including its origin, distribution, societal impact, and the complex challenges associated with its detection. The study explores various techniques GAI systems employ to create misleading content, ranging from textual misinformation to deepfake media, highlighting the alarming scope of fake news proliferation. Additionally, this paper examines the difficulties in detecting fake news through natural language processing, image analysis, and audio analysis, discussing both their advancements and inherent limitations. It also addresses ethical concerns tied to detection strategies, such as privacy violations and the potential erosion of public trust. Furthermore, it identifies a crucial gap in current research: the urgent demand for innovative and scalable solutions to combat the overwhelming surge of fake news in the digital ecosystem. Addressing this challenge is essential in mitigating the impact of GAI-generated fake news. One of the most pressing obstacles in the fight against misinformation today is managing the sheer volume of online fabricated content. The rapid and widespread dissemination of such content emphasizes the need for proactive strategies to curtail its influence before it inflicts significant harm. This evident knowledge gap highlights the necessity for continued research and innovation to strengthen digital security and enhance trustworthiness in online spaces.},
  archive      = {J_CC},
  author       = {Kumar, Sanjeev and Sai, Siva and Chamola, Vinay and Gaur, Aanchal and Agarwal, Chitwan and Huang, Kaizhu and Hussain, Amir},
  doi          = {10.1007/s12559-025-10457-7},
  journal      = {Cognitive Computation},
  month        = {6},
  number       = {3},
  pages        = {1-33},
  shortjournal = {Cogn. Comput.},
  title        = {Peeping into the future: Understanding and combating generative AI-based fake news},
  volume       = {17},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Application of explainable artificial intelligence in autism spectrum disorder detection. <em>CC</em>, <em>17</em>(3), 1-26. (<a href='https://doi.org/10.1007/s12559-025-10462-w'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Autism spectrum disorder (ASD) is a developmental disorder typically diagnosed in early childhood. With the advent of machine learning (ML) and deep learning (DL) models, accurate diagnosis of ASD has been enhanced. However, the widespread adoption of these AI models in real-life scenarios has been limited due to their “black box” nature, which lacks transparency and interpretability. To address this, eXplainable Artificial Intelligence (XAI) models have gained popularity, offering more transparent and interpretable detection methods. This review systematically explores XAI frameworks and underlying AI models by addressing four critical research questions (RQs). Relevant research outputs were selected using the Preferred Reporting Items for Systematic Reviews and Meta-Analyses (PRISMA) approach from five major databases: IEEE, PubMed, Springer, ScienceDirect and ACM. From an initial pool of 1551 articles, 38 studies were identified that focused on learning models and XAI in ASD prediction. These studies were critically analysed across six modalities, twenty classifiers, and five XAI frameworks. The selected studies demonstrate the application of various XAI frameworks in enhancing the transparency and interpretability of AI models used for ASD prediction. The review highlights the benefits of XAI in improving model trustworthiness and adoption, while identifying challenges, such as the trade-off between interpretability and model performance. This review provides a comprehensive overview of the current state of the art of XAI in ASD prediction, identifying key benefits, challenges, and future research avenues. The insights gained from this review could guide researchers in further developing XAI frameworks that balance interpretability and predictive accuracy, thereby facilitating broader adoption in clinical practice.},
  archive      = {J_CC},
  author       = {Vimbi, Viswan and Shaffi, Noushath and Sadiq, Mohamed A. K. and Sirasanagandla, Srinivasa Rao and Aradhya, V. N. Manjunath and Kaiser, M. Shamim and Wang, Shuqiang and Mahmud, Mufti},
  doi          = {10.1007/s12559-025-10462-w},
  journal      = {Cognitive Computation},
  month        = {6},
  number       = {3},
  pages        = {1-26},
  shortjournal = {Cogn. Comput.},
  title        = {Application of explainable artificial intelligence in autism spectrum disorder detection},
  volume       = {17},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Exploring cognitive difference in poetry collection via large language models and metaphors: A case study of the book of songs. <em>CC</em>, <em>17</em>(3), 1-15. (<a href='https://doi.org/10.1007/s12559-025-10466-6'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Both modern and classical poetry collections consist of poems with various contents and styles. Each poem is imbued with rich metaphors, which reflect the emotions, thoughts, and cultural backgrounds of the poets. Previously, researchers have analyzed various poetry collections, but do it manually in most cases and focused on representative works within the collections, lacking a framework for large-scale systematic analysis. Metaphor analysis is an essential tool for reflecting language concept mapping and facilitating the comprehension of complex ideas. Recently, it has been widely applied in studies across politics, society, economics, and literature. With the advancement of large language models, metaphor analysis’ application scope and capabilities have been enhanced. In this study, we propose a framework “Poetic Insight” and conduct a comprehensive analysis of all 305 poems in the earliest Chinese poetry anthology, the “Book of Songs,” utilizing multiple commercial and open-source large language models. Through detailed analysis of semantic similarities and metaphorical expressions, we unveil the differences and connections among the poems, providing supporting evidence for the historical origins of several pieces, and offering insights for humanities scholars to leverage digital humanities methods in the future.},
  archive      = {J_CC},
  author       = {Bao, Hui and He, Kai and Wang, Yige and Gao, Zeyu},
  doi          = {10.1007/s12559-025-10466-6},
  journal      = {Cognitive Computation},
  month        = {6},
  number       = {3},
  pages        = {1-15},
  shortjournal = {Cogn. Comput.},
  title        = {Exploring cognitive difference in poetry collection via large language models and metaphors: A case study of the book of songs},
  volume       = {17},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Revisiting a simple MLP framework for Z-axis rotation-invariant point cloud place recognition. <em>CC</em>, <em>17</em>(3), 1-16. (<a href='https://doi.org/10.1007/s12559-025-10467-5'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The objective of point cloud place recognition is to convert a point cloud into a global descriptor that can be utilized in autonomous driving applications to identify the best-matched road scene from an extensive dataset. However, capturing a point cloud from an arbitrary view by robots or self-driving vehicles often involves scene rotations, making existing deep learning-based methods susceptible to errors. To quantify this performance degradation, we introduce a novel metric: Average Recall@N under arbitrary rotations, denoted as “R-AR@N.” To address this issue, we propose a geometrical transformation module designed to convert rotation-sensitive coordinates into rotation-invariant representations. Additionally, we observe that the design of overly complex networks may not be crucial for effective point cloud analysis. In line with the straightforward architectural design of PointMLP [20], we introduce a local feature transformation module that utilizes statistical representations to transform local point features within a reasonable range. This enables the network to capture diverse geometric structures and generate a robust global descriptor. Our proposed method undergoes extensive evaluation on the Oxford outdoor dataset and three in-house datasets, demonstrating an improvement of at least 2% over previous methods on the newly proposed “R-AR@N” metric. Our code is available at https://github.com/jasonwjw/RI-PointMLP .},
  archive      = {J_CC},
  author       = {Wu, Junwei and Liu, Jiejie and Sun, Mingjie and Jiang, Chenru and Smith, Jeremy and Lim, Eng Gee and Zhang, Quan},
  doi          = {10.1007/s12559-025-10467-5},
  journal      = {Cognitive Computation},
  month        = {6},
  number       = {3},
  pages        = {1-16},
  shortjournal = {Cogn. Comput.},
  title        = {Revisiting a simple MLP framework for Z-axis rotation-invariant point cloud place recognition},
  volume       = {17},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Contrastive-based removal of negative information in multimodal emotion analysis. <em>CC</em>, <em>17</em>(3), 1-16. (<a href='https://doi.org/10.1007/s12559-025-10463-9'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multimodal sentiment analysis bridges the communication gap between humans and machines by accurately recognizing human emotions. However, existing approaches often focus on synchronizing multimodal data to enhance accuracy, overlooking the critical role of negative information. Negative information generally refers to noise or inconsistencies with primary emotional labels within the data, such as disparities in emotional expressions across different modalities and noisy data elements. These issues can significantly compromise the effectiveness of sentiment analysis systems. To address this challenge, we propose a novel method based on contrastive learning for the removal of non-relevant features within single modalities, aiming to eliminate negative information in speech, text, and image data. Additionally, we have designed an enhanced multi-head attention mechanism that integrates the cleansed features into a unified representation for emotion analysis. Experimental evaluations on the CMU-MOSI and CMU-MOSEI datasets demonstrate that our method significantly outperforms existing approaches in sentiment analysis tasks. This method not only improves accuracy but also ensures the system’s robustness against the diverse and noisy nature of real-world data. The relevant code is available at https://github.com/YaoYangWang/MECAM .},
  archive      = {J_CC},
  author       = {Wang, Rui and Wang, Yaoyang and Cambria, Erik and Fan, Xuhui and Yu, Xiaohan and Huang, Yao and E, Xiaosong and Zhu, Xianxun},
  doi          = {10.1007/s12559-025-10463-9},
  journal      = {Cognitive Computation},
  month        = {6},
  number       = {3},
  pages        = {1-16},
  shortjournal = {Cogn. Comput.},
  title        = {Contrastive-based removal of negative information in multimodal emotion analysis},
  volume       = {17},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Fuzzy preference completion with ranked and unranked preferences. <em>CC</em>, <em>17</em>(3), 1-21. (<a href='https://doi.org/10.1007/s12559-025-10468-4'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As for social choice, all alternatives are ranked by agents to form preferences as linear orders. However, in applications, sometimes some alternatives cannot be ranked, or it is unnecessary to rank them, which leads to unranked alternatives. Hence, without loss of generality, by dividing the set of alternatives into three ranked and unranked subsets, including top-k alternatives, intermediate-r alternatives, and last-l alternatives, the Mallows model on ranked and unranked preferences can be analyzed systematically. Technically, a repeated insertion model is adopted during sampling, and probability distributions are derived for ranked and unranked preferences of alternatives. Experimental results verify the accuracy of the probability distributions for different ranked and unranked preferences of alternatives. Furthermore, in order to solve the preference completion problem where agents have multiple partial rankings, a fuzzy preference completion algorithm, Fuzzy-Multi-Rankings, is proposed, which introduces a fuzzy ranking to complete the target agent’s preference in addition to the traditional nearest-neighbor-based methods. Based on the three ranked and unranked preferences, seven cases can be classified and analyzed for fuzzy preference completion. Experiments on the synthetic datasets and MovieLens dataset confirm the effectiveness and efficiency of our proposed Fuzzy-Multi-Rankings algorithm and also verify the accuracy of the evaluated probability distributions for the proposed seven cases.},
  archive      = {J_CC},
  author       = {Li, Lei and Liu, Pan and Zhang, Renjie and Tao, Zhenchao and Wu, Xindong},
  doi          = {10.1007/s12559-025-10468-4},
  journal      = {Cognitive Computation},
  month        = {6},
  number       = {3},
  pages        = {1-21},
  shortjournal = {Cogn. Comput.},
  title        = {Fuzzy preference completion with ranked and unranked preferences},
  volume       = {17},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Emotions for everyone: A low-cost, high-accuracy method for emotion classification. <em>CC</em>, <em>17</em>(3), 1-15. (<a href='https://doi.org/10.1007/s12559-025-10458-6'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The classification of emotions is of vital importance in health care, particularly in the context of early detection of cognitive disorders. Given the critical role of emotions as early indicators of cognitive health, this study addresses the need to develop effective and accessible classification methods. In this research, we present an innovative approach to emotion classification using a proprietary dataset and harnessing the power of deep learning. In particular, we use a specific, innovative combination of attentional layers and Long-Short Term Memory (LSTM) algorithms to achieve an emotion classification. A key differentiator of our methodology is the use of a compact and low-cost array of biometric sensors. This approach provides a cost-effective alternative to traditional systems, which often rely on more complex and expensive sensor arrays, such as those using electroencephalography (EEG). Despite the affordability of our sensor configuration, our classification model achieves an outstanding accuracy rate of 93.75%. This performance not only demonstrates the effectiveness of our method but also positions it at the forefront of emotion classification using these sensors. By significantly reducing cost while increasing classification accuracy, our method helps to push the boundaries of current state-of-the-art approaches and provides a novel and cost-effective solution for emotion classification and cognitive health monitoring.},
  archive      = {J_CC},
  author       = {Ajali-Hernández, Nabil I. and Travieso-González, Carlos M.},
  doi          = {10.1007/s12559-025-10458-6},
  journal      = {Cognitive Computation},
  month        = {6},
  number       = {3},
  pages        = {1-15},
  shortjournal = {Cogn. Comput.},
  title        = {Emotions for everyone: A low-cost, high-accuracy method for emotion classification},
  volume       = {17},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). RELUEM-reinforcing emotional understanding: Advancing speech emotion recognition through deep reinforcement learning. <em>CC</em>, <em>17</em>(3), 1-24. (<a href='https://doi.org/10.1007/s12559-025-10459-5'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Speech emotion recognition (SER) is a critical component in domains such as human–computer interaction, mental health monitoring, and affective computing. While existing approaches—particularly those based on deep learning—have made significant progress, they often struggle to accurately capture subtle emotional cues in speech, especially in real-world scenarios. A key gap remains in the adaptability and decision-making capabilities of current models when faced with dynamic and ambiguous emotional expressions. To address this, we propose a novel speech emotion recognition framework that integrates deep reinforcement learning (DRL) with conventional classification models. Unlike static models, our approach enables adaptive learning and optimal decision policies for emotion classification over time. We evaluate our method using the widely adopted RAVDESS benchmark dataset. Experimental results demonstrate a notable improvement in performance, with our method achieving an accuracy of 89.5% and an F1 score of 0.86—outperforming several state-of-the-art baselines. These findings suggest that DRL offers a promising pathway toward more robust and context-aware speech emotion recognition systems.},
  archive      = {J_CC},
  author       = {Zhang, Zhiping and Shamim, Rejuwan and Sinha, Anurag and Jha, Pooja and Alkhayyat, Ahmed},
  doi          = {10.1007/s12559-025-10459-5},
  journal      = {Cognitive Computation},
  month        = {6},
  number       = {3},
  pages        = {1-24},
  shortjournal = {Cogn. Comput.},
  title        = {RELUEM-reinforcing emotional understanding: Advancing speech emotion recognition through deep reinforcement learning},
  volume       = {17},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A novel multi-modality deepfake detection using cross-model Fusion–Based hybrid harris whale depth-wise separable dense convolutional bi-GRU deepfake model. <em>CC</em>, <em>17</em>(3), 1-26. (<a href='https://doi.org/10.1007/s12559-025-10465-7'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deepfake video detection is one of the advanced techniques used to detect deepfakes from videos based on video frames and audio. Many deepfake videos are created for malicious purposes, such as disseminating misinformation on social media platforms. While existing deep learning models have shown effectiveness in classifying deepfakes, they often encounter several challenges, including low performance, poor data quality, and high computational complexity. To address these challenges, this study introduces an efficient multi-modal deepfake detection framework that utilizes a cross-model fusion approach based on a hybrid Harris Whale depth-wise convolutional Bi-GRU model. This innovative model incorporates several key components to enhance detection accuracy, such as audio and video pre-processing, feature extraction, feature fusion, and classification. The feature fusion utilizes the cross-model fusion strategy, which enhances the model’s ability to classify deepfake content. The hybrid Harris Whale depth-wise separable dense convolutional Bi-GRU (H2W-DwSDCBi-G) model is utilized for classification. Furthermore, the hybrid Harris Whale optimization (H2WO) technique is implemented to optimize the model’s performance by identifying the most effective weights for the classifier. The proposed model is evaluated on three datasets: VidTIMIT Audio–Video, FakeAVCeleb, and DFDC, achieving impressive accuracies of 98.11%, 97.71%, and 98.59%, respectively.},
  archive      = {J_CC},
  author       = {Venkata Krishna Moorthy, T. and Reddy, Y. Narasimha and Venkataiah, C. and Kumar, Neravati Nagaraja and Prasad, D. Rajendra and Rao, Y. Mallikarjuna and Jayamma, Manjula},
  doi          = {10.1007/s12559-025-10465-7},
  journal      = {Cognitive Computation},
  month        = {6},
  number       = {3},
  pages        = {1-26},
  shortjournal = {Cogn. Comput.},
  title        = {A novel multi-modality deepfake detection using cross-model Fusion–Based hybrid harris whale depth-wise separable dense convolutional bi-GRU deepfake model},
  volume       = {17},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Generative AI and deepfake detection in biometric systems. <em>CC</em>, <em>17</em>(3), 1-21. (<a href='https://doi.org/10.1007/s12559-025-10469-3'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The emergence of generative Artificial Intelligence (AI), particularly Generative Adversarial Networks (GANs) and Variational Autoencoders (VAEs), introduces new challenges for biometric security systems. While AI research has improved methods for biometric authentication, these technologies are now the tools for producing advanced synthetic identities for defeating security measures. This paper provides a thorough survey of the current state of deepfake detection in biometric systems with regard to the evolving nature of generative models, the emerging threat of cross-modal deepfakes, and the dire need for robust multi-layer defense mechanisms. As part of the analysis, the current techniques of detection are evaluated, and their relative strengths and weaknesses are identified. Furthermore, research directions are put forward concerning advanced detection algorithms, making diverse and comprehensive datasets, and privacy-preserving technologies. The ethical use of deepfakes with respect to biometric systems further focuses on the necessity to construct new regulatory frameworks in light of accelerating generative technologies evolvement. It includes the development of sophisticated detection algorithms, multimodal datasets, and privacy-preserving methods to safeguard biometric systems against emerging deepfakes. Finally, this paper outlines future research directions.},
  archive      = {J_CC},
  author       = {Khan, Farrukh Aslam and Khan, Muhammad Khurram},
  doi          = {10.1007/s12559-025-10469-3},
  journal      = {Cognitive Computation},
  month        = {6},
  number       = {3},
  pages        = {1-21},
  shortjournal = {Cogn. Comput.},
  title        = {Generative AI and deepfake detection in biometric systems},
  volume       = {17},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). C3BAM-XAI: Convolutional block attention module enhanced explainable artificial intelligence-based parkinson’s disease stage classification. <em>CC</em>, <em>17</em>(3), 1-19. (<a href='https://doi.org/10.1007/s12559-025-10472-8'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Parkinson’s disease (PD) is a progressive neurodegenerative disorder that causes significant impairment in neurons, physiological structures, and behavior of people. However, these changes are very subtle in the early stages of PD, making diagnosis and treatment challenging. To overcome such challenges, we proposed a novel convolutional neural network (CNN) architecture called C3BAM-XAI that is based on the convolutional block attention module (CBAM) enhanced CNN and explainable artificial intelligence (XAI). In the proposed architecture, the data imbalance issue was resolved at the initial stage by employing augmentation techniques for the training data. After that, the proposed CNN architecture is designed with CBAM. The CBAM architecture is based on two sub-modules—the channel attention module and the spatial attention module. The input of the initial convolutional layer is passed to the channel attention module, which is further passed to the spatial attention module for more critical information extraction. For smooth training, hyperparameters are selected using Nadam Optimization (NO) instead of manual assignment. In the testing phase, features are extracted from the dense layer and passed to several neural network classifiers. In addition to this, the designed model is interpreted using the explainable AI technique to see the predicted label and insight decision features. The experimental process was performed on the publically available PD Kaggle dataset and obtained an improved accuracy of 93.33%. In addition, several ablation studies were conducted, and it is concluded that the CBAM not only improved accuracy but also made this model a reliable tool for clinical diagnosis, both in terms of accuracy and interpretability. The results thus open new avenues toward applying deep learning models practically for applications in the medical domain and early-stage detection of Parkinson’s disease, thereby helping to improve patient outcomes.},
  archive      = {J_CC},
  author       = {Abbas, Muhammad John and Khan, Muhammad Attique and Hamza, Ameer and Alsenan, Shrooq and Rehman, Aleesha and Baili, Jamel and Zhang, Yudong},
  doi          = {10.1007/s12559-025-10472-8},
  journal      = {Cognitive Computation},
  month        = {6},
  number       = {3},
  pages        = {1-19},
  shortjournal = {Cogn. Comput.},
  title        = {C3BAM-XAI: Convolutional block attention module enhanced explainable artificial intelligence-based parkinson’s disease stage classification},
  volume       = {17},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Enhancing the prediction of breast cancer progression through multi-modal data transformation. <em>CC</em>, <em>17</em>(3), 1-18. (<a href='https://doi.org/10.1007/s12559-025-10474-6'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The ability to predict breast cancer metastases is essential for making effective clinical decisions and managing patients. Traditional models predominantly rely on structured clinical data, which often lacks essential contextual details, limiting their predictive accuracy. In order to address this limitation, a multi-modal approach is introduced in which structured data is transformed into unstructured text, while contextual richness is preserved. Using this text, synthetic images are generated across three key diagnostic modalities, histopathology, mammography, and ultrasound, to enhance predictive capabilities. Based on converted text data, a pre-trained diffusion model was used to generate synthetic medical images in histopathology, mammography, and ultrasound modalities. The impact of a variety of text description variants on image quality and metastasis prediction was assessed. Comprehensive tumor descriptions or a combination of histological type and differentiation status were the most effective generation strategies. A comparison was conducted between three prediction approaches: a unimodal approach, an early fusion approach based on concatenation, and the Multi Co-Guided Attention (MCGA) approach. Through mutual attention, MCGA enhances feature alignment by addressing inter- and intra-modal heterogeneity and capturing complex relationships. Unimodal and multi-modal methods were evaluated with the application of SMOTE to mitigate the impact of data imbalance. Multi-modal fusion significantly outperforms unimodal methods, especially when class imbalances are mitigated by using SMOTE. When SMOTE was applied, Ultrasound+BERT achieved the highest level of accuracy (0.90), followed by Histopathology+BERT (0.88), and Mammogram+BERT (0.88). As compared to early fusion, MCGA demonstrated better class balance and improved minority class detection. Incorporating unstructured text with synthetic imaging modalities improves the accuracy of metastasis prediction by preserving contextual information. A MCGA fusion is particularly effective in ensuring balanced class performance, particularly for rectifying class imbalances. Through this approach, the complementary strengths of textual and visual data are leveraged to overcome limitations in multi-modality integration. These results demonstrate the potential for advancing the prediction of breast cancer metastasis, offering a more robust and context-aware framework for clinical decision-making.},
  archive      = {J_CC},
  author       = {Abdullakutty, Faseela and Akbari, Younes and Al-Maadeed, Somaya and Bouridane, Ahmed and Rifat Hamoudi, Rifat},
  doi          = {10.1007/s12559-025-10474-6},
  journal      = {Cognitive Computation},
  month        = {6},
  number       = {3},
  pages        = {1-18},
  shortjournal = {Cogn. Comput.},
  title        = {Enhancing the prediction of breast cancer progression through multi-modal data transformation},
  volume       = {17},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Action plan diversity in children during control exploration: Link between action and sense of agency. <em>CC</em>, <em>17</em>(3), 1-10. (<a href='https://doi.org/10.1007/s12559-025-10475-5'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The sense of agency refers to the subjective feeling of controlling one’s own actions and, through them, external events. Despite actions containing rich information about human subjective feelings, there are very few ways to abstract such information. The present study aims to use a new method of action analysis to examine whether action plans contain information for measuring the sense of agency both across different conditions (i.e., within participants) and among different individuals. The present study employed an action plan analysis utilizing transformer-LSTM-based autoencoders on a movement dataset of 167 children in a control detection task collected previously in a published paper. This analysis can capture high-level, abstract representations of sequences of motor commands (referred to as action plans) and quantify control exploration behaviors. The action plan diversity showed a sigmoid-like function of control, indicating that actions indeed contain rich information regarding the sense of agency. Furthermore, the individual slope of action plan diversity against control, referred to as action plan sensitivity, significantly correlated with individual control detection accuracy, suggesting that this index can also be used as an inter-individual measure of sense of agency. Our results suggest that simply observing how actions change under different control conditions can quantitatively reflect the emergence of the sense of agency in children. The findings and methodology provide a highly novel and useful tool for studying the sense of agency in broader populations and species in future studies.},
  archive      = {J_CC},
  author       = {Wen, Wen and Aktas, Hakan and Chang, Acer Yu-Chan and Mei, Jie and Suzuishi, Yosuke and Nagai, Yukie and Nobusako, Satoshi},
  doi          = {10.1007/s12559-025-10475-5},
  journal      = {Cognitive Computation},
  month        = {6},
  number       = {3},
  pages        = {1-10},
  shortjournal = {Cogn. Comput.},
  title        = {Action plan diversity in children during control exploration: Link between action and sense of agency},
  volume       = {17},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Deep feature disentanglement for supervised contrastive learning: Application to image classification. <em>CC</em>, <em>17</em>(3), 1-12. (<a href='https://doi.org/10.1007/s12559-025-10430-4'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In machine learning, deep metric learning from original data is essential, with supervised contrastive learning being a notable approach. This method aims to form a deep feature space where similar samples from the same class are clustered together, while dissimilar samples from different classes are separated. However, a common limitation of contrastive learning methods is that they utilize the entire feature space for data embedding and often neglect the within-class variability. To overcome this limitation, we propose a novel supervised contrastive learning method that decomposes deep features into two distinct components: common features, which encapsulate the essential, class-defining characteristics, and style features, which capture the within-class variability and nuanced differences. Additionally, we enhance this approach by introducing an overlapping field that synergistically integrates elements from both feature spaces, enabling a more comprehensive and robust feature representation. Our experiments with different image datasets and deep encoders, including CNNs and transformers, show that our approach outperforms traditional single-feature contrastive methods. On the CIFAR100 and PASCAL VOC databases, traditional supervised contrastive learning achieved accuracy rates of 75.5% and 51.41%, respectively, while our method improved them to 77.81% and 59.38%, respectively. We present an algorithm for deep contrastive learning that utilizes two feature spaces: one for encoding common class features and another for capturing within-class variability. This is achieved by partitioning the features of the last layer of the encoder into (i) a common field and (ii) a style field. Our loss function contrasts the common features while summarizing the style features within the same class so that the style field can capture the intra-class variability.},
  archive      = {J_CC},
  author       = {Dornaika, F. and Wang, B. and Charafeddine, J.},
  doi          = {10.1007/s12559-025-10430-4},
  journal      = {Cognitive Computation},
  month        = {6},
  number       = {3},
  pages        = {1-12},
  shortjournal = {Cogn. Comput.},
  title        = {Deep feature disentanglement for supervised contrastive learning: Application to image classification},
  volume       = {17},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). From RNNs to transformers and beyond: A deep dive into intent detection in goal-oriented conversational agents. <em>CC</em>, <em>17</em>(3), 1-19. (<a href='https://doi.org/10.1007/s12559-025-10470-w'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Conversational assistants (CAs), particularly task-oriented ones, are designed to engage users in natural language interactions, assisting them in completing specific tasks or accessing relevant information. These systems leverage advanced natural language understanding (NLU) and dialogue management techniques to interpret user inputs, infer intentions, and respond effectively. Over time, CAs have diversified, serving sectors such as e-commerce, health care, and tourism. Central to these systems is intent detection (ID), a core process that identifies the user’s primary goal based on their utterances. This paper explores the evolution of ID methodologies, transitioning from recurrent neural networks (RNNs) to transformer-based models, analyzing their performance, limitations, and prospective advancements. Additionally, it highlights hybrid approaches, emerging paradigms, and the broader implications of these innovations for next-generation human–computer interaction systems.},
  archive      = {J_CC},
  author       = {Jbene, Mourad and Chehri, Abdellah and Saadane, Rachid and Tigani, Smail and Jeon, Gwanggil},
  doi          = {10.1007/s12559-025-10470-w},
  journal      = {Cognitive Computation},
  month        = {6},
  number       = {3},
  pages        = {1-19},
  shortjournal = {Cogn. Comput.},
  title        = {From RNNs to transformers and beyond: A deep dive into intent detection in goal-oriented conversational agents},
  volume       = {17},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Dual-branch pre-activation bottleneck transformer for face forgery detection. <em>CC</em>, <em>17</em>(3), 1-13. (<a href='https://doi.org/10.1007/s12559-025-10473-7'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Malicious use of face forgery technology will lead to serious negative effects, so there is an urgent need for an effective face forgery detection approach. Most existing approaches directly take the cropped face regions from the whole image as input and leverage the extracted visual features to predict real and fake through a binary classifier. However, advanced forgery methods will leave no obvious visual forgery artifacts, especially under real-world high compression conditions. To address such limitations, a Dual-branch Pre-activation Bottleneck Transformer (DPBoT) is proposed for face forgery detection in this paper. Instead of directly extracting appearance features, this method designs a dual-branch model to capture both visual forgery artifacts and local noise feature inconsistencies. Furthermore, to reduce information loss and ensure smoother information propagation within the model, attention mechanisms are introduced into face forgery detection, and a novel PBoT backbone architecture is designed. Finally, features from both branches are fused by bilinear pooling. Extensive experimental results on four well-known datasets show that the proposed method achieves promising results, which outperform the state-of-the-art face forgery detection methods.},
  archive      = {J_CC},
  author       = {Lin, Kaihan and Lv, Jujian and Zhao, Huimin and Ren, Jinchang and Li, Shudong and Han, Weihong and Li, Jiawen and Gu, Zhaoquan},
  doi          = {10.1007/s12559-025-10473-7},
  journal      = {Cognitive Computation},
  month        = {6},
  number       = {3},
  pages        = {1-13},
  shortjournal = {Cogn. Comput.},
  title        = {Dual-branch pre-activation bottleneck transformer for face forgery detection},
  volume       = {17},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Motor imagery classification based on temporal-spatial domain adaptation for stroke patients. <em>CC</em>, <em>17</em>(3), 1-17. (<a href='https://doi.org/10.1007/s12559-025-10477-3'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The low accuracy of unilateral upper limb multitask motor imagery (MI) classification is a significant issue hindering the development of brain-computer interface–based rehabilitation training. The low spatial resolution of electroencephalogram (EEG) in stroke patients and the complexity of multitask classification are the main reasons for this issue. The objective of this study is to develop an algorithm for decoding multiclass motor imagery tasks of unilateral upper limbs of patients, which is used to improve the classification accuracy. The algorithm is named temporal-spatial domain adaptation (TSDA), which is based on self-attention convolutional neural networks (SACNN) as the backbone network and combines the advantages of model transfer and domain adaptation. The base model of healthy subject training is used as the initialization model for patient domain adaptation training in TSDA. In addition, we also propose two domain distance loss functions, temporal multi-kernel conditional maximum mean discrepancy, and spatial multi-kernel conditional maximum mean discrepancy, which used to constrain the optimization direction of spatial-temporal features during model training. In this study, we collected EEG data from 16 healthy subjects and 20 stroke patients for four types of unilateral upper limb MI tasks to verify the classification performance of TSDA and compare it with other advanced algorithms. The base model trained on EEG data of 16 healthy subjects by SACNN is used as the initialization model. The initialization model is further trained in TSDA with one patient as the target domain and another patient as the source domain. Each of the 20 patients served as a source domain and a target domain. The classification accuracy of TSDA is 51.7% ± 0.08, which is significantly higher than that of the benchmark algorithms (P < 0.05). The technical verification and visualization results show the stability and reliability of the TSDA model. This paper provides more theoretical basis for MI multitask classification of stroke patients based on transfer learning.},
  archive      = {J_CC},
  author       = {Ma, Jun and Zhang, Jingjing and Yang, Yanling and Yang, Banghua and Shan, Chunlei},
  doi          = {10.1007/s12559-025-10477-3},
  journal      = {Cognitive Computation},
  month        = {6},
  number       = {3},
  pages        = {1-17},
  shortjournal = {Cogn. Comput.},
  title        = {Motor imagery classification based on temporal-spatial domain adaptation for stroke patients},
  volume       = {17},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Deep learning innovations in the detection of lung cancer: Advances, trends, and open challenges. <em>CC</em>, <em>17</em>(2), 1-46. (<a href='https://doi.org/10.1007/s12559-025-10408-2'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cancer is the second leading cause of death worldwide, and within this type of disease, lung cancer is the second most diagnosed, but the leading cause of death. Early detection is crucial to increase patient survival rates. One of the primary methods for detecting this disease is through medical imaging, which, due to its features, is well-suited for analysis by deep learning techniques. These techniques have demonstrated exceptional results in similar tasks. Therefore, this paper focusses on analyzing the latest work related to lung cancer detection using deep learning, providing a clear overview of the state of the art and the most common research directions pursued by researchers. We have reviewed DL techniques for lung cancer detection between 2018 and 2023, analyzing the different datasets that have been used in this domain and providing an analysis between the different investigations. In this state-of-the-art review, we describe the main datasets used in this field and the primary deep learning techniques used to detect radiological signs, predominantly convolutional neural networks (CNNs). As the impact of these systems in medicine can pose risks to patients, we also examine the extent to which explainable AI techniques have been applied to enhance the understanding of these systems, a crucial aspect for their real-world application. Finally, we will discuss the trends that the domain is expected to follow in the coming years and the challenges that researchers will need to address.},
  archive      = {J_CC},
  author       = {Liz-López, Helena and de Sojo-Hernández, Áurea Anguera and D’Antonio-Maceiras, Sergio and Díaz-Martínez, Miguel Angel and Camacho, David},
  doi          = {10.1007/s12559-025-10408-2},
  journal      = {Cognitive Computation},
  month        = {4},
  number       = {2},
  pages        = {1-46},
  shortjournal = {Cogn. Comput.},
  title        = {Deep learning innovations in the detection of lung cancer: Advances, trends, and open challenges},
  volume       = {17},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multi-source adversarial domain adaptive fault diagnosis method based on multi-classifier alignment. <em>CC</em>, <em>17</em>(2), 1-17. (<a href='https://doi.org/10.1007/s12559-025-10414-4'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Transfer learning–based fault diagnosis has received intensive attention from researchers. Under various working conditions, high-precision cross-domain fault diagnosis remains a problem due to distribution differences between different source domains and between source and target domains. Therefore, reducing the distribution difference between source domain and target domain data is crucial for improving the model’s ability to learn domain-invariant features and fault-representative features. To address this challenge, this paper proposes a multi-source adversarial domain adaptation approach for fault diagnosis, referred to as MSD-MCA, which is based on the alignment of multiple classifiers. The method constructs a sub-network for each source domain and utilizes domain adversarial training to extract domain-invariant features. It then generates a fault feature set for each source domain by leveraging the domain-invariant features corresponding to various fault types. To align the target domain with the source domains, the Wasserstein distance is calculated between the target features and each fault feature set. Minimizing the entropy of the distribution distance vector facilitates the learning of fault-representative features. Additionally, an association matrix is employed to enhance the stability of the decision boundaries during the training process. This approach improves the model’s capacity to generalize across multiple domains while effectively capturing fault-related information. To validate the efficacy of the proposed MSD-MCA method, a comparative analysis was conducted against several state-of-the-art diagnostic approaches. The evaluation was performed on bearing fault data from Case Western Reserve University, as well as two real-world industrial datasets. The results indicate that MSD-MCA shows improved accuracy and enhanced generalization capabilities across both datasets. Consequently, MSD-MCA can better learn the domain-invariant features and fault-representative features and improve the accuracy of fault diagnosis.},
  archive      = {J_CC},
  author       = {Zheng, Zhiwei and He, Yu and Ma, Tianyu and Xiang, Qingsong},
  doi          = {10.1007/s12559-025-10414-4},
  journal      = {Cognitive Computation},
  month        = {4},
  number       = {2},
  pages        = {1-17},
  shortjournal = {Cogn. Comput.},
  title        = {Multi-source adversarial domain adaptive fault diagnosis method based on multi-classifier alignment},
  volume       = {17},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Conditional time series modeling for pneumoconiosis progression risk prediction with missing data. <em>CC</em>, <em>17</em>(2), 1-18. (<a href='https://doi.org/10.1007/s12559-025-10417-1'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Pneumoconiosis is a serious occupational disease with high morbidity and disability rates. However, the evolution of pneumoconiosis is complex and changeable, and the course of most cases is incomplete, resulting in a lack of continuity in a large number of data samples, which makes it challenging for radiologists to accurately assess the development of the disease. We propose a conditional self-attention TimesNet as the backbone network for time series analysis tasks, aiming to improve prognosis prediction based on disease progression information in pneumoconiosis image data at different time periods. In our approach, we train the model using chest X-ray images of the same patient at different time points, incorporating a hierarchical attention structure and self-attention blocks to fully consider the contextual correlation information of consecutive time-point images. Additionally, diverse clinical features of patients are utilized as conditional inputs in disease progression prediction. The goal is to better learn the progression status of the disease and reflect the disease trajectory representation of missing time series data, enhancing the model’s predictive capabilities. Simultaneously, an adversarial diffusion generation model is designed to fill in missing values in the time series data. The missing data generated by the model effectively improves radiologists’ judgment of pneumoconiosis progression. We trained our model using missing time series images to predict clinical outcomes. Experimental validation on two medical datasets shows that the AUC, sensitivity, specificity, and DSC achieved 90.33%, 87.89%, 85.01%, and 88.54%, respectively. These results highlight the competitive performance of our method across multiple evaluation metrics. Our model can capture the correlation between short-term/long-term/missing time series lesion features and time in pneumoconiosis images. This approach holds significant implications for predicting clinical outcomes and progression risk in pneumoconiosis, providing valuable guidance for the assessment and prognosis of pneumoconiosis.},
  archive      = {J_CC},
  author       = {Ren, Xueting and Zhao, Zijuan and Jia, Liye and Zhao, Juanjuan and Jia, Baoping and Qiang, Yan and Zhao, Huilan and Yue, Huajie},
  doi          = {10.1007/s12559-025-10417-1},
  journal      = {Cognitive Computation},
  month        = {4},
  number       = {2},
  pages        = {1-18},
  shortjournal = {Cogn. Comput.},
  title        = {Conditional time series modeling for pneumoconiosis progression risk prediction with missing data},
  volume       = {17},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). GSAC-UFormer: Groupwise self-attention convolutional transformer-based UNet for medical image segmentation. <em>CC</em>, <em>17</em>(2), 1-14. (<a href='https://doi.org/10.1007/s12559-025-10425-1'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Traditional transformers struggle to effectively capture local contextual information. Conversely, CNNs face challenges in modeling long-range dependencies. To address these limitations, this paper introduces GSAC-UFormer, an innovative Groupwise Self-Attention Convolutional Transformer-based UNet for medical image segmentation. The design of GSAC-UFormer focuses on efficiently integrating both local and global information, balancing the strengths of different processing techniques. At the core of GSAC-UFormer is the GSAC-Former block. This module combines groupwise convolution with a CNN-adaptive self-attention mechanism, enabling parallel integration of local and global contexts. This architecture allows the model to effectively capture intricate dependencies across various data dimensions while processing local features with high efficiency. The Guided Contextual Feature Attention (GCFA) mechanism further enhances feature selection. It emphasizes the most relevant contextual information, refining spatial and channel-wise relationships in the extracted features. This targeted approach mitigates noise and improves model accuracy. Additionally, the Multi-Depth Partitioned Depthwise Convolution Transformer (MDPDC-Former) serves as a bottleneck module. It optimizes feature mapping and enhances network learning efficiency by dynamically adjusting the receptive field. This enables the model to capture multi-scale semantic information more effectively. Experimental results highlight the superior performance of GSAC-UFormer compared to state-of-the-art methods. It achieves Dice coefficients of 91.6%, 94.61%, and 82.24% on the MICCAI 2017 (red lesion), PH2, and CVC-ClinicalDB datasets, respectively. These results underscore its effectiveness in advancing medical image segmentation.},
  archive      = {J_CC},
  author       = {Garbaz, Anass and Oukdach, Yassine and Charfi, Said and El Ansari, Mohamed and Koutti, Lahcen and Salihoun, Mouna},
  doi          = {10.1007/s12559-025-10425-1},
  journal      = {Cognitive Computation},
  month        = {4},
  number       = {2},
  pages        = {1-14},
  shortjournal = {Cogn. Comput.},
  title        = {GSAC-UFormer: Groupwise self-attention convolutional transformer-based UNet for medical image segmentation},
  volume       = {17},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Improved alternative queuing method of interval-set dissimilarity measures and possibility degrees for multi-expert multi-criteria decision-making. <em>CC</em>, <em>17</em>(2), 1-24. (<a href='https://doi.org/10.1007/s12559-025-10426-0'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-expert multi-criteria decision-making (MEMCDM) based on interval-set information is novel and valuable, and it already adopts an effective strategy of alternative queuing method (AQM), called AQM-IS. AQM-IS mainly relies on dissimilarity measures and possibility degrees of interval sets, and the two types of uncertainty measures have absolute-quantitative limitations on rough information extraction to imply improvement space. In this work, improved dissimilarity measures and possibility degrees of interval sets are constructed from a better perspective of relative quantization related to systematic structuring and statistical fusion, so improved AQM (called IAQM-IS) is established to advance MEMCDM by using the interval-set information transformation. As bases, relative dissimilarity measures are proposed to modify absolute dissimilarity measures for both interval-set pairs and families on closeness and deviation; thus, relevant internal relationships, mutual sizes, axiomatic properties, and illustrative examples are acquired. Aiming at interval-set information, improved AQM (i.e., IAQM-IS) is investigated for MEMCDM. Concretely, absolute dissimilarity measures are chosen to determine criterion weights based on judgement matrix and maximum deviation, and improved possibility degrees of interval sets are proposed by systematic likelihood characterizations and arithmetic mean combination; using the weight arithmetic mean of improved dissimilarity measures and possibility degrees, a more powerful index for sorting alternatives is generated to formulate IAQM-IS. For algorithmic evaluation, two assessment indices of decision rankings (called separability and goodness) are designed; accordingly, the two algorithms of MEMCDM — AQM-IS and IAQM-IS — are demonstrated and compared via both an applied examples of e-commerce platforms and six simulated experiments of public datasets, and thus the effectiveness and superiority of IAQM-IS are verified. In summary by the double-quantization technique, the improved dissimilarity measures and possibility degrees deepen uncertainty measures of interval-set information tables, and corresponding IAQM-IS has better decision performance than current AQM-IS in specific application scenarios of social cognition.},
  archive      = {J_CC},
  author       = {Xie, Xin and Zhang, Xianyong and Lv, Zhiying and Chen, Jiang},
  doi          = {10.1007/s12559-025-10426-0},
  journal      = {Cognitive Computation},
  month        = {4},
  number       = {2},
  pages        = {1-24},
  shortjournal = {Cogn. Comput.},
  title        = {Improved alternative queuing method of interval-set dissimilarity measures and possibility degrees for multi-expert multi-criteria decision-making},
  volume       = {17},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). CLKT: Optimizing cognitive load management in knowledge tracing. <em>CC</em>, <em>17</em>(2), 1-18. (<a href='https://doi.org/10.1007/s12559-025-10427-z'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the rise of online adaptive learning, Knowledge Tracing (KT) has become an indispensable component of online education systems. KT assesses the knowledge level of each learner by tracing their learning activities. Managing cognitive load is crucial in the learners’ cognitive process; too low a load may lead to a lack of concentration, while excessively high cognitive load can impede information processing. In pursuit of an ideal learning model, this paper proposes the Cognitive Load-based Knowledge Tracing (CLKT) model. This model employs a Heterogeneous Cognitive Graph Convolutional Network (HCGCN) to extract learners’ knowledge representations and establish connections between learning tasks or instructional resources and learners, providing the model with interpretable learning path recommendations. By introducing the Attention Concentration (AC) mechanism, the model dynamically processes information and efficiently integrates it into learners’ knowledge structures to maintain an appropriate cognitive load level, thus maximizing effective learning. Experiments conducted on the ASSISTMENTS dataset, which contains real-world student interaction data from an online tutoring system, focus on studying the impact of different cognitive loads on the learning process. The experimental results delve into the effects of cognitive load on learner performance, ensuring that learners can engage in learning with appropriate pace and difficulty, thereby enhancing their learning outcomes.},
  archive      = {J_CC},
  author       = {Wu, Qianxi and Ji, Weidong and Zhou, Guohui},
  doi          = {10.1007/s12559-025-10427-z},
  journal      = {Cognitive Computation},
  month        = {4},
  number       = {2},
  pages        = {1-18},
  shortjournal = {Cogn. Comput.},
  title        = {CLKT: Optimizing cognitive load management in knowledge tracing},
  volume       = {17},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Leveraging graph convolutional networks for semi-supervised learning in multi-view non-graph data. <em>CC</em>, <em>17</em>(2), 1-15. (<a href='https://doi.org/10.1007/s12559-025-10428-y'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Semi-supervised learning with a graph-based approach has gained prominence in machine learning, particularly in scenarios where labeling data involves substantial costs. Graph convolution networks (GCNs) have found widespread application in semi-supervised learning, predominantly on graph-structured data such as citation and social networks. However, a noticeable gap exists in the application of these methods to non-graph multi-view data, such as collections of images. In an effort to address this gap, we introduce two innovative deep semi-supervised multi-view classification models specifically tailored for non-graph data. Both models share a common architecture, leveraging GCNs and integrating a label smoothing constraint. The primary distinction lies in the construction of the consensus similarity graph. The first model directly reconstructs the consensus graph from different views using a specialized objective function designed for flexible graph-based semi-supervised classification. In contrast, the second model independently reconstructs individual graphs and subsequently adaptively merges them into a unified consensus graph. Our experiments encompass various multiple-view image datasets. The results consistently demonstrate the superior performance of our proposed approach compared to traditional fusion methods with GCNs. In this research, we present two approaches for tackling semi-supervised classification challenges involving multiple views. One method is named Semi-supervised Classification with a Unified Graph (SCUG), and the other is referred to as Semi-supervised Classification with a Fused Graph (SC-Fused). Both methods share a common semi-supervised classification process, utilizing the GCN framework and incorporating label smoothing. However, the key distinction lies in the construction of the similarity graph. Unlike traditional ad hoc graph construction approaches, our proposed methods, SCUG and SC-Fused, estimate the unified graph or individual graphs, respectively, alongside the labels. This results in more optimized graphs that benefit from data smoothing and the semi-supervised context.},
  archive      = {J_CC},
  author       = {Dornaika, F. and Bi, J. and Charafeddine, J.},
  doi          = {10.1007/s12559-025-10428-y},
  journal      = {Cognitive Computation},
  month        = {4},
  number       = {2},
  pages        = {1-15},
  shortjournal = {Cogn. Comput.},
  title        = {Leveraging graph convolutional networks for semi-supervised learning in multi-view non-graph data},
  volume       = {17},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A holistic comparative study of large language models as emotional support dialogue systems. <em>CC</em>, <em>17</em>(2), 1-21. (<a href='https://doi.org/10.1007/s12559-025-10429-x'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Emotional support conversation aims to convey understanding, sympathy, care, and support through conversation, to help others cope with emotional distress, pressure, or challenges. In this study, we conduct a holistic comparative study to investigate how well the most recent large language models (LLMs), which have recently proved to have empathy, and act as emotional supporters. To this end, we make use of the emotional support conversation (ESC) framework and assess multiple maintain LLMs accordingly. We then have an in-depth comparison between these LLM-based emotional supporters to humans in terms of the use of emotional support strategies as well as the use of language. Surprisingly, we find that there is still a huge gap until these LLMs become effective emotional supporters. This is because, on the one hand, they have strong preference biases on using a limited set of strategies, making them always show empathy but rarely take real actions (such as providing suggestions), which is key in ESC. On the other hand, they often over-generate responses, making what they utter a departure from those of human experts.},
  archive      = {J_CC},
  author       = {Bai, Xin and Chen, Guanyi and He, Tingting and Zhou, Chenlian and Guo, Cong},
  doi          = {10.1007/s12559-025-10429-x},
  journal      = {Cognitive Computation},
  month        = {4},
  number       = {2},
  pages        = {1-21},
  shortjournal = {Cogn. Comput.},
  title        = {A holistic comparative study of large language models as emotional support dialogue systems},
  volume       = {17},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Attention-enabled multi-layer subword joint learning for chinese word embedding. <em>CC</em>, <em>17</em>(2), 1-16. (<a href='https://doi.org/10.1007/s12559-025-10431-3'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, Chinese word embeddings have attracted significant attention in the field of natural language processing (NLP). The complex structures and diverse influences of Chinese characters present distinct challenges for semantic representation. As a result, Chinese word embeddings are primarily investigated in conjunction with characters and their subcomponents. Previous research has demonstrated that word vectors frequently fail to capture the subtle semantics embedded within the complex structure of Chinese characters. Furthermore, they often neglect the varying contributions of subword information to semantics at different levels. To tackle these challenges, we present a weight-based word vector model that takes into account the internal structure of Chinese words at various levels. The model further categorizes the internal structure of Chinese words into six layers of subword information: words, characters, components, pinyin, strokes, and structures. The semantics of Chinese words can be derived by integrating the subword information from various layers. Moreover, the model considers the varying contributions of each subword layer to the semantics of Chinese words. It utilizes an attention mechanism to determine the weights between and within the subword layers, facilitating the comprehensive extraction of word semantics. The word-level subwords act as the attention mechanism query for subwords in other layers to learn semantic bias. Experimental results show that the proposed word vector model achieves enhancements in various evaluation metrics, such as word similarity, word analogy, text categorization, and case studies.},
  archive      = {J_CC},
  author       = {Xue, Pengpeng and Xiong, Jing and Tan, Liang and Liu, Zhongzhu and Liu, Kanglong},
  doi          = {10.1007/s12559-025-10431-3},
  journal      = {Cognitive Computation},
  month        = {4},
  number       = {2},
  pages        = {1-16},
  shortjournal = {Cogn. Comput.},
  title        = {Attention-enabled multi-layer subword joint learning for chinese word embedding},
  volume       = {17},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Modeling visual attention based on gestalt theory. <em>CC</em>, <em>17</em>(2), 1-14. (<a href='https://doi.org/10.1007/s12559-025-10410-8'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Gestalt theory laid the foundation for modern cognitive learning theory and emphasizes that the whole is greater than the sum of its parts, where similarity and proximity are two important principles. However, exploiting Gestalt theory to detect multiple salient objects remains challenging. In this paper, we propose a very simple yet efficient saliency model based on Gestalt theory, namely, the color similarity and spatial proximity (CSSP) model. It utilizes content-based image retrieval (CBIR) techniques to detect salient objects. The methodology has three important highlights: (1) a novel weighted distance is proposed to calculate spatial proximity. It can control spatial proximity within a certain range and detect salient objects robustly. (2) Two novel and efficient saliency scoring calculation methods are proposed under the framework of CBIR techniques, where color similarity and spatial proximity are used for image matching and the ordering of retrieved images. This enables the robust identification of multiple salient objects. (3) A very simple yet efficient integration method is proposed to combine saliency maps. Using this integration method, impurities around salient objects are greatly reduced, and their interiors are highlighted robustly. Experiments with several well-known benchmark datasets validate the performance of the CSSP model. The CSSP method resulted in fewer grey patches inside salient objects, and it is superior to many existing state-of-the-art methods. The detected salient regions were brighter, improving the effectiveness of multiple salient objects detection. In addition, the CSSP method can detect salient objects robustly even when they touch the image boundaries. It has demonstrated that modeling visual attention based on Gestalt theory is a novel, viable approach.},
  archive      = {J_CC},
  author       = {Liu, Guang-Hai and Yang, Jing-Yu},
  doi          = {10.1007/s12559-025-10410-8},
  journal      = {Cognitive Computation},
  month        = {4},
  number       = {2},
  pages        = {1-14},
  shortjournal = {Cogn. Comput.},
  title        = {Modeling visual attention based on gestalt theory},
  volume       = {17},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Three-way decision approach based on utility and dynamic localization transformational procedures within a circular q-rung orthopair fuzzy set for ranking and grading large language models. <em>CC</em>, <em>17</em>(2), 1-26. (<a href='https://doi.org/10.1007/s12559-025-10432-2'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Large language models (LLMs) have made significant advancements in natural language processing (NLP), impacting both academia and industry. Evaluating LLMs is crucial, as these models are developed for multiple NLP tasks. However, no single LLM has successfully fulfilled all tasks simultaneously, creating a research gap. This, in turn, leads to the identification of the most and least effective LLMs in real-world problems, presenting a multi-criteria decision-making (MCDM) challenge due to the diversity of evaluation tasks, task prioritization, data variability, and issues related to ranking and grading with binary data. While the three-way decision (3WD) approach based on MCDM methods can address this, it often leaves uncertainty as an open issue, highlighting a theoretical gap. To address this, the contribution of this study is the development of a new 3WD approach based on utility and dynamic localization transformational procedures within a circular q-rung orthopair fuzzy set (C-q-ROFS) for ranking and grading LLMs. The methodology includes (1) reformulating the fuzzy weighted zero inconsistency-based interrelationship process (FWZICbIP) using C-q-ROFS (C-q-ROFS–FWZICbIP method) to prioritize tasks and address weighting uncertainty; (2) formulating a decision matrix by intersecting LLMs with NLP tasks while applying utility and dynamic localization procedures to handle binary input issues; and (3) reformulating the conditional probabilities by opinion scores (CPOS) method within the C-q-ROFS context (C-q-ROFS–CPOS method) to determine decision thresholds for each LLM. This involves incorporating Bayesian decision theory under C-q-ROFS to establish decision thresholds for all LLMs, thereby enhancing the certainty and effectiveness of the grading process. Based on this, the 3WD approach is developed to offer a robust mechanism for ranking and grading LLMs. Forty LLMs were ranked and graded across 11 NLP tasks, with the findings showing that LLM14 demonstrated high efficacy, ranking in the positive region for nine σ values, but falling into the boundary region at σ = 0.05. Sensitivity and comparison analyses were conducted to evaluate the robustness and stability of the methodology.},
  archive      = {J_CC},
  author       = {Qahtan, Sarah and Mourad, Nahia and Alsattar, H. A. and Zaidan, A. A. and Zaidan, B. B. and Pamucar, Dragan and Simic, Vladimir and Ding, Weiping and Yatim, Khaironi},
  doi          = {10.1007/s12559-025-10432-2},
  journal      = {Cognitive Computation},
  month        = {4},
  number       = {2},
  pages        = {1-26},
  shortjournal = {Cogn. Comput.},
  title        = {Three-way decision approach based on utility and dynamic localization transformational procedures within a circular q-rung orthopair fuzzy set for ranking and grading large language models},
  volume       = {17},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A communication-efficient distributed frank-wolfe online algorithm with an event-triggered mechanism. <em>CC</em>, <em>17</em>(2), 1-16. (<a href='https://doi.org/10.1007/s12559-025-10438-w'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Distributed learning is an effective method for solving large-scale cognitively inspired online machine learning problems. However, frequent communications between nodes lead to expensive communication burden. Meanwhile, projection operations because of the constraints in decision variables cause a lot of computational cost. In order to address the above problems, this paper presents a communication-efficient distributed Frank-Wolfe online optimization method, which integrates the event-triggered mechanism into the distributed projection-free online optimization algorithm. Furthermore, we provide a rigorous theoretical analysis for the regret of the proposed algorithm. Finally, we verify the performance of the proposed method through a variety of numerical experiments. The theoretical results show that the regret reaches a sublinear growth of iterations for convex objective functions. The proposed algorithm outperforms the baseline methods on two datasets. Our research indicates that, in addition to reducing computational overhead, the event-triggered scheme has the potential to enhance the communication efficiency of distributed network system.},
  archive      = {J_CC},
  author       = {Gao, Huimin and Liu, Muhua and Ji, Zhihang and Zheng, Ruijuan and Wu, Qingtao},
  doi          = {10.1007/s12559-025-10438-w},
  journal      = {Cognitive Computation},
  month        = {4},
  number       = {2},
  pages        = {1-16},
  shortjournal = {Cogn. Comput.},
  title        = {A communication-efficient distributed frank-wolfe online algorithm with an event-triggered mechanism},
  volume       = {17},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Online signature watermarking in the transform domain. <em>CC</em>, <em>17</em>(2), 1-14. (<a href='https://doi.org/10.1007/s12559-025-10436-y'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The increasing reliance on digital signatures for secure authentication and verification necessitates advanced watermarking techniques to protect signature integrity. Transform-domain methods, including the discrete cosine transform (DCT) and the discrete wavelet transform (DWT), are proposed for their potential to balance robustness, imperceptibility, and recognition accuracy in online signature biometrics. This study explores multi-bit watermarking approaches applied to online handwritten signatures using the MCYT signature database. We investigate the effects of embedding multiple bits per sample with adjustable watermark strength ( $$\alpha $$ ) in the DCT and DWT domains. The trade-offs between signal distortion, watermark extraction accuracy, and biometric recognition rates are systematically evaluated. Experimental results reveal that while increasing $$\alpha $$ enhances watermark robustness, it also leads to perceptible distortions in signature samples. We identify the minimum $$\alpha $$ thresholds required for error-free watermark extraction and analyze their impact on identification and verification performance. The proposed multi-bit embedding strategy in the DCT domain demonstrates a viable compromise between robustness and imperceptibility, maintaining acceptable biometric recognition rates. Transform-domain watermarking techniques provide a promising solution for secure and robust online signature biometrics. This study highlights the feasibility of incorporating multi-bit watermarking schemes with adjustable strength into online signature systems, enhancing security while preserving recognition accuracy.},
  archive      = {J_CC},
  author       = {Faundez-Zanuy, Marcos},
  doi          = {10.1007/s12559-025-10436-y},
  journal      = {Cognitive Computation},
  month        = {4},
  number       = {2},
  pages        = {1-14},
  shortjournal = {Cogn. Comput.},
  title        = {Online signature watermarking in the transform domain},
  volume       = {17},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Rain streak removal using improved generative adversarial network with loss function optimization. <em>CC</em>, <em>17</em>(2), 1-15. (<a href='https://doi.org/10.1007/s12559-025-10435-z'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Rain is a typical meteorological phenomenon that can significantly impair the functionality of outdoor computer vision systems, including autonomous navigation and surveillance. Depending on how far away the streaks are from the camera, they may appear differently in the images. One input image serves as the foundation for the majority of current rain removal techniques. However, estimating a trustworthy depth map for rain removal is challenging on a single image. To overcome these challenges, this research introduces a novel approach for rain streak removal utilizing generative adversarial networks (GANs). Leveraging the discriminative power of GANs, the proposed technique effectively distinguishes between rain streaks and clean image content, resulting in the generation of realistic, rain-free images. The workflow involves initial image pre-processing using a cross-guided bilateral filter for detail layer extraction. The rain streak removal is then executed through an improved de-rain GAN (DR_GAN), where the generator module is replaced with a dense bidirectional network with self-attention (Attn_DBNet). This integration incorporates DenseNet-121, bidirectional gated recurrent unit (BiGRU), and self-attention mechanisms, enhancing the overall performance of the rain streak removal process. The research further introduces chaotic logistic gazelle optimization (CL-G) for optimizing the loss function, addressing local optimal trapping issues through the incorporation of chaotic logistic mapping. With notable gains in the metrics, comparative analysis shows that the proposed method is superior to the state-of-the-art approaches. These successes demonstrate the usefulness and superiority of the proposed GAN-based rain streak removal method over dual CNN, QSAM-Net, and MGPDNet approaches, with significant percentage advantages over these networks.},
  archive      = {J_CC},
  author       = {R, Prabha and R, Suma and Babu D, Suresh and Saila, S},
  doi          = {10.1007/s12559-025-10435-z},
  journal      = {Cognitive Computation},
  month        = {4},
  number       = {2},
  pages        = {1-15},
  shortjournal = {Cogn. Comput.},
  title        = {Rain streak removal using improved generative adversarial network with loss function optimization},
  volume       = {17},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). T-norms and T-conorms of symmetrical linear orthopair fuzzy sets and their cognitive applications in multiple-criteria decision-making. <em>CC</em>, <em>17</em>(2), 1-29. (<a href='https://doi.org/10.1007/s12559-025-10439-9'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Orthopair fuzzy sets (OFSs) generally include q-rung orthopair fuzzy sets (q-ROFSs) and symmetrical linear orthopair fuzzy sets (SLOFSs), and the latter two models have a common element: intuitionistic fuzzy sets (IFSs). T-norms, t-conorms, and multiple-criteria decision-making (MCDM) are applied to q-ROFSs, but they have not been applied to SLOFSs. These valuable parts of SLOFSs are investigated by extending and simulating relevant results in IFSs, and their operational connections to addition and scalar multiplication are addressed in detail. For SLOFSs, axiomatic definitions, general properties, and concrete constructions for t-norms and t-conorms are first given. Then, special types of t-norms and t-conorms are used to motivate the addition and scalar multiplication operations, and related properties of the operations are obtained. Finally, addition and scalar multiplication are linearly combined with aggregation, and a relevant technique for order preference by similarity to an ideal solution (TOPSIS) method is designed for decision cognition. In this way, a new MCDM method based on SLOFSs is established, and its high reliability is validated by comparing the corresponding method based on q-ROFSs in two practical examples. This study advances work on SLOFSs and linearly extends IFS results, thus enriching OFSs, especially for cognitive computations and applications.},
  archive      = {J_CC},
  author       = {Gao, Shan and Zhang, Xianyong and Mo, Zhiwen},
  doi          = {10.1007/s12559-025-10439-9},
  journal      = {Cognitive Computation},
  month        = {4},
  number       = {2},
  pages        = {1-29},
  shortjournal = {Cogn. Comput.},
  title        = {T-norms and T-conorms of symmetrical linear orthopair fuzzy sets and their cognitive applications in multiple-criteria decision-making},
  volume       = {17},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Correction to: A novel interpretable graph convolutional neural network for multimodal brain tumor segmentation. <em>CC</em>, <em>17</em>(2), 1. (<a href='https://doi.org/10.1007/s12559-025-10440-2'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_CC},
  author       = {Choudhry, Imran Arshad and Iqbal, Saeed and Alhussein, Musaed and Aurangzeb, Khursheed and Qureshi, Adnan N. and Hussain, Amir},
  doi          = {10.1007/s12559-025-10440-2},
  journal      = {Cognitive Computation},
  month        = {4},
  number       = {2},
  pages        = {1},
  shortjournal = {Cogn. Comput.},
  title        = {Correction to: A novel interpretable graph convolutional neural network for multimodal brain tumor segmentation},
  volume       = {17},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multi-criteria group decision-making using complex p, q-quasirung orthopair fuzzy sets: Application in the selection of renewable energy projects for investments. <em>CC</em>, <em>17</em>(2), 1-23. (<a href='https://doi.org/10.1007/s12559-025-10424-2'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Evaluating the hesitation inherent in decision-making models is a significant hurdle for experts. The p, q-quasirung fuzzy set (p, q-QOFS) is a new development in fuzzy set theory that plays a vital role in addressing these challenges. This article integrates the notions of p, q-QOFSs and complex fuzzy sets (CFSs) to introduce complex p, q-quasirung orthopair fuzzy sets (Cp,qQOFSs) and outline their basic principles. The Cp,qQOFSs expend the range of membership and non-membership degree of p, q-QOFS from real numbers to encompass complex unit disc values. In the first phase, some basic operational laws and their properties are defined. Future, we will use these operational laws to define some aggregation operators such as complex p, q-quasirung orthopair fuzzy weighted averaging and complex p, q-quasirung orthopair fuzzy weighted geometric operators to aggregated complex p, q-quasirung orthopair fuzzy information. Based on these aggregation operators, a new multi-attribute group decision-making (MCGDM) approach is constructed to handle real-life complex decision-making problems. Moreover, the weights of the criteria are calculated using the entropy method. An illustrative numerical example showcasing the proposed MCGDM method has been provided, focusing on renewable energy investments with seven alternatives and five criteria. Additionally, we conduct the sensitivity analysis to demonstrate the impact of parameters p and q over aggregated results. Lastly, the proposed approach is compared with existing methods to highlight its superiority and flexibility.},
  archive      = {J_CC},
  author       = {Rahim, Muhammad and Bajri, Sanaa Ahmed and Alqahtani, Haifa and Alhabeeb, Somayah Abdualziz and Khalifa, Hamiden Abd El-Wahed},
  doi          = {10.1007/s12559-025-10424-2},
  journal      = {Cognitive Computation},
  month        = {4},
  number       = {2},
  pages        = {1-23},
  shortjournal = {Cogn. Comput.},
  title        = {Multi-criteria group decision-making using complex p, q-quasirung orthopair fuzzy sets: Application in the selection of renewable energy projects for investments},
  volume       = {17},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Medivision: Empowering colorectal cancer diagnosis and tumor localization through supervised learning classifications and grad-CAM visualization of medical colonoscopy images. <em>CC</em>, <em>17</em>(2), 1-39. (<a href='https://doi.org/10.1007/s12559-025-10433-1'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Medivision is a modern diagnostic system that has been developed to improve colorectal cancer detection.  By applying deep learning acquisitions, including convolutional neural networks, Gray-Level Co-occurrence Matrix feature extraction, and visualization with Grad-CAM. After developing the Medivision system, several colonoscopy image datasets were used for evaluating a number of state-of-the-art deep CNN architectures, such as ResNet50, VGG16, VGG19, and DenseNet201—namely, CVC Clinic DB, Kvasir2, and Hyper Kvasir, however, through exhaustive model comparisons with integrated CNNs DEV-22 and RV-22. By the incorporation of GLCM feature extraction, textual analysis has become enhanced in the capturing of some critical features within an image that enhances model sensitivity in the detection of subtle variation within colorectal polyps. Grad-CAM visualizations enhance interpretability by enabling clinicians to attribute diagnostically important regions and insight into the model’s process of decision-making, incorporated with cloud storage.  This work confirmed the stability and reliability of VGG16 in different conditions of image shooting. Among these, the best performances for all datasets were obtained by the model VGG16, confirming its consistency and robustness in varying conditions of image acquisition. On the CVC Clinic DB, VGG16 assured a training accuracy of 99.22% and a testing accuracy of 96.12%. The model, when trained on the Kvasir2 dataset, achieved an accuracy of 96.56% during training and 94.25% when testing, while on the Hyper Kvasir dataset, the model supported training accuracy of 95.19% and a testing accuracy of 98.87%. Apart from that, VGG16 also showed a good localization capability for the average IoU of 0.78 on CVC Clinic DB, 0.79 on Kvasir2, and 0.77 on Hyper Kvasir, indicating its precision in identifying polyp regions. It also tested the performance of integrated CNN models DEV-22 and RV-22 in complex multi-dataset scenarios. DEV-22 gave the best-performing integrated model against test accuracies of 97.86% against CVC Clinic DB, 89.37% against Kvasir2, and 76.08% against Hyper Kvasir, while RV-22 resulted in relatively poor performance across these datasets.This indicates that DEV-22 might be much better suited for colorectal cancer detection tasks whenever multiple datasets are concerned. The high testing accuracy and precise localization capabilities of VGG16 and DEV-22 show their robustness within the Medivision system for delivering appropriate clinically relevant returns on colorectal cancer screening across all three datasets.  Medivision enables real-time analysis in an accessible and efficient way for healthcare providers. This paper presents the clinical relevance of this system in enhancing diagnostic accuracy and interpretation for colorectal cancer screening workflows. It represents a state-of-the-art integration of high-performance deep CNN models, GLCM, and Grad-CAM for accurate, interpretable, and actionable outcomes in the diagnosis of colorectal cancer, by representing one great bound in AI-mediated medical diagnosis.},
  archive      = {J_CC},
  author       = {Raju, Akella S. Narasimha and Venkatesh, K and Gatla, Ranjith Kumar and Hussain, Shaik Jakeer and Polamuri, Subba Rao},
  doi          = {10.1007/s12559-025-10433-1},
  journal      = {Cognitive Computation},
  month        = {4},
  number       = {2},
  pages        = {1-39},
  shortjournal = {Cogn. Comput.},
  title        = {Medivision: Empowering colorectal cancer diagnosis and tumor localization through supervised learning classifications and grad-CAM visualization of medical colonoscopy images},
  volume       = {17},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Single-valued neutrosophic distance measure-based MEREC-RANCOM-WISP for solving sustainable energy storage technology problem. <em>CC</em>, <em>17</em>(2), 1-22. (<a href='https://doi.org/10.1007/s12559-025-10437-x'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Energy storage technology (EST) is crucial in mitigating the environmental impact of energy storage and reducing carbon footprints. It is a vital component of renewable energy sources and decarbonization of world energy structures. The selection of a suitable EST depends on multiple aspects of sustainability; thus, the decision-making methods are a more proper way to systematically deal with this problem. Uncertainty is commonly occurred in the selection of a suitable EST. As a generalization of a fuzzy set, a single-valued neutrosophic set (SVNS) has been demonstrated as a useful framework for handling indeterminate, inconsistent, and uncertain data of realistic decision-making situations. Considering the idea of SVNS, this paper develops a hybrid multi-criteria group decision-making (MCGDM) approach to assess and prioritize ESTs over different qualitative and quantitative criteria. The proposed “simple weighted sum product (WISP)” method combines the Hellinger distance measure-based decision experts’ weighting tool and integrated criteria weight-determining model to deal the single-valued neutrosophic information (SVNI)-based MCGDM problems with completely unknown weights of decision experts (DEs) as well as defined criteria. To evade the drawbacks of extent distance measures, we introduce a novel single-valued neutrosophic Hellinger distance measure to compute the degree of discrimination on SVNSs. Some illustrative examples are taken to exemplify the efficiency of developed Hellinger distances over existing ones. Further, the developed Hellinger distance is utilized to derive the weight of DEs. In addition, the criteria weight-determining procedure is given by integrating objective weight through a “method based on the removal effects of criteria (MEREC)” and subjective weight through “ranking comparison (RANCOM)” under SVNI. Based on these models, a combined WISP approach is developed to rank the alternatives on SVNI. The proposed ranking framework is employed in an empirical study of the EST selection problem, which shows its practicality and feasibility. In this study, the evaluation criteria are categorized into technical, environmental, social, economic, and performance dimensions with the DEs’ opinions. Comparative and sensitivity analyses are made to approve the validity and stability of the developed ranking approach. The present study offers valuable insights for choosing multi-criteria EST under an indeterminate, inconsistent, and uncertain environment, which also expands the application scopes of the combined MEREC-SWARA-WISP method.},
  archive      = {J_CC},
  author       = {Mishra, Arunodaya Raj and Pamucar, Dragan and Rani, Pratibha and Hezam, Ibrahim M.},
  doi          = {10.1007/s12559-025-10437-x},
  journal      = {Cognitive Computation},
  month        = {4},
  number       = {2},
  pages        = {1-22},
  shortjournal = {Cogn. Comput.},
  title        = {Single-valued neutrosophic distance measure-based MEREC-RANCOM-WISP for solving sustainable energy storage technology problem},
  volume       = {17},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Curriculum-guided self-supervised representation learning of dynamic heterogeneous networks. <em>CC</em>, <em>17</em>(2), 1-17. (<a href='https://doi.org/10.1007/s12559-025-10441-1'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Since most real-world network data include nodes and edges that evolve gradually, an embedding model for dynamic heterogeneous networks is crucial for network analysis. Transformer models have remarkable success in natural language processing but are rarely applied to learning representations of dynamic heterogeneous networks. In this study, we propose a new transformer model (DHG-BERT) that (i) constructs a dataset based on a network curriculum and (ii) includes pre/post-learning through self-supervised learning. Our proposed model learns complex relationships by leveraging an easier understanding of relationships through data reconstruction. Additionally, we use self-supervised learning to learn network structural features and temporal changes in structure and then fine-tune the proposed model by focusing on specific meta-paths by considering domain characteristics or target tasks. We evaluated the quality of the vector representation produced by the proposed transducer model using real bibliographic networks. Our model achieved an average accuracy of 0.94 in predicting research collaboration between researchers, outperforming existing models by a minimum of 0.13 and a maximum of 0.35. As a result, we confirmed that DHG-BERT is an effective transformer model tailored to dynamic heterogeneous network embeddings. Our study highlights the model’s ability to understand complex network relationships and appropriately capture the structural nuances and temporal changes inherent in networks. This study provides future research directions for applying the transformer model to real-world network data and a new approach to analyzing dynamic heterogeneous networks using transformers.},
  archive      = {J_CC},
  author       = {Jung, Namgyu and Camacho, David and Choi, Chang and Lee, O.-Joun},
  doi          = {10.1007/s12559-025-10441-1},
  journal      = {Cognitive Computation},
  month        = {4},
  number       = {2},
  pages        = {1-17},
  shortjournal = {Cogn. Comput.},
  title        = {Curriculum-guided self-supervised representation learning of dynamic heterogeneous networks},
  volume       = {17},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). $$A^{2}$$ DM: Enhancing EEG artifact removal by fusing artifact representation into the time-frequency domain. <em>CC</em>, <em>17</em>(2), 1-17. (<a href='https://doi.org/10.1007/s12559-025-10442-0'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The electroencephalogram (EEG) provides essential data for analyzing brain activities. However, artifacts such as electrooculography (EOG) and electromyography (EMG) often interleave with the EEG signals, significantly affecting the quality of EEG signal analysis. The heterogeneous distribution of these artifacts in the time-frequency domain makes it challenging to remove multiple artifacts using a unified model. In this paper, we propose an artifact-aware EEG denoising model, referred to as $$A^2$$ DM, to effectively remove various types of artifacts in a unified manner. We first obtain an artifact representation that indicates the type of artifact from a pre-trained artifact classification model. This artifact representation is then used as prior knowledge, which is fused into the denoising model in the time-frequency domain. This enables the model to become aware of the artifact type and precisely remove artifacts based on their type. Due to the heterogeneous distributions of artifacts in the frequency domain, we introduce a frequency enhancement module that can identify specific types of artifacts based on their representation and remove them using a hard attention mechanism. Additionally, we design a time-domain compensation module to enhance the denoising capability of $$A^2$$ DM by compensating for potential losses of global information. Comprehensive experiments demonstrate that $$A^2$$ DM significantly outperforms the novel CNN in denoising EEG signals, showing a notable 12% improvement in correlation coefficient (CC) metrics. This work demonstrates that artifact representation can be used in artifact removal models to effectively remove multiple types of artifacts.},
  archive      = {J_CC},
  author       = {Li, Haoran and Feng, Fan and Kang, Jiarong and Zhang, Jin and Gong, Xiaoli and Lu, Tingjuan and Li, Shuang and Sun, Zhe and Solé-Casals, Jordi},
  doi          = {10.1007/s12559-025-10442-0},
  journal      = {Cognitive Computation},
  month        = {4},
  number       = {2},
  pages        = {1-17},
  shortjournal = {Cogn. Comput.},
  title        = {$$A^{2}$$ DM: Enhancing EEG artifact removal by fusing artifact representation into the time-frequency domain},
  volume       = {17},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Comparative analysis of hybrid and ensemble machine learning approaches in predicting football player transfer values. <em>CC</em>, <em>17</em>(2), 1-25. (<a href='https://doi.org/10.1007/s12559-025-10443-z'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In football economics, a player’s transfer market value extends beyond performance metrics, with popularity playing a crucial role in clubs’ decisions. Reputation indexes, reflecting a player’s standing in the industry, are derived from various sources. Traditional metrics include goals, assists, and defensive prowess, while social media activity (likes on Facebook and Instagram), press citations, and Wikipedia page views add a new dimension. This study utilized Fédération Internationale de Football Association 19 data and a real-world statistical dataset, encompassing 54 features for 491 players across various leagues. After adding valuable data and removing ineffective features and outliers, two filtering-based feature selection methods identified the 20 most critical features for predicting market value. The study applied Extreme Gradient Boosting and Adaptive Boosting regression models, along with their hybrid forms optimized by metaheuristic algorithms. The Extreme Gradient Boosting optimized with the Ali Baba and Forty Thieves algorithm model showed the best performance, with a 99% match to actual values and a misestimation of around €1.9 million. Ensemble models, averaging predictions from all hybrid models, provided reliable market value estimates. These insights help managers make informed decisions to improve team performance and secure financial benefits for the club.},
  archive      = {J_CC},
  author       = {Zhang, Wenjing and Cao, Dan},
  doi          = {10.1007/s12559-025-10443-z},
  journal      = {Cognitive Computation},
  month        = {4},
  number       = {2},
  pages        = {1-25},
  shortjournal = {Cogn. Comput.},
  title        = {Comparative analysis of hybrid and ensemble machine learning approaches in predicting football player transfer values},
  volume       = {17},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A multi-scale feature fusion network focusing on small objects in UAV-view. <em>CC</em>, <em>17</em>(2), 1-17. (<a href='https://doi.org/10.1007/s12559-025-10445-x'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Small object detection in unmanned aerial vehicle (UAV)-view images remains a significant challenge due to the irregular distribution of small objects and their representation by only a few pixels. The performance of state-of-the-art object detectors tends to be suboptimal in these scenarios, primarily because these systems are not finely tuned to the unique attributes of small object detection. Specifically, we observe that although some existing object detectors introduce multi-scale feature fusion modules to improve detection accuracy, these conventional methods frequently overlook the weight relationship between the object and the background. This oversight results in the diminished importance of small objects in deep feature maps. Additionally, the widely used intersection over union (IoU) metric, along with its variants, is particularly sensitive to the positional inaccuracies of small objects. This sensitivity significantly impairs the efficacy of label assignment in anchor-based detectors. To address these challenges, we introduce AFF-YOLO, a novel detector inspired by the network architecture of YOLOv8 that is specifically tailored to amplify the detection capabilities for small objects in aerial imagery. In particular, we propose an attention feature fusion module (AFFM) to enhance feature representation at each layer. Second, we propose a small object feature layer (SOFL) to better integrate semantic and localization features across different scales. Furthermore, we propose a triangular centroid-based IoU loss (TriC-IoU loss) as part of the bounding box regression loss, which is more suitable for calculating this loss in small object detection. The experimental results show that the proposed method reaches a detection accuracy of 52.5% $$\textbf{mAP}_{\textbf{50}}$$ on VisDrone2019, marking a notable improvement of 30.6% over the existing YOLO-based object detectors.},
  archive      = {J_CC},
  author       = {Li, Jiantao and Yu, Chenbin and Wei, Wenhui and Li, Jiadong and Huang, Kaizhu and Hussian, Amir and Liu, Xin and Zhou, Yangfan},
  doi          = {10.1007/s12559-025-10445-x},
  journal      = {Cognitive Computation},
  month        = {4},
  number       = {2},
  pages        = {1-17},
  shortjournal = {Cogn. Comput.},
  title        = {A multi-scale feature fusion network focusing on small objects in UAV-view},
  volume       = {17},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Investigating brain lobe biomarkers to enhance dementia detection using EEG data. <em>CC</em>, <em>17</em>(2), 1-17. (<a href='https://doi.org/10.1007/s12559-025-10447-9'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Dementia is a growing global health concern that significantly impacts the quality of life for millions of individuals and imposes substantial burdens on healthcare systems. Early detection and accurate diagnosis are crucial for effective dementia management. Electroencephalography (EEG) has emerged as a non-invasive tool for identifying dementia-related abnormalities and assessing brain function. However, existing EEG-based methods often fail to pinpoint specific biomarkers, particularly brain lobe changes. Brain lobe analysis in EEG is essential for advancing dementia detection and improving diagnostic accuracy. This study aims to address this gap by exploring key brain lobes involved in dementia detection and classification, focusing on Alzheimer’s disease (AD) and Frontotemporal dementia (FTD). We introduce a Short-Time Fourier Transform to generate spectrogram images from EEG signals combined with Convolutional Neural Networks to identify the most critical brain lobes for enhanced dementia detection. We have applied Grad-CAM method to improve result interpretability and offer meaningful insights to the research community. Our experiments on OpenNeuro ds004504 EEG dataset for AD and FTD indicate that the parietal lobe exhibits the most significant changes in both conditions, achieving 95.72% accuracy for FTD and 92.25% for AD, followed by the temporal and frontal lobes. When applying the proposed framework to the entire brain region, we achieved 95.59% accuracy for AD and 93.14% for FTD. The findings from EEG-based brain lobe analysis aid experts in improving diagnostic and monitoring tools for neurodegenerative disorders, thereby advancing the understanding and clinical management of dementias like AD and FTD.},
  archive      = {J_CC},
  author       = {Siuly, Siuly and Tawhid, Md.Nurul Ahad and Li, Yan and Acharya, Rajendra and Sadiq, Muhammad Tariq and Wang, Hua},
  doi          = {10.1007/s12559-025-10447-9},
  journal      = {Cognitive Computation},
  month        = {4},
  number       = {2},
  pages        = {1-17},
  shortjournal = {Cogn. Comput.},
  title        = {Investigating brain lobe biomarkers to enhance dementia detection using EEG data},
  volume       = {17},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A new similarity measure for picture fuzzy sets and its various applications. <em>CC</em>, <em>17</em>(2), 1-21. (<a href='https://doi.org/10.1007/s12559-025-10449-7'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Similarity measures offer a useful way to assess how similar two collections of things are to one another. They are also useful tools for handling cognitively inspired decision-making problems. Compared to intuitionistic fuzzy sets, the picture fuzzy set theory offers advantages for representing ambiguous and uncertain concepts in practical contexts. This is because picture fuzzy sets take into account the degree of neutrality of a factor that is crucial in many different decision-making scenarios, such as human voting, personnel selection, and medical diagnosis. The similarity measurements are crucial when comparing two picture fuzzy sets. Numerous studies on picture fuzzy sets’ similarity measurements are available in the literature. All these similarity measures, however, produce irrational outcomes in the majority of the issues such as satisfying the axiomatic requirements, computation of similarity between different picture fuzzy sets, and classifying an unknown pattern into one of the known patterns. Therefore, we propose a novel similarity measure based on the inverse tangent function for picture fuzzy sets in this study that is more efficient than all existing similarity measures. We also demonstrate its utility in classification and medical diagnostic problems and contrast its performance with the available ones. At last, we suggest a new decision-making technique in a picture fuzzy setting that is more robust than the technique for order preference by similarity to the ideal solution.},
  archive      = {J_CC},
  author       = {Chammam, Wathek and Ganie, Abdul Haseeb and Saeed, Maha Mohammed and Sief, Amira M. and Khalaf, Mohammad M.},
  doi          = {10.1007/s12559-025-10449-7},
  journal      = {Cognitive Computation},
  month        = {4},
  number       = {2},
  pages        = {1-21},
  shortjournal = {Cogn. Comput.},
  title        = {A new similarity measure for picture fuzzy sets and its various applications},
  volume       = {17},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). EPDTNet + -EM: Advanced transfer learning and SubNet architecture for medical image diagnosis. <em>CC</em>, <em>17</em>(2), 1-22. (<a href='https://doi.org/10.1007/s12559-025-10446-w'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper presents a novel medical imaging framework, Efficient Parallel Deep Transfer SubNet+-based Explainable Model (EPDTNet + -EM), designed to improve the detection and classification of abnormalities in medical images. The model integrates Enhanced Exponential Transfer Learning (En-ETL) and the Parallel SubNet (PSNet +) architecture, addressing challenges like overfitting, high computational cost, and limited generalization across complex datasets. En-ETL accelerates learning and enhances precision, while PSNet + balances computational efficiency and performance through parallel convolutional layers and an axis attention mechanism. The proposed framework preprocesses medical images using resizing, noise removal, and contrast enhancement, ensuring high-quality inputs for analysis. For classification, the PSNet + model categorizes medical images into normal and abnormal cases. Explainable AI (XAI) is applied to highlight key features in classified images, improving interpretability and clinical decision-making. Experimental results validate the framework on various datasets, including MRI, CT, X-ray, and mammography images. The proposed model achieves a classification accuracy of 98.83%, a false positive rate of 2%, and an execution time of 5.3 ms. Comparative Analysis show superior performance over existing approaches in classification accuracy and efficiency. Ablation studies confirmed the significant contributions of the proposed model using each component in enhancing performance.},
  archive      = {J_CC},
  author       = {K, Dhivya and K, Sangamithrai and S, Indra Priyadharshini and M, Vedaraj},
  doi          = {10.1007/s12559-025-10446-w},
  journal      = {Cognitive Computation},
  month        = {4},
  number       = {2},
  pages        = {1-22},
  shortjournal = {Cogn. Comput.},
  title        = {EPDTNet + -EM: Advanced transfer learning and SubNet architecture for medical image diagnosis},
  volume       = {17},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Cognitive-inspired spectral spatiotemporal analysis for emotion recognition utilizing electroencephalography signals. <em>CC</em>, <em>17</em>(1), 1-12. (<a href='https://doi.org/10.1007/s12559-024-10361-6'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The rapid advancement of computer technologies, along with the significant role of emotions in daily life, has driven interest in intelligent emotion recognition systems. Electroencephalography (EEG) serves as a prominent objective tool in affective computing. However, effectively integrating multichannel EEG spatial and temporal information remains a critical challenge. This study introduces a novel emotion recognition model grounded in cognitive and biological principles, emphasizing the importance of spatiotemporal dynamics in emotional processing. In this research, brain frequency bands were extracted through wavelet analysis, and the signals within predefined time windows were quantified. These features were then concatenated across distinct brain channels to create a comprehensive matrix representing spatiotemporal brain information. The matrix was characterized using both the summation of matrix cells and the highest singular value to optimize computational costs during classification. The resulting attributes were input into a classification module for emotion detection. Experimental results on the Database for Emotion Analysis using Physiological Signals (DEAP) achieved a maximum accuracy of 89.55%. This work introduces a novel approach to analyzing and classifying EEG signals elicited by various emotional stimuli, demonstrating that the proposed model is competitive with the state-of-the-art classification schemes, thereby paving the way for future development of a robust spatiotemporal-based EEG emotion recognition system.},
  archive      = {J_CC},
  author       = {Goshvarpour, Atefeh and Goshvarpour, Ateke},
  doi          = {10.1007/s12559-024-10361-6},
  journal      = {Cognitive Computation},
  month        = {2},
  number       = {1},
  pages        = {1-12},
  shortjournal = {Cogn. Comput.},
  title        = {Cognitive-inspired spectral spatiotemporal analysis for emotion recognition utilizing electroencephalography signals},
  volume       = {17},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A levitated controlled attention for named entity recognition. <em>CC</em>, <em>17</em>(1), 1-12. (<a href='https://doi.org/10.1007/s12559-024-10381-2'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Controlled attention is a mechanism developed in cognitive neuroscience. It has been successfully applied to support named entity recognition, where the start and end boundaries of a possible named entity are marked by two specific tokens to indicate its position in a sentence. Then, it is fed into a deep network for classification. The entity boundary markers enable a deep neural network to be aware of entity boundaries and build the contextual dependency of a sentence relevant to entity boundaries. The problem with this strategy is that every possible named entity must be evaluated independently. This leads to very high computational complexity and cannot construct the semantic dependency between different named entities. In this paper, a levitated controlled attention mechanism is presented for named entity recognition. In this method, all possible named entities are fed together into a deep network for one-pass classification, which can establish the semantic dependency between contextual features and possible named entities. In the experiments, the levitated controlled attention is evaluated on four public datasets. The results show that it not only considerably reduces the computational complexity but also improves the performance of named entity recognition.},
  archive      = {J_CC},
  author       = {Huang, Rong and Chen, Yanping and Huang, Ruizhang},
  doi          = {10.1007/s12559-024-10381-2},
  journal      = {Cognitive Computation},
  month        = {2},
  number       = {1},
  pages        = {1-12},
  shortjournal = {Cogn. Comput.},
  title        = {A levitated controlled attention for named entity recognition},
  volume       = {17},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A graph neural network with spatial attention for emotion analysis. <em>CC</em>, <em>17</em>(1), 1-12. (<a href='https://doi.org/10.1007/s12559-024-10358-1'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Emotion recognition plays a crucial role in the diagnosis and treatment of various mental disorders. Research studies revealed the close relationship between brain regions and their functional roles in emotions. Propose a learning method that extends graph neural networks and takes into account the spatial relationship between EEG channels and their contributions of different regions of the brain to human emotions. Our method uses the adjacency matrix to model the spatial topological relationships in multi-channel EEG signals and learns weights to adjust their contributions to the classification. Extensive evaluation is conducted using public data sets, including comparison studies with state-of-the-art methods and performance analysis. In our comparison studies, our method demonstrates superior performance in terms of average accuracy. It is demonstrated that the proposed method improves the accuracy of emotion recognition and analyzes the brain at a fine granularity to decide the part that is most related to the triggering of the emotion.},
  archive      = {J_CC},
  author       = {Chen, Tian and Li, Lubao and Yuan, Xiaohui},
  doi          = {10.1007/s12559-024-10358-1},
  journal      = {Cognitive Computation},
  month        = {2},
  number       = {1},
  pages        = {1-12},
  shortjournal = {Cogn. Comput.},
  title        = {A graph neural network with spatial attention for emotion analysis},
  volume       = {17},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Combining interpretable embedded multicriteria feature cross-selection engineering and machine learning to mimic the brain for stock trading signal prediction. <em>CC</em>, <em>17</em>(1), 1-21. (<a href='https://doi.org/10.1007/s12559-024-10365-2'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Stock trading signal prediction is very important for investors’ trading decisions. However, since the stock market is a complex and nonlinear system, stock trading is frequent and complex. Human beings cannot integrate all the relevant information in time and make the right decisions by their brains alone. Machine learning can mimic the brain, learn from experience, and discover the connection between different things, thus realizing correct prediction and decision-making. Therefore, this study innovatively proposes a fusion of interpretable embedded multicriteria feature cross-selection engineering to capture effective features. Meanwhile, an optimized neural network prediction model is proposed where the Bayesian (BO) algorithm assumes the task of searching for hyperparameter combinations. The methods are as follow: (1) Daily stock prices are categorized into four types of key points for stock trading signals based on the time series extreme point algorithm. (2) A more comprehensive range of impact factors is constructed. Starting from the stock’s historical trading data, based on the stock’s trend, volatility, and turnover flow, five categories of technical indicators are constructed: Overlap Study, Momentum Indicator, Momentum Indicator, Volatility Indicator, and Price Conversion. (3) To construct a feature cross-selection method with multiple feature screening criteria to find the optimal feature influencing factors from different evaluation dimensions. (4) The hyper-parameters of the Artificial Neural Network (ANN) are optimized using Bayesian optimization algorithm. The optimized ANN is then used to model the data and obtain predictions. Twenty stocks were randomly selected from Shanghai Stock Exchange and Shenzhen Stock Exchange as experimental data to verify the validity of the model. The accuracy of the model proposed in this paper is 54.83%, 55.46%, and 54.70% for stocks with upward, steady, and downward trends respectively. The accuracy is on average 7.93%, 8.09%, and 8.09% higher than the comparison model. The return on investment through the predicted results of the model is 21.87%, 7.76%, and −3.51% respectively, which is better than the other comparative models. It can be seen from the experiments that the feature cross-selection method with multi-feature screening criterion can help the model to better find the optimal feature influencing factor, which helps to improve the accuracy of prediction. The Bayesian optimization algorithm contributes to the performance improvement of the ANN. After modeling the features using the Bayesian optimized ANN, the stock trading signal prediction model proposed in this paper is significantly better than other prediction models.},
  archive      = {J_CC},
  author       = {Wang, Jujie and Dong, Ying},
  doi          = {10.1007/s12559-024-10365-2},
  journal      = {Cognitive Computation},
  month        = {2},
  number       = {1},
  pages        = {1-21},
  shortjournal = {Cogn. Comput.},
  title        = {Combining interpretable embedded multicriteria feature cross-selection engineering and machine learning to mimic the brain for stock trading signal prediction},
  volume       = {17},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Density estimation-based stein variational gradient descent. <em>CC</em>, <em>17</em>(1), 1-12. (<a href='https://doi.org/10.1007/s12559-024-10370-5'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Approximating a target distribution, such as a Bayesian posterior, is important in many areas, including cognitive computation. We introduce a variant of Stein variational gradient descent (SVGD) (Liu and Wang Adv Neural Inf Process Syst 29, 2016), called the density estimation-based Stein variational gradient descent (DESVGD). SVGD has proven to be promising as a sampling method for approximating target distributions. SVGD, however, suffers from discontinuity inherent in the empirical measure, making it difficult to closely monitor the convergence of the sampling-based approximation to the target. DESVGD utilizes kernel density estimation to replace the empirical measure in SVGD with its continuous counterpart. This allows direct computation of the KL divergence between the current approximation and the target distribution, thereby helping to monitor the numerical convergence of the iterative optimization process. DESVGD also offers derivatives of the KL divergence, which can be used to better design learning rates and thus to achieve faster convergence. By simply replacing the kernel used in SVGD with its weighted average, one can easily implement DESVGD based on existing SVGD algorithms. Our numerical experiments demonstrate that DESVGD approximates the target distribution well and outperforms the original SVGD in terms of approximation quality.},
  archive      = {J_CC},
  author       = {Kim, Jeongho and Lee, Byungjoon and Min, Chohong and Park, Jaewoo and Ryu, Keunkwan},
  doi          = {10.1007/s12559-024-10370-5},
  journal      = {Cognitive Computation},
  month        = {2},
  number       = {1},
  pages        = {1-12},
  shortjournal = {Cogn. Comput.},
  title        = {Density estimation-based stein variational gradient descent},
  volume       = {17},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). HC3: A three-way clustering method based on hierarchical clustering. <em>CC</em>, <em>17</em>(1), 1-14. (<a href='https://doi.org/10.1007/s12559-024-10379-w'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Three-way decision is a field of research pertaining to human-inspired computation. Guided by the principle of three-way decision, three-way clustering addresses the information uncertainty problem by using the core region and the fringe region to characterize a cluster. The universe is split into three parts by these two regions, which capture three kinds of relationships between objects and a cluster, namely, belonging to, partially belonging to, and not belonging to. In recent years, there have been considerable three-way clustering algorithms. However, the generalization and scalability of current three-way cluster algorithms remain relatively weak, with most algorithms adhering to a fixed allocation strategy or fixed threshold parameters. In order to overcome this problem, this paper proposes a multilevel three-way clustering algorithm based on a hierarchical strategy (HC3 for short). The proposed algorithm uses kernel density estimation information of data to adaptively construct a multilevel structure of data, where the higher levels (or the internal layers) with the high-density objects are closer to core regions of clusters, and the lower levels (or the external layers) with the low-density objects are closer to fringe regions of clusters. Under the multilevel structure, we establish a three-way allocation strategy based on the stability of subclass clusters, obtaining the correct attribution of data after fully considering neighboring information. The experiments are conducted on 13 data sets with different dimensions. By comparing to other 8 clustering algorithms, the effectiveness of the proposed HC3 is verified through accuracy (ACC), adjusted Rand index (ARI), and adjusted mutual information (AMI).},
  archive      = {J_CC},
  author       = {Guan, Wenrui and Wang, Pingxin and Jiang, Wengang and Zhang, Ying},
  doi          = {10.1007/s12559-024-10379-w},
  journal      = {Cognitive Computation},
  month        = {2},
  number       = {1},
  pages        = {1-14},
  shortjournal = {Cogn. Comput.},
  title        = {HC3: A three-way clustering method based on hierarchical clustering},
  volume       = {17},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A novel multi-head attention and long short-term network for enhanced inpainting of occluded handwriting. <em>CC</em>, <em>17</em>(1), 1-14. (<a href='https://doi.org/10.1007/s12559-024-10382-1'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the domain of handwritten character recognition, inpainting occluded offline characters is essential. Relying on the remarkable achievements of transformers in various tasks, we present a novel framework called “Enhanced Inpainting with Multi-head Attention and stacked long short-term memory (LSTM) Network” (E-Inpaint). This framework aims to restore occluded offline handwriting while capturing its online signal counterpart, enriched with dynamic characteristics. The proposed approach employs Convolutional Neural Network (CNN) and Multi-Layer Perceptron (MLP) in order to extract essential hidden features from the handwriting image. These features are then decoded by stacked LSTM with Multi-head Attention, achieving the inpainting process and generating the online signal corresponding to the uncorrupted version. To validate our work, we utilize the recognition system Beta-GRU on Latin, Indian, and Arabic On/Off dual datasets. The obtained results show the efficiency of using stacked-LSTM network with multi-head attention, enhancing the quality of the restored image and significantly improving the recognition rate using the innovative Beta-GRU system. Our research mainly highlights the potential of E-Inpaint in enhancing handwritten character recognition systems.},
  archive      = {J_CC},
  author       = {Rabhi, Besma and Elbaati, Abdelkarim and Hamdi, Yahia and Dhahri, Habib and Pal, Umapada and Chabchoub, Habib and Ouahada, Khmaies and Alimi, Adel M.},
  doi          = {10.1007/s12559-024-10382-1},
  journal      = {Cognitive Computation},
  month        = {2},
  number       = {1},
  pages        = {1-14},
  shortjournal = {Cogn. Comput.},
  title        = {A novel multi-head attention and long short-term network for enhanced inpainting of occluded handwriting},
  volume       = {17},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A survey of artificial neural network computing systems. <em>CC</em>, <em>17</em>(1), 1-20. (<a href='https://doi.org/10.1007/s12559-024-10383-0'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {An artificial neural network (ANN) is currently used in multiple different applications such as bio-medicine, finance, Internet, and mobile networks. Since their inception, many advances have taken place introducing new models and features. Such progress resulted in different ANN models and most importantly different types of implementation, which vary from software (SW) to hardware (HW) following specific development principles. Researchers have been working significantly the last decade in this area tackling with different aspects of ANN’s implementations. In this survey, we present the progress of ANN in terms of implementation as part of computing platforms. Thus, we present the ANN-enabled computing platforms in terms of algorithmic models, computing architectures, and SW/HW implementations. This work concludes with open challenges and lessons learned in order to summarize what is potentially useful for further research in the area of ANN computing platforms with a wide spectrum of applications. An artificial neural network (ANN) is considered the key element of future computing systems applied to different domains. While the algorithmic design of an ANN is one of the major engineering elements, the implementation of ANN is equally important with many difficulties that should be overcome by future engineers. This survey aims to provide a comprehensive tutorial about the ANN-enabled computing systems, i.e., computing architectures with embedded artificial intelligence (AI). Starting with the ANN models and their applications, the survey provides a taxonomy of the types of ANN computing systems. Both SW and HW implementations are provided for each of those types, which highlight the key architectural elements as well as the performance of the ANN-enabled computing systems. Open challenges and lessons learned follow to provide a discussion for future research in the area of AI computing systems.},
  archive      = {J_CC},
  author       = {Foukalas, Fotis},
  doi          = {10.1007/s12559-024-10383-0},
  journal      = {Cognitive Computation},
  month        = {2},
  number       = {1},
  pages        = {1-20},
  shortjournal = {Cogn. Comput.},
  title        = {A survey of artificial neural network computing systems},
  volume       = {17},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A novel multi-type image coding method acting on supervised hierarchical deep spiking convolutional neural networks for image classification. <em>CC</em>, <em>17</em>(1), 1-17. (<a href='https://doi.org/10.1007/s12559-024-10355-4'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Spiking neural networks (SNNs) have gained significant momentum in recent times as they transmit information via discrete spikes, similar to neuromorphic low-power systems. However, existing spike coding methods are often limited to a single scale of time or rate, and typically suffer from drawbacks such as reduced accuracy or long classification latency. In this paper, we propose a pixel-based multi-type image coding (PMIC) method inspired by the functional organization of primate visual systems to address the issues at hand. The encoded information comprises both spatial and temporal details, represented by spiking firing time and intensity, respectively. Subsequently, we combine the spiking firing time and intensity as inputs of a hierarchical spiking convolutional neural network (SCNN) including several convolutional and pooling layers. During the training phase, we use error backpropagation to optimize parameters. Comparison of experimental results with some state-of-the-art approaches on MNIST dataset, Fashion-MNIST dataset as well as ETH-80 dataset of image classification demonstrates that SCNN using PMIC can achieve the best test accuracy, which is 99.13%, 90.31%, and 94.29%, respectively. The proposed PMIC utilizes multiple filters and coding strategies to extract multi-type information and is more beneficial to the performance of SNNs compared to methods that extract single-scale or single-type information.},
  archive      = {J_CC},
  author       = {Liu, Fang and Xu, Jialin and Yang, Jie and Wu, Wei},
  doi          = {10.1007/s12559-024-10355-4},
  journal      = {Cognitive Computation},
  month        = {2},
  number       = {1},
  pages        = {1-17},
  shortjournal = {Cogn. Comput.},
  title        = {A novel multi-type image coding method acting on supervised hierarchical deep spiking convolutional neural networks for image classification},
  volume       = {17},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). CRCFusionAICADx: Integrative CNN-LSTM approach for accurate colorectal cancer diagnosis in colonoscopy images. <em>CC</em>, <em>17</em>(1), 1-37. (<a href='https://doi.org/10.1007/s12559-024-10357-2'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Colorectal cancer (CRC) is a critical health issue worldwide and is very treatable if diagnosed on time. This paper proposes an innovative CADx system, namely CRCFusionAICADx, that enhances the efficiency of diagnosis by fusing CNNs with LSTM networks and feature integration techniques. Using data from the CKHK-22 colonoscopy image dataset, we preprocess the images into grayscale first and then apply LBP analysis for emphasizing textural features. These are further analyzed using three different pre-trained CNN models: VGG16, DenseNet-201, and ResNet50. These were chosen because of their complementary feature extraction capabilities. The resultant features from grayscale, LBP, and raw images will be fused to create an integrated dataset. To increase variability in the dataset and reduce overfitting for the network, we decided to apply a series of data augmentation techniques, which included zooming in, rotation, and horizontal flipping. By doing so, we expanded the dataset into 57,148 images. This augmented dataset is then used to train a model, RDV-22, which includes an integration of the architectures of VGG16, DenseNet-201, and ResNet50, with CNN and CNN + LSTM layers. The LSTM network learns the temporal dependencies of frames in a sequence and hence allows for more sensitive and specific detection of CRC. CRCFusionAICADx produces very impressive results, where the RDV-22 model produces a testing accuracy of 90.81%, precision of 91.00%, recall of 90.00%, and an F1 score of 90.49% in its results. This gives the model an ROC AUC of 0.98, reflecting very strong discriminatory power. This integrative approach thus shows tremendous promise for early CRC detection by offering a strong diagnostic tool that integrates both spatial and temporal features into a new standard in clinical diagnostics.},
  archive      = {J_CC},
  author       = {Raju, Akella S. Narasimha and Jayavel, Kayalvizhi and Rajalakshmi, Thulasi and Rajababu, M.},
  doi          = {10.1007/s12559-024-10357-2},
  journal      = {Cognitive Computation},
  month        = {2},
  number       = {1},
  pages        = {1-37},
  shortjournal = {Cogn. Comput.},
  title        = {CRCFusionAICADx: Integrative CNN-LSTM approach for accurate colorectal cancer diagnosis in colonoscopy images},
  volume       = {17},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Computer-aided diagnosis of graphomotor difficulties utilizing direction-based fractional order derivatives. <em>CC</em>, <em>17</em>(1), 1-19. (<a href='https://doi.org/10.1007/s12559-024-10360-7'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Children who do not sufficiently develop graphomotor skills essential for handwriting often develop graphomotor disabilities (GD), impacting the self-esteem and academic performance of the individual. Current examination methods of GD consist of scales and questionaries, which lack objectivity, rely on the perceptual abilities of the examiner, and may lead to inadequately targeted remediation. Nowadays, one way to address the factor of subjectivity is to incorporate supportive machine learning (ML) based assessment. However, even with the increasing popularity of decision-support systems facilitating the diagnosis and assessment of GD, this field still lacks an understanding of deficient kinematics concerning the direction of pen movement. This study aims to explore the impact of movement direction on the manifestations of graphomotor difficulties in school-aged. We introduced a new fractional-order derivative-based approach enabling quantification of kinematic aspects of handwriting concerning the direction of movement using polar plot representation. We validated the novel features in a barrage of machine learning scenarios, testing various training methods based on extreme gradient boosting trees (XGBboost), Bayesian, and random search hyperparameter tuning methods. Results show that our novel features outperformed the baseline and provided a balanced accuracy of 87 % (sensitivity = 82 %, specificity = 92 %), performing binary classification (children with/without graphomotor difficulties). The final model peaked when using only 43 out of 250 novel features, showing that XGBoost can benefit from feature selection methods. Proposed features provide additional information to an automated classifier with the potential of human interpretability thanks to the possibility of easy visualization using polar plots.},
  archive      = {J_CC},
  author       = {Gavenciak, Michal and Mucha, Jan and Mekyska, Jiri and Galaz, Zoltan and Zvoncakova, Katarina and Faundez-Zanuy, Marcos},
  doi          = {10.1007/s12559-024-10360-7},
  journal      = {Cognitive Computation},
  month        = {2},
  number       = {1},
  pages        = {1-19},
  shortjournal = {Cogn. Comput.},
  title        = {Computer-aided diagnosis of graphomotor difficulties utilizing direction-based fractional order derivatives},
  volume       = {17},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A fixed-time control algorithm for multiagent systems with input delay via event-triggered strategy. <em>CC</em>, <em>17</em>(1), 1-12. (<a href='https://doi.org/10.1007/s12559-024-10366-1'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multiagent systems have become increasingly relevant in numerous fields due to their potential for cooperative control and distributed networks. Developing effective adaptive algorithms for these systems is crucial, as adaptive techniques can enhance the ability of multiple agents to collaboratively complete specific tasks. In this paper, we propose an event-triggered adaptive fixed-time containment algorithm for multiagent systems with input delay. Firstly, our methodology employs Fuzzy Logic Systems to approximate uncertain terms in system dynamics and addresses the algebraic loop problem inherent in non-strict feedback functions. Additionally, an integral term is introduced to mitigate the adverse effects of time delay on system performance. Under the fixed-time stability framework, the proposed algorithm achieves containment within an exact time constant, independent of initial conditions. Furthermore, a dynamic event-triggered mechanism is incorporated to optimize the number of triggers, thereby conserving communication resources. As a result, the designed control algorithm guarantees that the closed-loop systems signals meet the criteria for semi-global practical fixed-time stability, allowing the outputs of followers to converge to the convex hull of leaders within a finite time. Finally, the feasibility and effectiveness of the algorithm are demonstrated through simulations involving underwater vehicle systems.},
  archive      = {J_CC},
  author       = {Liu, Guijiang and Wang, Xin and Guang, Weiwei and Chen, Hongyu},
  doi          = {10.1007/s12559-024-10366-1},
  journal      = {Cognitive Computation},
  month        = {2},
  number       = {1},
  pages        = {1-12},
  shortjournal = {Cogn. Comput.},
  title        = {A fixed-time control algorithm for multiagent systems with input delay via event-triggered strategy},
  volume       = {17},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A hybrid semantics and syntax-based graph convolutional network for aspect-level sentiment classification. <em>CC</em>, <em>17</em>(1), 1-19. (<a href='https://doi.org/10.1007/s12559-024-10367-0'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Aspect-level sentiment classification seeks to ascertain the sentiment polarities of individual aspects within a sentence. Most existing research in this field focuses on individually assessing the importance of contexts on individual aspects, disregarding the negative impact of imbalanced relations between aspects due to their mutual influence. This paper presents a hybrid semantics and syntax-based graph convolutional network (SS-GCN) for aspect-level sentiment classification. This model addresses the imbalanced limitation by creating aspects-based balance relations between the strengths and weaknesses of different aspects through an auxiliary task. Furthermore, the multi-head self-attention mechanism utilizes position-enhanced encoding to identify the most relevant aspects of the current word. Extensive experiments demonstrate that SS-GCN outperforms other baselines in terms of classification performance. Compared to state-of-the-art methods, SS-GCN significantly improves 0.39–1.66% in accuracy and 0.43–1.92% in Macro-F1 on the SemEval 14-15 and MAMS datasets.},
  archive      = {J_CC},
  author       = {Huang, Chen and Li, Xianyong and Du, Yajun and Dong, Zhicheng and Huang, Dong and Kumar Jain, Deepak and Hussain, Amir},
  doi          = {10.1007/s12559-024-10367-0},
  journal      = {Cognitive Computation},
  month        = {2},
  number       = {1},
  pages        = {1-19},
  shortjournal = {Cogn. Comput.},
  title        = {A hybrid semantics and syntax-based graph convolutional network for aspect-level sentiment classification},
  volume       = {17},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). DmrNet: Dual-stream mutual information contraction and re-discrimination network for semi-supervised temporal action detection. <em>CC</em>, <em>17</em>(1), 1-18. (<a href='https://doi.org/10.1007/s12559-024-10374-1'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Semi-supervised temporal action detection only requires a small number of labeled samples from the dataset and utilizes the remaining unlabeled samples for model training, effectively alleviating the significant time and manpower costs associated with annotating large-scale temporal action detection datasets. However, previous semi-supervised temporal action detection methods relied on sequential action localization and classification, which leads to erroneous localization predictions that can easily affect subsequent classification predictions, resulting in error propagation problem. To overcome error propagation, we propose a dual-stream mutual information contraction and re-discrimination network (DmrNet). Specifically, the traditional two-step strategy of temporal action detection has been changed to a four-step parallel strategy by us. Firstly, this paper designs the first-step classification prediction and the second-step localization prediction as a parallel structure to prevent error propagation from localization to classification. Then, in the third step, the dual-stream mutual information contraction part maps the dual-stream features to a new vector space to ensure the cross-correlation between classification and action localization. Finally, the fourth step of classification re-discrimination part captures the consistency information of the dual-stream structure to enhance internal representation. Compared with existing methods, DmrNet achieved an average accuracy improvement of 10.7% on ActivityNet v1.3 and 5.2% on THUMOS14 using only 10% annotation data. The experimental results show that the proposed DmrNet not only achieves good detection performance in semi-supervised learning but also achieves performance comparable to state-of-the-art methods in fully supervised learning.},
  archive      = {J_CC},
  author       = {Zhang, Qiming and Hu, Zhengping and Wang, Yulu and Bi, Shuai and Zhang, Hehao and Di, Jirui},
  doi          = {10.1007/s12559-024-10374-1},
  journal      = {Cognitive Computation},
  month        = {2},
  number       = {1},
  pages        = {1-18},
  shortjournal = {Cogn. Comput.},
  title        = {DmrNet: Dual-stream mutual information contraction and re-discrimination network for semi-supervised temporal action detection},
  volume       = {17},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Event classification on subsea pipeline inspection data using an ensemble of deep learning classifiers. <em>CC</em>, <em>17</em>(1), 1-23. (<a href='https://doi.org/10.1007/s12559-024-10377-y'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Subsea pipelines are the backbone of the modern oil and gas industry, transporting a total of 28% of global oil production. Due to several factors, such as corrosion or deformations, the pipelines might degrade over time, which might lead to serious economic and environmental damages if not addressed promptly. Therefore, it is crucial to detect any serious damage to subsea pipelines before they cause dangerous catastrophes. Inspections of subsea pipelines are usually made using a Remote Operating Vehicle and the inspection data is usually processed manually, which is subject to human errors, and requires experienced Remote Operating Vehicle operators. It is thus necessary to automate the inspection process to enable more efficiency as well as reduce costs. Besides, it is recognised that specific challenges of noisy and low-quality inspection data arising from the underwater environment prevent the industry from taking full advantage of the recent development in the Artificial Intelligence field to the problem of subsea pipeline inspection. In this paper, we developed an ensemble of deep learning classifiers to further improve the performance of single deep learning models in classifying anomalous events on the subsea pipeline inspection data. The output of the proposed ensemble was combined based on a weighted combining method. The weights of base classifiers were found by minimising the difference between the weighted combining result and the given associated ground truth annotation information. Three inspection datasets, gathered from different oil and gas companies in the United Kingdom, were analysed. These datasets were recorded under varying conditions and include a range of anomalies. The results showed that the proposed ensemble achieves around 78% accuracy on two datasets and more than 99% accuracy on one dataset, which is better compared to base classifiers and two popular ensembles.},
  archive      = {J_CC},
  author       = {Dang, Truong and Nguyen, Tien Thanh and Liew, Alan Wee-Chung and Elyan, Eyad},
  doi          = {10.1007/s12559-024-10377-y},
  journal      = {Cognitive Computation},
  month        = {2},
  number       = {1},
  pages        = {1-23},
  shortjournal = {Cogn. Comput.},
  title        = {Event classification on subsea pipeline inspection data using an ensemble of deep learning classifiers},
  volume       = {17},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). DisTrack: A new tool for semi-automatic misinformation tracking in online social networks. <em>CC</em>, <em>17</em>(1), 1-18. (<a href='https://doi.org/10.1007/s12559-024-10378-x'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article introduces DisTrack, a methodology and a tool developed for tracking and analyzing misinformation within online social networks (OSNs). DisTrack is designed to combat the spread of misinformation through a combination of natural language processing (NLP) social network analysis (SNA) and graph visualization. The primary goal is to detect misinformation, track its propagation, identify its sources, and assess the influence of various actors within the network. DisTrack’s architecture incorporates a variety of methodologies including keyword search, semantic similarity assessments, and graph generation techniques. These methods collectively facilitate the monitoring of misinformation, the categorization of content based on alignment with known false claims, and the visualization of dissemination cascades through detailed graphs. The tool is tailored to capture and analyze the dynamic nature of misinformation spread in digital environments. The effectiveness of DisTrack is demonstrated through three case studies focused on different themes: discredit/hate speech, anti-vaccine misinformation, and false narratives about the Russia-Ukraine conflict. These studies show DisTrack’s capabilities in distinguishing posts that propagate falsehoods from those that counteract them, and tracing the evolution of misinformation from its inception. The research confirms that DisTrack is a valuable tool in the field of misinformation analysis. It effectively distinguishes between different types of misinformation and traces their development over time. By providing a comprehensive approach to understanding and combating misinformation in digital spaces, DisTrack proves to be an essential asset for researchers and practitioners working to mitigate the impact of false information in online social environments.},
  archive      = {J_CC},
  author       = {Villar-Rodríguez, Guillermo and Huertas-García, Álvaro and Martín, Alejandro and Huertas-Tato, Javier and Camacho, David},
  doi          = {10.1007/s12559-024-10378-x},
  journal      = {Cognitive Computation},
  month        = {2},
  number       = {1},
  pages        = {1-18},
  shortjournal = {Cogn. Comput.},
  title        = {DisTrack: A new tool for semi-automatic misinformation tracking in online social networks},
  volume       = {17},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Robust synchronization of stochastic markovian jumping CVNs with randomly occurring nonlinearities and generally uncertain transition rates. <em>CC</em>, <em>17</em>(1), 1-20. (<a href='https://doi.org/10.1007/s12559-024-10362-5'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article intents to the robust synchronization issue for Markovian jumping complex-valued networks (CVNs) subject to the stochastic noises and randomly occurring nonlinearities, where the considered transition rates are generally uncertain, which expands the existed relevant results. Meanwhile, two random variables with pre-given statistical characteristics are proposed to explain the involved randomly occurring nonlinearities phenomenon, and random variables are mutually independent. By designing the appropriate mode-dependent controller, combined with generalized complex It $$\hat{o}$$ ’s formula, Lyapunov stability theory, and the properties of the transition rate matrix, some sufficient conditions are achieved to ensure error system realizes stochastically asymptotically mean-square stable. Furthermore, sufficient mode/delay-dependent criteria of globally exponential synchronization for considered CVNs are also investigated. In the end, two illustrative examples with simulations are proposed to verify the effectiveness and feasibility of the designed control schemes.},
  archive      = {J_CC},
  author       = {Li, Qiang and Wei, Hanqing and Hua, Dingli and Wang, Jinling and Zheng, Yuanshi},
  doi          = {10.1007/s12559-024-10362-5},
  journal      = {Cognitive Computation},
  month        = {2},
  number       = {1},
  pages        = {1-20},
  shortjournal = {Cogn. Comput.},
  title        = {Robust synchronization of stochastic markovian jumping CVNs with randomly occurring nonlinearities and generally uncertain transition rates},
  volume       = {17},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Extensions and detailed analysis of synergy between traditional classification and classification based on negative features in deep convolutional neural networks. <em>CC</em>, <em>17</em>(1), 1-16. (<a href='https://doi.org/10.1007/s12559-024-10369-y'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent times, deep convolutional neural networks became an irreplaceable tool for pattern recognition in many different machine learning applications, especially in image classification. On the other hand, these models are often used in critical systems which are the reason for new and recent research regarding their robustness and reliability. One of the most important issues for these models is their susceptibility to different adversarial attacks. In our previous work Milošević and Racković (Neural Network World. 2019;29(4):221–34), and Milošević and Racković (Neural Comput Applic. 2021;33:7593–602), the new type of learning applicable to all the convolutional neural networks was introduced: the classification based on the negative features and the synergy of traditional and those newly introduced network models. In the case of partial inputs/image occlusion, it was shown that our new method creates models that are more robust and perform better when compared to traditional models of the same architecture. In this paper, some extensions of the earlier proposed synergy are given by introducing negatively trained features and additional synergy between four independent neural network models. A detailed analysis of the robustness of the newly proposed model is performed on EMNIST and CIFAR-10 image classification data sets in the case of the selected input occlusions and adversarial attacks. The newly proposed neural network architecture improves the robustness of the neural network and increases its resistance to various types of input damage and adversarial attacks.},
  archive      = {J_CC},
  author       = {Racković, Miloš and Vidaković, Jovana and Milošević, Nemanja},
  doi          = {10.1007/s12559-024-10369-y},
  journal      = {Cognitive Computation},
  month        = {2},
  number       = {1},
  pages        = {1-16},
  shortjournal = {Cogn. Comput.},
  title        = {Extensions and detailed analysis of synergy between traditional classification and classification based on negative features in deep convolutional neural networks},
  volume       = {17},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). EXplainable AI for word embeddings: A survey. <em>CC</em>, <em>17</em>(1), 1-24. (<a href='https://doi.org/10.1007/s12559-024-10373-2'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, word embeddings have become integral to natural language processing (NLP), offering sophisticated machine understanding and manipulation of human language. Yet, the complexity of these models often obscures their inner workings, posing significant challenges in scenarios requiring transparency and explainability. This survey conducts a comprehensive review of eXplainable artificial intelligence (XAI) strategies focused on enhancing the interpretability of word embeddings. By classifying the existing body of work into six broad categories based on their methodological approaches—a classification that, to our knowledge, does not exist in the literature—we provide a structured overview of current techniques and their characteristics. Additionally, we uncover a noteworthy oversight: a predominant emphasis on interpreting model outputs at the expense of exploring the models’ internal mechanics. This finding underscores the necessity of shifting research efforts toward not only clarifying the results these models produce but also demystifying the models themselves. Such a shift is crucial for uncovering and addressing biases inherent in word embeddings, thus ensuring the development of fair and trustworthy AI systems. Through this analysis, we identify key research questions for future studies and advocate for a holistic approach to transparency in word embeddings, encouraging the research community to explore both the outcomes and the underlying algorithms of these models.},
  archive      = {J_CC},
  author       = {Boselli, Roberto and D’Amico, Simone and Nobani, Navid},
  doi          = {10.1007/s12559-024-10373-2},
  journal      = {Cognitive Computation},
  month        = {2},
  number       = {1},
  pages        = {1-24},
  shortjournal = {Cogn. Comput.},
  title        = {EXplainable AI for word embeddings: A survey},
  volume       = {17},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Artificial intelligence inspired task offloading and resource orchestration in intelligent transportation systems. <em>CC</em>, <em>17</em>(1), 1-30. (<a href='https://doi.org/10.1007/s12559-024-10380-3'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Internet of Vehicles (IoV) applications require the support of communication, caching, and computation (3C) resources to offload the computation-intensive tasks and for uplifting the traffic conditions in the development of sustainable smart cities. Intelligent Transportation Systems (ITS) lack the integrated ecosystems of addressing the low-latency task handovers, resource management issues, and centralized incentivization strategies. Digital Twin (DT) aids in capturing the real-time varying resource needs of the vehicles and the communication infrastructure that will regulate the task offloading process and facilitates in incentivizing the vehicular instances. In this manuscript, we establish a digital twin counterpart ( $$DT_{PIoV}$$ ) of the physical IoV (PIoV) to meet the QoS requirements during dynamic offloading and the time-varying resource supply–demand of computationally intensive applications. We formulate a response delay minimization function which is solved by the proposed DT-driven context-aware dynamic offloading method (CADOM). Furthermore, we use M/M/1/N/FCFS queueing method that combats the drawbacks of handling the simultaneous deadline-based tasks in a volatile environment of PIoV. In addition, we also maximize the utilities of vehicle and RSU service satisfaction by employing a reward-based mechanism for on-demand allocation of resources based on the Stackelberg game, where the DT of vehicle is deemed as a leader and service provider RSUs as a follower. The simulation results establish that the proposed system outpaces the conventional traffic management system by emphasizing the role of $$DT_{PIoV}$$ in jointly optimizing the overall response latency for different task sizes and also ensure a better utility satisfaction by catering on-demand resource allocation.},
  archive      = {J_CC},
  author       = {Rawlley, Oshin and Gupta, Shashank and Chandrakar, Jyotsana and Johnson, Manisha K. and Kalra, Chahat},
  doi          = {10.1007/s12559-024-10380-3},
  journal      = {Cognitive Computation},
  month        = {2},
  number       = {1},
  pages        = {1-30},
  shortjournal = {Cogn. Comput.},
  title        = {Artificial intelligence inspired task offloading and resource orchestration in intelligent transportation systems},
  volume       = {17},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Fine-tuned BERT algorithm-based automatic query expansion for enhancing document retrieval system. <em>CC</em>, <em>17</em>(1), 1-16. (<a href='https://doi.org/10.1007/s12559-024-10354-5'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Online retrieval systems are mostly web-based, which makes document collecting more dynamic or fluid than in traditional information retrieval systems. With the web growing in size every day, finding meaningful information on it using a search query consisting of only a few keywords which has become increasingly difficult. One important factor in making Internet searches better is query expansion, or QE. Manual query expansion method involves the user adding terms to the query, which takes a long time but produces good results. However, the automatic query expansion (AQE) method determines the best statements with minimal time consumption. Therefore, to improve document retrieval system, a fine-tuned BERT algorithm is developed for automatic query expansion. Initially, the input text was augmented using embedding augmentation (EA) approach. The augmented text was pre-processed using tokenization, normalization, splitting, stemming, stop word removal, as well as lemmatization. Then extracting the technical keywords from the pre-processed text using co-occurrence statistical information. After extracting the keywords, a fine-tuned BERT model is utilized for expanding the query to improve document retrieval system. The hyper parameters present in the BERT was tuned using frilled lizard optimization to enhance the performance of the BERT model. Proposed model provides 92% accuracy, 95% precision, and 95.6% recall. Thus, a fine-tuned BERT model minimizing query-document mismatch and thereby improving retrieval performance.},
  archive      = {J_CC},
  author       = {Vishwakarma, Deepak and Kumar, Suresh},
  doi          = {10.1007/s12559-024-10354-5},
  journal      = {Cognitive Computation},
  month        = {2},
  number       = {1},
  pages        = {1-16},
  shortjournal = {Cogn. Comput.},
  title        = {Fine-tuned BERT algorithm-based automatic query expansion for enhancing document retrieval system},
  volume       = {17},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Dynamic neighborhood selection for context aware temporal evolution using graph neural networks. <em>CC</em>, <em>17</em>(1), 1-19. (<a href='https://doi.org/10.1007/s12559-024-10359-0'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Graph neural networks (GNN) have seen significant growth recently for modeling temporal evolution in dynamic networks. Representation of complex networks in the form of graph data structures has enabled researchers to study how entities within these networks interact with each other. These interactions evolve over time. Developing a generic methodology for modeling this temporal evolution in complex networks for tracking evolving relationships has been a significant challenge. Most of the existing methods fail to extract contextual representations of historical neighborhood interactions for future link prediction. To address these challenges, this paper presents a novel method for modeling temporal evolution in complex networks using GNNs. A Context-Aware Graph Temporal Neural Network (CATGNN) method that uses dynamic neighborhood selection based on common neighbors for a given node is presented. The method uses dynamic neighborhood selection using contextual embeddings extracted from the historical interactions of the down-sampled set of neighbors of a central node based on a common neighborhood. Fixed-sized contextual memory modules are constructed for each node that store the historical interactions of its neighbors and are updated based on the recency and significance of interactions. The proposed method has been evaluated using six real-world datasets and has comparable performance against state-of-the-art methods, both in terms of accuracy and efficiency. It shows an improvement of 7.52 to 0.05% over the baselines in terms of average precision. The results demonstrate that the proposed CATGNN model can capture complex patterns of change that are difficult to identify using traditional techniques by propagating information over the graph structure. The model can be applied in various fields involving complex systems.},
  archive      = {J_CC},
  author       = {Zeb, Muhammad Ali and Uddin, M. Irfan and Alarood, Ala Abdulsalam and Shafiq, Muhammad and Habibullah, Safa and Alsulami, Abdulkream A.},
  doi          = {10.1007/s12559-024-10359-0},
  journal      = {Cognitive Computation},
  month        = {2},
  number       = {1},
  pages        = {1-19},
  shortjournal = {Cogn. Comput.},
  title        = {Dynamic neighborhood selection for context aware temporal evolution using graph neural networks},
  volume       = {17},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). NeuroInteract: An innovative deep learning strategy for effective drug repositioning in schizophrenia therapy. <em>CC</em>, <em>17</em>(1), 1-24. (<a href='https://doi.org/10.1007/s12559-024-10384-z'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Schizophrenia (SCZ) is a serious physiological and neurological disorder that affects an individual’s perception of factuality. It expresses different symptoms such as thinking, aberrant behavior, delusions, and hallucinations. An efficient approach for inferring potential indications for drugs is through drug repositioning. In this context, drug repositioning imparts a valuable strategy to gain safer, faster, and potentially efficient treatment options to improve schizophrenia therapy. Current treatments are insufficient and existing drug repositioning methods are unsuccessful in solving the drug-disease interactions’ difficulties, including long-term efficacy, drug synergy, and capturing genetic variations. Also, existing methods are restrained because of the incapacity to efficiently integrate heterogeneous biomedical data, which results in suboptimal predictions. This research introduces a NeuroInteract model using deep learning in order to predict candidate drugs for SCZ therapy. The proposed model enhances the accuracy of drug repositioning through the collection of various data sources such as genetic information and drug-disease associations. The novelty of the proposed model is the utilization of the heterogeneous data network that is integrated with the progressive optimization model for the purpose of improving prediction accuracy. The developed method imparts effective learning from various data characteristics through the integration of various types of neural network layers such as fully connected layers, convolutional layers, recurrent layers, and graph convolutional layers. The collected data from DrugBank 5.0 and repoDB undergoes a process of data integration, which aids in generating precise predictions for candidate drugs for repositioning. A data pre-processing technique is employed to improve the data quality. After data pre-processing, the proposed method effectively extracts the meaningful features and finds the spatial dependencies to predict the potential candidate drugs for SCZ treatment. Also, it efficiently handles sequential dependencies and genetic information. The oppositional crossover boosted meerkat optimization (OCMO) algorithm is deployed to optimize the performance of the model. The OCMO optimizes the learning process and enhances the model accuracy by dynamically adjusting its search strategy. Ultimately, comprehensive experimental analyses are conducted using several estimation parameters. The proposed method gains greater effectiveness and outperforms existing methods in drug repositioning. The developed method reaches an accuracy of 98.84% and a hit rate of 98.76%. These experimental findings ascertain the ability of NeuroInteract to find promising drugs for repurposing, furnishing a robust and more cost-effective model for SCZ treatment.},
  archive      = {J_CC},
  author       = {J., Sherine Glory and P., Durgadevi and P., Ezhumalai},
  doi          = {10.1007/s12559-024-10384-z},
  journal      = {Cognitive Computation},
  month        = {2},
  number       = {1},
  pages        = {1-24},
  shortjournal = {Cogn. Comput.},
  title        = {NeuroInteract: An innovative deep learning strategy for effective drug repositioning in schizophrenia therapy},
  volume       = {17},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A novel interpretable graph convolutional neural network for multimodal brain tumor segmentation. <em>CC</em>, <em>17</em>(1), 1-25. (<a href='https://doi.org/10.1007/s12559-024-10387-w'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep convolutional neural networks (CNNs) have revolutionized computer vision, demonstrating remarkable performance in various tasks. However, their end-to-end learning strategy poses challenges to explainability. In this work, we explore the application of explainability techniques in brain tumor segmentation using magnetic resonance imaging (MRI) data. Our adaptive learning class activation map (AL-CAM) employs a unique multiple-pop-out training strategy and contrastive learning to enhance internal outputs, improving interpretability. Additionally, we introduce a novel approach to explainability in graph convolutional neural networks (GCNNs). The usage of traditional CNN interpretability tools such as saliency maps, CAM, and EB are often unable to handle the complexity of graph-structured data. Our work brim this gap by adapting and improving these techniques for graph convolutional neural networks (GCNN). We present two innovative tools: adaptive CAM for differentiated interpretability and contrastive EB for deeper insights into functions. Using a novel feature fusion approach, we further push the boundaries and combine the feature strengths of GNN and CNN for a holistic understanding of GCNN decision-making. Our proposed framework enables interpretability in various areas, not just medical imaging. Our work demonstrates the versatility of explainability methods and demonstrates their power in unlocking the secrets of GCNNs and ultimately solving real-world challenges, particularly in the field of medical image analysis.},
  archive      = {J_CC},
  author       = {Arshad Choudhry, Imran and Iqbal, Saeed and Alhussein, Musaed and Aurangzeb, Khursheed and Qureshi, Adnan N. and Hussain, Amir},
  doi          = {10.1007/s12559-024-10387-w},
  journal      = {Cognitive Computation},
  month        = {2},
  number       = {1},
  pages        = {1-25},
  shortjournal = {Cogn. Comput.},
  title        = {A novel interpretable graph convolutional neural network for multimodal brain tumor segmentation},
  volume       = {17},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). An adaptive decision-making system for behavior analysis among young adults. <em>CC</em>, <em>17</em>(1), 1-18. (<a href='https://doi.org/10.1007/s12559-024-10372-3'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The global spread of the pandemic, secure isolation regulations, logistical limitations, and delays in reopening educational institutions such as colleges and universities have all had a severe psychological impact. Students, in particular, are regarded as a vulnerable population, experiencing higher levels of fear, stress, depression, and unhealthy eating compared to the general population. To reduce these psychological consequences, the study provides a multi-dimensional evaluation approach to bridge the gap between governments and health institutions in preventing and controlling biological hazards, such as mental illness among students. In the aftermath of the pandemic, this study presents a comprehensive evaluation approach designed to mitigate the psychological impact on students by connecting governments and health institutions in preventing and controlling biological hazards, particularly mental illness. To establish complex and vague data concerning the discussed communities, psychological details were obtained in the complex spherical fuzzy $$ \mathscr {N}$$ -soft context. An enhanced group decision-making methodology was then established in two phases. Initially, weight analytics were defined using the Reyni entropy technique. In the subsequent phase, the Combined Compromise Solution (CoCoSo) approach was applied to examine the possibilities. Students attending schools and colleges experience significant psychological impacts. To evaluate these effects, an analytical study was conducted, suggesting that improved educational amenities are necessary to mitigate these psychological consequences. Furthermore, the study validates the significance of the proposed decision system through its analysis. A unique score function is suggested for analyzing the psychological consequences among adults because it effectively addresses the ambiguity in periodic data, resulting in accurate and consistent judgments within a two-dimensional framework. Experts thoroughly analyzed the data using the complex spherical fuzzy $$ \mathscr {N}$$ -soft set-integrated CoCoSo method, and its limitations were also addressed.},
  archive      = {J_CC},
  author       = {Pragathi, Subramaniam and Narayanamoorthy, Samayan and Pamucar, Dragan and Kang, Daekook},
  doi          = {10.1007/s12559-024-10372-3},
  journal      = {Cognitive Computation},
  month        = {2},
  number       = {1},
  pages        = {1-18},
  shortjournal = {Cogn. Comput.},
  title        = {An adaptive decision-making system for behavior analysis among young adults},
  volume       = {17},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Enhanced self-attention-based rapid CNN for detecting dense objects in varying illumination. <em>CC</em>, <em>17</em>(1), 1-20. (<a href='https://doi.org/10.1007/s12559-024-10376-z'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper addresses the challenge of efficient detection of densely arranged unordered items under varying illumination. Specifically, a novel convolutional neural network-based method is proposed for item vector detection, recognition, and classification, termed Self-Attention and Concatenation-Based Detector (ACDet). In a benchmark pharmaceutical case study, rapid and accurate detection of pharmaceutical package contours is achieved, enabling the automatic and fast verification of both the quantity and types of pharmaceuticals during distribution. At the input stage, a combined image augmentation method is applied to improve the detection model’s ability to learn the appearance features of items from multiple angles. Based on YOLOv8 model, integrating computational module C2F with Attention (C2F-A), multidimensional self-attention reinforcement is applied to the outputs of multiple gradient streams. The designed Weighted Concatenation (WConcat) module self-learns to weight and concatenate multi-level feature maps, enhancing the model’s cognitive capability. Finally, simulation experiments are conducted to determine the optimal timing for utilizing each module. Simulation experiments compared the proposed ACDet with several state-of-the-art YOLO architecture models utilizing the benchmark Comprehensive Pharmaceutical Package Dataset (CPPD). ACDet achieved 81.0% mAP and 79.5% Smooth mAP on the CPPD dataset, outperforming other models by an average of 5.5% to 16.6%. On public datasets, the results were 52.2% and 51.0%, respectively. The impact of utilizing C2F-A at different stages on performance was also tested, concluding that the WConcat module does not necessitate spatial attention. Finally, in zero-shot testing, the verification success rate reached 99.91%. Our work shows that the proposed ACDet can overcome many challenges in complex object detection scenarios, enhancing robustness while maintaining a lightweight design. The proposed model can serve as a new benchmark.},
  archive      = {J_CC},
  author       = {Chen, Lu and Yang, Li and Jie, Tan and Haoyuan, Ma and Yu, Liu and Shenbing, Fu and Wang, Junkang and Wu, Hao and Li, Gun},
  doi          = {10.1007/s12559-024-10376-z},
  journal      = {Cognitive Computation},
  month        = {2},
  number       = {1},
  pages        = {1-20},
  shortjournal = {Cogn. Comput.},
  title        = {Enhanced self-attention-based rapid CNN for detecting dense objects in varying illumination},
  volume       = {17},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Evolutionary multitasking with adaptive tradeoff selection strategy. <em>CC</em>, <em>17</em>(1), 1-25. (<a href='https://doi.org/10.1007/s12559-024-10386-x'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As a new emerging evolutionary framework, evolutionary multitasking aims to optimize multiple tasks simultaneously. Knowledge transfer is an important component of evolutionary multitasking. How to extract and transfer knowledge significantly affects the performance of the algorithm. A serious challenge for evolutionary multitasking is the inappropriate knowledge transfer or insufficient exploration and exploitation. To address this challenge, an evolutionary multitasking with adaptive tradeoff selection strategy (EMT-ATS) is proposed. To enhance global exploration and local exploitation during the evolution, an adaptive tradeoff selection mechanism is developed to select promising offspring during different stages to guide the population toward more promising solution regions. In addition, a Cohen’s d indicator-based is used to adjust knowledge transfer. To verify the effectiveness of the proposed EMT-ATS, a series of experiments are conducted with several popular evolutionary multitasking algorithms on multitasking benchmark problems. In addition, a multitask optimization problem involving two real-world problems is used to validate the practicability of the proposed EMT-ATS. Experimental results demonstrate the effectiveness of the proposed EMT-ATS.},
  archive      = {J_CC},
  author       = {Li, Wei and Zhou, Yinhui and Wang, Lei},
  doi          = {10.1007/s12559-024-10386-x},
  journal      = {Cognitive Computation},
  month        = {2},
  number       = {1},
  pages        = {1-25},
  shortjournal = {Cogn. Comput.},
  title        = {Evolutionary multitasking with adaptive tradeoff selection strategy},
  volume       = {17},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Adaptive unsupervised graph convolution network for data clustering with graph reconstruction. <em>CC</em>, <em>17</em>(1), 1-14. (<a href='https://doi.org/10.1007/s12559-024-10364-3'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, graph clustering has emerged as one of the most challenging problems in the field of deep learning. With the increasing complexity of real-world networks, such as social and biological networks, more and more effective methods are needed to organize and understand these structures. Various cognition-based techniques have been explored for classifying nodes within these graphs, and graph convolution networks (GCNs) have attracted great interest. GCNs, a deep semi-supervised learning approach, provide a powerful framework for learning node representations by utilizing both local and global graph information. By iteratively aggregating information from neighboring nodes, GCNs effectively capture the intricate relationships and dependencies within complex networks. We introduce a novel deep unsupervised learning scheme built upon the foundation of GCN architecture. The key contributions are outlined below. First, the entire architecture is trained with three unsupervised learning losses. The first loss focuses on kernelized features that use node attributes to reflect the information extracted from the data. The second loss leverages spectral smoothness that uses connections between nodes to capture global cluster structure. The third loss is based on graph reconstruction that introduces additional regularization of the representation of nodes by the output of the model. Second, the spectral smoothing loss involves an adaptive approach using an additional graph matrix associated with the node representations. This adaptive integration of additional structural information increases the learning efficiency during the training phase. The adaptive fused graph used for spectral smoothing loss incorporates structural insights derived from both data features and deep node representations. To assess the performance of our approach, we conducted extensive experimental evaluations on four benchmark datasets widely used in the field of graph clustering. These datasets were carefully selected to cover diverse domains and varying degrees of complexity, ensuring a comprehensive evaluation of our method’s efficacy. Our results showcase the remarkable performance of our unsupervised GCN across multiple metrics, surpassing other state-of-the-art graph neural network techniques in terms of clustering accuracy, purity, and other relevant measures. Notably, our method consistently outperforms competing approaches across the used datasets, demonstrating its versatility and effectiveness in handling various real-world scenarios.},
  archive      = {J_CC},
  author       = {Jreidy, M. Al and Constantin, J. and Dornaika, F. and Hamad, D.},
  doi          = {10.1007/s12559-024-10364-3},
  journal      = {Cognitive Computation},
  month        = {2},
  number       = {1},
  pages        = {1-14},
  shortjournal = {Cogn. Comput.},
  title        = {Adaptive unsupervised graph convolution network for data clustering with graph reconstruction},
  volume       = {17},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Disturbance observer-based control: Weighted aggregated aczel-alsina sum product assessment based on power operators for managing fuzzy 2-tuple linguistic neural networks. <em>CC</em>, <em>17</em>(1), 1-18. (<a href='https://doi.org/10.1007/s12559-024-10371-4'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Disturbance observer–based control (DOBC) is a valuable strategy for enhancing control system performance by compensating for disturbances. The core idea is to design an observer that accurately estimates external disturbances affecting the system, and then use this estimation to adjust the control input accordingly. This article introduces a novel approach involving fuzzy 2-tuple linguistic (F2-TL) sets, incorporating algebraic and Aczel-Alsina operational laws. Additionally, we propose several operators: the F2-TL Aczel-Alsina power averaging (F2-TLAAPA) operator, the F2-TL Aczel-Alsina power weighted averaging (F2-TLAAPWA) operator, the F2-TL Aczel-Alsina power geometric (F2-TLAAPG) operator, and the F2-TL Aczel-Alsina power weighted geometric (F2-TLAAPWG) operator. These operators are used to aggregate information into a singleton set, and we discuss their fundamental properties, including idempotency, monotonicity, and boundedness. Moreover, we explain the WASPAS (weighted aggregated sum product assessment) technique, applying the proposed methods and illustrating them with relevant examples. The article also explores the application of these techniques to multi-attribute decision-making (MADM) problems, demonstrating their value. Finally, to validate the introduced methods and highlight the superiority of the proposed approach, we compare the ranking results obtained using our methods with those from existing techniques.},
  archive      = {J_CC},
  author       = {Ali, Zeeshan and Hila, Kostaq},
  doi          = {10.1007/s12559-024-10371-4},
  journal      = {Cognitive Computation},
  month        = {2},
  number       = {1},
  pages        = {1-18},
  shortjournal = {Cogn. Comput.},
  title        = {Disturbance observer-based control: Weighted aggregated aczel-alsina sum product assessment based on power operators for managing fuzzy 2-tuple linguistic neural networks},
  volume       = {17},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multi-view hierarchical graph neural network for argumentation mining. <em>CC</em>, <em>17</em>(1), 1-14. (<a href='https://doi.org/10.1007/s12559-024-10391-0'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Argumentation mining (AM) aims to detect the arguments and their relations from argumentative texts. Generally, AM contains three key challenging subtasks, including argument component type classification (ACTC), argumentative relation identification (ARI), and argumentative relation type classification (ARTC). Most previous studies solve these three subtasks separately, neglecting the rich interrelation information among the three tasks. In this paper, we propose a multi-view hierarchical graph neural network (MHGNN) for AM, which resolves the three interacted subtasks in a unified multi-task learning framework. Concretely, MHGNN learns graph embeddings from multiple views (i.e., word view and semantic view) that often provide more comprehensive information. Each graph view is equipped with a two-level graph structure: (i) the first level is the argumentation graph with each argumentation component (AC) as a graph node, which learns the inter-AC knowledge from the input text; (ii) the second level is the AC graph with each word or semantic role as graph node respectively, which learn the fine-grained intra-AC knowledge within each AC from the word level or semantic level. The multi-view hierarchical GNN makes our model more effective to utilize the rich information among and within the ACs in the input text. Then, we transform ACTC, ARI, and ARTC into node classification, edge prediction, and edge type classification on the argumentation graph by devising novel graph attention mechanisms to learn comprehensive and relation-aware graph embeddings. These three subtasks are integrated into a unified model through multi-task learning and partial parameters sharing. Extensive experiments on two benchmark datasets demonstrate that the proposed MHGNN framework outperforms the strong baseline methods for all three subtasks.},
  archive      = {J_CC},
  author       = {Sun, Yang and Bao, Jianzhu and Tu, Geng and Liang, Bin and Yang, Min and Xu, Ruifeng},
  doi          = {10.1007/s12559-024-10391-0},
  journal      = {Cognitive Computation},
  month        = {2},
  number       = {1},
  pages        = {1-14},
  shortjournal = {Cogn. Comput.},
  title        = {Multi-view hierarchical graph neural network for argumentation mining},
  volume       = {17},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). On the integration of large-scale time series distance matrices into deep visual analytic tools. <em>CC</em>, <em>17</em>(1), 1-18. (<a href='https://doi.org/10.1007/s12559-024-10394-x'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Time series are essential for modeling a lot of activities such as software behavior, heart rate, and business processes. The analysis of the series data can prevent errors, boost profits, and improve the understanding of behaviors. Among the many techniques available, we can find deep learning techniques and data mining techniques. In data mining, distance matrices between subsequences (similarity matrices, recurrence plots) have already shown their potential in fast large-scale time series behavior analysis. In deep learning, there exist different tools for analyzing the models’ embedding space to get insights into the data behavior. DeepVATS is a tool for large time series analysis that allows the visual interaction within the embedding space (latent space) of deep learning models and the original data. The training and analysis of the model may result in a large use of computational resources, resulting in a lack of interactivity. To solve this issue, we integrate distance matrix plots within the tool. The incorporation of these plots with the associated downsampling techniques makes DeepVATS a more efficient and user-friendly tool for a first quick analysis of the data, achieving runtimes reductions of up to $$10^4$$ seconds, allowing fast preliminary analysis of datasets of up to 7 M elements. Also, this incorporation allows us to detect trends, extending its capabilities. The new functionality is tested in three use cases: the M-Toy synthetic dataset for anomaly detection, the S3 synthetic dataset for trend detection, and the real-world dataset pulsus paradoxus for anomaly checking.},
  archive      = {J_CC},
  author       = {Santamaria-Valenzuela, Inmaculada and Rodriguez-Fernandez, Victor and Camacho, David},
  doi          = {10.1007/s12559-024-10394-x},
  journal      = {Cognitive Computation},
  month        = {2},
  number       = {1},
  pages        = {1-18},
  shortjournal = {Cogn. Comput.},
  title        = {On the integration of large-scale time series distance matrices into deep visual analytic tools},
  volume       = {17},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Fuzzy soft topological numbers with an operation of vertex deletion: A comparative study with TOPSIS method and its application in car import decision-making. <em>CC</em>, <em>17</em>(1), 1-25. (<a href='https://doi.org/10.1007/s12559-024-10396-9'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {There are two distinct soft computing models for representing ambiguity and uncertainty: fuzzy and soft sets. A novel mathematical technique for handling uncertainties is the soft set. This set offers a parameterized viewpoint for soft computing and uncertainty modeling. Soft sets have been shown to have potential uses in a number of disciplines, including probability theory, operations research, game theory, measurement theory, and the smoothness of functions. Topological numbers possess significant importance in graph theory. On topological numbers in fuzzy graph theory, there is also a wealth of literature. However, there has not been much research done on fuzzy soft topological numbers up until now. Fuzzy soft graphs are very versatile instruments available for decision-making. Therefore, in fuzzy soft graph theory, it would be very beneficial to introduce and apply topological numbers. Multi-criteria decision-making approaches give decision-makers the required instruments, but their underlying theories and assumptions may differ. Therefore, choosing the best way to make decisions is just as crucial as actually making the decision. The technique for order preference by similarity to ideal solution (TOPSIS) is the most widely used multi-criteria decision-making method. TOPSIS can solve the real-world problems. TOPSIS is a technique for ranking, based on the weights and impacts of the given elements. However, prior to this study, no work has been done on the application of fuzzy soft topological numbers in decision-making systems. The novelty of this research manuscript is to calculate three fuzzy soft topological numbers before and after the deletion of the vertex for a generalized graph and in a fuzzy soft framework and make a comparison in the results obtained before and after deleting the vertex. Subsequently, we demonstrated an application of international automobile importation into the United States by several nations using various graphical networks, employing a parameterized fuzzy soft graph point of view and three different modes of transportation. It is established that if a vertex is removed from a network, the profit made will decrease significantly. Additionally, the optimal network of nations for all purposes is evaluated. By using the TOPSIS approach, the ranking of networks is also generated from a set of alternatives.},
  archive      = {J_CC},
  author       = {Anwar, Shabana and Kamran Jamil, Muhammad and Azeem, Muhammad and Deveci, Muhammet and Antucheviciene, Jurgita},
  doi          = {10.1007/s12559-024-10396-9},
  journal      = {Cognitive Computation},
  month        = {2},
  number       = {1},
  pages        = {1-25},
  shortjournal = {Cogn. Comput.},
  title        = {Fuzzy soft topological numbers with an operation of vertex deletion: A comparative study with TOPSIS method and its application in car import decision-making},
  volume       = {17},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Neural adaptive dynamic event-triggered containment control for uncertain multi-agent systems under markovian switching dynamics. <em>CC</em>, <em>17</em>(1), 1-14. (<a href='https://doi.org/10.1007/s12559-024-10388-9'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we propose the containment control problem for multi-agent systems with Markovian switching dynamics by proposing a novel adaptive dynamic event-triggered sliding mode control scheme based on radial basis function neural networks. First, the unknown nonlinear dynamics of the system were approximated by using radial basis function neural networks. The dynamic event-triggered control scheme designed in the framework of sliding mode control operated at specific event sampling moments, thereby reducing computational and communication burdens. The containment control was achieved through a synergistic approach integrating dynamic event-triggered control with neural network-based adaptive control in a stochastic switching system. Moreover, we proved that Zeno behavior was effectively avoided. The proposed distributed containment control technique was validated through simulations, demonstrating its effectiveness and superiority.},
  archive      = {J_CC},
  author       = {Cai, Jiayi and Wu, Wenjun and Yi, Chengbo and Chen, Yanxian},
  doi          = {10.1007/s12559-024-10388-9},
  journal      = {Cognitive Computation},
  month        = {2},
  number       = {1},
  pages        = {1-14},
  shortjournal = {Cogn. Comput.},
  title        = {Neural adaptive dynamic event-triggered containment control for uncertain multi-agent systems under markovian switching dynamics},
  volume       = {17},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Convolutional cross-modal autoencoder-based few-shot learning for data augmentation with application to alzheimer dementia diagnosis. <em>CC</em>, <em>17</em>(1), 1-14. (<a href='https://doi.org/10.1007/s12559-024-10390-1'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper presents a novel deep few-shot learning method for magnetic resonance images-based Alzheimer’s dementia (AD) diagnosis. The proposed method consists of two main phases namely data augmentation and data classification. With regard to data augmentation and, to deal with data scarcity issues, we designed a convolutional cross-modal autoencoder (CCMAE) model for data generation. This model, which consists of two encoders and one decoder, receives two image modalities namely longitudinal and cross-section MRI, and generates a new cross-section image. We opt for a convolutional version of the autoencoder to capture the spatial information more effectively and reduce the number of trainable parameters. Moreover, to make the model able to perform deep analysis of input image, we establish a skip connection strategy between the first encoder and the decoder similar to the UNet mechanism. With regard to classification, we design a convolutional neural network-based model in which both textual and visual features are fused to strengthen the network performance and produce more reliable decisions. A comprehensive experiment on a publicly available dataset has been conducted to demonstrate the effectiveness of the proposed method compared to some related works. The code is publicly available at: https://github.com/Bazine-Othmane/scientific-paper-code .},
  archive      = {J_CC},
  author       = {Bazine, Othmane and Rai, Omar and Aiadi, Oussama and Hedjam, Rachid and Khaldi, Belal and Zhong, Guoqiang},
  doi          = {10.1007/s12559-024-10390-1},
  journal      = {Cognitive Computation},
  month        = {2},
  number       = {1},
  pages        = {1-14},
  shortjournal = {Cogn. Comput.},
  title        = {Convolutional cross-modal autoencoder-based few-shot learning for data augmentation with application to alzheimer dementia diagnosis},
  volume       = {17},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A comparative analysis of metaphorical cognition in ChatGPT and human minds. <em>CC</em>, <em>17</em>(1), 1-12. (<a href='https://doi.org/10.1007/s12559-024-10393-y'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {ChatGPT represents a significant advancement in the field of Artificial Intelligence (AI), showcasing the development of a robust AI system capable of multitasking and generating human-like language. At present, many scholars have done evaluations on ChatGPT in terms of language, reasoning, and scientific knowledge abilities, based on benchmarks or well-crafted questions. However, to the best of our knowledge, there is currently no existing comparative analysis from a cognitive perspective that directly assesses ChatGPT alongside humans. Metaphor, serving as a manifestation of linguistic creativity, provides a valuable avenue for examining cognition. This is due to the mapping relationship it establishes between the target and source conceptual domains, reflecting distinct cognitive patterns. In this paper, we use a metaphor processing tool, MetaPro, to analyze the cognitive differences between ChatGPT and humans through the metaphorical expressions in ChatGPT- and human-generated text. We illustrate the preferences in metaphor usage, concept mapping, and cognitive pattern variances across different domains. The methodology utilized in this study makes a valuable contribution to the task-agnostic evaluation of AI systems and cognitive research. The insights garnered from this research prove instrumental in comprehending the cognitive distinctions between ChatGPT and humans, facilitating the identification of potential cognitive biases within ChatGPT.},
  archive      = {J_CC},
  author       = {Mao, Rui and Chen, Guanyi and Li, Xiao and Ge, Mengshi and Cambria, Erik},
  doi          = {10.1007/s12559-024-10393-y},
  journal      = {Cognitive Computation},
  month        = {2},
  number       = {1},
  pages        = {1-12},
  shortjournal = {Cogn. Comput.},
  title        = {A comparative analysis of metaphorical cognition in ChatGPT and human minds},
  volume       = {17},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A novel depth-connected region-based convolutional neural network for small defect detection in additive manufacturing. <em>CC</em>, <em>17</em>(1), 1-17. (<a href='https://doi.org/10.1007/s12559-024-10397-8'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Defect detection on the computed tomography (CT) images plays an important role in the development of metallic additive manufacturing (AM). Although some deep learning techniques have been adopted in the CT image-based defect detection problem, it is still a challenging task to accurately detect small-size defects in the presence of undesirable noises. In this paper, a novel defect detection method, namely, the depth-connected region-based convolutional neural network (DC-RCNN), is proposed to detect small defects and reduce the influence of noises. In particular, a saliency-guided region proposal method is first developed to generate small-size region proposals with the aim to accommodate the small defects. Then, the main architecture of DC-RCNN is proposed to extract and connect the consistent features across multiple frames, thereby reducing the influence of randomly distributed noises. Moreover, the transfer learning technique is utilized to improve the generalization ability of the proposed DC-RCNN. In order to verify the effectiveness and superiority, the proposed method is applied to the real-world AM data for defect detection. The experimental validations show that the proposed DC-RCNN is able to detect the small-size defects under noises and outperforms the original RCNN method in terms of detection accuracy and running time.},
  archive      = {J_CC},
  author       = {Wang, Yiming and Wang, Zidong and Liu, Weibo and Zeng, Nianyin and Lauria, Stanislao and Prieto, Camilo and Sikström, Fredrik and Yu, Hui and Liu, Xiaohui},
  doi          = {10.1007/s12559-024-10397-8},
  journal      = {Cognitive Computation},
  month        = {2},
  number       = {1},
  pages        = {1-17},
  shortjournal = {Cogn. Comput.},
  title        = {A novel depth-connected region-based convolutional neural network for small defect detection in additive manufacturing},
  volume       = {17},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Interval-valued intuitionistic fuzzy yager power operators and possibility degree-based group decision-making model. <em>CC</em>, <em>17</em>(1), 1-25. (<a href='https://doi.org/10.1007/s12559-024-10368-z'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As an extended form of intuitionistic fuzzy set, the theory of interval-valued intuitionistic fuzzy set (IVIFS) can describe fuzziness more flexibly. This study aims to develop a group decision-making model based on the distance measure, Yager power aggregation operators and the possibility measure in the context of IVIFSs. For this purpose, new distance measure is proposed to quantify the dissimilarity between two IVIFSs. In addition, comparison with existing distance measures is performed to illustrate the efficiency of introduced measure. Combining the Yager’s triangular norms with the proposed distance-based power operators, a series of interval-valued intuitionistic fuzzy (IVIF) Yager power aggregation operators are introduced with their desirable properties. Moreover, a possibility measure is developed for pairwise comparisons of IVIFSs, which overcomes the shortcomings of existing IVIF-score function, IVIF-accuracy function, and IVIF-possibility measures. The developed possibility measure is further utilized to compute the weights of criteria. To prove the practicality and effectiveness of introduced model, it is applied to a case study of manufacturing plant location selection problem with IVIF information. Finally, sensitivity and comparative analyses are carried out to test the stability and robustness of the proposed method under the setting of IVIFSs.},
  archive      = {J_CC},
  author       = {Rani, Pratibha and Mishra, Arunodaya Raj and Deveci, Muhammet and Alrasheedi, Adel Fahad and Alshamrani, Ahmad M. and Pedrycz, Witold},
  doi          = {10.1007/s12559-024-10368-z},
  journal      = {Cognitive Computation},
  month        = {2},
  number       = {1},
  pages        = {1-25},
  shortjournal = {Cogn. Comput.},
  title        = {Interval-valued intuitionistic fuzzy yager power operators and possibility degree-based group decision-making model},
  volume       = {17},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Unified multi-view data clustering: Simultaneous learning of consensus coefficient matrix and similarity graph. <em>CC</em>, <em>17</em>(1), 1-16. (<a href='https://doi.org/10.1007/s12559-024-10392-z'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Integrating data from multiple sources or views has become increasingly common in data analysis, particularly in fields like healthcare, finance, and social sciences. However, clustering such multi-view data poses unique challenges due to the heterogeneity and complexity of the data sources. Traditional clustering methods are often unable to effectively leverage the information from different views, leading to suboptimal clustering results. To address this challenge, multi-view clustering techniques have been developed, aiming to integrate information from multiple views to improve clustering performance. These techniques typically involve learning a similarity matrix for each view and then combining these matrices to form a consensus similarity matrix, which is subsequently used for clustering. However, existing approaches often suffer from limitations such as the need for manual tuning of parameters and the inability to effectively capture the underlying structure of the data. In this paper, we propose a novel approach for multi-view clustering that addresses these limitations by jointly learning the consensus coefficient matrix and similarity graph. Unlike existing methods that follow a sequential approach of first learning the coefficient matrix and then constructing the similarity graph, our approach simultaneously learns both matrices, ensuring a more regularized consensus graph. Additionally, our method automatically adjusts the weight of each view, eliminating the need for manual parameter tuning. Our approach involves several key steps. First, we formulate an optimization problem that jointly optimizes the consensus coefficient matrix, unified spectral projection matrix, coefficient matrix, and soft cluster assignment matrix. We then propose an efficient algorithm to solve this optimization problem, which involves iteratively updating the matrices until convergence. To learn the consensus coefficient matrix and similarity graph, we leverage techniques from matrix factorization and graph-based learning. Specifically, we use a self-representation technique to learn the coefficient matrix (regularization graPh) and a graph regularization technique to learn the similarity graph. By jointly optimizing these matrices, we ensure that the resulting consensus graph is more regularized and better captures the underlying structure of the data. We evaluate our approach on several public image datasets, comparing it against state-of-the-art multi-view clustering methods. Our experimental results demonstrate that our approach consistently outperforms existing methods in terms of clustering accuracy and robustness. Additionally, we conduct sensitivity analysis to evaluate the impact of different hyperparameters on the clustering performance. We present a novel approach for multi-view data clustering that jointly learns the consensus coefficient matrix and similarity graph. By simultaneously optimizing these matrices, our approach achieves better clustering performance compared to existing methods. Our results demonstrate the effectiveness and robustness of our approach across different datasets, highlighting its potential for real-world applications in various domains.},
  archive      = {J_CC},
  author       = {Dornaika, F. and El Hajjar, S. and Charafeddine, J. and Barrena, N.},
  doi          = {10.1007/s12559-024-10392-z},
  journal      = {Cognitive Computation},
  month        = {2},
  number       = {1},
  pages        = {1-16},
  shortjournal = {Cogn. Comput.},
  title        = {Unified multi-view data clustering: Simultaneous learning of consensus coefficient matrix and similarity graph},
  volume       = {17},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Optimizing social issues strategies by using bipolar complex fuzzy muirhead mean decision-making approach. <em>CC</em>, <em>17</em>(1), 1-23. (<a href='https://doi.org/10.1007/s12559-024-10353-6'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cognitive processes that affect people perceptions, comprehension, and interactions with others, such as attribution mistakes and heuristics, are responsible for social issues. These social issues includes polarization, prejudice, and inequality. To address these issues, we must comprehend cognitive mechanisms, and this can be made by using some appropriate multi-attribute decision-making (MADM) approach, that can handle people perceptions of complex and bipolar nature. Thus, in this manuscript, we concentrate on a MADM technique that relies on certain novel aggregation operators in the framework of bipolar complex fuzzy sets. These aggregation operators include Muirhead mean (MM) operator and dual Muirhead mean (DMM) operator of several types. To authenticate the validity of these defined aggregation operators, certain properties of these operators are proved. Furthermore, we consider the interpreted operators to produce a decision-making (DM) technique to deal with bipolar complex fuzzy MADM issues. We then consider a real life example to show the application and need of the interpreted work in daily life. To confirm the viability and potential of the offered technique, we compare our established technique with some other prevailing techniques.},
  archive      = {J_CC},
  author       = {Rehman, Ubaid ur and Mahmood, Tahir and García, Gustavo Santos},
  doi          = {10.1007/s12559-024-10353-6},
  journal      = {Cognitive Computation},
  month        = {2},
  number       = {1},
  pages        = {1-23},
  shortjournal = {Cogn. Comput.},
  title        = {Optimizing social issues strategies by using bipolar complex fuzzy muirhead mean decision-making approach},
  volume       = {17},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A weakly supervised data labeling framework for machine lexical normalization in vietnamese social media. <em>CC</em>, <em>17</em>(1), 1-32. (<a href='https://doi.org/10.1007/s12559-024-10356-3'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This study introduces an innovative automatic labeling framework to address the challenges of lexical normalization in social media texts for low-resource languages like Vietnamese. Social media data is rich and diverse, but the evolving and varied language used in these contexts makes manual labeling labor-intensive and expensive. To tackle these issues, we propose a framework that integrates semi-supervised learning with weak supervision techniques. This approach enhances the quality of the training dataset and expands its size while minimizing manual labeling efforts. Our framework automatically labels raw data, converting non-standard vocabulary into standardized forms, thereby improving the accuracy and consistency of the training data. Experimental results demonstrate the effectiveness of our weak supervision framework in normalizing Vietnamese text, especially when utilizing pre-trained language models. The proposed framework achieves an impressive F1-score of 82.72% and maintains vocabulary integrity with an accuracy of up to 99.22%. Additionally, it effectively handles undiacritized text under various conditions. This framework significantly enhances natural language normalization quality and improves the accuracy of various NLP tasks, leading to an average accuracy increase of 1–3%.},
  archive      = {J_CC},
  author       = {Nguyen, Dung Ha and Nguyen, Anh Thi Hoang and Van Nguyen, Kiet},
  doi          = {10.1007/s12559-024-10356-3},
  journal      = {Cognitive Computation},
  month        = {2},
  number       = {1},
  pages        = {1-32},
  shortjournal = {Cogn. Comput.},
  title        = {A weakly supervised data labeling framework for machine lexical normalization in vietnamese social media},
  volume       = {17},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). BrainEnsemble: A brain-inspired effective ensemble pruning algorithm for pattern classification. <em>CC</em>, <em>17</em>(1), 1-21. (<a href='https://doi.org/10.1007/s12559-024-10363-4'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The human brain comprises distinct regions, each with specific functions. Interconnected through neural pathways, the brain regions collaborate to process complex information. Similarly, ensemble learning enhances pattern classification by leveraging the collaboration and complementarity between classifiers. The similarity between the two suggests that simulating the brain’s functional network holds the potential for groundbreaking advancements in the design of ensemble learning algorithms. Motivated by this, our paper proposes a brain-inspired ensemble pruning method called BrainEnsemble. This method provides an example of using classifier combinations to emulate the functions of brain regions. Guided by the principles of curriculum learning and the divide-and-conquer strategy, each artificial brain region can specialize in specific functions and tasks. Additionally, BrainEnsemble simulates the brain regions’ responses and connectivity mechanisms through graph connections. In this model, different artificial brain regions can dynamically reorganize and adjust their interactions to adapt to continuously changing environments or data distributions, enabling the model to maintain high performance when confronted with new data. Extensive experimental results demonstrate the superior performance of BrainEnsemble. In summary, drawing inspiration from the information processing mechanism of the human brain can provide new ideas for the design of ensemble learning algorithms, and more research can be conducted in this direction in the future.},
  archive      = {J_CC},
  author       = {Li, Danyang and Huang, Shisong and Wen, Guihua and Zhang, Zhuhong},
  doi          = {10.1007/s12559-024-10363-4},
  journal      = {Cognitive Computation},
  month        = {2},
  number       = {1},
  pages        = {1-21},
  shortjournal = {Cogn. Comput.},
  title        = {BrainEnsemble: A brain-inspired effective ensemble pruning algorithm for pattern classification},
  volume       = {17},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Exploiting deep contrast feature for image retrieval. <em>CC</em>, <em>17</em>(1), 1-15. (<a href='https://doi.org/10.1007/s12559-024-10375-0'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the field of content-based image retrieval (CBIR), fused feature-based methods have demonstrated their advanced performance on the popular benchmark datasets. However, it is inevitable increase the vector dimensionality because the fused features have diversity. Therefore, achieving both a low-dimensional representation and high retrieval performance remains challenging. To address this problem, an image retrieval method based on the deep contrast-based layer is proposed, namely the deep contrast feature histogram (DCFH), to image retrieval. There are three highlights as follows: (1) texture features based on the edge orientation are calculated to build contrast-based layer; it can enhance the discriminative power of deep features; (2) a generalized mean aggregation method is introduced to effectively aggregate the representative information in the deep feature maps of convolutional neural network (CNN); (3) a multi-orientational PCA whitening method is proposed to provide a compact yet discriminative representation. Comparative experiments demonstrated that our method can provide outstandingly competitive retrieval performance on popular benchmark datasets. This work captures visual information from both global and local perspectives, presenting an approach in line with human visual cognitive. Experiments demonstrated that our method can efficiently combine the strengths of various features to provide the robust representation, thereby improving the retrieval performance. Moreover, our method is easily to be implemented without requiring to retrain the CNN models and not the use of additional supervision.},
  archive      = {J_CC},
  author       = {Lu, Zhou and Liu, Guang-Hai and Li, Zuoyong and Yang, Lu},
  doi          = {10.1007/s12559-024-10375-0},
  journal      = {Cognitive Computation},
  month        = {2},
  number       = {1},
  pages        = {1-15},
  shortjournal = {Cogn. Comput.},
  title        = {Exploiting deep contrast feature for image retrieval},
  volume       = {17},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Development of a decision support system for performance measurement of social movements. <em>CC</em>, <em>17</em>(1), 1-27. (<a href='https://doi.org/10.1007/s12559-024-10385-y'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Social movements encompass the collective actions of groups gathered under the same goal, operating within a specific organizational structure to reflect their thoughts and views through a series of actions. Social movements are expected to exhibit characteristics of violence, rationality, continuity, and public benefit. The successful performance of social movements is contingent upon their ability to embody these characteristics. The primary motivation of this research is to render the performance of social movements measurable and comparable, aiming to determine their effectiveness. The underlying motivation for this approach is to move beyond subjective evaluations of social movements and instead treat them as structured decision-making processes. The core objective is to develop a decision support system for assessing the performance of social movements and facilitating insights into the performance of movements within a country or region. In this way, the performance evaluation processes of social movements are enhanced, fostering the development of more informed and deliberate social movements. To determine the performance of social movements, the type-2 neutrosophic number (T2NN)–Schweizer Sklar (SS)–symmetry point of criterion (SPC)–evaluation based on relative utility and nonlinear standardization (ERUNS)–(T2NN-SS-SPC-ERUNS) hybrid model is developed and proposed in this research. The T2NN-SS-weighted arithmetic mean aggregation operator is used to combine expert evaluations. The weights of criteria are calculated using the T2NN-SPC method. Performance rankings of social movements are determined using the T2NN-ERUNS method. An algorithm for the three-stage T2NN-SS-SPC-ERUNS hybrid model is developed to evaluate the performance of social movements in Türkiye through a case study. The robustness and consistency of the proposed hybrid method are supported by scenarios. As a result of the research, the “Early Retirement Scheme Victims” social movement is identified as having the highest performance among social movements in Türkiye.},
  archive      = {J_CC},
  author       = {Yalçın, Galip Cihan and Kara, Karahan and Işık, Gülcan and Simic, Vladimir and Pamucar, Dragan},
  doi          = {10.1007/s12559-024-10385-y},
  journal      = {Cognitive Computation},
  month        = {2},
  number       = {1},
  pages        = {1-27},
  shortjournal = {Cogn. Comput.},
  title        = {Development of a decision support system for performance measurement of social movements},
  volume       = {17},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). I2V-CMGAN: Generative adversarial cross-modal network-based image-to-video person re-identification. <em>CC</em>, <em>17</em>(1), 1-22. (<a href='https://doi.org/10.1007/s12559-024-10389-8'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Information asymmetry situation amongst image and video features in image-to-video (I2V) person re-identification (Re-ID) refers to the difficulty in extracting consistent and dependable features from both image and video data in order to accurately match and identify a person in both modalities. The problem arises because images and videos have different characteristics and represent different aspects of a person. This difference in representation can result in inconsistent and unreliable features being extracted from image and video data, leading to difficulty in accurately matching and re-identifying a person between modalities. The temporal information provided by videos can also boost the accuracy of person re-identification, especially in crowded and cluttered environments. To address the information asymmetry problem, a generative adversarial cross-modal network–based I2V Person Re-ID (I2V-CMGAN) is proposed, which works by using a generator to transform the features learned from the video network into an image network with an additional loss function to improve the consistency and reliability of features extracted from both image and video data and also preserve identity information. Extensive studies show the efficacy of the proposed approach, and the aggregate results on the MARS dataset outperform the state-of-the-art methods by a substantial margin and achieved rank-1 accuracy of 88.9% (+ 2.9), rank-5 accuracy of 95.5% (+ 2.3), rank-10 accuracy of 97.1% (+ 2.9), and mean average precision of 81.2% (+ 1.1) for I2V Re-ID. On iLIDS-VID and PRID2011 datasets, the proposed method attains outstanding margins with rank-1, rank-5, rank-10, mAP of 64.7%, 89.3%, 92.7%, 74.2%, and 80.9%, 93.3%, 98.9%, and 86.8% respectively.},
  archive      = {J_CC},
  author       = {Joshi, Aditya and Diwakar, Manoj},
  doi          = {10.1007/s12559-024-10389-8},
  journal      = {Cognitive Computation},
  month        = {2},
  number       = {1},
  pages        = {1-22},
  shortjournal = {Cogn. Comput.},
  title        = {I2V-CMGAN: Generative adversarial cross-modal network-based image-to-video person re-identification},
  volume       = {17},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Systematic review and thematic analysis of digital games for cognitive enhancement in children with autism spectrum disorder: Toward a conceptual framework. <em>CC</em>, <em>17</em>(1), 1-38. (<a href='https://doi.org/10.1007/s12559-024-10395-w'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Compared to typically developing people, children with autism spectrum disorder (ASD) have distinct cognitive and intelligence profiles. Some of these children require cognitive rehabilitation. Through the use of cutting-edge therapy and cognitive empowerment methods, some cognitive skills in children with ASD can be strengthened by digital game-based tools. This study’s main purpose is to provide a systematic review and qualitative study about designing digital games for cognitive enhancement in autistic children and to determine the main design components of such digital games. The primary focus of this study is to explore the citations in which the technical and functional elements are provided thoroughly. Furthermore, a conceptual framework is elaborated for designing a digital game for autism. A thorough review of the literature was conducted in the databases of Medline (via PubMed), Web of Science (WOS), Scopus, and IEEE Xplore for English publications published before January 23, 2023. Of 976 papers, 34 studies were found to be eligible in this systematic review. The bulk of the studies were carried out in Asia and Europe. Three (8.8%) studies used games that were built to be multilingual, while 22 (64.7%) studies used games that were only created in English. Creating motivation through narratives, providing incentive systems, raising the complexity level, targeting main skills, and adjusting the choices are the principal components of digital game design. (1) Main cognitive rehabilitation domains in ASD; (2) game designing details: platforms and game genres, motivations, evaluations, game graphics designs, aesthetic mechanisms, incentive systems, and famous game development engines; and (3) mutual interaction between child, therapist, and parents are the crucial categories that are described to devise a conceptual framework in this qualitative study. Of the total number of included studies, 25 studies reported positive effects on autism cases, and in nine, there has not been any evaluation of real cases; however, only usability tests have been conducted. Children with autism may benefit from using appropriate digital game-based interventions to improve mental indices. According to a review, it can be stated that the suitable computerized and digital game-based solutions could enhance cognitive outcomes in children with autism spectrum disorder. However, more research is required to ascertain the true efficacy of these new technologies.},
  archive      = {J_CC},
  author       = {Rezayi, Sorayya and Shahmoradi, Leila and Tehrani-Doost, Mehdi},
  doi          = {10.1007/s12559-024-10395-w},
  journal      = {Cognitive Computation},
  month        = {2},
  number       = {1},
  pages        = {1-38},
  shortjournal = {Cogn. Comput.},
  title        = {Systematic review and thematic analysis of digital games for cognitive enhancement in children with autism spectrum disorder: Toward a conceptual framework},
  volume       = {17},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A non-invasive approach for early alzheimer’s detection through spontaneous speech analysis using deep visibility graphs. <em>CC</em>, <em>17</em>(1), 1-18. (<a href='https://doi.org/10.1007/s12559-024-10398-7'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Identifying Alzheimer’s disease (AD) in its early stages is a challenging task for physicians and clinicians. This paper proposes a new algorithm for diagnosing AD, which is based on analyzing spontaneous speech signals. The proposed method uses two visibility graph methods, Natural Visibility Graph (NVG) and Horizontal Visibility Graph (HVG), to derive features from speech windows. These features are then given to a deep BiLSTM-based classifier to decide about segments of the signal. The proposed approach could obtain a sensitivity of 98.33%, specificity of 99.44%, and accuracy of 99.17%. The advantage of converting speech signals into graphs using NVG and HVG is that it allows for the extraction of complex structural features that are not easily captured by traditional methods. This method is highly beneficial due to its non-invasive nature, low cost, and lack of side effects. Patients can undergo the procedure without experiencing any discomfort, while also benefiting from its affordability and accessibility. The method’s safety and practicality make it an ideal choice for those seeking a reliable and effective solution. Moreover, the proposed algorithm has a high accuracy in detecting the early stage of AD, which makes it a promising tool to evaluate Alzheimer’s disease diagnosis in its pre-clinical stage.},
  archive      = {J_CC},
  author       = {Mohammadpoory, Zeynab and Nasrolahzadeh, Mahda and Amiri, Sekineh Asadi and Haddadnia, Javad},
  doi          = {10.1007/s12559-024-10398-7},
  journal      = {Cognitive Computation},
  month        = {2},
  number       = {1},
  pages        = {1-18},
  shortjournal = {Cogn. Comput.},
  title        = {A non-invasive approach for early alzheimer’s detection through spontaneous speech analysis using deep visibility graphs},
  volume       = {17},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Short-term power load forecasting in city based on ISSA-BiTCN-LSTM. <em>CC</em>, <em>17</em>(1), 1-24. (<a href='https://doi.org/10.1007/s12559-024-10401-1'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Accurate short-term power load forecasting is crucial for the stable operation of power systems. In this paper, we propose an advanced forecasting model that combines the Salp Swarm Algorithm (SSA), Bidirectional Temporal Convolutional Network (BiTCN), and Long Short-Term Memory (LSTM). The model first exploits the parallel fusion of BiTCN and LSTM (BiTCN-LSTM), taking full advantage of BiTCN’s strength in parallel processing of local features and the LSTM’s ability to capture long-term dependencies through its gating mechanisms. Subsequently, the Improved Salp Swarm Algorithm (ISSA) is enhanced through adaptive leader ratio adjustment, dual-food design, and food lure follower strategy. Finally, the hyperparameters of the BiTCN-LSTM model are optimized using ISSA to improve the model performance. In the short-term load forecasting experiments, electric load data and weather data from Los Angeles, Tetouan, and Johor were used to compare the proposed model with eight existing models. The evaluation metrics included root mean square error (RMSE), mean absolute error (MAE), normalized root mean square error (NRMSE), and mean absolute percentage error (MAPE). The experimental results showed that the model achieved lower error values than the comparison model in most cases in different seasons, working days, and rest days in different cities. In particular, the error values of RMSE, MAE, NRMSE, and MAPE were 925.11 kW, 732.63 kW, 0.019, and 1.034% for the rest days in the city of Tetouan, respectively. Compared with other algorithms, ISSA demonstrates stronger optimization capability and shorter optimization time. Additionally, model structure analysis was conducted through optimization comparison and ablation experiments, further demonstrating the proposed model’s strong predictive performance.},
  archive      = {J_CC},
  author       = {Fan, Chaodong and Li, Gongrong and Xiao, Leyi and Yi, Lingzhi and Nie, Shanghao},
  doi          = {10.1007/s12559-024-10401-1},
  journal      = {Cognitive Computation},
  month        = {2},
  number       = {1},
  pages        = {1-24},
  shortjournal = {Cogn. Comput.},
  title        = {Short-term power load forecasting in city based on ISSA-BiTCN-LSTM},
  volume       = {17},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Application of metaheuristic algorithms with supervised machine learning for accurate power consumption prediction. <em>CC</em>, <em>17</em>(1), 1-35. (<a href='https://doi.org/10.1007/s12559-025-10402-8'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Accurate power consumption prediction is a crucial part of energy management. Some of the machine learning models that are the focus of this study for the prediction of power use include Support Vector Regression, Adaptive Boosting, and Decision Tree Regression. These models have been improved with the use of some novel optimizers-namely, the Trochoid Search Optimization, Red-Tailed Hawk, and Giant Armadillo Optimization methods-for hyper-parameter tuning to enhance prediction accuracy. When tested against real data, DTGA outperformed with R2 values of 0.9918, 0.9924, and 0.9934 for three zones. This work extends the study on the forecast of power consumption by integrating machine learning and optimization techniques that provide effective energy management strategies.},
  archive      = {J_CC},
  author       = {Wang, Mengxia and Zhu, Chaoyang and Zhang, Yunxiang and Deng, Jinxin and Cai, Yiwei and Wei, Wei and Guo, Mengxing},
  doi          = {10.1007/s12559-025-10402-8},
  journal      = {Cognitive Computation},
  month        = {2},
  number       = {1},
  pages        = {1-35},
  shortjournal = {Cogn. Comput.},
  title        = {Application of metaheuristic algorithms with supervised machine learning for accurate power consumption prediction},
  volume       = {17},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Context-aware prediction with secure and lightweight cognitive decision model in smart cities. <em>CC</em>, <em>17</em>(1), 1-12. (<a href='https://doi.org/10.1007/s12559-025-10403-7'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cognitive networks with the integration of smart and physical devices are rapidly utilized for the development of smart cities. They are explored by many real-time applications such as smart homes, healthcare, safety systems, and other unpredictable environments to gather data and process network requests. However, due to the external conditions and inherent uncertainty of wireless systems, most of the existing approaches cannot cope with routing disturbances and timely delivery performance. Further, due to limited resources, the demand for a secure communication system raises another potential research challenge to protect sensitive data and maintain the integrity of the urban environment. This paper presents a secured decision-making model using reinforcement learning with the combination of blockchain to enhance the degree of trust and data protection. The proposed model increases the network efficiency for resource utilization and the management of communication devices with the alliance of security. It provides a reliable and more adaptive paradigm by exploring learning techniques for dealing with the intrinsic uncertainty and imprecision of cognitive systems. Also, the incorporation of blockchain technology reduces the risk of a single point of failure, malicious vulnerabilities, and data leakage, ultimately fostering trust for urban sensor applications. It validates the incoming routing links and identifies any communication fault incurred due to malicious interference. The proposed model is rigorously tested and verified using simulations and its significance has been proven for network metrics in comparison to existing solutions.},
  archive      = {J_CC},
  author       = {Al-Quayed, Fatima and Humayun, Mamoona and Alnusairi, Thanaa S. and Ullah, Inam and Bashir, Ali Kashif and Hussain, Tariq},
  doi          = {10.1007/s12559-025-10403-7},
  journal      = {Cognitive Computation},
  month        = {2},
  number       = {1},
  pages        = {1-12},
  shortjournal = {Cogn. Comput.},
  title        = {Context-aware prediction with secure and lightweight cognitive decision model in smart cities},
  volume       = {17},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Engaging preference optimization alignment in large language model for continual radiology report generation: A hybrid approach. <em>CC</em>, <em>17</em>(1), 1-25. (<a href='https://doi.org/10.1007/s12559-025-10404-6'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Large language models (LLMs) remain relatively underutilized in medical imaging, particularly in radiology, which is essential for disease diagnosis and management. Nonetheless, radiology report generation (RRG) is a time-consuming task that can result in delays and inconsistencies. To address these challenges, we present a novel hybrid approach that integrates multi-modal radiology information and preference optimization alignment in LLM for continual RRG. Our method integrates a pre-trained small multi-modal model to analyze radiology images and generate an initial report, which is subsequently refined and aligned by an LLM using odds ratio preference optimization (ORPO) and with historical patient data and assessments to mimic radiologist-like responses, bypassing reinforcement learning from human feedback-based (RLHF) alignment. This two-stage fusion—supervised fine-tuning followed by preference optimization—ensures high accuracy while minimizing hallucinations and errors. We also propose a data field curation strategy extendable to various other RRG modality datasets, focusing on selecting relevant responses for preference alignment. We evaluate our approach on two public datasets, achieving state-of-the-art performance with average Bleu scores of 0.375 and 0.647, Meteor scores of 0.495 and 0.714, Rouge-L scores of 0.483 and 0.732, and average F1-RadGraph scores of 0.488 and 0.487, for chest X-rays and lung CT scan datasets, respectively. We further provide in-depth qualitative analyses and ablation studies to explain the workings of our model and grasp the clinical relevance for RRG. This work presents the first application of preference optimization in continual RRG, representing a significant advancement in automating clinically reliable report generation. By reducing cognitive burdens on radiologists through AI-powered reasoning and alignment in LLMs, the proposed model improves decision-making, perception, and diagnostic precision, streamlining workflows and enhancing patient care. Our code is available at https://github.com/AI-14/r2gpoallm .},
  archive      = {J_CC},
  author       = {Izhar, Amaan and Idris, Norisma and Japar, Nurul},
  doi          = {10.1007/s12559-025-10404-6},
  journal      = {Cognitive Computation},
  month        = {2},
  number       = {1},
  pages        = {1-25},
  shortjournal = {Cogn. Comput.},
  title        = {Engaging preference optimization alignment in large language model for continual radiology report generation: A hybrid approach},
  volume       = {17},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A novel hyperparameter optimization approach for supervised classification: Phase prediction of multi-principal element alloys. <em>CC</em>, <em>17</em>(1), 1-14. (<a href='https://doi.org/10.1007/s12559-025-10405-5'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, a hyperparameter optimization approach is proposed for the phase prediction of multi-principal element alloys (MPEAs) through the introduction of two novel hyperparameters: outlier detection and feature subset selection. To gain a deeper understanding of the connection between alloy phases and their elemental properties, an artificial neural network is employed, with hyperparameter optimization performed using a genetic algorithm to select the optimum hyperparameters. The two novel hyperparameters, outlier detection and feature subset selection, are introduced within the optimization framework, along with new crossover and mutation operators for handling single and multi-valued genes simultaneously. Ablation studies are conducted, illustrating an improvement in prediction accuracy with the inclusion of these new hyperparameters. A comparison with five existing algorithms in multi-class classification is made, demonstrating an improvement in the performance of phase prediction, thereby providing a better perception of the alloy phase space for high-throughput MPEA design.},
  archive      = {J_CC},
  author       = {Fatimi, Syed Hassan and Wang, Zidong and Chang, Isaac T. H. and Liu, Weibo and Liu, Xiaohui},
  doi          = {10.1007/s12559-025-10405-5},
  journal      = {Cognitive Computation},
  month        = {2},
  number       = {1},
  pages        = {1-14},
  shortjournal = {Cogn. Comput.},
  title        = {A novel hyperparameter optimization approach for supervised classification: Phase prediction of multi-principal element alloys},
  volume       = {17},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Augmenting cardiovascular disease prediction through CWCF integration leveraging harris hawks search in deep belief networks. <em>CC</em>, <em>17</em>(1), 1-20. (<a href='https://doi.org/10.1007/s12559-025-10406-4'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cardiovascular disease (CVD) is a major global health concern, demanding accurate predictive models to aid preventive healthcare strategies. Heart failure, stroke, and coronary artery disease are among the disorders that fall under the umbrella term of cardiovascular disease (CVD). Leveraging the Harris Hawks Optimization (HHO) algorithm in conjunction with deep belief networks (DBNs) aims to improve CVD risk prediction accuracy. Harris Hawks Optimization (HHO) draws inspiration from the cooperative behavior of Harris’s hawks in nature, providing an efficient metaheuristic search algorithm for optimization problems. Integrating HHO into machine learning frameworks enhances the exploration and exploitation of search spaces, leading to improved model performance and convergence rates, particularly in deep learning tasks like feature selection and hyperparameter tuning. Powerful generative models called deep belief networks (DBNs) are made up of several layers of latent, stochastic variables. Restricted Boltzmann machines (RBMs) serve as building blocks in the training process of deep belief networks (DBNs), facilitating the unsupervised pre-training of hidden layers. Leveraging RBMs within DBNs enables the extraction of hierarchical representations, enhancing the network’s ability to learn intricate patterns and improve predictive performance in complex data settings. They leverage unsupervised learning techniques to extract intricate patterns and hierarchical representations from complex data, making them ideal for tasks such as feature learning and classification in machine learning research. This study introduces innovative algorithms, including the correlation-based weighted compound feature generation (CWCFG) technique, to enhance the optimization process of HHO. Comparative analysis against traditional machine learning models and rule-based firefly optimizer (RBFO) and Grey Wolf Optimizer (GWO) with the state-of-the-art deep learning techniques demonstrates the efficacy of the CWCFG-HHO-DBN model. Additionally, an in-depth feature importance analysis identifies key predictors, enriching the model’s interpretability. The research conducts a comprehensive evaluation of the proposed model, employing various performance metrics such as accuracy, precision, recall, and F-measure. With a remarkable accuracy of 97.19%, the HHO-DBN model shows promise in enhancing CVD risk prediction. The findings underscore its potential in personalized medicine, facilitating tailored interventions for high-risk individuals. Future directions include refining the algorithm and expanding its application in healthcare settings.},
  archive      = {J_CC},
  author       = {Savitha, S. and Kannan, A. Rajiv and Logeswaran, K.},
  doi          = {10.1007/s12559-025-10406-4},
  journal      = {Cognitive Computation},
  month        = {2},
  number       = {1},
  pages        = {1-20},
  shortjournal = {Cogn. Comput.},
  title        = {Augmenting cardiovascular disease prediction through CWCF integration leveraging harris hawks search in deep belief networks},
  volume       = {17},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). An asymmetric semantic segmentation model via lightweight attention-guided feature enhancement and fusion. <em>CC</em>, <em>17</em>(1), 1-16. (<a href='https://doi.org/10.1007/s12559-025-10407-3'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Semantic segmentation is widely used in fields such as autonomous driving and unmanned aerial vehicle navigation. However, the huge computational burden and redundant parameters limit its application in edge devices such as mobile phones. In this study, we propose an asymmetric lightweight semantic segmentation model via lightweight attention-guided feature enhancement and fusion. Specifically, the proposed model adopts an encoder-decoder structure. In the encoder, we design an asymmetric feature extraction module to extract image information and use the locally sensitive Hash self-attention to enhance the global information. In the decoder, we first use channel attention to filter out the useless information in shallow layers and adopt the spatial attention to refine local features in deep layers. We then fuse the multi-scale features by the gating mechanism. Additionally, we also design an auxiliary loss to supervise the segmentation of small objects. The results on Cityscapes and CamVid show that the proposed model achieves a good balance between accuracy and the number of parameters. It obtains 70.68% and 72.19% mIoU on the two test datasets with 0.86M parameters, respectively. Code is available on https://github.com/year410/LAANET},
  archive      = {J_CC},
  author       = {Tang, Qingsong and Zhao, Minghui and Ren, Yalei and Shi, Xiaomeng and Jiang, Wuming},
  doi          = {10.1007/s12559-025-10407-3},
  journal      = {Cognitive Computation},
  month        = {2},
  number       = {1},
  pages        = {1-16},
  shortjournal = {Cogn. Comput.},
  title        = {An asymmetric semantic segmentation model via lightweight attention-guided feature enhancement and fusion},
  volume       = {17},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Dynamic behavior of three-layer fractional-order neural networks with multiple delays. <em>CC</em>, <em>17</em>(1), 1-17. (<a href='https://doi.org/10.1007/s12559-025-10411-7'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Most of the complex network in the real world are not single-layer networks, and networks will be connected with each other. Networks with multi-layer is important because it means cognitive and artificial intelligence. Most current studies of networks consider the case that with n-nodes including ring network, small-word network, scale-free network, etc. This type of network is not enough to describe the complex structure of actual neural networks. However, it is more actual to study the dynamic behavior of multi-layer networks than single-layer networks. In this paper, the stability and bifurcation of a class of three-layer fractional-order neural networks with multiple delays was studied for the first time. By selecting the appropriate bifurcation parameter, the internal dynamic behavior of the given model was discussed by using the theory of Hopf bifurcation, and the critical value and criterion for Hopf bifurcation are derived. The influence of delay, fractional order, and the number of hidden neurons on the bifurcation point were discussed in detail. And the critical value of Hopf bifurcation is accurately calculated. The results show that the stability of the system can be destroyed by increasing the fractional order and the number of hidden neurons. The correctness of the theoretical results is verified by numerical simulation.},
  archive      = {J_CC},
  author       = {Li, Xinyu and Cheng, Zunshui and Xin, Youming and Shang, Yun},
  doi          = {10.1007/s12559-025-10411-7},
  journal      = {Cognitive Computation},
  month        = {2},
  number       = {1},
  pages        = {1-17},
  shortjournal = {Cogn. Comput.},
  title        = {Dynamic behavior of three-layer fractional-order neural networks with multiple delays},
  volume       = {17},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Learning to calibrate prototypes for few-shot image classification. <em>CC</em>, <em>17</em>(1), 1-13. (<a href='https://doi.org/10.1007/s12559-025-10412-6'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Few-shot learning (FSL) aims to generalise the model to novel classes by using a limited amount of discriminative samples (a.k.a., prototypes). With few labelled samples, there is much uncertainty and randomness in the data, which makes it more difficult for the model to learn the complete underlying patterns. This paper proposes a Discriminative Property Calibration Network (DPCNet) to enable model building in linear space with robust separability. Concretely, the property features of samples are extracted to facilitate filtering out low-informative key points at the instance level, and then the key points are further refined from the perspective of property features to retain those dimensions that contain the most relevant properties. Furthermore, the discriminative key properties are re-weighted by accounting for the correlation between images, thus forcing the model to focus more on the key property information. Moreover, a new margin algorithm is proposed to optimise the data distribution of features by dynamically adjusting the distance between classes. We conduct extensive experiments on four datasets, i.e., miniImageNet, tiredImagenet, CUB-200-2011 and CIFAR-FS, achieving the accuracies of 67.96%, 72.57%, 79.6% and 74.56%, respectively, on the 5-way 1-shot setting, and the same very competitive performance on the 5-way 5-shot setting. The proposed method can well extract the most relevant and discriminative properties, the re-weighted features further emphasise the discrimination and the dynamic margin algorithm enhances the stability and generalisation ability. The proposed method achieves the state-of-the-art performance, and it will have meaningful inspiration for future works.},
  archive      = {J_CC},
  author       = {Liang, Chenchen and Jiang, Chenyi and Wang, Shidong and Zhang, Haofeng},
  doi          = {10.1007/s12559-025-10412-6},
  journal      = {Cognitive Computation},
  month        = {2},
  number       = {1},
  pages        = {1-13},
  shortjournal = {Cogn. Comput.},
  title        = {Learning to calibrate prototypes for few-shot image classification},
  volume       = {17},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Tweet credibility ranker: A credibility features’ fusion model. <em>CC</em>, <em>17</em>(1), 1-36. (<a href='https://doi.org/10.1007/s12559-025-10413-5'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Misinformation on social media has emerged as a modern weapon of warfare, disrupting societal peace, trust, justice, and democracy. It is quite challenging to address the issue of information credibility for microblogs. It becomes more challenging when the authenticity of the poster is hidden. The concept of information credibility has multi-perspectives. There are many necessary aspects of information credibility which must be considered for effective credibility assessment. It is observed that some important aspects of credibility are not considered in existing studies. The complete credibility assessment solution needs a comprehensive and diverse set of features for such complex identification. Therefore, these features are identified and proposed by exploring the related research studies consisting of the necessary credibility aspects. These features consist of diverse levels provided by microblogs. These levels include the post, poster, poster’s social network, and actual information propagation network. An exploratory study is also conducted to propose the best credibility features that are used in the proposed solution. The attempt is made for a hybrid features fusion model which combines feature-based or machine learning and graph-based approaches. It is a lightweight, high-performing, non-latent features model to avoid their drawbacks. It assesses the levels of credibility of the concerned post. It is designed for high-impact applications to combat low-credibility content during elections, crises, and other critical scenarios. The model is executed over a publicly available dataset extended for credibility assessment. The model provides good results with 95.6% accuracy by XGBoost using platinum features. The performance of the proposed model is compared with state-of-the-art that produced much-appreciating results.},
  archive      = {J_CC},
  author       = {Qureshi, Khubaib Ahmed and Malick, Rauf Ahmed Shams},
  doi          = {10.1007/s12559-025-10413-5},
  journal      = {Cognitive Computation},
  month        = {2},
  number       = {1},
  pages        = {1-36},
  shortjournal = {Cogn. Comput.},
  title        = {Tweet credibility ranker: A credibility features’ fusion model},
  volume       = {17},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). An adaptive neural network algorithm with quasi opposition-based learning for numerical optimization problems. <em>CC</em>, <em>17</em>(1), 1-29. (<a href='https://doi.org/10.1007/s12559-025-10415-3'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The structure of artificial neural networks and the biological nervous systems serve as the foundation for the creation of the neural network algorithm (NNA). The robust global search capability of NNAs makes it an effective tool for solving a wide range of complex optimization problems. Unfortunately, its limited relevance to many optimization problems is due to its poor exploitation, weak convergence, and tendency to fall into local optima. The paper’s goal is to introduce an enhanced version of the NNA known as the adaptive quasi-opposition-based neural network algorithm (AQOBNNA) in order to overcome these issues. The quasi-opposition-based learning (QOBL) and an adaptive strategy are combined in this suggested algorithm, where the adaptive technique is added to determine whether or not to use QOBL. The QOBL technique replaces a random search individual with the best one throughout the position update phase in order to enhance exploitation and increase exploration capabilities. The performance of the suggested AQOBNNA is assessed using a set of 23 traditional benchmark functions and compared with a number of current methods. It is evident from the experimental data that AQOBNNA performs better overall and outperforms all the algorithms that were examined.},
  archive      = {J_CC},
  author       = {Kundu, Tanmay and Garg, Harish},
  doi          = {10.1007/s12559-025-10415-3},
  journal      = {Cognitive Computation},
  month        = {2},
  number       = {1},
  pages        = {1-29},
  shortjournal = {Cogn. Comput.},
  title        = {An adaptive neural network algorithm with quasi opposition-based learning for numerical optimization problems},
  volume       = {17},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Exploring influence of different emotions on decision-making by analyzing the temporal, spatial, and spectral domains of EEG. <em>CC</em>, <em>17</em>(1), 1-14. (<a href='https://doi.org/10.1007/s12559-025-10416-2'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Decision-making is a complex cognitive process, in which emotion is one of the most important factors. But insights into the influence of emotion on decision-making are scarce, especially the underlying mechanism of the brain. To reveal the brain’s underlying mechanisms of the influence of emotion on decision-making, an experiment involving emotion elicitation and decision-making tasks was designed. Electroencephalography (EEG), behavioral, and subjective data were collected and conducted. We constructed time-varying weighted directed networks by phase slope index (PSI) in four frequency bands and calculated graph theory metrics. Firstly, the period that the brain processes information most efficiently is 100–300 ms after the appearance of the decision-making task. Secondly, by analyzing the temporal-spatial domains of EEG, the significant differences in global efficiency (GE) and local efficiency (LE) were found among three different emotion groups in the alpha band in the low-difficulty task during 100–300 ms. Thirdly, most activation regions of different emotions were similar and concentrated in the parietal, and occipital lobes but there were still slight differences that were more likely to be found in the prefrontal and left temporal lobes. Graph theory metrics in the decision-making process changed dynamically in the temporal domain and graph theory metrics of different emotions were different.},
  archive      = {J_CC},
  author       = {Wang, Xinyuan and Wang, Danli and Zhao, Yanyan},
  doi          = {10.1007/s12559-025-10416-2},
  journal      = {Cognitive Computation},
  month        = {2},
  number       = {1},
  pages        = {1-14},
  shortjournal = {Cogn. Comput.},
  title        = {Exploring influence of different emotions on decision-making by analyzing the temporal, spatial, and spectral domains of EEG},
  volume       = {17},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). HLAE: Hierarchical local attention encoder for MRI brain tumor image classification. <em>CC</em>, <em>17</em>(1), 1-16. (<a href='https://doi.org/10.1007/s12559-025-10419-z'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {MRI-based brain tumor classification is a challenging neuroimaging task, where the key lies in leveraging ensemble information from brain images. However, current algorithms primarily encode global appearance features of brain images and fail to account for local dependencies inherent in brain tissue adequately. In addition, most existing approaches do not thoroughly investigate the importance coefficients among different regions of brain images. To address these issues, in this work, we propose a novel cognitively-inspired hierarchical local attention encoder (HLAE) framework for capturing local dependency information in MRI images. Based on the characteristics of MRI images, we focus on local information from two perspectives: long-range visual feature dependencies and high-order structural context correlations to fully describe the content association and location relations of brain MRI images. For this purpose, a Swin Transformer is first utilized for encoding the patch-wise content dependencies of a brain MRI image by shifting windows and skipping connections. Meanwhile, a graph structure is also extracted from the MRI image, and a graph attention network is employed to capture the image’s contextual correlations. Finally, the two local information are integrated, and a softmax layer is used to obtain the final brain tumor classification result. This framework naturally contains the attention mechanism, which can effectively quantify the importance among different brain image regions, so as to locate the most discriminative regions in brain tumor classification accurately. Extensive experiments are conducted on two publicly available brain tumor MRI datasets. The results demonstrate its ability to automatically detect brain tumors with superior performance compared to state-of-the-art algorithms.},
  archive      = {J_CC},
  author       = {Dong, Changxu and Sun, Dengdi and Luo, Bin},
  doi          = {10.1007/s12559-025-10419-z},
  journal      = {Cognitive Computation},
  month        = {2},
  number       = {1},
  pages        = {1-16},
  shortjournal = {Cogn. Comput.},
  title        = {HLAE: Hierarchical local attention encoder for MRI brain tumor image classification},
  volume       = {17},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Unleashing the power of generative AI in agriculture 4.0 for smart and sustainable farming. <em>CC</em>, <em>17</em>(1), 1-18. (<a href='https://doi.org/10.1007/s12559-025-10420-6'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Generative artificial intelligence (GAI) represents a pioneering class of artificial intelligence systems renowned for producing diverse media, such as text and images. Agriculture 4.0 (AG-4.0) is a concept that integrates advanced technologies such as the Internet of Things (IoT), data analytics, artificial intelligence, and precision agriculture into the agricultural sector. The integration of GAI and AG-4.0 can generate new and valuable agricultural insights and solutions through pattern recognition and data analysis. This integration enhances farming practices by generating predictive models, simulating optimal growth conditions, diagnosing plant diseases, and optimizing genetic traits. In spite of the tremendous scope of GAI in agriculture, there has been no detailed study concerning the applications and scope of GAI in AG-4.0. Addressing this research gap, we explore various applications, real-world products, and limitations of GAI in agriculture. We explore how GAI models such as ChatGPT and Dall-E can be personalized advisors for farmers, help increase awareness about farmer relief programs, design farm layouts, and many other such applications. Additionally, we cover four real-world GAI products deployed to assist farmers. Since GAI is a growing technology, it poses challenges such as scarcity of data, data privacy, and interpretability. We elaborately discuss these limitations and suggest multiple directions for future research in GAI for agriculture.},
  archive      = {J_CC},
  author       = {Sai, Siva and Kumar, Sanjeev and Gaur, Aanchal and Goyal, Shivam and Chamola, Vinay and Hussain, Amir},
  doi          = {10.1007/s12559-025-10420-6},
  journal      = {Cognitive Computation},
  month        = {2},
  number       = {1},
  pages        = {1-18},
  shortjournal = {Cogn. Comput.},
  title        = {Unleashing the power of generative AI in agriculture 4.0 for smart and sustainable farming},
  volume       = {17},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Innovative deep learning framework for accurate plant disease detection and crop productivity enhancement. <em>CC</em>, <em>17</em>(1), 1-17. (<a href='https://doi.org/10.1007/s12559-025-10421-5'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In modern agriculture, the detection of plant diseases is crucial for enhancing crop productivity. Predicting disease onset and providing advice to farmers are essential steps to achieve increased yields on a large scale. The research addresses the critical need for timely and accurate plant leaf disease diagnosis to prevent growth issues. Leveraging deep learning advancements, the work confronts challenges like small lesion characteristics, distorted backgrounds, data imbalances, and limited generalization in agricultural datasets. After preprocessing the leaf images with tasks like data augmentation and resizing, a sheaf attention U-net with K-means clustering (SAUKC) is employed for segmentation to identify the region of interest. The segmented features are then input into the Orientation-guided Crystal Edge Deep Network (OCEDN) for infection detection. Fine-tuning with the improved kookaburra optimization algorithm (IKOA) addresses training challenges. The proposed method accurately identifies plant leaf diseases, achieving a remarkable accuracy rate of 98%. The validity of the statistical analysis is confirmed to substantiate the outcomes regarding accuracy, specificity, and recall.},
  archive      = {J_CC},
  author       = {M., Mohan and Anandamurugan, S.},
  doi          = {10.1007/s12559-025-10421-5},
  journal      = {Cognitive Computation},
  month        = {2},
  number       = {1},
  pages        = {1-17},
  shortjournal = {Cogn. Comput.},
  title        = {Innovative deep learning framework for accurate plant disease detection and crop productivity enhancement},
  volume       = {17},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Verifying technical indicator effectiveness in cryptocurrency price forecasting: A deep-learning time series model based on sparrow search algorithm. <em>CC</em>, <em>17</em>(1), 1-21. (<a href='https://doi.org/10.1007/s12559-025-10422-4'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Forecasting cryptocurrency prices is challenging due to market volatility and dynamic behavior. This study aims to enhance prediction accuracy for Bitcoin (BTC), Ethereum (ETH), and Litecoin (LTC) by proposing a novel deep learning framework. The framework integrates the Sparrow Search Algorithm (SSA) for selecting optimal technical indicators with Bidirectional Long Short-Term Memory (Bi-LSTM) networks. Technical indicators derived from historical market data, including prices and trading volume, are analyzed to improve forecasting. The results demonstrate that the proposed framework effectively enhances prediction accuracy for BTC and LTC. For ETH, the best performance is achieved using all 34 indicators with the Bi-LSTM model. These findings highlight the importance of selecting relevant indicators and demonstrate the potential of advanced deep learning models in addressing the complexities of cryptocurrency markets. This research provides valuable insights and a reliable framework for improving cryptocurrency price predictions.},
  archive      = {J_CC},
  author       = {Cheng, Ching-Hsue and Yang, Jun-He and Dai, Jia-Pei},
  doi          = {10.1007/s12559-025-10422-4},
  journal      = {Cognitive Computation},
  month        = {2},
  number       = {1},
  pages        = {1-21},
  shortjournal = {Cogn. Comput.},
  title        = {Verifying technical indicator effectiveness in cryptocurrency price forecasting: A deep-learning time series model based on sparrow search algorithm},
  volume       = {17},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Functional connectivity imbalance between positive and negative networks in mild cognitive impairment via feature selection. <em>CC</em>, <em>17</em>(1), 1-17. (<a href='https://doi.org/10.1007/s12559-024-10399-6'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_CC},
  author       = {Wu, Haifeng and Pu, Changlin},
  doi          = {10.1007/s12559-024-10399-6},
  journal      = {Cognitive Computation},
  month        = {2},
  number       = {1},
  pages        = {1-17},
  shortjournal = {Cogn. Comput.},
  title        = {Functional connectivity imbalance between positive and negative networks in mild cognitive impairment via feature selection},
  volume       = {17},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Attribute reduction in a hybrid decision information system based on fuzzy conditional information entropy using iterative model and matrix operation. <em>CC</em>, <em>17</em>(1), 1-20. (<a href='https://doi.org/10.1007/s12559-024-10400-2'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Attribute reduction of hybrid decision information systems (HDISs) is a significant research area within the field of machine learning. Due to the presence of nominal attributes, it is difficult to accurately measure the distance between objects in HDISs, which often results in poor attribute reduction for these systems. Rough set theory (RST) is a crucial tool for attribute reduction, but it requires computation of upper and lower approximations, which often leads to computational difficulties. In response to the aforementioned issues, this paper proposes a fast attribute reduction algorithm for HDISs based on fuzzy conditional information entropy that utilizes an iterative model and matrix operations. Firstly, a novel measurement of the distance between nominal attribute values is defined using decision attributes. Subsequently, fuzzy conditional information entropy is calculated from the perspective of “the attribute values is fed back to the attribute set” and its properties are provided. Additionally, an iterative attribute reduction model and difference matrix are established, and two new matrix operations are introduced. Finally, an iterative attribute reduction algorithm is provided. The results of experiments and statistical tests on fifteen UCI datasets, including three large datasets, demonstrate that the proposed algorithm is more effective and efficient than nine state-of-the-art algorithms. This paper not only addresses the issue of difficulty in measuring the distance between nominal attribute values but also significantly improves the computational efficiency of attribute reduction algorithms based on RST, making it possible for them to be applied to large datasets.},
  archive      = {J_CC},
  author       = {Ma, Xiaoqin and Peng, Yichun and Yu, Wenchang and Xu, Yi and Zhang, Qinli and Li, Zhaowen},
  doi          = {10.1007/s12559-024-10400-2},
  journal      = {Cognitive Computation},
  month        = {2},
  number       = {1},
  pages        = {1-20},
  shortjournal = {Cogn. Comput.},
  title        = {Attribute reduction in a hybrid decision information system based on fuzzy conditional information entropy using iterative model and matrix operation},
  volume       = {17},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A novel multi-attribute group decision-making method under linguistic q-rung orthopair fuzzy environment based on archimedean copula and extended power average operator. <em>CC</em>, <em>17</em>(1), 1-28. (<a href='https://doi.org/10.1007/s12559-025-10409-1'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-attribute group decision-making (MAGDM) refers to a series of decision-making problems that rank all possible alternatives based on decision makers’ cognition and evaluations over alternatives from multiple attributes. Hence, the precondition of MAGDM is felicitously describing decision makers’ fuzzy and uncertain cognitive information in complicated decision-making issues. The recently proposed linguistic q-rung orthopair fuzzy set (Lq-ROFS), which uses two linguistic terms to denote membership and non-membership degrees, has been proved to be an effective and promising tool to depict decision makers’ complex cognition in real MAGDM problems. Considering the drawbacks of existing Lq-ROFS-based decision-making methods, this paper focuses on MAGDM approaches where decision makers’ cognitive information is denoted by Lq-ROFSs. The main contribution of this paper is to propose a novel MAGDM method based on Lq-ROFSs. This paper introduces a new MAGDM method under Lq-ROFSs. In order to do this, this study first puts forward some new operational rules for linguistic q-rung orthopair fuzzy numbers (Lq-ROFNs) based on Archimedean copula. These new operational rules are more flexible than existing ones and some other operations can be derived by using different generators. Second, to effectively aggregate Lq-ROFNs, the extended power average operator is applied in linguistic q-rung orthopair fuzzy environment and based on the new operational rules, some novel aggregation operators are generated. Afterward, the developed aggregation operators are used in decision-making problems and a novel MAGDM method which concentrates on linguistic q-rung orthopair fuzzy decision environment is introduced. Specific steps of the new method are illustrated in detail and it is then applied in some illustrative examples to verify its effectiveness. Our proposed method is effective for handling MAGDM problems under Lq-ROFSs. Numerical examples have shown the effectiveness in handling realistic MAGDM problems. In addition, comparison with some existing methods illustrates the advantages and superiorities of our method. This paper introduces a new MAGDM method under Lq-ROFSs. This method is based on Archimedean copula, extended power average operator, and Lq-ROFSs, and is powerful and flexible to cope with MAGDM problems in reality.},
  archive      = {J_CC},
  author       = {Tang, Fangcheng and Zhang, Yushu and Wang, Jun},
  doi          = {10.1007/s12559-025-10409-1},
  journal      = {Cognitive Computation},
  month        = {2},
  number       = {1},
  pages        = {1-28},
  shortjournal = {Cogn. Comput.},
  title        = {A novel multi-attribute group decision-making method under linguistic q-rung orthopair fuzzy environment based on archimedean copula and extended power average operator},
  volume       = {17},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Discovering the cognitive bias of toxic language through metaphorical concept mappings. <em>CC</em>, <em>17</em>(1), 1-21. (<a href='https://doi.org/10.1007/s12559-025-10423-3'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the prosperity of social media, toxic language spreading over social media has become an unignorable challenge for individual mental health and social harmony. Many researchers have studied toxic language identification to control or mitigate it. However, it still leaves a blank in the cognitive patterns of toxic language. Metaphors as a common feature in natural language connect literal and metaphorical meanings, which could be a useful tool to study the underlying cognitive patterns of the text. In this paper, we utilize a metaphor processing tool, MetaPro, to process a public toxic language dataset and analyze the cognitive biases between toxic and non-toxic language, multiple levels and subtypes of toxic language as well as toxic language mentioning different genders, sexual orientations, and races. Our study demonstrates that significant differences exist in cognitive patterns of the above-mentioned categories and analyzes the differences with machine learning methods.},
  archive      = {J_CC},
  author       = {Ge, Mengshi and Mao, Rui and Cambria, Erik},
  doi          = {10.1007/s12559-025-10423-3},
  journal      = {Cognitive Computation},
  month        = {2},
  number       = {1},
  pages        = {1-21},
  shortjournal = {Cogn. Comput.},
  title        = {Discovering the cognitive bias of toxic language through metaphorical concept mappings},
  volume       = {17},
  year         = {2025},
}
</textarea>
</details></li>
</ul>

</body>
</html>
