<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>AIR</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="air">AIR - 393</h2>
<ul>
<li><details>
<summary>
(2025). AI apology: A critical review of apology in AI systems. <em>AIR</em>, <em>58</em>(12), 1-78. (<a href='https://doi.org/10.1007/s10462-025-11305-8'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Apologies are a powerful tool used in human-human interactions to provide affective support, regulate social processes, and exchange information following a trust violation. The emerging field of AI apology investigates the use of apologies by artificially intelligent systems, with recent research suggesting how this tool may provide similar value in human-machine interactions. Until recently, contributions to this area were sparse, and these works have yet to be synthesised into a cohesive body of knowledge. This article provides the first synthesis and critical analysis of the state of AI apology research, focusing on studies published between 2020 and 2023. We derive a framework of attributes to describe five core elements of apology: outcome, interaction, offence, recipient, and offender. With this framework as the basis for our critique, we show how apologies can be used to recover from misalignment in human-AI interactions, and examine trends and inconsistencies within the field. Among the observations, we outline the importance of curating a human-aligned and cross-disciplinary perspective in this research, with consideration for improved system capabilities and long-term outcomes.},
  archive      = {J_AIR},
  author       = {Harland, Hadassah and Dazeley, Richard and Senaratne, Hashini and Vamplew, Peter and Cruz, Francisco and Nakisa, Bahareh},
  doi          = {10.1007/s10462-025-11305-8},
  journal      = {Artificial Intelligence Review},
  month        = {12},
  number       = {12},
  pages        = {1-78},
  shortjournal = {Artif. Intell. Rev.},
  title        = {AI apology: A critical review of apology in AI systems},
  volume       = {58},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Effectiveness of data resampling and ensemble learning in multiclass imbalance learning. <em>AIR</em>, <em>58</em>(12), 1-60. (<a href='https://doi.org/10.1007/s10462-025-11357-w'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Classification tasks in many real-world problems often involve multiclass datasets with imbalanced class distributions, which have more difficulty factors than binary classification. Previous studies have proposed various methods to address this multiclass imbalanced learning issue. Data resampling and ensemble learning are the most popular among the proposed methods. However, no comprehensive review or survey has provided an in-depth comparison of ad hoc methods in multiclass imbalance learning, particularly with a focus on data resampling and ensemble learning. Moreover, there is a lack of studies that analyze the effectiveness of each method in terms of the difficulty factors in multiclass imbalance learning. This paper provides a comprehensive review and comparative analysis to identify the strengths and weaknesses of each method and assess their effectiveness in improving classification performance. The analysis shows that not all methods effectively enhance classification performance on multiclass imbalanced datasets. Some methods even perform worse than the baseline performance. The review also reveals that datasets with certain difficulty factors are more challenging for most existing methods to handle. Ultimately, this paper summarizes several important lessons and identifies research gaps to guide future work in the field.},
  archive      = {J_AIR},
  author       = {Fachrie, Muhammad and Musdholifah, Aina and Pulungan, Reza},
  doi          = {10.1007/s10462-025-11357-w},
  journal      = {Artificial Intelligence Review},
  month        = {12},
  number       = {12},
  pages        = {1-60},
  shortjournal = {Artif. Intell. Rev.},
  title        = {Effectiveness of data resampling and ensemble learning in multiclass imbalance learning},
  volume       = {58},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). AI in motion: A systematic review of artificial intelligence-driven virtual assistants for physical activity promotion and their comparison with traditional strategies. <em>AIR</em>, <em>58</em>(12), 1-28. (<a href='https://doi.org/10.1007/s10462-025-11361-0'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Physical inactivity remains a major public health concern globally, prompting the need for scalable, cost-effective interventions. Artificial Intelligence-driven Virtual Assistants (AIVAs) such as chatbots and virtual agents have emerged as novel methods to promote physical activity (PA), yet their effectiveness compared to traditional strategies remains unclear. This systematic review aimed at examining the characteristics, strategies, and effectiveness of AIVAs in promoting PA in adults and to compare them with traditional interventions. A systematic search of Scopus, Web of Science, PubMed, and Cochrane was conducted through May 2025. Eight interventional studies that employed AIVAs targeting PA were included. Risk of bias was assessed using ROBINS-I and RoB 2 tools. Intervention characteristics, outcomes, and behavioral strategies were extracted and synthesized. AIVAs were found to incorporate established behavior change techniques such as goal setting, feedback, and motivational support. Several studies demonstrated positive effects on PA metrics such as step counts and moderate to vigorous PA, though results were heterogeneous. Engagement and usability were generally high, particularly in interventions incorporating relational features. Compared to traditional interventions, AIVAs offered advantages in scalability and user autonomy but often lacked rigorous designs and long-term evaluation. AIVAs show promise as complementary tools for PA promotion, potentially overcoming scalability barriers associated with human-delivered programs. However, future research should prioritize methodologically robust designs, long-term assessments, and hybrid models that integrate both human and AI elements.},
  archive      = {J_AIR},
  author       = {Montelaghi, Alice and Ciorciari, Andrea and Roklicer, Roberto and Jurak, Gregor and Carraro, Attilio},
  doi          = {10.1007/s10462-025-11361-0},
  journal      = {Artificial Intelligence Review},
  month        = {12},
  number       = {12},
  pages        = {1-28},
  shortjournal = {Artif. Intell. Rev.},
  title        = {AI in motion: A systematic review of artificial intelligence-driven virtual assistants for physical activity promotion and their comparison with traditional strategies},
  volume       = {58},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). CTWA: A novel incremental deep learning-based intrusion detection method for the internet of things. <em>AIR</em>, <em>58</em>(12), 1-24. (<a href='https://doi.org/10.1007/s10462-025-11358-9'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Class incremental learning aims to learn new courses in an incremental manner without forgetting the categories previously learned. A novel incremental Internet of Things (IoT) intrusion detection method CTWA based on Convolutional Autoencoder (CAE) and Temporal Convolutional Network (TCN) is proposed to address the issues of insufficient generalization ability, high computational resources, and redundant features in class incremental learning. This method first completes the training of the CAE-TCN module, extracts and concatenates local features of data samples through CAE and TCN, and then initializes the incremental learning module. Residual modules are added to the CAE to improve the training effect of the model and avoid gradient vanishing problems. The CAE-TCN module shares lower-level feature representations through task-specific layers in incremental learning module. It distinguishes between old and new tasks using Gaussian distribution, and applies Weight alignment (WA) techniques between task heads to ensure that learning the new task does not result in forgetting the knowledge of the old tasks. Ultimately, the outputs of both new and old tasks are weighted and fused to ensure the optimal classification result. Additionally, a loss function combining cross-entropy loss and label smoothing loss is used to enhance the model’s performance. We conducted experiments on two datasets. The experimental results on CICIoT2023 dataset demonstrate that the proposed model excels in terms of accuracy, precision, recall, and F1-Score, achieving 0.9643, 0.9659, 0.9643, and 0.9645, respectively. With 40 training epochs, the model’s runtime is 789.58 s, which is higher than most comparison models, but the accuracy is significantly improved. The proposed method can effectively distinguishes between different known and unknown types of attacks, highlighting its potential applications in the field of cybersecurity.},
  archive      = {J_AIR},
  author       = {Wang, Haizhen and Yang, Yutong and Tan, Pan},
  doi          = {10.1007/s10462-025-11358-9},
  journal      = {Artificial Intelligence Review},
  month        = {12},
  number       = {12},
  pages        = {1-24},
  shortjournal = {Artif. Intell. Rev.},
  title        = {CTWA: A novel incremental deep learning-based intrusion detection method for the internet of things},
  volume       = {58},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Empowering scientific discovery with explainable small domain-specific and large language models. <em>AIR</em>, <em>58</em>(12), 1-39. (<a href='https://doi.org/10.1007/s10462-025-11365-w'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As artificial intelligence (AI) increasingly integrates into scientific research, explainability has become a cornerstone for ensuring reliability and innovation in discovery processes. This review offers a forward-looking integration of explainable AI (XAI)-based research paradigms, encompassing small domain-specific models, large language models (LLMs), and agent-based large-small model collaboration. For domain-specific models, we introduce a knowledge-oriented taxonomy categorizing methods into knowledge-agnostic, knowledge-based, knowledge-infused, and knowledge-verified approaches, emphasizing the balance between domain knowledge and innovative insights. For LLMs, we examine three strategies for integrating domain knowledge—prompt engineering, retrieval-augmented generation, and supervised fine-tuning—along with advances in explainability, including local, global, and conversation-based explanations. We also envision future agent-based model collaborations within automated laboratories, stressing the need for context-aware explanations tailored to research goals. Additionally, we discuss the unique characteristics and limitations of both explainable small domain-specific models and LLMs in the realm of scientific discovery. Finally, we highlight methodological challenges, potential pitfalls, and the necessity of rigorous validation to ensure XAI’s transformative role in accelerating scientific discovery and reshaping research paradigms.},
  archive      = {J_AIR},
  author       = {Yu, Hengjie and Wang, Yizhi and Cheng, Tao and Yan, Yan and Dawson, Kenneth A. and Li, Sam F. Y. and Zheng, Yefeng and Jin, Yaochu},
  doi          = {10.1007/s10462-025-11365-w},
  journal      = {Artificial Intelligence Review},
  month        = {12},
  number       = {12},
  pages        = {1-39},
  shortjournal = {Artif. Intell. Rev.},
  title        = {Empowering scientific discovery with explainable small domain-specific and large language models},
  volume       = {58},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Learning diversified representations for visual abstract reasoning. <em>AIR</em>, <em>58</em>(12), 1-29. (<a href='https://doi.org/10.1007/s10462-025-11372-x'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Learning effective representations suitable for decision making in high-level cognitive space is crucial for visual abstract reasoning tasks. The visual system of the mammalian brain is organized into parallel networks that can be roughly classified in dichotomy as the dorsal and ventral streams. How do parallel networks learn efficient representations for cognitive tasks is still an elusive question. We propose the Information Competition Learning Network (ICNet) within a mutual information-constrained framework to learn diversified representations for visual abstract reasoning tasks. ICNet comprises a representation learning module and a rule extractor module. The representation learning module learns two complementary sets of representation under different constraints. These two sets compete to prevent from learning what the other has learned, thereby minimizing mutual predictability. Subsequently, these sets are combined synergistically and relayed to the rule extractor module, where discrete abstract rules are formed to predict the correct option. Empirical experiments consistently show that ICNet achieves superior results across several visual abstract reasoning datasets. Additionally, in Out-of-Distribution relationship reasoning benchmarks, ICNet demonstrates robust generalization ability.},
  archive      = {J_AIR},
  author       = {Zhao, Kai and Zhu, Yao and Si, Bailu},
  doi          = {10.1007/s10462-025-11372-x},
  journal      = {Artificial Intelligence Review},
  month        = {12},
  number       = {12},
  pages        = {1-29},
  shortjournal = {Artif. Intell. Rev.},
  title        = {Learning diversified representations for visual abstract reasoning},
  volume       = {58},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Securing (vision-based) autonomous systems: Taxonomy, challenges, and defense mechanisms against adversarial threats. <em>AIR</em>, <em>58</em>(12), 1-59. (<a href='https://doi.org/10.1007/s10462-025-11373-w'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The rapid integration of computer vision into Autonomous Systems (AS) has introduced new vulnerabilities, particularly in the form of adversarial threats capable of manipulating perception and control modules. While multiple surveys have addressed adversarial robustness in deep learning, few have systematically analyzed how these threats manifest across the full stack and life-cycle of AS. This review bridges that gap by presenting a structured synthesis that spans both, foundational vision-centric literature and recent AS-specific advances, with focus on digital and physical threat vectors. We introduce a unified framework mapping adversarial threats across the AS stack and life-cycle, supported by three novel analytical matrices: the Life-cycle–Attack Matrix (linking attacks to data, training, and inference stages), the Stack–Threat Matrix (localizing vulnerabilities throughout the autonomy stack), and the Exposure–Impact Matrix (connecting attack exposure to AI design vulnerabilities and operational consequences). Drawing on these models, we define holistic requirements for effective AS defenses and critically appraise the current landscape of adversarial robustness. Finally, we propose the AS-ADS scoring framework to enable comparative assessment of defense methods in terms of their alignment with the practical needs of AS, and outline actionable directions for advancing the robustness of vision-based autonomous systems.},
  archive      = {J_AIR},
  author       = {Lopez Pellicer, Alvaro and Angelov, Plamen and Suri, Neeraj},
  doi          = {10.1007/s10462-025-11373-w},
  journal      = {Artificial Intelligence Review},
  month        = {12},
  number       = {12},
  pages        = {1-59},
  shortjournal = {Artif. Intell. Rev.},
  title        = {Securing (vision-based) autonomous systems: Taxonomy, challenges, and defense mechanisms against adversarial threats},
  volume       = {58},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A systematic review of bias detection methods for non-english word embeddings and language models. <em>AIR</em>, <em>58</em>(12), 1-56. (<a href='https://doi.org/10.1007/s10462-025-11375-8'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Biases in applications of machine learning and artificial intelligence are a major limitation of these applications. Stereotypes of the society are reflected in different types of applications, including image generation, machine translation or CV ranking. This is in particular also the case for language models and word embeddings, encoding human language as mathematical vectors. Research addressing the challenging problem of detection (and mitigation) of the bias in these embeddings is often conducted for the English language. However, the stereotypes encoded can be language dependent and impacted by a cultural environment. Thus, dedicated research efforts for languages other than English are required. In this paper, we conduct a systematic literature review to identify and compare existing bias detection methods for non-English word embeddings and language models. In an interdisciplinary team we examine the technical aspects, as well as the definitions of bias used by researchers in the field. Based on our findings, we outline a research plan for making bias detection in the field of NLP more inclusive for languages other than English.},
  archive      = {J_AIR},
  author       = {Puttick, Alexandre and Ikae, Catherine and Rigotti, Carlotta and Fosch-Villaronga, Eduard and Kharas, Mark W. and Søraa, Roger A. and Kurpicz-Briki, Mascha},
  doi          = {10.1007/s10462-025-11375-8},
  journal      = {Artificial Intelligence Review},
  month        = {12},
  number       = {12},
  pages        = {1-56},
  shortjournal = {Artif. Intell. Rev.},
  title        = {A systematic review of bias detection methods for non-english word embeddings and language models},
  volume       = {58},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Unlocking the potential of deep learning in brain stroke prognosis: A systematic literature review. <em>AIR</em>, <em>58</em>(12), 1-64. (<a href='https://doi.org/10.1007/s10462-025-11353-0'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Stroke remains a significant global health concern, necessitating accurate and timely prognosis to optimize patient care and outcomes. In recent years, deep learning, a subset of artificial intelligence, has emerged as a promising tool for enhancing stroke prognosis by leveraging its capability to analyze complex clinical and imaging data. This advancement has sparked a significant increase in research publications in this domain. Therefore, our objective in this systematic literature review (SLR) is to: systematically review and analyze the existing body of literature to identify key deep learning architectures, evaluate their performance against conventional prognosis methods, explore the range of clinical and neuroimaging data sources employed, and investigate the potential impact of deep learning on personalized stroke management. Our findings reveal that deep learning holds considerable promise in improving stroke prognosis accuracy, offering opportunities for more precise clinical decision-making. However, challenges related to data heterogeneity, interpretability, and clinical integration persist. We discuss these challenges and propose future directions to facilitate the successful integration of deep learning into routine stroke care. As the demand for precise stroke prognosis intensifies, this review serves as a valuable resource for researchers, clinicians, and policymakers alike, offering insights into the current state of deep learning applications in stroke prognosis and guiding efforts toward leveraging artificial intelligence to alleviate the burden of stroke on individuals and healthcare systems.},
  archive      = {J_AIR},
  author       = {Barouhou, Annas and Benhlima, Laila and Bah, Slimane},
  doi          = {10.1007/s10462-025-11353-0},
  journal      = {Artificial Intelligence Review},
  month        = {12},
  number       = {12},
  pages        = {1-64},
  shortjournal = {Artif. Intell. Rev.},
  title        = {Unlocking the potential of deep learning in brain stroke prognosis: A systematic literature review},
  volume       = {58},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Optimization of the composting process using artificial neural networks—a literature review. <em>AIR</em>, <em>58</em>(12), 1-27. (<a href='https://doi.org/10.1007/s10462-025-11380-x'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Composting is a complex biological process, and due to the numerous variables affecting its course, it requires constant supervision and, depending on the needs, appropriate modifications. In particular, it is necessary to strive to ensure the quality of substrates, the elimination of possible contaminants, the efficient and inexpensive conduct of the process, and the fulfillment by the finished compost of the quality requirements allowing its use as a fertilizer or crop improvement agent. Therefore, new effective methods for composting optimization are needed. This paper reviews the state of the art on the use of artificial neural networks (ANN) in bio-waste composting with a special focus on applying machine learning tools. Artificial neural networks were characterized along with their division into different types, the basics of the composting process and legal requirements for bio-waste recycling were described. Different types of machine learning were compared with attention paid to the effectiveness of the tools used. Also, for further studies, the appropriate independent variables were proposed to be used in ANN designing. The presented examples of the application of ANN confirm the usefulness of this method, to solve the complexity of the composting issue, and the need for further research.},
  archive      = {J_AIR},
  author       = {Gręziak, Bartosz and Białowiec, Andrzej},
  doi          = {10.1007/s10462-025-11380-x},
  journal      = {Artificial Intelligence Review},
  month        = {12},
  number       = {12},
  pages        = {1-27},
  shortjournal = {Artif. Intell. Rev.},
  title        = {Optimization of the composting process using artificial neural networks—a literature review},
  volume       = {58},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Efficient mortality prediction for acute respiratory failure: A resource-constrained machine learning approach using MIMIC databases. <em>AIR</em>, <em>58</em>(12), 1-38. (<a href='https://doi.org/10.1007/s10462-025-11387-4'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Accurate prediction of mortality in Acute Respiratory Failure (ARF) patients at intensive care unit (ICU) admission can improve patient outcomes and resource management. However, ICU environments often face challenges like missing test results and limited resources. This study presents a complete pipeline for predicting ARF mortality, focusing on effective feature extraction, data imputation, and class imbalance handling. Key preprocessing steps include iterative imputation for missing data and upsampling techniques like SMOTE and deep learning-based generators. Using the MIMIC-III and MIMIC-IV databases, logistic regression, random forest, extreme gradient boosting, and neural networks were tested. Findings demonstrate that neural networks, along with ensemble methods, achieved high sensitivity and $$\hbox {F}_\beta $$ scores, which are essential for accurate mortality predictions. Notably, when class distribution was balanced, the models performed equally well on specificity and sensitivity. SMOTE proved particularly effective in addressing class imbalance, suggesting that advanced upsampling methods like GANs could further enhance prediction accuracy without reducing dataset size. This graphical abstract of the work that illustrates that a patient is admitted to the hospital, admission to the ICU is determined, the test results of the first 24 hours are collected, missing parameters are imputed, the data are normalized and a machine learning model is applied to predict mortality outcomes.},
  archive      = {J_AIR},
  author       = {Khan, Muhammad Talha and Gulzar, Maryam and Ali, Arshad and Wali, Aamir and Amir, Rida},
  doi          = {10.1007/s10462-025-11387-4},
  journal      = {Artificial Intelligence Review},
  month        = {12},
  number       = {12},
  pages        = {1-38},
  shortjournal = {Artif. Intell. Rev.},
  title        = {Efficient mortality prediction for acute respiratory failure: A resource-constrained machine learning approach using MIMIC databases},
  volume       = {58},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Interpretable and explainable machine learning methods for predictive process monitoring: A systematic literature review. <em>AIR</em>, <em>58</em>(12), 1-92. (<a href='https://doi.org/10.1007/s10462-025-11399-0'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This study presents a systematic literature review on the explainability and interpretability of machine learning models within the context of predictive process monitoring. Given the rapid advancement and increasing opacity of artificial intelligence systems, understanding the "black-box" nature of these technologies has become critical, particularly for models trained on complex operational and business process data. Using the PRISMA framework, this review systematically analyzes and synthesizes the literature of the past decade, including recent and forthcoming works from 2025, to provide a timely and comprehensive overview of the field. We differentiate between intrinsically interpretable models and more complex systems that require post-hoc explanation techniques, offering a structured panorama of current methodologies and their real-world applications. Through this rigorous bibliographic analysis, our research provides a detailed synthesis of the state of explainability in predictive process mining, identifying key trends, persistent challenges and a clear agenda for future research. Ultimately, our findings aim to equip researchers and practitioners with a deeper understanding of how to develop and implement more trustworthy, transparent and effective intelligent systems for predictive process analytics.},
  archive      = {J_AIR},
  author       = {Mehdiyev, Nijat and Majlatow, Maxim and Fettke, Peter},
  doi          = {10.1007/s10462-025-11399-0},
  journal      = {Artificial Intelligence Review},
  month        = {12},
  number       = {12},
  pages        = {1-92},
  shortjournal = {Artif. Intell. Rev.},
  title        = {Interpretable and explainable machine learning methods for predictive process monitoring: A systematic literature review},
  volume       = {58},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). What fifty-one years of linguistics and artificial intelligence research tell us about their correlation: A scientometric analysis. <em>AIR</em>, <em>58</em>(12), 1-32. (<a href='https://doi.org/10.1007/s10462-025-11332-5'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {There is a strong correlation between linguistics and artificial intelligence (AI), best manifested by deep learning language models. This study provides a thorough scientometric analysis of this correlation, synthesizing the intellectual production over 51 years, from 1974 to 2024. Web of Science Core Collection (WoSCC) database was the data source. The data collected were analyzed by two powerful software, viz., CiteSpace and VOSviewer, through which mapping visualizations of the intellectual landscape, trending issues and (re)emerging hotspots were generated. The results indicate that in the 1980s and 1990s, linguistics and AI (AIL) research was not robust, characterized by unstable publication over time. It has, however, witnessed a remarkable increase of publication since then, reaching 1478 articles in 2023, and 546 articles in January-March timespan in 2024, involving emerging issues including Natural language processing, Cross-sectional study, Using bidirectional encoder representation, and Using ChatGPT and hotspots such as Novice programmer, Prioritization, and Artificial intelligence, addressing new horizons, new topics, and launching new applications and powerful deep learning language models including ChatGPT. It concludes that linguistics and AI correlation is established at several levels, research centers, journals, and countries shaping AIL knowledge production and reshaping its future frontiers.},
  archive      = {J_AIR},
  author       = {Shormani, Mohammed Q.},
  doi          = {10.1007/s10462-025-11332-5},
  journal      = {Artificial Intelligence Review},
  month        = {12},
  number       = {12},
  pages        = {1-32},
  shortjournal = {Artif. Intell. Rev.},
  title        = {What fifty-one years of linguistics and artificial intelligence research tell us about their correlation: A scientometric analysis},
  volume       = {58},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A survey of classification tasks and approaches for legal contracts. <em>AIR</em>, <em>58</em>(12), 1-55. (<a href='https://doi.org/10.1007/s10462-025-11359-8'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Given the large size and volumes of contracts and their underlying inherent complexity, manual reviews become inefficient and prone to errors, creating a clear need for automation. Automatic Legal Contract Classification (LCC) revolutionizes the way legal contracts are analyzed, offering substantial improvements in speed, accuracy, and accessibility. This survey delves into the challenges of automatic LCC and a detailed examination of key tasks, datasets, and methodologies. We identify seven classification tasks within LCC, and review fourteen datasets related to English-language contracts, including public, proprietary, and non-public sources. We also introduce a methodology taxonomy for LCC, categorized into Traditional Machine Learning, Deep Learning, and Transformer-based approaches. Additionally, the survey discusses evaluation techniques and highlights the best-performing results from the reviewed studies. By providing a thorough overview of current methods and their limitations, this survey suggests future research directions to improve the efficiency, accuracy, and scalability of LCC. As the first comprehensive survey on LCC, it aims to support legal NLP researchers and practitioners in improving legal processes, making legal information more accessible, and promoting a more informed and equitable society.},
  archive      = {J_AIR},
  author       = {Singh, Amrita and Joshi, Aditya and Jiang, Jiaojiao and Paik, Hye-young},
  doi          = {10.1007/s10462-025-11359-8},
  journal      = {Artificial Intelligence Review},
  month        = {12},
  number       = {12},
  pages        = {1-55},
  shortjournal = {Artif. Intell. Rev.},
  title        = {A survey of classification tasks and approaches for legal contracts},
  volume       = {58},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Developing user-centered system design guidelines for explainable AI: A systematic literature review. <em>AIR</em>, <em>58</em>(12), 1-50. (<a href='https://doi.org/10.1007/s10462-025-11363-y'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The rapid advancement of AI technology has led to increasingly complex and opaque systems, creating a critical need for explainable AI (XAI) that enhances transparency and user trust. Despite extensive research on XAI methods and applications, the user-centered design (UCD) process for XAI remains fragmented and unclear. This systematic review analyzes 27 studies from 2020 to 2024 to develop a comprehensive framework for user-centered XAI system design (UCXAISD). We identify five key stages: contextual inquiry, explanation needs identification, XAI method selection, user interface design, and evaluation and refinement. Our framework aligns with traditional UCD processes while incorporating specialized elements for XAI, including user-centricity, transparency, and actionability. For each stage, we provide evidence-based guidelines derived from the literature. The framework serves as a blueprint for developing XAI systems that balance technical sophistication with user needs.},
  archive      = {J_AIR},
  author       = {Hong, San and Park, Woojin},
  doi          = {10.1007/s10462-025-11363-y},
  journal      = {Artificial Intelligence Review},
  month        = {12},
  number       = {12},
  pages        = {1-50},
  shortjournal = {Artif. Intell. Rev.},
  title        = {Developing user-centered system design guidelines for explainable AI: A systematic literature review},
  volume       = {58},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A survey on deep learning fundamentals. <em>AIR</em>, <em>58</em>(12), 1-108. (<a href='https://doi.org/10.1007/s10462-025-11368-7'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep learning, driven by big data and graphic processing units, has garnered significant attention across various domains. The flexibility of network architectures, combined with their diverse components, has allowed deep learning techniques to be applied across a wide range of domains, expanding from low- and high-level computer vision tasks to encompass video processing, natural language processing (NLP), and 3D data processing. However, there has been relatively little effort to systematically summarise these works from principles to applications in terms of deep learning fundamentals. The present study aims to address this gap in the literature by presenting components of deep networks for image applications, and describing several classical deep networks for image applications. The study then introduces principles, relations, ranges, and applications of deep networks across an expanded scope, covering low-level vision tasks, high-level vision tasks, video processing, NLP, and 3D data processing. The study then compares the performance of different networks across these diverse tasks. Finally, it summarises potential focuses and challenges of deep learning research for these applications with concluding remarks.},
  archive      = {J_AIR},
  author       = {Tian, Chunwei and Cheng, Tongtong and Peng, Zhe and Zuo, Wangmeng and Tian, Yonglin and Zhang, Qingfu and Wang, Fei-Yue and Zhang, David},
  doi          = {10.1007/s10462-025-11368-7},
  journal      = {Artificial Intelligence Review},
  month        = {12},
  number       = {12},
  pages        = {1-108},
  shortjournal = {Artif. Intell. Rev.},
  title        = {A survey on deep learning fundamentals},
  volume       = {58},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). XAI-HD: An explainable artificial intelligence framework for heart disease detection. <em>AIR</em>, <em>58</em>(12), 1-78. (<a href='https://doi.org/10.1007/s10462-025-11385-6'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cardiovascular disease (CVD) is the leading global cause of death, highlighting the urgent need for early, accurate, and interpretable diagnostic tools. However, many AI-based heart disease prediction models lack transparency, hindering their acceptance in clinical settings. This study proposes XAI-HD, a hybrid framework integrating machine learning (ML), deep learning (DL), and explainable AI (XAI) techniques for heart disease detection. The framework systematically addresses key challenges, including class imbalance, missing data, and feature inconsistency, through advanced preprocessing and class-balancing methods such as OSS, NCR, SMOTEN, ADASYN, SMOTETomek, and SMOTEENN. Comparative performance evaluations across multiple datasets (CHD, FHD, SHD) demonstrate that XAI-HD reduces classification error rates by 20–25% compared to traditional ML-based models, achieving superior accuracy, precision, recall, and F1-score. Additionally, SHAP and LIME-based feature importance analysis enhances model interpretability, fostering trust among medical professionals. The proposed framework holds significant real-world applicability, including seamless integration into hospital decision support systems, electronic health records (EHR), and real-time cardiac risk assessment platforms. Unlike conventional AI-driven cardiovascular risk prediction models, XAI-HD offers a more balanced, interpretable, and computationally efficient solution, ensuring both predictive accuracy and practical feasibility in clinical environments. Statistical validation using Wilcoxon signed-rank tests confirms the performance gains, and complexity analysis shows the framework is scalable for large-scale deployment.},
  archive      = {J_AIR},
  author       = {Talukder, Md. Alamin and Talaat, Amira Samy and Kazi, Mohsin and Khraisat, Ansam},
  doi          = {10.1007/s10462-025-11385-6},
  journal      = {Artificial Intelligence Review},
  month        = {12},
  number       = {12},
  pages        = {1-78},
  shortjournal = {Artif. Intell. Rev.},
  title        = {XAI-HD: An explainable artificial intelligence framework for heart disease detection},
  volume       = {58},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). CNNs, RNNs and transformers in human action recognition: A survey and a hybrid model. <em>AIR</em>, <em>58</em>(12), 1-44. (<a href='https://doi.org/10.1007/s10462-025-11388-3'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Human action recognition (HAR) encompasses the task of monitoring human activities across various domains, including but not limited to medical, educational, entertainment, visual surveillance, video retrieval, and the identification of anomalous activities. Over the past decade, the field of HAR has witnessed substantial progress by leveraging convolutional neural networks (CNNs) and recurrent neural networks (RNNs) to effectively extract and comprehend intricate information, thereby enhancing the overall performance of HAR systems. Recently, the domain of computer vision has witnessed the emergence of Vision Transformers (ViTs) as a potent solution. The efficacy of Transformer architecture has been validated beyond the confines of image analysis, extending their applicability to diverse video-related tasks. Notably, within this landscape, the research community has shown keen interest in HAR, acknowledging its manifold utility and widespread adoption across various domains. However, HAR remains a challenging task due to variations in human motion, occlusions, viewpoint differences, background clutter, and the need for efficient spatio-temporal feature extraction. Additionally, the trade-off between computational efficiency and recognition accuracy remains a significant obstacle, particularly with the adoption of deep learning models requiring extensive training data and resources. This article aims to present an encompassing survey that focuses on CNNs and the evolution of RNNs to ViTs given their importance in the domain of HAR. By conducting a thorough examination of existing literature and exploring emerging trends, this study undertakes a critical analysis and synthesis of the accumulated knowledge in this field. Additionally, it investigates the ongoing efforts to develop hybrid approaches. Following this direction, this article presents a novel hybrid model that seeks to integrate the inherent strengths of CNNs and ViTs.},
  archive      = {J_AIR},
  author       = {Alomar, Khaled and Aysel, Halil Ibrahim and Cai, Xiaohao},
  doi          = {10.1007/s10462-025-11388-3},
  journal      = {Artificial Intelligence Review},
  month        = {12},
  number       = {12},
  pages        = {1-44},
  shortjournal = {Artif. Intell. Rev.},
  title        = {CNNs, RNNs and transformers in human action recognition: A survey and a hybrid model},
  volume       = {58},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Safeguarding large language models: A survey. <em>AIR</em>, <em>58</em>(12), 1-56. (<a href='https://doi.org/10.1007/s10462-025-11389-2'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the burgeoning field of Large Language Models (LLMs), developing a robust safety mechanism, colloquially known as “safeguards” or “guardrails”, has become imperative to ensure the ethical use of LLMs within prescribed boundaries. This article provides a systematic literature review on the current status of this critical mechanism. It discusses its major challenges and how it can be enhanced into a comprehensive mechanism dealing with ethical issues in various contexts. First, the paper elucidates the current landscape of safeguarding mechanisms that major LLM service providers and the open-source community employ. This is followed by the techniques to evaluate, analyze, and enhance some (un)desirable properties that a guardrail might want to enforce, such as hallucinations, fairness, privacy, and so on. Based on them, we review techniques to circumvent these controls (i.e., attacks), to defend the attacks, and to reinforce the guardrails. While the techniques mentioned above represent the current status and the active research trends, we also discuss several challenges that cannot be easily dealt with by the methods and present our vision on how to implement a comprehensive guardrail through the full consideration of multi-disciplinary approach, neural-symbolic method, and systems development lifecycle.},
  archive      = {J_AIR},
  author       = {Dong, Yi and Mu, Ronghui and Zhang, Yanghao and Sun, Siqi and Zhang, Tianle and Wu, Changshun and Jin, Gaojie and Qi, Yi and Hu, Jinwei and Meng, Jie and Bensalem, Saddek and Huang, Xiaowei},
  doi          = {10.1007/s10462-025-11389-2},
  journal      = {Artificial Intelligence Review},
  month        = {12},
  number       = {12},
  pages        = {1-56},
  shortjournal = {Artif. Intell. Rev.},
  title        = {Safeguarding large language models: A survey},
  volume       = {58},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Optimization of road route alignment: A systematic literature review with meta analysis. <em>AIR</em>, <em>58</em>(12), 1-46. (<a href='https://doi.org/10.1007/s10462-025-11396-3'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This systematic literature review (SLR) integrates Geographic Information Systems (GIS), deep learning, and Multi-Criteria Decision Making (MCDM) to enhance road route optimization, crucial for global infrastructure development. This SLR aims to identify existing research trends, methodologies, research gaps and propose a generalized framework for streamlining the road route optimization process. The review addresses three key research questions: RQ-1. The application of deep learning for Land Use and Land Cover (LULC) classification, RQ-2. The use of MCDM techniques in road route alignment and RQ-3. Techniques for optimizing road route alignment. Utilizing PRISMA, we assessed 370 papers, selected 132 through full-text evaluation, and added 25 via. snowball sampling, totalling 157 records for analysis. The results reveal trends in current research, geographical distribution and the evolution of methodologies. It is found that Deep learning techniques significantly improve LULC classification accuracy, while MCDM techniques enable a holistic approach to road route alignment by incorporating diverse factors. The proposed generalized framework outlines a systematic approach encompassing problem definition, criteria selection, data preparation, deep learning-based LULC classification, MCDM and Least Cost Path analysis for road route alignment. This work uniquely identifies research trends, methodologies, and gaps in road route optimization, addressing three specific research questions (RQ-1 to RQ-3) on deep learning (LULC classification), MCDM techniques, and route alignment optimization. This work also highlights the scope for integrating emerging technologies, enhancing MCDM approaches, promoting cross-disciplinary collaboration, addressing data availability and quality, conducting case studies, emphasizing sustainability, resilience and focusing on global and regional contexts. This SLR will surely contribute to the development of efficient, sustainable and equitable road route optimization strategies for better infrastructure planning and worldwide development.},
  archive      = {J_AIR},
  author       = {Agrawal, Shitij and Jamadar, Sanskar and Sawant, Suraj and Bidwe, Ranjeet Vasant and Joshi, Amit},
  doi          = {10.1007/s10462-025-11396-3},
  journal      = {Artificial Intelligence Review},
  month        = {12},
  number       = {12},
  pages        = {1-46},
  shortjournal = {Artif. Intell. Rev.},
  title        = {Optimization of road route alignment: A systematic literature review with meta analysis},
  volume       = {58},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A systematic review of multi-modal large language models on domain-specific applications. <em>AIR</em>, <em>58</em>(12), 1-47. (<a href='https://doi.org/10.1007/s10462-025-11398-1'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {While Large Language Models (LLMs) have shown remarkable proficiency in text-based tasks, they struggle to interact effectively with the more realistic world without the perceptions of other modalities such as visual and audio. Multi-modal LLMs, which integrate these additional modalities, have become increasingly important across various domains. Despite the significant advancements and potential of multi-modal LLMs, there has been no comprehensive PRISMA-based systematic review that examines their applications across different domains. The objective of this work is to fill this gap by systematically reviewing and synthesising the quantitative research literature on domain-specific applications of multi-modal LLMs. This systematic review follows the PRISMA guidelines to analyse research literature published after 2022, the release of OpenAI’s ChatGPT $$-$$ 3.5. The literature search was conducted across several online databases, including Nature, Scopus, and Google Scholar. A total of 22 studies were identified, with 11 focusing on the medical domain, 3 on autonomous driving, and 2 on geometric analysis. The remaining studies covered a range of topics, with one each on climate, music, e-commerce, sentiment analysis, human-robot interaction, and construction. This review provides a comprehensive overview of the current state of multi-modal LLMs, highlights their domain-specific applications, and identifies gaps and future research directions.},
  archive      = {J_AIR},
  author       = {Li, Sirui and Wong, Kok Wai and Wang, Guanjin and Duong, Thach-Thao},
  doi          = {10.1007/s10462-025-11398-1},
  journal      = {Artificial Intelligence Review},
  month        = {12},
  number       = {12},
  pages        = {1-47},
  shortjournal = {Artif. Intell. Rev.},
  title        = {A systematic review of multi-modal large language models on domain-specific applications},
  volume       = {58},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Research on a distributed strategy for UAV detection and tracking of specific targets. <em>AIR</em>, <em>58</em>(12), 1-22. (<a href='https://doi.org/10.1007/s10462-025-11364-x'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Target tracking is one of the important tasks of UAV (Unmanned Aerial Vehicle). The indicators for completing the UAV tracking task are the system’s tracking speed, tracking error, and tracking of specific targets. This paper proposes a distributed system based on the YOLO (You Only Look Once) detection algorithm and the DIMP (Discriminative Model Prediction) tracking algorithm for real-time tracking of specific targets. The system separates algorithm initialization and online operation, coordinates detection and tracking, and gives full play to the accuracy and speed advantages of both. Then, on the basis of the YOLO detection algorithm, the CBAM (Convolutional Block Attention Module) module is integrated to improve the network structure, which improves the detection accuracy of the YOLO algorithm for specific targets. Then, on the basis of the DIMP tracking algorithm, the tracking speed and stability of the DIMP algorithm are improved by distributed design, replacing the backbone network, optimizing training and updating, and integrating the attention mechanism. Finally, simulation and experiments verified that the distributed system can achieve long-term and stable tracking of specific targets. The detection accuracy (mAP0.5:0.95) of the YOLO algorithm integrated with CBAM was improved by 5.2%, and the tracking speed of the improved DIMP algorithm was increased by 10FPS.},
  archive      = {J_AIR},
  author       = {Liang, X. and Qi, Y. and Chen, G. and Meng, G.},
  doi          = {10.1007/s10462-025-11364-x},
  journal      = {Artificial Intelligence Review},
  month        = {12},
  number       = {12},
  pages        = {1-22},
  shortjournal = {Artif. Intell. Rev.},
  title        = {Research on a distributed strategy for UAV detection and tracking of specific targets},
  volume       = {58},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Applications, classifications, and challenges: A comprehensive evaluation of recently developed metaheuristics for search and analysis. <em>AIR</em>, <em>58</em>(12), 1-110. (<a href='https://doi.org/10.1007/s10462-025-11377-6'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Metaheuristic algorithms (MAs) are powerful tools for solving complex optimization problems across diverse domains. This comprehensive study analyzes 162 MAs through a unified multi-criteria taxonomy classifying algorithms by control parameters (parameter-free, low-parameter, high-parameter), inspiration sources (biological, physical, human-based), search space scope, and exploration–exploitation balance—alongside bibliometric assessment of publications from 2000 to 2024 (including articles, reviews, books, and conference papers). We evaluate the time complexity of 24 highly cited MAs and their real-world applications in engineering, healthcare, and energy systems. Results demonstrate that different MAs exhibit algorithmic simplicity, often requiring only a small number of control parameters, and are capable of efficient global search. Additionally, several algorithms demonstrate strong adaptability for hybridization with other techniques. However, certain methods are prone to premature convergence, primarily due to unbalanced exploration and exploitation dynamics. The proposed work also critically examines metaphor-inspired algorithms, whose contributions are critically evaluated in light of their conceptual complexity. The rise of such methods has led to redundancy and fragmentation in the field, as many reframe familiar optimization principles using superficial metaphors rather than advancing core algorithmic mechanisms. Bibliometric analysis reveals accelerated growth in MA research (64% journal articles, 31% conference papers, 3% from reviews, 1% from conference reviews, and 1% from book chapters). Similarly, in terms of classification, human-inspired methods constitute the largest category (45%), followed by evaluation-inspired (33%), swarm-inspired (14%), and game-inspired and physics-based algorithms (4%). This work consolidates two decades of advancements, identifies critical performance limitations, and provides practical guidelines for algorithm selection and hybridization to enhance optimization efficiency.},
  archive      = {J_AIR},
  author       = {Shaikh, Muhammad Suhail and Raj, Saurav and Zheng, Gengzhong and Xie, Senlin and Wang, Chang and Dong, Xiaoqing and Lin, Yifan and Wang, Chunwu and Junejo, Naveed Ur Rehman},
  doi          = {10.1007/s10462-025-11377-6},
  journal      = {Artificial Intelligence Review},
  month        = {12},
  number       = {12},
  pages        = {1-110},
  shortjournal = {Artif. Intell. Rev.},
  title        = {Applications, classifications, and challenges: A comprehensive evaluation of recently developed metaheuristics for search and analysis},
  volume       = {58},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). PPG-based glucose sensors: A review. <em>AIR</em>, <em>58</em>(12), 1-33. (<a href='https://doi.org/10.1007/s10462-025-11379-4'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Non-invasive and continuous blood glucose monitoring is crucial for effective diabetes management. Photoplethysmography (PPG) signal in wearable devices has gained recognition as a potential approach because of its simplicity, accessibility, and remote monitoring capability. This systematic analysis comprehensively assesses the feasibility, accuracy, and limitations of using PPG for measuring blood glucose levels. This review synthesizes 106 peer-reviewed studies, comprehensively analyzing the physiological principles and technological advancements of PPG-based glucose monitoring, while comparing it with conventional and emerging methods. Our analysis reveals several promising research directions in key areas. For PPG sensors, near-infrared wavelengths (850–940 nm) with reflective mode show better glucose sensitivity. AI-based methods, particularly deep learning approaches, often show improved performance in PPG signal preprocessing for motion artifact reduction compared to traditional techniques. Physical–mathematical models incorporating blood volume pulse characteristics could help identify novel PPG features correlating with glucose variations. Furthermore, hybrid approaches combining machine learning with physiological models show the most potential for accurate glucose level interpretation from PPG signals. These findings provide guidance for future research to advance PPG-based glucose monitoring toward clinical implementation.},
  archive      = {J_AIR},
  author       = {Jiang, Hui and Yao, Tianliang and Ding, Cheng},
  doi          = {10.1007/s10462-025-11379-4},
  journal      = {Artificial Intelligence Review},
  month        = {12},
  number       = {12},
  pages        = {1-33},
  shortjournal = {Artif. Intell. Rev.},
  title        = {PPG-based glucose sensors: A review},
  volume       = {58},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Quantum computing revolution in healthcare: A systematic review of applications, issues and future directions. <em>AIR</em>, <em>58</em>(12), 1-35. (<a href='https://doi.org/10.1007/s10462-025-11381-w'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Conventional treatment methods make even the most basic healthcare issues more complicated, which in turn increases the number of parties involved. Classical computing lacks the speed and accuracy needed for effective stakeholder collaboration in COVID-19 healthcare solutions, such as patients, insurance agents, healthcare practitioners, pharmaceutical suppliers, etc. The research uses organizational information processing theory (OIPT) to examine how quantum computing which is applications of artificial intelligence (AI) could transform the healthcare business, creating a more sustainable and less burdened system. The study of quantum computing (QC) has the potential to bring about “quantum leaps,” which might have unforeseen consequences for healthcare. The discovery of new medications, the personalization of medicinal treatments, and the acceleration of DNA sequencing are just a few of the many possible applications of this method. The potential of QC to transform compute-intensive healthcare tasks like drug-discovery, personalized-medicine, DNA-sequencing, medical-imaging, and operational-optimization is the primary focus of this survey paper, which offers the first comprehensive analysis of QCs diverse capabilities in improving healthcare systems. After a thorough literature study, we created taxonomies on the healthcare QC paradigm’s history and supporting technologies, applications, needs, architectures, security, outstanding questions, and future research prospects. We hope that by conducting this survey, researchers with varying levels of experience in quantum computing and healthcare will better understand the state of the art, assess opportunities and threats, and make informed decisions as they develop novel architectures and applications for this emerging field.},
  archive      = {J_AIR},
  author       = {Bukkarayasamudram, Vamshi Krishna and Reddy, Pundru Chandra Shaker and Arun Kumar, K and Jagadish, R. M. and Sharma, Swati and Prasad, Mudarakola Lakshmi and Sucharitha, Yadala and Tayubi, Iftikhar Aslam and Thakur, Gopal Kumar},
  doi          = {10.1007/s10462-025-11381-w},
  journal      = {Artificial Intelligence Review},
  month        = {12},
  number       = {12},
  pages        = {1-35},
  shortjournal = {Artif. Intell. Rev.},
  title        = {Quantum computing revolution in healthcare: A systematic review of applications, issues and future directions},
  volume       = {58},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Generative AI for biomedical video synthesis: A review. <em>AIR</em>, <em>58</em>(12), 1-50. (<a href='https://doi.org/10.1007/s10462-025-11394-5'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Generative AI models have emerged as transformative tools in the healthcare domain, offering significant advances in disease detection, diagnosis, prognosis, and treatment planning. Although substantial progress has been made in the generation of 2D medical images, the synthesis of complex medical videos remains an unexplored area. The available literature on the generation of synthetic medical videos is minimal, highlighting a significant gap in this emerging area of research. This paper reviews the literature related to biomedical video synthesis using diffusion models and generative adversarial networks. The review aims to consolidate all relevant literature and highlight the different publicly available datasets, performance matrices, and the challenges associated with the generation of medical videos, along with some potential mitigation strategies. The findings of this review reveal that key challenges, such as maintaining temporal consistency, addressing computational inefficiencies, and overcoming data scarcity, are interconnected issues. Addressing these issues collectively is essential for the development of accurate and robust generative models tailored for medical video synthesis. The proposed potential mitigation strategies for the limitations of generative models in this review serve as a foundational resource for future research, aiming to enhance the reliability and applicability of generative AI models in clinical settings. These advances have the potential to significantly impact the domains of connected healthcare and personalized medicine by enabling the generation of realistic, high-quality medical video data that can enhance the training of diagnostic algorithms, improve the robustness of AI-assisted video interpretation, simulate disease progression or regression for more precise treatment planning, and support the development of personalized medicine techniques through enriched longitudinal data analysis.},
  archive      = {J_AIR},
  author       = {Algethami, Nahlah and Iqbal, Talha and Ullah, Ihsan},
  doi          = {10.1007/s10462-025-11394-5},
  journal      = {Artificial Intelligence Review},
  month        = {12},
  number       = {12},
  pages        = {1-50},
  shortjournal = {Artif. Intell. Rev.},
  title        = {Generative AI for biomedical video synthesis: A review},
  volume       = {58},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Correction: Enhancing CCTA image quality: A review of deep learning approaches for advanced artifact correction and denoising. <em>AIR</em>, <em>58</em>(12), 1. (<a href='https://doi.org/10.1007/s10462-025-11427-z'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_AIR},
  author       = {Alkhodari, Mohanad and Alefishat, Eman and Jelinek, Herbert F. and Kaabneh, Ahmed and Liatsis, Panos},
  doi          = {10.1007/s10462-025-11427-z},
  journal      = {Artificial Intelligence Review},
  month        = {12},
  number       = {12},
  pages        = {1},
  shortjournal = {Artif. Intell. Rev.},
  title        = {Correction: Enhancing CCTA image quality: A review of deep learning approaches for advanced artifact correction and denoising},
  volume       = {58},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Oriented object detection in optical remote sensing images using deep learning: A survey. <em>AIR</em>, <em>58</em>(11), 1-61. (<a href='https://doi.org/10.1007/s10462-025-11256-0'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Oriented object detection is a fundamental yet challenging task in remote sensing (RS), aiming to locate and classify objects with arbitrary orientations. Recent advancements in deep learning have significantly enhanced the capabilities of oriented object detection methods. Given the rapid development of this field, a comprehensive survey of the recent advances in oriented object detection is presented in this paper. Specifically, we begin by tracing the technical evolution from horizontal object detection to oriented object detection and highlighting the specific related challenges, including feature misalignment, spatial misalignment, oriented bounding box (OBB) regression problems, and common issues encountered in RS. Subsequently, we further categorize the existing methods into detection frameworks, OBB regression techniques, feature representation approaches, and solutions to common issues and provide an in-depth discussion of how these methods address the above challenges. In addition, we cover several publicly available datasets and evaluation protocols. Furthermore, we provide a comprehensive comparison and analysis involving the state-of-the-art methods. Toward the end of this paper, we identify several future directions for oriented object detection research.},
  archive      = {J_AIR},
  author       = {Wang, Kun and Wang, Zi and Li, Zhang and Su, Ang and Teng, Xichao and Pan, Erting and Liu, Minhao and Yu, Qifeng},
  doi          = {10.1007/s10462-025-11256-0},
  journal      = {Artificial Intelligence Review},
  month        = {11},
  number       = {11},
  pages        = {1-61},
  shortjournal = {Artif. Intell. Rev.},
  title        = {Oriented object detection in optical remote sensing images using deep learning: A survey},
  volume       = {58},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). AI-mediated healthcare and trust. a trust-construct and trust-factor framework for empirical research. <em>AIR</em>, <em>58</em>(11), 1-17. (<a href='https://doi.org/10.1007/s10462-025-11306-7'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Application of Artificial Intelligence (AI) in healthcare is growing exponentially, and its use is expected to continue expanding in the coming years. However, lack of trust in AI systems remains a significant barrier to their widespread adoption. This article analyzes the problem of trust, its various features and its application in AI-mediated healthcare. We first review the literature on trust and trust in technology to detect which theoretical constructs are essential to trust. We then identify the factors that we consider fundamental for a rich and complex comprehension of trust in AI-mediated healthcare. We finally propose a trust-factor framework that could be used for empirical research on AI-mediated healthcare and its practical implementation.},
  archive      = {J_AIR},
  author       = {Alonso, Marcos and Astobiza, Aníbal M. and Ortega Lozano, Ramón},
  doi          = {10.1007/s10462-025-11306-7},
  journal      = {Artificial Intelligence Review},
  month        = {11},
  number       = {11},
  pages        = {1-17},
  shortjournal = {Artif. Intell. Rev.},
  title        = {AI-mediated healthcare and trust. a trust-construct and trust-factor framework for empirical research},
  volume       = {58},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Advancements in multimodal differential evolution: A comprehensive review and future perspectives. <em>AIR</em>, <em>58</em>(11), 1-73. (<a href='https://doi.org/10.1007/s10462-025-11314-7'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-modal optimization involves identifying multiple global and local optima of a function, offering valuable insights into diverse optimal solutions within the search space. Evolutionary algorithms (EAs) excel at finding various solutions in a single run, providing a distinct advantage over classical optimization techniques that often require multiple restarts without guarantee of obtaining diverse solutions. Among these EAs, differential evolution (DE) stands out as a powerful and versatile optimizer for continuous parameter spaces. DE has shown significant success in multi-modal optimization by utilizing its population-based search to promote the formation of multiple stable subpopulations, each targeting different optima. Recent advancements in DE for multi-modal optimization have focused on niching methods, parameter adaptation, hybridization with other algorithms, including machine learning, and applications across various domains. Given these developments, it is an opportune moment to present a critical review of the latest literature and identify key future research directions. This paper offers a comprehensive overview of recent DE advancements in multimodal optimization, including methods for handling multiple optima, hybridization with EAs, and machine learning, and highlights a range of real-world applications. Additionally, the paper outlines a set of compelling open problems and future research issues from multiple perspectives.},
  archive      = {J_AIR},
  author       = {Chauhan, Dikshit and Shivani and Jung, Donghwi and Yadav, Anupam},
  doi          = {10.1007/s10462-025-11314-7},
  journal      = {Artificial Intelligence Review},
  month        = {11},
  number       = {11},
  pages        = {1-73},
  shortjournal = {Artif. Intell. Rev.},
  title        = {Advancements in multimodal differential evolution: A comprehensive review and future perspectives},
  volume       = {58},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Ai-enabled framework for anomaly detection in power distribution networks under false data injection attacks. <em>AIR</em>, <em>58</em>(11), 1-38. (<a href='https://doi.org/10.1007/s10462-025-11318-3'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Modern power distribution networks are becoming cyber physical systems due to the addition of more advanced metering infrastructure (AMI). This has introduced new vulnerabilities to cyber threats, particularly false data injection (FDI) attacks. These attacks compromise the integrity of power consumption data, leading to financial losses, operational inefficiencies, and grid instability. Rule-based techniques and traditional machine learning models are two examples of traditional anomaly detection methods that often have problems. Often, these methods generate an excessive number of false alarms, struggle to adapt to new attack patterns, and perform poorly in large-scale deployments. This research suggests a robust anomaly identification framework (AIF) that uses an autoencoder (AE) for feature transformation and a multi-layer perceptron (MLP) to identify anomalies in AMI integrated with smart grids. The proposed approach first applies synthetic features extraction inspired by real-world smart meter capabilities and transforms the dataset using a denoising AE. MLP assisted in the classification to detect multiple FDI attack types with improved accuracy and reliability. Numerous experiments have been performed, and the results indicate that the suggested method works better than popular methods like correlation analysis, techniques based on clustering, and standard outlier identification algorithms. Compared to baseline methods, the proposed technique improves detection accuracy by up to approximately 25%, reduces false positives, and enhances the system’s ability to generalize across different cyberattack strategies. The proposed work computes seven different types of criterion matrices to verify the effectiveness of finding anomalies. The overall average results include mean squared error (0.0793), accuracy (92%), F1-Score (92%), recall (91%), specificity (94%), area under the curve (97%), and mean average precision (96%). These findings accentuate the potential of the proposed AIF performance in fortifying smart grid cybersecurity.},
  archive      = {J_AIR},
  author       = {Ahmad, Hasnain and Mustafa, Ghulam and Gulzar, Muhammad Majid and Ahmed, Ijaz and Khalid, Muhammad},
  doi          = {10.1007/s10462-025-11318-3},
  journal      = {Artificial Intelligence Review},
  month        = {11},
  number       = {11},
  pages        = {1-38},
  shortjournal = {Artif. Intell. Rev.},
  title        = {Ai-enabled framework for anomaly detection in power distribution networks under false data injection attacks},
  volume       = {58},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Artificial intelligence for secure and sustainable industrial control systems - A survey of challenges and solutions. <em>AIR</em>, <em>58</em>(11), 1-86. (<a href='https://doi.org/10.1007/s10462-025-11320-9'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In modern industrial environments, the security and sustainability of Industrial Control Systems (ICS) have become crucial. This comprehensive review examines the transformative potential of Artificial Intelligence (AI) in ICS, focusing on technologies like Machine Learning (ML), Deep Learning (DL), Large Language Models (LLMs), and cloud computing. Moreover, this research explores integrating existing and proposed sustainable practices within the ICS framework, with a particular emphasis on energy efficiency and carbon footprint reduction, to enhance the overall sustainability of ICS. This review employed a systematic approach to select relevant articles from multiple reputable databases, such as Scopus, IEEE Explore, Science Direct, ACM digital library, Web of Science, and IET digital library, including 250 articles that provide valuable insights into the intersection of AI, security, and sustainability in ICS. This review examines vulnerabilities in ICS, such as data breaches, insider threats, and malware, emphasizing the need for effective anomaly detection. It highlights how AI technologies like anomaly detection and predictive analytics can enhance threat detection and response in ICS by improving accuracy and efficiency. The review offers insights to researchers and professionals on the future of secure, sustainable ICS, supporting a resilient industrial landscape that meets cybersecurity, compliance, and sustainability goals.},
  archive      = {J_AIR},
  author       = {Aslam, Muhammad Muzamil and Tufail, Ali and Gul, Haji and Irshad, Muhammad Nauman and Namoun, Abdallah},
  doi          = {10.1007/s10462-025-11320-9},
  journal      = {Artificial Intelligence Review},
  month        = {11},
  number       = {11},
  pages        = {1-86},
  shortjournal = {Artif. Intell. Rev.},
  title        = {Artificial intelligence for secure and sustainable industrial control systems - A survey of challenges and solutions},
  volume       = {58},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Don’t push the button! exploring data leakage risks in machine learning and transfer learning. <em>AIR</em>, <em>58</em>(11), 1-58. (<a href='https://doi.org/10.1007/s10462-025-11326-3'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Machine Learning (ML) has revolutionized various domains, offering predictive capabilities in several areas. However, there is growing evidence in the literature that ML approaches are not always used appropriately, leading to incorrect and sometimes overly optimistic results. One reason for this inappropriate use of ML may be the increasing availability of machine learning tools, leading to what we call the “push the button” approach. While this approach provides convenience, it raises concerns about the reliability of outcomes, leading to challenges such as incorrect performance evaluation. In particular, this paper addresses a critical issue in ML, known as data leakage, where unintended information contaminates the training data, impacting model performance evaluation. Indeed, crucial steps in ML pipeline can be inadvertently overlooked, leading to optimistic performance estimates that may not hold in real-world scenarios. The discrepancy between evaluated and actual performance on new data is a significant concern. In particular, this paper categorizes data leakage in ML, discussing how certain conditions can propagate through the ML approach workflow. Furthermore, it explores the connection between data leakage and the specific task being addressed, investigates its occurrence in Transfer Learning framework, and compares standard inductive ML with transductive ML paradigms. The conclusion summarizes key findings, emphasizing the importance of addressing data leakage for robust and reliable ML applications considering tasks and generalization goals.},
  archive      = {J_AIR},
  author       = {Apicella, Andrea and Isgrò, Francesco and Prevete, Roberto},
  doi          = {10.1007/s10462-025-11326-3},
  journal      = {Artificial Intelligence Review},
  month        = {11},
  number       = {11},
  pages        = {1-58},
  shortjournal = {Artif. Intell. Rev.},
  title        = {Don’t push the button! exploring data leakage risks in machine learning and transfer learning},
  volume       = {58},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Fault-tolerant control strategies for industrial robots: State of the art and future perspective on AI-based fault management. <em>AIR</em>, <em>58</em>(11), 1-33. (<a href='https://doi.org/10.1007/s10462-025-11327-2'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Fault-tolerant control schemes are essential for affirming the safe and dependable operation of industrial robots. In this detailed review, we discuss the current developments in fault-tolerant control strategies for industrial robots. The main focus is given to highlight some major contributions in fault-tolerant control systems used in robotic manipulators with single or multiple joints, incorporating any linear or non-linear robust approach for industrial robots, design, and implementation. The paper also discusses adaptive fault-tolerant control of robots with sensor and/or actuator faults and unknown parameters, and fault-tolerant cooperative control of multiple robot teams for collaborative tasks. The present work provides a comprehensive overview of the recent advancements in fault-tolerant control strategies for industrial robots using both classical nonlinear methods as well as intelligent approaches using AI and machine learning, which will be useful for researchers and engineers working in this field.},
  archive      = {J_AIR},
  author       = {Khan, Zeashan and Nasir, Ali and Mekid, Samir},
  doi          = {10.1007/s10462-025-11327-2},
  journal      = {Artificial Intelligence Review},
  month        = {11},
  number       = {11},
  pages        = {1-33},
  shortjournal = {Artif. Intell. Rev.},
  title        = {Fault-tolerant control strategies for industrial robots: State of the art and future perspective on AI-based fault management},
  volume       = {58},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Inclusive prompt engineering for large language models: A modular framework for ethical, structured, and adaptive AI. <em>AIR</em>, <em>58</em>(11), 1-51. (<a href='https://doi.org/10.1007/s10462-025-11330-7'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Large language models have achieved impressive results across various tasks but remain limited in their ability to adapt ethically and structurally across diverse domains without retraining. This paper presents the Inclusive Prompt Engineering Model (IPEM), a modular framework designed to enhance LLM performance, adaptability, and ethical alignment through prompt-level strategies alone. IPEM integrates four components: Memory-of-Thought for multi-turn consistency, Enhanced Chain-of-Thought prompting for logical verification, Structured and Analogical Reasoning modules for tabular and cross-domain tasks, and Evaluation and Feedback Loops that incorporate uncertainty-aware selection and bias mitigation mechanisms. Evaluated across tasks in arithmetic reasoning, healthcare triage, financial forecasting, and inclusive question answering, IPEM consistently improves model outputs over a GPT-4 baseline. Notable outcomes include up to twenty percentage points in accuracy gains, a 25 percent reduction in logical errors, and nearly 20 percent reduction in social bias scores, all without modifying model weights. Moreover, IPEM reduces annotation demands by one-third while preserving performance, demonstrating its utility in low-resource environments. By unifying ethical safeguards and reasoning mechanisms in a prompt-based system, IPEM offers a reproducible and auditable pathway for deploying adaptable and fair AI systems. The framework contributes both practical solutions and theoretical insights to the evolving field of prompt engineering.},
  archive      = {J_AIR},
  author       = {Torkestani, Mohamad Saleh and Alameer, Ali and Palaiahnakote, Shivakumara and Manosuri, Taha},
  doi          = {10.1007/s10462-025-11330-7},
  journal      = {Artificial Intelligence Review},
  month        = {11},
  number       = {11},
  pages        = {1-51},
  shortjournal = {Artif. Intell. Rev.},
  title        = {Inclusive prompt engineering for large language models: A modular framework for ethical, structured, and adaptive AI},
  volume       = {58},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Video diffusion generation: Comprehensive review and open problems. <em>AIR</em>, <em>58</em>(11), 1-55. (<a href='https://doi.org/10.1007/s10462-025-11331-6'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Video generation has become an increasingly important component of AI-generated content (AIGC), owing to its rich semantic expressiveness and growing application potential. Among various generative paradigms, diffusion models have recently gained prominence due to their strong controllability, competitive visual quality, and compatibility with multimodal inputs. However, most existing surveys provide limited coverage of diffusion-based video generation, often lacking systematic analysis and comprehensive comparisons. To address this gap, this paper presents a thorough and structured review of diffusion models for video generation. We first outline the theoretical foundations and core architectures of diffusion models, and then the key design principles of representative methods for video generation were introduced. We propose a unified taxonomy that categorizes over two hundred methods, analyzing their key characteristics, strengths, and limitations. In addition, we compared the performance of classical methods and summarized commonly used datasets and evaluation metrics in this field for ease of model benchmarking and selection. Finally, we discuss open problems and future research directions, aiming to provide a valuable reference for both academic research and practical development.},
  archive      = {J_AIR},
  author       = {Ma, Wenping and Yang, Xiaoting and Jiao, Licheng and Li, Lingling and Liu, Xu and Liu, Fang and Chen, Puhua and Yang, Yuting and Ma, Mengru and Sun, Long and Zhang, Ruohan and Geng, Xueli and Guo, Yuwei and Yang, Shuyuan and Feng, Zhixi},
  doi          = {10.1007/s10462-025-11331-6},
  journal      = {Artificial Intelligence Review},
  month        = {11},
  number       = {11},
  pages        = {1-55},
  shortjournal = {Artif. Intell. Rev.},
  title        = {Video diffusion generation: Comprehensive review and open problems},
  volume       = {58},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Privacy and security in recommenders: An analytical review. <em>AIR</em>, <em>58</em>(11), 1-41. (<a href='https://doi.org/10.1007/s10462-025-11333-4'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recommender systems (RSs) effectively curb information overload by providing personalized suggestions of items to users across different online domains. Their widespread use in e-commerce enhances user engagement, personalizes shopping experiences, and drives sales growth. However, despite the effectiveness of these systems at generating recommendations for users, they still raise major privacy and security concerns as their data could be exploited for malicious purposes, which can lead to data breaches and misuse. Therefore, this paper presents a comprehensive and systematic review of the underlying causes of privacy and security challenges in RS. It also provides a detailed taxonomy categorizing these concerns based on their targets and the risks they create. It further presents potential solutions that have been used in the literature while identifying challenges and possible research directions to pursue in a bid to address privacy and security concerns in RSs. This paper will be a useful resource for current and upcoming researchers in the domain of RSs. It will support knowledge advancement and steer appropriate research directions.},
  archive      = {J_AIR},
  author       = {Ojokoh, Bolanle Adefowoke and Isinkaye, Folasade Olubusola and Zhang, Ming and Tom, Joshua Joshua and Gabriel, Arome Junior and Afolabi, Olaitan and Afolabi, Bamidele},
  doi          = {10.1007/s10462-025-11333-4},
  journal      = {Artificial Intelligence Review},
  month        = {11},
  number       = {11},
  pages        = {1-41},
  shortjournal = {Artif. Intell. Rev.},
  title        = {Privacy and security in recommenders: An analytical review},
  volume       = {58},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Applications of deep learning in tea quality monitoring: A review. <em>AIR</em>, <em>58</em>(11), 1-45. (<a href='https://doi.org/10.1007/s10462-025-11335-2'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Tea is a popular beverage which can offer numerous benefits to human health and support the local economy. There is an increasing demand for accurate and rapid tea quality evaluation methods to ensure that the quality and safety of tea products meet the customers’ expectations. Advanced sensing technologies in combination with deep learning (DL) offer significant opportunities to enhance the efficiency and accuracy for tea quality evaluation. This review aims to summarize the application of DL technologies for tea quality assessment in three stages: cultivation, tea processing, and product evaluation. Various state-of-the-art sensing technologies (e.g., computer vision, spectroscopy, electronic nose and tongue) have been used to collect key data (images, spectral signals, aroma profiles) from tea samples. By utilizing DL models, researchers are able to analyze a wide range of tea quality attributes, including tea variety, geographical origin, quality grade, fermentation stage, adulteration level, and chemical composition. The findings from this review indicate that DL, with its end-to-end analytical capability and strong generalization performance, can serve as a powerful tool to support various sensing technologies for accurate tea quality detection. However, several challenges remain, such as limited sample availability for data training, difficulties for fusing data from multiple sources, and lack of interpretability of DL models. To this end, this review proposes potential solutions and future studies to address these issues, providing practical considerations for tea industry to effectively uptake new technologies and to support the development of the tea industry.},
  archive      = {J_AIR},
  author       = {Wu, Tao and Zhou, Lei and Zhao, Yiying and Qi, Hengnian and Pu, Yuanyuan and Zhang, Chu and Liu, Yufei},
  doi          = {10.1007/s10462-025-11335-2},
  journal      = {Artificial Intelligence Review},
  month        = {11},
  number       = {11},
  pages        = {1-45},
  shortjournal = {Artif. Intell. Rev.},
  title        = {Applications of deep learning in tea quality monitoring: A review},
  volume       = {58},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Artificial intelligence (AI) and machine learning (ML) in procurement and purchasing decision-support (DS): A taxonomic literature review and research opportunities. <em>AIR</em>, <em>58</em>(11), 1-36. (<a href='https://doi.org/10.1007/s10462-025-11336-1'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Artificial intelligence (AI), machine learning (ML) and decision-support (DS) are gaining increasing interest with widening adoption. This article investigates the enabler role of AI and ML for providing decision-support in procurement&purchasing domain. The study follows a systematic review approach via taxonomic analysis. Comprehensive analysis and discussions are provided for: (a) the relevance and applicability of AI and ML in procurement&purchasing decision-support; (b) functionalities/processes for which they are utilized; (c) related methodologies; and (d) implementation benefits as well as challenges. Findings reveal that procurement&purchasing area holds significant potential in terms of AI-ML applications for decision-support almost every related sub-process. This study is original by offering a process-oriented approach to the research domain; providing unique clustering and classification; and presenting detailed analyses via unique taxonomy tables with respect to approach, topic, focus, context and methodologies of the literature items reviewed. The study offers further research opportunities and has significant potential to provide managerial insights by the identified sectoral applications, benefits and challenges.},
  archive      = {J_AIR},
  author       = {Balkan, Dursun and Akyuz, Goknur Arzu},
  doi          = {10.1007/s10462-025-11336-1},
  journal      = {Artificial Intelligence Review},
  month        = {11},
  number       = {11},
  pages        = {1-36},
  shortjournal = {Artif. Intell. Rev.},
  title        = {Artificial intelligence (AI) and machine learning (ML) in procurement and purchasing decision-support (DS): A taxonomic literature review and research opportunities},
  volume       = {58},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Automatic radiology report generation with deep learning: A comprehensive review of methods and advances. <em>AIR</em>, <em>58</em>(11), 1-42. (<a href='https://doi.org/10.1007/s10462-025-11337-0'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Automatic report generation refers to the process of generating medical reports from medical images without the need for manual intervention, enabling faster, more consistent, and objective analysis of radiological data. The rapid progress in deep learning, particularly in the fields of computer vision and natural language processing, has significantly improved the efficacy of this approach. By leveraging deep learning techniques, which seamlessly integrate image analysis with natural language generation, these methods have shown promise in interpreting complex medical images and producing highly accurate textual descriptions. In this paper, we provide a thorough review of various deep learning models and techniques employed for generating radiological reports, with a focus on chest X-ray images as a representative case. We propose a unified encoder-decoder framework that consists of an image encoder for extracting feature representations from medical images, a language decoder for generating textual reports, and enhancement components designed to refine model performance. Through a comprehensive comparison of existing state-of-the-art methods on the widely utilized MIMIC-CXR dataset, we highlight the innovative contributions made by recent advancements in the field. Furthermore, we discuss the current challenges and identify potential research directions for future advancements in this field.},
  archive      = {J_AIR},
  author       = {Li, Yilin and Kong, Chao and Zhao, Guosheng and Zhao, Zijian},
  doi          = {10.1007/s10462-025-11337-0},
  journal      = {Artificial Intelligence Review},
  month        = {11},
  number       = {11},
  pages        = {1-42},
  shortjournal = {Artif. Intell. Rev.},
  title        = {Automatic radiology report generation with deep learning: A comprehensive review of methods and advances},
  volume       = {58},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Generative AI for cyber threat intelligence: Applications, challenges, and analysis of real-world case studies. <em>AIR</em>, <em>58</em>(11), 1-134. (<a href='https://doi.org/10.1007/s10462-025-11338-z'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper presents a comprehensive survey of the applications, challenges, and limitations of Generative AI (GenAI) in enhancing threat intelligence within cybersecurity, supported by real-world case studies. We examine a wide range of data sources in Cyber Threat Intelligence (CTI), including security reports, blogs, social media, network traffic, malware samples, dark web data, and threat intelligence platforms (TIPs). This survey provides a full reference for integrating GenAI into CTI. We discuss various GenAI models such as Large Language Models (LLMs) and Deep Generative Models (DGMs) like Variational Autoencoders (VAEs), Generative Adversarial Networks (GANs), and Diffusion Models, explaining their roles in detecting and addressing complex cyber threats. The survey highlights key applications in areas such as malware detection, network traffic analysis, phishing detection, threat actor attribution, and social engineering defense. We also explore critical challenges in deploying GenAI, including data privacy, security concerns, and the need for interpretable and transparent models. As regulations like the European Commission’s AI Act emerge, ensuring trustworthy AI solutions is becoming more crucial. Real-world case studies, such as the impact of the WannaCry ransomware, the rise of deepfakes, and AI-driven social engineering, demonstrate both the potential and current limitations of GenAI in CTI. Our goal is to provide foundational insights and strategic direction for advancing GenAI’s role in future cybersecurity frameworks, emphasizing the importance of innovation, adaptability, and ongoing learning to enhance resilience against evolving cyber threats. Ultimately, this survey offers critical insights into how GenAI can shape the future of cybersecurity by addressing key challenges and providing actionable guidance for effective implementation.},
  archive      = {J_AIR},
  author       = {Balasubramanian, Prasasthy and Liyana, Sonali and Sankaran, Hamsini and Sivaramakrishnan, Shambavi and Pusuluri, Sruthi and Pirttikangas, Susanna and Peltonen, Ella},
  doi          = {10.1007/s10462-025-11338-z},
  journal      = {Artificial Intelligence Review},
  month        = {11},
  number       = {11},
  pages        = {1-134},
  shortjournal = {Artif. Intell. Rev.},
  title        = {Generative AI for cyber threat intelligence: Applications, challenges, and analysis of real-world case studies},
  volume       = {58},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multi-agent reinforcement learning for resources allocation optimization: A survey. <em>AIR</em>, <em>58</em>(11), 1-49. (<a href='https://doi.org/10.1007/s10462-025-11340-5'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-Agent Reinforcement Learning (MARL) has become a powerful framework for numerous real-world applications, modeling distributed decision-making and learning from interactions with complex environments. Resource Allocation Optimization (RAO) benefits significantly from MARL’s ability to tackle dynamic and decentralized contexts. MARL-based approaches are increasingly applied to RAO challenges across sectors playing a pivotal role in industry 4.0 developments. This survey provides a comprehensive review of recent MARL algorithms for RAO, encompassing core concepts, classifications, design steps and benchmarks. By outlining the current research landscape and identifying primary challenges and future directions, this survey aims to support researchers and practitioners in leveraging MARL’s potential to advance resource allocation solutions.},
  archive      = {J_AIR},
  author       = {Hady, Mohamad A. and Hu, Siyi and Pratama, Mahardhika and Cao, Zehong and Kowalczyk, Ryszard},
  doi          = {10.1007/s10462-025-11340-5},
  journal      = {Artificial Intelligence Review},
  month        = {11},
  number       = {11},
  pages        = {1-49},
  shortjournal = {Artif. Intell. Rev.},
  title        = {Multi-agent reinforcement learning for resources allocation optimization: A survey},
  volume       = {58},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). (Ir)rationality in AI: State of the art, research challenges and open questions. <em>AIR</em>, <em>58</em>(11), 1-39. (<a href='https://doi.org/10.1007/s10462-025-11341-4'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The concept of rationality is central to the field of artificial intelligence (AI). Whether we are seeking to simulate human reasoning, or trying to achieve bounded optimality, our goal is generally to make artificial agents as rational as possible. Despite the centrality of the concept within AI, there is no unified definition of what constitutes a rational agent. This article provides a survey of rationality and irrationality in AI, and sets out the open questions in this area. We consider how the understanding of rationality in other fields has influenced its conception within AI, in particular work in economics, philosophy and psychology. Focusing on the behaviour of artificial agents, we examine irrational behaviours that can prove to be optimal in certain scenarios. Some methods have been developed to deal with irrational agents, both in terms of identification and interaction, however work in this area remains limited. Methods that have up to now been developed for other purposes, namely adversarial scenarios, may be adapted to suit interactions with artificial agents. We further discuss the interplay between human and artificial agents, and the role that rationality plays within this interaction; many questions remain in this area, relating to potentially irrational behaviour of both humans and artificial agents.},
  archive      = {J_AIR},
  author       = {Macmillan-Scott, Olivia and Musolesi, Mirco},
  doi          = {10.1007/s10462-025-11341-4},
  journal      = {Artificial Intelligence Review},
  month        = {11},
  number       = {11},
  pages        = {1-39},
  shortjournal = {Artif. Intell. Rev.},
  title        = {(Ir)rationality in AI: State of the art, research challenges and open questions},
  volume       = {58},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A comprehensive review on key technologies toward smart healthcare systems based IoT: Technical aspects, challenges and future directions. <em>AIR</em>, <em>58</em>(11), 1-122. (<a href='https://doi.org/10.1007/s10462-025-11342-3'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The unexpected death of humans due to a lack of medical care is a serious problem. Additionally, the number of elderly people requiring continuous care is increasing. A global aging population poses a challenge to the sustainability of conventional healthcare systems for the future. Simultaneously, recent years have seen remarkable progress in the Internet of Things (IoT) and communication technologies, alongside the growing importance of artificial intelligence (AI) explainability and information fusion. Therefore, developing smart healthcare systems based on IoT and advanced technologies is crucial. This would open up new possibilities for efficient and intelligent medical systems. Hence, it is imperative to present a prospective vision of smart healthcare systems and explore the key technologies that enable the development of these intelligent medical systems. With smart healthcare systems, the future of healthcare can be significantly enhanced, providing higher-quality care, improved treatment, and more efficient patient care. This paper aims to provide a comprehensive review of the key enabling and innovative technologies for smart healthcare systems. To this end, it will cover the primary goals of each technology, the current state of research, potential applications envisioned, associated challenges, and future research directions. This paper is intended to be a valuable resource for researchers and healthcare providers. Ultimately, this paper provides valuable insights for both industry professionals and academic researchers, while also identifying potential new research avenues.},
  archive      = {J_AIR},
  author       = {Alsabah, Muntadher and Naser, Marwah Abdulrazzaq and Albahri, A. S. and Albahri, O. S. and Alamoodi, A. H. and Abdulhussain, Sadiq H. and Alzubaidi, Laith},
  doi          = {10.1007/s10462-025-11342-3},
  journal      = {Artificial Intelligence Review},
  month        = {11},
  number       = {11},
  pages        = {1-122},
  shortjournal = {Artif. Intell. Rev.},
  title        = {A comprehensive review on key technologies toward smart healthcare systems based IoT: Technical aspects, challenges and future directions},
  volume       = {58},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A novel ensemble learning-based soft measurement method for rod-pumping system efficiency. <em>AIR</em>, <em>58</em>(11), 1-39. (<a href='https://doi.org/10.1007/s10462-025-11343-2'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Accurate prediction of rod-pumping system efficiency is crucial for evaluating the performance of such systems. Currently, the efficiency of rod-pumping systems is primarily estimated using mechanistic models. With the continuous advancement of information technology and the improvement of oilfield databases, some researchers have employed single neural networks for prediction. However, single neural networks often suffer from low prediction accuracy and poor robustness to noise. To solve this problem, we propose a new integrated learning-based soft measurement of the efficiency of rod pumping systems. Firstly, we proposed five soft measurement methods for rod pumping system efficiency: BiGRU-BiLSTM-CrossAttention, BiRNN-BiGRU-KAN, CNN-BiGRU-KAN, BiLSTM-BiGRU-KAN, and BiLSTM-Transformer-KAN. Then, using these five methods as base learners and FNN as the meta-learner, we constructed a novel rod pumping system efficiency soft measurement method based on the Stacking ensemble learning framework. The hyperparameters were optimized using a multi-strategy integrated Crayfish optimization algorithm, and the model was validated using 5-fold cross-validation. To verify the accuracy of the proposed soft measurement method, we applied it to 10,250 real oil wells for calculation and conducted a comparative analysis with baseline models. The results demonstrate that the proposed soft measurement method can effectively predict the efficiency of rod pumping systems.},
  archive      = {J_AIR},
  author       = {Ma, Biao and Dong, Shimin},
  doi          = {10.1007/s10462-025-11343-2},
  journal      = {Artificial Intelligence Review},
  month        = {11},
  number       = {11},
  pages        = {1-39},
  shortjournal = {Artif. Intell. Rev.},
  title        = {A novel ensemble learning-based soft measurement method for rod-pumping system efficiency},
  volume       = {58},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). SFI-ensemble: Sugeno fuzzy integral-based ensemble of CNN models with meta-heuristic fuzzy measures for mouth and oral disease detection. <em>AIR</em>, <em>58</em>(11), 1-40. (<a href='https://doi.org/10.1007/s10462-025-11345-0'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The rising prevalence of mouth and oral diseases (MOD), including gum disease and oral cancer, presents a major global health challenge, where timely detection is crucial for effective intervention. Recognizing the limitations of a single learning model in capturing intricate information for precise disease prediction from complex data, we introduce a robust deep learning ensemble framework named Sugeno Fuzzy Integral (SFI)-Ensemble in this paper. Our methodology involves a meticulous preprocessing of the dataset utilizing fuzzy contrast enhancement (FCE) to enhance data quality and contrast. Additionally, we propose a reconstruction approach that employs transfer learning (TL) and fine-tuning on four Convolutional Neural Network (CNN) models—DenseNet121, MobileNetV1, DenseNet169, and Resnet101V2—to optimize their architectures specifically for MOD classification. The focal point of this contribution lies in the introduction of a groundbreaking ensemble method. This ensemble method dynamically combines decision scores from the CNN models using the SFI-based technique, offering a resilient and adaptive approach that factors in the confidence of base learners’ predictions through fuzzy integrals. To overcome the prevalent challenge of experimentally defining fuzzy measures in ensemble methods based on fuzzy integrals, we surpass conventional manual tuning. Our approach involves the utilization of seven distinct meta-heuristic optimization algorithms for the optimal determination of fuzzy measures. This not only ensures stability but also highlights the effectiveness of the proposed SFI-Ensemble. A comprehensive assessment is carried out on publicly accessible datasets to detect MOD, complemented by Grad-CAM interpretability and meticulous statistical analyses. Additionally, we benchmark the results against baseline models and state-of-the-art methods, with our proposed framework consistently surpassing them, attaining an impressive accuracy of 99.70%. This underscores the superior performance and robustness of our proposed methodology in contrast to traditional ensemble methods. Our approach, integrating dataset preprocessing, model reconstruction, and ensemble innovation, provides doctors with an effective tool for accurate MOD diagnosis, enhancing adaptability and performance through fuzzy integral-based fusion.},
  archive      = {J_AIR},
  author       = {Asif, Sohaib and Chen, Shasha and Ying, Yajun and Zheng, Changfu and Wang, Vicky Yang and Wang, Enyu and Xu, Dong},
  doi          = {10.1007/s10462-025-11345-0},
  journal      = {Artificial Intelligence Review},
  month        = {11},
  number       = {11},
  pages        = {1-40},
  shortjournal = {Artif. Intell. Rev.},
  title        = {SFI-ensemble: Sugeno fuzzy integral-based ensemble of CNN models with meta-heuristic fuzzy measures for mouth and oral disease detection},
  volume       = {58},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Deep learning for intrusion detection in emerging technologies: A comprehensive survey and new perspectives. <em>AIR</em>, <em>58</em>(11), 1-63. (<a href='https://doi.org/10.1007/s10462-025-11346-z'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Intrusion Detection Systems (IDS) can help cybersecurity analysts detect malicious activities in computational environments. Recently, Deep Learning (DL) methods in IDS have demonstrated notable performance, revealing new underlying cybersecurity patterns in systems’ operations. Conversely, issues such as low performance in real systems, high false positive rates, and lack of explainability hinder its real-world deployment. In addition, the adoption of many new emerging technologies, such as cloud, edge computing, and the Internet of Things (IoT) introduces new forms of vulnerabilities. Therefore, the improvement of intrusion detection in emerging technologies depends on the clear definitions of challenging security problems and the limitations of existing solutions. The main goal of this research is to conduct a literature review of DL solutions for intrusion detection in emerging technologies to understand the state-of-the-art solutions and their limitations. Specifically, we conduct a comprehensive review of IDS-based automated threat defense methods, with the objective of identifying the landscape of, and opportunities for, incorporating DL methods into IDS. To accomplish this, a thorough review of IDS methods is conducted for multiple platforms and technologies, focusing on the use of common DL techniques. To expand on the study, several widely used IDS datasets are evaluated to assess their ability to train DL models and support researchers in understanding their characteristics and limitations. The analysis of attack vectors in emerging technologies is conducted, enabling an in-depth evaluation of security solutions in the future. Our findings show many clear opportunities for future research, including addressing the gap between solutions for controlled/simulated environments versus real systems, overcoming trustworthiness issues, including lack of explainability, and further exploring operationalization issues such as deployable solutions and continuous detection. Our analysis highlights that the operationalization of DL for intrusion detection in emerging technologies represents a key challenge to be addressed in the next few years.},
  archive      = {J_AIR},
  author       = {Neto, Euclides Carlos Pinto and Iqbal, Shahrear and Buffett, Scott and Sultana, Madeena and Taylor, Adrian},
  doi          = {10.1007/s10462-025-11346-z},
  journal      = {Artificial Intelligence Review},
  month        = {11},
  number       = {11},
  pages        = {1-63},
  shortjournal = {Artif. Intell. Rev.},
  title        = {Deep learning for intrusion detection in emerging technologies: A comprehensive survey and new perspectives},
  volume       = {58},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Applications of machine learning for computer-aided diagnosis of parkinson’s disease: Progress and benchmark case study. <em>AIR</em>, <em>58</em>(11), 1-58. (<a href='https://doi.org/10.1007/s10462-025-11347-y'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Machine learning (ML) has emerged as a vital tool for the diagnosis of Parkinson’s Disease (PD). This study presents a comprehensive review on the applications of ML for computer-aided diagnosis (CAD) of PD. We conducted a comprehensive review by searching articles published from 2010 till 2024. The risk of bias is assessed using the PROBAST checklist. Case studies are also provided. This review includes 117 articles with six categories: neuroimaging data (20.5%); voice data (40.2%); handwriting data (12.0%); gait data (14.5%); EEG data (8.5%); and other data (4.3%). According to the PROBAST checklist, only 28 articles (23.9%) have a low risk of bias. A benchmark case study is conducted for five different data modalities. We also discuss current limitations and future directions of applying ML to the diagnosis of PD. This review reduces the gap between Artificial Intelligence (AI) and PD medical professionals and provides helpful information for future research.},
  archive      = {J_AIR},
  author       = {Zhang, Juntao and Zhang, Yiming and Weng, Ying and Hosseini, Akram A. and Wang, Boding and Dening, Tom and Fan, Weinyu and Xiao, Weizhong},
  doi          = {10.1007/s10462-025-11347-y},
  journal      = {Artificial Intelligence Review},
  month        = {11},
  number       = {11},
  pages        = {1-58},
  shortjournal = {Artif. Intell. Rev.},
  title        = {Applications of machine learning for computer-aided diagnosis of parkinson’s disease: Progress and benchmark case study},
  volume       = {58},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). UAV target tracking: A survey. <em>AIR</em>, <em>58</em>(11), 1-62. (<a href='https://doi.org/10.1007/s10462-025-11348-x'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Unmanned Aerial Vehicles (UAVs) have become critical enablers of integrated air-space-ground Internet of Things (IoT) ecosystems, with target tracking serving as a foundational technology. This paper classifies UAV target tracking into two distinct paradigms: active tracking and passive tracking, differentiated by their operational scopes and technical objectives. Active tracking is defined as a closed-loop spatial pursuit system, whereby UAVs dynamically track targets through iterative cycles centered on three primary stages: online passive tracking, state fusion estimation, and tracking strategy generation, with subsequent execution phases implied in the loop. This workflow bridges perception and action, enabling spatial engagement through continuous sensor-to-control feedback. In contrast, passive tracking acts as a vision-centric analytical module that exclusively extracts target image-domain attributes from visual sensors—devoid of physical state inference or control mechanisms. As a preprocessing stage for active systems, it is constrained to the visual perception layer, lacking the spatial engagement capabilities inherent in closed-loop tracking systems. This paper conducts an in-depth analysis of the application, key challenges, and future trends in both active and passive UAV target tracking. By systematically discussing the relationships among relevant technologies, this work aims to establish a foundational reference framework and offer citation material for guiding the future development of UAV target tracking technologies.},
  archive      = {J_AIR},
  author       = {Wu, Pengnian and Li, Yixuan and Xue, Dong},
  doi          = {10.1007/s10462-025-11348-x},
  journal      = {Artificial Intelligence Review},
  month        = {11},
  number       = {11},
  pages        = {1-62},
  shortjournal = {Artif. Intell. Rev.},
  title        = {UAV target tracking: A survey},
  volume       = {58},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A comprehensive survey on subspace clustering: Methods and applications. <em>AIR</em>, <em>58</em>(11), 1-50. (<a href='https://doi.org/10.1007/s10462-025-11349-w'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As a pivotal strategy to deal with complicated and high-dimensional data, subspace clustering is to find a set of subspaces of a high-dimensional space and then partition each data point in dataset into the corresponding subspace. This field has witnessed remarkable progress over recent decades, with substantial theoretical advancements and successful applications spanning image processing, genomic analysis and text analysis. However, existing surveys predominantly focus on conventional shallow-structured methods, with few up-to-date reviews on deep-structured methods, i.e., deep neural network-based approaches. In fact, recent years has witnessed the overwhelming success of deep neural network in various fields, including computer vision, natural language processing, subspace clustering. To address this gap, this paper presents a comprehensive review on subspace clustering methods, including conventional shallow-structured and deep neural network based approaches, which systematically analyzes over 150 papers published in peer-reviewed journals and conferences, highlighting the latest research achievements, methods, algorithms and applications. Specifically, we first briefly introduce the basic principles and evolution of subspace clustering. Subsequently, we present an overview of research on subspace clustering, dividing the existing works into two categories: shallow subspace clustering and deep subspace clustering, based on the model architecture. Within each category, we introduce a refined taxonomy distinguishing linear and nonlinear approaches based on data characteristics and subspace structural assumptions. Finally, we discuss the challenges currently faced and future research direction for development in the field of subspace clustering.},
  archive      = {J_AIR},
  author       = {Miao, Jianyu and Zhang, Xiaochan and Yang, Tiejun and Fan, Chao and Tian, Yingjie and Shi, Yong and Xu, Mingliang},
  doi          = {10.1007/s10462-025-11349-w},
  journal      = {Artificial Intelligence Review},
  month        = {11},
  number       = {11},
  pages        = {1-50},
  shortjournal = {Artif. Intell. Rev.},
  title        = {A comprehensive survey on subspace clustering: Methods and applications},
  volume       = {58},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Defending against attacks in deep learning with differential privacy: A survey. <em>AIR</em>, <em>58</em>(11), 1-37. (<a href='https://doi.org/10.1007/s10462-025-11350-3'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, we have witnessed the revolutionary development of deep learning. As the application domain of deep learning has expanded, its privacy risks have attracted attention since deep leaning methods often use private data for training. Some methods for attacking deep learning, such as membership inference attacks, increase the privacy risks of deep learning models. One risk-reducing defensive strategy with great potential is to apply some degree of random perturbation during the training (or other) phase. Therefore, differential privacy, as a privacy protection framework originally designed for publishing data, is widely used to protect the privacy of deep learning models due to its solid mathematical foundation. In this paper, we first introduce several attack methods that threaten deep learning. Then, we systematically review the cross-applications of differential privacy and deep learning to protect deep learning models. We encourage researchers to visually demonstrate the defense effects of their approaches in the literature rather than solely providing rigorous mathematical proofs. In addition to privacy, we also discuss and review the impact of differential privacy on the robustness, overfitting, and fairness of deep neural networks. Finally, we analyze some potential future research directions, highlighting the significant potential for differential privacy to make positive contributions to future deep learning systems.},
  archive      = {J_AIR},
  author       = {Xiangfei, Zhang and Qingchen, Zhang},
  doi          = {10.1007/s10462-025-11350-3},
  journal      = {Artificial Intelligence Review},
  month        = {11},
  number       = {11},
  pages        = {1-37},
  shortjournal = {Artif. Intell. Rev.},
  title        = {Defending against attacks in deep learning with differential privacy: A survey},
  volume       = {58},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Swarm intelligence techniques and their applications in fog/edge computing: An in-depth review. <em>AIR</em>, <em>58</em>(11), 1-182. (<a href='https://doi.org/10.1007/s10462-025-11351-2'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent advances in the Internet of Things (IoT) have connected diverse devices that often have limited resources and processing power. Artificial intelligence (AI) applications in fog and edge computing are greatly enhanced by Swarm Intelligence (SI) techniques. These SI methods improve resource allocation, task scheduling, and load balancing, making distributed systems more efficient and responsive to changing conditions. This paper systematically reviews 91 studies (2019–2023) on SI applications in fog/edge environments. We compare fog, edge, and cloud computing paradigms and analyze SI-based approaches using case studies, performance metrics, and evaluation tools. This review identifies key advantages and limitations of current SI-based approaches and highlights open issues and future research directions to enhance distributed computing systems. These insights aim to guide the development of more efficient and responsive AI-driven resource management strategies in fog/edge environments.},
  archive      = {J_AIR},
  author       = {Ghafari, Reyhane and Mansouri, Najme},
  doi          = {10.1007/s10462-025-11351-2},
  journal      = {Artificial Intelligence Review},
  month        = {11},
  number       = {11},
  pages        = {1-182},
  shortjournal = {Artif. Intell. Rev.},
  title        = {Swarm intelligence techniques and their applications in fog/edge computing: An in-depth review},
  volume       = {58},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). AI-induced deskilling in medicine: A mixed-method review and research agenda for healthcare and beyond. <em>AIR</em>, <em>58</em>(11), 1-40. (<a href='https://doi.org/10.1007/s10462-025-11352-1'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The integration of Artificial Intelligence (AI) in healthcare is reshaping clinical practice, offering both opportunities for enhanced decision-making and risks of skill degradation among medical professionals. This growing impact calls for a comprehensive evaluation of its effects on medical expertise. This study presents a mixed-method literature review, combining systematic analysis with narrative synthesis to examine AI-induced deskilling and upskilling inhibition-the erosion of medical expertise and the reduction of opportunities for skill acquisition due to AI-driven decision support systems. Anchoring the discussion in the core medical competencies outlined by the Federation of Royal Colleges of Physicians of the UK-Practical Assessment of Clinical Examination Skills (PACES-MRCPUK), the systematic review identifies key vulnerabilities in physical examination, differential diagnosis, clinical judgment, and physician-patient communication. The narrative review explores broader themes related to Human–AI Interaction and the Impact of AI on Human Skills in Organizations. In response to concerns about the Second Singularity-a scenario in which decision-making autonomy is increasingly ceded to AI, weakening human oversight-this review advocates for a research agenda that prioritizes longitudinal studies, real-time monitoring of AI’s impact, and the development of frameworks to mitigate skill erosion, ensuring the preservation of professional autonomy and the safeguarding of the irreplaceable elements of human judgment in medicine and beyond.},
  archive      = {J_AIR},
  author       = {Natali, Chiara and Marconi, Luca and Dias Duran, Leslye Denisse and Cabitza, Federico},
  doi          = {10.1007/s10462-025-11352-1},
  journal      = {Artificial Intelligence Review},
  month        = {11},
  number       = {11},
  pages        = {1-40},
  shortjournal = {Artif. Intell. Rev.},
  title        = {AI-induced deskilling in medicine: A mixed-method review and research agenda for healthcare and beyond},
  volume       = {58},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Improved decisions for unknown behaviours in interactive dynamic influence diagrams. <em>AIR</em>, <em>58</em>(11), 1-28. (<a href='https://doi.org/10.1007/s10462-025-11355-y'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Interactive dynamic influence diagrams (I-DIDs) are a general decision framework for a subject agent who interacts with other agents (of either collaborative or competitive) in a common environment with partial observability. The subject agent aims to optimize its decision-making (response strategy) while other agents concurrently adapt their behaviors over time. The I-DID model has faced a long-term challenge when other agents exhibit unknown behaviors that go beyond what the subject agent has planned for prior to their interactions. This is because the subject agent does not hold the capability of modeling unknown behaviours of other agents in traditional I-DID techniques. In this article, we adapt two different swarm intelligence (SI) techniques to develop new behaviours for other agents in I-DIDs. The SI-based algorithms have the strength of generating a collective set of behaviours that could potentially contain various types of agents’ behaviours. We theoretically analyze how the two algorithms impact the subject agent’s decision quality, and empirically demonstrate the algorithm performance in two commonly used problem domains.},
  archive      = {J_AIR},
  author       = {Pan, Yinghui and Zhou, Mengen and Ma, Biyang and Zeng, Yifeng and Ong, Yew-soon and Liu, Guoquan},
  doi          = {10.1007/s10462-025-11355-y},
  journal      = {Artificial Intelligence Review},
  month        = {11},
  number       = {11},
  pages        = {1-28},
  shortjournal = {Artif. Intell. Rev.},
  title        = {Improved decisions for unknown behaviours in interactive dynamic influence diagrams},
  volume       = {58},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Ameliorated elk herd optimizer for global optimization and engineering problems. <em>AIR</em>, <em>58</em>(11), 1-80. (<a href='https://doi.org/10.1007/s10462-025-11360-1'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Optimization techniques have received significant attention for reliably addressing practical problems. A potential meta-heuristic called elk herd optimizer (EHO) was created, inspired by the social behavior and reproduction of elks. EHO has drawbacks, including poor convergence competency and a tendency to fall into local extrema in various optimization problems. Furthermore, this algorithm does not account for the memory of its search agents and has difficulty effectively balancing exploration and exploitation, which can lead to early convergence toward a local optimum. This study addresses the above issues by proposing an ameliorated EHO (AEHO) by incorporating several modifications into the basic EHO algorithm, which can be described as follows: A new hybrid memory-based EHO is developed that uses the particle swarm optimization (PSO) algorithm to guide EHO to search for reasonable candidate solutions. This hybrid approach was proposed to enhance EHO’s diversity and balance search capabilities to achieve strong search performance. Initially, a memory component was added to EHO using the idea of pbest from PSO to tap into promising search regions, which focuses on improving the best solutions and preventing the algorithm from getting stuck in a local optimum. In addition, the PSO concepts of (gbest) and (pbest) are used to enhance the best placements of the search agents in EHO. Finally, a greedy selection method was used to improve the efficiency of exhaustive exploration in AEHO, using the fitness values before and after updates as an indicator for efficacy of the best solutions. To evaluate the performance of the AEHO algorithm against a group of well-known competitors, we use ten complex test functions from the global CEC2022 test suite and thirty complex test functions from the global CEC2014 test suite. Based on the analysis of the experimental findings, AEHO performed optimally on 84% of the CEC2014 functions and 74% of the CEC2022 functions, ranking first in both suites with an average ranking of 3.11 and 1.62, respectively. The mean computation time of AEHO is about one-third of the average computation time for the first-ranked method, indicating that AEHO not only performs very well in global searches but also exhibits greater search efficiency when compared to newer optimization algorithms. The applicability and reliability of AEHO were thoroughly studied on four constrained engineering design problems and a real-world industrial process. The results demonstrate the superiority and promising potential of AEHO in addressing a wide range of challenging real-world problems.},
  archive      = {J_AIR},
  author       = {Al-Betar, Mohammed Azmi and Braik, Malik Sh. and Shambour, Qusai Yousef and Al-Naymat, Ghazi and Porntaveetus, Thantrira},
  doi          = {10.1007/s10462-025-11360-1},
  journal      = {Artificial Intelligence Review},
  month        = {11},
  number       = {11},
  pages        = {1-80},
  shortjournal = {Artif. Intell. Rev.},
  title        = {Ameliorated elk herd optimizer for global optimization and engineering problems},
  volume       = {58},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). DAoG: Decayed adaptation over gradients for parameter-free step size control. <em>AIR</em>, <em>58</em>(11), 1-37. (<a href='https://doi.org/10.1007/s10462-025-11362-z'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As the scale of parameters in deep learning models continues to grow, the cost of training such models increases accordingly, posing increasingly significant challenges for stochastic optimization methods. A central issue in gradient-based optimization lies in the selection of the step size, whose appropriateness directly affects training efficiency and model performance. To address this issue, a series of parameter-free optimization methods that do not require manual tuning of the step size have been proposed in recent years. Among them, DoG and its improved variant DoWG are the most representative. Despite demonstrating strong performance across various tasks, DoG and DoWG still suffer from performance instability or slow convergence under certain model architectures or training conditions. This paper introduces Decayed Adaptation over Gradients (DAoG), a novel parameter-free optimization method that systematically addresses these limitations. Our key innovation lies in incorporating a principled step size decay mechanism for the first time within the parameter-free optimization framework, which substantially enhances both optimization stability and model generalization. Additionally, a parameter compression strategy is employed to reduce sensitivity to the initial step size. Theoretical analysis demonstrates that DAoG exhibits favorable convergence properties under L-smooth and G-Lipschitz conditions. Empirical studies across representative tasks in natural language processing and computer vision demonstrate that DAoG outperforms both DoG and DoWG in terms of convergence speed and generalization performance. Notably, it even rivals or surpasses Adam with cosine annealing in several challenging scenarios. These theoretical and experimental results suggest that DAoG effectively mitigates the overly conservative step size issue in DoG and the instability problem in DoWG, thereby advancing the development of parameter-free optimization methods in deep learning.},
  archive      = {J_AIR},
  author       = {Zhang, Yifan and Zhao, Di and Li, Hongyi and Pan, Chengwei},
  doi          = {10.1007/s10462-025-11362-z},
  journal      = {Artificial Intelligence Review},
  month        = {11},
  number       = {11},
  pages        = {1-37},
  shortjournal = {Artif. Intell. Rev.},
  title        = {DAoG: Decayed adaptation over gradients for parameter-free step size control},
  volume       = {58},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Place recognition meet multiple modalities: A comprehensive review, current challenges and future development. <em>AIR</em>, <em>58</em>(11), 1-48. (<a href='https://doi.org/10.1007/s10462-025-11367-8'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Place recognition is a cornerstone of vehicle navigation and mapping, which is pivotal in enabling systems to determine whether a location has been previously visited. This capability is critical for tasks such as loop closure in Simultaneous Localization and Mapping (SLAM) and long-term navigation under varying environmental conditions. This survey comprehensively reviews recent advancements in place recognition, emphasizing three representative methodological paradigms: Convolutional Neural Network (CNN)-based approaches, Transformer-based frameworks, and cross-modal strategies. We begin by elucidating the significance of place recognition within the broader context of autonomous systems. Subsequently, we trace the evolution of CNN-based methods, highlighting their contributions to robust visual descriptor learning and scalability in large-scale environments. We then examine the emerging class of Transformer-based models, which leverage self-attention mechanisms to capture global dependencies and offer improved generalization across diverse scenes. Furthermore, we discuss cross-modal approaches that integrate heterogeneous data sources such as Lidar, vision, and text description, thereby enhancing resilience to viewpoint, illumination, and seasonal variations. We also summarize standard datasets and evaluation metrics widely adopted in the literature. To the best of our knowledge, no prior survey has systematically reviewed visual, LiDAR, and cross-modal place recognition concurrently. This work thus resolves a critical gap in existing literature dominated by single-modality studies. Finally, we identify current research challenges and outline prospective directions, including domain adaptation, real-time performance, and lifelong learning, to inspire future advancements in this domain. The unified framework of leading-edge place recognition methods, i.e., code library, and the results of their experimental evaluations are available at https://github.com/CV4RA/SOTA-Place-Recognitioner .},
  archive      = {J_AIR},
  author       = {Li, Zhenyu and Shang, Tianyi and Xu, Pengjie and Deng, Zhaojun},
  doi          = {10.1007/s10462-025-11367-8},
  journal      = {Artificial Intelligence Review},
  month        = {11},
  number       = {11},
  pages        = {1-48},
  shortjournal = {Artif. Intell. Rev.},
  title        = {Place recognition meet multiple modalities: A comprehensive review, current challenges and future development},
  volume       = {58},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A dynamic operation room scheduling DORS strategy based on explainable AI and fuzzy interface engine. <em>AIR</em>, <em>58</em>(11), 1-39. (<a href='https://doi.org/10.1007/s10462-025-11366-9'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Poor surgical scheduling causes major problems in hospital operating rooms, such as long patient wait times, underutilized operating rooms, and high costs. Existing scheduling approaches, which are static or less adaptable, fail to handle real-time unpredictability. To overcome these constraints, this study presents Dynamic Operation Room Scheduling (DORS), a new intraday surgical scheduling system. DORS uses a two-layered architecture: (1) Explainable AI for feature selection that is based on critical scheduling criteria such as Round Robin, and (2) a dynamic scheduling system that includes a Receiving Module, a Checking Module for patient prioritization, and a Scheduling Module provided by a Fuzzy Interface Engine. This system allows for proactive schedule preparation and reactive modifications, making it possible to smoothly include unscheduled surgical operations. In comparison to traditional (FCFS, Round Robin) and optimization-based (genetic algorithm) methods. DORS dynamically modifies schedules to reduce average wait times (AWT), consistently outperforming other approaches by 120–560 min. DORS completes surgical operations more quickly (half of surgical operations in 255–725 min). In addition, DORS retains a modest runtime (45 ms) while increasing scheduling efficiency (98.6%). DORS also demonstrates strong stability, with low Relative Percentage Deviation (RPD) on high-demand days. Finally, DORS achieves the optimal blend of speed, efficiency, and responsiveness, making it the greatest choice for hospitals aiming to eliminate delays, optimize operating room usage, and effectively manage changing surgical needs.},
  archive      = {J_AIR},
  author       = {El-Balka, Rana Mohamed and Sakr, Noha and Rabie, Asmaa H. and Saleh, Ahmed I.},
  doi          = {10.1007/s10462-025-11366-9},
  journal      = {Artificial Intelligence Review},
  month        = {11},
  number       = {11},
  pages        = {1-39},
  shortjournal = {Artif. Intell. Rev.},
  title        = {A dynamic operation room scheduling DORS strategy based on explainable AI and fuzzy interface engine},
  volume       = {58},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Revisiting natural selection: Evolving dynamic neural networks using genetic algorithms for complex control tasks. <em>AIR</em>, <em>58</em>(11), 1-35. (<a href='https://doi.org/10.1007/s10462-025-11382-9'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Reinforcement learning (RL) and Genetic Algorithms (GAs) are widely used in decision-making and control tasks, but they often suffer from prolonged training times and inefficiencies. This paper addresses the need for a faster and more precise method to train neural networks in RL tasks, without sacrificing performance. The proposed approach enhances GAs by introducing mechanisms that optimize network architectures dynamically, minimizing unnecessary complexity while maintaining accuracy. The methodology includes a dynamic architecture adaptation technique that trims the neural network to its most compact and effective configuration. A Blending mechanism is introduced to improve the propagation of essential features across network layers, reducing the usage of non-linearity until necessary. An experience replay buffer is integrated to avoid redundant fitness evaluations, significantly reducing computational overhead. Additionally, a novel approach combines back-propagation with GAs for further refinement in supervised or RL tasks, using it as a mutation method to fine-tune the model. Experimental results demonstrate convergence speeds of around several seconds for simple tasks with well-defined rewards, and several minutes for more complex tasks. Training time is reduced by nearly 70%, and the approach provides faster inference speeds due to minimal architecture, making it applicable for mobile and edge devices. The method reduces computation, especially during inference, by over 90% due to the extremely low number of parameters. The performance metrics show comparable results to conventional approaches at the end of training. The proposed method is scalable and resource-efficient, outperforming existing neural network optimization techniques in both simulated environments and real-world applications. The developed framework is publicly available under the MIT license at https://github.com/AhmedBoin/atgen offering an open-source solution for the broader research community.},
  archive      = {J_AIR},
  author       = {Taha, Mohamed A. and Saafan, Mahmoud M. and Ayyad, Sarah M.},
  doi          = {10.1007/s10462-025-11382-9},
  journal      = {Artificial Intelligence Review},
  month        = {11},
  number       = {11},
  pages        = {1-35},
  shortjournal = {Artif. Intell. Rev.},
  title        = {Revisiting natural selection: Evolving dynamic neural networks using genetic algorithms for complex control tasks},
  volume       = {58},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Script identification in multilingual environment: A survey in recent years. <em>AIR</em>, <em>58</em>(10), 1-28. (<a href='https://doi.org/10.1007/s10462-025-11194-x'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multilingualism is an important trend in the field of optical character recognition (OCR). In a multilingual environment, the task of script identification often combines with other tasks to complete multilingual work jointly. As a front-end function of a multilingual OCR system, it automatically identifies the language of the text image and further recognizes text in multilingual engines. In reality, script identification plays a major role, especially in multilingual scene understanding, as well as intelligent document analysis and recognition. This survey introduces the technology of script identification and summarizes the related work developed in this field from 2017 to date, including traditional learning, deep learning, and available datasets. Based on a comprehensive analysis of existing work, it provides a new survey for researchers to grasp recent script identification work. By discussing the problems that need to be solved, it can lay the foundation for related research work and activities.},
  archive      = {J_AIR},
  author       = {Yang, Yaowei and Eli, Elham and Aysa, Alimjan and Ubul, Kurban},
  doi          = {10.1007/s10462-025-11194-x},
  journal      = {Artificial Intelligence Review},
  month        = {10},
  number       = {10},
  pages        = {1-28},
  shortjournal = {Artif. Intell. Rev.},
  title        = {Script identification in multilingual environment: A survey in recent years},
  volume       = {58},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Techniques and challenges for nuclei segmentation in cervical smear images: A review. <em>AIR</em>, <em>58</em>(10), 1-53. (<a href='https://doi.org/10.1007/s10462-025-11207-9'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cervical cancer is one of the fastest-growing cancers affecting women, leading to a significant number of deaths. However, early detection and timely treatment can greatly reduce the mortality rate and improve the chances of recovery. A widely used method for the diagnosis of cancer is the manual analysis of tissue biopsy specimens on slides. This manual process of specimen examination is time-consuming and error-prone, resulting in an increasing interest in digitizing histopathological workflows to refine and expedite analysis. This paper has compiled a comprehensive review of automated cervical nuclei segmentation approaches in histopathological images. We have examined both deep learning and traditional image segmentation methods, working mechanisms, and their variants, as well as the datasets used to evaluate these approaches. To find relevant studies, we searched on platforms such as IEEE Xplore, Google Scholar, ACM Digital Library, SpringerLink, and ScienceDirect using keywords such as cervical nuclei, nuclear, and nucleus, deep learning network (DNN), convolutional neural networks (CNNs), cervical cytology or histopathology, and traditional or classical image segmentation methods. We reviewed 78 research papers on both classical image segmentation and deep learning-based techniques specifically designed for nuclei segmentation in cervical histopathological images, published from 2010 until October 2024. We organized these studies into two main categories: Classical image segmentation methods and deep learning approaches, subdividing them into relevant subcategories and compiled a comparative analysis of their results, identified ongoing challenges in nuclei segmentation, and highlighted opportunities and future prospects for this task. We discussed recent studies on cervical nuclei segmentation using automated methods. The implementation of automated image segmentation methods and their various extensions has significantly improved the performance of automated diagnostic systems for cervical cancer.},
  archive      = {J_AIR},
  author       = {Rasheed, Assad and Shirazi, Syed Hamad and Khan, Pordil and Aseere, Ali M. and Shahzad, Muhammad},
  doi          = {10.1007/s10462-025-11207-9},
  journal      = {Artificial Intelligence Review},
  month        = {10},
  number       = {10},
  pages        = {1-53},
  shortjournal = {Artif. Intell. Rev.},
  title        = {Techniques and challenges for nuclei segmentation in cervical smear images: A review},
  volume       = {58},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Peeping at creAItivity through a keyhole: Creative self-perceptions, potential, and enhancement of GenAI chatbots. <em>AIR</em>, <em>58</em>(10), 1-44. (<a href='https://doi.org/10.1007/s10462-025-11288-6'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The work in this paper investigates (a) the (emerging) creative self-perceptions of GenAI chatbots, (b) their creative potential, (c) their ability to self-assess the creativity of their own outcomes and that of their peers, and (d) how their creative outcomes can be improved. To this end, an exploratory study was implemented involving three popular commercial chatbots: ChatGPT (GPT-4o - paid) by OpenAI, Claude (3.5 Sonnet - paid) by Anthropic and Gemini (1.5 Flash - free) by Google. The study included four phases and employed well-established methods and tools from the scientific domain of (human) creativity research, including the Short Scale of Creative Self (SSCS) questionnaire, a verbal test of convergent-integrative thought from the Evaluation of Potential Creativity (EPoC) battery which was scored by the chatbots and human experts, and the Dynamic Assessment (DA) approach and humor as means for enhancing the chatbots’ creative outcomes. The results of the study are encapsulated in 21 original and, sometimes, surprising observations and in 6 practical insights regarding the use of GenAI chatbots as creativity-support tools.},
  archive      = {J_AIR},
  author       = {Grammenos, Dimitris and Lubart, Todd},
  doi          = {10.1007/s10462-025-11288-6},
  journal      = {Artificial Intelligence Review},
  month        = {10},
  number       = {10},
  pages        = {1-44},
  shortjournal = {Artif. Intell. Rev.},
  title        = {Peeping at creAItivity through a keyhole: Creative self-perceptions, potential, and enhancement of GenAI chatbots},
  volume       = {58},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A review of feature selection methods for actual evapotranspiration prediction. <em>AIR</em>, <em>58</em>(10), 1-37. (<a href='https://doi.org/10.1007/s10462-025-11298-4'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Accurate prediction of actual evapotranspiration (AET) is critical for hydrological modeling, agricultural planning, and climate studies. Machine learning models have emerged as powerful AET prediction tools because they can handle complex, nonlinear relationships in large datasets. However, selecting relevant input features significantly impacts model performance, efficiency, and interpretability. Feature selection techniques reduce high-dimensional datasets by identifying redundant and uncorrelated variables. This paper reviews feature selection approaches for predicting ML-based AETs by analyzing 62 studies; a total of 416 were retrieved from seven digital libraries. Our analysis shows that filtering methods are the most widely used $$(38.8\%)$$ , followed by manual selection based on domain expertise $$(28.7\%)$$ , embedded methods $$(17.5\%)$$ , and wrapper methods $$(11.2\%)$$ . Dimensionality reduction techniques, such as principal component analysis (PCA), are the least used $$(3.8\%)$$ . Among machine learning models, Random Forest (RF) and Artificial Neural Networks (ANN) are the most commonly used, with 29 and 27 instances, respectively. The study highlights the strengths and limitations of each category of feature selection, emphasizing the potential of hybrid approaches integrating filter, wrapper, embedded, and manual selection methods. These combinations improve model accuracy, robustness, and generalization, while mitigating overfitting, computational inefficiency, and sensitivity to noise. This review provides insights into optimal feature selection strategies for improving ML-based AET prediction.},
  archive      = {J_AIR},
  author       = {Liyew, Chalachew Muluken and Ferraris, Stefano and Di Nardo, Elvira and Meo, Rosa},
  doi          = {10.1007/s10462-025-11298-4},
  journal      = {Artificial Intelligence Review},
  month        = {10},
  number       = {10},
  pages        = {1-37},
  shortjournal = {Artif. Intell. Rev.},
  title        = {A review of feature selection methods for actual evapotranspiration prediction},
  volume       = {58},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Efficient and autonomous detection of olive leaf diseases using AI-enhanced MetaFormer. <em>AIR</em>, <em>58</em>(10), 1-35. (<a href='https://doi.org/10.1007/s10462-025-11131-y'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Agriculture forms the cornerstone of global food security, with olives playing a pivotal role not only as a food source but also in cosmetics, medicine, and other industries. However, diseases affecting olive trees pose significant threats to agricultural productivity and economic stability, underscoring the need for innovative detection solutions. A promising solution to these challenges is the development of deep learning-based computer-aided diagnostic applications, which have shown remarkable success in various fields, especially in recent years. This study presents a novel deep-learning approach for olive leaf disease detection, introducing a MetaFormer-based architecture that combines the power of transformer-based components, specifically separable self-attention, with the efficiency of a lightweight design. The proposed model was evaluated using two distinct datasets, Dataset-1 and Dataset-2, where it achieved impressive accuracy rates of 99.31% and 96.91%, respectively. When compared to other cutting-edge models such as Swin-Base, MaxViT-Base, DeiT3-Base, CAFormer-s18, CAFormer-m36, ResNet50, and MobileNetv3, the Proposed Model outperformed them in terms of accuracy, precision, recall, and F1-score. These advancements were made possible through the incorporation of separable self-attention, which allows for capturing both local and global dependencies in olive leaf images, and a streamlined architecture that reduces computational complexity without sacrificing performance. Furthermore, Grad-CAM visualizations highlighted the interpretability of the model, confirming its ability to focus on disease-relevant regions of the images. This study offers a significant contribution to the field of agricultural disease detection, particularly in olive farming, and sets the stage for future work in adapting the model for other crops and real-time applications in agriculture.},
  archive      = {J_AIR},
  author       = {Pacal, Ishak and Kilicarslan, Serhat and Ozdemir, Burhanettin and Deveci, Muhammet and Kadry, Seifedine},
  doi          = {10.1007/s10462-025-11131-y},
  journal      = {Artificial Intelligence Review},
  month        = {10},
  number       = {10},
  pages        = {1-35},
  shortjournal = {Artif. Intell. Rev.},
  title        = {Efficient and autonomous detection of olive leaf diseases using AI-enhanced MetaFormer},
  volume       = {58},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Decoding synthetic news: An interpretable multimodal framework for the classification of news articles in a novel news corpus. <em>AIR</em>, <em>58</em>(10), 1-31. (<a href='https://doi.org/10.1007/s10462-025-11188-9'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent advancements in Artificial Intelligence (AI), notably the development of Large Language Models (LLMs) and text-to-image diffusion models, have facilitated the creation of realistic textual content and images. Specifically, platforms like ChatGPT and Midjourney have simplified the creation of high-quality text and visuals with minimal expertise and cost. The increasing sophistication of Generative AI presents challenges in ensuring the integrity of news, media, and information quality, making it increasingly difficult to distinguish between real and artificially generated textual and visual content. Our work addressed this problem in two ways. First, by means of ChatGPT and Midjourney, we created a comprehensive novel multimodal news corpus named SyN24News based on the N24News corpus, on which we evaluated our model. Second, we developed a novel explainable synthetic news detector for discriminating between real and synthetic news articles. We leveraged a Neural Additive Model (NAM)-like network structure that ensures effect separation by handling input data in separate subnetworks. Complex structures and patterns are extracted by deep features from unstructured data, i.e., images and texts, using fine-tuned VGG and DistilBERT subnetworks. We ensured further explainability by individually processing carefully chosen handcrafted text and image features in simple Multilayer Perceptrons (MLPs), allowing for graphical interpretation of corresponding structured effects. Our findings indicate that textual information are the main drivers in the decision-making finding process. Structured textual effects, particularly Flesch-Kincaid reading ease and sentiment, have a much higher influence on the classification outcome than visual features such as dissimilarity and homogeneity.},
  archive      = {J_AIR},
  author       = {Schlee, Michael and Kant, Gillian and Ehrling, Christoph and Säfken, Benjamin and Kneib, Thomas},
  doi          = {10.1007/s10462-025-11188-9},
  journal      = {Artificial Intelligence Review},
  month        = {10},
  number       = {10},
  pages        = {1-31},
  shortjournal = {Artif. Intell. Rev.},
  title        = {Decoding synthetic news: An interpretable multimodal framework for the classification of news articles in a novel news corpus},
  volume       = {58},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Neural architecture search: Two constant shared weights initialisations. <em>AIR</em>, <em>58</em>(10), 1-27. (<a href='https://doi.org/10.1007/s10462-025-11238-2'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the last decade, zero-cost metrics have gained prominence in neural architecture search (NAS) due to their ability to evaluate architectures without training. These metrics are significantly faster and less computationally expensive than traditional NAS methods and provide insights into neural architectures’ internal workings. This paper introduces epsinas, a novel zero-cost NAS metric that assesses architecture potential using two constant shared weight initialisations and the statistics of their outputs. We show that the dispersion of raw outputs, normalised by their average magnitude, strongly correlates with trained accuracy. This effect holds across image classification and language tasks on NAS-Bench-101, NAS-Bench-201, and NAS-Bench-NLP. Our method requires no data labels, operates on a single minibatch, and eliminates the need for gradient computation, making it independent of training hyperparameters, loss metrics, and human annotations. It evaluates a network in a fraction of a GPU second and integrates seamlessly into existing NAS frameworks. The code supporting this study can be found on GitHub at https://github.com/egracheva/epsinas .},
  archive      = {J_AIR},
  author       = {Gracheva, Ekaterina},
  doi          = {10.1007/s10462-025-11238-2},
  journal      = {Artificial Intelligence Review},
  month        = {10},
  number       = {10},
  pages        = {1-27},
  shortjournal = {Artif. Intell. Rev.},
  title        = {Neural architecture search: Two constant shared weights initialisations},
  volume       = {58},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). An overview of learning-based dexterous grasping: Recent advances and future directions. <em>AIR</em>, <em>58</em>(10), 1-44. (<a href='https://doi.org/10.1007/s10462-025-11262-2'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, the practical implications of dexterous grasping technology have become a key point of research in robotics and artificial intelligence. At its core, this technology aims to empower robots to achieve human-level grasping capabilities. To help researchers quickly acquire the latest advancements, we have conducted a comprehensive review of the recent research developments, focusing on learning-based approaches, from two perspectives: Grasp Generation (GG) and Grasp Execution (GE). Specifically, GG refers to generating appropriate grasping poses for the target object. GE refers to executing grasp poses by motion planning and motion control. Afterwards, we introduce recent benchmark datasets and evaluation metrics. Based on these extensive benchmarks, we offer a comparative analysis of the state-of-the-art solutions. Lastly, we highlight several research directions that need to be further addressed, which will greatly facilitate the practical deployment of dexterous grasping technology in industrial manufacturing, household services, medical rehabilitation, etc. We believe it is a crucial area of research for future progress in robotic manipulation.},
  archive      = {J_AIR},
  author       = {Song, Xu and Li, Yongyao and Zhang, Yunfan and Liu, Yufei and Jiang, Lei},
  doi          = {10.1007/s10462-025-11262-2},
  journal      = {Artificial Intelligence Review},
  month        = {10},
  number       = {10},
  pages        = {1-44},
  shortjournal = {Artif. Intell. Rev.},
  title        = {An overview of learning-based dexterous grasping: Recent advances and future directions},
  volume       = {58},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). QDeepColonNet: A quantum-based deep learning network for colorectal cancer classification using attention-driven DenseNet and shuffled dynamic local feature extraction network. <em>AIR</em>, <em>58</em>(10), 1-33. (<a href='https://doi.org/10.1007/s10462-025-11295-7'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Colorectal Cancer (CRC) is one of the most common and severe types of cancer globally, affecting millions of people each year. It primarily develops from benign polyps in the colon or rectum, which can turn malignant if not detected and treated early, leading to serious health risks. Current diagnostic methods for CRC detection are primarily manual and require significant time, resources and expertise. This creates a pressing need for automated solutions that are both efficient and highly accurate. This research proposes a hybrid Deep Learning (DL) and Quantum Machine Learning (QML)-based system for CRC classification, designed to address these challenges using a dual-track approach. The proposed QDeepColonNet leverages DL for robust feature extraction, combining DenseNet with an Enhanced Feature Learnable Group Attention (EFLGA) block to capture both high and mid-level features. Additionally, it integrates the Shuffled Dynamic Local Feature Extraction Network (SDLFEN) with a Lightweight Multi-Kernel Convolution (LMKC) block to capture short-range dependencies. The concatenated feature maps from both tracks are further refined by Efficient Channel Attention (ECA), enhancing cross-channel interactions without increasing complexity. Finally, the refined features are classified using a QML-based classifier, which effectively handles intricate data and captures complex feature relationships. To the best of our understanding, this is the first study to incorporate a QML-based hybrid classification network CRC detection. The performance of the proposed QDeepColonNet surpassed several state-of-the-art DL models and achieved a classification accuracy of 98.92% when tested on the EBHI dataset.},
  archive      = {J_AIR},
  author       = {Ajay, Armaano and Karthik, R. and Bisht, Akshaj Singh and Singh, Abhay Karan},
  doi          = {10.1007/s10462-025-11295-7},
  journal      = {Artificial Intelligence Review},
  month        = {10},
  number       = {10},
  pages        = {1-33},
  shortjournal = {Artif. Intell. Rev.},
  title        = {QDeepColonNet: A quantum-based deep learning network for colorectal cancer classification using attention-driven DenseNet and shuffled dynamic local feature extraction network},
  volume       = {58},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Physics-informed machine learning for advancing computational medical imaging: Integrating data-driven approaches with fundamental physical principles. <em>AIR</em>, <em>58</em>(10), 1-49. (<a href='https://doi.org/10.1007/s10462-025-11303-w'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Medical imaging is a cornerstone of modern healthcare, enabling precise diagnosis, treatment planning, and disease monitoring. Traditional machine learning (ML) approaches have significantly improved medical image analysis, yet they face challenges such as data scarcity, lack of interpretability, and variability in imaging protocols. Physics-Informed Machine Learning (PIML) offers a transformative solution by integrating fundamental physical laws, usually in partial differential equations and boundary conditions, into data-driven ML models. PIML constrains the solution space, enhances interpretability, and reduces the dependency on large, annotated datasets. This review provides an overview of the principles, methodologies, and applications of PIML in medical imaging, with a focus on imaging modalities such as MRI, CT, and ultrasound. We discuss the taxonomy of PIML approaches based on observational, inductive, and learning biases, showing their roles in enhancing model accuracy and generalization. Additionally, we explore the impact of PIML on image reconstruction, segmentation, enhancement, and anomaly detection, demonstrating its effectiveness in addressing noise, resolution, and diagnostic accuracy challenges. Despite its advantages, PIML faces challenges in the accurate representation of complex physiological processes, computational efficiency, and the integration of physics-based priors across diverse applications. This review points out future research directions including the development of hybrid models that combine PIML with deep learning techniques and large foundation models, improved benchmark datasets, and scalable algorithms for real-time applications. The findings of this review highlight PIML as a pivotal approach for advancing medical imaging, bridging the gap between theoretical models and practical implementation in clinical settings.},
  archive      = {J_AIR},
  author       = {Ahmadi, Mohsen and Biswas, Debojit and Lin, Maohua and Vrionis, Frank D. and Hashemi, Javad and Tang, Yufei},
  doi          = {10.1007/s10462-025-11303-w},
  journal      = {Artificial Intelligence Review},
  month        = {10},
  number       = {10},
  pages        = {1-49},
  shortjournal = {Artif. Intell. Rev.},
  title        = {Physics-informed machine learning for advancing computational medical imaging: Integrating data-driven approaches with fundamental physical principles},
  volume       = {58},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Optimizing deep learning with improved harris hawks optimization for alzheimer’s disease detection. <em>AIR</em>, <em>58</em>(10), 1-59. (<a href='https://doi.org/10.1007/s10462-025-11304-9'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As the global population ages, Alzheimer’s disease (AD) poses a significant worldwide challenge as a leading cause of dementia, with a slow early progression that eventually leads to nerve cell death and currently lacks effective treatment. However, early diagnosis can slow its progression through pharmaceutical intervention, making accurate early diagnosis using computer-aided diagnosis (CAD) systems crucial. This study aims to enhance the accuracy of early AD diagnosis by developing an improved optimization approach for deep learning-based CAD systems. To achieve this, this paper proposes an improved Harris Hawks optimization algorithm (HHO), named CAHHO, which incorporates crisscross search and adaptive β-Hill climbing mechanisms, thereby enhancing population diversity and search space coverage during the exploration phase, while adaptively adjusting the step size during the exploitation phase to improve local search precision. Comparative experiments with classical algorithms, HHO variants, and advanced optimization methods validate the superiority of the proposed CAHHO. Specifically, this study employs the deep learning model residual network with 18 layers (ResNet18) as the base model for AD diagnosis and uses CAHHO to optimize key hyperparameters, including the number of channels and learning rate. Experiments on the AD neuroimaging initiative dataset demonstrate that the ResNet18-CAHHO model outperforms existing methods in classifying AD, mild cognitive impairment (MCI), and normal control (NC) subjects. Specifically, it achieves accuracies of 0.93077, 0.80102, and 0.80513 in the diagnosis of AD versus NC, MCI versus NC, and AD versus MCI, respectively. Furthermore, Gradient-Weighted Class Activation Mapping (Grad-CAM) visualizations reveal critical brain regions associated with AD, providing valuable diagnostic support for clinicians and holding significant promise for early intervention.},
  archive      = {J_AIR},
  author       = {Zhang, Qian and Sheng, Jinhua and Zhang, Qiao and Yang, Ze and Xin, Yu and Wang, Binbing and Zhang, Rong},
  doi          = {10.1007/s10462-025-11304-9},
  journal      = {Artificial Intelligence Review},
  month        = {10},
  number       = {10},
  pages        = {1-59},
  shortjournal = {Artif. Intell. Rev.},
  title        = {Optimizing deep learning with improved harris hawks optimization for alzheimer’s disease detection},
  volume       = {58},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Novel coefficients for improved robustness in multi-criteria decision analysis. <em>AIR</em>, <em>58</em>(10), 1-41. (<a href='https://doi.org/10.1007/s10462-025-11307-6'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In multi-criteria decision-making (MCDM), decision-makers face increasing complexity and the need for enhanced tools to facilitate informed and well-aligned decision outcomes. A critical challenge in MCDM is the determination of criteria weights, which significantly influence the final ranking of alternatives. While recent approaches aim to eliminate the need for explicit weight assignment, certain decision contexts necessitate their inclusion. This study introduces two novel coefficients, Rank Stability (RS) and Balance Point (BP), designed to provide deeper insights into the decision problem and its solution properties. Rank Stability quantifies the robustness of a solution against perturbations, while Balance Point evaluates the conditioning of the solution within the problem’s structure. The decision problem is defined by a set of alternatives and criteria, where modifications to alternatives require a reassessment of the decision model. To examine the properties of these coefficients, this study employs simulation experiments utilizing the Technique for Order of Preference by Similarity to Ideal Solution (TOPSIS) and VIseKriterijumska Optimizacija I Kompromisno Resenje (VIKOR) methods, alongside case-based analyses demonstrating their practical applications. Additionally, extreme cases of RS and BP values are explored to enhance interpretability for decision-makers. A real-world decision problem is further analyzed to illustrate the applicability of these coefficients and introduce a novel framework for comparing MCDM methodologies. This approach facilitates a more systematic and comprehensive assessment of MCDM methods, contributing to the advancement of decision-support tools.},
  archive      = {J_AIR},
  author       = {Paradowski, Bartosz and Wątróbski, Jarosław and Sałabun, Wojciech},
  doi          = {10.1007/s10462-025-11307-6},
  journal      = {Artificial Intelligence Review},
  month        = {10},
  number       = {10},
  pages        = {1-41},
  shortjournal = {Artif. Intell. Rev.},
  title        = {Novel coefficients for improved robustness in multi-criteria decision analysis},
  volume       = {58},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Advances in liver, liver lesion, hepatic vasculature, and biliary segmentation: A comprehensive review of traditional and deep learning approaches. <em>AIR</em>, <em>58</em>(10), 1-95. (<a href='https://doi.org/10.1007/s10462-025-11310-x'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Liver segmentation plays a critical role in medical imaging, aiding in diagnosis, treatment planning, and surgical interventions for liver diseases. Precise segmentation of liver structures, including vessels, tumors, and other substructures, is essential for effective patient management. Traditional manual methods are time-consuming and prone to variability, prompting the development of automated techniques. This review aims to evaluate the evolution of liver segmentation methodologies, focusing on recent advancements in deep learning and hybrid approaches. This review follows the PRISMA guidelines for systematic analysis, including a detailed database search across PubMed, Web of Science, Scopus, and IEEE Xplore. The search focused on segmentation techniques for various liver structures using deep learning, traditional methods, and hybrid models. A total of 7819 studies were initially identified, with 190 selected for detailed analysis based on inclusion criteria like Dice Similarity Coefficient (DSC) metrics and clinical applicability. The analysis identified deep learning models, such as U-Net variants and Swin Transformer-based architectures, as leading methods for liver parenchyma and tumor segmentation, achieving DSC values up to 98.9% on benchmark datasets. For vessel segmentation, methods like DeepLabV3+ and the feature-based approaches demonstrated robustness across different datasets. Despite progress, challenges remain in segmenting structures like biliary ducts and hematomas due to limited annotated data and imaging variability. While deep learning has significantly improved segmentation accuracy, challenges such as class imbalance and variability across imaging modalities persist. Hybrid approaches that combine traditional image processing with advanced neural networks show potential for further improvement. Future research should focus on enhancing generalizability through multi-modal data integration and exploring semi-supervised learning methods to overcome data scarcity. This comprehensive review highlights the advancements and ongoing challenges in liver segmentation, emphasizing the need for continuous innovation. By addressing current limitations, future methodologies can improve accuracy, efficiency, and clinical relevance, ultimately enhancing patient outcomes in hepatology.},
  archive      = {J_AIR},
  author       = {Sethia, Khyati and Strakos, Petr and Jaros, Milan and Kubicek, Jan and Roman, Jan and Penhaker, Marek and Riha, Lubomir},
  doi          = {10.1007/s10462-025-11310-x},
  journal      = {Artificial Intelligence Review},
  month        = {10},
  number       = {10},
  pages        = {1-95},
  shortjournal = {Artif. Intell. Rev.},
  title        = {Advances in liver, liver lesion, hepatic vasculature, and biliary segmentation: A comprehensive review of traditional and deep learning approaches},
  volume       = {58},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Fractional order dung beetle optimizer with reduction factor for global optimization and industrial engineering optimization problems. <em>AIR</em>, <em>58</em>(10), 1-90. (<a href='https://doi.org/10.1007/s10462-025-11239-1'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Dung beetle optimizer (DBO) is a novel meta-heuristic algorithm inspired by the behaviors of dung beetles in nature, including ball rolling, dancing, foraging, stealing, and breeding. However, the standard DBO has weaknesses in global optimization, including the imbalance between the ability of exploration and exploitation, low accuracy in function solution, and susceptibility to falling into local optimum. To overcome the weaknesses of DBO, the fractional order dung beetle optimizer with reduction factor (FORDBO) is proposed. Firstly, the good nodes set sequence is employed to replace the randomly initialized population in the algorithm, aiming to enhance the diversity of the population. To enhance the global optimization performance of the algorithm, a reduction factor is designed to balance between the ability of exploration and exploitation. On the other hand, the fractional order calculus strategy is employed to adjust the dynamic boundary of the optimization region. The strategy enables the algorithm to focus on exploiting the potential optimization region. Finally, the repetitive renewal mechanism of the pathfinder dung beetle is proposed to enhance the ability of the algorithm to escape the local optimum. To evaluate the performance of FORDBO, on the one hand, we analyze the complexity of FORDBO and prove its convergence mathematically in this work. On the other hand, this work also compares the FORDBO with 23 similar swarm intelligence technologies through CEC2005, CEC2017, and CEC2022 benchmark functions for global optimization. At the same time, the FORDBO is applied to six industrial engineering optimization problems. The experimental numerical results show that the performance of FORDBO is better than other most swarm intelligence technologies. The source code of FORDBO is publicly available at https://github.com/Huangzhi-Xia/FORDBO .},
  archive      = {J_AIR},
  author       = {Xia, Huangzhi and Ke, Yifen and Liao, Riwei and Zhang, Huai},
  doi          = {10.1007/s10462-025-11239-1},
  journal      = {Artificial Intelligence Review},
  month        = {10},
  number       = {10},
  pages        = {1-90},
  shortjournal = {Artif. Intell. Rev.},
  title        = {Fractional order dung beetle optimizer with reduction factor for global optimization and industrial engineering optimization problems},
  volume       = {58},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A comprehensive review of AI methods in upper extremity/limb bone fracture detection. <em>AIR</em>, <em>58</em>(10), 1-94. (<a href='https://doi.org/10.1007/s10462-025-11296-6'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Accurate detection of bone fractures is crucial for patient care, however, the traditional manual review of medical images like X-rays, Computed Tomography (CT) scans, Magnetic Resonance Imaging (MRIs), and ultrasounds is time-consuming and labor-intensive. The shortage of clinicians, limited access to expert radiologists, and heavy workloads increase the risk of errors, which can slow down patients recovery. Artificial Intelligence (AI) models like Faster R-CNN have shown significant diagnostic accuracy (ACC) and sensitivity (SEN), often outperforming on-call radiologists in detecting complex fracture types. For example, Faster R-CNN has achieved SEN exceeding 90% in distal radius fracture detection. However, despite these advancements, AI-driven fracture detection systems still face several challenges, including the need for extensive annotated datasets, variability in imaging quality across clinical settings, potential biases in model training, and concerns regarding the interpretability and reliability of AI-generated predictions. This review provides a comprehensive analysis of recent advancements and limitations in AI-based fracture detection, offering quantitative insights into model performance. By examining these aspects, the study highlights the importance of integrating AI systems into clinical workflows, while addressing existing barriers to their widespread adoption. This analysis underscores AI’s potential to enhance diagnostic efficiency, reduce human error, and improve patient outcomes.},
  archive      = {J_AIR},
  author       = {Pour, Zahra Moradi and Berretti, Stefano},
  doi          = {10.1007/s10462-025-11296-6},
  journal      = {Artificial Intelligence Review},
  month        = {10},
  number       = {10},
  pages        = {1-94},
  shortjournal = {Artif. Intell. Rev.},
  title        = {A comprehensive review of AI methods in upper extremity/limb bone fracture detection},
  volume       = {58},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Exploring the frontiers of LLMs in psychological applications: A comprehensive review. <em>AIR</em>, <em>58</em>(10), 1-35. (<a href='https://doi.org/10.1007/s10462-025-11297-5'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This review explores the frontiers of large language models (LLMs) in psychological applications. Psychology has undergone several theoretical changes, and the current use of artificial intelligence (AI) and machine learning, particularly LLMs, promises to open up new research directions. We aim to provide a detailed exploration of how LLMs are transforming psychological research. We discuss the impact of LLMs across various branches of psychology—including cognitive and behavioral, clinical and counseling, educational and developmental, and social and cultural psychology—highlighting their ability to model patterns, cognition, and behavior similar to those observed in humans. Furthermore, we explore the ability of such models to generate coherent, contextually relevant text, offering innovative tools for literature reviews, hypothesis generation, experimental designs, experimental subjects, and data analysis in psychology. We emphasize the importance of addressing technical and ethical challenges, including data privacy, the ethics of using LLMs in psychological research, and the need for a deeper understanding of these models’ limitations. Researchers should use LLMs responsibly in psychological studies, adhering to ethical standards and considering the potential consequences of deploying these technologies in sensitive areas. Overall, this review provides a comprehensive overview of the current state of LLMs in psychology, exploring the potential benefits and challenges. We hope it can serve as a call to action for researchers to responsibly leverage LLMs’ advantages while addressing the associated risks.},
  archive      = {J_AIR},
  author       = {Ke, Luoma and Tong, Song and Cheng, Peng and Peng, Kaiping},
  doi          = {10.1007/s10462-025-11297-5},
  journal      = {Artificial Intelligence Review},
  month        = {10},
  number       = {10},
  pages        = {1-35},
  shortjournal = {Artif. Intell. Rev.},
  title        = {Exploring the frontiers of LLMs in psychological applications: A comprehensive review},
  volume       = {58},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Advances in artificial intelligence for olfaction and gustation: A comprehensive review. <em>AIR</em>, <em>58</em>(10), 1-47. (<a href='https://doi.org/10.1007/s10462-025-11309-4'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This review explores the transformative role of artificial intelligence (AI) in enhancing our understanding of olfaction and gustation, two senses that significantly influence human behavior and decision-making. AI methodologies, including machine learning and neural networks, are revolutionizing sensory experiences in industries such as food, fragrance, and healthcare. In the food industry, AI is enhancing flavor profiling, ensuring product safety, and aligning offerings with consumer preferences, all while preserving nutritional value. In fragrance, AI is enabling personalized scent creation, allowing for bespoke products tailored to diverse consumer needs. In healthcare, AI is advancing the diagnosis and treatment of sensory disorders, ultimately improving the quality of life for individuals with sensory impairments. Despite these advancements, challenges persist, such as the need for diverse and representative datasets, the subjective nature of sensory perception, and ethical concerns surrounding privacy. Addressing these issues requires interdisciplinary collaboration across neuroscience, computer science, and food technology. Future research should focus on developing adaptive AI systems that can dynamically interpret and predict sensory preferences, reshaping interactions with food, fragrances, and health products. This review underscores AI’s critical role in driving sensory innovation and lays the groundwork for future studies aimed at enhancing consumer experiences and improving quality of life.},
  archive      = {J_AIR},
  author       = {Hao, Zhihao and Li, Haisheng and Guo, Jianhua and Xu, Yong},
  doi          = {10.1007/s10462-025-11309-4},
  journal      = {Artificial Intelligence Review},
  month        = {10},
  number       = {10},
  pages        = {1-47},
  shortjournal = {Artif. Intell. Rev.},
  title        = {Advances in artificial intelligence for olfaction and gustation: A comprehensive review},
  volume       = {58},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Linear projection fused graph-based semi-supervised learning on multi-view data. <em>AIR</em>, <em>58</em>(10), 1-28. (<a href='https://doi.org/10.1007/s10462-025-11313-8'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, the surge in data-driven applications across various domains has spurred heightened interest in semi-supervised learning applied to graphs. This surge is attributed to the ubiquitous presence of graph data structures in real-world contexts, such as social networks’ interpersonal relationships, recommender systems’ user behavior graphs, and bioinformatics’ molecular interaction networks. However, for certain data types like images, not only is there a dearth of explicit graph structure, but also the existence of multiple view description methods complicates matters further. The intricacies of multi-view data pose challenges in directly applying traditional semi-supervised learning techniques to graphs. Consequently, researchers have begun exploring the fusion of semi-supervised learning with deep learning to leverage its wealth of information and enhance model efficacy. Effectively amalgamating graph structures with multi-view data remains a challenging problem necessitating further research. This paper introduces the Linear projection Fused Graph-based Semi-supervised Classification (LFGSC) method tailored for multi-view data, building upon the Graph Convolutional Network (GCN) architecture. Firstly, for each view, we leverage a semi-supervised approach that provides the concurrent estimation of the corresponding graph and the flexible linear data representations in a low-dimensional feature space. Subsequently, an adaptive and unified graph is generated, followed by the utilization of a fully connected network to fuse the projected features further and reduce dimensionality. Finally, the fused features and graph are inputted into a GCN to conduct semi-supervised classification. During training, the model incorporates cross-entropy loss, manifold regularization loss, graph auto-encoder loss, and supervised contrastive loss. Leveraging linear transformation significantly diminishes the input feature dimensions for GCN, thereby achieving high accuracy while substantially reducing computational overhead. Furthermore, experimental results conducted on various bench-marked multi-view image datasets demonstrate the superiority of LFGSC over existing semi-supervised learning methods for multi-view scenarios. (Source code: https://github.com/BiJingjun/LFGSC. )},
  archive      = {J_AIR},
  author       = {Bi, Jingjun and Dornaika, Fadi and Charafeddine, Jinan},
  doi          = {10.1007/s10462-025-11313-8},
  journal      = {Artificial Intelligence Review},
  month        = {10},
  number       = {10},
  pages        = {1-28},
  shortjournal = {Artif. Intell. Rev.},
  title        = {Linear projection fused graph-based semi-supervised learning on multi-view data},
  volume       = {58},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Benchmarking machine learning methods for the identification of mislabeled data. <em>AIR</em>, <em>58</em>(10), 1-31. (<a href='https://doi.org/10.1007/s10462-025-11293-9'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Supervised machine learning recently gained growing importance in various fields of research. To train reliable models, data scientists need credible data, which is not always available. A particularly hard and widespread problem deteriorating the performance of methods are mislabeled samples (Northcutt in J Artif Intell Res 70:1373-1411, 2021). Common sources of mislabeling are weakly defined classes, labels that change their meaning, unsuitable annotators, or ambiguous guidelines for labeling. Because mislabeling lowers prediction quality, it is essential for scientists to be able to identify wrong labels before actually starting the learning process. For that, numerous algorithms for the identification of noisy instances have been developed. However, so far, a comprehensive empirical comparison of available methods has been missing. In this paper, we survey and benchmark methods for the identification of mislabeled samples in tabular data. We discuss the theoretical background of label noise and how it can lead to mislabeling, review categorizations of identification methods, and briefly introduce 34 specific approaches together with popular data sets. Finally, 20 selected methods are benchmarked using artificially blurred data with controllable mislabeling and a new real-life genomic dataset with known errors. We compare methods varying the amount and the type of noise, as well as the sample size and domain of data. We find that most of the methods have the highest performance on datasets with a noise level of around 20-30% where the best filters identify around 80% of the noisy instances with relatively high precision (0.58 $$-$$ 0.65). Acquiring precise predictions seems to be a more challenging task than identifying most of the noisy instances: while the average recall score over all models ranges from 0.48 to 0.77, the average precision score ranges from 0.16 to 0.55. Furthermore, none of the methods excels over all others in isolation, while ensemble-based methods often outperform individual models. We provide all data sets and analysis code to enable a better handling of mislabeled data and give recommendations on usage of noise filters depending on various dataset parameters.},
  archive      = {J_AIR},
  author       = {Nazaretyan, Lusine and Leser, Ulf and Kircher, Martin},
  doi          = {10.1007/s10462-025-11293-9},
  journal      = {Artificial Intelligence Review},
  month        = {10},
  number       = {10},
  pages        = {1-31},
  shortjournal = {Artif. Intell. Rev.},
  title        = {Benchmarking machine learning methods for the identification of mislabeled data},
  volume       = {58},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A hybrid control approach to improve power quality in microgrid systems. <em>AIR</em>, <em>58</em>(10), 1-39. (<a href='https://doi.org/10.1007/s10462-025-11300-z'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Power quality (PQ) in distributed energy resources (DERs) is paramount for maintaining a stable and efficient electricity supply. The consistency and cleanliness of power are integral to ensuring reliability, sustainability, and optimal performance, thereby supporting a resilient and eco-friendly energy infrastructure. This paper introduces a hybrid control method designed to address two significant challenges in microgrid (MG) applications: active resonance damping (ARD) and unbalanced voltage compensation (UVC). Furthermore, the proposed hybrid method combines effective ARD with UVC at MG terminals. The active damping technique employs an external control level to counteract undesirable resonant harmonics, overcoming control bandwidth limitations. This approach offers simplicity in setup and performance without requiring additional system parameter adjustments. For UVC, the suggested control technique estimates the compensation reference using the dual d-q control, reducing the complexity and cost associated with load current measurement issues. The hybrid method integrates the resonant damping signal and the MG negative sequence reference (NSR) voltage, which are fed into a two-level sine-pulse width modulation block (SPWM) to control the MG converter. Simulation results validate the robustness of the proposed combined method in simultaneously compensating for unbalanced voltage and active resonance.},
  archive      = {J_AIR},
  author       = {Khosravi, Nima},
  doi          = {10.1007/s10462-025-11300-z},
  journal      = {Artificial Intelligence Review},
  month        = {10},
  number       = {10},
  pages        = {1-39},
  shortjournal = {Artif. Intell. Rev.},
  title        = {A hybrid control approach to improve power quality in microgrid systems},
  volume       = {58},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A recent advances on autism spectrum disorders in diagnosing based on machine learning and deep learning. <em>AIR</em>, <em>58</em>(10), 1-92. (<a href='https://doi.org/10.1007/s10462-025-11302-x'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Neurological disorders affect communication ability, social interaction, and a person’s conduct. Early diagnosis and treatment of ASD during the early stages of a person’s life may result in better outcomes and a higher quality of life for patients. Current methods of diagnosis are based on behavioral observations and interviews, which are subjective, time-consuming, and costly. EEG does not include invasive techniques, and it is a safe and painless way of measuring electrical activity in the brain. EEG signals may reflect neural differences and abnormalities related to ASD and serve as a potential biomarker for diagnosis. Due to the increase in prevalence, there has been an increased need to develop more sensitive and unbiased diagnostic methods for ASD. ML and DL are two sophisticated methods that researchers developed for detecting ASD by doing neural network analyses. The review paper incorporates the analysis of previous studies; more than 200 works have been analyzed from top publishers like Elsevier, IEEE, MDPI, and Springer, specifically related to EEG signal analysis and feature extraction techniques. It considers significant methods for ASD detection, including SVMs, CNN, and other models like KNN, ResNet50, and ANFIS. Other datasets central in these studies are KAU, BCIAUT-P300, and ADOS-2. The performance metrics adopted in this research include accuracy, sensitivity, and specificity. For example, the cubic SVM realized an accuracy of 95.8%, while the CNN models reached 95%. Other models, like ResNet50, achieved 99.39%, while ANFIS reached 98.9%. Sensitivity and specificity also showed varying scores across the methods, between 85 and 100%, indicating the high potential of these approaches in ASD diagnosis. Future studies could pay more attention to dataset representativeness improvements and do the clinical validation of these models for better generalization and relevance toward early diagnosis in ASD.},
  archive      = {J_AIR},
  author       = {Hatim, Hajir Ammar and Alyasseri, Zaid Abdi Alkareem and Jamil, Norziana},
  doi          = {10.1007/s10462-025-11302-x},
  journal      = {Artificial Intelligence Review},
  month        = {10},
  number       = {10},
  pages        = {1-92},
  shortjournal = {Artif. Intell. Rev.},
  title        = {A recent advances on autism spectrum disorders in diagnosing based on machine learning and deep learning},
  volume       = {58},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Revolutionizing scholarly impact: Advanced evaluations, predictive models, and future directions. <em>AIR</em>, <em>58</em>(10), 1-27. (<a href='https://doi.org/10.1007/s10462-025-11315-6'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Artificial intelligence (AI) is revolutionising scholarly impact evaluation and prediction. By integrating AI and machine learning techniques, researchers can leverage diverse academic networks and multiple sources of academic big data. This integration transforms traditional evaluation methods that rely on structured measurements such as citation counts and journal impact factors, into more comprehensive and objective evaluations. In this paper, we dive deep into latest advancements in scholarly impact evaluation and prediction within the context of AI. We categorize existing models, highlighting their similarities and distinctions, with a particular emphasis on AI-enabled approaches. Building upon the analysis, we discuss the ongoing challenges in scholarly impact research and outline future directions in this field.},
  archive      = {J_AIR},
  author       = {Bai, Xiaomei and Zhang, Fuli and Liu, Jiaying and Wang, Xiaoxia and Xia, Feng},
  doi          = {10.1007/s10462-025-11315-6},
  journal      = {Artificial Intelligence Review},
  month        = {10},
  number       = {10},
  pages        = {1-27},
  shortjournal = {Artif. Intell. Rev.},
  title        = {Revolutionizing scholarly impact: Advanced evaluations, predictive models, and future directions},
  volume       = {58},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A survey of NLP methods for oncology in the past decade with a focus on cancer registry applications. <em>AIR</em>, <em>58</em>(10), 1-24. (<a href='https://doi.org/10.1007/s10462-025-11316-5'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Clinical texts from pathology and radiology reports provide critical information for cancer diagnosis and staging. This study surveys the application of natural language processing (NLP) in cancer registry operations from 2014 to 2024. A total of 156 articles from Scopus and PubMed were reviewed and were categorized by NLP methods, document types, cancer sites, and research aims. NLP approaches were evenly distributed across rule-based (n=70), machine learning (n=66), and traditional deep learning (n=70), with transformer models (n=29) gaining prominence since 2019. Encoder-only models like BERT and its clinical adaptations (e.g., ClinicalBERT, RadBERT) show significant promise, though methods for increasing context length are needed. Decoder-only models (e.g., GPT-3, GPT-4) are less explored due to privacy concerns and computational demands. Notably, pediatric cancers, melanomas, and lymphomas are underrepresented, as are research areas such as disease progression, clinical trial matching, and patient communication. Multi-modal models, important for precision oncology and cancer screening, are also scarce. Our study highlights the potential of NLP to enhance data abstraction efficiency and accuracy in cancer registries, making greater use of cancer registry data for patient benefit. However, further research is needed to fully leverage transformer-based models, particularly for underrepresented cancer types and outcomes. Addressing these gaps can improve the timeliness, completeness, and accuracy of structured data collection from clinical text, ultimately enhancing cancer research and patient outcomes.},
  archive      = {J_AIR},
  author       = {Hands, Isaac and Kavuluru, Ramakanth},
  doi          = {10.1007/s10462-025-11316-5},
  journal      = {Artificial Intelligence Review},
  month        = {10},
  number       = {10},
  pages        = {1-24},
  shortjournal = {Artif. Intell. Rev.},
  title        = {A survey of NLP methods for oncology in the past decade with a focus on cancer registry applications},
  volume       = {58},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A review of named entity recognition: From learning methods to modelling paradigms and tasks. <em>AIR</em>, <em>58</em>(10), 1-87. (<a href='https://doi.org/10.1007/s10462-025-11321-8'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Named Entity Recognition (NER) is commonly used when summarising news articles and legal documents. It can extract the names of politicians or organisations and help determine the aspect of a positive or negative sentiment. Previous surveys have only provided a shallow review of NER with respect to a certain datatype. In contrast, here a much deeper coverage of different approaches is provided. First articles with respect to the learning method are discussed, such as supervised or unsupervised. Next, popular models that combine two or more learning methods are introduced in a bottom-up approach. The most popular NER algorithms are compared on a recently crawled 2024 election dataset from Australia. The effect of different parameters such as number of epochs and learning rate is explored. It is concluded that pre-trained NER models are limited in their ability to model new entities and disambiguate their context. Using the sentiment score together with a state space model over entities in a sentence might help overcome these challenges.},
  archive      = {J_AIR},
  author       = {Seow, Wei Liang and Chaturvedi, Iti and Hogarth, Amber and Mao, Rui and Cambria, Erik},
  doi          = {10.1007/s10462-025-11321-8},
  journal      = {Artificial Intelligence Review},
  month        = {10},
  number       = {10},
  pages        = {1-87},
  shortjournal = {Artif. Intell. Rev.},
  title        = {A review of named entity recognition: From learning methods to modelling paradigms and tasks},
  volume       = {58},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Artificial intelligence in environmental and earth system sciences: Explainability and trustworthiness. <em>AIR</em>, <em>58</em>(10), 1-23. (<a href='https://doi.org/10.1007/s10462-025-11165-2'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Explainable artificial intelligence (XAI) methods have recently emerged to gain insights into complex machine learning models. XAI can be promising for environmental and Earth system science because high-stakes decision-making for management and planning requires justification based on evidence and systems understanding. However, an overview of XAI applications and trust in AI in environmental and Earth system science is still missing. To close this gap, we reviewed 575 articles. XAI applications are popular in various domains, including ecology, engineering, geology, remote sensing, water resources, meteorology, atmospheric sciences, geochemistry, and geophysics. XAI applications focused primarily on understanding and predicting anthropogenic changes in geospatial patterns and impacts on human society and natural resources, especially biological species distributions, vegetation, air quality, transportation, and climate-water related topics, including risk and management. Among XAI methods, the SHAP and Shapley methods were the most popular (135 articles), followed by feature importance (27), partial dependence plots (22), LIME (21), and saliency maps (15). Although XAI methods are often argued to increase trust in model predictions, only seven studies (1.2%) addressed trustworthiness as a core research objective. This gap is critical because understanding the relationship between explainability and trust is lacking. While XAI applications continue to grow, they do not necessarily enhance trust. Hence, more studies on how to strengthen trust in AI applications are critically needed. Finally, this review underlines the recommendation of developing a “human-centered” XAI framework that incorporates the distinct views and needs of multiple stakeholder groups to enable trustworthy decision-making.},
  archive      = {J_AIR},
  author       = {Schiller, Josepha and Stiller, Stefan and Ryo, Masahiro},
  doi          = {10.1007/s10462-025-11165-2},
  journal      = {Artificial Intelligence Review},
  month        = {10},
  number       = {10},
  pages        = {1-23},
  shortjournal = {Artif. Intell. Rev.},
  title        = {Artificial intelligence in environmental and earth system sciences: Explainability and trustworthiness},
  volume       = {58},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Plant leaf disease detection and classification using convolution neural networks model: A review. <em>AIR</em>, <em>58</em>(10), 1-66. (<a href='https://doi.org/10.1007/s10462-025-11234-6'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Plants play a vital role in providing food on a global scale. Several environmental factors contribute to the occurrence of plant leaf diseases, leading to substantial reductions in crop yields. Nevertheless, the process of manually detecting plant leaf diseases is both time-consuming and prone to errors. Adopting deep learning technologies can address these challenges, and the efficacy of deep learning techniques in precision agriculture has been explored over the past decades. However, despite these applications, several gaps in plant leaf disease research still need to be addressed for efficient disease control. This paper, therefore, provides an in-depth review of the trends in using convolutional neural networks for leaf disease detection and classification. In addition, we also present the existing plant leaf disease datasets. It was found that convolutional neural network models, such as VGG, EfficientNet, GoogleNet, and ResNet, provide the highest accuracy in classifying plant leaf disease images. This review will provide valuable information for scholars who are seeking effective deep learning-based classifiers for plant leaf disease detection and classification.},
  archive      = {J_AIR},
  author       = {Salka, Tanko Daniel and Hanafi, Marsyita Binti and Rahman, Sharifah M. Syed Ahmad Abdul and Zulperi, Dzarifah Binti Mohamed and Omar, Zaid},
  doi          = {10.1007/s10462-025-11234-6},
  journal      = {Artificial Intelligence Review},
  month        = {10},
  number       = {10},
  pages        = {1-66},
  shortjournal = {Artif. Intell. Rev.},
  title        = {Plant leaf disease detection and classification using convolution neural networks model: A review},
  volume       = {58},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Informed machine learning to reconcile interpretability with fidelity in scientific applications. <em>AIR</em>, <em>58</em>(10), 1-38. (<a href='https://doi.org/10.1007/s10462-025-11282-y'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Notwithstanding their impressive performances, unfortunately some of the most powerful machine learning (ML) models are obscure and almost impossible to interpret. Consequently, in the last years, there has been a rapid increase in research about eXplainable Artificial Intelligence, whose objective consists of improving their transparency. In scientific applications, explainability assumes a different flavour and cannot be reduced to pure user understanding but there is a premium also on fidelity, on developing models that reflect the actual mechanisms at play in the investigated phenomena. To this end, Genetic Programming supported Symbolic Regression (GPSR), conceived explicitly to manipulate symbols, can present various competitive advantages in finding a good trade-off between interpretability and realism. However, the search spaces are typically too large and the algorithms have to be steered to converge on the desired solutions. The present work describes techniques to constrain GPSR and to combine it with deep learning tools, so that the final models are expressed in terms of interpretable and realistic mathematical equations. The strategies to guide convergence include dimensional analysis, integration of prior information about symmetries and conservation laws, refinements of the fitness function and robust statistics. The performances are improved according to all the main metrics: accuracy, robustness against noise and outliers, capability of handling data sparsity and interpretability. Great attention has been paid to introducing practical solutions, covering most essential aspects of the data analysis process, from the treatment of the uncertainties to the quantification of the equations’ complexity. All the main applications of supervised ML, from regression to classification, are considered (and the extension to unsupervised and reinforcement learning are not expected to pose major difficulties). Theoretical considerations, systematic numerical tests, simulations with multiphysics codes and the results of actual experiments prove the potential of the proposed improvements.},
  archive      = {J_AIR},
  author       = {Murari, Andrea and Rossi, Riccardo and Spolladore, Luca and Wyss, Ivan and Gelfusa, Michela},
  doi          = {10.1007/s10462-025-11282-y},
  journal      = {Artificial Intelligence Review},
  month        = {10},
  number       = {10},
  pages        = {1-38},
  shortjournal = {Artif. Intell. Rev.},
  title        = {Informed machine learning to reconcile interpretability with fidelity in scientific applications},
  volume       = {58},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Policy weighting via discounted thomson sampling for non-stationary market-making. <em>AIR</em>, <em>58</em>(10), 1-30. (<a href='https://doi.org/10.1007/s10462-025-11312-9'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Market-making is an essential activity in every financial market. They provide liquidity to the system by placing buy and sell orders at multiple price levels. While performing this task, they aim to earn profit and manage inventory levels simultaneously. However, financial markets are not stationary environments; they constantly evolve, influenced by changes in participants, the occurrence of economic events, or the market trading hours, among others. This study introduces a novel approach to address the challenge of market-making in non-stationary financial markets with multi-objective Reinforcement Learning (RL). Traditional RL methods often struggle when applied to non-stationary environments, as the learned optimal policy may not be adapted to the new dynamics. We present Policy Weighting through Discounted Thompson Sampling (POW-dTS), a novel dynamic algorithm that adapts to changing market conditions by effectively weighting pre-trained policies across various contexts. Unlike some conventional methods, POW-dTS does not require additional artifacts such as change-point detection or models of transitions, making it robust against the unpredictability inherent in financial markets. Our approach focuses on optimizing trade profitability and managing inventory risk, the dual objectives of market makers. Through a detailed comparative analysis, we highlight the strengths and adaptability of POW-dTS against traditional techniques in non-stationary environments, demonstrating its potential to enhance market liquidity and efficiency.},
  archive      = {J_AIR},
  author       = {Fernández Vicente, Óscar and García, Javier and Fernández, Fernando},
  doi          = {10.1007/s10462-025-11312-9},
  journal      = {Artificial Intelligence Review},
  month        = {10},
  number       = {10},
  pages        = {1-30},
  shortjournal = {Artif. Intell. Rev.},
  title        = {Policy weighting via discounted thomson sampling for non-stationary market-making},
  volume       = {58},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A comprehensive systematic literature review on artificial intelligence for error correction and modulation schemes in next-generation satellite communications. <em>AIR</em>, <em>58</em>(10), 1-67. (<a href='https://doi.org/10.1007/s10462-025-11317-4'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Communication systems continue to embrace the potential of Artificial Intelligence (AI) in error correction codes (ECC) with coded modulation schemes (CMS). Despite this, there remains a substantial performance gap in AI methods in terrestrial and satellite communication systems. Additionally, AI and power efficiency for Low Earth Orbit (LEO) satellites have shown a critical gap. To the best of the author’s knowledge, this is the first Systematic literature review attempting to bridge this vital gap to boost efficiency and add fault tolerance. From 389 articles published between 1993 and 2023, the construction and performance of 33 AI algorithms have been comprehensively reviewed for 16 ECC, seven higher-order CMS, and LEO satellites. Based on four key parameters: error correction, modulation, power, and energy efficiency, the PRISMA strategy with a 27-item checklist was adopted and 63 studies were selected to investigate the AI-based performance of terrestrial (40-studies) and LEO satellites (23-studies). Analysing nine performance metrics, Convolutional Neural Network was the most popular choice (20.6%) with an accuracy of 99% and SNR from 6-20dB, followed by Deep Neural Network (19.04%). The least used algorithm was Reinforcement learning (9.52%). Modified Reed Solomon codes showed the best measurement of power consumption and error rate. Adaptive LDPC codes provided a 45% increase in energy efficiency with an 11% computation decrease. Considering appropriate merits and challenges, the review identifies, discusses, and synthesises AI results to create a summary of current evidence for terrestrial and LEO satellites contributing to evidence-based practice for future researchers.},
  archive      = {J_AIR},
  author       = {Sharma, Ekta and Davey, Christopher P. and Deo, Ravinesh C. and Carter, Brad D. and Salcedo-Sanz, Sancho},
  doi          = {10.1007/s10462-025-11317-4},
  journal      = {Artificial Intelligence Review},
  month        = {10},
  number       = {10},
  pages        = {1-67},
  shortjournal = {Artif. Intell. Rev.},
  title        = {A comprehensive systematic literature review on artificial intelligence for error correction and modulation schemes in next-generation satellite communications},
  volume       = {58},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Physics-informed neural networks for PDE problems: A comprehensive review. <em>AIR</em>, <em>58</em>(10), 1-43. (<a href='https://doi.org/10.1007/s10462-025-11322-7'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As AI for Science continues to grow, Physics-informed neural networks (PINNs) have emerged as a transformative approach within the realm of scientific computing and deep learning, offering a robust and flexible framework for solving partial differential equations (PDEs) and other complex physical systems. By embedding physical laws directly into the architecture of neural networks, PINNs enable the integration of domain-specific knowledge, ensuring that the models adhere to known physics while fitting available data. In this paper, we provide a comprehensive overview of the state-of-the-art advancements and applications of PINNs across a broad spectrum of PDE problems. In particular, focus is given on the PINN architectures, data resampling methods for PINN, loss and activation functions, feature embedding methods and so on. What’s more, the potential future directions and the anticipated evolution of PINNs are also discussed. We aim to provide valuable insights into PINNs for PDE problems, with hope to encourage further exploration and research in this promising area.},
  archive      = {J_AIR},
  author       = {Luo, Kuang and Zhao, Jingshang and Wang, Yingping and Li, Jiayao and Wen, Junjie and Liang, Jiong and Soekmadji, Henry and Liao, Shaolin},
  doi          = {10.1007/s10462-025-11322-7},
  journal      = {Artificial Intelligence Review},
  month        = {10},
  number       = {10},
  pages        = {1-43},
  shortjournal = {Artif. Intell. Rev.},
  title        = {Physics-informed neural networks for PDE problems: A comprehensive review},
  volume       = {58},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Discriminative projective dictionary pair based broad metric learning system: Algorithm and its applications in pattern classification. <em>AIR</em>, <em>58</em>(10), 1-35. (<a href='https://doi.org/10.1007/s10462-025-11324-5'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Pattern classification plays a pivotal role in a wide range of domains, including computer vision and healthcare. The Broad Learning System (BLS) has attracted considerable attention for its competitive classification performance and computational efficiency. However, its reliance on randomly initialized parameters and lack of iterative updates often lead to performance instability. Directly applying backpropagation to refine these parameters may further result in overfitting. To address these limitations, this research propose a novel framework called the Discriminative Projective Dictionary Pair-based Broad Metric Learning System (D-BMLS). The foundation of this system is the Broad Metric Learning System (BMLS), which integrates a metric subsystem that employs iterative learning to reduce sensitivity to random initialization while leveraging the structural advantages of metric learning to suppress overfitting. Although this improves robustness, it can also introduce computational overhead and still struggle with nonlinear data modeling due to the dual-mapping structure of BLS. To overcome these challenges, D-BMLS incorporates Discriminative Projective Dictionary Pair Learning, which encodes input data into a low-dimensional, linearly separable space. This reduces the number of learnable parameters and enhances the model’s capacity to capture nonlinear relationships through linear transformations. Extensive experiments on five different tasks including image classification, signal recognition, and high-dimensional feature analysis demonstrate the superior performance of D-BMLS. Ablation studies on three benchmark datasets verify the contributions of each component, and results on a synthetic dataset highlight the metric subsystem’s effectiveness in mitigating overfitting.},
  archive      = {J_AIR},
  author       = {Duan, Junwei and Zou, Yutong},
  doi          = {10.1007/s10462-025-11324-5},
  journal      = {Artificial Intelligence Review},
  month        = {10},
  number       = {10},
  pages        = {1-35},
  shortjournal = {Artif. Intell. Rev.},
  title        = {Discriminative projective dictionary pair based broad metric learning system: Algorithm and its applications in pattern classification},
  volume       = {58},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Content moderation by LLM: From accuracy to legitimacy. <em>AIR</em>, <em>58</em>(10), 1-32. (<a href='https://doi.org/10.1007/s10462-025-11328-1'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {One trending application of LLM (large language model) is to use it for content moderation in online platforms. Most current studies on this application have focused on the metric of accuracy—the extent to which LLMs make correct decisions about content. This article argues that accuracy is insufficient and misleading because it fails to grasp the distinction between easy cases and hard cases, as well as the inevitable trade-offs in achieving higher accuracy. Closer examination reveals that content moderation is a constitutive part of platform governance, the key to which is to gain and enhance legitimacy. Instead of making moderation decisions correctly, the chief goal of LLMs is to make them legitimate. In this regard, this article proposes a paradigm shift from the single benchmark of accuracy towards a legitimacy-based framework for evaluating the performance of LLM moderators. The framework suggests that for easy cases, the key is to ensure accuracy, speed, and transparency, while for hard cases, what matters is reasoned justification and user participation. Examined under this framework, LLMs’ real potential in moderation is not accuracy improvement. Rather, LLMs can better contribute in four other aspects: to conduct screening of hard cases from easy cases, to provide quality explanations for moderation decisions, to assist human reviewers in getting more contextual information, and to facilitate user participation in a more interactive way. To realize these contributions, this article proposes a workflow for incorporating LLMs into the content moderation system. Using normative theories from law and social sciences to critically assess the new technological application, this article seeks to redefine LLMs’ role in content moderation and redirect relevant research in this field.},
  archive      = {J_AIR},
  author       = {Huang, Tao},
  doi          = {10.1007/s10462-025-11328-1},
  journal      = {Artificial Intelligence Review},
  month        = {10},
  number       = {10},
  pages        = {1-32},
  shortjournal = {Artif. Intell. Rev.},
  title        = {Content moderation by LLM: From accuracy to legitimacy},
  volume       = {58},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Kirchhoff’s law algorithm (KLA): A novel physics-inspired non-parametric metaheuristic algorithm for optimization problems. <em>AIR</em>, <em>58</em>(10), 1-60. (<a href='https://doi.org/10.1007/s10462-025-11289-5'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This research introduces Kirchhoff’s Law Algorithm (KLA), a novel optimization method inspired by electrical circuit laws, particularly Kirchhoff’s Current Law (KCL). The KLA is evaluated using real-parameter test functions including CEC-2005, 2014, and 2017, comparing its performance with several established algorithms. Results from real-parameter and constrained benchmark functions affirm KLA’s accuracy and convergence rate superiority compared to other algorithms. Notably, when applied to the CEC-2005 benchmarks with dimensions ranging from 30 to 100, KLA demonstrates a remarkable ability to maintain population diversity throughout the search process within a feasible search space. Based on the average rank criteria, KLA consistently outperforms other algorithms despite its simplicity and lack of control parameters (aside from population size). This inherent simplicity makes KLA easy to use as-is, adaptable, and compatible with other optimization techniques. The source codes of the KLA algorithm are publicly available at https://nimakhodadadi.com/algorithms-%2B-codes .},
  archive      = {J_AIR},
  author       = {Ghasemi, Mojtaba and Khodadadi, Nima and Trojovský, Pavel and Li, Li and Mansor, Zulkefli and Abualigah, Laith and Alharbi, Amal H. and El-Kenawy, El-Sayed M.},
  doi          = {10.1007/s10462-025-11289-5},
  journal      = {Artificial Intelligence Review},
  month        = {10},
  number       = {10},
  pages        = {1-60},
  shortjournal = {Artif. Intell. Rev.},
  title        = {Kirchhoff’s law algorithm (KLA): A novel physics-inspired non-parametric metaheuristic algorithm for optimization problems},
  volume       = {58},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). MAAPO: An innovative membrane algorithm based on artificial protozoa optimizer for multilevel threshold image segmentation. <em>AIR</em>, <em>58</em>(10), 1-40. (<a href='https://doi.org/10.1007/s10462-025-11319-2'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper proposes a novel membrane algorithm based on artificial protozoa optimizer (MAAPO) for global optimization problems. The artificial protozoa optimizer (APO) is adopted as the base meta-heuristic algorithm due to its novelty and competitive performance. MAAPO integrates two key innovations: (1) a membrane computing (MC) framework that introduces a parallel distributed paradigm to improve population diversity and search dynamics, and (2) an enhanced autotrophic model within APO that uses a roulette-based fitness-distance balance (RFDB) mechanism for adaptive reference point selection. These strategies collectively enhance the algorithm’s exploration-exploitation balance and global search capabilities. To validate its performance, MAAPO is tested against 12 advanced algorithms on the CEC2017 test suite, and further applied to the multilevel thresholding image segmentation problem using Otsu and Kapur entropy as objective functions. The quality of segmented images is assessed using peak signal-to-noise ratio (PSNR), structural similarity index (SSIM), and feature similarity index (FSIM) metrics. Experimental results demonstrate that MAAPO outperforms its counterparts, delivering superior segmentation quality. This research on MAAPO contributes an effective enhancement strategy to meta-heuristic algorithms and introduces a novel, highly applicable approach for complex image segmentation tasks. The source codes of MAAPO are publicly available at https://ww2.mathworks.cn/matlabcentral/fileexchange/181534-maapo .},
  archive      = {J_AIR},
  author       = {Wang, Xiaopeng and Snášel, Václav and Mirjalili, Seyedali and Pan, Jeng-Shyang},
  doi          = {10.1007/s10462-025-11319-2},
  journal      = {Artificial Intelligence Review},
  month        = {10},
  number       = {10},
  pages        = {1-40},
  shortjournal = {Artif. Intell. Rev.},
  title        = {MAAPO: An innovative membrane algorithm based on artificial protozoa optimizer for multilevel threshold image segmentation},
  volume       = {58},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multigroup cooperative evolutionary optimization algorithm combined with quantum entanglement for cross-field applications. <em>AIR</em>, <em>58</em>(10), 1-32. (<a href='https://doi.org/10.1007/s10462-025-11279-7'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Swarm intelligence algorithms are a class of bionic probabilistic heuristic search methods that are inspired by the collective behaviors of biological agents. In this paper, a multigroup cooperative evolutionary optimization algorithm is proposed by referring to the interaction behaviors of species diversity and stability in the ecosystem. First, the group updating mechanism of the traditional seeking and tracking mode with a dynamic population update mechanism is adopted. The multi-population interactive update group and the quantum entanglement update group are introduced to guide the algorithm to gradually approach the global optimal solution. Second, the proposed bionic algorithm is extended for cross-field applications. The algorithm is applied to solve the function optimization problems, as well as problems in four distinct application fields, including robot routing optimization of grid maps, vehicle scheduling optimization of dairy enterprises, location optimization of logistics centers, and plasma trajectory planning optimization. The proposed multigroup cooperative evolutionary optimization algorithm achieves competitive results in these application fields, thus demonstrating its versatility and robustness.},
  archive      = {J_AIR},
  author       = {Lian, Zhaoyang and Si, Bailu},
  doi          = {10.1007/s10462-025-11279-7},
  journal      = {Artificial Intelligence Review},
  month        = {10},
  number       = {10},
  pages        = {1-32},
  shortjournal = {Artif. Intell. Rev.},
  title        = {Multigroup cooperative evolutionary optimization algorithm combined with quantum entanglement for cross-field applications},
  volume       = {58},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Cuckoo catfish optimizer: A new meta-heuristic optimization algorithm. <em>AIR</em>, <em>58</em>(10), 1-80. (<a href='https://doi.org/10.1007/s10462-025-11291-x'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A new meta-heuristic algorithm, Cuckoo Catfish Optimizer (CCO), is proposed for numerical optimization problems. It simulates the search, predation, and parasitic behavior observed in cichlids. Early iterations of the algorithm focus on executing a multidimensional enveloping search strategy and a compressed space strategy, combined with an auxiliary search strategy to efectively limit the escape space of cichlids. This phase ensures extensive exploration of the solution space. In the intermediate stage of iteration, the algorithm uses a transition strategy to promote a smooth transition from exploration to exploitation, endowing the algorithm with both a certain degree of exploration capability and exploitation capability. In later stages, the algorithm uses chaotic predation mechanisms to create disturbances around cichlids to improve the exploitation of optimal solutions. Throughout the entire optimization process, the guidance, parasitism, and death mechanisms of individuals are integrated, allowing individuals to adjust their positions in real-time and improve the overall convergence accuracy. This paper rigorously evaluates the performance of CCO through 23 classic test functions and three CEC test suites. The experimental results show that compared with 11 famous algorithms and 10 novel improved algorithms, CCO can obtain the optimal solution in 91.52% of the test functions, demonstrating its excellent ability in solving various numerical optimization problems. Additionally, through the successful application to 6 mechanical optimization problems, 3 photovoltaic cell parameter optimization problems, and 1 path opti- mization problem, the competitiveness of CCO in solving real-world problems is verified and highlighted. The CCO source code can be downloaded here: https://ww2.mathworks.cn/matlabcentral/fileexchange/176828-cuckoo-catfish-optimizer-a-new-meta-heuristic-optimization},
  archive      = {J_AIR},
  author       = {Wang, Tian-Lei and Gu, Shao-Wei and Liu, Ren-Ju and Chen, Le-Qing and Wang, Zhu and Zeng, Zhi-Qiang},
  doi          = {10.1007/s10462-025-11291-x},
  journal      = {Artificial Intelligence Review},
  month        = {10},
  number       = {10},
  pages        = {1-80},
  shortjournal = {Artif. Intell. Rev.},
  title        = {Cuckoo catfish optimizer: A new meta-heuristic optimization algorithm},
  volume       = {58},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A lightweight few-shot learning model for crop pest and disease identification. <em>AIR</em>, <em>58</em>(10), 1-23. (<a href='https://doi.org/10.1007/s10462-025-11323-6'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Production quality is directly related to the economic development of agriculture. However, the growth of crops is susceptible to pest and disease infestations, which can negatively affect agricultural yields. Therefore, adopting efficient pest and disease identification methods is of the utmost importance. This paper proposes a lightweight few-shot learning model for crop pest and disease identification. The model utilizes a lightweight backbone network and incorporates adaptive spatial feature fusion to aggregate multi-scale features, thus avoiding feature redundancy and interference between multi-scale features. Additionally, a lightweight and efficient attention module is introduced to further explore the salient information in images from both channel and spatial dimensions. Experimental results demonstrate that, compared to the state-of-the-art methods in the field, the model achieved an average recognition accuracy improvement of 0.41% under the 10-shot setting on the PlantVillage dataset and improvements of 4.03% and 2.47% under the 5-shot and 10-shot settings, respectively, on the PlantDoc dataset. Furthermore, the model achieved a 1.46% increase in overall average recognition accuracy on the IP102 dataset, while also showing strong generalization capabilities on locally collected datasets.},
  archive      = {J_AIR},
  author       = {Wei, Linsen and Tang, Jingjun and Chen, Jinxiu and Mukamakuza, Carine Pierrette and Zhang, Defu and Zhang, Tong},
  doi          = {10.1007/s10462-025-11323-6},
  journal      = {Artificial Intelligence Review},
  month        = {10},
  number       = {10},
  pages        = {1-23},
  shortjournal = {Artif. Intell. Rev.},
  title        = {A lightweight few-shot learning model for crop pest and disease identification},
  volume       = {58},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Artificial intelligence-based methods for protein structure prediction: A survey. <em>AIR</em>, <em>58</em>(10), 1-36. (<a href='https://doi.org/10.1007/s10462-025-11325-4'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Protein structure prediction (PSP) is a meaningful problem that has drawn worldwide attention, where various artificial intelligence (AI) techniques, such as evolutionary computation (EC)-based and neural networks (NNs)-based methods, have been applied to PSP and have obtained promising results in recent years. Considering the rapid and significant advances of AI-based methods for PSP, it is vital to make a survey on this progress to summarize the existing research experience and to provide guidelines for further development of related research fields. With these aims, a broad survey of AI-based methods for solving PSP problems is provided in this article. First, EC-based PSP methods are reviewed, which are organized by three key steps involved in using EC-based methods for PSP. Second, NNs-based PSP methods are reviewed. More specifically, typical NNs-based methods to predict protein structural features are described and state-of-the-art NNs-based methods with end-to-end architecture and attention mechanism are reviewed. Third, the accuracy, interpretability, accessibility, and ethical challenges of AI-based methods are discussed. Last, the future directions including hybrid AI paradigm, protein language models, and the prediction of protein complexes and biomolecular interactions are given, and the conclusion is drawn. This survey is expected to draw attention, raise discussions, and inspire new ideas in the wonderful interdisciplinary field of biology and AI.},
  archive      = {J_AIR},
  author       = {Zhan, Zhi-Hui and Hong, Jun and Li, Jian-Yu and Wang, Cheng and He, Langchong and Xu, Zongben and Zhang, Jun},
  doi          = {10.1007/s10462-025-11325-4},
  journal      = {Artificial Intelligence Review},
  month        = {10},
  number       = {10},
  pages        = {1-36},
  shortjournal = {Artif. Intell. Rev.},
  title        = {Artificial intelligence-based methods for protein structure prediction: A survey},
  volume       = {58},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A dual-driven MAGDM method based on single-valued neutrosophic credibility numbers einstein variable extended power geometric aggregation operator and SPA-MARCOS. <em>AIR</em>, <em>58</em>(10), 1-74. (<a href='https://doi.org/10.1007/s10462-025-11299-3'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Traditional multi-attribute group decision-making (MAGDM) methods primarily rely on expert knowledge-driven information, often overlooking the value of objective data in decision-making processes. To address this gap, this paper proposes a novel dual-driven MAGDM method that incorporates knowledge-driven information, expressed through single-valued neutrosophic credibility numbers (SvNCNs), and data-driven information, represented by exact numbers (ENs). The primary innovations of this method include the development of the variable extended power geometric (VEPG) operator, which effectively aggregates knowledge-driven information from SvNCNs. The SvNCN Einstein variable extended power geometric (SvNCNEVEPG) operator is also introduced, and its mathematical properties are rigorously proven, offering an advanced approach to handling extreme values. To resolve ambiguity and uncertainty in decision analysis, a new subjective weight determination method, SvNCN-PIPRECIA, is introduced, complemented by an objective entropy-based weighting method. These are integrated into a new combined weight determination model using the Uninorm operator, enhancing the accuracy and reliability of the decision-making process. The dual-driven MAGDM method, combining knowledge-driven and data-driven information, improves decision-making comprehensiveness and precision through the dual-driven SPA-Entropy-PIPRECIA-MARCOS approach. The proposed methodology is validated through a case study on evaluating data trading platforms (DTPs), where sensitivity analysis of parameters and a comparative study with existing methods demonstrate the flexibility and scientific robustness of the approach.},
  archive      = {J_AIR},
  author       = {Liu, Pingqing and Shen, Junxin and Zhang, Peng},
  doi          = {10.1007/s10462-025-11299-3},
  journal      = {Artificial Intelligence Review},
  month        = {10},
  number       = {10},
  pages        = {1-74},
  shortjournal = {Artif. Intell. Rev.},
  title        = {A dual-driven MAGDM method based on single-valued neutrosophic credibility numbers einstein variable extended power geometric aggregation operator and SPA-MARCOS},
  volume       = {58},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Enhancing CCTA image quality: A review of deep learning approaches for advanced artifact correction and denoising. <em>AIR</em>, <em>58</em>(10), 1-67. (<a href='https://doi.org/10.1007/s10462-025-11311-w'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cardiac imaging is vital for diagnosing coronary artery disease (CAD), with coronary computed tomography angiography (CCTA) being commonly used to evaluate coronary vessels for stenosis, calcification, and atherosclerosis. However, CCTA images often suffer from artifacts like beam hardening, scatter, and noise, degrading image quality and obscuring anatomical details, leading to diagnostic uncertainty. Conventional post-processing techniques, such as filtered back projection and iterative reconstruction, have limited effectiveness in correcting these artifacts, posing challenges in CCTA, where precise visualization of coronary arteries is crucial. Artifacts can blur vessel boundaries, obscure calcified plaques, and misrepresent stenosis severity, potentially leading to misdiagnosis and suboptimal clinical decisions. Recent advancements in computational imaging, particularly deep learning algorithms, offer clinical benefits for artifact reduction in CCTA. Deep learning models, such as convolutional neural networks (CNNs), outperform traditional methods by effectively de-noising and correcting artifacts through learning complex patterns from large datasets. These models adapt to the non-linear, heterogeneous nature of artifacts, enhancing image clarity and diagnostic reliability. Improved image quality in CCTA enables better visualization of coronary arteries, aiding in accurate assessment of stenosis and calcification. This review highlights deep learning approaches for artifact correction in CCTA, emphasizing their potential to improve CAD diagnosis.},
  archive      = {J_AIR},
  author       = {Alkhodari, Mohanad and Alefisha, Eman and Jelinek, Herbert F. and Kaabneh, Ahmed and Liatsis, Panos},
  doi          = {10.1007/s10462-025-11311-w},
  journal      = {Artificial Intelligence Review},
  month        = {10},
  number       = {10},
  pages        = {1-67},
  shortjournal = {Artif. Intell. Rev.},
  title        = {Enhancing CCTA image quality: A review of deep learning approaches for advanced artifact correction and denoising},
  volume       = {58},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Do you actually need an LLM? rethinking language models for customer reviews analysis. <em>AIR</em>, <em>58</em>(10), 1-20. (<a href='https://doi.org/10.1007/s10462-025-11308-5'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {LLarge language models (LLMs) demonstrate strong natural language processing capabilities but come with significant computational costs, raising questions about their practical utility compared to small language models (SLMs). This study systematically compares SLMs (DistilBERT, ELECTRA) and LLMs (Flan-T5, Flan-UL2) on two customer review analysis tasks: sentiment polarity classification and product correlation analysis. Our results show that while LLMs outperform in sentiment classification, they do so at a much higher computational cost, whereas fine-tuned SLMs excel in domain-specific correlation analysis with greater efficiency. To balance accuracy and cost, we propose a context-enhanced hybrid (CE-Hybrid) model, which refines traditional hybrid methods by enriching LLM input with SLM-generated insights, reducing redundant computation while maintaining accuracy. Our findings quantify the trade-offs between model performance and resource efficiency, offering actionable insights for businesses to optimize AI deployment. These results have significant implications for real-world applications such as e-commerce, customer service automation, and business analytics.},
  archive      = {J_AIR},
  author       = {Xiao, Yang and Li, Yunke and Chen, Shaoyujie and Barker, Hayden and Rad, Ryan},
  doi          = {10.1007/s10462-025-11308-5},
  journal      = {Artificial Intelligence Review},
  month        = {10},
  number       = {10},
  pages        = {1-20},
  shortjournal = {Artif. Intell. Rev.},
  title        = {Do you actually need an LLM? rethinking language models for customer reviews analysis},
  volume       = {58},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A review and critical analysis of multimodal datasets for emotional AI. <em>AIR</em>, <em>58</em>(10), 1-65. (<a href='https://doi.org/10.1007/s10462-025-11271-1'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the increasing interest in digital technologies, emotion recognition plays an important role in several applications such as healthcare computer-aided diagnosis, social media analysis, opinion mining and recommendation systems, understanding human behavior and interaction in workplaces, effective communication and linguistic analysis, and cognitive human–machine interaction. This field is receiving a growing interest in recent years. In this paper, we present a thorough review of emotional artificial intelligence through identification and in-depth analysis of existing multimodal datasets along with their related research directions and methodologies. It establishes essential requirements for the development of a multimodal dataset and outlines challenges spanning its entire lifecycle, from recording to deployment. Moreover, a taxonomy of various categories and applications is introduced based on the key characteristics of various multimodal datasets. Finally, the paper concludes with discussions and insights into future directions and prospects for standard schemes to facilitate the efficient development of reliable and reusable benchmark datasets that can help researchers and developers advance this field.},
  archive      = {J_AIR},
  author       = {Al-Azani, Sadam and El-Alfy, El-Sayed M.},
  doi          = {10.1007/s10462-025-11271-1},
  journal      = {Artificial Intelligence Review},
  month        = {10},
  number       = {10},
  pages        = {1-65},
  shortjournal = {Artif. Intell. Rev.},
  title        = {A review and critical analysis of multimodal datasets for emotional AI},
  volume       = {58},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). An efficient model for diabetic detection using heuristic approach based serial cascaded convolutional ensemble network. <em>AIR</em>, <em>58</em>(10), 1-43. (<a href='https://doi.org/10.1007/s10462-025-11334-3'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Diabetes is a chronic pathology that poses significant risks to people. If diabetes is not properly diagnosed and treated, it may contribute to serious health problems. Delayed diagnosis causes many health issues and leads to numerous deaths every year. So, researchers have developed efficient diabetes detection systems for the early detection of this pathology. However, the existing model raises serious issues about the security and privacy of private medical information, and it requires rigorous safety precautions to prevent intrusions and unapproved access. In addition, the unclear characteristics of existing models cause difficulty in healthcare facilities. Thus, the advanced deep learning-based diabetic detection model was designed in this work to overcome these challenges. Also, it aims to detect diabetics and helps to prevent the progression of diabetes in patients. At first, the required data is gathered from the online data source and then fed to the optimal feature selection phase. Here, the features and weight are optimally selected using the Fitness-based Billiards-Inspired Optimization (FBIO) algorithm. This process helps the model to focus on the most impactful information within the data. Further, the obtained optimal weighted feature is passed to the Serial Cascaded Convolutional Ensemble Network (SCCEN) for detection. Here, the SCCEN model serially cascades techniques such as Convolutional Autoencoder (CAE), “1-dimensional Convolutional Neural Network” (1DCNN), and “Convolutional Long Short-Term Memory” (ConvLSTM). This process helps to improve the detection accuracy. Finally, the designed approach’s effectiveness is analyzed by comparing its performance with existing techniques. The suggested approach’s accuracy for dataset-1 is 97.4%, dataset-2 is 97.31%, and dataset-3 is 96.69%, which is higher than the conventional techniques and optimization algorithms. Thus, the result proved that the introduced framework can detect diabetics in premature stages and help the patient to take suitable treatment.},
  archive      = {J_AIR},
  author       = {Bejugam, Santosh Kumar and Vankara, Jyothi},
  doi          = {10.1007/s10462-025-11334-3},
  journal      = {Artificial Intelligence Review},
  month        = {10},
  number       = {10},
  pages        = {1-43},
  shortjournal = {Artif. Intell. Rev.},
  title        = {An efficient model for diabetic detection using heuristic approach based serial cascaded convolutional ensemble network},
  volume       = {58},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Machine learning and deep learning based psoriasis recognition system: Evaluation, management, prognosis—where we are and the way to the future. <em>AIR</em>, <em>58</em>(9), 1-47. (<a href='https://doi.org/10.1007/s10462-025-11195-w'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Psoriasis, a chronic inflammatory skin disease, poses significant diagnostic challenges due to its heterogeneous clinical manifestations and overlap with other dermatological conditions. The inconsistencies in diagnostic standards and variability in disease presentation further exacerbate the controversy surrounding the diagnosis of psoriasis. Recent advances in artificial intelligence (AI), particularly deep learning (DL) techniques such as convolutional neural networks (CNNs) and transformers, have shown promising results in improving recognition of psoriasis. This comprehensive review aims to provide an in-depth examination of the current state of AI applications in the diagnosis of psoriasis, highlighting the strengths and limitations of existing approaches. We evaluate the performance of various AI models on publicly available datasets, including the Psoriasis Image Dataset (PID) and the International Skin Imaging Collaboration (ISIC) dataset, and compare their accuracy, sensitivity, and specificity with traditional diagnostic methods. Furthermore, we discuss the challenges and future directions of AI-powered psoriasis diagnosis, including the need for large, diverse datasets, addressing data quality and generalization issues, and developing explainable AI models. Our review highlights the potential of AI to revolutionize the diagnosis and management of psoriasis and provides a pathway for future research in this field.},
  archive      = {J_AIR},
  author       = {Shehzadi, Nimra and Rehman, Arshia and Naz, Saeeda and Rehman, Saba and Khalifa, Fahmi},
  doi          = {10.1007/s10462-025-11195-w},
  journal      = {Artificial Intelligence Review},
  month        = {9},
  number       = {9},
  pages        = {1-47},
  shortjournal = {Artif. Intell. Rev.},
  title        = {Machine learning and deep learning based psoriasis recognition system: Evaluation, management, prognosis—where we are and the way to the future},
  volume       = {58},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Web intelligence (WI) 3.0: In search of a better-connected world to create a future intelligent society. <em>AIR</em>, <em>58</em>(9), 1-50. (<a href='https://doi.org/10.1007/s10462-025-11203-z'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Over the past two decades, Web Intelligence (WI) has emerged as a key field driving the evolution of AI in the connected world, addressing the demands of a future intelligent society. This paper provides a comprehensive review of WI’s contributions since its inception in 2000, spanning three distinct phases: Wisdom World Wide Web (WI 1.0, 2000–2009), Wisdom Web of Things (WI 2.0, 2010–2017), and Wisdom Web of Everything (WI 3.0, since 2018). For each phase, we examine key advancements, challenges, and future directions from the perspectives of both intelligent machines and human experts, highlighting significant societal impacts. To advance WI research, we propose a large language model-based learning framework for topic analysis and trend prediction. Moving beyond single-perspective approaches, we emphasize the Connected Intelligence Ecosystem defined by the HIGH5 scheme comprising one goal, two twins, three fundamentals, four functions, and five services that are realized through WI 3.0. This vision serves as a bridge from localized models to a global reference framework for addressing sustainability challenges in future societies. To illustrate the real-world implications of WI 3.0, we present case studies focusing on brain-inspired research, particularly in the intersection of brain intelligence, brain health, and brainternet-fostering interdisciplinary collaboration across diverse research communities.},
  archive      = {J_AIR},
  author       = {Kuai, Hongzhi and Huang, Jimmy X. and Tao, Xiaohui and Pasi, Gabriella and Yao, Yiyu and Liu, Jiming and Zhong, Ning},
  doi          = {10.1007/s10462-025-11203-z},
  journal      = {Artificial Intelligence Review},
  month        = {9},
  number       = {9},
  pages        = {1-50},
  shortjournal = {Artif. Intell. Rev.},
  title        = {Web intelligence (WI) 3.0: In search of a better-connected world to create a future intelligent society},
  volume       = {58},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Explainable AI and vision transformers for detection and classification of brain tumor: A comprehensive survey. <em>AIR</em>, <em>58</em>(9), 1-60. (<a href='https://doi.org/10.1007/s10462-025-11221-x'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Brain tumor detection and classification are critical for timely diagnosis and effective treatment. The surge in demand for automated and accurate methods is driven by the advancements in deep learning and the need for faster, more reliable diagnostic tools to assist clinicians. Despite the current literature about brain tumor classification and detection, several limitations persist. This survey reviews and contrasts the state-of-the-art deep learning diagnostic techniques that utilized brain tumor datasets such as Figshare and BraTS. This study provides a technical analysis of research papers on brain tumor diagnosis techniques, covering the period from 2020 to 2024 from well-known databases such as Scopus and Web of Science. Recent deep learning methodologies, including convolutional neural networks (CNNs), transfer learning, vision transformers (ViTs), hybrid techniques, and explainable AI, are explored regarding their performance, advantages, and limitations. We examine various architectures, preprocessing techniques, and datasets commonly used in brain tumor studies, focusing on multi-class classification, detection, and interpretability. Furthermore, the survey discusses the challenges in deep learning-based approaches related to brain tumor detection, including data scarcity and model interpretability, and outlines future directions.},
  archive      = {J_AIR},
  author       = {Hosny, Khalid M. and Mohammed, Mahmoud A.},
  doi          = {10.1007/s10462-025-11221-x},
  journal      = {Artificial Intelligence Review},
  month        = {9},
  number       = {9},
  pages        = {1-60},
  shortjournal = {Artif. Intell. Rev.},
  title        = {Explainable AI and vision transformers for detection and classification of brain tumor: A comprehensive survey},
  volume       = {58},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Optimized deep learning architecture for predicting maximum temperatures in key egyptian regions using hybrid genetic algorithm and mountain gazelle optimizer. <em>AIR</em>, <em>58</em>(9), 1-55. (<a href='https://doi.org/10.1007/s10462-025-11247-1'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Climate change significantly impacts plant growth, food production, ecosystems, sustainable socio-economic development, and human health. With the growing availability of extensive historical climate data and the increasing demand for accurate production forecasting, there is a pressing need for reliable methods to determine the stochastic relationship between past and future values. This article introduces a novel deep learning model designed to overcome the limitations of traditional forecasting methods and achieve highly accurate predictions. The proposed approach is a deep long short-term memory (DLSTM) model optimized using genetic algorithms (GAs) and the mountain gazelle optimizer (MGO), which collectively fine-tune the architecture of the DLSTM model. The experiment utilized historical climate data from nine Egyptian cities: Asswan, Bane-Suef, Behira, Dakhalia, Menoufia, Minia, Qalyubia, Sharkia, and Sohag to evaluate the model’s performance. To ensure a fair and comprehensive evaluation, the effectiveness of the proposed MGO-GA-DLSTM model was compared with other established forecasting techniques. The evaluation metrics included mean absolute error (MAE), root mean square error (RMSE), mean absolute percentage error (MAPE), and R-squared ( $$R^2$$ ) to quantify prediction accuracy and model robustness. The findings demonstrate that the MGO-GA-DLSTM model outperforms existing methods in climate prediction, offering improved accuracy and reliability.},
  archive      = {J_AIR},
  author       = {Houssein, Essam H. and Dirar, Mahmoud and Khalil, A. A. and Ali, Abdelmaged A. and Mohamed, Waleed M.},
  doi          = {10.1007/s10462-025-11247-1},
  journal      = {Artificial Intelligence Review},
  month        = {9},
  number       = {9},
  pages        = {1-55},
  shortjournal = {Artif. Intell. Rev.},
  title        = {Optimized deep learning architecture for predicting maximum temperatures in key egyptian regions using hybrid genetic algorithm and mountain gazelle optimizer},
  volume       = {58},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Agent-in-the-loop to distill expert knowledge into artificial intelligence models: A survey. <em>AIR</em>, <em>58</em>(9), 1-55. (<a href='https://doi.org/10.1007/s10462-025-11255-1'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Large-scale neural networks have revolutionized many general knowledge areas (e.g., computer vision and language processing), but are still rarely applied in many expert knowledge areas (e.g., healthcare), due to data sparsity and high annotation expenses. Human-in-the-loop machine learning (HIL-ML) incorporates expert domain knowledge into the modeling process, effectively addressing these challenges. Recently, some researchers have started using large models to substitute for certain tasks typically performed by humans. Although large models have limitations in expert knowledge areas, after being trained on trillions of examples, they have demonstrated advanced capabilities in reasoning, semantic understanding, grounding, and planning. These capabilities can serve as proxies of human, which introduces new opportunities and challenges in HIL-ML area. Based on the above, we summarize a more comprehensive framework, Agent-in-the-Loop Machine Learning (AIL-ML), where agent represents both humans and large models. AIL-ML can efficiently collaborate human and large model to construct vertical AI models with lower costs. This paper presents the first review of recent advancements in this area. First, we provide a formal definition of AIL-ML and discuss its related fields. Then, we categorize the AIL-ML methods based on data processing and model development, providing formal definitions for each, and present representative works in detail for each category. Third, we highlight relative applications of AIL-ML. Finally, we summarize the current literature and highlight future research directions.},
  archive      = {J_AIR},
  author       = {Gao, Jiayuan and Zhang, Yingwei and Chen, Yiqiang and Dong, Yihan and Chen, Yuanzhe and Song, Shuchao and Tang, Boshi and Gu, Yang},
  doi          = {10.1007/s10462-025-11255-1},
  journal      = {Artificial Intelligence Review},
  month        = {9},
  number       = {9},
  pages        = {1-55},
  shortjournal = {Artif. Intell. Rev.},
  title        = {Agent-in-the-loop to distill expert knowledge into artificial intelligence models: A survey},
  volume       = {58},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Alzheimer’s disease detection using deep learning and machine learning: A review. <em>AIR</em>, <em>58</em>(9), 1-39. (<a href='https://doi.org/10.1007/s10462-025-11258-y'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Alzheimer’s disease (AD) is a progressive neurodegenerative disorder that significantly impacts cognitive function, posing challenges in early diagnosis and treatment. Advances in artificial intelligence (AI) have revolutionized medical image analysis, providing robust frameworks for accurate and automated AD detection. This paper reviews recent developments in deep learning (DL) and machine learning (ML) models for AD classification, like convolutional neural networks (CNNs), transfer learning, hybrid architectures, and novel attention mechanisms. Additionally, applications of AD based on AI models, datasets, preprocessing techniques, challenges, and recent studies in this field are discussed. Also, the paper provides different medical modalities, factors of increasing risk of Alzheimer, progress stages of this disease, and several metrics of assessing AI models’ performance. These metrics such as accuracy, matthews correlation coefficient (MCC), F1-score, recall, precision, area under the receiver operating characteristic (ROC) curve, confusion matrix, and loss. Further, the paper presents several comparisons of different DL approaches for AD, limitations, new trends, suggestions, and future directions for this evolving field.},
  archive      = {J_AIR},
  author       = {Mohsen, Saeed},
  doi          = {10.1007/s10462-025-11258-y},
  journal      = {Artificial Intelligence Review},
  month        = {9},
  number       = {9},
  pages        = {1-39},
  shortjournal = {Artif. Intell. Rev.},
  title        = {Alzheimer’s disease detection using deep learning and machine learning: A review},
  volume       = {58},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A survey of transformers and large language models for ECG diagnosis: Advances, challenges, and future directions. <em>AIR</em>, <em>58</em>(9), 1-42. (<a href='https://doi.org/10.1007/s10462-025-11259-x'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Electrocardiograms (ECGs) are widely utilized in clinical practice as a non-invasive diagnostic tool for detecting cardiovascular diseases. Convolutional neural networks (CNNs) have been the primary choice for ECG analysis due to their capability to process raw signals. However, their localized convolutional operations limit the ability to capture long-range temporal dependencies across heartbeats, impeding a comprehensive cardiovascular assessment. To address these limitations, transformer-based frameworks have been introduced, employing self-attention mechanisms to effectively model complex temporal patterns over entire ECG sequences. Recent advancements in large language models (LLMs) have further expanded the utility of transformers by enabling multimodal integration and facilitating zero-shot diagnosis, thereby enhancing the scope of ECG-based clinical applications. Despite the increasing adoption of these methodologies, a comprehensive survey systematically examining transformer and LLM-based approaches for ECG analysis is absent from the literature. Consequently, this article surveys existing methods and proposes a novel hierarchical taxonomy based on the complexity of diagnosis, ranging from single-beat analysis to multi-beat and full-length signal evaluations. A thorough cross-category comparison is performed to highlight overarching commonalities and limitations. In light of these limitations, the paper presents a discussion of critical gaps and introduces new future directions aimed at improving ECG representation, enhancing positional encodings, refining self-attention architectures, and addressing challenges related to hallucinations and confidence measures in LLMs. The insights and guidelines presented aim to inform future research and clinical practices, enabling the next generation of intelligent ECG diagnostic systems.},
  archive      = {J_AIR},
  author       = {Ansari, Mohammed Yusuf and Yaqoob, Mohammed and Ishaq, Mohammed and Flushing, Eduardo Feo and Mangalote, Iffa Afsa changaai and Dakua, Sarada Prasad and Aboumarzouk, Omar and Righetti, Raffaella and Qaraqe, Marwa},
  doi          = {10.1007/s10462-025-11259-x},
  journal      = {Artificial Intelligence Review},
  month        = {9},
  number       = {9},
  pages        = {1-42},
  shortjournal = {Artif. Intell. Rev.},
  title        = {A survey of transformers and large language models for ECG diagnosis: Advances, challenges, and future directions},
  volume       = {58},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Advances in artificial intelligence and digital twin for tunnel boring machines. <em>AIR</em>, <em>58</em>(9), 1-61. (<a href='https://doi.org/10.1007/s10462-025-11261-3'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The deep integration of artificial intelligence (AI) and tunnel boring machine (TBM) has emerged as a critical research direction in intelligent tunnel construction. However, variable geological conditions, numerous electromechanical systems, and complex rock-machine interactions pose significant challenges to its implementation. This paper systematically reviews the latest research advances of AI in the TBM field, focusing on key technologies such as environmental perception, automated control, and predictive health management. Additionally, an intelligent TBM system architecture based on Digital Twin (DT) technology is proposed. The architecture integrates and coordinates a perception layer, an analysis layer, a decision-making layer, and an execution layer, enabling full-process autonomous control from environmental perception to intelligent decision-making. Finally, several critical issues and prospects in developing intelligent TBM, including data integration, model interpretability, and computational efficiency are discussed. This study aims to provide theoretical references and practical guidance for researchers in related fields, further promoting the in-depth application and development of digital twin technology in the intelligent TBM domain.},
  archive      = {J_AIR},
  author       = {Liu, Guangyuan and Tang, Yuanshou and Zhang, Huiping and Li, Runqi and Wang, Hongmei and Liu, Bright and Zhang, Siyong and Zhu, Hongtao and Liu, Dun and Ma, Sai},
  doi          = {10.1007/s10462-025-11261-3},
  journal      = {Artificial Intelligence Review},
  month        = {9},
  number       = {9},
  pages        = {1-61},
  shortjournal = {Artif. Intell. Rev.},
  title        = {Advances in artificial intelligence and digital twin for tunnel boring machines},
  volume       = {58},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Artificial intelligence applications in delirium prediction, diagnosis, and management: A systematic review. <em>AIR</em>, <em>58</em>(9), 1-61. (<a href='https://doi.org/10.1007/s10462-025-11264-0'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Delirium is a prevalent, acute, and reversible neuropsychiatric syndrome in elderly populations, notable for its high incidence and mortality rates, both of which impose a substantial burden on patient outcomes and healthcare systems. Currently, the detection of delirium primarily depends on clinical assessments performed by physicians, which poses challenges for less experienced clinicians in early identification of the condition. The application of artificial intelligence (AI) technologies to analyze large-scale data from delirium patients enables the identification and quantification of relevant delirium markers, thereby effectively assisting clinicians in the diagnosis, prediction, and monitoring of patient status. However, existing reviews on AI in the field of delirium exhibit several limitations, particularly regarding the comprehensive classification of existing studies. Current reviews often focus on a single type of data and often lack a systematic analysis of studies by data type. Most clinical models for delirium rely on electronic medical records, though physiological time-series data and imaging features also offer crucial biomarkers for the identification of delirium. This paper offers an overview of the medical foundation and recent technological advancements in delirium, aiming to establish a theoretical framework for novices. It systematically reviews the prediction, diagnosis, and management of delirium from a multi-source data perspective, enumerates relevant data sources and public databases, and examines various AI models used in this field, along with their advantages and limitations. Finally, this review addresses the challenges and emerging trends of AI in delirium, with the aim of elucidating key directions for future research and development.},
  archive      = {J_AIR},
  author       = {Lv, Sirui and Li, Jianqiang and He, Hang and Zhao, Qing and Jiang, Yinan},
  doi          = {10.1007/s10462-025-11264-0},
  journal      = {Artificial Intelligence Review},
  month        = {9},
  number       = {9},
  pages        = {1-61},
  shortjournal = {Artif. Intell. Rev.},
  title        = {Artificial intelligence applications in delirium prediction, diagnosis, and management: A systematic review},
  volume       = {58},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Deep learning for land use classification: A systematic review of HS-LiDAR imagery. <em>AIR</em>, <em>58</em>(9), 1-43. (<a href='https://doi.org/10.1007/s10462-025-11265-z'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Remote sensing (RS) technologies have significantly advanced Earth observation capabilities, enhancing the characterization and identification of surface materials through both spaceborne and airborne systems. These advancements are crucial for improving environmental monitoring and urban planning. As RS datasets have become more accessible, their increased complexity has necessitated a shift from traditional machine learning techniques to more robust deep learning approaches, particularly convolutional neural networks (CNNs) and transformer-based models known for their superior feature extraction capabilities. This systematic review focuses on the application of these deep learning techniques in land use classification, emphasizing the fusion of hyperspectral (HS) and LiDAR data. It critically examines the transition from traditional methods to advanced deep learning models, details comparative methodologies between different deep learning approaches, and discusses challenges in multimodal data fusion. The review also highlights potential areas for future research that can benefit researchers in developing robust and generalized techniques for land use classification.},
  archive      = {J_AIR},
  author       = {Rehman, Muhammad Zia Ur and Islam, Syed Mohammed Shamsul and Blake, David and Ulhaq, Anwaar and Janjua, Naeem},
  doi          = {10.1007/s10462-025-11265-z},
  journal      = {Artificial Intelligence Review},
  month        = {9},
  number       = {9},
  pages        = {1-43},
  shortjournal = {Artif. Intell. Rev.},
  title        = {Deep learning for land use classification: A systematic review of HS-LiDAR imagery},
  volume       = {58},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Synergistic integration of metaheuristics and machine learning: Latest advances and emerging trends. <em>AIR</em>, <em>58</em>(9), 1-64. (<a href='https://doi.org/10.1007/s10462-025-11266-y'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Metaheuristic algorithms (MH) and machine learning (ML) are important components of artificial intelligence (AI). The synergy between MH’s optimization search capabilities and ML’s data analysis strengths has proven to be highly effective, providing a powerful combination for delivering high-quality solutions across diverse fields. Particularly in real-world applications such as autonomous driving and healthcare, the integration of MH and ML can significantly enhance the intelligence level and decision-making efficiency of systems, addressing the urgent societal and industrial demands for high-efficiency and high-precision solutions. This paper clarifies the logic behind the surge in research on MH-ML hybrid algorithms and addresses the gaps in current reviews regarding their timeliness and breadth of perspective. We begin by elucidating the fundamental concepts underpinning MH and ML, followed by a comprehensive classification framework that categorizes and synthesizes the latest research findings systematically. The paper concludes with an exploration of the challenges inherent in MH-ML hybrid algorithms and proposes future research directions. The analysis of the collected literature demonstrates that the integration of MH and ML generally enhances the performance of algorithms in specific problems. Despite progress from simple combinations to deeper integrations, challenges such as theoretical lag and interpretability remain. Future research will focus on solving these challenges by exploring further integration, using open-source tools, and adapting across diverse domains to expand the use of MH-ML hybrid algorithms.},
  archive      = {J_AIR},
  author       = {Zhang, Ruining and Wang, Jian and Liu, Chanjuan and Su, Kaile and Ishibuchi, Hisao and Jin, Yaochu},
  doi          = {10.1007/s10462-025-11266-y},
  journal      = {Artificial Intelligence Review},
  month        = {9},
  number       = {9},
  pages        = {1-64},
  shortjournal = {Artif. Intell. Rev.},
  title        = {Synergistic integration of metaheuristics and machine learning: Latest advances and emerging trends},
  volume       = {58},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Solving the traveling salesman problem with machine learning: A review of recent advances and challenges. <em>AIR</em>, <em>58</em>(9), 1-48. (<a href='https://doi.org/10.1007/s10462-025-11267-x'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Traveling Salesman Problem (TSP) is a classic NP-hard problem that is central to various applications in logistics, robotics, and scheduling. This paper presents a comprehensive review of Machine Learning (ML) approaches for solving the TSP. Unlike traditional solvers, including exact and heuristic methods, ML models can generalize beyond training data, solving larger problems without the need for re-optimization, and enabling fast inference in milliseconds. These properties make ML-based TSP solvers particularly advantageous in real-time, dynamic, and resource-constrained environments. ML-based approaches are classified into traditional ML and deep learning (DL) approaches, including Pointer Network (Ptr-Net)-based, Graph Neural Network (GNN)-based, Transformer-based, hybrid DL-heuristic-based, and multi-model DL-based approaches. Our analysis highlights the growing dominance of GNNs and Transformers, attributed to their ability to capture complex inter-node relationships and manage the graph-based nature of TSP. Hybrid DL-heuristic approaches, which integrate DL with heuristics like Lin-Kernighan-Helsgaun (LKH), exhibit superior performance on large instances, combining learning flexibility with heuristic efficiency. A comparative summary and discussion highlight each approach’s strengths and limitations, identifying key challenges such as scalability, computational overhead, generalization and preserving quality at scale. Finally, the paper outlines promising future directions, emphasizing the need for scalable, generalizable, and resource-efficient solutions to address real-world TSP applications.},
  archive      = {J_AIR},
  author       = {Alanzi, Entesar and Menai, Mohamed El Bachir},
  doi          = {10.1007/s10462-025-11267-x},
  journal      = {Artificial Intelligence Review},
  month        = {9},
  number       = {9},
  pages        = {1-48},
  shortjournal = {Artif. Intell. Rev.},
  title        = {Solving the traveling salesman problem with machine learning: A review of recent advances and challenges},
  volume       = {58},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Structural knowledge: From brain to artificial intelligence. <em>AIR</em>, <em>58</em>(9), 1-39. (<a href='https://doi.org/10.1007/s10462-025-11270-2'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Organisms rely on structural knowledge derived from dynamic memory processes to adapt to the external world, employing cognitive maps and schemas. Similarly, artificial intelligence (AI) systems either explicitly or implicitly learn and utilize structural knowledge. Skillful utilization of this structural knowledge not only enhances the performance of AI models but also improves their transferability, generalization, and interpretability, crucial for developing robust and reliable AI systems. The importance of integrating insights from brain-based structural knowledge into AI systems cannot be overestimated. In this survey, we review research on structural knowledge in the brain, focusing on cognitive maps and schemas. We then examine computational models, highlighting potential mechanisms that serve as a bridge to AI. Finally, we provide an overview of AI models that leverage both brain-inspired and traditional structural knowledge and discuss how biological mechanisms can be applied to enhance AI systems. This survey aims to deepen the understanding of how structural knowledge in the brain can be constructed and utilized in AI, thereby bridging the gap between natural intelligence and artificial intelligence.},
  archive      = {J_AIR},
  author       = {Yu, Yingchao and Yan, Yuping and Jin, Yaochu},
  doi          = {10.1007/s10462-025-11270-2},
  journal      = {Artificial Intelligence Review},
  month        = {9},
  number       = {9},
  pages        = {1-39},
  shortjournal = {Artif. Intell. Rev.},
  title        = {Structural knowledge: From brain to artificial intelligence},
  volume       = {58},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). The effect of pooling parameters on the performance of convolution neural network. <em>AIR</em>, <em>58</em>(9), 1-25. (<a href='https://doi.org/10.1007/s10462-025-11273-z'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Pooling is a crucial aspect of Convolutional Neural Networks (CNNs), a prominent machine learning technique. It plays an essential role in the learning process by reducing the spatial dimensions of feature maps and minimizing computational costs. Common pooling methods, such as Max Pooling (MaxPool) and Average Pooling (AvgPool), have notable limitations. While MaxPool efficiently extracts important features, it often discards useful information, which can negatively impact CNN performance. In contrast, AvgPool preserves background information (maintaining global detail) but treats all inputs equally (assigning the same importance to every pixel in a region of an image), potentially leading to inefficiencies. We propose enhancing pooling adaptability by introducing learnable parameters to address these issues. These parameters (pooling kernels) are incorporated into the pooling methods to make them learnable. The study also explores how this modification affects the image classification performance of CNNs across various widely used datasets. The findings suggest that integrating parameters into the pooling process produces better results compared to the traditional pooling techniques previously discussed.},
  archive      = {J_AIR},
  author       = {Shadoul, Inas and Al-Hmouz, Rami and Hossen, Abdulnasir and Mesbah, Mostefa and Deveci, Muhammet},
  doi          = {10.1007/s10462-025-11273-z},
  journal      = {Artificial Intelligence Review},
  month        = {9},
  number       = {9},
  pages        = {1-25},
  shortjournal = {Artif. Intell. Rev.},
  title        = {The effect of pooling parameters on the performance of convolution neural network},
  volume       = {58},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Machine learning after a decade: Is it still a missing keystone in genomic-based plant breeding?. <em>AIR</em>, <em>58</em>(9), 1-35. (<a href='https://doi.org/10.1007/s10462-025-11274-y'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Plant breeding plays a crucial role in addressing the pressing challenges of food insecurity and global hunger, issues that are expected to worsen in the coming years. The development of effective plant breeding pipelines relies on a deep understanding and proficiency in various disciplines, such as phenomics and genomics. Leveraging the five G’s - germplasm characterization, genome assembly, genomic breeding, gene function identification, and gene editing - can significantly boost the pace of crop improvement initiatives. In the past decade, the integration of machine learning (ML) algorithms into the five G’s has gathered increasing attention for their abilities to integrate diverse omics and biological datasets to create precise breeding predictive models. Despite the promise of ML in advancing genomic-assisted breeding, there remains a critical question regarding the extent to which ML can help genomic-assisted breeding and what are their true potentials and efficacies compared to conventional methods. This review evaluates ML’s role in genomics-based plant breeding, highlighting its strengths in prediction accuracy and breeding efficiency but also addressing challenges such as data biases and implementation barriers. It explores applications for improving crop resilience and productivity through the integration of multi-omics data and addressing data biases. Ultimately, the review underscores ML’s potential to transform genomics-based plant breeding while identifying gaps and future research opportunities.},
  archive      = {J_AIR},
  author       = {Yoosefzadeh-Najafabadi, Mohsen and Xavier, Alencar and Eskandari, Milad and Hesami, Mohsen},
  doi          = {10.1007/s10462-025-11274-y},
  journal      = {Artificial Intelligence Review},
  month        = {9},
  number       = {9},
  pages        = {1-35},
  shortjournal = {Artif. Intell. Rev.},
  title        = {Machine learning after a decade: Is it still a missing keystone in genomic-based plant breeding?},
  volume       = {58},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Dynamic optimization of min-df in the GreedSum algorithm for enhanced extractive summarization. <em>AIR</em>, <em>58</em>(9), 1-33. (<a href='https://doi.org/10.1007/s10462-025-11276-w'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This study aims to improve extractive text summarization by dynamically optimizing the min-df (minimum document frequency) parameter in the GreedSum algorithm. To achieve this, three methods are proposed for dynamic tuning: a geometric approach, a percentile-based threshold, and a clustering-based adaptation strategy. These methods were applied to a large-scale dataset of 17,038 scientific articles from arXiv and PubMed. The experiments demonstrate a 2% improvement in ROUGE-1 F-measure over fixed min-df settings, achieving a peak ROUGE-1 F1-score of 45%. This performance surpasses several established extractive and hybrid baselines. Our contributions include a comparative evaluation of dynamic tuning strategies and the demonstration of their effectiveness for adaptive summarization. These findings confirm the potential of dynamic min-df optimization for generating accurate and efficient summaries, with future research focused on deep learning integration and real-time, multi-modal summarization.},
  archive      = {J_AIR},
  author       = {Aubakirov, Shakarim and Akhmetov, Iskander and Gelbukh, Alexander and Mussabayev, Rustam},
  doi          = {10.1007/s10462-025-11276-w},
  journal      = {Artificial Intelligence Review},
  month        = {9},
  number       = {9},
  pages        = {1-33},
  shortjournal = {Artif. Intell. Rev.},
  title        = {Dynamic optimization of min-df in the GreedSum algorithm for enhanced extractive summarization},
  volume       = {58},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). YOLO advances to its genesis: A decadal and comprehensive review of the you only look once (YOLO) series. <em>AIR</em>, <em>58</em>(9), 1-83. (<a href='https://doi.org/10.1007/s10462-025-11253-3'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This review systematically examines the progression of the You Only Look Once (YOLO) object detection algorithms from YOLOv1 to the recently unveiled YOLOv12. Employing a reverse chronological analysis, this study examines the advancements introduced by YOLO algorithms, beginning with YOLOv12 and progressing through YOLO11 (or YOLOv11), YOLOv10, YOLOv9, YOLOv8, and subsequent versions to explore each version’s contributions to enhancing speed, detection accuracy, and computational efficiency in real-time object detection. Additionally, this study reviews the alternative versions derived from YOLO architectural advancements of YOLO-NAS, YOLO-X, YOLO-R, DAMO-YOLO, and Gold-YOLO. Moreover, the study highlights the transformative impact of YOLO models across five critical application areas: autonomous vehicles and traffic safety, healthcare and medical imaging, industrial manufacturing, surveillance and security, and agriculture. By detailing the incremental technological advancements in subsequent YOLO versions, this review chronicles the evolution of YOLO, and discusses the challenges and limitations in each of the earlier versions. The evolution signifies a path towards integrating YOLO with multimodal, context-aware, and Artificial General Intelligence (AGI) systems for the next YOLO decade, promising significant implications for future developments in AI-driven applications.},
  archive      = {J_AIR},
  author       = {Sapkota, Ranjan and Flores-Calero, Marco and Qureshi, Rizwan and Badgujar, Chetan and Nepal, Upesh and Poulose, Alwin and Zeno, Peter and Vaddevolu, Uday Bhanu Prakash and Khan, Sheheryar and Shoman, Maged and Yan, Hong and Karkee, Manoj},
  doi          = {10.1007/s10462-025-11253-3},
  journal      = {Artificial Intelligence Review},
  month        = {9},
  number       = {9},
  pages        = {1-83},
  shortjournal = {Artif. Intell. Rev.},
  title        = {YOLO advances to its genesis: A decadal and comprehensive review of the you only look once (YOLO) series},
  volume       = {58},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). HIAT: Human-in-the-loop reinforcement learning with auxiliary task. <em>AIR</em>, <em>58</em>(9), 1-23. (<a href='https://doi.org/10.1007/s10462-025-11260-4'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Human-in-the-loop reinforcement learning (HIRL) tackles the challenges of inefficient exploration and low sample efficiency by incorporating human feedback into the learning process to guide exploration and speed up learning. Nevertheless, most recent approaches overlook the problem of sample imbalance caused by varying behavioral policy distributions in the early stages of training, which results in policy performance degradation due to biased learning toward dominant low-quality data. In this paper, we propose a human-in-the-loop reinforcement learning method by constructing an auxiliary task, namely Human-in-the-Loop Reinforcement Learning with Auxiliary Task (HIAT). Specifically, state-action pairs are utilized to create an auxiliary task that assesses the similarity between different policies, forcing the model to focus on the differences in data sources. The HIAT method guides the learning process of new policies during the policy evaluation phase by implicitly influencing the assessment of different actions, thereby compensating for sample imbalance. Subsequently, we introduce an adaptive weighting version, namely adaptive weighting based HIAT (AWHIAT), which adjusts the impact of the auxiliary task on the primary task based on their task-relevance and mitigates conflicts between different tasks caused by cognitive load. Through empirical evaluations on six continuous control benchmarks, the results demonstrate that AWHIAT significantly improves mean episode reward and sample efficiency compared to four representative HIRL methods.},
  archive      = {J_AIR},
  author       = {Niu, Bo and Luo, Biao and Cui, Yongzheng and Xu, Xiaodong and Zhao, Yuqian and Feng, Yu},
  doi          = {10.1007/s10462-025-11260-4},
  journal      = {Artificial Intelligence Review},
  month        = {9},
  number       = {9},
  pages        = {1-23},
  shortjournal = {Artif. Intell. Rev.},
  title        = {HIAT: Human-in-the-loop reinforcement learning with auxiliary task},
  volume       = {58},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A systematic literature review on human activity recognition using smart devices: Advances, challenges, and future directions. <em>AIR</em>, <em>58</em>(9), 1-57. (<a href='https://doi.org/10.1007/s10462-025-11275-x'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With improvement in AI and deep learning, Human Activity Recognition (HAR) has started to play an important role in many real-world applications. These range from healthcare and elderly care to security to enhancing user experience in smart environments. Smart devices with their ubiquity and multi-modal sensors, provide critical data that can be used to improve the quality of life and efficiency of services. Given the rapid technological advancements and recent developments in the field, it is essential to examine the current state of Human Activity Recognition (HAR), highlighting its strengths as well as the challenges it continues to face. This study presents a systematic review of the literature (SLR) of HAR that is conducted through a detailed study of articles published from 2021 to 2024 using open data sets. This study explores major databases i.e., Springer, IEEE Xplore, Science Direct, Taylor & Francis, and ACM to access the recent advancements in HAR that are facilitated by smartphones and smartwatches. Our SLR looks at the most commonly used datasets i.e. WISDM, PAMAP2, UCI-HAR, Opportunity, and MHealth. Moreover, this SLR compares the performance of commonly used deep learning techniques i.e. Convolutional Neural Networks (CNN), Long Short Term Memory (LSTM), Transformers and Ensemble models on these datasets. The objectives of this study are threefold i.e. to identify recent studies that employ deep learning techniques for Human Activity Recognition (HAR) tasks, to review publicly available open datasets commonly used in HAR research during 2021 to 2024, and to identify common challenges and limitations in current HAR research while proposing potential future research directions.},
  archive      = {J_AIR},
  author       = {Qureshi, Tayyab Saeed and Shahid, Muhammad Haris and Farhan, Asma Ahmad and Alamri, Sultan},
  doi          = {10.1007/s10462-025-11275-x},
  journal      = {Artificial Intelligence Review},
  month        = {9},
  number       = {9},
  pages        = {1-57},
  shortjournal = {Artif. Intell. Rev.},
  title        = {A systematic literature review on human activity recognition using smart devices: Advances, challenges, and future directions},
  volume       = {58},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Advancements in air quality monitoring: A systematic review of IoT-based air quality monitoring and AI technologies. <em>AIR</em>, <em>58</em>(9), 1-67. (<a href='https://doi.org/10.1007/s10462-025-11277-9'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Air quality monitoring is a critical component of environmental management, especially in developing countries where pollution levels are a growing concern. This review systematically analyzes recent advances in Internet-of-Things (IoT)-based air quality monitoring systems and highlights the impact of artificial intelligence (AI) technologies. A comprehensive selection process was used to identify 220 relevant studies using databases such as Google Scholar, Scopus, and ResearchRabbit. Following a selection process based on specific eligibility criteria, a total of 147 studies were chosen for a detailed analysis. These studies present notable advances in the application of artificial intelligence to improve data accuracy, predictive capabilities, and real-time analysis in air quality monitoring systems. A key contribution of this review is the proposal of a classification framework for AI techniques in air quality monitoring, organized into five main application areas: data imputation, sensor calibration, anomaly detection, air quality index (AQI) estimation, and short-term forecasting. The review takes an in-depth look at the different uses of these technologies in both urban and industrial settings, presenting successful case studies that showcase their effectiveness in addressing pressing air quality issues. Additionally, the paper identifies research gaps in the literature, particularly related to data quality, system scalability, and integration challenges in AI-driven IoT systems. The insights provided aim to guide researchers and practitioners in selecting appropriate AI techniques and system architectures, inform the design of more reliable and scalable air quality monitoring frameworks, and support future efforts to mitigate air pollution through data-driven decision-making.},
  archive      = {J_AIR},
  author       = {Garcia, Antony and Saez, Yessica and Harris, Itamar and Huang, Xinming and Collado, Edwin},
  doi          = {10.1007/s10462-025-11277-9},
  journal      = {Artificial Intelligence Review},
  month        = {9},
  number       = {9},
  pages        = {1-67},
  shortjournal = {Artif. Intell. Rev.},
  title        = {Advancements in air quality monitoring: A systematic review of IoT-based air quality monitoring and AI technologies},
  volume       = {58},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Physics-informed stochastic configuration network promoted model predictive control with multi-objective optimization. <em>AIR</em>, <em>58</em>(9), 1-30. (<a href='https://doi.org/10.1007/s10462-025-11278-8'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Model predictive control(MPC) has attracted much attention for its superior control performance in industrial processes. However, due to the challenges in building models for industrial processes and the necessary multiple optimization objectives during the MPC optimization steps, it is difficult to achieve satisfactory control results. In this work, we propose a physics-informed stochastic configuration network(PISCN) modeling method, and a predictive control scheme based on PISCN combined with multi-objective optimization(MOO) for a class of nonlinear dynamic systems. We first develop a data-driven and physically guided hybrid modeling method that embeds physical knowledge into the loss function of stochastic configuration networks(SCN) to improve model accuracy. During the model training, we employ a parallel configuration method(PCM) to randomly assign input weights and bias of hidden nodes, reducing the number of training iterations. Secondly, the PISCN model is incorporated into MPC framework and multiple optimization objectives are considered simultaneously. Particularly, the corresponding closed-loop stability is analyzed and proven. Finally, the proposed method is applied in the dehydration reaction stage in sintering process of ternary cathode materials. The results show that compared with SCN based MPC, PISCN can obtain a more accurate model and achieve better control performance by considering multiple objectives. The sintering time and energy consumption are significantly reduced.},
  archive      = {J_AIR},
  author       = {Xu, Lei and Yang, Chunhua and Xu, Xiaodong and Luo, Biao and Huang, Tingwen},
  doi          = {10.1007/s10462-025-11278-8},
  journal      = {Artificial Intelligence Review},
  month        = {9},
  number       = {9},
  pages        = {1-30},
  shortjournal = {Artif. Intell. Rev.},
  title        = {Physics-informed stochastic configuration network promoted model predictive control with multi-objective optimization},
  volume       = {58},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Self-organizing fuzzy neural network with adaptive evolution strategy for nonlinear and nonstationary processes. <em>AIR</em>, <em>58</em>(9), 1-23. (<a href='https://doi.org/10.1007/s10462-025-11283-x'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Fuzzy neural networks, which combine the strengths of fuzzy logic systems and artificial neural networks, prove to be effective in modeling industrial processes. However, because of the nonlinearity and nonstationarity exhibited in complex industrial processes, constructing an accurate model and maintaining its performance in uncertain environments have remained challenging. Hence, a self-organizing fuzzy neural network with an adaptive evolution strategy (AE-SOFNN) is proposed for nonlinear and nonstationary process modeling. First, a self-organizing mechanism based on the network learning accuracy and the activity of rules is developed to achieve a compact structure. Meanwhile, by integrating the least squares method and an improved second-order algorithm, a hybrid learning algorithm is applied to adjust network parameters. Then, an adaptive evolution strategy is proposed to enable the AE-SOFNN to better adapt to changes, aiming to ensure the accuracy and robustness of the constructed network in nonstationary environments. Specifically, an adaptive activation threshold based on generalization ability is developed to determine how to update, namely by either local updating or global updating. The variation of linear parameters during local updating is taken as an indicator of concept drift, helping to improve the global updating performance via the selection of appropriate samples. Finally, the effectiveness of the AE-SOFNN is evaluated by a chaotic time-series prediction problem and an industrial application, demonstrating the superiority of AE-SOFNN in modeling nonlinear and nonstationary processes.},
  archive      = {J_AIR},
  author       = {Meng, Xi and Hou, Qizheng and Quan, Limin and Qiao, Junfei},
  doi          = {10.1007/s10462-025-11283-x},
  journal      = {Artificial Intelligence Review},
  month        = {9},
  number       = {9},
  pages        = {1-23},
  shortjournal = {Artif. Intell. Rev.},
  title        = {Self-organizing fuzzy neural network with adaptive evolution strategy for nonlinear and nonstationary processes},
  volume       = {58},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Comprehensive review of recent developments in visual object detection based on deep learning. <em>AIR</em>, <em>58</em>(9), 1-97. (<a href='https://doi.org/10.1007/s10462-025-11284-w'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This comprehensive review looks into the recent developments in visual object detection, focusing on the transformative effect of deep learning (DL) technologies. In object detection, computer vision is a basic issue. This involves object detection and location in the video and image frames, which has notable advantages in robotics, autonomous driving, medical imaging, and surveillance. This review, therefore, presents a thorough integration analysis in visual object detection of the latest developments, providing both the historical context and state-of-the-art analysis. This review categorizes current methods into one-stage and two-stage frameworks, studying their architectural innovations, detection accuracy, computational speed, and deployment readiness. This review further scrutinizes the performance measures, emphasizes the inevitability of large-scale annotated datasets, and provides a curated overview of the widely used datasets in the field. Notable features include a discussion of practical applications and current research trends, and a comprehensive comparative analysis that compares models based on accuracy, speed, and trade-offs. A unique addition of this work is a thorough comparative analysis table that benchmarks traditional and modern models in terms of mean Average Precision (mAP), frames per second (FPS), advantages, limitations, and the coverage of transformer-based models and real-time deployments. The review’s holistic approach provides significant insights for researchers and practitioners seeking to understand, benchmark, develop, or benchmark object detection systems.},
  archive      = {J_AIR},
  author       = {Edozie, Enerst and Shuaibu, Aliyu Nuhu and John, Ukagwu Kelechi and Sadiq, Bashir Olaniyi},
  doi          = {10.1007/s10462-025-11284-w},
  journal      = {Artificial Intelligence Review},
  month        = {9},
  number       = {9},
  pages        = {1-97},
  shortjournal = {Artif. Intell. Rev.},
  title        = {Comprehensive review of recent developments in visual object detection based on deep learning},
  volume       = {58},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A self-supervised BEiT model with a novel hierarchical patchReducer for efficient facial deepfake detection. <em>AIR</em>, <em>58</em>(9), 1-37. (<a href='https://doi.org/10.1007/s10462-025-11286-8'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The spread of deepfake technology has become a growing concern, especially with the rapid advancement of generative models. This progress has made it increasingly difficult to distinguish between real and fake facial videos. This poses serious threats to security, privacy, and the spread of misinformation. Despite the existence of deepfake detection models, many suffer from high computational costs. This makes them impractical for deployment in resource-constrained environments. To address this challenge, this paper proposes a solution that accurately identifies deepfakes while reducing computational complexity. To achieve this, a deepfake detection system using an improved version of a Self-Supervised BEiT, called BEiT-HPR (Hierarchical PatchReducer), is proposed. The enhancement adds a Hierarchical PatchReducer layer to reduce the number of patches in successive encoder blocks. This reduces computational complexity while maintaining high detection accuracy. Additionally, training speed increases by over 50%, while the number of parameters is reduced by 63.4%. The BEiT-HPR model was tested using three publicly available benchmark datasets: FaceForensics++ (FF++), Celeb-DF, and the Deepfake Detection Dataset (DFD). The evaluation results revealed that reducing the model complexity by 62.2% allowed the proposed model to achieve an accuracy of 83.92% on FF++, 97.59% on Celeb-DF, and 98.25% on DFD. These findings emphasize the importance of computationally efficient deepfake detection methods that maintain high detection accuracy while reducing the burden of heavy computation. Therefore, it offers a scalable solution for identifying deepfakes across diverse datasets.},
  archive      = {J_AIR},
  author       = {Al Redhaei, Aneesa and Fraihat, Salam and Al-Betar, Mohammed Azmi},
  doi          = {10.1007/s10462-025-11286-8},
  journal      = {Artificial Intelligence Review},
  month        = {9},
  number       = {9},
  pages        = {1-37},
  shortjournal = {Artif. Intell. Rev.},
  title        = {A self-supervised BEiT model with a novel hierarchical patchReducer for efficient facial deepfake detection},
  volume       = {58},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A survey of deep learning for industrial visual anomaly detection. <em>AIR</em>, <em>58</em>(9), 1-82. (<a href='https://doi.org/10.1007/s10462-025-11287-7'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Industrial visual anomaly detection is critical for ensuring system reliability, safety, and efficiency. This paper presents a comprehensive survey of state-of-the-art anomaly detection techniques, analyzing methodologies, implementations, and recent advancements. Our survey aims to accelerate researchers’ understanding of emerging trends while providing a structured foundation for newcomers. We systematically review 196 recent papers covering five learning strategies, including fully supervised, semi-supervised, self-supervised, weakly supervised, and unsupervised approaches. This paper provides a detailed introduction to twelve industrial anomaly detection methods, revealing their theoretical foundations, technical principles, and practical applications. Additionally, we provide a detailed overview to 2D and 3D datasets for industrial visual anomaly detection. In addition, we critically analyze and summarize the experimental results, identify key performance indicators, and discuss the latest trends in the field of industrial anomaly detection. Beyond analysis, we contribute actionable insights for selecting optimal models for real-world deployment. Finally, we highlight open challenges and outline future research directions to drive innovation in this evolving field. The detailed resources are available at https://github.com/IHPCRits/IAD-Survey .},
  archive      = {J_AIR},
  author       = {Li, Zhuo and Yan, Yuhao and Wang, Xiangheng and Ge, Yifei and Meng, Lin},
  doi          = {10.1007/s10462-025-11287-7},
  journal      = {Artificial Intelligence Review},
  month        = {9},
  number       = {9},
  pages        = {1-82},
  shortjournal = {Artif. Intell. Rev.},
  title        = {A survey of deep learning for industrial visual anomaly detection},
  volume       = {58},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Comprehensive review of reinforcement learning for medical ultrasound imaging. <em>AIR</em>, <em>58</em>(9), 1-90. (<a href='https://doi.org/10.1007/s10462-025-11268-w'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Medical Ultrasound (US) imaging has seen increasing demands over the past years, becoming one of the most preferred imaging modalities in clinical practice due to its affordability, portability, and real-time capabilities. However, it faces several challenges that limit its applicability, such as operator dependency, variability in interpretation, and limited resolution, which are amplified by the low availability of trained experts. This calls for the need of autonomous systems that are capable of reducing the dependency on humans for increased efficiency and throughput. Reinforcement Learning (RL) comes as a rapidly advancing field under Artificial Intelligence (AI) that allows the development of autonomous and intelligent agents through rewarded interactions with their environments. Several existing surveys on advancements in US imaging predominantly focus on partially autonomous AI solutions. However, none of these surveys explore the intersection between the stages of the US process and the recent advancements in RL solutions. To bridge this gap, this survey proposes a comprehensive taxonomy that integrates the stages of the US process with the RL development pipeline -including data preparation, problem formulation, simulation environment, RL training, validation and finetuning- and reviews current research efforts under this taxonomy. This work aims to highlight the potential of RL in building autonomous US solutions while identifying limitations and opportunities for further advancements in this field.},
  archive      = {J_AIR},
  author       = {Elmekki, Hanae and Islam, Saidul and Alagha, Ahmed and Sami, Hani and Spilkin, Amanda and Zakeri, Ehsan and Zanuttini, Antonela Mariel and Bentahar, Jamal and Kadem, Lyes and Xie, Wen-Fang and Pibarot, Philippe and Mizouni, Rabeb and Otrok, Hadi and Singh, Shakti and Mourad, Azzam},
  doi          = {10.1007/s10462-025-11268-w},
  journal      = {Artificial Intelligence Review},
  month        = {9},
  number       = {9},
  pages        = {1-90},
  shortjournal = {Artif. Intell. Rev.},
  title        = {Comprehensive review of reinforcement learning for medical ultrasound imaging},
  volume       = {58},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Tianji’s horse racing optimization (THRO): A new metaheuristic inspired by ancient wisdom and its engineering optimization applications. <em>AIR</em>, <em>58</em>(9), 1-111. (<a href='https://doi.org/10.1007/s10462-025-11269-9'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this study, we introduce a novel metaheuristic algorithm named Tianji’s horse racing optimization (THRO), inspired by the Chinese historical story of Tianji’s horse racing. The story illustrates how Tianji leveraged his strengths to counteract his opponent’s weaknesses, ultimately leading to his victory in the competition. This strategic principle, which led to Tianji’s victory, forms the foundation of THRO’s design. The need for such a proposal arises from the limitations of existing optimization algorithms, which often struggle with convergence speed and solution accuracy when solving complex problems. THRO addresses these challenges by employing a unique dynamic individual matching strategy that enhances the algorithm’s convergence rate and solution precision. In this algorithm, an effective greedy strategy is employed to maximize benefits by selecting individuals from its population and matching them with individuals from the opponent’s population, thereby facilitating individual updates. This paper provides mathematically grounded explanations and analysis of how the algorithm converges to the global optimum with probability 1. To validate the efficacy of THRO, comparative experiments with 12 popular algorithms are conducted on 23 classical benchmark functions and the CEC2017 test suite. For the 29 CEC2017 functions across 10, 30, 50, and 100 dimensions, THRO achieves the slowest Friedman average ranking values among all competing methods, which are 2.052, 2.500, 2.293, and 2.259, respectively. Additionally, we conduct a comprehensive comparison with several advanced algorithms, including high-performance hybrid optimizers and the CEC winners, across the CEC2014, CEC2017, CEC2020, and CEC2022 suites, where THRO again achieves the slowest Friedman average ranking value of 1.729. Furthermore, six engineering design problems are employed to comprehensively check the applicability of THRO. Eventually, THRO’s proficiency extends to the application of identifying damping parameters of magnetorheological damper (MRD) models in mechanical systems. The results confirm that THRO exhibits remarkable competitiveness in solving various complex problems.The source code of THRO is publicly available at https://github.com/zwg770123/THRO .},
  archive      = {J_AIR},
  author       = {Wang, Liying and Du, Haiping and Zhang, Zhenxing and Hu, Gang and Mirjalili, Seyedali and Khodadadi, Nima and Hussien, Abdelazim G. and Liao, Yingying and Zhao, Weiguo},
  doi          = {10.1007/s10462-025-11269-9},
  journal      = {Artificial Intelligence Review},
  month        = {9},
  number       = {9},
  pages        = {1-111},
  shortjournal = {Artif. Intell. Rev.},
  title        = {Tianji’s horse racing optimization (THRO): A new metaheuristic inspired by ancient wisdom and its engineering optimization applications},
  volume       = {58},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Estimation of photovoltaic parameters by dynamic updating and selecting a snake optimizer based on multi-directional optimization. <em>AIR</em>, <em>58</em>(9), 1-76. (<a href='https://doi.org/10.1007/s10462-025-11272-0'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A multi-strategy synergistic learning snake optimizer (MSSO) is proposed to address the challenges of high computational cost, limited identification of key parameters, and inaccurate results that are still prevalent in swarm intelligence algorithms when managing the nonlinear dynamics of solar photovoltaic systems. This paper, for the first time, explores the intrinsic mechanisms between the four behavioral patterns generated by snakes, which are influenced by food quantity and temperature, and their impact on the diversity and convergence of snake optimization algorithms. It also discusses the limitations observed, offering a novel interpretation of snake optimization algorithms from a fresh perspective. The innovatively proposed superior point strategy, adaptive snake spiral foraging strategy, and dynamic update selection mechanism with multi-directional optimization enable the algorithm to learn from the global optimum and the neighborhood optimum, reducing individuals’ over-reliance on optimal positions and accelerating convergence. Extensive experiments are conducted using CEC2017 and CEC2011 benchmarks on 43 function problems and three application problems for photovoltaic parameter estimation to evaluate the performance of MSSO. A comparative analysis with 26 metaheuristic algorithms (MAs) indicates that MSSO converges more rapidly and ranks first in terms of mean, standard deviation, and Wilcoxon and Friedman tests. The results of the half violin plot combined with scatter plots further illustrate that MSSO exhibits a denser data cloud, a more concentrated distribution density of optimal values, fewer outliers, and enhanced stability. Additionally, a higher quality solution can be obtained with only 50% of the iterations required, without any additional computational time. Finally, on the three application problems of photovoltaic parameter estimation, compared to 26 MAs, the solutions provided by MSSO ranked first in terms of mean and Root Mean Square Error (RMSE), and the performance of the algorithms can be improved by up to 94.0% and 2.7%, which highlights the superiority, universality, and applicability of the algorithms.},
  archive      = {J_AIR},
  author       = {Wang, Yi and Ping, Fushuai and Li, Yuchen and Gu, Tianfeng and Wang, Tan},
  doi          = {10.1007/s10462-025-11272-0},
  journal      = {Artificial Intelligence Review},
  month        = {9},
  number       = {9},
  pages        = {1-76},
  shortjournal = {Artif. Intell. Rev.},
  title        = {Estimation of photovoltaic parameters by dynamic updating and selecting a snake optimizer based on multi-directional optimization},
  volume       = {58},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A survey on cutting-edge relation extraction techniques based on language models. <em>AIR</em>, <em>58</em>(9), 1-58. (<a href='https://doi.org/10.1007/s10462-025-11280-0'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This comprehensive survey examines the latest advancements in Relation Extraction (RE), a pivotal task in natural language processing essential for applications across biomedical, financial, and legal sectors. This study highlights the evolution and current state of RE techniques by analyzing 137 papers presented at the Association for Computational Linguistics (ACL) conferences from 2020 to 2023, focusing on models that leverage language models. Our findings underscore the dominance of BERT-based methods in achieving state-of-the-art results for RE while also noting the promising capabilities of emerging Large Language Models (LLMs) like T5, especially in few-shot relation extraction scenarios where they excel in identifying previously unseen relations.},
  archive      = {J_AIR},
  author       = {Diaz-Garcia, Jose A. and Lopez, Julio Amador Diaz},
  doi          = {10.1007/s10462-025-11280-0},
  journal      = {Artificial Intelligence Review},
  month        = {9},
  number       = {9},
  pages        = {1-58},
  shortjournal = {Artif. Intell. Rev.},
  title        = {A survey on cutting-edge relation extraction techniques based on language models},
  volume       = {58},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). General information metrics for improving AI model training efficiency. <em>AIR</em>, <em>58</em>(9), 1-31. (<a href='https://doi.org/10.1007/s10462-025-11281-z'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To address the growing size of AI model training data and the lack of a universal data selection methodology–factors that significantly drive up training costs–this paper presents the General Information Metrics Evaluation (GIME) method. GIME leverages general information metrics from Objective Information Theory (OIT), including volume, delay, scope, granularity, variety, duration, sampling rate, aggregation, coverage, distortion, and mismatch to optimize dataset selection for training purposes. Comprehensive experiments conducted across diverse domains, such as CTR Prediction, Civil Case Prediction, and Weather Forecasting, demonstrate that GIME effectively preserves model performance while substantially reducing both training time and costs. Additionally, applying GIME within the Judicial AI Program led to a remarkable 39.56% reduction in total model training expenses, underscoring its potential to support efficient and sustainable AI development.},
  archive      = {J_AIR},
  author       = {Xu, Jianfeng and Liu, Congcong and Tan, Xiaoying and Zhu, Xiaojie and Wu, Anpeng and Wan, Huan and Kong, Weijun and Li, Chun and Xu, Hu and Kuang, Kun and Wu, Fei},
  doi          = {10.1007/s10462-025-11281-z},
  journal      = {Artificial Intelligence Review},
  month        = {9},
  number       = {9},
  pages        = {1-31},
  shortjournal = {Artif. Intell. Rev.},
  title        = {General information metrics for improving AI model training efficiency},
  volume       = {58},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Generative AI: A double-edged sword in the cyber threat landscape. <em>AIR</em>, <em>58</em>(9), 1-27. (<a href='https://doi.org/10.1007/s10462-025-11285-9'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Generative AI’s swift progress advances onto profound cybersecurity dilemmas. Its usage by malevolent entities to automate intricate malware creation poses a significant threat, circumventing conventional defensive measures. This paradigm shift enables the generation of polymorphic malware, eluding signature-based detection and facilitating precision-targeted assaults. The democratization of Generative AI exacerbates these threats by extending advanced capabilities to a broader spectrum of malicious actors. A comprehensive examination of AI-generated malware’s prevalence, and its repercussions is imperative to fortify cyber resilience. Such scrutiny informs proactive defense strategies vital for safeguarding digital assets within increasingly interconnected systems. Robust threat intelligence frameworks and AI-centric defensive mechanisms emerge as imperative shields against evolving cyber perils. Addressing this emergent challenge stands as an indispensable endeavor in contemporary cybersecurity discourse.},
  archive      = {J_AIR},
  author       = {Ibrar, Werisha and Mahmood, Danish and Al-Shamayleh, Ahmad Sami and Ahmed, Ghufran and Alharthi, Salman Z. and Akhunzada, Adnan},
  doi          = {10.1007/s10462-025-11285-9},
  journal      = {Artificial Intelligence Review},
  month        = {9},
  number       = {9},
  pages        = {1-27},
  shortjournal = {Artif. Intell. Rev.},
  title        = {Generative AI: A double-edged sword in the cyber threat landscape},
  volume       = {58},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). From concept to manufacturing: Evaluating vision-language models for engineering design. <em>AIR</em>, <em>58</em>(9), 1-75. (<a href='https://doi.org/10.1007/s10462-025-11290-y'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Engineering design is undergoing a transformative shift with the advent of AI, marking a new era in how we approach product, system, and service planning. Large language models have demonstrated impressive capabilities in enabling this shift. Yet, with text as their only input modality, they cannot leverage the large body of visual artifacts that engineers have used for centuries and are accustomed to. This gap is addressed with the release of multimodal vision-language models (VLMs), such as GPT-4V, enabling AI to impact many more types of tasks. Our work presents a comprehensive evaluation of VLMs across a spectrum of engineering design tasks, categorized into four main areas: Conceptual Design, System-Level and Detailed Design, Manufacturing and Inspection, and Engineering Education Tasks. Specifically in this paper, we assess the capabilities of two VLMs, GPT-4V and LLaVA 1.6 34B, in design tasks such as sketch similarity analysis, CAD generation, topology optimization, manufacturability assessment, and engineering textbook problems. Through this structured evaluation, we not only explore VLMs’ proficiency in handling complex design challenges but also identify their limitations in complex engineering design applications. Our research establishes a foundation for future assessments of vision language models. It also contributes a set of benchmark testing datasets, with more than 1000 queries, for ongoing advancements and applications in this field.},
  archive      = {J_AIR},
  author       = {Picard, Cyril and Edwards, Kristen M. and Doris, Anna C. and Man, Brandon and Giannone, Giorgio and Alam, Md Ferdous and Ahmed, Faez},
  doi          = {10.1007/s10462-025-11290-y},
  journal      = {Artificial Intelligence Review},
  month        = {9},
  number       = {9},
  pages        = {1-75},
  shortjournal = {Artif. Intell. Rev.},
  title        = {From concept to manufacturing: Evaluating vision-language models for engineering design},
  volume       = {58},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Adaptive anomaly detection for identifying attacks in cyber-physical systems: A systematic literature review. <em>AIR</em>, <em>58</em>(9), 1-46. (<a href='https://doi.org/10.1007/s10462-025-11292-w'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Modern cyberattacks in cyber-physical systems (CPS) rapidly evolve and cannot be deterred effectively with most current methods, which focus on characterizing past threats. Adaptive anomaly detection (AAD) is among the most promising techniques to detect evolving cyberattacks, with an emphasis on fast data processing and model adaptation. AAD has been researched extensively; however, to the best of our knowledge, our work is the first systematic literature review (SLR) on current research in this field. We present a comprehensive SLR, gathering 397 relevant papers and systematically analyzing 65 of them (47 research and 18 survey papers) on AAD in CPS from 2013 to November 2023. We introduce a novel taxonomy considering attack types, CPS application, learning paradigm, data management, and algorithms. Our findings show that most studies addressed either model adaptation or data processing, but rarely both simultaneously. This indicates a research gap in fully adaptive solutions. We also categorize algorithms, datasets, and attack characteristics, and summarize strengths and weaknesses across the literature. Our review provides a structured and accessible reference for researchers and practitioners, offering insights into key trends and highlighting limitations in current approaches. Finally, we outline several future research directions, including the need for integrated real-time processing and adaptive learning, explainability, and uncertainty quantification in AAD for CPS.},
  archive      = {J_AIR},
  author       = {Moriano, Pablo and Hespeler, Steven C. and Li, Mingyan and Mahbub, Maria},
  doi          = {10.1007/s10462-025-11292-w},
  journal      = {Artificial Intelligence Review},
  month        = {9},
  number       = {9},
  pages        = {1-46},
  shortjournal = {Artif. Intell. Rev.},
  title        = {Adaptive anomaly detection for identifying attacks in cyber-physical systems: A systematic literature review},
  volume       = {58},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Image enhancement framework combining interval-valued intuitionistic fuzzy sets and fractional sobel operator. <em>AIR</em>, <em>58</em>(9), 1-35. (<a href='https://doi.org/10.1007/s10462-025-11294-8'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Image enhancement in low-light conditions is a difficult challenge due to noise, visual impairment and colour distortion. This research describes a new method for improving low-light images utilizing fuzzy-based and fractional approach. Firstly, normalize the low-light image to minimize noise and increase clarity, resulting a fuzzy image. The fuzzy image is then turned into an intuitionistic fuzzy image (IFI), which considers membership and non-membership values, presenting a more accurate characterization of uncertainty in pixel intensities. The IFI eventually transforms into an interval-valued intuitionistic fuzzy image (IVIFI) which captures a broader range of uncertainty. A fractional Sobel mask is then used to convolute the IVIF image, resulting in an improved accurate intensity distribution. The result is an optimally enhanced image with excellent contrast and detail preservation. This proposed method gives highest values of entropy, contrast improvement index, absolute mean brightness error and colourfulness are 7.9017, 6.4658, 125.5033 and 0.2853 respectively. A comparison with existing approaches indicates the proposed method’s superiority in visual quality and quantitative metrics, emphasizing its efficacy in improving low-light images.},
  archive      = {J_AIR},
  author       = {Chinnappan, Ravindar Raj and Sundaram, Dhanasekar},
  doi          = {10.1007/s10462-025-11294-8},
  journal      = {Artificial Intelligence Review},
  month        = {9},
  number       = {9},
  pages        = {1-35},
  shortjournal = {Artif. Intell. Rev.},
  title        = {Image enhancement framework combining interval-valued intuitionistic fuzzy sets and fractional sobel operator},
  volume       = {58},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multi-view learning with graph convolution networks adopting diverse graphs and genuine deep feature fusion. <em>AIR</em>, <em>58</em>(9), 1-27. (<a href='https://doi.org/10.1007/s10462-025-11301-y'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-view data significantly enhances the accuracy of machine learning algorithms by providing a comprehensive representation of object features. Despite their potential, research on the use of Graph Convolutional Networks (GCNs) for processing node connectivity and data features remains limited. Existing methods mainly focus on weighted summation of graph matrices, with only a few approaches effectively integrating the feature information into the graph structures. To overcome these limitations, this paper proposes a novel deep learning architecture: the Feature Fusion and Multi-Graph Fusion Learning Framework (MGCN-FN). The framework consists of two core modules: Feature Fusion Network (FFN): Designed to extract and consolidate key features from multiple views. Multi-Graph Fusion Network (MGFN): Constructs multiple graphs for each view and jointly optimizes both the graph weights and the GCN model. Extensive experiments on various multi-view datasets show that MGCN-FN achieves superior performance compared to state-of-the-art methods, especially on semi-supervised multi-view classification tasks.},
  archive      = {J_AIR},
  author       = {Peng, Guowen and Dornaika, Fadi and Charafeddine, Jinan},
  doi          = {10.1007/s10462-025-11301-y},
  journal      = {Artificial Intelligence Review},
  month        = {9},
  number       = {9},
  pages        = {1-27},
  shortjournal = {Artif. Intell. Rev.},
  title        = {Multi-view learning with graph convolution networks adopting diverse graphs and genuine deep feature fusion},
  volume       = {58},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Deep learning for pancreas segmentation on computed tomography: A systematic review. <em>AIR</em>, <em>58</em>(8), 1-109. (<a href='https://doi.org/10.1007/s10462-024-11050-4'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Pancreas segmentation has been traditionally challenging due to its small size in computed tomography abdominal volumes, high variability of shape and positions among patients, and blurred boundaries due to low contrast between the pancreas and surrounding organs. Many deep learning models for pancreas segmentation have been proposed in the past few years. We present a thorough systematic review based on the Preferred Reporting Items for Systematic Reviews and Meta-analyses statement. The literature search was conducted on PubMed, Web of Science, Scopus, and IEEE Xplore on original studies published in peer-reviewed journals from 2013 to 2023. Overall, 130 studies were retrieved. We initially provide an overview of the technical background of the most common network architectures and publicly available datasets. Then, the analysis of the studies combining visual presentation in tabular form and text description is reported. The tables group the studies specifying the application, dataset size, design (model architecture, learning strategy, loss function, and training protocol), results, and main contributions. We first analyze the studies focusing on parenchyma segmentation using datasets with only pancreas annotations, followed by those using datasets with multi-organ annotations. Then, we analyze the studies on the segmentation of tumors, cysts, and inflammation. The studies are clustered according to the different deep learning architectures. Finally, we discuss the main findings from the published literature, the challenges, and the directions for future research on the clinical need, deep learning and foundation models, datasets, and clinical translation.},
  archive      = {J_AIR},
  author       = {Moglia, Andrea and Cavicchioli, Matteo and Mainardi, Luca and Cerveri, Pietro},
  doi          = {10.1007/s10462-024-11050-4},
  journal      = {Artificial Intelligence Review},
  month        = {8},
  number       = {8},
  pages        = {1-109},
  shortjournal = {Artif. Intell. Rev.},
  title        = {Deep learning for pancreas segmentation on computed tomography: A systematic review},
  volume       = {58},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Classification of animal species via deep neural networks and species distribution modeling: A systematic review. <em>AIR</em>, <em>58</em>(8), 1-35. (<a href='https://doi.org/10.1007/s10462-024-11074-w'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The automated classification of animal species is a challenging task of great ecological importance, which is usually carried out through species distribution modeling (SDM), relying on animal locations and their environment, and/or through image classification from animal photos. On the one hand, there is the well-known SDM, used mainly to estimate the existence of species in certain regions and their ideal conditions. On the other hand, the use of deep neural networks is gaining popularity in ecology, with substantial use in animal identification from photos. A more recent trend is to combine both approaches to improve the final accuracy, from simple to sophisticated combination strategies. This review focuses on works that combine animal image classification models through deep learning with animal SDMs. We obtained 728 articles from the literature, from which we selected and synthesized 13 studies related to the simultaneous use of deep learning and ecological modeling of species in the context of environmental conservation. Thus, we present a summary of applications that integrate deep learning in ecology and SDMs and discuss their limitations and challenges to overcome them.},
  archive      = {J_AIR},
  author       = {Oliveira, Mateus Braga and Bernardino, Heder Soares and Vieira, Alex Borges and Augusto, Douglas A.},
  doi          = {10.1007/s10462-024-11074-w},
  journal      = {Artificial Intelligence Review},
  month        = {8},
  number       = {8},
  pages        = {1-35},
  shortjournal = {Artif. Intell. Rev.},
  title        = {Classification of animal species via deep neural networks and species distribution modeling: A systematic review},
  volume       = {58},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Adversarial machine learning: A review of methods, tools, and critical industry sectors. <em>AIR</em>, <em>58</em>(8), 1-87. (<a href='https://doi.org/10.1007/s10462-025-11147-4'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The rapid advancement of Artificial Intelligence (AI), particularly Machine Learning (ML) and Deep Learning (DL), has produced high-performance models widely used in various applications, ranging from image recognition and chatbots to autonomous driving and smart grid systems. However, security threats arise from the vulnerabilities of ML models to adversarial attacks and data poisoning, posing risks such as system malfunctions and decision errors. Meanwhile, data privacy concerns arise, especially with personal data being used in model training, which can lead to data breaches. This paper surveys the Adversarial Machine Learning (AML) landscape in modern AI systems, while focusing on the dual aspects of robustness and privacy. Initially, we explore adversarial attacks and defenses using comprehensive taxonomies. Subsequently, we investigate robustness benchmarks alongside open-source AML technologies and software tools that ML system stakeholders can use to develop robust AI systems. Lastly, we delve into the landscape of AML in four industry fields –automotive, digital healthcare, electrical power and energy systems (EPES), and Large Language Model (LLM)-based Natural Language Processing (NLP) systems– analyzing attacks, defenses, and evaluation concepts, thereby offering a holistic view of the modern AI-reliant industry and promoting enhanced ML robustness and privacy preservation in the future.},
  archive      = {J_AIR},
  author       = {Pelekis, Sotiris and Koutroubas, Thanos and Blika, Afroditi and Berdelis, Anastasis and Karakolis, Evangelos and Ntanos, Christos and Spiliotis, Evangelos and Askounis, Dimitris},
  doi          = {10.1007/s10462-025-11147-4},
  journal      = {Artificial Intelligence Review},
  month        = {8},
  number       = {8},
  pages        = {1-87},
  shortjournal = {Artif. Intell. Rev.},
  title        = {Adversarial machine learning: A review of methods, tools, and critical industry sectors},
  volume       = {58},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Exploring privacy mechanisms and metrics in federated learning. <em>AIR</em>, <em>58</em>(8), 1-51. (<a href='https://doi.org/10.1007/s10462-025-11170-5'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The federated learning (FL) principle ensures multiple clients jointly develop a machine learning model without exchanging their local data. Various government enacted prohibition policies on data exchange between organizations have led to the need for privacy-preserved federated learning. Many industries have cultivated this idea of model development through federated learning to enhance performance and accuracy. This paper offers a detailed overview of the background of FL, highlighting existing aggregation algorithms, frameworks, implementation aspects, and dataset repositories, establishing itself as an essential reference for researchers in the field. The paper thoroughly reviews existing centralized and decentralized FL approaches proposed in the literature and gives an overview about the methodology, privacy techniques implemented and limitations to guide other researchers to advance their research in the field of federated learning. The paper discusses the critical role of privacy-enhancing technologies like differential privacy (DP), homomorphic encryption (HE), and secure multiparty computation (SMPC) in federated learning highlighting their effectiveness in safeguarding sensitive data while optimizing the balance between privacy, communication efficiency, and computational cost. The paper explores the applications of federated learning in privacy-sensitive areas like natural language processing (NLP), healthcare, and Internet of Things (IoT) with edge computing. We believe our work provides a novel addition by identifying privacy evaluation metrics and spotlighting the measures in terms of data privacy and correctness, communication cost, computational cost and scalability. Furthermore, it identifies emerging challenges and suggests promising research directions in the federated learning domain.},
  archive      = {J_AIR},
  author       = {Shenoy, Dhanya and Bhat, Radhakrishna and Krishna Prakasha, K},
  doi          = {10.1007/s10462-025-11170-5},
  journal      = {Artificial Intelligence Review},
  month        = {8},
  number       = {8},
  pages        = {1-51},
  shortjournal = {Artif. Intell. Rev.},
  title        = {Exploring privacy mechanisms and metrics in federated learning},
  volume       = {58},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Review of AI-assisted design of low-carbon cost-effective concrete toward carbon neutrality. <em>AIR</em>, <em>58</em>(8), 1-47. (<a href='https://doi.org/10.1007/s10462-025-11182-1'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Decarbonizing concrete production is a critical step toward achieving carbon neutrality by 2050. This paper highlights the advancements in artificial intelligence-assisted design of low-carbon cost-effective concrete, focusing on integrating machine learning-based property prediction with multi-objective optimization. Data collection and processing techniques, such as automatic data extraction, artificial data generation, and anomaly detection, are first discussed to address the importance of dataset quality. Strategies that capture physicochemical information of ingredients, including by-product supplementary cementitious materials and recycled aggregates, are then examined to enhance model generalizability. Various machine learning models—from individual regression approaches to heterogeneous ensemble methods—are compared for their predictive accuracy and robustness. Methods for hyperparameter tuning, model evaluation, and interpretation to ensure reliable and interpretable predictions are reviewed. Design optimization approaches are then highlighted, ranging from grid/random searches to more sophisticated gradient-based and metaheuristic algorithms, aimed at minimizing carbon footprint, embodied energy, and cost. Future research avenues encompass (1) application-specific design frameworks that integrate critical objectives—mechanical performance, durability, fresh-state behavior, and time-dependent properties such as creep and shrinkage—tailored to specific structural and environmental requirements; (2) holistic design optimization that simultaneously refines mixture design and structural parameters; and (3) probabilistic approaches to systematically manage uncertainties in materials, structural performance, and loading conditions systematically.},
  archive      = {J_AIR},
  author       = {Mahjoubi, Soroush and Barhemat, Rojyar and Meng, Weina and Bao, Yi},
  doi          = {10.1007/s10462-025-11182-1},
  journal      = {Artificial Intelligence Review},
  month        = {8},
  number       = {8},
  pages        = {1-47},
  shortjournal = {Artif. Intell. Rev.},
  title        = {Review of AI-assisted design of low-carbon cost-effective concrete toward carbon neutrality},
  volume       = {58},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Evaluating AI-powered predictive solutions for MRI in lumbar spinal stenosis: A systematic review. <em>AIR</em>, <em>58</em>(8), 1-78. (<a href='https://doi.org/10.1007/s10462-025-11185-y'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Lumbar spinal stenosis (LSS) involves the narrowing of the spinal canal, leading to compression of the spinal cord and nerves in the lower back. Common causes include injuries, degenerative age-related changes, congenital conditions, and tumors, all of which contribute to back pain. Early diagnosis is critical for symptom management, preventing progression, and preserving quality of life. This study systematically reviews AI-based approaches for predicting LSS using MRI axial and sagittal imaging. The review focuses on various AI tasks: detection, segmentation, classification, hybrid approaches, spinal index measurements (SIM), and explainable AI frameworks. The aim is to highlight current knowledge, identify limitations in existing models, and propose future research directions. Following PRISMA guidelines and the PICO method (Population, Intervention, Comparison, Outcome), the review collects data from databases like PubMed, Web of Science, ScienceDirect, and IEEE Xplore (2005–2024). The Rayyan AI tool is used for duplicate removal and screening. The screening process includes an initial review of titles and abstracts, followed by full-text appraisal. The Meta Quality Appraisal Tool (MetaQAT) assesses the quality of selected articles. Of 1323 records, 97 duplicates were removed. After screening, 895 records were excluded, leaving 331 for full-text review. Among these, 184 articles were excluded for lacking AI relevance. Ultimately, 95 key articles (91 technical papers and 4 reviews) were identified for their contributions to AI-based LSS prediction. This review provides a comprehensive analysis of AI techniques in LSS prediction, guiding future research and advancing understanding in areas like explainable AI and large language models (LLMs).},
  archive      = {J_AIR},
  author       = {Al-antari, Mugahed A. and Salem, Saied and Raza, Mukhlis and Elbadawy, Ahmed S. and Bütün, Ertan and Aydin, Ahmet Arif and Aydoğan, Murat and Ertuğrul, Bilal and Talo, Muhammed and Gu, Yeong Hyeon},
  doi          = {10.1007/s10462-025-11185-y},
  journal      = {Artificial Intelligence Review},
  month        = {8},
  number       = {8},
  pages        = {1-78},
  shortjournal = {Artif. Intell. Rev.},
  title        = {Evaluating AI-powered predictive solutions for MRI in lumbar spinal stenosis: A systematic review},
  volume       = {58},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Overlapping community-based fair influence maximization in social networks under open-source development model algorithm. <em>AIR</em>, <em>58</em>(8), 1-31. (<a href='https://doi.org/10.1007/s10462-025-11210-0'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The aim of Influence Maximization (IM) in social networks is to identify an optimal subset of users to maximize the spread of influence across the network. Fair Influence Maximization (FIM) develops the IM problem with the aim of equitable distribution of influence across communities and enhancing the fair propagation of information. Among the solutions for FIM, community-based techniques enhance performance by effectively capturing the structural properties and ensuring a more equitable influence spread. However, these techniques often ignore the overlapping nature of communities and suffer from a trade-off between complexity and fairness. With this motivation, this study handles the FIM based on Overlapping Community detection under optimization algorithms (FIMOC). FIMOC includes an overlapping community detection approach that can consider the importance of influential overlapping nodes in communities. Meanwhile, FIMOC uses a non-overlapping and overlapping node selection module based on communities to identify potential candidate nodes. Subsequently, FIMOC uses the Open-Source Development Model Algorithm (ODMA) as an optimization algorithm to identify the set of influential nodes. Our method considers the dynamic and overlapping nature of social communities, ensuring that the influence spread is not only maximized but also equitably distributed across diverse groups. By leveraging real‐world social networks, we demonstrate the effectiveness of our method compared to state-of-the-art methods through extensive experiments. The results show that our method achieves a more balanced influence spread, providing a fairer solution, while also enhancing the overall reach of information dissemination.},
  archive      = {J_AIR},
  author       = {Wei, Pengcheng and Yan, Bei and Huang, Sixing and Zhou, ZhiHong},
  doi          = {10.1007/s10462-025-11210-0},
  journal      = {Artificial Intelligence Review},
  month        = {8},
  number       = {8},
  pages        = {1-31},
  shortjournal = {Artif. Intell. Rev.},
  title        = {Overlapping community-based fair influence maximization in social networks under open-source development model algorithm},
  volume       = {58},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Model-agnostic explainable artificial intelligence methods in finance: A systematic review, recent developments, limitations, challenges and future directions. <em>AIR</em>, <em>58</em>(8), 1-65. (<a href='https://doi.org/10.1007/s10462-025-11215-9'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The increasing integration of Artificial Intelligence (AI) and Machine Learning (ML)—algorithms that enable computers to identify patterns from data—in financial applications has significantly improved predictive capabilities in areas such as credit scoring, fraud detection, portfolio management, and risk assessment. Despite these advancements, the opaque, “black box” nature of many AI and ML models raises critical concerns related to transparency, trust, and regulatory compliance. Explainable Artificial Intelligence (XAI) aims to address these issues by providing interpretable and transparent decision-making processes. This study systematically reviews Model-Agnostic Explainable AI techniques, which can be applied across different types of ML models in finance, to evaluate their effectiveness, scalability, and practical applicability. Through analysis of 150 peer-reviewed studies, the paper identifies key challenges, such as balancing interpretability with predictive accuracy, managing computational complexity, and meeting regulatory requirements. The review highlights emerging trends toward hybrid models that combine powerful ML algorithms with interpretability techniques, real-time explanations suitable for dynamic financial markets, and XAI frameworks explicitly designed to align with regulatory standards. The study concludes by outlining specific future research directions, including the development of computationally efficient explainability methods, regulatory-compliant frameworks, and ethical AI solutions to ensure transparent and accountable financial decision-making.},
  archive      = {J_AIR},
  author       = {Khan, Farhina Sardar and Mazhar, Syed Shahid and Mazhar, Kashif and A. AlSaleh, Dhoha and Mazhar, Amir},
  doi          = {10.1007/s10462-025-11215-9},
  journal      = {Artificial Intelligence Review},
  month        = {8},
  number       = {8},
  pages        = {1-65},
  shortjournal = {Artif. Intell. Rev.},
  title        = {Model-agnostic explainable artificial intelligence methods in finance: A systematic review, recent developments, limitations, challenges and future directions},
  volume       = {58},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A review on clustering algorithms for spatiotemporal seismicity analysis. <em>AIR</em>, <em>58</em>(8), 1-61. (<a href='https://doi.org/10.1007/s10462-025-11229-3'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Spatiotemporal seismicity analysis has been conducted for a long time, yet significant effort is still needed to mitigate the adverse effects of earthquakes. Seismicity analysis also encompasses fundamental research into seismic patterns, for understanding the frequency, magnitude, temporal and spatial distribution of seismic events. Over the past few decades, it has been carried out through empirical relations, physics-based approaches, stochastic modeling, various machine learning algorithms, and deep learning algorithms for any given seismically active region. Clustering is an essential aspect of seismicity analysis, making it more complex, difficult, and challenging due to significant deviation from the stochastic phenomenon. In this paper, a comprehensive review of all potential data-driven earthquake clustering algorithms, models, and mechanisms are encapsulated for a variety of applications in seismology. The paper also describes the importance of an earthquake catalog with a short review of the fundamental empirical laws frequently used in statistical seismology. This paper also highlights the problem of seismicity declustering and reviews all the available algorithms to deal with it.},
  archive      = {J_AIR},
  author       = {Vijay, Rahul Kumar and Nanda, Satyasai Jagannath and Sharma, Ashish},
  doi          = {10.1007/s10462-025-11229-3},
  journal      = {Artificial Intelligence Review},
  month        = {8},
  number       = {8},
  pages        = {1-61},
  shortjournal = {Artif. Intell. Rev.},
  title        = {A review on clustering algorithms for spatiotemporal seismicity analysis},
  volume       = {58},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Soft voting ensemble classifier for liquefaction prediction based on SPT data. <em>AIR</em>, <em>58</em>(8), 1-27. (<a href='https://doi.org/10.1007/s10462-025-11230-w'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Soil liquefaction, caused by increased porewater pressure, is a significant risk in seismically active areas, impacting infrastructure stability and challenging liquefaction forecasting due to intricate nonlinear interactions. This study proposes a soft voting ensemble classifier (SVEC) that integrates CatBoost Classifier (CBC), Random Forest Classifier (RFC), and Gradient Boost Classifiers (GBC) to predict liquefaction using Standard Penetration Test (SPT) data. The dataset of 540 soil and seismic parameters was utilized to develop SVCE. The dataset incorporates depth, SPT-N60 values, Fine Content of soils (FC), Ground Water Table (GWT), Effective Stresses of Overburden (ESO), Total Stresses of Overburden (TSO), Earthquake magnitude (Mw), and Peak Ground Acceleration (PGA), as input factors for liquefaction prediction. The proposed model was evaluated through performance metrics (Accuracy, Recall, Precision, and F1-score), confusion matrix, sensitivity analysis, feature importance, and Shapley additive explanation (SHAP) analysis. SHAP improves the reliability of ensemble techniques in liquefaction analysis by highlighting the most critical input features, such as PGA, SPT-N60, FC, and GWT. tenfold cross-validation and precision-recall curve confirms the SVEC model’s robustness, achieving a high accuracy of 99.38% in accurately predicting liquefaction.},
  archive      = {J_AIR},
  author       = {Chithuloori, Pravallika and Kim, Jin-Man},
  doi          = {10.1007/s10462-025-11230-w},
  journal      = {Artificial Intelligence Review},
  month        = {8},
  number       = {8},
  pages        = {1-27},
  shortjournal = {Artif. Intell. Rev.},
  title        = {Soft voting ensemble classifier for liquefaction prediction based on SPT data},
  volume       = {58},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Refined causal graph structure learning via curvature for brain disease classification. <em>AIR</em>, <em>58</em>(8), 1-21. (<a href='https://doi.org/10.1007/s10462-025-11231-9'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Graph neural networks (GNNs) have been developed to model the relationship between regions of interest (ROIs) in brains and have shown significant improvement in detecting brain diseases. However, most of these frameworks do not consider the intrinsic relationship of causality factor between brain ROIs, which is arguably more essential to observe cause and effect interaction between signals rather than typical correlation values. We propose a novel framework called Causal Graphs for Brains (CGB) for brain disease classification/detection, which models refined brain networks based on the causal discovery method, transfer entropy, and geometric curvature strategy. CGB unveils causal relationships between ROIs that bring vital information to enhance brain disease classification performance. Furthermore, CGB also performs a graph rewiring through a geometric curvature strategy to refine the generated causal graph to become more expressive and reduce potential information bottlenecks when GNNs model it. Our extensive experiments show that CGB outperforms state-of-the-art methods in classification tasks on brain disease datasets, as measured by average F1 scores.},
  archive      = {J_AIR},
  author       = {Febrinanto, Falih Gozi and Simango, Adonia and Xu, Chengpei and Zhou, Jingjing and Ma, Jiangang and Tyagi, Sonika and Xia, Feng},
  doi          = {10.1007/s10462-025-11231-9},
  journal      = {Artificial Intelligence Review},
  month        = {8},
  number       = {8},
  pages        = {1-21},
  shortjournal = {Artif. Intell. Rev.},
  title        = {Refined causal graph structure learning via curvature for brain disease classification},
  volume       = {58},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Parameter-efficient fine-tuning in large language models: A survey of methodologies. <em>AIR</em>, <em>58</em>(8), 1-64. (<a href='https://doi.org/10.1007/s10462-025-11236-4'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The large language models, as predicted by scaling law forecasts, have made groundbreaking progress in many fields, particularly in natural language generation tasks, where they have approached or even surpassed human levels. However, the unprecedented scale of their parameters brings significant computational and storage costs. These large language models require substantial computational resources and GPU memory to operate. When adapting large language models to specific downstream tasks, their massive parameter scale poses a significant challenge in fine-tuning on hardware platforms with limited computational power and GPU memory. To address this issue, parameter-efficient fine-tuning (PEFT) offers a practical solution by efficiently adjusting the parameters of large pre-trained models to suit various downstream tasks. Specifically, PEFT adjusts the parameters of pre-trained large language models to adapt to specific tasks or domains, minimizing the introduction of additional parameters and the computational resources required. This review mainly introduces the preliminary knowledge of PEFT, the core ideas and principles of various PEFT algorithms, the applications of PEFT, and potential future research directions. By reading this review, we believe that interested parties can quickly grasp the PEFT methodology, thereby accelerating its development and innovation.},
  archive      = {J_AIR},
  author       = {Wang, Luping and Chen, Sheng and Jiang, Linnan and Pan, Shu and Cai, Runze and Yang, Sen and Yang, Fei},
  doi          = {10.1007/s10462-025-11236-4},
  journal      = {Artificial Intelligence Review},
  month        = {8},
  number       = {8},
  pages        = {1-64},
  shortjournal = {Artif. Intell. Rev.},
  title        = {Parameter-efficient fine-tuning in large language models: A survey of methodologies},
  volume       = {58},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Transformer-enhanced hierarchical encoding with multi-decoder for diversified MCQ distractor generation. <em>AIR</em>, <em>58</em>(8), 1-24. (<a href='https://doi.org/10.1007/s10462-025-11237-3'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The validity of multiple-choice questions (MCQs) in reading comprehension assessments relies heavily on the quality of the distractors. However, the manual design of these distractors is both time-consuming and costly, prompting researchers to turn to computer technology for the automatic generation of distractors. This task involves the process of taking a reading comprehension article, a question and its corresponding correct answer as input, with the goal of generating distractors that are related to the answer, semantically consistent with the question, and traceable within the article. Initially, heuristic rule-based approaches were employed, to generate only word-level or phrase-level distractors. Recent studies have shifted towards using sequence-to-sequence neural networks for sentence-level distractor generation. Despite these advancements, these methods face two key challenges: difficulty in capturing long-distance semantic relationships within the context, leading to overly general or context-independent distractors, and the tendency for the generated distractors to be semantically similar. To address these limitations, this paper proposes a Transformer-Enhanced Hierarchical Encoding with Multi-Decoder (THE-MD) network, composed of a hierarchical encoder and multiple decoders. Specifically, the encoder employs the Transformer architecture to encode the context and capture long-range semantic information, thereby generating more contextually relevant distractors. The decoder utilizes multiple decoding strategies and a dissimilarity loss function to collaboratively generate diverse distractors. The experimental results show that the THE-MD model outperforms existing baselines on both automatic and manual evaluation metrics. On the RACE and RACE++ datasets, the model increased the BLEU-4 scores to 7.45 and 10.60, and the ROUGE-L scores to 22.96 and 34.88, while also demonstrating excellent performance in fluency and coherence metrics. These improvements highlight their potential to enhance the generation of MCQ distractors in educational assessments.},
  archive      = {J_AIR},
  author       = {Dong, Xiaohui and Li, Zhengluo and Su, Haoming and Xue, Jixiang and Dang, Xiaochao},
  doi          = {10.1007/s10462-025-11237-3},
  journal      = {Artificial Intelligence Review},
  month        = {8},
  number       = {8},
  pages        = {1-24},
  shortjournal = {Artif. Intell. Rev.},
  title        = {Transformer-enhanced hierarchical encoding with multi-decoder for diversified MCQ distractor generation},
  volume       = {58},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multi-object tracking review: Retrospective and emerging trend. <em>AIR</em>, <em>58</em>(8), 1-46. (<a href='https://doi.org/10.1007/s10462-025-11212-y'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-object tracking (MOT) is a critical task involving detecting and continuously tracking multiple objects within a video sequence. It is widely used in various fields, such as autonomous driving and intelligent security. In recent years, deep learning architectures have effectively promoted the development of MOT. However, this task poses significant challenges regarding accuracy due to occlusion/truncation, light variation, camera movement. Researchers have proposed many methods to address these issues to reduce trajectory fragmentation, identity switches, and missing targets. To better understand these advancements, it is essential to categorize the approaches based on their methodologies. This article reviewed the recent development of MOT, divided into Tracking by Detection (TBD) and End-to-End (E2E). By introducing and comparing the two types of tracking algorithms, readers can quickly understand the current development status of MOT. Meanwhile, this review summarizes the links to open-source code of excellent algorithms and common benchmark datasets in the appendix. And provide a unified MOT toolkit that includes evaluation and visualization at https://github.com/guanzhiyu817/MOT-tools . In addition, this review discusses the future directions of MOT, specifically cross-modal reasoning.},
  archive      = {J_AIR},
  author       = {Guan, Zhiyu and Wang, Zhaofa and Zhang, Gan and Li, Luwei and Zhang, Miaomiao and Shi, Zhiping and Jiang, Na},
  doi          = {10.1007/s10462-025-11212-y},
  journal      = {Artificial Intelligence Review},
  month        = {8},
  number       = {8},
  pages        = {1-46},
  shortjournal = {Artif. Intell. Rev.},
  title        = {Multi-object tracking review: Retrospective and emerging trend},
  volume       = {58},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Machine learning innovations in CPR: A comprehensive survey on enhanced resuscitation techniques. <em>AIR</em>, <em>58</em>(8), 1-41. (<a href='https://doi.org/10.1007/s10462-025-11214-w'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This survey paper explores the transformative role of Machine Learning (ML) and Artificial Intelligence (AI) in Cardiopulmonary Resuscitation (CPR), marking a paradigm shift from conventional, manually driven resuscitation practices to intelligent, data-driven interventions. It examines the evolution of CPR through the lens of predictive modeling, AI-enhanced devices, and real-time decision-making tools that collectively aim to improve resuscitation outcomes and survival rates. Unlike prior surveys that either focus solely on traditional CPR methods or offer general insights into ML applications in healthcare, this work provides a novel interdisciplinary synthesis tailored specifically to the domain of CPR. It presents a comprehensive taxonomy that classifies ML techniques into four key CPR-related tasks: rhythm analysis, outcome prediction, non-invasive blood pressure and chest compression modeling, and real-time detection of pulse and Return of Spontaneous Circulation (ROSC). The paper critically evaluates emerging ML approaches-including Reinforcement Learning (RL) and transformer-based models-while also addressing real-world implementation barriers such as model interpretability, data limitations, and deployment in high-stakes clinical settings. Furthermore, it highlights the role of eXplainable AI (XAI) in fostering clinical trust and adoption. By bridging the gap between resuscitation science and advanced ML techniques, this survey establishes a structured foundation for future research and practical innovation in ML-enhanced CPR. It offers clear insights, identifies unexplored opportunities, and sets a forward-looking research agenda identifying emerging trends and practical implementation challenges aiming to improve both the reliability and effectiveness of CPR in real-world emergencies.},
  archive      = {J_AIR},
  author       = {Islam, Saidul and Rjoub, Gaith and Elmekki, Hanae and Bentahar, Jamal and Pedrycz, Witold and Cohen, Robin},
  doi          = {10.1007/s10462-025-11214-w},
  journal      = {Artificial Intelligence Review},
  month        = {8},
  number       = {8},
  pages        = {1-41},
  shortjournal = {Artif. Intell. Rev.},
  title        = {Machine learning innovations in CPR: A comprehensive survey on enhanced resuscitation techniques},
  volume       = {58},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Generative AI revolution in cybersecurity: A comprehensive review of threat intelligence and operations. <em>AIR</em>, <em>58</em>(8), 1-39. (<a href='https://doi.org/10.1007/s10462-025-11219-5'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cyber threats are increasingly frequent in today’s world, posing challenges for organizations and individuals to protect their data from cybercriminals. On the other hand, Generative Artificial Intelligence (GAI) technology offers an efficient way to automatically address these issues with the help of AI models and algorithms. It can work on more critical security aspects where human intervention is required and handle everyday threat situations autonomously. This research paper explores GAI in enhancing cybersecurity by leveraging AI Models and algorithms. GAI can autonomously address common security issues, detect novel threats, and augment human intervention in critical security aspects. Moreover, this research study also highlights autonomous security enhancements, improved security posture against emerging threats, anomaly detection, and threat response. Besides this, we have discussed the GAI limitations, such as occasional incorrect results, expensive training, and the potential for misuse by malicious actors for illegal activities. This research study also provides valuable insights into the balanced adoption of GAI in cybersecurity, ensuring effective threat migration without compromising system integrity.},
  archive      = {J_AIR},
  author       = {Uddin, Mueen and Irshad, Muhammad Saad and Kandhro, Irfan Ali and Alanazi, Fuhid and Ahmed, Fahad and Maaz, Muhammad and Hussain, Saddam and Ullah, Syed Sajid},
  doi          = {10.1007/s10462-025-11219-5},
  journal      = {Artificial Intelligence Review},
  month        = {8},
  number       = {8},
  pages        = {1-39},
  shortjournal = {Artif. Intell. Rev.},
  title        = {Generative AI revolution in cybersecurity: A comprehensive review of threat intelligence and operations},
  volume       = {58},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Advanced unsupervised learning: A comprehensive overview of multi-view clustering techniques. <em>AIR</em>, <em>58</em>(8), 1-52. (<a href='https://doi.org/10.1007/s10462-025-11240-8'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Machine learning techniques face numerous challenges to achieve optimal performance. These include computational constraints, the limitations of single-view learning algorithms and the complexity of processing large datasets from different domains, sources or views. In this context, multi-view clustering (MVC), a class of unsupervised multi-view learning, emerges as a powerful approach to overcome these challenges. MVC compensates for the shortcomings of single-view methods and provides a richer data representation and effective solutions for a variety of unsupervised learning tasks. In contrast to traditional single-view approaches, the semantically rich nature of multi-view data increases its practical utility despite its inherent complexity. This survey makes a threefold contribution: (1) a systematic categorization of multi-view clustering methods into well-defined groups, including co-training, co-regularization, subspace, deep learning, kernel-based, anchor-based, and graph-based strategies; (2) an in-depth analysis of their respective strengths, weaknesses, and practical challenges, such as scalability and incomplete data; and (3) a forward-looking discussion of emerging trends, interdisciplinary applications, and future directions in MVC research. This study represents an extensive workload, encompassing the review of over 140 foundational and recent publications, the development of comparative insights on integration strategies such as early fusion, late fusion, and joint learning, and the structured investigation of practical use cases in the areas of healthcare, multimedia, and social network analysis. By integrating these efforts, this work aims to fill existing gaps in MVC research and provide actionable insights for the advancement of the field.},
  archive      = {J_AIR},
  author       = {Moujahid, Abdelmalik and Dornaika, Fadi},
  doi          = {10.1007/s10462-025-11240-8},
  journal      = {Artificial Intelligence Review},
  month        = {8},
  number       = {8},
  pages        = {1-52},
  shortjournal = {Artif. Intell. Rev.},
  title        = {Advanced unsupervised learning: A comprehensive overview of multi-view clustering techniques},
  volume       = {58},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Rethinking and recomputing the value of machine learning models. <em>AIR</em>, <em>58</em>(8), 1-23. (<a href='https://doi.org/10.1007/s10462-025-11242-6'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we argue that the prevailing approach to training and evaluating machine learning models often fails to consider their real-world application within organizational or societal contexts, where they are intended to create beneficial value for people. We propose a shift in perspective, redefining model assessment and selection to emphasize integration into workflows that combine machine predictions with human expertise, particularly in scenarios requiring human intervention for low-confidence predictions. Traditional metrics like accuracy and f-score fail to capture the beneficial value of models in such hybrid settings. To address this, we introduce a simple yet theoretically sound “value” metric that incorporates task-specific costs for correct predictions, errors, and rejections, offering a practical framework for real-world evaluation. Through extensive experiments, we show that existing metrics fail to capture real-world needs, often leading to suboptimal choices in terms of value when used to rank classifiers. Furthermore, we emphasize the critical role of calibration in determining model value, showing that simple, well-calibrated models can often outperform more complex models that are challenging to calibrate.},
  archive      = {J_AIR},
  author       = {Sayin, Burcu and Yang, Jie and Chen, Xinyue and Passerini, Andrea and Casati, Fabio},
  doi          = {10.1007/s10462-025-11242-6},
  journal      = {Artificial Intelligence Review},
  month        = {8},
  number       = {8},
  pages        = {1-23},
  shortjournal = {Artif. Intell. Rev.},
  title        = {Rethinking and recomputing the value of machine learning models},
  volume       = {58},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). ED-filter: Dynamic feature filtering for eating disorder classification. <em>AIR</em>, <em>58</em>(8), 1-28. (<a href='https://doi.org/10.1007/s10462-025-11244-4'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Eating disorders (ED) are critical psychiatric problems that have alarmed the mental health community. Mental health professionals are increasingly recognizing the utility of data derived from social media platforms such as Twitter. However, high dimensionality and extensive feature sets of Twitter data present remarkable challenges for ED classification. To overcome these hurdles, we introduce a novel method, an informed branch and bound search technique known as ED-Filter. This strategy significantly improves the drawbacks of conventional feature selection algorithms such as filters and wrappers. ED-Filter iteratively identifies an optimal set of promising features that maximize the eating disorder classification accuracy. In order to adapt to the dynamic nature of Twitter ED data, we enhance the ED-Filter with a hybrid greedy-based deep learning algorithm. This algorithm swiftly identifies sub-optimal features to accommodate the ever-evolving data landscape. Experimental results on Twitter eating disorder data affirm the effectiveness and efficiency of ED-Filter. The method demonstrates significant improvements in classification accuracy and proves its value in eating disorder detection on social media platforms.},
  archive      = {J_AIR},
  author       = {Naseriparsa, Mehdi and Sukunesan, Suku and Cai, Zhen and Alfarraj, Osama and Tolba, Amr and Fathi Rabooki, Saba and Xia, Feng},
  doi          = {10.1007/s10462-025-11244-4},
  journal      = {Artificial Intelligence Review},
  month        = {8},
  number       = {8},
  pages        = {1-28},
  shortjournal = {Artif. Intell. Rev.},
  title        = {ED-filter: Dynamic feature filtering for eating disorder classification},
  volume       = {58},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Machine and deep learning for personality traits detection: A comprehensive survey and open research challenges. <em>AIR</em>, <em>58</em>(8), 1-57. (<a href='https://doi.org/10.1007/s10462-025-11245-3'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Natural language processing (NLP), a prominent research domain of Artificial Intelligence (AI), analyzes users’ generated content on social media for various purposes like sentiment analysis, text summarization, chatbots, fake news detection, etc. Recent advancements in NLP have helped for analysis of human behavior analysis and predicting various human personality traits. Understanding personality traits has long been a fundamental pursuit in psychology and cognitive sciences due to its vast applications for understanding from individuals to social dynamics. Due to online social platforms where people express their views, experiences and comments, NLP is applied for users’ behavior and personality analysis, which is helpful in defining marketing strategies, consumers’ behavior analysis, team building, etc. This research study provides a comprehensive overview of existing methodologies, applications, and challenges in the field of personality traits detection using shallow machine learning, ensemble learning and deep learning. To conduct this study, recent research publications relevant to NLP for this new but emerging research domain are reviewed. The background knowledge of personality models of various nature is discussed for better domain understanding. The study encompasses machine learning and deep learning models with thorough analysis of traditional and innovative techniques including ensemble learning and transformer-based models in chronological order highlighting the trend analysis showing evolution of application of advanced methods. The review also presents and compares the widely used datasets which may guide the researchers for selection of datasets in future studies. Performance evaluation metrics have been discussed which are used in the relevant literature. Furthermore, it explores the application of research of personality traits detection in various domains highlighting its significance. We have also carried out extensive empirical analysis using conventional textual to advanced deep embedding features and applying machine learning, ensemble learning and deep learning algorithms. Finally, before conclusion, the review highlights the open research issues and challenges as potential future directions for the researchers.},
  archive      = {J_AIR},
  author       = {Naz, Anam and Khan, Hikmat Ullah and Bukhari, Amal and Alshemaimri, Bader and Daud, Ali and Ramzan, Muhammad},
  doi          = {10.1007/s10462-025-11245-3},
  journal      = {Artificial Intelligence Review},
  month        = {8},
  number       = {8},
  pages        = {1-57},
  shortjournal = {Artif. Intell. Rev.},
  title        = {Machine and deep learning for personality traits detection: A comprehensive survey and open research challenges},
  volume       = {58},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Recent trends on mammogram breast density analysis using deep learning models: Neoteric review. <em>AIR</em>, <em>58</em>(8), 1-44. (<a href='https://doi.org/10.1007/s10462-025-11232-8'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Breast cancer is a globally prevalent and potentially fatal illness affecting women. Timely identification of screening mammography may decrease the occurrence of incorrect positive results and enhance the rate of patient survival. Nevertheless, the density of breast tissue in mammograms can impact the precision and effectiveness of detecting breast cancer. This paper examines the existing body of research on the analysis of breast density in mammograms utilising advanced deep learning models, including convolutional neural networks (CNN), transfer learning (TL), and ensemble learning (EL). Additionally, it examines various datasets and evaluation measures employed in the investigations. The study demonstrates that deep learning models can attain exceptional accuracy in categorising breast density. However, they encounter obstacles such as limited data availability, intricate model structures, and difficulties in interpreting the results. The research asserts that categorising breast density is an essential undertaking in order to enhance the identification and survival rates of breast cancer. Further investigation is warranted to examine the most effective deep learning structures, data augmentation methods, and interpretable models for this undertaking.},
  archive      = {J_AIR},
  author       = {Jeba Prasanna Idas, S. and Hemalatha, K. and Naveenkumar, Jayakumar and Joshva Devadas, T.},
  doi          = {10.1007/s10462-025-11232-8},
  journal      = {Artificial Intelligence Review},
  month        = {8},
  number       = {8},
  pages        = {1-44},
  shortjournal = {Artif. Intell. Rev.},
  title        = {Recent trends on mammogram breast density analysis using deep learning models: Neoteric review},
  volume       = {58},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Abstracting general syntax for XAI after decomposing explanation sub-components. <em>AIR</em>, <em>58</em>(8), 1-58. (<a href='https://doi.org/10.1007/s10462-025-11216-8'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Healthcare providers, policymakers, and defense contractors need to understand many types of machine learning model behaviors. While eXplainable Artificial Intelligence (XAI) provides tools for interpreting these behaviors, few frameworks, surveys, and taxonomies produce succinct yet general notation to help researchers and practitioners describe their explainability needs and quantify whether these needs are met. Such quantified comparisons could help individuals rank XAI methods by their relevance to use-cases, select explanations best suited for individual users, and evaluate what explanations are most useful for describing model behaviors. This paper collects, decomposes, and abstracts subcomponents of common XAI methods to identify a mathematically grounded syntax that applies generally to describing modern and future explanation types while remaining useful for discovering novel XAI methods. The resulting syntax, introduced as the Qi-Framework, generally defines explanation types in terms of the information being explained, their utility to inspectors, and the methods and information used to produce explanations. Just as programming languages define syntax to structure, simplify, and standardize software development, so too the Qi-Framework acts as a common language to help researchers and practitioners select, compare, and discover XAI methods. Derivative works may extend and implement the Qi-Framework to develop a more rigorous science for interpretable machine learning and inspire collaborative competition across XAI research.},
  archive      = {J_AIR},
  author       = {Wormald, Stephen and Maldaner, Matheus Kunzler and O’Connor, Kristian D. and Dizon-Paradis, Olivia P. and Woodard, Damon L.},
  doi          = {10.1007/s10462-025-11216-8},
  journal      = {Artificial Intelligence Review},
  month        = {8},
  number       = {8},
  pages        = {1-58},
  shortjournal = {Artif. Intell. Rev.},
  title        = {Abstracting general syntax for XAI after decomposing explanation sub-components},
  volume       = {58},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). High-frequency-based multi-spectral attention for domain generalization. <em>AIR</em>, <em>58</em>(8), 1-20. (<a href='https://doi.org/10.1007/s10462-025-11217-7'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep learning models have made great progress in many vision tasks, but they suffer from domain shift problem when exposed to out-of-distribution scenarios. Domain generalization (DG) is proposed to learn a model from several observable source domains that can generalize well to unknown target domains. Although recent advances in DG works have achieved promising performance, there is a high demand for computational resource, especially those that employ meta-learning or ensemble learning strategies. However, some pioneering works propose to replace convolutional neural network (CNN) as the backbone architecture with multi-layer perceptron (MLP)-like models that can not only learn long-range spatial dependencies but also reduce network parameters using Fourier transform-based techniques. Inspired by this, in this paper, we propose a high-frequency-based multi-spectral attention (HMCA) to facilitate a lightweight MLP-like model to learn global domain-invariant features by focusing on high-frequency components sufficiently. Moreover, we adopt a data augmentation strategy based on Fourier transform to simulate domain shift, thus enabling the model to pay more attention on robust features. Extensive experiments on benchmark datasets demonstrate that our method is superior to the existing CNN-based and MLP-based DG methods.},
  archive      = {J_AIR},
  author       = {Ying, Surong and Song, Xinghao and Wang, Hongpeng},
  doi          = {10.1007/s10462-025-11217-7},
  journal      = {Artificial Intelligence Review},
  month        = {8},
  number       = {8},
  pages        = {1-20},
  shortjournal = {Artif. Intell. Rev.},
  title        = {High-frequency-based multi-spectral attention for domain generalization},
  volume       = {58},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Building intelligence identification system via large language model watermarking: A survey and beyond. <em>AIR</em>, <em>58</em>(8), 1-58. (<a href='https://doi.org/10.1007/s10462-025-11222-w'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Large Language Models (LLMs) are increasingly integrated into diverse industries, posing substantial security risks due to unauthorized replication and misuse. To mitigate these concerns, robust identification mechanisms are widely acknowledged as an effective strategy. Identification systems for LLMs now rely heavily on watermarking technology to manage and protect intellectual property and ensure data security. However, previous studies have primarily concentrated on the basic principles of algorithms and lacked a comprehensive analysis of watermarking theory and practice from the perspective of intelligent identification. To bridge this gap, firstly, we explore how a robust identity recognition system can be effectively implemented and managed within LLMs by various participants using watermarking technology. Secondly, we propose a mathematical framework based on mutual information theory, which systematizes the identification process to achieve more precise and customized watermarking. Additionally, we present a comprehensive evaluation of performance metrics for LLM watermarking, reflecting participant preferences and advancing discussions on its identification applications. Lastly, we outline the existing challenges in current watermarking technologies and theoretical frameworks, and provide directional guidance to address these challenges. Our systematic classification and detailed exposition aim to enhance the comparison and evaluation of various methods, fostering further research and development toward a transparent, secure, and equitable LLM ecosystem.},
  archive      = {J_AIR},
  author       = {Wang, Xuhong and Jiang, Haoyu and Yu, Yi and Yu, Jingru and Lin, Yilun and Yi, Ping and Wang, Yingchun and Qiao, Yu and Li, Li and Wang, Fei-Yue},
  doi          = {10.1007/s10462-025-11222-w},
  journal      = {Artificial Intelligence Review},
  month        = {8},
  number       = {8},
  pages        = {1-58},
  shortjournal = {Artif. Intell. Rev.},
  title        = {Building intelligence identification system via large language model watermarking: A survey and beyond},
  volume       = {58},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A review of LLMs and their applications in the architecture, engineering and construction industry. <em>AIR</em>, <em>58</em>(8), 1-46. (<a href='https://doi.org/10.1007/s10462-025-11241-7'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {During the past decade, there has been rapid emergence, continuous development and advancements in the field of Artificial Intelligence (AI), and a broad adaptation ofLarge Language Models (LLMs) in a wide variety of application domains transforming and streamlining industry practices. However, the construction industry has yet to fully incorporate these technologies, delaying their wide-scale adaptation. Only a limited number of recent studies have explored the opportunities, capabilities and potential of current LLM implementations in the broad domain of Architecture Engineering and Construction (AEC) industry, leaving a significant gap in this field of research. This study aims to address this gap and provide an extensive review of already established state-of-the-art applications and use case scenarios of LLMs in the AEC industry. Apart from that, by exploring the key contributions and limitations of these applications, and by considering relative reviews on this subject, it was possible to categorize them, to extract the emerging challenges and future directions of the field and propose actionable recommendations for industry stakeholders. This study also includes an introduction to important concepts and recent advancements of LLM technologies, focusing on transformer-based architectures and providing an extensive list of LLM families.},
  archive      = {J_AIR},
  author       = {Kampelopoulos, Dimitrios and Tsanousa, Athina and Vrochidis, Stefanos and Kompatsiaris, Ioannis},
  doi          = {10.1007/s10462-025-11241-7},
  journal      = {Artificial Intelligence Review},
  month        = {8},
  number       = {8},
  pages        = {1-46},
  shortjournal = {Artif. Intell. Rev.},
  title        = {A review of LLMs and their applications in the architecture, engineering and construction industry},
  volume       = {58},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Groundbreaking taxonomy of metaverse characteristics. <em>AIR</em>, <em>58</em>(8), 1-50. (<a href='https://doi.org/10.1007/s10462-025-11243-5'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Metaverse, a dynamic and immersive virtual realm, has captured the imagination of researchers and enthusiasts worldwide. This survey paper aims to introduce a groundbreaking taxonomy for the characteristics of the Metaverse offering a structured and adaptable framework that extends beyond existing categorizations by incorporating dynamic transformations. Unlike prior taxonomies, which often focus on fixed attributes, our approach emphasizes the dynamic evolution of Metaverse characteristics. Through an extensive review of published literature, this study explores key technological, social, economic, and ethical dimensions of the Metaverse. It introduces a process-oriented classification based on 23 distinct characteristics, including immersification, spatiotemporalification, interactification, persistentification, presentification, personification, unification, imaginification, economification, uncertaintification, and credification. By mapping these evolving aspects, we provide a structured and future-proof foundation for understanding the Metaverse’s continuous development. This survey establishes a new standard for comprehensiveness and innovation, shedding light on the diverse facets that have been explored in literature. Through this novel taxonomy, we provide a detailed map of the current landscape and offer insights that pave the way for future research and development in this burgeoning digital frontier.},
  archive      = {J_AIR},
  author       = {Sadeghi-Niaraki, Abolghasem and Rahimi, Fatema and Azlan, Nur Alya Emanuelle Binti and Song, Houbing and Ali, Farman and Choi, Soo-Mi},
  doi          = {10.1007/s10462-025-11243-5},
  journal      = {Artificial Intelligence Review},
  month        = {8},
  number       = {8},
  pages        = {1-50},
  shortjournal = {Artif. Intell. Rev.},
  title        = {Groundbreaking taxonomy of metaverse characteristics},
  volume       = {58},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Face-based machine learning diagnostics: Applications, challenges and opportunities. <em>AIR</em>, <em>58</em>(8), 1-71. (<a href='https://doi.org/10.1007/s10462-025-11246-2'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Traditional medical diagnostic methods face bottlenecks such as high cost, poor accessibility, and delayed diagnosis in genetic syndromes, neurological disorders, psychiatric disorders, and endocrine disorders. Face-based machine learning (ML) technology provides a new path for early screening of diseases by analyzing facial phenotypes, dynamic expressions, facial skin, and 3D structural abnormalities, and is gradually becoming a clinically assisted screening tool. This paper provides a comprehensive overview of the applications, advances, and challenges of the technology. We summarize the range of diseases for which facial diagnosis is applicable and describe the basic process and related techniques for face-based ML diagnostic systems. In addition, this paper organizes the resources of current publicly available facial medical datasets and clarifies their disease coverage and sample size. Finally, possible future solutions to challenges hindering widespread adoption in clinical practice such as data bias, privacy, interpretability, generalizability, clinical value, and resource constraints are discussed. This review aims to provide researchers with a comprehensive foundation that integrates clinical perspectives, technological insights, and practical resources, to facilitate the development and successful implementation of face-based ML diagnostics in real-world clinical practice.},
  archive      = {J_AIR},
  author       = {Song, Jie and He, Mengqiao and Zheng, Xin and Zhang, Yuxin and Bi, Cheng and Feng, Jinhua and Du, Jiale and Li, Hang and Shen, Bairong},
  doi          = {10.1007/s10462-025-11246-2},
  journal      = {Artificial Intelligence Review},
  month        = {8},
  number       = {8},
  pages        = {1-71},
  shortjournal = {Artif. Intell. Rev.},
  title        = {Face-based machine learning diagnostics: Applications, challenges and opportunities},
  volume       = {58},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Deep learning model inversion attacks and defenses: A comprehensive survey. <em>AIR</em>, <em>58</em>(8), 1-52. (<a href='https://doi.org/10.1007/s10462-025-11248-0'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The rapid adoption of deep learning in sensitive domains has brought tremendous benefits. However, this widespread adoption has also given rise to serious vulnerabilities, particularly model inversion (MI) attacks, posing a significant threat to the privacy and integrity of personal data. The increasing prevalence of these attacks in applications such as biometrics, healthcare, and finance has created an urgent need to understand their mechanisms, impacts, and defense methods. This survey aims to fill the gap in the literature by providing a structured and in-depth review of MI attacks and defense strategies. Our contributions include a systematic taxonomy of MI attacks, extensive research on attack techniques and defense mechanisms, and a discussion about the challenges and future research directions in this evolving field. By exploring the technical and ethical implications of MI attacks, this survey aims to offer insights into the impact of AI-powered systems on privacy, security, and trust. In conjunction with this survey, we have developed a comprehensive repository to support research on MI attacks and defenses. The repository includes state-of-the-art research papers, datasets, evaluation metrics, and other resources to meet the needs of both novice and experienced researchers interested in MI attacks and defenses, as well as the broader field of AI security and privacy. The repository will be continuously maintained to ensure its relevance and utility. It is accessible at https://github.com/overgter/Deep-Learning-Model-Inversion-Attacks-and-Defenses .},
  archive      = {J_AIR},
  author       = {Yang, Wencheng and Wang, Song and Wu, Di and Cai, Taotao and Zhu, Yanming and Wei, Shicheng and Zhang, Yiying and Yang, Xu and Tang, Zhaohui and Li, Yan},
  doi          = {10.1007/s10462-025-11248-0},
  journal      = {Artificial Intelligence Review},
  month        = {8},
  number       = {8},
  pages        = {1-52},
  shortjournal = {Artif. Intell. Rev.},
  title        = {Deep learning model inversion attacks and defenses: A comprehensive survey},
  volume       = {58},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Decoupling feature-driven and multimodal fusion attention for clothing-changing person re-identification. <em>AIR</em>, <em>58</em>(8), 1-26. (<a href='https://doi.org/10.1007/s10462-025-11250-6'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Person Re-Identification (ReID) plays a crucial role in intelligent surveillance, public safety, and intelligent transportation systems. However, clothing variation remains a significant challenge in this field. To address this issue, this paper introduces a method named Decoupling Feature-Driven and Multimodal Fusion Attention for Clothing-Changing Person Re-Identification (DM-ReID). The proposed approach employs a dual-stream feature extraction framework, consisting of a global RGB image feature stream and a clothing-irrelevant feature enhancement stream. These streams respectively capture comprehensive appearance information and identity features independent of clothing. Additionally, two feature fusion strategies are proposed: firstly, an initial fusion of RGB features and clothing-irrelevant features is achieved through the Hadamard product in the mid-network stage to enhance feature complementarity; secondly, a multimodal fusion attention mechanism is integrated at the network’s end to dynamically adjust feature weights, further improving feature representation capabilities. To optimize model performance, a composite loss function combining identity loss and triplet loss is utilized, effectively enhancing the model’s discriminative ability and feature distinctiveness. Experimental results on multiple public datasets, including PRCC, LTCC, and VC-Clothes, demonstrate that DM-ReID surpasses most existing mainstream methods in Rank-1 accuracy and mean Average Precision (mAP) metrics under clothing-changing scenarios. These findings validate the method’s effectiveness and robustness in handling complex clothing variations, highlighting its promising prospects for practical applications.},
  archive      = {J_AIR},
  author       = {Ding, Yongkang and Wang, Xiaoyin and Yuan, Hao and Qu, Meina and Jian, Xiangzhou},
  doi          = {10.1007/s10462-025-11250-6},
  journal      = {Artificial Intelligence Review},
  month        = {8},
  number       = {8},
  pages        = {1-26},
  shortjournal = {Artif. Intell. Rev.},
  title        = {Decoupling feature-driven and multimodal fusion attention for clothing-changing person re-identification},
  volume       = {58},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Bio-inspired optimization methods for visible light communication: A comprehensive review. <em>AIR</em>, <em>58</em>(8), 1-25. (<a href='https://doi.org/10.1007/s10462-025-11251-5'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Visible light communication (VLC) offers a promising alternative to traditional radio frequency communication due to its greater bandwidth, energy efficiency, and security advantages. This paper presents a comprehensive review of bio-inspired optimization algorithms, including swarm intelligence and genetic algorithms, that enhance the performance and robustness of VLC systems. These techniques have demonstrated significant potential in addressing challenges such as channel optimization and noise reduction. However, despite their advantages, bio-inspired algorithms also face limitations, including computational complexity and limited adaptability to dynamic real-world conditions. Additionally, the integration of bio-inspired methods with artificial intelligence (AI) may further enhance their adaptability and efficiency in VLC systems. This review highlights both the opportunities and challenges associated with bio-inspired optimization in VLC and provides insights into future directions for research and practical implementation, which will focus on developing more efficient and scalable bio-inspired approaches that can operate in highly variable environments while minimizing energy consumption.},
  archive      = {J_AIR},
  author       = {Dratnal, Martin and Danys, Lukas and Martinek, Radek},
  doi          = {10.1007/s10462-025-11251-5},
  journal      = {Artificial Intelligence Review},
  month        = {8},
  number       = {8},
  pages        = {1-25},
  shortjournal = {Artif. Intell. Rev.},
  title        = {Bio-inspired optimization methods for visible light communication: A comprehensive review},
  volume       = {58},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Carrier platform-enhanced multiple-UAV cooperative task assignment with dual heterogeneities. <em>AIR</em>, <em>58</em>(8), 1-34. (<a href='https://doi.org/10.1007/s10462-025-11254-2'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Heterogeneous unmanned aerial vehicle (UAV) cooperation has been widely used in modern warfare. Due to the limited UAV flight endurance, the operational range is generally constrained. This issue can be effectively addressed by utilizing various airborne or shipborne carrier platforms (CPs) such as large transporters and aircraft carriers. However, such a topic is rarely studied in existing research. This paper studies the carrier platform-enhanced multiple-UAV cooperative task assignment (CPMCTA) with dual heterogeneities (i.e., in both UAVs and CPs). Additionally, the approaching unattacked target-induced risk (AUTIR), which isneglected in traditional research, is also considered to improve the task implementation safety. A novel CPMCTA model with comprehensive factors (i.e., priority, obstacles, AUTIR and heterogeneities) is first established. Aiming at an efficient solution, an adaptive self-motivated teaching-learning-based optimization algorithm (AMTLBO) is then developed by integrating various mechanisms (i.e., multiple teachers, adaptive learning rate and self-motivation). Simulations under various scenarios demonstrate the advantages of the AMTLBO in optimum-seeking capability over the other six state-of-the-art algorithms. Moreover, the necessity of considering AUTIR is highlighted. A simulation animation is available at bilibili.com/video/BV1Ht421A7Qx to provide a clearer illustration.},
  archive      = {J_AIR},
  author       = {Xinyong, Yu and Xin, Li and Lei, Wang and Junhong, Jin and Genlai, Zhang and Xichao, Su and Laifa, Tao and Chen, Lu and Xinwei, Wang},
  doi          = {10.1007/s10462-025-11254-2},
  journal      = {Artificial Intelligence Review},
  month        = {8},
  number       = {8},
  pages        = {1-34},
  shortjournal = {Artif. Intell. Rev.},
  title        = {Carrier platform-enhanced multiple-UAV cooperative task assignment with dual heterogeneities},
  volume       = {58},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A multi-strategy enhanced dung beetle algorithm for solving real-world engineering problems. <em>AIR</em>, <em>58</em>(8), 1-79. (<a href='https://doi.org/10.1007/s10462-025-11235-5'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Dung Beetle optimization (DBO) algorithm is an innovative and effective metaheuristic algorithm widely recognised for its excellent numerical optimization performance. However, DBO converges slowly and tends to fall into local optima due to the imbalance between exploration and exploitation, the lack of collaborative search capability and population diversity. To overcome these challenges, this paper proposes a dung beetle optimization algorithm based on multi-strategy collaborative enhancement (MDBO). The algorithm constructs a “search-enhance-escape” collaborative optimization framework through adaptive regulation and elite information sharing, and contains three major innovations: (1) a dual adaptive search strategy, which combines the adaptive contraction mechanism of the leader’s centre of mass and the brownian bidirectional crossover perturbation strength regulation, to enhance the population diversity and collaborative search ability of the dung beetle, achieving a dynamic balance between exploration and exploitation; (2) Elite Enhanced Solution Quality (EESQ) mechanism, which improves the quality of both the current local and global optimal positions and accelerates convergence through structured elite information and dual-phase adaptive perturbation; and (3) Dynamic Oppositional Learning (DOL), which introduces an asymmetric adaptive perturbation in the dung beetle’s foraging and Breeding phases and enhances the ability to escape from local optima. The three act synergistically to achieve a more efficient optimised search. The performance of MDBO is evaluated using the IEEE CEC 2017, CEC 2019 and CEC 2020 benchmarking functions. Compared to the DBO algorithm, the MDBO algorithm improves the convergence accuracy and stability on the CEC2017 benchmark functions by 60.91 % and 63.98 %, respectively. For the CEC2019 benchmark functions, the corresponding improvements are 54.47 % and 41.36 %, while for the CEC2020 benchmark functions, they are 50.71 % and 55.16 %, respectively. In addition, its overall performance is evaluated against two complex real-world engineering problems: UAV path planning and wireless sensor network coverage optimization. The experimental results show that MDBO provides very competitive optimization results compared to DBO, two highly referenced algorithms and five advanced algorithms.},
  archive      = {J_AIR},
  author       = {Mao, Zhengxing and Yang, Zhen and Luo, Dan and Lin, Dong and Jiang, Qinghong and Huang, Guoxian and Liao, Zhixian},
  doi          = {10.1007/s10462-025-11235-5},
  journal      = {Artificial Intelligence Review},
  month        = {8},
  number       = {8},
  pages        = {1-79},
  shortjournal = {Artif. Intell. Rev.},
  title        = {A multi-strategy enhanced dung beetle algorithm for solving real-world engineering problems},
  volume       = {58},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Non-parametric insights in option pricing: A systematic review of theory, implementation and future directions. <em>AIR</em>, <em>58</em>(8), 1-33. (<a href='https://doi.org/10.1007/s10462-025-11249-z'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The task of pricing options is seen as significant and receives considerable attention due to its potential to generate attractive profits through informed decision-making. Over the past few decades, researchers have extensively investigated both classical and machine-learning techniques for this purpose. Our motivation for undertaking this survey is to provide a comprehensive review and analyze systematically the recent works focusing on non-parametric models for option pricing. The analysis of the articles involves the utilization of several components such as input, output, dataset, assessment metrics, and other relevant factors. Research gaps and challenges are meticulously identified and outlined to serve as guiding insights for future improvements and advancements in the field. We categorize the implementation to assist interested researchers in easily reproducing previous studies as baselines. Based on the findings of this study, it can be inferred that the process of pricing options is a highly complicated task, requiring the consideration of several elements to enhance the accuracy and efficiency of models.},
  archive      = {J_AIR},
  author       = {Sharma, Akanksha and Kumar Verma, Chandan},
  doi          = {10.1007/s10462-025-11249-z},
  journal      = {Artificial Intelligence Review},
  month        = {8},
  number       = {8},
  pages        = {1-33},
  shortjournal = {Artif. Intell. Rev.},
  title        = {Non-parametric insights in option pricing: A systematic review of theory, implementation and future directions},
  volume       = {58},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Computational imaging based on single-photon detection: A survey. <em>AIR</em>, <em>58</em>(8), 1-54. (<a href='https://doi.org/10.1007/s10462-025-11252-4'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The rapid advancements in single-photon detectors with picosecond timing resolution over the past decade have significantly driven the development of time-correlated single-photon counting (TCSPC) for computational imaging applications, including bioimaging and remote sensing. In this review, we utilize the CiteSpace tool to create knowledge maps and perform a bibliometric analysis of this research area. Furthermore, we provide a comprehensive overview of the key challenges associated with computational imaging using temporal single-photon counting. We also highlight how these challenges have been addressed under extreme conditions to establish a reference model for future imaging solutions. We examine the performance evaluation parameters of single-photon detectors to enhance the understanding of detector array scaling and their application in constructing efficient computational imaging systems. Lastly, we aim to elucidate the current technical challenges in single-photon detector-based computational imaging and explore their potential future developments.},
  archive      = {J_AIR},
  author       = {Pu, Yanyun and Zhu, Chengyuan and Yao, Gongxin and Li, Chao and Pan, Yu and Yang, Kaixiang and Yang, Qinmin},
  doi          = {10.1007/s10462-025-11252-4},
  journal      = {Artificial Intelligence Review},
  month        = {8},
  number       = {8},
  pages        = {1-54},
  shortjournal = {Artif. Intell. Rev.},
  title        = {Computational imaging based on single-photon detection: A survey},
  volume       = {58},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multiple sclerosis classification and segmentation in neuroimaging MRI using different machine and deep learning techniques: A review. <em>AIR</em>, <em>58</em>(8), 1-49. (<a href='https://doi.org/10.1007/s10462-025-11143-8'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multiple Sclerosis (MS) is a type of brain disease that affects both the brain and spinal cord. In order to diagnose MS, many modalities are used, including Magnetic Resonance Imaging (MRI). MRI modalities are noninvasive medical tests that provide physicians with detailed images containing essential details on the anatomy and physiology of the brain. Diagnosing with MRI of the brain is crucial since it is laborious, time-consuming, and, above all, prone to manual or human error. In the past ten years, Artificial Intelligence (AI)- based Computer Aided Diagnostic (CAD) tools have been increasingly popular due to their low manual error rate and ability to produce accurate and dependable findings for diagnosing Multiple Sclerosis (MS) using MRI neuroimaging modalities. Automated MS diagnosis in AI is performed using both traditional and contemporary AI. Machine Learning (ML) approaches, which rely on selecting and extracting features through trial and error, were utilized in classical AI. Meanwhile, modern AI uses Deep Learning (DL) techniques, which extract and select suitable features automatically, save time and are more efficient than standard classical ML approaches. In this work, we give a summary of recent automated MS diagnosis approaches that combine MRI neuroimaging modalities with ML and DL algorithms. Segmentation and classification are two of the main categories into which AI techniques for MS medical diagnosis can be roughly divided, where each one of them is divided based on whether a supervised learning, un-supervised learning or both are used together to get better analysis for MS diagnosis. We briefly discuss the related work in each category, and finally, we present the important drawbacks and challenges for each work as well as some proposed ideas that can solve the drawbacks and challenges faced.},
  archive      = {J_AIR},
  author       = {Thabet, Rezq Muhammed and Shedeed, Howida A. and Al-Berry, Maryam and Khattab, Dina},
  doi          = {10.1007/s10462-025-11143-8},
  journal      = {Artificial Intelligence Review},
  month        = {8},
  number       = {8},
  pages        = {1-49},
  shortjournal = {Artif. Intell. Rev.},
  title        = {Multiple sclerosis classification and segmentation in neuroimaging MRI using different machine and deep learning techniques: A review},
  volume       = {58},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Towards few-call model stealing via active self-paced knowledge distillation and diffusion-based image generation. <em>AIR</em>, <em>58</em>(8), 1-25. (<a href='https://doi.org/10.1007/s10462-025-11184-z'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Diffusion models showcase strong capabilities in image synthesis, being used in many computer vision tasks with great success. To this end, we propose to explore a new use case, namely to copy black-box classification models without having access to the original training data, the architecture, and the weights of the model, i.e. the model is only exposed through an inference API. More specifically, we can only observe the (soft or hard) labels for some image samples passed as input to the model. Furthermore, we consider an additional constraint limiting the number of model calls, mostly focusing our research on few-call model stealing. In order to solve the model extraction task given the applied restrictions, we propose the following framework. As training data, we create a synthetic data set (called proxy data set) by leveraging the ability of diffusion models to generate realistic and diverse images. Given a maximum number of allowed API calls, we pass the respective number of samples through the black-box model to collect labels. Finally, we distill the knowledge of the black-box teacher (attacked model) into a student model (copy of the attacked model), harnessing both labeled and unlabeled data generated by the diffusion model. We employ a novel active self-paced learning framework to make the most of the proxy data during distillation. Our empirical results on three data sets confirm the superiority of our framework over four state-of-the-art methods in the few-call model extraction scenario. We release our code for free non-commercial use at https://github.com/vladhondru25/model-stealing .},
  archive      = {J_AIR},
  author       = {Hondru, Vlad and Ionescu, Radu Tudor},
  doi          = {10.1007/s10462-025-11184-z},
  journal      = {Artificial Intelligence Review},
  month        = {8},
  number       = {8},
  pages        = {1-25},
  shortjournal = {Artif. Intell. Rev.},
  title        = {Towards few-call model stealing via active self-paced knowledge distillation and diffusion-based image generation},
  volume       = {58},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A review of methods for gene regulatory networks reconstruction and analysis. <em>AIR</em>, <em>58</em>(8), 1-37. (<a href='https://doi.org/10.1007/s10462-025-11257-z'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Gene regulatory networks (GRNs) are one of the most promising techniques for modeling and understanding biological processes. GRNs help to understand complex interactions involving genes. Regulator molecules control gene expression within these networks through interactions with each other and other substances in cells. They play a crucial role in deciphering regulatory relationships among genes and modeling changes in gene expression under various conditions. In recent years, there has been a surge in the generation of vast quantities and diverse varieties of gene expression data. This bulk production has created a demand for new data management methods. It is expected that integrative analysis of various types of gene information will provide a comprehensive overview for researching complex biological systems and processes. This review covers recent developments related to Gene Regulatory Networks and associated gene expression data from 2019 to 2023. Key topics are presented according to the data type used for network building, machine learning approaches, tools used in network inference, model optimization, and computational validation techniques. This paper presents an in-depth technical review of the techniques currently used for constructing GRNs, explores possibilities for future research, and aims to enhance the understanding of GRNs.},
  archive      = {J_AIR},
  author       = {Ali, Sura I. Mohammed and Alrashid, Sura Zaki},
  doi          = {10.1007/s10462-025-11257-z},
  journal      = {Artificial Intelligence Review},
  month        = {8},
  number       = {8},
  pages        = {1-37},
  shortjournal = {Artif. Intell. Rev.},
  title        = {A review of methods for gene regulatory networks reconstruction and analysis},
  volume       = {58},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Neutrosophic Z-number Schweizer–Sklar prioritized aggregation operators and new score function for multi-attribute decision making. <em>AIR</em>, <em>58</em>(7), 1-47. (<a href='https://doi.org/10.1007/s10462-025-11124-x'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-attribute decision making (MADM) is an important branch of modern decision science and has been applied to many real-world scenarios. As decision scenarios become more complex and multidimensional with time, the neutrosophic Z-number can effectively represent this kind of complex and fuzzy information. On the one hand, it takes into account the uncertain, inconsistent and discontinuous information existing in the MADM environment, and on the other hand, it covers the reliability measure of the evaluation information to enhance the credibility of the decision. In the decision process, it will be necessary to consider the flexibility of information fusion and the existence of priority relationships between decision attributes in order to extend the decision processing performance of the neutrosophic Z-number. To this end, based on the features of Schweizer–Sklar t-norm and t-conorm to improve the flexibility and utility of the aggregation process through parameter variations, neutrosophic Z-number Schweizer–Sklar operation laws are proposed. Furthermore, in order to address the features of attributes with linear priority relationships, the advantages of prioritized aggregation operators are considered in the face of this situation. We proposed neutrosophic Z-number Schweizer–Sklar prioritized aggregation operators including the following neutrosophic Z-number Schweizer–Sklar prioritized weighted averaging (NZNSSPRWA) operators and neutrosophic Z-number Schweizer–Sklar prioritized weighted geometric (NZNSSPRWG) operators and the related theorem is proved. Further, the original score function of neutrosophic Z-numbers appears to be incompetent in dealing with more complex and difficult situations, which motivates us to propose a new score function for neutrosophic Z-numbers to effectively enhance the differentiation and ensure the reliability of the decision results. In order to illustrate the methodology, this paper considers and solves MADM problems related to the location of Internet data centres, and in order to demonstrate the effectiveness and practicality of the proposed methodology, a sensitivity analysis of the parameters as well as a discussion of the developed methodology in comparison with the existing methodologies are carried out. The results show that the proposed MADM approach balances the decision attribute priority preferences with the strong flexibility required in practical applications. The developed technique can significantly improve the accuracy and reliability of decisions in complex and changing decision environments and bring the decision process closer to the real needs.},
  archive      = {J_AIR},
  author       = {Wu, Meiqin and Chen, Donghao and Fan, Jianping},
  doi          = {10.1007/s10462-025-11124-x},
  journal      = {Artificial Intelligence Review},
  month        = {7},
  number       = {7},
  pages        = {1-47},
  shortjournal = {Artif. Intell. Rev.},
  title        = {Neutrosophic Z-number Schweizer–Sklar prioritized aggregation operators and new score function for multi-attribute decision making},
  volume       = {58},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Power aggregation operators based on aczel-alsina T-norm and T-conorm for intuitionistic hesitant fuzzy information and their application to logistics service provider selection. <em>AIR</em>, <em>58</em>(7), 1-41. (<a href='https://doi.org/10.1007/s10462-025-11155-4'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Logistics service provider selection is a skilled and effective technique used to evaluate and identify third-party enterprises or organizations capable of managing and performing logistics tasks on behalf of a business. In this study, we aim to propose a logistics service provider selection method based on aggregation operators and fuzzy sets. First, we analyze the Aczel-Alsina operational laws building on the intuitionistic hesitant fuzzy sets technique, which combines hesitant fuzzy and intuitionistic fuzzy models. Subsequently, we derive power averaging/geometric aggregation operators for the intuitionistic hesitant fuzzy model based on Aczel-Alsina operational laws, called the intuitionistic hesitant fuzzy Aczel-Alsina power averaging operator, intuitionistic hesitant fuzzy Aczel-Alsina weighted power averaging operator, intuitionistic hesitant fuzzy Aczel-Alsina power geometric operator, and intuitionistic hesitant fuzzy Aczel-Alsina weighted power geometric operator, with their flexible and basic properties such as idempotency, monotonicity, and boundedness. The existing model of drastic aggregation operators, max–min aggregation operators, and algebraic aggregation operators are the special cases of the proposed theory. To address the selection of logistics service providers using the proposed operators, we explore the multi-attribute decision-making methods to evaluate the required problems. Finally, we compare the ranking results of the invented model with those of existing technologies to describe the effectiveness and stability of the proposed methods.},
  archive      = {J_AIR},
  author       = {Wang, Peng and Zhu, Baoying and Yan, Keyan and Zhang, Ziyu and Ali, Zeeshan and Pamucar, Dragan},
  doi          = {10.1007/s10462-025-11155-4},
  journal      = {Artificial Intelligence Review},
  month        = {7},
  number       = {7},
  pages        = {1-41},
  shortjournal = {Artif. Intell. Rev.},
  title        = {Power aggregation operators based on aczel-alsina T-norm and T-conorm for intuitionistic hesitant fuzzy information and their application to logistics service provider selection},
  volume       = {58},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A systematic survey of hybrid ML techniques for predicting peak particle velocity (PPV) in open-cast mine blasting operations. <em>AIR</em>, <em>58</em>(7), 1-59. (<a href='https://doi.org/10.1007/s10462-025-11156-3'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Blasting operations in open-cast mines, while essential for mineral extraction, can generate significant peak particle velocity (PPV), posing environmental and structural risks. Accurate PPV prediction is critical to mitigate these effects and optimize blasting practices. This review introduces a hybrid ML approach that combines traditional methods, such as decision trees and SVMs, with advanced techniques like ensemble learning and neural networks. The performance of these models is evaluated based on blast parameters, geographical conditions, and monitoring data. The study highlights that hybrid and ensemble methods outperform other techniques in the majority of cases, especially in surface blasting scenarios. The increasing use of these advanced methods underscores their potential to address key challenges in blasting operations. Hybrid machine learning models over traditional methods by combining the strengths of multiple algorithms, effectively reducing bias and variance while enhancing predictive accuracy. Unlike conventional models, which often struggle with nonlinear relationships and high-dimensional data, hybrid approaches leverage advanced feature engineering, ensemble learning, and optimization techniques to improve robustness and generalization. In our study, these models demonstrated superior reliability in predicting PPV, achieving higher accuracy in terms of RMSE and $$\text {R}^2$$ . By combining different techniques, they mitigate individual model weaknesses, reduce errors, and improve feature selection. In addition, hybrid models prevent overfitting and optimize predictions through ensemble strategies such as boosting and stacking. This study explores the advantages of hybrid ML models, demonstrating their superior performance compared to conventional approaches. The review also identifies gaps in research on underground blasting and suggests future directions, emphasizing the importance of ongoing technological advancements and industry awareness of ML techniques benefits. Enhanced accuracy and robustness in PPV prediction, driven by hybrid approaches and real-time systems, are crucial to improve safety and efficiency in mining operations.},
  archive      = {J_AIR},
  author       = {Shylaja, Gundaveni and Prashanth, Ragam},
  doi          = {10.1007/s10462-025-11156-3},
  journal      = {Artificial Intelligence Review},
  month        = {7},
  number       = {7},
  pages        = {1-59},
  shortjournal = {Artif. Intell. Rev.},
  title        = {A systematic survey of hybrid ML techniques for predicting peak particle velocity (PPV) in open-cast mine blasting operations},
  volume       = {58},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Challenges in detecting security threats in WoT: A systematic literature review. <em>AIR</em>, <em>58</em>(7), 1-57. (<a href='https://doi.org/10.1007/s10462-025-11176-z'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The rapid expansion of the Web of Things (WoT) and the Internet of Things (IoT) has raised security issues, with Denial of Service (DoS) attacks becoming increasingly prevalent. So, the aim of this study is to identify the security concerns in the four architectural layers of the Web of Things, particularly DoS attacks. For this study, existing literature are identified using search queries, and approximately 80 of relevant primary papers published in the recent decade are obtained after a thorough review which helps in addressing our research questions. After finding the relevant primary studies, we applied strict quality evaluation criteria to verify that all studies are evaluated. In addition, a taxonomy of deep learning (DL) techniques is presented on the basis of literature analysis conducted in this research, which is then used to characterize the various security concerns that occur in IoT and WoT systems. The study also examines which DL approaches are used to detect DoS/DDoS attacks in IoT and WoT. Our findings indicate that the optimal form of Intrusion Detection System (IDS) for dealing with DoS attacks is a hybrid IDS, which uses both the signature-based and the anomaly-based IDS. Moreover, DL techniques such as, CNNs and LSTMs, produced excellent results but are still in the development stage in terms of scalability and practical use. This review further highlights the present state of security mechanisms and sets the basis for future research, with an emphasis on refining DL-based techniques and improving the scalability and adaptability of security systems for WoT networks.},
  archive      = {J_AIR},
  author       = {Sardar, Ruhma and Anees, Tayyaba and Al-Shamayleh, Ahmad Sami and Mehmood, Erum and Khalil, Wajeeha and Akhunzada, Adnan and Shaikh, Fatema Sabeen},
  doi          = {10.1007/s10462-025-11176-z},
  journal      = {Artificial Intelligence Review},
  month        = {7},
  number       = {7},
  pages        = {1-57},
  shortjournal = {Artif. Intell. Rev.},
  title        = {Challenges in detecting security threats in WoT: A systematic literature review},
  volume       = {58},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Modeling and effect analysis of machining parameters for surface roughness and specific energy consumption during TC18 machining using deep reinforcement learning and neural networks. <em>AIR</em>, <em>58</em>(7), 1-31. (<a href='https://doi.org/10.1007/s10462-025-11178-x'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Under the impetus of green manufacturing and a low-carbon economy, the critical challenge lies in reducing energy consumption while maintaining machining quality. Against this background, this paper presents the method of modeling and effect analysis for surface roughness and specific energy consumption during TC18 machining using Deep Reinforcement Learning and Neural Networks. In this method, to reduce the experiment cost, multilayer-layer design (MLD) for computer simulation is applied to design a physical experiment, and to improve modeling accuracy, backpropagation neural network (BPNN) optimized by Double deep Q network algorithm (DDQN) is utilized to develop the prediction models of surface roughness (Ra) and specific energy consumption of cutting (Esec). Finaly, the synergistic influence of cutting parameters on Ra and Esec is analyzed based on the prediction models of Ra and Esec built by MLD and DDQN-BPNN. The effectiveness and low cost of MLD and the excellent prediction performance of DDQN-BPNN are verified by comparisons of optimized BPNNs using common heuristic optimization algorithms through the milling experiment of TC18. These technologies provide effective solutions for modeling and factor impact of target features in machining field, and research results provides an effective guidance for the selection of milling parameters of TC18 to reduce the specific energy consumption of cutting under ensuring or improving machining quality.},
  archive      = {J_AIR},
  author       = {Lu, Juan and Mu, Huailong and Ouyang, Haibin and Zhang, Zhenkun and Ding, Weiping},
  doi          = {10.1007/s10462-025-11178-x},
  journal      = {Artificial Intelligence Review},
  month        = {7},
  number       = {7},
  pages        = {1-31},
  shortjournal = {Artif. Intell. Rev.},
  title        = {Modeling and effect analysis of machining parameters for surface roughness and specific energy consumption during TC18 machining using deep reinforcement learning and neural networks},
  volume       = {58},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Dual representation learning for one-step clustering of multi-view data. <em>AIR</em>, <em>58</em>(7), 1-30. (<a href='https://doi.org/10.1007/s10462-025-11183-0'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In real-world applications, multi-view data is widely available and multi-view learning is an effective method for mining multi-view data. In recent years, multi-view clustering, as an important part of multi-view learning, has been receiving more and more attention, while how to design an effective multi-view data mining method and make it more pertinent for clustering is still a challenging mission. For this purpose, a new one-step multi-view clustering method with dual representation learning is proposed in this paper. First, based on the fact that multi-view data contain both consistent knowledge between views and unique knowledge of each view, we propose a new dual representation learning method by improving the matrix factorization to explore them and to form common and specific representations. Then, we design a novel one-step multi-view clustering framework, which unifies the dual representation learning and multi-view clustering partition into one process. In this way, a mutual self-taught mechanism is developed in this framework and leads to more promising clustering performance. Finally, we also introduce the maximum entropy and orthogonal constraint to achieve optimal clustering results. Extensive experiments on seven real world multi-view datasets demonstrate the effectiveness of the proposed method.},
  archive      = {J_AIR},
  author       = {Zhang, Wei and Deng, Zhaohong and Choi, Kup-Sze and Wang, Jun and Wang, Shitong},
  doi          = {10.1007/s10462-025-11183-0},
  journal      = {Artificial Intelligence Review},
  month        = {7},
  number       = {7},
  pages        = {1-30},
  shortjournal = {Artif. Intell. Rev.},
  title        = {Dual representation learning for one-step clustering of multi-view data},
  volume       = {58},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Speech emotion recognition in conversations using artificial intelligence: A systematic review and meta-analysis. <em>AIR</em>, <em>58</em>(7), 1-48. (<a href='https://doi.org/10.1007/s10462-025-11197-8'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Manifestations of emotion in social conversational interactions stand at a focal point in the rapidly growing affective computing area, with applications in healthcare, education and human-computer interaction. Artificial intelligence (AI) holds great potential in modeling the challenging dynamic nature of affect in speech conversation. In this paper, we analyze and criticize latest trends and open problems through a systematic review and multi-subgroup meta-analysis of AI approaches for emotion recognition in conversation (ERC). We adopt the PRISMA-DTA guidelines toward analysis of AI-driven speech ERC. A comprehensive database search through predefined query strings and selection criteria allowed for data extraction of essential diagnostic performance parameters. We analyze salient patterns related to methodological quality and risk of bias. Univariate random-effects models are then designed with a multi-subgroup perspective, centered around affective annotations models, while encompassing the ERC parameters of modalities, feature extraction and conversation style. 51 studies were systematically reviewed for qualitative analysis, whereas 27 articles were included in the meta-analysis. Diagnostic test performance manifested with high heterogeneity, with intriguing insights regarding affective state annotation, input modality, feature extraction methods, and dataset conversation style. Our analysis raised concerns regarding bias, reporting quality and inter-rater reliability in annotations. Our research contributes fine-grained insights as recommendations that tackle open-problems in ERC. While providing valuable information on diagnostic performance of AI in speech ERC, we underscore the imperative need for further advancements in annotations and models capable of handling diverse emotional expressions. Trial Registration: PROSPERO identifier - CRD42023416879.},
  archive      = {J_AIR},
  author       = {Alhussein, Ghada and Ziogas, Ioannis and Saleem, Shiza and Hadjileontiadis, Leontios J.},
  doi          = {10.1007/s10462-025-11197-8},
  journal      = {Artificial Intelligence Review},
  month        = {7},
  number       = {7},
  pages        = {1-48},
  shortjournal = {Artif. Intell. Rev.},
  title        = {Speech emotion recognition in conversations using artificial intelligence: A systematic review and meta-analysis},
  volume       = {58},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A comprehensive survey of loss functions and metrics in deep learning. <em>AIR</em>, <em>58</em>(7), 1-172. (<a href='https://doi.org/10.1007/s10462-025-11198-7'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper presents a comprehensive review of loss functions and performance metrics in deep learning, highlighting key developments and practical insights across diverse application areas. We begin by outlining fundamental considerations in classic tasks such as regression and classification, then extend our analysis to specialized domains like computer vision and natural language processing including retrieval-augmented generation. In each setting, we systematically examine how different loss functions and evaluation metrics can be paired to address task-specific challenges such as class imbalance, outliers, and sequence-level optimization. Key contributions of this work include: (1) a unified framework for understanding how losses and metrics align with different learning objectives, (2) an in-depth discussion of multi-loss setups that balance competing goals, and (3) new insights into specialized metrics used to evaluate modern applications like retrieval-augmented generation, where faithfulness and context relevance are pivotal. Along the way, we highlight best practices for selecting or combining losses and metrics based on empirical behaviors and domain constraints. Finally, we identify open problems and promising directions, including the automation of loss-function search and the development of robust, interpretable evaluation measures for increasingly complex deep learning tasks. Our review aims to equip researchers and practitioners with clearer guidance in designing effective training pipelines and reliable model assessments for a wide spectrum of real-world applications.},
  archive      = {J_AIR},
  author       = {Terven, Juan and Cordova-Esparza, Diana-Margarita and Romero-González, Julio-Alejandro and Ramírez-Pedraza, Alfonso and Chávez-Urbiola, E. A.},
  doi          = {10.1007/s10462-025-11198-7},
  journal      = {Artificial Intelligence Review},
  month        = {7},
  number       = {7},
  pages        = {1-172},
  shortjournal = {Artif. Intell. Rev.},
  title        = {A comprehensive survey of loss functions and metrics in deep learning},
  volume       = {58},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Dombi weighted geometric aggregation operators on the class of trapezoidal-valued intuitionistic fuzzy numbers and their applications to multi-attribute group decision-making. <em>AIR</em>, <em>58</em>(7), 1-60. (<a href='https://doi.org/10.1007/s10462-025-11200-2'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The trapezoidal-valued intuitionistic fuzzy numbers (TrVIFNs) are vital in dealing with real-life decision-making problems (containing uncertainty and vagueness) in engineering and management. The study of aggregation operators on the set of trapezoidal-valued intuitionistic fuzzy numbers is essential for solving decision-making problems modelled under a trapezoidal-valued intuitionistic fuzzy (TrVIF) environment. Since TrVIFNs are the generalization class of different types of intuitionistic fuzzy numbers. The main contribution of this paper is to introduce the idea of Dombi t-norm and Dombi t-conorm based aggregation operators on the class of TrVIFNs. In this paper, firstly, we develop a Trapezoidal-Valued Intuitionistic Fuzzy Dombi Weighted Geometric operator, Trapezoidal-Valued Intuitionistic Fuzzy Dombi Order Weighted Geometric operator, Trapezoidal-Valued Intuitionistic Fuzzy Dombi Hybrid Geometric operator, and we establish mathematical properties through various theorems. Secondly, we propose a multiattribute group decision-making algorithm, such as a trapezoidal-valued multiattribute group decision-making algorithm that uses the proposed aggregation operators. Thirdly, we show the applicability of the proposed decision-making method in solving a multiattribute group decision-making problem involving the photovoltaic site selection. Further, we discuss the sensitivity analysis of the proposed algorithms to demonstrate their stability and reliability. Finally, we show the efficacy of the proposed decision-making approach by comparing it with a few familiar group decision-making methods.},
  archive      = {J_AIR},
  author       = {Meher, Bibhuti Bhusana and S, Jeevaraj and Alrasheedi, Melfi},
  doi          = {10.1007/s10462-025-11200-2},
  journal      = {Artificial Intelligence Review},
  month        = {7},
  number       = {7},
  pages        = {1-60},
  shortjournal = {Artif. Intell. Rev.},
  title        = {Dombi weighted geometric aggregation operators on the class of trapezoidal-valued intuitionistic fuzzy numbers and their applications to multi-attribute group decision-making},
  volume       = {58},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multi-criteria decision support system for the evaluation of UAV intelligent agricultural sensors. <em>AIR</em>, <em>58</em>(7), 1-25. (<a href='https://doi.org/10.1007/s10462-025-11201-1'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Precision agriculture is an emerging approach aimed at enhancing agricultural productivity through advanced technological solutions. One of the key technologies integrated into modern agriculture is Unmanned Aerial Vehicles (UAVs), which rely on various sensors to provide critical information about crop fields. However, selecting the most suitable UAV sensors remains a significant challenge due to multiple evaluation criteria and compromises. This paper proposes a novel decision-support framework based on multi-criteria decision-making/analysis (MCDA/MCDM) methods to facilitate UAV sensor selection in precision agriculture. The framework incorporates objective weight selection techniques-Standard Deviation, Entropy, CRITIC, and MEREC-eliminating the need for subjective expert involvement. Furthermore, four MCDA/MCDM methods, including the newly proposed COmbined COmpromise solution with Characteristic Objects METhod (COCOCOMET), are applied to evaluate sensor alternatives. To validate the framework, a case study is conducted using a dataset of UAV sensors, where multiple evaluation criteria are analyzed to determine the most suitable sensor. The results confirm the framework’s effectiveness, demonstrating its robustness and stability in decision-making. Sensitivity analysis and comparative studies further highlight its reliability, particularly in addressing rank reversal issues commonly found in existing MCDA methods such as TOPSIS and AHP. The proposed framework not only provides a structured and adaptable evaluation process for UAV sensors but also offers broader applicability in agricultural decision-making.},
  archive      = {J_AIR},
  author       = {Kizielewicz, Bartłomiej and Wątróbski, Jarosław and Sałabun, Wojciech},
  doi          = {10.1007/s10462-025-11201-1},
  journal      = {Artificial Intelligence Review},
  month        = {7},
  number       = {7},
  pages        = {1-25},
  shortjournal = {Artif. Intell. Rev.},
  title        = {Multi-criteria decision support system for the evaluation of UAV intelligent agricultural sensors},
  volume       = {58},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Pythagorean linguistic information-based green supplier selection using quantum-based group decision-making methodology and the MULTIMOORA approach. <em>AIR</em>, <em>58</em>(7), 1-34. (<a href='https://doi.org/10.1007/s10462-025-11205-x'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The selection of environmentally sustainable suppliers has been a significant challenge in management decision-making (DM). Multicriteria group decision-making (MCGDM) is a ranking methodology used to select suppliers, but it is complex and influenced by the different opinions of decision-makers. Once again, extensive research on MCGDM has exposed inadequacies in the trustworthiness of experts’ judgements, which profoundly impact the ultimate ranking results. The Pythagorean linguistic number (PLN) concept has been used to address MCGDM by considering experts’ confidence levels and real-world scenarios. This study introduces an extensive technique using a quantum scenario-based Bayesian network (QSBN) and Deng entropy-based belief entropy to account for the interference of beliefs. The goal is to replicate the subjectivity of experts’ opinions during different stages of DM, including the accumulation of experts’ weights and alternative probabilities. The correlation coefficient of PLNs is introduced for determining criterion weights and employing new techniques based on entropy methods for experts’ weights. The MULTIMOORA approach consolidates the probability of alternatives in QSBN among all experts, and the interference value is computed using belief entropy, an index for quantifying the probability of uncertainty. The study provides a numerical example to illustrate the proposed methodology, specifically focusing on selecting environmentally sustainable suppliers, and demonstrates its applicability and effectiveness.},
  archive      = {J_AIR},
  author       = {Mandal, Prasenjit and Mrsic, Leo and Kalampakas, Antonios and Allahviranloo, Tofigh and Samanta, Sovan},
  doi          = {10.1007/s10462-025-11205-x},
  journal      = {Artificial Intelligence Review},
  month        = {7},
  number       = {7},
  pages        = {1-34},
  shortjournal = {Artif. Intell. Rev.},
  title        = {Pythagorean linguistic information-based green supplier selection using quantum-based group decision-making methodology and the MULTIMOORA approach},
  volume       = {58},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). An enhanced framework for real-time dense crowd abnormal behavior detection using YOLOv8. <em>AIR</em>, <em>58</em>(7), 1-50. (<a href='https://doi.org/10.1007/s10462-025-11206-w'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abnormal behavior detection in dense crowd, during the Hajj pilgrimage is vital to public security. Existing approaches face challenges due to factors like occlusions, illumination variations, and uniform attire. This research introduces the Crowd Anomaly Detection Framework (CADF), an improved YOLOv8-based model, integrating Soft-NMS to improve detection accuracy under complex conditions. CADF extensively evaluated on the Hajjv2 dataset, delivering an AUC of 88.27%, a 13.09% improvement over YOLOv2 and 12.19% over YOLOv5, with an Accuracy of 91.6%. To validate its generalizability, the framework is also tested on UCSD and ShanghaiTech datasets. Comparisons with state-of-the-art models, including VGG19 and EfficientDet, demonstrated CADF’s superiority in accuracy, AUC, precision, recall, and mAP metrics. By addressing the unique challenges of Hajj crowd and achieving strong performance across diverse datasets, CADF highlights its potential for real-time crowd anomaly detection, contributing to enhanced safety in large-scale public gatherings and aligning with Sustainable Development Goals 3 and 11.},
  archive      = {J_AIR},
  author       = {Nasir, Rabia and Jalil, Zakia and Nasir, Muhammad and Alsubait, Tahani and Ashraf, Maria and Saleem, Sadia},
  doi          = {10.1007/s10462-025-11206-w},
  journal      = {Artificial Intelligence Review},
  month        = {7},
  number       = {7},
  pages        = {1-50},
  shortjournal = {Artif. Intell. Rev.},
  title        = {An enhanced framework for real-time dense crowd abnormal behavior detection using YOLOv8},
  volume       = {58},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). AI-driven job scheduling in cloud computing: A comprehensive review. <em>AIR</em>, <em>58</em>(7), 1-113. (<a href='https://doi.org/10.1007/s10462-025-11208-8'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The demand for efficient job scheduling in cloud computing has grown significantly with the rise of dynamic and heterogeneous cloud environments. While effective in simpler systems, traditional scheduling algorithms fail to meet the complex requirements of modern cloud infrastructures. These limitations motivate the need for AI-driven solutions that offer adaptability, scalability, and energy efficiency. This paper comprehensively reviews AI-based job scheduling techniques, addressing several key research gaps in current approaches. The existing methods face challenges such as resource heterogeneity, energy consumption, and real-time adaptability in multi-cloud systems. Accordingly, the support of AI-based job scheduling in cloud computing is summarized here toward machine learning, optimization techniques, heuristic techniques, and hybrid AI models. This paper pointedly underlines the strengths and weaknesses of various approaches through deep comparative analysis and focuses on how AI will overcome traditional algorithm shortcomings. Is worth noticing that several important improvements this kind of AI-driven model provides, for example, in resource allocation, cost efficiency, energy consumption, and complex dependencies between jobs and system faults. In the end, AI-driven job scheduling seems to be a promising avenue toward effectively responding to the booming demands of cloud infrastructures. Future research should concentrate on three major outlooks: scalability, better integration of AI with traditional scheduling methods, and the use of other emerging technologies like edge computing and blockchain for better optimization of cloud-based job scheduling. The paper underscores the need for more adaptive, secure, and energy-efficient scheduling frameworks to meet the evolving challenges of cloud environments.},
  archive      = {J_AIR},
  author       = {Sanjalawe, Yousef and Al-E’mari, Salam and Fraihat, Salam and Makhadmeh, Sharif},
  doi          = {10.1007/s10462-025-11208-8},
  journal      = {Artificial Intelligence Review},
  month        = {7},
  number       = {7},
  pages        = {1-113},
  shortjournal = {Artif. Intell. Rev.},
  title        = {AI-driven job scheduling in cloud computing: A comprehensive review},
  volume       = {58},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A comprehensive review of machine learning applications for internet of nano things: Challenges and future directions. <em>AIR</em>, <em>58</em>(7), 1-76. (<a href='https://doi.org/10.1007/s10462-025-11211-z'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, advances in nanotechnology and the Internet of Things (IoT) have led to the development of the revolutionary Internet of Nano Things (IoNT). IoNT, has found very similar real-life applications in agriculture, military, multimedia, and healthcare. However, despite the rapid advancements in both IoNT and machine learning (ML), there has been no comprehensive review explicitly focused on the integration of these two fields. Existing surveys and reviews on IoNT primarily address its architecture, communication methods, and domain-specific applications, yet overlook the critical role ML could play in enhancing IoNT’s capabilities–particularly in data processing, anomaly detection, and security. This survey addresses this gap by providing an in-depth analysis of IoNT-ML integration, reviewing state-of-the-art ML applications within IoNT, and systematically discussing the challenges that persist in this integration. Additionally, we propose future research directions, establishing a framework to guide advancements in IoNT through ML-driven solutions.},
  archive      = {J_AIR},
  author       = {Rana, Aryan and Gautam, Deepika and Kumar, Pankaj and Kumar, Kranti and Vasilakos, Athanasios V. and Das, Ashok Kumar and K, Vivekananda Bhat},
  doi          = {10.1007/s10462-025-11211-z},
  journal      = {Artificial Intelligence Review},
  month        = {7},
  number       = {7},
  pages        = {1-76},
  shortjournal = {Artif. Intell. Rev.},
  title        = {A comprehensive review of machine learning applications for internet of nano things: Challenges and future directions},
  volume       = {58},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Comparative analysis of methodologies and approaches in recommender systems utilizing large language models. <em>AIR</em>, <em>58</em>(7), 1-36. (<a href='https://doi.org/10.1007/s10462-025-11189-8'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recommendation systems are indispensable technologies nowadays, as they enable analysis of the huge amount of information available on the internet, helping consumers to make decisions effectively. Ongoing efforts are essential to further develop and align them with the evolving demands of the modern era. In the last few years, large language models (LLMs) have made a huge leap in natural language processing. This advancement has directed researchers’ efforts towards employing these models in various fields, including recommender systems, to leverage the vast amount of data they were trained on. This paper presents a comparative study of a set of recent methodologies that adapt LLMs to recommendations. Throughout the discussed research work, we come up with the insight that LLMs offer significant benefits due to the amount of knowledge they possess and their powerful ability to represent textual data effectively, making them useful in common recommendation issues like cold-start. Also, the variety of fine-tuning and in-context learning techniques enables adaptation of LLMs to a wide range of recommendation tasks. We discussed issues addressed in the reviewed research work and the solutions proposed to enhance recommendation systems. To provide a clearer understanding, we propose taxonomies to categorize the reviewed work based on underlying techniques, involving the role of LLMs in recommendations, learning paradigms, and system structures. We explore datasets, recommendation- and language-related metrics commonly used in this domain. Finally, we analyzed findings in related work, highlighting possible strengths and limitations of using LLMs in recommender systems.},
  archive      = {J_AIR},
  author       = {Elmoghazy, Salma S. and Shouman, Marwa A. and Elminir, Hamdy K. and Selim, Gamal Eldin I.},
  doi          = {10.1007/s10462-025-11189-8},
  journal      = {Artificial Intelligence Review},
  month        = {7},
  number       = {7},
  pages        = {1-36},
  shortjournal = {Artif. Intell. Rev.},
  title        = {Comparative analysis of methodologies and approaches in recommender systems utilizing large language models},
  volume       = {58},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Enhancing decentralized energy storage investments with artificial intelligence-driven decision models. <em>AIR</em>, <em>58</em>(7), 1-35. (<a href='https://doi.org/10.1007/s10462-025-11204-y'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Decentralized energy storage investments play a crucial role in enhancing energy efficiency and promoting renewable energy integration. However, the complexity of these projects and the limited resources of the companies make it necessary to determine strategic priorities. This paper tries to define effective investment strategies for the improvements of the decentralized energy storage projects. In the first stage, the selection of mass experts is made via information gain-based mass expert selection. Next, the assessments of the experts are balanced based on the opinion of the best expert by using q-learning algorithm. Moreover, determinants of decentralized energy storage investments are examined with molecular fuzzy (MF) cognitive maps. Finally, strategy alternatives for decentralized energy storage investments are ranked with MF multi-objective particle swarm optimization (MOPSO). The main contribution of this study is the identification of the most effective decentralized energy storage investment alternatives by establishing a novel model. The main novelty of the proposed model is that considering information gain-based mass expert selection technique allows for higher consistency and decision efficiency. Owing to this issue, the decision-making process is accelerated, and the applicability of the results increases. The findings indicate that customer expectations (weight: 0.2577) and financial issues (weight: 0.2513) are the most essential criteria in improving the performance of decentralized energy storage investments. Furthermore, hydrogen-based energy storage (average value: 0.1878) and distributed battery swapping stations (average value: 0.1877) are the most important decentralized energy storage investment alternatives.},
  archive      = {J_AIR},
  author       = {Kou, Gang and Dinçer, Hasan and Ergün, Edanur and Eti, Serkan and Yüksel, Serhat and Hacıoğlu, Ümit},
  doi          = {10.1007/s10462-025-11204-y},
  journal      = {Artificial Intelligence Review},
  month        = {7},
  number       = {7},
  pages        = {1-35},
  shortjournal = {Artif. Intell. Rev.},
  title        = {Enhancing decentralized energy storage investments with artificial intelligence-driven decision models},
  volume       = {58},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Gene selection for single cell RNA-seq data via fuzzy rough iterative computation model. <em>AIR</em>, <em>58</em>(7), 1-37. (<a href='https://doi.org/10.1007/s10462-025-11213-x'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Single cell RNA-seq data have the characteristics of small samples, high dimension and noise. Due to these characteristics, gene selection must be carried out before clustering and classifying. This study explores gene selection in a single cell gene decision space (scgd-space) via fuzzy rough iterative computation model (FRIC-model). First, in order to overcome the strictness of the equality between gene expression values, the equality between gene expression values is replaced by the distance between gene expression values, and the fuzzy symmetric relation on the cell set of a scgd-space is defined. In this fuzzy symmetric relation, two variable parameters are introduced: one controls gene subsets, the other dominates the distance between gene expression values. Then, FRIC-model in a scgd-space is established, which overcomes the deficiencies of classical rough set model and fuzzy rough set model. This model applies the iterative computation strategy to define some evaluation functions. These functions include fuzzy rough approximations and dependency functions. Next, a gene selection algorithm based on FRIC-model is designed. At last, the designed algorithm is testified in several publicly open single cell RNA-seq datasets to estimate its performance. The experimental results show that the designed algorithm is more effective than some existing algorithms, is fast and does not occupy too much memory.},
  archive      = {J_AIR},
  author       = {Li, Zhaowen and Zhang, Jie and Wang, Yuxian and Liu, Fang and Wen, Ching-Feng},
  doi          = {10.1007/s10462-025-11213-x},
  journal      = {Artificial Intelligence Review},
  month        = {7},
  number       = {7},
  pages        = {1-37},
  shortjournal = {Artif. Intell. Rev.},
  title        = {Gene selection for single cell RNA-seq data via fuzzy rough iterative computation model},
  volume       = {58},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Scalable multi-modal representation learning networks. <em>AIR</em>, <em>58</em>(7), 1-24. (<a href='https://doi.org/10.1007/s10462-025-11224-8'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-modal representation learning is recognized for its comprehensive interpretation across diverse modalities. Although existing approaches have yielded favorable results, they face challenges in high-order information preservation and out-of-sample data generalization. To tackle these issues, we propose a scalable multi-modal representation learning networks framework, which aims to learn optimal modality-specific projection matrices to project multi-modal features to a shared representation space. Specifically, weight guided modality-wise and row-sparsity driven feature-wise measures are considered to achieve adaptively hierarchical feature selection from the original data. Then, within the unified latent representation space, we employ hypergraph embedding to preserve the intricate high-order local geometric structures within the modality-specific high-dimensional spaces. Finally, we propose a proximal operator-inspired network architecture to resolve the optimization objectives, streamlining the process of feature auto-weighted selection and representation learning. The experimental results highlight the effectiveness and superiority of the proposed method, while online testing on out-of-sample data further demonstrates robust generalization. The code of the proposed method is publicly available at: https://github.com/ZihanFang11/SMMRL .},
  archive      = {J_AIR},
  author       = {Fang, Zihan and Zou, Ying and Lan, Shiyang and Du, Shide and Tan, Yanchao and Wang, Shiping},
  doi          = {10.1007/s10462-025-11224-8},
  journal      = {Artificial Intelligence Review},
  month        = {7},
  number       = {7},
  pages        = {1-24},
  shortjournal = {Artif. Intell. Rev.},
  title        = {Scalable multi-modal representation learning networks},
  volume       = {58},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Fast machine learning for building management systems. <em>AIR</em>, <em>58</em>(7), 1-48. (<a href='https://doi.org/10.1007/s10462-025-11226-6'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Building management systems (BMSs) are increasingly integrating advanced machine learning (ML) and artificial intelligence (AI) capabilities to enhance operational efficiency and responsiveness. The transformation of BMSs involves a wide range of environmental, behavioural, economical and technical factors as well as optimum performance considerations in order to reach energy efficiency and for long term sustainability. Existing BMSs can only provide local adaptability by creating and managing information for a built asset lacking the capability to learn and adapt based on performance objectives. This research provides a comprehensive review of ML techniques in BMSs, with particular emphasis and demonstration of fast machine learning (FastML) techniques in a real-case study application. The study reviews optimization methods for ML algorithms, focusing on Long Short-Term Memory (LSTM) networks for energy consumption forecasting and exploring solutions that leverage hardware accelerators for low-latency and high-throughput processing. The High-Level Synthesis for Machine Learning (HLS4ML) framework facilitates deployment of fast machine learning models with BMSs, achieving substantial gains in hardware efficiency and inference speed in resource-constrained environments. Findings reveal that HLS4ML-optimized models maintain accuracy while offering computational efficiency through techniques like pruning and quantization, supporting real-time BMS applications. This research significantly contributes to the development of intelligent BMSs by integrating ML algorithms with advanced hardware solutions, ultimately improving energy management, occupant comfort, and safety in modern buildings.},
  archive      = {J_AIR},
  author       = {Mshragi, Mohammed and Petri, Ioan},
  doi          = {10.1007/s10462-025-11226-6},
  journal      = {Artificial Intelligence Review},
  month        = {7},
  number       = {7},
  pages        = {1-48},
  shortjournal = {Artif. Intell. Rev.},
  title        = {Fast machine learning for building management systems},
  volume       = {58},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Artificial intelligence in chemical exchange saturation transfer magnetic resonance imaging. <em>AIR</em>, <em>58</em>(7), 1-25. (<a href='https://doi.org/10.1007/s10462-025-11227-5'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This review delves into the transformative role of Artificial Intelligence (AI) in advancing Chemical Exchange Saturation Transfer (CEST) Magnetic Resonance Imaging (MRI), a cutting-edge imaging method for non-invasive biochemical mapping. CEST MRI faces many technical challenges that hinder its clinical adoption. AI-driven approaches have emerged as one of the promising solutions to address some of these limitations. The evolution of AI in CEST MRI is traced from its inception, with pioneering studies in AI-driven image analysis, to current trends reflecting a marked increase in AI-related CEST publications. This review highlights AI’s impact on various stages of the CEST MRI pipeline, including accelerated imaging acquisition and reconstruction, improved pre-processing and denoising methods, and advanced quantification techniques. Furthermore, AI has demonstrated potential in clinical applications, such as disease diagnosis, molecular subtyping, and treatment monitoring, underscoring its growing relevance in the field. This review also examines the challenges in AI applications and future directions in CEST MRI, including the use of synthetic data, the explainability and interpretability of AI models, and their implications for clinical adoption. Overall, this review provides a comprehensive understanding of the current state of AI applications in CEST MRI and will inspire further research to unlock the full potential of this powerful molecular imaging technique.},
  archive      = {J_AIR},
  author       = {Pan, Swee Qi and Hum, Yan Chai and Lai, Khin Wee and Yap, Wun-She and Zhang, Yi and Heo, Hye-Young and Tee, Yee Kai},
  doi          = {10.1007/s10462-025-11227-5},
  journal      = {Artificial Intelligence Review},
  month        = {7},
  number       = {7},
  pages        = {1-25},
  shortjournal = {Artif. Intell. Rev.},
  title        = {Artificial intelligence in chemical exchange saturation transfer magnetic resonance imaging},
  volume       = {58},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Learning semantic consistency for audio-visual zero-shot learning. <em>AIR</em>, <em>58</em>(7), 1-28. (<a href='https://doi.org/10.1007/s10462-025-11228-4'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Audio-visual zero-shot learning requires an understanding of the relationship between audio and visual information to determine unseen classes. Despite many efforts and significant progress in the field, many existing methods tend to focus on learning strong representations, neglecting the semantic consistency between audio and video as well as the inherent hierarchical structure of the data. To address these issues, we propose Learning Semantic Consistency for Audio-Visual Zero-shot Learning. Specifically, we employ an attention mechanism to enhance cross-modal information interactions, aiming to capture the semantic consistency between audio and visual data. Meanwhile, we introduce a hyperbolic space to model the hierarchical structure of the data itself. Moreover, the proposed approach includes a novel loss function that considers the relationships between input modalities, reducing the distance between features of different modalities. To evaluate the proposed method, we test it on three benchmark datasets $$\hbox {VGGSound-GZS}{{\textrm{L}}^{cls}}$$ , $$\hbox {UCF-GZS}{{\textrm{L}}^{cls}}$$ , and $$\hbox {ActivityNet-GZS}{{\textrm{L}}^{cls}}$$ . Extensive experimental results show that the proposed method achieves state-of-the-art performance on all three datasets. For example, on the $$\hbox {UCF-GZS}{{\textrm{L}}^{cls}}$$ dataset, the harmonic mean is improved by 5.7%. Code and data available at https://github.com/ybyangjing/LSC-AVZSL .},
  archive      = {J_AIR},
  author       = {Li, Xiaoyong and Yang, Jing and Chen, Yuling and Zhang, Wei and Ruan, Xiaoli and Li, Chengjiang and Su, Zhidong},
  doi          = {10.1007/s10462-025-11228-4},
  journal      = {Artificial Intelligence Review},
  month        = {7},
  number       = {7},
  pages        = {1-28},
  shortjournal = {Artif. Intell. Rev.},
  title        = {Learning semantic consistency for audio-visual zero-shot learning},
  volume       = {58},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Natural language processing in the patent domain: A survey. <em>AIR</em>, <em>58</em>(7), 1-62. (<a href='https://doi.org/10.1007/s10462-025-11168-z'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Patents, which encapsulate crucial technical and legal information in text form and referenced drawings, present a rich domain for natural language processing (NLP). As NLP technologies evolve, large language models (LLMs) have demonstrated outstanding capabilities in general text processing and generation tasks. However, the application of LLMs in the patent domain remains under-explored and under-developed due to the complexity of patents, particularly their language and legal framework. Understanding the unique characteristics of patent documents and related research in the patent domain becomes essential for researchers to apply these tools effectively. Therefore, this paper aims to equip NLP researchers with the essential knowledge to navigate this complex domain efficiently. We introduce the relevant fundamental aspects of patents to provide solid background information. In addition, we systematically break down the structural and linguistic characteristics unique to patents and map out how NLP can be leveraged for patent analysis and generation. Moreover, we demonstrate the spectrum of text-based and multimodal patent-related tasks, including nine patent analysis and four patent generation tasks.},
  archive      = {J_AIR},
  author       = {Jiang, Lekang and Goetz, Stephan M.},
  doi          = {10.1007/s10462-025-11168-z},
  journal      = {Artificial Intelligence Review},
  month        = {7},
  number       = {7},
  pages        = {1-62},
  shortjournal = {Artif. Intell. Rev.},
  title        = {Natural language processing in the patent domain: A survey},
  volume       = {58},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A review on 3D gaussian splatting for sparse view reconstruction. <em>AIR</em>, <em>58</em>(7), 1-40. (<a href='https://doi.org/10.1007/s10462-025-11171-4'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Sparse view 3D reconstruction remains challenging due to inherent data scale limitations. Mainstream sparse view 3D reconstruction algorithms based on the NeRF framework struggle to balance generation quality and real-time performance. Recently, the advent of 3D Gaussian Splatting technology has demonstrated remarkable results, becoming increasingly prominent in 3D scene representation and reconstruction. Exploring the application of 3D Gaussian Splatting technology for sparse view 3D reconstruction represents a promising research avenue. Based on this, our paper provides a comprehensive review of current sparse view 3D reconstruction methods leveraging 3D Gaussian Splatting, with an emphasis on extracting effective reconstruction information from input images and utilizing these data to generate realistic scenes efficiently and reliably. We then provide a detailed discussion on how the algorithm addresses issues such as artifacts and scale ambiguous, which are common challenges in this field. In the subsequent sections, we present both quantitative and qualitative comparisons of various sparse-view 3D reconstruction methods, roughly demonstrating the advantages of sparse view 3D Gaussian splatting methods in terms of reconstruction quality and efficiency. Furthermore, we analyze the potential applications of sparse view 3D Gaussian splatting methods. Finally, we identify the challenges faced by sparse-view 3D Gaussian splatting reconstruction and suggest potential solutions. We hope that our analysis will provide valuable insights for future research efforts.},
  archive      = {J_AIR},
  author       = {Liu, Haitian and Liu, Binglin and Hu, Qianchao and Du, Peilun and Li, Jing and Bao, Yang and Wang, Feng},
  doi          = {10.1007/s10462-025-11171-4},
  journal      = {Artificial Intelligence Review},
  month        = {7},
  number       = {7},
  pages        = {1-40},
  shortjournal = {Artif. Intell. Rev.},
  title        = {A review on 3D gaussian splatting for sparse view reconstruction},
  volume       = {58},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A novel fuzzy neural network approach with triangular fuzzy information for the selection of logistics service providers. <em>AIR</em>, <em>58</em>(7), 1-29. (<a href='https://doi.org/10.1007/s10462-025-11209-7'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, we presents a novel fuzzy neural network approach designed to address multi criteria decision making (MCDM) problems, specifically for selecting logistics service providers. The proposed decision making model integrates triangular fuzzy numbers (TFNs) with a triangular fuzzy Einstein weighted averaging (TFEWA) aggregation operator to enhance the decision making process under uncertainty. Initially, we discussed the concept of triangular fuzzy numbers, which allows for the representation of uncertain and imprecise data typically presented in real-world decision making environments. The operational laws, score function, and Hamming distance measures for TFNs are presented to ensure accurate handling of the fuzzy input data. The TFEWA aggregation operator, which is based on Einstein norms and plays a crucial role in aggregating expert opinions in the evaluation process. In the decision making process, we collect expert opinions regarding logistics service providers, expressed as TFNs, which are then processed through the fuzzy neural network model. After that, we apply the proposed decision making model to select the best logistics service providers. The TFEWA operator computes values at the hidden and output layers, and activation functions are applied to produce final output values. These outputs provide a ranked list of logistics service providers based on their overall performance across multiple criteria. The effectiveness of this novel approach is validated through a comparative analysis with existing MCDM methods. The results demonstrate that the triangular fuzzy neural network approach outperforms traditional methods in terms of flexibility, accuracy, and its ability to handle uncertain, fuzzy data. Our method provides a robust decision support system, capable of managing complex decision making tasks in logistics and other fields.},
  archive      = {J_AIR},
  author       = {Wang, Lifang and Abdullah, Saleem and Rahimzai, Ariana Abdul and Ullah, Ihsan},
  doi          = {10.1007/s10462-025-11209-7},
  journal      = {Artificial Intelligence Review},
  month        = {7},
  number       = {7},
  pages        = {1-29},
  shortjournal = {Artif. Intell. Rev.},
  title        = {A novel fuzzy neural network approach with triangular fuzzy information for the selection of logistics service providers},
  volume       = {58},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Unsupervised clustering optimization-based efficient attention in YOLO for underwater object detection. <em>AIR</em>, <em>58</em>(7), 1-38. (<a href='https://doi.org/10.1007/s10462-025-11218-6'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Underwater object detection is a prerequisite for underwater robots to realize ocean exploration and autonomous grasping. However, underwater detection tasks face some inevitable interference factors, such as poor imaging quality, strong environment randomness, and high organism concealment. These phenomena will lead to strong underwater background interference and weak underwater object perception, which greatly aggravates the difficulty of underwater object detection. In order to deal with the above problems, we propose an unsupervised clustering optimization-based efficient attention (UCOEA). Different from the channel-wise strategy, cross-channel strategy and channel grouping strategy, we design a channel clustering strategy, which achieves autonomous dynamic screening of channel information by using the K-Means algorithm. Same types of channel information with high redundancy are learned uniformly to share the same operation. Different types of channel information with high specificity are learned independently to avoid channel noise information interference. Different from the single spatial strategy and multiple spatial strategy, we design a spatial clustering strategy, which achieves autonomous dynamic stripping of spatial information by using the EM algorithm. This strategy can extract multiple required spatial information at one time from different spatial locations. We further assign learnable weight parameters to distinguish dominant information and auxiliary information, which can alleviate spatial noise information interference. Our strategies can better balance additional cost overhead and information processing quality, which is crucial for the proposed attention to achieve fast and accurate underwater information calibration. In order to achieve high-precision and real-time underwater object detection, we propose a combined system of UCOEA underwater adapter and one-stage YOLO detector, which can efficiently detect small, medium and large targets at the same time. Extensive experiments demonstrate the effectiveness of our work. More importantly, we publish an underwater detection dataset DLMU2024 with low image continuity and high data diversity, which provides reliable support for the rapid development of underwater detection research. Our dataset is available at https://github.com/shenxin-dlmu/DLMU2024 .},
  archive      = {J_AIR},
  author       = {Shen, Xin and Yuan, Guoliang and Wang, Huibing and Fu, Xianping},
  doi          = {10.1007/s10462-025-11218-6},
  journal      = {Artificial Intelligence Review},
  month        = {7},
  number       = {7},
  pages        = {1-38},
  shortjournal = {Artif. Intell. Rev.},
  title        = {Unsupervised clustering optimization-based efficient attention in YOLO for underwater object detection},
  volume       = {58},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A comprehensive survey of deep learning for time series forecasting: Architectural diversity and open challenges. <em>AIR</em>, <em>58</em>(7), 1-95. (<a href='https://doi.org/10.1007/s10462-025-11223-9'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Time series forecasting is a critical task that provides key information for decision-making across various fields, such as economic planning, supply chain management, and medical diagnosis. After the use of traditional statistical methodologies and machine learning in the past, various fundamental deep learning architectures such as MLPs, CNNs, RNNs, and GNNs have been developed and applied to solve time series forecasting problems. However, the structural limitations caused by the inductive biases of each deep learning architecture constrained their performance. Transformer models, which excel at handling long-term dependencies, have become significant architectural components for time series forecasting. However, recent research has shown that alternatives such as simple linear layers can outperform Transformers. These findings have opened up new possibilities for using diverse architectures, ranging from fundamental deep learning models to emerging architectures and hybrid approaches. In this context of exploration into various models, the architectural modeling of time series forecasting has now entered a renaissance. This survey not only provides a historical context for time series forecasting but also offers comprehensive and timely analysis of the movement toward architectural diversification. By comparing and re-examining various deep learning models, we uncover new perspectives and present the latest trends in time series forecasting, including the emergence of hybrid models, diffusion models, Mamba models, and foundation models. By focusing on the inherent characteristics of time series data, we also address open challenges that have gained attention in time series forecasting, such as channel dependency, distribution shift, causality, and feature extraction. This survey explores vital elements that can enhance forecasting performance through diverse approaches. These contributions help lower entry barriers for newcomers by providing a systematic understanding of the diverse research areas in time series forecasting (TSF), while offering seasoned researchers broader perspectives and new opportunities through in-depth exploration of TSF challenges.},
  archive      = {J_AIR},
  author       = {Kim, Jongseon and Kim, Hyungjoon and Kim, HyunGi and Lee, Dongjun and Yoon, Sungroh},
  doi          = {10.1007/s10462-025-11223-9},
  journal      = {Artificial Intelligence Review},
  month        = {7},
  number       = {7},
  pages        = {1-95},
  shortjournal = {Artif. Intell. Rev.},
  title        = {A comprehensive survey of deep learning for time series forecasting: Architectural diversity and open challenges},
  volume       = {58},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). SFIAD: Deepfake detection through spatial-frequency feature integration and dynamic margin optimization. <em>AIR</em>, <em>58</em>(7), 1-22. (<a href='https://doi.org/10.1007/s10462-025-11225-7'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The rapid advancement of generative models has profoundly transformed the field of digital content creation, bringing unprecedented opportunities for media generation. However, the widespread adoption of this technology has also led to the emergence of highly realistic fake facial images and videos, which pose significant threats to public trust and societal security. To address the challenges of deepfake detection, this paper proposes a novel method based on Spatial-Frequency Feature Integration (SFFI), which effectively identifies fake content by combining spatial and frequency features of images. Additionally, to tackle the issue of class imbalance in the datasets, we propose an Authenticity-Aware Margin Loss (AAML). This loss function dynamically adjusts the decision boundary to enhance the model’s ability to recognize minority class samples. The proposed method was trained and evaluated on four challenging datasets: FaceForensics++, Celeb-DF v1, Celeb-DF v2, and the DeepFake Detection Challenge Preview, and compared against ten state-of-the-art methods. Experimental results demonstrate that the proposed method consistently outperforms all existing approaches across all datasets.},
  archive      = {J_AIR},
  author       = {Kou, Yi and Li, Peng and Ma, Hongjiang and Zhou, Jiliu and Huang, Zhan ao and Li, Xiaojie},
  doi          = {10.1007/s10462-025-11225-7},
  journal      = {Artificial Intelligence Review},
  month        = {7},
  number       = {7},
  pages        = {1-22},
  shortjournal = {Artif. Intell. Rev.},
  title        = {SFIAD: Deepfake detection through spatial-frequency feature integration and dynamic margin optimization},
  volume       = {58},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A comprehensive survey of specularity detection: State-of-the-art techniques and breakthroughs. <em>AIR</em>, <em>58</em>(7), 1-53. (<a href='https://doi.org/10.1007/s10462-025-11233-7'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Specularity poses significant challenges in computer vision (CV), often leading to performance degradation in various tasks. Despite its importance, the CV field lacks a comprehensive review of specularity detection techniques. This survey addresses this gap by synthesizing diverse definitions of specularity and providing a unified framework to enhance consistency. It also presents a systematic review of traditional and deep learning-based methods for detecting specularity. Comparative experiments on a standardized dataset enable in-depth evaluation of each method, highlighting their strengths and limitations. The survey further provides structured insights and guidance for selecting appropriate methods across diverse scenarios. Through this, it identifies key areas for future research, aiming to support the development of more advanced detection models. By integrating diverse methodologies and quantitative analyzes, this survey contributes to a deeper understanding of current advancements and potential innovations in specularity detection.},
  archive      = {J_AIR},
  author       = {Li, Fengze and Ma, Jieming and Liang, Hai-Ning and Tian, Zhongbei and Wu, Zhijing and Wen, Tianxi and Liu, Dawei},
  doi          = {10.1007/s10462-025-11233-7},
  journal      = {Artificial Intelligence Review},
  month        = {7},
  number       = {7},
  pages        = {1-53},
  shortjournal = {Artif. Intell. Rev.},
  title        = {A comprehensive survey of specularity detection: State-of-the-art techniques and breakthroughs},
  volume       = {58},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Learn to optimise for job shop scheduling: A survey with comparison between genetic programming and reinforcement learning. <em>AIR</em>, <em>58</em>(6), 1-53. (<a href='https://doi.org/10.1007/s10462-024-11059-9'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Job shop scheduling holds significant importance due to its relevance and impact on various industrial and manufacturing processes. It involves dynamically assigning and sequencing jobs to machines in a flexible production environment, where job characteristics, machine availability, and other factors might change over time. Genetic programming and reinforcement learning have emerged as powerful approaches to automatically learn high-quality scheduling heuristics or directly optimise sequences of specific job-machine pairs to generate efficient schedules in manufacturing. Existing surveys on job shop scheduling typically provide overviews from a singular perspective, focusing solely on genetic programming or reinforcement learning, but overlook the hybridisation and comparison of both approaches. This survey aims to bridge this gap by reviewing recent developments in genetic programming and reinforcement learning approaches for job shop scheduling problems, providing a comparison in terms of the learning principles and characteristics for solving different kinds of job shop scheduling problems. In addition, this survey identifies and discusses current issues and challenges in the field of learning to optimise for job shop scheduling. This comprehensive exploration of genetic programming and reinforcement learning in job shop scheduling provides valuable insights into the learning principles for optimising different job shop scheduling problems. It deepens our understanding of recent developments, suggesting potential research directions for future advancements.},
  archive      = {J_AIR},
  author       = {Xu, Meng and Mei, Yi and Zhang, Fangfang and Zhang, Mengjie},
  doi          = {10.1007/s10462-024-11059-9},
  journal      = {Artificial Intelligence Review},
  month        = {6},
  number       = {6},
  pages        = {1-53},
  shortjournal = {Artif. Intell. Rev.},
  title        = {Learn to optimise for job shop scheduling: A survey with comparison between genetic programming and reinforcement learning},
  volume       = {58},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Adaptive gaining-sharing knowledge-based variant algorithm with historical probability expansion and its application in escape maneuver decision making. <em>AIR</em>, <em>58</em>(6), 1-43. (<a href='https://doi.org/10.1007/s10462-024-11096-4'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To further improve the performance of adaptive gaining-sharing knowledge-based algorithm (AGSK), a novel adaptive gaining sharing knowledge-based algorithm with historical probability expansion (HPE-AGSK) is proposed by modifying the search strategies. Based on AGSK, three improvement strategies are proposed. First, expansion sharing strategy is proposed and added in junior gaining-sharing phase to boost local search ability. Second, historical probability expansion strategy is proposed and added in senior gaining-sharing phase to strengthen global search ability. Last, reverse gaining strategy is proposed and utilized to expand population distribution at the beginning of iterations. The performance of HPE-AGSK is initially evaluated using IEEE CEC 2021 test suite, compared with fifteen state-of-the-art algorithms (AGSK, APGSK, APGSK-IMODE, GLAGSK, EDA2, AAVS-EDA, EBOwithCMAR, LSHADE-SPACMA, HSES, IMODE, MadDE, CJADE, and iLSHADE-RSP). The results demonstrate that HPE-AGSK outperforms both state-of-the-art GSK-based variants and past winners of IEEE CEC competitions. Subsequently, GSK-based variants and other exceptional algorithms in CEC 2021 are selected to further evaluate the performance of HPE-AGSK using IEEE CEC 2018 test suite. The statistical results show that HPE-AGSK has superior exploration ability than the comparison algorithms, and has strong competition with APGSK (state-of-the-art AGSK variant) and IMODE (CEC 2020 Winner) in exploitation ability. Finally, HPE-AGSK is utilized to solve the beyond visual range escape maneuver decision making problem. Its success rate is 100%, and mean maneuver time is 9.10 s, these results show that HPE-AGSK has good BVR escape maneuver decision-making performance. In conclusion, HPE-AGSK is a highly promising AGSK variant that significantly enhances the performance, and is an outstanding development of AGSK. The code of HPE-AGSK can be downloaded from https://github.com/xieleilei0305/HPE-AGSK-CODE.git . (The link will be available for readers after the paper is published).},
  archive      = {J_AIR},
  author       = {Xie, Lei and Wang, Yuan and Tang, Shangqin and Li, Yintong and Zhang, Zhuoran and Huang, Changqiang},
  doi          = {10.1007/s10462-024-11096-4},
  journal      = {Artificial Intelligence Review},
  month        = {6},
  number       = {6},
  pages        = {1-43},
  shortjournal = {Artif. Intell. Rev.},
  title        = {Adaptive gaining-sharing knowledge-based variant algorithm with historical probability expansion and its application in escape maneuver decision making},
  volume       = {58},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Environmental sound recognition on embedded devices using deep learning: A review. <em>AIR</em>, <em>58</em>(6), 1-35. (<a href='https://doi.org/10.1007/s10462-025-11106-z'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Sound recognition has a wide range of applications beyond speech and music, including environmental monitoring, sound source classification, mechanical fault diagnosis, audio fingerprinting, and event detection. These applications often require real-time data processing, making them well-suited for embedded systems. However, embedded devices face significant challenges due to limited computational power, memory, and low power consumption. Despite these constraints, achieving high performance in environmental sound recognition typically requires complex algorithms. Deep Learning models have demonstrated high accuracy on existing datasets, making them a popular choice for such tasks. However, these models are resource-intensive, posing challenges for real-time edge applications. This paper presents a comprehensive review of integrating Deep Learning models into embedded systems, examining their state-of-the-art applications, key components, and steps involved. It also explores strategies to optimise performance in resource-constrained environments through a comparison of various implementation approaches such as knowledge distillation, pruning, and quantization, with studies achieving a reduction in complexity of up to 97% compared to the unoptimized model. Overall, we conclude that in spite of the availability of lightweight deep learning models, input features, and compression techniques, their integration into low-resource devices, such as microcontrollers, remains limited. Furthermore, more complex tasks, such as general sound classification, especially with expanded frequency bands and real-time operation have yet to be effectively implemented on these devices. These findings highlight the need for a standardised research framework to evaluate these technologies applied to resource-constrained devices, and for further development to realise the wide range of potential applications.},
  archive      = {J_AIR},
  author       = {Gairí, Pau and Pallejà, Tomàs and Tresanchez, Marcel},
  doi          = {10.1007/s10462-025-11106-z},
  journal      = {Artificial Intelligence Review},
  month        = {6},
  number       = {6},
  pages        = {1-35},
  shortjournal = {Artif. Intell. Rev.},
  title        = {Environmental sound recognition on embedded devices using deep learning: A review},
  volume       = {58},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multi-model integration for dynamic forecasting (MIDF): A framework for wind speed and direction prediction. <em>AIR</em>, <em>58</em>(6), 1-37. (<a href='https://doi.org/10.1007/s10462-025-11140-x'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Accurate forecasting of wind speed and direction is critical for the efficient integration of wind power into energy systems, ensuring reliable renewable energy production and grid stability. Traditional methods often struggle with capturing nonlinear interdependencies, quantifying uncertainties, and providing reliable long-term predictions, particularly in complex atmospheric conditions. To address these challenges, this study introduces multi-model Integration for dynamic forecasting (MIDF), an ensemble machine learning framework that combines the strengths of DeepAR and temporal fusion transformer (TFT) models through a two-step meta-learning process. MIDF leverages DeepAR’s probabilistic forecasting capabilities and TFT’s attention mechanisms to enhance accuracy, robustness, and interpretability. Using a custom meteorological dataset spanning January 2010 to May 2023, the model was evaluated against standalone alternatives across multiple metrics, including MSE, RMSE, and R2. MIDF achieved superior performance, with MSE, RMSE, and R2 values of 0.0035, 0.01913, and 0.89 for wind speed, and 0.00052, 0.02507, and 0.86 for wind direction, significantly reducing errors compared to existing methods. These results underscore the potential of ensemble learning in advancing wind forecasting accuracy, enabling more reliable renewable energy management, operational planning, and risk mitigation in meteorological applications.},
  archive      = {J_AIR},
  author       = {Maruthi, Molaka and Kim, Bubryur and Sujeen, Song and An, Jinwoo and Chen, Zengshun},
  doi          = {10.1007/s10462-025-11140-x},
  journal      = {Artificial Intelligence Review},
  month        = {6},
  number       = {6},
  pages        = {1-37},
  shortjournal = {Artif. Intell. Rev.},
  title        = {Multi-model integration for dynamic forecasting (MIDF): A framework for wind speed and direction prediction},
  volume       = {58},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A survey of small object detection based on deep learning in aerial images. <em>AIR</em>, <em>58</em>(6), 1-67. (<a href='https://doi.org/10.1007/s10462-025-11150-9'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Small object detection poses a formidable challenge in the field of computer vision, particularly when it comes to analyzing aerial remote sensing images. Despite the rapid development of deep learning and significant progress in detection techniques in natural scenes, the migration of these algorithms to aerial images has not met expectations. This is primarily due to limitations in imaging acquisition conditions, including small target size, viewpoint specificity, background complexity, as well as scale and orientation diversity. Although the increasing application of deep learning-based algorithms to overcome these problems, few studies have summarized the optimization of different deep learning strategies used for small target detection in aerial images. Therefore, this paper aims to explore the application of deep learning methods for small object detection in aerial images. The primary challenges in small object detection in aerial images will be summarized. Next, a meticulous analysis and categorization of the prevailing deep learning optimization strategies employed to surmount the challenges encountered in aerial image detection is undertaken. Following that, we provide a comprehensive presentation of the object detection datasets utilized in aerial remote sensing images, along with the evaluation metrics employed. Additionally, we furnish experimental data pertaining to the currently proposed detection algorithms. Finally, the advantages and disadvantages of various optimization strategies and potential development trends are discussed. Hopefully, it can provide a reference for researchers in this field.},
  archive      = {J_AIR},
  author       = {Hua, Wei and Chen, Qili},
  doi          = {10.1007/s10462-025-11150-9},
  journal      = {Artificial Intelligence Review},
  month        = {6},
  number       = {6},
  pages        = {1-67},
  shortjournal = {Artif. Intell. Rev.},
  title        = {A survey of small object detection based on deep learning in aerial images},
  volume       = {58},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). BERT applications in natural language processing: A review. <em>AIR</em>, <em>58</em>(6), 1-49. (<a href='https://doi.org/10.1007/s10462-025-11162-5'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {BERT (Bidirectional Encoder Representations from Transformers) has revolutionized Natural Language Processing (NLP) by significantly enhancing the capabilities of language models. This review study examines the complex nature of BERT, including its structure, utilization in different NLP tasks, and the further development of its design via modifications. The study thoroughly analyses the methodological aspects, conducting a comprehensive analysis of the planning process, the implemented procedures, and the criteria used to decide which data to include or exclude in the evaluation framework. In addition, the study thoroughly examines the influence of BERT on several NLP tasks, such as Sentence Boundary Detection, Tokenization, Grammatical Error Detection and Correction, Dependency Parsing, Named Entity Recognition, Part of Speech Tagging, Question Answering Systems, Machine Translation, Sentiment analysis, fake review detection and Cross-lingual transfer learning. The review study adds to the current literature by integrating ideas from multiple sources, explicitly emphasizing the problems and prospects in BERT-based models. The objective is to comprehensively comprehend BERT and its implementations, targeting both experienced researchers and novices in the domain of NLP. Consequently, the present study is expected to inspire more research endeavors, promote innovative adaptations of BERT, and deepen comprehension of its extensive capabilities in various NLP applications. The results presented in this research are anticipated to influence the advancement of future language models and add to the ongoing discourse on enhancing technology for understanding natural language.},
  archive      = {J_AIR},
  author       = {Gardazi, Nadia Mushtaq and Daud, Ali and Malik, Muhammad Kamran and Bukhari, Amal and Alsahfi, Tariq and Alshemaimri, Bader},
  doi          = {10.1007/s10462-025-11162-5},
  journal      = {Artificial Intelligence Review},
  month        = {6},
  number       = {6},
  pages        = {1-49},
  shortjournal = {Artif. Intell. Rev.},
  title        = {BERT applications in natural language processing: A review},
  volume       = {58},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). GFSNet: Gaussian fourier with sparse attention network for visual question answering. <em>AIR</em>, <em>58</em>(6), 1-30. (<a href='https://doi.org/10.1007/s10462-025-11163-4'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Visual question answering (VQA), a core task in multimodal learning, requires models to effectively integrate visual and natural language information to perform reasoning and semantic understanding in complex scenarios. However, self-attention mechanisms often struggle to capture multi-scale information and key region features within images comprehensively. Moreover, VQA involves multidimensional and deep reasoning about image content, particularly in scenarios involving spatial relationships and frequency-domain features. Existing methods face limitations in modeling multi-scale features and filtering irrelevant information effectively. This paper proposes an innovative Gaussian Fourier with Sparse Attention Network (GFSNet) to address these challenges. GFSNet leverages Fourier transforms to map image attention weights generated by the self-attention mechanism from the spatial domain to the frequency domain, enabling comprehensive modeling of multi-scale frequency information. This enhances the model’s adaptability to complex structures and its capacity for relational modeling. To further improve feature robustness, a Gaussian filter is introduced to suppress high-frequency noise in the frequency domain, preserving critical visual information. Additionally, a sparse attention mechanism dynamically selects optimized frequency-domain features, effectively reducing interference from redundant information while improving interpretability and computational efficiency. Without increasing parameter counts or computational complexity, GFSNet achieves efficient modeling of multi-scale visual information. Experimental results on benchmark VQA datasets (VQA v2, GQA, and CLEVR) demonstrate that GFSNet significantly enhances reasoning capabilities and cross-modal alignment performance, validating its superiority and effectiveness. The code is available at https://github.com/shenxiang-vqa/GFSNet .},
  archive      = {J_AIR},
  author       = {Shen, Xiang and Han, Dezhi and Chang, Chin-Chen and Oad, Ammar and Wu, Huafeng},
  doi          = {10.1007/s10462-025-11163-4},
  journal      = {Artificial Intelligence Review},
  month        = {6},
  number       = {6},
  pages        = {1-30},
  shortjournal = {Artif. Intell. Rev.},
  title        = {GFSNet: Gaussian fourier with sparse attention network for visual question answering},
  volume       = {58},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Intelligent games meeting with multi-agent deep reinforcement learning: A comprehensive review. <em>AIR</em>, <em>58</em>(6), 1-53. (<a href='https://doi.org/10.1007/s10462-025-11166-1'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent years have witnessed the great achievement of the AI-driven intelligent games, such as AlphaStar defeating the human experts, and numerous intelligent games have come into the public view. Essentially, deep reinforcement learning (DRL), especially multiple-agent DRL (MADRL) has empowered a variety of artificial intelligence fields, including intelligent games. However, there is lack of systematical review on their correlations. This article provides a holistic picture on smoothly connecting intelligent games with MADRL from two perspectives: theoretical game concepts for MADRL, and MADRL for intelligent games. From the first perspective, information structure and game environmental features for MADRL algorithms are summarized; and from the second viewpoint, the challenges in intelligent games are investigated, and the existing MADRL solutions are correspondingly explored. Furthermore, the state-of-the-art (SOTA) MADRL algorithms for intelligent games are systematically categorized, especially from the perspective of credit assignment. Moreover, a comprehensively review on notorious benchmarks are conducted to facilitate the design and test of MADRL based intelligent games. Besides, a general procedure of MADRL simulations is offered. Finally, the key challenges in integrating intelligent games with MADRL, and potential future research directions are highlighted. This survey hopes to provide a thoughtful insight of developing intelligent games with the assistance of MADRL solutions and algorithms.},
  archive      = {J_AIR},
  author       = {Wang, Yiqin and Wang, Yufeng and Tian, Feng and Ma, Jianhua and Jin, Qun},
  doi          = {10.1007/s10462-025-11166-1},
  journal      = {Artificial Intelligence Review},
  month        = {6},
  number       = {6},
  pages        = {1-53},
  shortjournal = {Artif. Intell. Rev.},
  title        = {Intelligent games meeting with multi-agent deep reinforcement learning: A comprehensive review},
  volume       = {58},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Deep learning frameworks for MRI-based diagnosis of neurological disorders: A systematic review and meta-analysis. <em>AIR</em>, <em>58</em>(6), 1-37. (<a href='https://doi.org/10.1007/s10462-025-11146-5'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The automatic diagnosis of neurological disorders using Magnetic Resonance Imaging (MRI) is a widely researched problem. MRI is a non-invasive and highly informative imaging modality, which is one of the most widely accepted and used neuroimaging modalities for visualizing the human brain. The advent of tremendous processing capabilities, multi-modal data, and deep-learning techniques has enabled researchers to develop intelligent, sufficiently accurate classification methods. A comprehensive literature review has revealed extensive research on the automatic diagnosis of neurological disorders. However, despite numerous studies, a systematically developed framework is lacking, that relies on a sufficiently robust dataset or ensures reliable accuracy. To date, no consolidated framework has been established to classify multiple diseases and their subtypes effectively based on various types and their planes of orientation in structural and functional MR images. This systematic review provides a detailed and comprehensive analysis of research reported from 2000 to 2023. Systems developed in prior art have been categorized according to their disease diagnosis capabilities. The datasets employed and the tools developed are also summarized to assist researchers to conduct further studies in this crucial domain. The contributions of this research include facilitating the design of a unified framework for multiple neurological disease diagnoses, resulting in the development of a generic assistive tool for hospitals and neurologists to diagnose disorders precisely and swiftly thus potentially saving lives, in addition to increasing the quality of life of patients suffering from neurodegenerative disorders.},
  archive      = {J_AIR},
  author       = {Ali, Syed Saad Azhar and Memon, Khuhed and Yahya, Norashikin and Khan, Shujaat},
  doi          = {10.1007/s10462-025-11146-5},
  journal      = {Artificial Intelligence Review},
  month        = {6},
  number       = {6},
  pages        = {1-37},
  shortjournal = {Artif. Intell. Rev.},
  title        = {Deep learning frameworks for MRI-based diagnosis of neurological disorders: A systematic review and meta-analysis},
  volume       = {58},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Machine learning operations landscape: Platforms and tools. <em>AIR</em>, <em>58</em>(6), 1-37. (<a href='https://doi.org/10.1007/s10462-025-11164-3'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As the field of machine learning advances, managing and monitoring intelligent models in production, also known as machine learning operations (MLOps), has become essential. Organizations are increasingly adopting artificial intelligence as a strategic tool, thus increasing the need for reliable, and scalable MLOps platforms. Consequently, every aspect of the machine learning life cycle, from workflow orchestration to performance monitoring, presents both challenges and opportunities that require sophisticated, flexible, and scalable technological solutions. This research addresses this demand by providing a comprehensive assessment framework of MLOps platforms highlighting the key features necessary for a robust MLOps solution. The paper examines 16 MLOps tools widely used, which revolve around capabilities within AI infrastructure management, including but not limited to experiment tracking, model deployment, and model inference. Our three-step evaluation framework starts with a feature analysis of the MLOps platforms, then GitHub stars growth assessment for adoption and prominence, and finally, a weighted scoring method to single out the most influential platforms. From this process, we derive valuable insights into the essential components of effective MLOps systems and provide a decision-making flowchart that simplifies platform selection. This framework provides hands-on guidance for organizations looking to initiate or enhance their MLOps strategies, whether they require an end-end solutions or specialized tools.},
  archive      = {J_AIR},
  author       = {Berberi, Lisana and Kozlov, Valentin and Nguyen, Giang and Sáinz-Pardo Díaz, Judith and Calatrava, Amanda and Moltó, Germán and Tran, Viet and López García, Álvaro},
  doi          = {10.1007/s10462-025-11164-3},
  journal      = {Artificial Intelligence Review},
  month        = {6},
  number       = {6},
  pages        = {1-37},
  shortjournal = {Artif. Intell. Rev.},
  title        = {Machine learning operations landscape: Platforms and tools},
  volume       = {58},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A review of hyperspectral image classification based on graph neural networks. <em>AIR</em>, <em>58</em>(6), 1-56. (<a href='https://doi.org/10.1007/s10462-025-11169-y'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Hyperspectral images provide rich spectral-spatial information but pose significant classification challenges due to high dimensionality, noise, mixed pixels, and limited labeled samples. Graph Neural Networks (GNNs) have emerged as a promising solution, offering a semi-supervised framework that can capture complex spatial-spectral relationships inherent in non-Euclidean hyperspectral image data. However, existing reviews often concentrate on specific aspects, thus limiting a comprehensive understanding of GNN-based hyperspectral image classification. This review systematically outlines the fundamental concepts of hyperspectral image classification and GNNs, and summarizes leading approaches from both traditional machine learning and deep learning. Then, it categorizes GNN-based methods into four paradigms: graph recurrent neural networks, graph convolutional networks, graph autoencoders, and hybrid graph neural networks, discussing their theoretical underpinnings, architectures, and representative applications. Finally, five key directions are further highlighted: adaptive graph construction, dynamic graph processing, deeper architectures, self-supervised strategies, and robustness enhancement. These insights aim to facilitate continued innovation in GNN-based hyperspectral imaging, guiding researchers toward more efficient and accurate classification frameworks.},
  archive      = {J_AIR},
  author       = {Zhao, Xiaofeng and Ma, Junyi and Wang, Lei and Zhang, Zhili and Ding, Yao and Xiao, Xiongwu},
  doi          = {10.1007/s10462-025-11169-y},
  journal      = {Artificial Intelligence Review},
  month        = {6},
  number       = {6},
  pages        = {1-56},
  shortjournal = {Artif. Intell. Rev.},
  title        = {A review of hyperspectral image classification based on graph neural networks},
  volume       = {58},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Polyp segmentation in medical imaging: Challenges, approaches and future directions. <em>AIR</em>, <em>58</em>(6), 1-61. (<a href='https://doi.org/10.1007/s10462-025-11173-2'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Colorectal cancer has been considered as the third most dangerous disease among the most common cancer types. The early diagnosis of the polyps weakens the spread of colorectal cancer and is significant for more productive treatment. The segmentation of polyps from the colonoscopy images is very critical and significant to identify colorectal cancer. In this comprehensive study, we meticulously scrutinize research papers focused on the automated segmentation of polyps in clinical settings using colonoscopy images proposed in the past five years. Our analysis delves into various dimensions, including input data (datasets and preprocessing methods), model design (encompassing CNNs, transformers, and hybrid approaches), loss functions, and evaluation metrics. By adopting a systematic perspective, we examine how different methodological choices have shaped current trends and identify critical limitations that need to be addressed. To facilitate meaningful comparisons, we provide a detailed summary table of all examined works. Moreover, we offer in-depth future recommendations for polyp segmentation based on the insights gained from this survey study. We believe that our study will serve as a great resource for future researchers in the subject of polyp segmentation offering vital support in the development of novel methodologies.},
  archive      = {J_AIR},
  author       = {Qayoom, Abdul and Xie, Juanying and Ali, Haider},
  doi          = {10.1007/s10462-025-11173-2},
  journal      = {Artificial Intelligence Review},
  month        = {6},
  number       = {6},
  pages        = {1-61},
  shortjournal = {Artif. Intell. Rev.},
  title        = {Polyp segmentation in medical imaging: Challenges, approaches and future directions},
  volume       = {58},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A context and sequence-based recommendation framework using GRU networks. <em>AIR</em>, <em>58</em>(6), 1-36. (<a href='https://doi.org/10.1007/s10462-025-11174-1'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recommendation systems play a significant contribution in e-commerce for predicting the more relevant product to the customers based on their interests. The recommendation system refers to the user-item interaction and predicts the next item by considering the similar kind of user interest or item purchased. The context-aware and sequential recommendation is built to predict the interested product based on the current context and sequential behavior pattern interactions. To fulfill the customers’ requirements, this paper proposes a new hybrid personalized recommendation system framework called Target User Context Sequential Prediction Gated Recurrent Unit (TUCSP-GRU) using deep learning methods to recommend suitable products to the users based on their interests and context. The proposed system uses the newly calculated Target User Specific Product Rating (TUS-PR) score, the proposed TUS Gated Recurrent Unit (TUS-GRU) model, and the proposed Top-N item prediction method. Here, (i) the TUS-PR score is used to improve the product rating, (ii) the new TUS-GRU model is used to find the sequence purchase behavior pattern of customers by considering their long-term and short-term interests, and (iii) the proposed Top-N item dynamic prediction method is used to adjust the next interested item list based on the response using the back propagation continuous learning method. The experiment results of the TUCSP-GRU framework show better accuracy in predicting the interested and relevant products or items when compared to existing similar recommendation systems with respect to the standard evaluation metrics.},
  archive      = {J_AIR},
  author       = {Karthik, R. V. and Pandiyaraju, V. and Ganapathy, Sannasi},
  doi          = {10.1007/s10462-025-11174-1},
  journal      = {Artificial Intelligence Review},
  month        = {6},
  number       = {6},
  pages        = {1-36},
  shortjournal = {Artif. Intell. Rev.},
  title        = {A context and sequence-based recommendation framework using GRU networks},
  volume       = {58},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Application of machine learning in early warning system of geotechnical disaster: A systematic and comprehensive review. <em>AIR</em>, <em>58</em>(6), 1-45. (<a href='https://doi.org/10.1007/s10462-025-11175-0'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Enhancements in monitoring and computational technology have facilitated data accessibility and utilization. Machine learning, as an integral component of the realm of computational technology, is renowned for its universality and efficacy, rendering it pervasive across various domains. Geotechnical disaster early warning systems serve as a crucial safeguard for the preservation of human lives and assets. Machine learning exhibits the capacity to meet the exigencies of prompt and precise disaster prediction, prompting substantial interest in the nexus of these two domains in recent decades. This study accentuates the deployment of machine learning in addressing geotechnical engineering disaster prediction issues through an examination of four types of engineering-specialized research articles spanning the period 2009 to 2024. The study elucidates the evolution and significance of machine learning within the domain of geotechnical engineering disaster prediction, with an emphasis on data analytics and modeling. Addressing the lacunae in existing literature, a user-friendly front-end graphical interface, integrated with machine learning algorithms, is devised to better cater to the requisites of engineering professionals. Furthermore, this research delves into a critical analysis of the prevalent research limitations and puts forth prospective investigational avenues from an applied standpoint.},
  archive      = {J_AIR},
  author       = {Lin, Shan and Liang, Zenglong and Guo, Hongwei and Hu, Quanke and Cao, Xitailang and Zheng, Hong},
  doi          = {10.1007/s10462-025-11175-0},
  journal      = {Artificial Intelligence Review},
  month        = {6},
  number       = {6},
  pages        = {1-45},
  shortjournal = {Artif. Intell. Rev.},
  title        = {Application of machine learning in early warning system of geotechnical disaster: A systematic and comprehensive review},
  volume       = {58},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Classification using hyperdimensional computing: A review with comparative analysis. <em>AIR</em>, <em>58</em>(6), 1-41. (<a href='https://doi.org/10.1007/s10462-025-11181-2'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Hyperdimensional computing (HD), also known as vector symbolic architectures (VSA), is an emerging and promising paradigm for cognitive computing. At its core, HD/VSA is characterized by its distinctive approach to compositionally representing information using high-dimensional randomized vectors. The recent surge in research within this field gains momentum from its computational efficiency stemming from low-resolution representations and ability to excel in few-shot learning scenarios. Nonetheless, the current literature is missing a comprehensive comparative analysis of various methods since each of them uses a different benchmark to evaluate its performance. This gap obstructs the monitoring of the field’s state-of-the-art advancements and acts as a significant barrier to its overall progress. To address this gap, this review not only offers a conceptual overview of the latest literature but also introduces a comprehensive comparative study of HD/VSA classification methods. The exploration starts with an overview of the strategies proposed to encode information as high-dimensional vectors. These vectors serve as integral components in the construction of classification models. Furthermore, we evaluate diverse classification methods as proposed in the existing literature. This evaluation encompasses techniques such as retraining and regenerative training to augment the model’s performance. To conclude our study, we present a comprehensive empirical study. This study serves as an in-depth analysis, systematically comparing various HD/VSA classification methods using two benchmarks, the first being a set of seven popular datasets used in HD/VSA and the second consisting of 121 datasets being the subset from the UCI Machine Learning repository. To facilitate future research on classification with HD/VSA, we open-sourced the benchmarking and the implementations of the methods we review. Since the considered data are tabular, encodings based on key-value pairs emerge as optimal choices, boasting superior accuracy while maintaining high efficiency. Secondly, iterative adaptive methods demonstrate remarkable efficacy, potentially complemented by a regenerative strategy, depending on the specific problem. Furthermore, we show how HD/VSA is able to generalize while training with a limited number of training instances. Lastly, we demonstrate the robustness of HD/VSA methods by subjecting the model memory to a large number of bit-flips. The results illustrate that the model’s performance remains reasonably stable until the occurrence of 40% of bit flips, where the model’s performance is drastically degraded. Overall, this study performed a thorough performance evaluation on different methods and, on the one hand, a positive trend was observed in terms of improving classification performance but, on the other hand, these developments could often be surpassed by off-the-shelf methods. This calls for better integration with the broader machine learning literature; the developed benchmarking framework provides practical means for doing so.},
  archive      = {J_AIR},
  author       = {Vergés, Pere and Heddes, Mike and Nunes, Igor and Kleyko, Denis and Givargis, Tony and Nicolau, Alexandru},
  doi          = {10.1007/s10462-025-11181-2},
  journal      = {Artificial Intelligence Review},
  month        = {6},
  number       = {6},
  pages        = {1-41},
  shortjournal = {Artif. Intell. Rev.},
  title        = {Classification using hyperdimensional computing: A review with comparative analysis},
  volume       = {58},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Context in object detection: A systematic literature review. <em>AIR</em>, <em>58</em>(6), 1-89. (<a href='https://doi.org/10.1007/s10462-025-11186-x'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Context is an important factor in computer vision as it offers valuable information to clarify and analyze visual data. Utilizing the contextual information inherent in an image or a video can improve the precision and effectiveness of object detectors. For example, where recognizing an isolated object might be challenging, context information can improve comprehension of the scene. This study explores the impact of various context-based approaches to object detection. Initially, we investigate the role of context in object detection and survey it from several perspectives. We then review and discuss the most recent context-based object detection approaches and compare them. Finally, we conclude by addressing research questions and identifying gaps for further studies. More than 265 publications are included in this survey, covering different aspects of context in different categories of object detection, including general object detection, video object detection, small object detection, camouflaged object detection, zero-shot, one-shot, and few-shot object detection. This literature review presents a comprehensive overview of the latest advancements in context-based object detection, providing valuable contributions such as a thorough understanding of contextual information and effective methods for integrating various context types into object detection, thus benefiting researchers.},
  archive      = {J_AIR},
  author       = {Jamali, Mahtab and Davidsson, Paul and Khoshkangini, Reza and Ljungqvist, Martin Georg and Mihailescu, Radu-Casian},
  doi          = {10.1007/s10462-025-11186-x},
  journal      = {Artificial Intelligence Review},
  month        = {6},
  number       = {6},
  pages        = {1-89},
  shortjournal = {Artif. Intell. Rev.},
  title        = {Context in object detection: A systematic literature review},
  volume       = {58},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). LiDAR, IMU, and camera fusion for simultaneous localization and mapping: A systematic review. <em>AIR</em>, <em>58</em>(6), 1-59. (<a href='https://doi.org/10.1007/s10462-025-11187-w'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Simultaneous Localization and Mapping (SLAM) is a crucial technology for intelligent unnamed systems to estimate their motion and reconstruct unknown environments. However, the SLAM systems with merely one sensor have poor robustness and stability due to the defects in the sensor itself. Recent studies have demonstrated that SLAM systems with multiple sensors, mainly consisting of LiDAR, camera, and IMU, achieve better performance due to the mutual compensation of different sensors. This paper investigates recent progress on multi-sensor fusion SLAM. The review includes a systematic analysis of the advantages and disadvantages of different sensors and the imperative of multi-sensor solutions. It categorizes multi-sensor fusion SLAM systems into four main types by the fused sensors: LiDAR-IMU SLAM, Visual-IMU SLAM, LiDAR-Visual SLAM, and LiDAR-IMU-Visual SLAM, with detailed analysis and discussions of their pipelines and principles. Meanwhile, the paper surveys commonly used datasets and introduces evaluation metrics. Finally, it concludes with a summary of the existing challenges and future opportunities for multi-sensor fusion SLAM.},
  archive      = {J_AIR},
  author       = {Fan, Zheng and Zhang, Lele and Wang, Xueyi and Shen, Yilan and Deng, Fang},
  doi          = {10.1007/s10462-025-11187-w},
  journal      = {Artificial Intelligence Review},
  month        = {6},
  number       = {6},
  pages        = {1-59},
  shortjournal = {Artif. Intell. Rev.},
  title        = {LiDAR, IMU, and camera fusion for simultaneous localization and mapping: A systematic review},
  volume       = {58},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A comprehensive review on financial explainable AI. <em>AIR</em>, <em>58</em>(6), 1-49. (<a href='https://doi.org/10.1007/s10462-024-11077-7'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The success of artificial intelligence (AI), and deep learning models in particular, has led to their widespread adoption across various industries due to their ability to process huge amounts of data and learn complex patterns. However, due to their lack of explainability, there are significant concerns regarding their use in critical sectors, such as finance and healthcare, where decision-making transparency is of paramount importance. In this paper, we provide a comparative survey of methods that aim to improve the explainability of deep learning models within the context of finance. We categorize the collection of explainable AI methods according to their corresponding characteristics, and we review the concerns and challenges of adopting explainable AI methods, together with future directions we deemed appropriate and important.},
  archive      = {J_AIR},
  author       = {Yeo, Wei Jie and Van Der Heever, Wihan and Mao, Rui and Cambria, Erik and Satapathy, Ranjan and Mengaldo, Gianmarco},
  doi          = {10.1007/s10462-024-11077-7},
  journal      = {Artificial Intelligence Review},
  month        = {6},
  number       = {6},
  pages        = {1-49},
  shortjournal = {Artif. Intell. Rev.},
  title        = {A comprehensive review on financial explainable AI},
  volume       = {58},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Dna coding theory and algorithms. <em>AIR</em>, <em>58</em>(6), 1-37. (<a href='https://doi.org/10.1007/s10462-025-11132-x'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {DNA computing is an emerging computational model that has garnered significant attention due to its distinctive advantages at the molecular biological level. Since it was introduced by Adelman in 1994, this field has made remarkable progress in solving NP-complete problems, enhancing information security, encrypting images, controlling diseases, and advancing nanotechnology. A key challenge in DNA computing is the design of DNA coding, which aims to minimize nonspecific hybridization and enhance computational reliability. The DNA coding design is a classical combinatorial optimization problem focused on generating high-quality DNA sequences that meet specific constraints, including distance, thermodynamics, secondary structure, and sequence requirements. This paper comprehensively examines the advances in DNA coding design, highlighting mathematical models, counting theory, and commonly used DNA coding methods. These methods include the template method, multi-objective evolutionary methods, and implicit enumeration techniques.},
  archive      = {J_AIR},
  author       = {Xu, Jin and Liu, Wenbin and Zhang, Kai and Zhu, Enqiang},
  doi          = {10.1007/s10462-025-11132-x},
  journal      = {Artificial Intelligence Review},
  month        = {6},
  number       = {6},
  pages        = {1-37},
  shortjournal = {Artif. Intell. Rev.},
  title        = {Dna coding theory and algorithms},
  volume       = {58},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Current and future roles of artificial intelligence in retinopathy of prematurity. <em>AIR</em>, <em>58</em>(6), 1-55. (<a href='https://doi.org/10.1007/s10462-025-11153-6'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Retinopathy of prematurity (ROP) is a severe condition affecting premature infants, leading to abnormal retinal blood vessel growth, retinal detachment, and potential blindness. While semi-automated systems have been used in the past to diagnose ROP-related plus disease by quantifying retinal vessel features, traditional machine learning (ML) models face challenges like accuracy and overfitting. Recent advancements in deep learning (DL), especially convolutional neural networks (CNNs), have significantly improved ROP detection and classification. The i-ROP deep learning (i-ROP-DL) system also shows promise in detecting plus disease, offering reliable ROP diagnosis potential. This research comprehensively examines the contemporary progress and challenges associated with using retinal imaging and artificial intelligence (AI) to detect ROP, offering valuable insights that can guide further investigation in this domain. Based on 84 original studies in this field (out of 2025 studies that were comprehensively reviewed), we concluded that traditional methods for ROP diagnosis suffer from subjectivity and manual analysis, leading to inconsistent clinical decisions. AI holds great promise for improving ROP management. This review explores AI’s potential in ROP detection, classification, diagnosis, and prognosis.},
  archive      = {J_AIR},
  author       = {Jafarizadeh, Ali and Maleki, Shadi Farabi and Pouya, Parnia and Sobhi, Navid and Abdollahi, Mirsaeed and Pedrammehr, Siamak and Lim, Chee Peng and Asadi, Houshyar and Alizadehsani, Roohallah and Tan, Ru-San and Islam, Sheikh Mohammed Shariful and Acharya, U. Rajendra},
  doi          = {10.1007/s10462-025-11153-6},
  journal      = {Artificial Intelligence Review},
  month        = {6},
  number       = {6},
  pages        = {1-55},
  shortjournal = {Artif. Intell. Rev.},
  title        = {Current and future roles of artificial intelligence in retinopathy of prematurity},
  volume       = {58},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multi-scale graph diffusion convolutional network for multi-view learning. <em>AIR</em>, <em>58</em>(6), 1-23. (<a href='https://doi.org/10.1007/s10462-025-11158-1'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-view learning has attracted considerable attention owing to its capability to learn more comprehensive representations. Although graph convolutional networks have achieved encouraging results in multi-view research, their limitation to considering only nearest neighbors results in the decrease on the ability to obtain high-order information. Many existing methods acquire high-order correlation by stacking multiple layers onto the model, yet they could lead to the issue of over-smoothing. In this paper, we propose a framework termed multi-scale graph diffusion convolutional network, which aims to gather comprehensive higher-order information without stacking multiple convolutional layers. Specifically, in order to better expand the receptive field of the node and reduce the parameter complexity, the proposed framework utilizes a contractive mapping to transform features from multiple views on decoupled propagation rules. Our framework introduces a multi-scale graph-based diffusion mechanism to adaptively extract the abundant high-order knowledge embedded within multi-scale graphs. Experiments show that the proposed method outperforms other state-of-the-art methods in terms of multi-view semi-supervised classification.},
  archive      = {J_AIR},
  author       = {Wang, Shiping and Li, Jiacheng and Chen, Yuhong and Wu, Zhihao and Huang, Aiping and Zhang, Le},
  doi          = {10.1007/s10462-025-11158-1},
  journal      = {Artificial Intelligence Review},
  month        = {6},
  number       = {6},
  pages        = {1-23},
  shortjournal = {Artif. Intell. Rev.},
  title        = {Multi-scale graph diffusion convolutional network for multi-view learning},
  volume       = {58},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Emrnet: Enhanced micro-expression recognition network with attention and distance correlation. <em>AIR</em>, <em>58</em>(6), 1-27. (<a href='https://doi.org/10.1007/s10462-025-11159-0'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Micro-expression recognition (MER) is inherently challenging due to the difficulty of extracting subtle, localized changes in micro-expressions (MEs). Various optical flow-based methods have been proposed for MER, as optical flow can effectively suppress facial identity information while capturing the movement patterns of MEs. However, these methods, characterized by simple architectures, often fail to extract discriminative features, resulting in suboptimal performance. In this paper, we propose an Enhanced Micro-expression Recognition Network with attention and distance correlation (EMRNet) for MER. EMRNet consists of three key phases: First, we introduce a novel channel-wise region-aware attention mechanism within two identical Inception networks, designed to extract global and local expression features in parallel, based on the optical flow input of the same ME. Second, to enhance ME representations, we propose a regularized dilated loss function incorporating distance correlation, which improves the information entropy transferred between the two branches. Last, emotion categories are predicted by fusing the expression-dilated features in the classification branch. Extensive experiments conducted on the composite database from the MEGC 2019 challenge demonstrate the effectiveness of EMRNet under both leave-one-subject-out (LOSO) cross-validation and the composite database evaluation (CDE) protocol. The results show that our approach successfully generates discriminative features, achieving substantial performance gains. Furthermore, EMRNet outperforms existing single-stream and dual-stream models, delivering superior results in MER.},
  archive      = {J_AIR},
  author       = {Liu, Gaqiong and Huang, Shucheng and Wang, Gang and Li, Mingxing},
  doi          = {10.1007/s10462-025-11159-0},
  journal      = {Artificial Intelligence Review},
  month        = {6},
  number       = {6},
  pages        = {1-27},
  shortjournal = {Artif. Intell. Rev.},
  title        = {Emrnet: Enhanced micro-expression recognition network with attention and distance correlation},
  volume       = {58},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Bibliometric analysis of artificial intelligence cyberattack detection models. <em>AIR</em>, <em>58</em>(6), 1-40. (<a href='https://doi.org/10.1007/s10462-025-11167-0'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cybercriminals have increasingly adopted advanced and cutting-edge methods that expand the scale and speed of their attacks in recent years. This trend coincides with the rising demand for and scarcity of highly skilled cybersecurity specialists, making them both expensive and difficult to find. Recently, researchers have demonstrated the effectiveness of Artificial Intelligence (AI) approaches in combating sophisticated cyberattacks. However, comprehensive bibliometric data illustrating the study of AI approaches in cyberattack detection remain sparse. This study addresses this gap by investigating the current state of AI-based cyberattack detection research. The study analyzed the Scopus database using bibliometric analysis on a pool of over 2,338 articles published between 2014 and 2024, including 1217 journal articles, 828 conference papers, 121 conference reviews, 85 book chapters, 70 reviews, 5 editorials, and 2 books and short surveys. The study explores various AI-based cyberattack detection approaches globally, focusing on machine learning and deep learning algorithms. The bibliometric analysis was conducted using R, an open-source statistical tool, and Biblioshiny. The findings establish that AI, particularly machine learning and deep learning, enhances intrusion detection accuracy and is a growing research trend. Researchers have effectively employed these techniques for malware detection. The USA leads in AI cyberattack research, followed by India, China, Saudi Arabia, and Australia. Despite publishing fewer articles, Canada and Italy received significant citations. Additionally, strong research collaboration exists among the USA, China, Australia, Saudi Arabia, and India. Keyword analysis highlights AI’s effectiveness in identifying patterns and malicious behaviours, enhancing intrusion detection even in complex cyberattacks. Machine learning can detect intrusions based on anomalies caused by malicious or compromised devices, as well as unknown threats, with speed, accuracy, and a low false-positive rate.},
  archive      = {J_AIR},
  author       = {Guembe, Blessing and Misra, Sanjay and Azeta, Ambrose and Lopez-Baldominos, Ines},
  doi          = {10.1007/s10462-025-11167-0},
  journal      = {Artificial Intelligence Review},
  month        = {6},
  number       = {6},
  pages        = {1-40},
  shortjournal = {Artif. Intell. Rev.},
  title        = {Bibliometric analysis of artificial intelligence cyberattack detection models},
  volume       = {58},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). MambaYOLACT: You only look at mamba prediction head for head-neck lymph nodes. <em>AIR</em>, <em>58</em>(6), 1-24. (<a href='https://doi.org/10.1007/s10462-025-11177-y'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Lymph nodes in the head-neck are often infected when malignant tumors metastasize. At present, Magnetic Resonance Imaging (MRI) is widely used in the evaluation of head-neck lymph nodes. However, there are some problems, such as different sizes, low contrast of head-neck lymph nodes. The instance segmentation accuracy of head-neck lymph nodes is decreased, which affects the patients treatment decision and the surgical effect evaluation. To solve these problems, a single stage Mamba YOLACT instance segmentation model is proposed in this paper. The main contributions are as follows: Firstly, a Cross-field and Cross-direction Feature Enhancement module (CCFE) is designed. The module through the channel grouping mechanism, effectively enhances the ability of each group of features to express different spatial semantic information, by mixing attention mechanism to improve the feature extraction ability of lesions with different dimensions. Secondly, a MambaNet-based prediction head module is designed. The module combined the State-Space Model (SSM) and self-attention mechanism to accurately capture global image dependencies, highlight the lesion area. Thirdly, A dataset of MRI images of head-neck lymph nodes is used to verify the model effectiveness. The results show that the values of APdet, APseg, ARdet, ARseg, mAPdet and mAPseg are 69.8%, 70.9%, 55.3%, 56.4%, 39.4% and 41.0%, respectively. The model can achieve accurate segmentation of the lymph nodes, which has positive significance for lymph nodes auxiliary diagnosis.},
  archive      = {J_AIR},
  author       = {Zhou, Tao and Chai, Wenwen and Chang, Defang and Chen, Kaixiong and Zhang, Zhe and Lu, HuiLing},
  doi          = {10.1007/s10462-025-11177-y},
  journal      = {Artificial Intelligence Review},
  month        = {6},
  number       = {6},
  pages        = {1-24},
  shortjournal = {Artif. Intell. Rev.},
  title        = {MambaYOLACT: You only look at mamba prediction head for head-neck lymph nodes},
  volume       = {58},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Guidelines for designing visualization tools for group fairness analysis in binary classification. <em>AIR</em>, <em>58</em>(6), 1-38. (<a href='https://doi.org/10.1007/s10462-025-11179-w'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The use of machine learning in decision-making has become increasingly pervasive across various fields, from healthcare to finance, enabling systems to learn from data and improve their performance over time. The transformative impact of these new technologies warrants several considerations that demand the development of modern solutions through responsible artificial intelligence—the incorporation of ethical principles into the creation and deployment of AI systems. Fairness is one such principle, ensuring that machine learning algorithms do not produce biased outcomes or discriminate against any group of the population with respect to sensitive attributes, such as race or gender. In this context, visualization techniques can help identify data imbalances and disparities in model performance across different demographic groups. However, there is a lack of guidance towards clear and effective representations that support entry-level users in fairness analysis, particularly when considering that the approaches to fairness visualization can vary significantly. In this regard, the goal of this work is to present a comprehensive analysis of current tools directed at visualizing and examining group fairness in machine learning, with a focus on both data and binary classification model outcomes. These visualization tools are reviewed and discussed, concluding with the proposition of a focused set of visualization guidelines directed towards improving the comprehensibility of fairness visualizations.},
  archive      = {J_AIR},
  author       = {Cruz, António and Salazar, Teresa and Carvalho, Manuel and Maçãs, Catarina and Machado, Penousal and Abreu, Pedro Henriques},
  doi          = {10.1007/s10462-025-11179-w},
  journal      = {Artificial Intelligence Review},
  month        = {6},
  number       = {6},
  pages        = {1-38},
  shortjournal = {Artif. Intell. Rev.},
  title        = {Guidelines for designing visualization tools for group fairness analysis in binary classification},
  volume       = {58},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Computer vision based approaches for fish monitoring systems: A comprehensive study. <em>AIR</em>, <em>58</em>(6), 1-44. (<a href='https://doi.org/10.1007/s10462-025-11180-3'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Fish monitoring has become increasingly popular due to its growing real-world applications and recent advancements in intelligent technologies such as AI, Computer Vision, and Robotics. The primary objective of this article is to review benchmark datasets used in fish monitoring while introducing a novel framework that categorizes fish monitoring applications into four main domains: Fish Detection and Recognition (FDR), Fish Biomass Estimation (FBE), Fish Behavior Classification (FBC), and Fish Health Analysis (FHA). Additionally, this study proposes dedicated workflows for each domain, marking the first comprehensive effort to establish such a structured approach in this field. The detection and recognition of fish involve identifying fish and fish species. Estimating fish biomass focuses on counting fish and measuring their size and weight. Fish Behavior Classification tracks and analyzes movement and extracts behavioral patterns. Finally, health analysis assesses the general health of the fish. The methodologies and techniques are analyzed separately within each domain, providing a detailed examination of their specific applications and contributions to fish monitoring. These innovations enable fish species classification, fish freshness evaluation, fish counting, and body length measurement for biomass estimation. The study concludes by reviewing the development of key datasets and techniques over time, identifying existing gaps and limitations in current frameworks, and proposing future research directions in fish monitoring applications.},
  archive      = {J_AIR},
  author       = {Al-Abri, Said and Keshvari, Sanaz and Al-Rashdi, Khalfan and Al-Hmouz, Rami and Bourdoucen, Hadj},
  doi          = {10.1007/s10462-025-11180-3},
  journal      = {Artificial Intelligence Review},
  month        = {6},
  number       = {6},
  pages        = {1-44},
  shortjournal = {Artif. Intell. Rev.},
  title        = {Computer vision based approaches for fish monitoring systems: A comprehensive study},
  volume       = {58},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Pathologyvlm: A large vision-language model for pathology image understanding. <em>AIR</em>, <em>58</em>(6), 1-19. (<a href='https://doi.org/10.1007/s10462-025-11190-1'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The previous advancements in pathology image understanding primarily involved developing models tailored to specific tasks. Recent studies have demonstrated that the large vision-language model can enhance the performance of various downstream tasks in medical image understanding. In this study, we developed a domain-specific large vision-language model (PathologyVLM) for pathology image understanding. Specifically, (1) we first construct a human pathology image-text dataset by cleaning the public medical image-text data for domain-specific alignment; (2) Using the proposed image-text data, we first train a pathology language-image pretraining (PLIP) model as the specialized visual encoder to extract the features of pathology image, and then we developed scale-invariant connector to avoid the information loss caused by image scaling; (3) We adopt two-stage learning to train PathologyVLM, first stage for domain alignment, and second stage for end to end visual question & answering (VQA) task. In experiments, we evaluate our PathologyVLM on both supervised and zero-shot VQA datasets, our model achieved the best overall performance among multimodal models of similar scale. The ablation experiments also confirmed the effectiveness of our design. We posit that our PathologyVLM model and the datasets presented in this work can promote research in field of computational pathology. All codes are available at: https://github.com/ddw2AIGROUP2CQUPT/PA-LLaVA},
  archive      = {J_AIR},
  author       = {Dai, Dawei and Zhang, Yuanhui and Yang, Qianlan and Xu, Long and Shen, Xiaojing and Xia, Shuyin and Wang, Guoyin},
  doi          = {10.1007/s10462-025-11190-1},
  journal      = {Artificial Intelligence Review},
  month        = {6},
  number       = {6},
  pages        = {1-19},
  shortjournal = {Artif. Intell. Rev.},
  title        = {Pathologyvlm: A large vision-language model for pathology image understanding},
  volume       = {58},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Sustainable AI-driven wind energy forecasting: Advancing zero-carbon cities and environmental computation. <em>AIR</em>, <em>58</em>(6), 1-35. (<a href='https://doi.org/10.1007/s10462-025-11191-0'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Accurate forecasting of wind speed and power is transforming renewable wind farm management, facilitating efficient energy supply for smart and zero-energy cities. This paper introduces a novel low-carbon Sustainable AI-Driven Wind Energy Forecasting System (SAI-WEFS) developed from a promising real-world case study in MENA region. The SAI-WEFS evaluates twelve machine learning algorithms, utilizing both single and ensemble models for forecasting wind speed (WSF) and wind power (WPF) across multiple timeframes (10 min, 30 min, 6 h, 24 h, and 36 h). The system integrates multi-time horizon predictions, where the WSF output is input for the WPF model. The environmental impact of each algorithm is assessed based on CO2 emissions for each computational hour. Predictive accuracy is assessed using mean square error (MSE) and mean absolute percentage error (MAPE). Results indicate that ensemble algorithms consistently outperform single ML models, with tree-based models demonstrating a lower environmental impact, emitting approximately 60 g of CO2 per computational hour compared to deep learning models, which emit up to 500 g per hour. This system enhances the Urban Energy Supply Decarbonization Framework (UESDF) by predicting the Urban Carbon Emission Index (UCEI) to illustrate the Urban Carbon Transition Curve.},
  archive      = {J_AIR},
  author       = {Elmousalami, Haytham and Alnaser, Aljawharah A. and Hui, Felix Kin Peng},
  doi          = {10.1007/s10462-025-11191-0},
  journal      = {Artificial Intelligence Review},
  month        = {6},
  number       = {6},
  pages        = {1-35},
  shortjournal = {Artif. Intell. Rev.},
  title        = {Sustainable AI-driven wind energy forecasting: Advancing zero-carbon cities and environmental computation},
  volume       = {58},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multi-strategy improved snow ablation optimizer: A case study of optimization of kernel extreme learning machine for flood prediction. <em>AIR</em>, <em>58</em>(6), 1-47. (<a href='https://doi.org/10.1007/s10462-025-11192-z'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Kernel Extreme Learning Machine (KELM) has the advantage of automatically extracting data features, learning and processing nonlinear problems from historical data, which can help achieve better prediction results for flood prediction problems with complex and sudden causes. Traditional flood disaster prediction usually only considers one influencing factor without considering the complex factors that affect flood occurrence. This article develops a new method for predicting the probability of flood occurrence based on 20 influencing factors. Firstly, in order to better utilize KELM performance, an improved snow ablation optimization algorithm (MESAO) was proposed for subsequent experiments by introducing a level based selection pressure mechanism, covariance matrix learning strategy, historical position based boundary adjustment strategy, and random centroid reverse learning strategy into snow ablation optimization (SAO). Secondly, MESAO is used to perform hyperparameter optimization on the regularization coefficient C and kernel function parameter S of the KELM model. Finally, the construction of a multi feature input–output model for the application of MESAO-KELM in flood prediction problems was completed. In terms of hyperparameter optimization, the numerical experimental results of this method were superior to the prediction results of 10 other intelligent algorithms and 5 regression prediction models. According to the evaluation index results, the best adaptability of MESAO optimized KELM and higher prediction accuracy and stability compared to other prediction models were demonstrated. This method overcomes the limitations of traditional prediction models based on a single influencing factor and can predict the probability of flood occurrence based on complex and variable factors. It can be said that MESAO-KELM has strong generalization ability. Accurate flood prediction can provide early warning and take measures in advance to protect and reduce the impact of floods on human and social development.},
  archive      = {J_AIR},
  author       = {Cui, Lele and Hu, Gang and Zhu, Yaolin},
  doi          = {10.1007/s10462-025-11192-z},
  journal      = {Artificial Intelligence Review},
  month        = {6},
  number       = {6},
  pages        = {1-47},
  shortjournal = {Artif. Intell. Rev.},
  title        = {Multi-strategy improved snow ablation optimizer: A case study of optimization of kernel extreme learning machine for flood prediction},
  volume       = {58},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A step gravitational search algorithm for function optimization and STTM’s synchronous feature selection-parameter optimization. <em>AIR</em>, <em>58</em>(6), 1-49. (<a href='https://doi.org/10.1007/s10462-025-11193-y'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The support tensor train machine (STTM) can make full use of the correlation of tensor data structures, while the parameter training is inefficient and feature redundancy is large. For this, a step gravitational search algorithm (SGSA) is proposed and used for synchronous feature selection and parameter optimization of STTM in this paper. Since the single population structure of the gravitational search algorithm is difficult to balance exploration and exploitation effectively, a new dual population structure is defined by the step function. Subpopulation Pop1 focuses on exploration, and a Kbest-Elite hybrid learning strategy is designed to avoid the rapid decline of exploration ability due to the rapid reduction of the size of Kbest set as well as the gravitational constant G. Subpopulation Pop2 focuses on exploitation, and a position update strategy that integrates Cauchy distribution and Gaussian distribution is designed to make Pop2 always have a certain exploration ability. Finally, use SGSA to solve the synchronous feature selection and parameter optimization problem of STTM (the resulting model is denoted as SGSA-STTM). The algorithm’s optimization performance test results show that SGSA can obtain relatively best results on most test functions compared with other state-of-the-art algorithms. The classification performance test on fMRI datasets shows that SGSA-STTM can remove more than 40% of redundant features on most datasets, which can effectively improve the efficiency of the algorithm, and the classification accuracy for the StarPlus fMRI dataset and the CMU Science 2008 fMRI dataset reached 60 and 70%, respectively.},
  archive      = {J_AIR},
  author       = {Fan, Chaodong and Yang, Laurence T. and Xiao, Leyi},
  doi          = {10.1007/s10462-025-11193-y},
  journal      = {Artificial Intelligence Review},
  month        = {6},
  number       = {6},
  pages        = {1-49},
  shortjournal = {Artif. Intell. Rev.},
  title        = {A step gravitational search algorithm for function optimization and STTM’s synchronous feature selection-parameter optimization},
  volume       = {58},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A systematic literature review on municipal solid waste management using machine learning and deep learning. <em>AIR</em>, <em>58</em>(6), 1-51. (<a href='https://doi.org/10.1007/s10462-025-11196-9'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Population growth and urbanization have led to a significant increase in solid waste. However, conventional methods of treating and recycling this waste have inherent problems, such as low efficiency, poor precision, high cost, and severe environmental hazards. To address these challenges, Artificial Intelligence (AI) has gained popularity in recent years as a potential solution for municipal solid-waste management (MSWM). A few applications of AI, based on Machine Learning (ML) and Deep Learning (DL) techniques, have been used for MSWM. This study reviews the current landscape in MSWM, highlighting the existing advantages and disadvantages of 69 studies published between 2018 and 2024 using the PRISMA methodology. The applications of ML and DL algorithms demonstrate their ability to enhance decision-making processes, improve resource recovery rates, and promote circular economy principles. Although these technologies offer promising solutions, challenges such as data availability, quality, and interdisciplinary collaboration hinder their effective implementation. The paper suggests future research directions focusing on developing robust datasets, fostering partnerships across sectors, and integrating advanced technologies with traditional waste management strategies. This research aligns with the United Nations’ Sustainable Development Goals (SDG), particularly Goal 11, which aims to make cities inclusive, safe, resilient, and sustainable. In the future, this research can contribute to making cities smarter, greener, and more resilient using ML and DL techniques.},
  archive      = {J_AIR},
  author       = {Dawar, Ishaan and Srivastava, Anisha and Singal, Maanas and Dhyani, Nirjara and Rastogi, Suvi},
  doi          = {10.1007/s10462-025-11196-9},
  journal      = {Artificial Intelligence Review},
  month        = {6},
  number       = {6},
  pages        = {1-51},
  shortjournal = {Artif. Intell. Rev.},
  title        = {A systematic literature review on municipal solid waste management using machine learning and deep learning},
  volume       = {58},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Self-supervised hypergraph structure learning. <em>AIR</em>, <em>58</em>(6), 1-30. (<a href='https://doi.org/10.1007/s10462-025-11199-6'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Traditional Hypergraph Neural Networks (HGNNs) often assume that hypergraph structures are perfectly constructed, yet real-world hypergraphs are typically corrupted by noise, missing data, or irrelevant information, limiting the effectiveness of hypergraph learning. To address this challenge, we propose SHSL, a novel Self-supervised Hypergraph Structure Learning framework that jointly explores and optimizes hypergraph structures without external labels. SHSL consists of two key components: a self-organizing initialization module that constructs latent hypergraph representations, and a differentiable optimization module that refines hypergraphs through gradient-based learning. These modules collaboratively capture high-order dependencies to enhance hypergraph representations. Furthermore, SHSL introduces a dual learning mechanism to simultaneously guide structure exploration and optimization within a unified framework. Experiments on six public datasets demonstrate that SHSL outperforms state-of-the-art baselines, achieving Accuracy improvements of 1.36% $$-$$ 32.37% and 2.23% $$-$$ 27.54% on hypergraph exploration and optimization tasks, and 1.19% $$-$$ 8.4% on non-hypergraph datasets. Robustness evaluations further validate SHSL’s effectiveness under noisy and incomplete scenarios, highlighting its practical applicability. The implementation of SHSL and all experimental codes are publicly available at: https://github.com/MingyuanLi88888/SHSL.},
  archive      = {J_AIR},
  author       = {Li, Mingyuan and Yang, Yanlin and Meng, Lei and Peng, Lu and Zhao, Haixing and Ye, Zhonglin},
  doi          = {10.1007/s10462-025-11199-6},
  journal      = {Artificial Intelligence Review},
  month        = {6},
  number       = {6},
  pages        = {1-30},
  shortjournal = {Artif. Intell. Rev.},
  title        = {Self-supervised hypergraph structure learning},
  volume       = {58},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A survey of artificial intelligence enabled channel estimation methods: Recent advance, performance, and outlook. <em>AIR</em>, <em>58</em>(6), 1-35. (<a href='https://doi.org/10.1007/s10462-025-11202-0'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the continuous advancement of wireless communication and the emergence of new communication scenarios, channel estimation, as a core component of wireless system design, has become increasingly significant. This paper reviews important advancements in channel estimation within wireless communication systems, including applications in single-input single-output (SISO), multi-input multi-output (MIMO), orthogonal time frequency space (OTFS), orthogonal frequency division multiplexing (OFDM), and the latest reconfigurable intelligent surface (RIS) systems. We first revisit traditional channel estimation methods, such as least squares (LS), minimum mean square error (MMSE), and compressed sensing (CS), and detail their fundamental principles and scopes of application. Subsequently, we discuss how deep learning techniques offer new perspectives and solutions for channel estimation through models like convolutional neural network (CNN), recurrent neural network (RNN), generative adversarial network (GAN), long short-term memory (LSTM), and graph neural network (GNN), particularly in terms of their potential to handle complicated and dynamic environments. Additionally, we analyze the advantages and disadvantages of these methods in emerging scenarios, including RIS-assisted communications, vehicular networks, indoor positioning, sensing mobile networks, and satellite communications. We also address current methods for evaluating channel estimation performance and highlight the importance of standardization and open data in advancing the field. Finally, we summarize potential future directions for channel estimation and consider its prospects in sixth-generation (6 G) wireless communication systems, aiming to provide a comprehensive technical reference on channel estimation and promote the design of efficient and intelligent wireless communication systems.},
  archive      = {J_AIR},
  author       = {Li, Binglin and Zheng, Qinghe and Tian, Xinyu and Yang, Mingqiang and Gui, Guan and Jiang, Weiwei and Lei, Hongjiang and Jiang, Jing and Shu, Feng and Elhanashi, Abdussalam and Saponara, Sergio},
  doi          = {10.1007/s10462-025-11202-0},
  journal      = {Artificial Intelligence Review},
  month        = {6},
  number       = {6},
  pages        = {1-35},
  shortjournal = {Artif. Intell. Rev.},
  title        = {A survey of artificial intelligence enabled channel estimation methods: Recent advance, performance, and outlook},
  volume       = {58},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Image-based deep learning for smart digital twins: A review. <em>AIR</em>, <em>58</em>(5), 1-36. (<a href='https://doi.org/10.1007/s10462-024-11002-y'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Smart Digital Twins (SDTs) are being increasingly used to virtually replicate and predict the behaviors of complex physical systems through continual data assimilation, enabling the optimization of the performance of these systems by controlling the actions of systems. Recently, the Deep Learning (DL) models have significantly enhanced the capabilities of SDTs, particularly for tasks such as predictive maintenance, anomaly detection, and optimization. In many domains, including medicine, engineering, and education, SDTs use image data (image-based SDTs) to observe, learn, and control system behaviors. This paper focuses on various approaches and associated challenges in developing image-based SDTs by continually assimilating image data from physical systems. The paper also discusses the challenges in designing and implementing DL models for SDTs, including data acquisition, processing, and interpretation. In addition, insights into the future directions and opportunities for developing new image-based DL approaches to develop robust SDTs are provided. This includes the potential for using generative models for data augmentation, developing multi-modal DL models, and exploring the integration of DL models with other technologies, including Fifth Generation (5 G), edge computing, and the Internet of Things (IoT). In this paper, we describe the image-based SDTs, which enable broader adoption of the Digital Twins (DTs) paradigms across a broad spectrum of areas and the development of new methods to improve the abilities of SDTs in replicating, predicting, and optimizing the behavior of complex systems.},
  archive      = {J_AIR},
  author       = {Islam, Md Ruman and Subramaniam, Mahadevan and Huang, Pei-Chi},
  doi          = {10.1007/s10462-024-11002-y},
  journal      = {Artificial Intelligence Review},
  month        = {5},
  number       = {5},
  pages        = {1-36},
  shortjournal = {Artif. Intell. Rev.},
  title        = {Image-based deep learning for smart digital twins: A review},
  volume       = {58},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Neural combinatorial optimization with reinforcement learning in industrial engineering: A survey. <em>AIR</em>, <em>58</em>(5), 1-37. (<a href='https://doi.org/10.1007/s10462-024-11045-1'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent trends, machine learning is widely used to support decision-making in various domains and industrial operations. Because of the increasing complexity of modern industries, industrial engineering aims not only to increase cost-effectiveness and productivity but also to consider sustainability, resilience, and human centricity, resulting in many-objective, constrained, and stochastic operations research. Based on the above stringent requirements, combinatorial optimization (CO) problems are thus developed to support the complicated decision-making process in operations research. Due to the computational complexity of exact algorithms and the uncertain solution quality of heuristic methods, there is a growing trend to leverage the power of machine learning in solving CO problems, known as neural combinatorial optimization (NCO), where reinforcement learning (RL) is the core to achieve the sequential decision support. This survey study provides a comprehensive investigation of the theories and recent advancements in applying RL to solve hard CO problems, such as vehicle routing, bin packing, assignment, scheduling, and planning problems, and, in addition, summarizes the applications of neural combinatorial optimization with reinforcement learning (NCO-RL). The detailed review found that although the research domain of NCO-RL is still under-explored, its research potential has been proven to address environmental sustainability, adaptability, and human factors. Last but not least, the technical challenges and opportunities of the NCO-RL to embrace the industry 5.0 paradigm are discussed.},
  archive      = {J_AIR},
  author       = {Chung, K. T. and Lee, C. K. M. and Tsang, Y. P.},
  doi          = {10.1007/s10462-024-11045-1},
  journal      = {Artificial Intelligence Review},
  month        = {5},
  number       = {5},
  pages        = {1-37},
  shortjournal = {Artif. Intell. Rev.},
  title        = {Neural combinatorial optimization with reinforcement learning in industrial engineering: A survey},
  volume       = {58},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Scalogram based performance comparison of deep learning architectures for dysarthric speech detection. <em>AIR</em>, <em>58</em>(5), 1-27. (<a href='https://doi.org/10.1007/s10462-024-11085-7'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Dysarthria, a speech disorder commonly associated with neurological conditions, poses challenges in early detection and accurate diagnosis. This study addresses these challenges by implementing preprocessing steps, such as noise reduction and normalization, to enhance the quality of raw speech signals and extract relevant features. Scalogram images generated through wavelet transform effectively capture the time-frequency characteristics of the speech signal, offering a visual representation of the spectral content over time and providing valuable insights into speech abnormalities related to dysarthria. Fine-tuned deep learning models, including pre-trained convolutional neural network (CNN) architectures like VGG19, DenseNet-121, Xception, and a modified InceptionV3, were optimized with specific hyperparameters using training and validation sets. Transfer learning enables these models to adapt features from general image classification tasks to classify dysarthric speech signals better. The study evaluates the models using two public datasets TORGO and UA-Speech and a third dataset collected by the authors and verified by medical practitioners. The results reveal that the CNN models achieve an accuracy (acc) range of 90% to 99%, an F1-score range of 0.95 to 0.99, and a recall range of 0.96 to 0.99, outperforming traditional methods in dysarthria detection. These findings highlight the effectiveness of the proposed approach, leveraging deep learning and scalogram images to advance early diagnosis and healthcare outcomes for individuals with dysarthria.},
  archive      = {J_AIR},
  author       = {Shabber, Shaik Mulla and Sumesh, E. P. and Ramachandran, Vidhya Lavanya},
  doi          = {10.1007/s10462-024-11085-7},
  journal      = {Artificial Intelligence Review},
  month        = {5},
  number       = {5},
  pages        = {1-27},
  shortjournal = {Artif. Intell. Rev.},
  title        = {Scalogram based performance comparison of deep learning architectures for dysarthric speech detection},
  volume       = {58},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Psychological and physiological computing based on multi-dimensional foot information. <em>AIR</em>, <em>58</em>(5), 1-56. (<a href='https://doi.org/10.1007/s10462-024-11087-5'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As the population ages, utilizing foot information to continuously monitor the physiological and psychological health status of the elderly is emerging as a pivotal tool for meeting this crucial societal demand. However, few reviews explored how multi-dimensional foot data has been integrated into physiological and psychological computing. This review is essential as it fills a critical knowledge gap in understanding the connections between physiological and psychological disorders and various components of foot information. To identify relevant literature, a thorough search was conducted across IEEE, DBLP, Elsevier, Springer, Google Scholar, and PubMed, initially yielding 2386 publications. After multiple rounds of systematic filtering, 404 publications were selected for in-depth analysis. This review examines (1) the mechanisms linking foot information to human physiological and psychological conditions, (2) the monitoring devices that collect diverse foot-based data, (3) the datasets correlating diseases with multiple foot data, (4) the prevalent feature engineering of different foot data, and (5) the cutting-edge machine and deep learning algorithms for diseases analysis. It also provides insights into future developments in foot information health monitoring for psychological and physiological computing.},
  archive      = {J_AIR},
  author       = {Li, Shengyang and Yao, Huilin and Peng, Ruotian and Ma, Yuanjun and Zhang, Bowen and Zhao, Zhiyao and Zhang, Jincheng and Chen, Siyuan and Wu, Shibin and Shu, Lin},
  doi          = {10.1007/s10462-024-11087-5},
  journal      = {Artificial Intelligence Review},
  month        = {5},
  number       = {5},
  pages        = {1-56},
  shortjournal = {Artif. Intell. Rev.},
  title        = {Psychological and physiological computing based on multi-dimensional foot information},
  volume       = {58},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Deep crowd anomaly detection: State-of-the-art, challenges, and future research directions. <em>AIR</em>, <em>58</em>(5), 1-111. (<a href='https://doi.org/10.1007/s10462-024-11092-8'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Crowd anomaly detection is one of the most popular topics in computer vision in the context of smart cities. A plethora of deep learning methods have been proposed that generally outperform other machine learning solutions. Our review primarily discusses algorithms that were published in mainstream conferences and journals between 2020 and 2022. We present datasets that are typically used for benchmarking, produce a taxonomy of the developed algorithms, and discuss and compare their performances. Our main findings are that the heterogeneities of pre-trained convolutional models have a negligible impact on crowd video anomaly detection performance. We conclude our discussion with fruitful directions for future research.},
  archive      = {J_AIR},
  author       = {Sharif, Md. Haidar and Jiao, Lei and Omlin, Christian W.},
  doi          = {10.1007/s10462-024-11092-8},
  journal      = {Artificial Intelligence Review},
  month        = {5},
  number       = {5},
  pages        = {1-111},
  shortjournal = {Artif. Intell. Rev.},
  title        = {Deep crowd anomaly detection: State-of-the-art, challenges, and future research directions},
  volume       = {58},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Musical heritage historical entity linking. <em>AIR</em>, <em>58</em>(5), 1-41. (<a href='https://doi.org/10.1007/s10462-024-11102-9'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Linking named entities occurring in text to their corresponding entity in a Knowledge Base (KB) is challenging, especially when dealing with historical texts. In this work, we introduce Musical Heritage named Entities Recognition, Classification and Linking (mhercl), a novel benchmark consisting of manually annotated sentences extrapolated from historical periodicals of the music domain. mhercl contains named entities under-represented or absent in the most famous KBs. We experiment with several State-of-the-Art models on the Entity Linking (EL) task and show that mhercl is a challenging dataset for all of them. We propose a novel unsupervised EL model and a method to extend supervised entity linkers by using Knowledge Graphs (KGs) to tackle the main difficulties posed by historical documents. Our experiments reveal that relying on unsupervised techniques and improving models with logical constraints based on KGs and heuristics to predict NIL entities (entities not represented in the KB of reference) results in better EL performance on historical documents.},
  archive      = {J_AIR},
  author       = {Graciotti, Arianna and Lazzari, Nicolas and Presutti, Valentina and Tripodi, Rocco},
  doi          = {10.1007/s10462-024-11102-9},
  journal      = {Artificial Intelligence Review},
  month        = {5},
  number       = {5},
  pages        = {1-41},
  shortjournal = {Artif. Intell. Rev.},
  title        = {Musical heritage historical entity linking},
  volume       = {58},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Applications of deep learning algorithms in ischemic stroke detection, segmentation, and classification. <em>AIR</em>, <em>58</em>(5), 1-48. (<a href='https://doi.org/10.1007/s10462-025-11119-8'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Ischemic, one of the fatal diseases characterized by insufficient blood supply to tissues poses a significant global health burden, necessitating the development of robust diagnostic and classification methodologies. Timely identification, intervention, and treatment are essential to reduce associated risk factors. Modern machine learning methods like deep learning and neural networks are being successfully employed on medical images to detect and segment the region of interest for various diseases where the performance of these computational methods is improving daily and for various tasks has surpassed natural intelligence. This success has convinced medical practitioners to trust computational methods and incorporate computer-based solutions into their clinical practices. It is, therefore, essential to examine the available solutions critically by considering their strengths and weaknesses to establish their trust and clinical applicability. In the context of the above-mentioned task, this work focuses on two aspects: first, a broad review has been done for Ischemic stroke prognostication using various brain-imaging biomarkers via diverse deep learning frameworks, and second, the reviewed works are categorized based on their computational approach employed for Ischemic stroke detection, segmentation, and classification. Finally, this work presents recent advances and future research directions to invent high-performance methods. It was concluded that recent advancements in ischemic stroke detection have achieved 85–98% accuracy using CNNs and transformer-based models with separate imaging, clinical, and molecular data, though combined analysis remains largely underexplored. Integrating vascular imaging, clinical signs, and proteomic data can enhance real-time monitoring. However, challenges persist in unifying diverse parameters, necessitating advanced methodologies such as transfer learning, multi-task learning, advanced transformers, federated learning, and standardized protocols. These findings pave the way for improved diagnostics, treatment, and outcomes in stroke management.},
  archive      = {J_AIR},
  author       = {Kousar, Tanzeela and Rahim, Mohd Shafry Mohd and Iqbal, Sajid and Yousaf, Fatima and Sanaullah, Muhammad},
  doi          = {10.1007/s10462-025-11119-8},
  journal      = {Artificial Intelligence Review},
  month        = {5},
  number       = {5},
  pages        = {1-48},
  shortjournal = {Artif. Intell. Rev.},
  title        = {Applications of deep learning algorithms in ischemic stroke detection, segmentation, and classification},
  volume       = {58},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A review on persian question answering systems: From traditional to modern approaches. <em>AIR</em>, <em>58</em>(5), 1-27. (<a href='https://doi.org/10.1007/s10462-025-11122-z'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Question answering systems (QAS) are designed to answer questions in natural language. The objective of these types of systems is to reduce the user’s effort to manually check the retrieved documents to find the answer to the query in natural language and to create an accurate answer to the user’s query. In recent years, with the emergence of Large Language Models (LLMs), these systems have evolved significantly across different languages. However, the development of QAS in low resource languages such as Persian, while progressing, still faces unique challenges. Development of these systems has become problematic in Persian language due to the lack of comprehensive processing tools, limited question answering datasets, and specific challenges of this language. The current study provides a brief explanation of these systems’ evolution from traditional architectures to LLM-based approaches, their classification, the challenges specific to Persian language, existing question-answering datasets and language models, and studies conducted concerning Persian QAS.},
  archive      = {J_AIR},
  author       = {Jolfaei, Safoura Aghadavoud and Mohebi, Azadeh},
  doi          = {10.1007/s10462-025-11122-z},
  journal      = {Artificial Intelligence Review},
  month        = {5},
  number       = {5},
  pages        = {1-27},
  shortjournal = {Artif. Intell. Rev.},
  title        = {A review on persian question answering systems: From traditional to modern approaches},
  volume       = {58},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Dmixnet: A dendritic multi-layered perceptron architecture for image recognition. <em>AIR</em>, <em>58</em>(5), 1-22. (<a href='https://doi.org/10.1007/s10462-025-11123-y'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the field of image recognition, the all-MLP architecture (MLP-Mixer) shows superior performance. However, the current MLP-Mixer is solely based on fully connected layers. The nonlinear capability of fully connected layers is relatively weak, and their simple stacked structure has limitations under complex conditions. Therefore, inspired by the diversity of neurons in the human brain, we propose an innovative DMixNet, a dendritic multi-layered perceptron architecture. Rooted in the theory of dendritic neurons from neuroscience, we propose a dendritic neural unit (DNU) that enhances DMixNet with stronger biological interpretability and more robust nonlinear processing capabilities. The flexibility of dendritic structures allows the DNU to adjust its architecture to achieve different functionalities. Based on the DNU, we propose a novel channel fusion network $$\text {DNU}_\text {E}$$ and a dendritic classifier $$\text {DNU}_\text {C}$$ . The $$\text {DNU}_\text {E}$$ substitutes the traditional two fully connected layers as the channel mixer, constructing a dendritic mixer layer to enhance the fusion capability of channel information within the entire framework. Meanwhile, the $$\text {DNU}_\text {C}$$ replaces the traditional linear classifier, effectively improving the model’s classification performance. Experimental results demonstrate that DMixNet achieves improvements of 2.13%, 4.79%, 4.71%, 23.14% on the CIFAR-10, CIFAR-100, Tiny-ImageNet and COIL-100 benchmark image recognition datasets, respectively, as well as a 14.78% enhancement on the medical image classification dataset PathMNIST, outperforming other state-of-the-art architectures. Code is available at https://github.com/KarilynXu/DMixNet .},
  archive      = {J_AIR},
  author       = {Xu, Weixiang and Song, Yaotong and Gupta, Shubham and Jia, Dongbao and Tang, Jun and Lei, Zhenyu and Gao, Shangce},
  doi          = {10.1007/s10462-025-11123-y},
  journal      = {Artificial Intelligence Review},
  month        = {5},
  number       = {5},
  pages        = {1-22},
  shortjournal = {Artif. Intell. Rev.},
  title        = {Dmixnet: A dendritic multi-layered perceptron architecture for image recognition},
  volume       = {58},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Metaheuristic optimization algorithms for multi-area economic dispatch of power systems: Part II—a comparative study. <em>AIR</em>, <em>58</em>(5), 1-51. (<a href='https://doi.org/10.1007/s10462-025-11125-w'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-Area Economic Dispatch (MAED) plays an important role in the operation and planning of power systems. In Part I of this series, we have summarized various optimization techniques to the MAED problem comprehensively, showing clearly that metaheuristic optimization algorithms (MOAs) have become the dominant approach for solving this problem due to their ease of application and powerful search capability. Although many different types of MOAs have been proposed, there is no study on the comprehensive evaluation, comparison and recommendation of different MOAs for the MAED problem. In this part, we selected 32 algorithms including differential evolution, particle swarm optimization, teaching–learning based algorithm, JAYA algorithm, and their advanced variants to evaluate and compare their performance on the eleven reported MAED cases summarized in Part I of this series. The comparative study was comprehensively conducted based on various performance criteria including solution quality, convergence, robustness, computational efficiency, and statistical analysis. The comparisons reveal that the DE series is the most competitive overall. Nevertheless, there is no single algorithm that ranks in the top three on all cases. This study can provide a practical reference and applicability recommendation for the selection of MOAs for solving the MAED problem.},
  archive      = {J_AIR},
  author       = {Wang, Yang and Xiong, Guojiang},
  doi          = {10.1007/s10462-025-11125-w},
  journal      = {Artificial Intelligence Review},
  month        = {5},
  number       = {5},
  pages        = {1-51},
  shortjournal = {Artif. Intell. Rev.},
  title        = {Metaheuristic optimization algorithms for multi-area economic dispatch of power systems: Part II—a comparative study},
  volume       = {58},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A review on EEG-based multimodal learning for emotion recognition. <em>AIR</em>, <em>58</em>(5), 1-63. (<a href='https://doi.org/10.1007/s10462-025-11126-9'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Emotion recognition from electroencephalography (EEG) signals is crucial for human–computer interaction yet poses significant challenges. While various techniques exist for detecting emotions through EEG signals, contemporary studies have explored the combination of EEG signals with other modalities. However, the field is still rapidly evolving, and new advancements are constantly being made. Comprehensive research is essential by distilling all factors in one manuscript to stay up-to-date with the latest research findings. This review offers an overview of multimodal leaning in EEG-based emotion recognition and discusses current literature in this domain from 2017 to 2024. Three primary challenges addressed are the fusion algorithm, representation of different modalities, and classification scheme. The review thoroughly explores the challenges of fusion algorithms, representation of different modalities, and classification schemes through empirical studies, offering a detailed analysis of their effectiveness. The approach of fusion algorithms is compared and evaluated based on convention and deep learning fusion methods. The research results show that poor performance is attributed to a lack of rigor and inadequate methods to identify correlated patterns across modalities to create a unified representation for experiments. This indicates a need for more thorough analysis and integration of data in future studies. When more than two modalities are involved, it becomes increasingly important to consider different aspects of classification schemes, such as the number of features and model selection. However, designing a classification scheme without considering the number of parameters and emotional categories may compromise the accuracy of classification. To aid readers in understanding the findings better, the results of different classification schemes and their corresponding accuracies are summarized. The tables in this draft display the fusion algorithms researchers utilize and evaluate the effectiveness of selected modalities, providing valuable insights for decision-making. Key contributions include a systematic survey of EEG features, an exploration of EEG integration with behavioral modalities, an investigation of fusion methods, and an overview of key challenges and future research directions in implementing multimodal emotion recognition systems.},
  archive      = {J_AIR},
  author       = {Pillalamarri, Rajasekhar and Shanmugam, Udhayakumar},
  doi          = {10.1007/s10462-025-11126-9},
  journal      = {Artificial Intelligence Review},
  month        = {5},
  number       = {5},
  pages        = {1-63},
  shortjournal = {Artif. Intell. Rev.},
  title        = {A review on EEG-based multimodal learning for emotion recognition},
  volume       = {58},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A synergetic intuitionistic fuzzy model combining AHP, entropy, and ELECTRE for data fabric solution selection. <em>AIR</em>, <em>58</em>(5), 1-54. (<a href='https://doi.org/10.1007/s10462-025-11128-7'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Amidst the ongoing digital transformation, enterprises face the challenge of managing ever-expanding volumes of data from multiple sources and diverse structures. Semantic data fabric emerges as a promising solution, offering an innovative approach to integrate data resources from various channels and produce meaningful insights. The selection of an appropriate data fabric solution has become a focal point amidst burgeoning data lakes and silos, garnering international attention. This research aims to precisely evaluate potential data fabric solutions using an innovative synergetic intuitionistic fuzzy evaluation model. We propose a hybrid approach, IF-AHP-Entropy-ELECTRE, which integrates the analytic hierarchy process (AHP), entropy, and elimination et choix traduisant la réalité (ELECTRE) techniques within the framework of intuitionistic fuzzy (IF) sets. This model is utilized to a data fabric solution selection (DFSS) issue for an appliance company, identifying the optimal solution based on its superior performance in foundational technology, real-time analytics, and customizable features. The effectiveness and adaptability of this approach stem from a novel hierarchical evaluative criteria system encompassing technology, capability, cost, and security. The criteria weights, derived from IF-AHP-Entropy, reflect both subjective and objective judgments of decision-makers, while the ranking generated by IF-ELECTRE employs a piecewise scoring function and a unique distance measure, factoring in optimistic perspectives and cross-information. Through sensitivity and comparative analyses, our approach demonstrates enhanced robustness, precision, and adaptability in dynamic DFSS contexts when compared to traditional multicriteria decision-making methods, such as IF-WSM, IF-TOPSIS, and IF-ELECTRE. Specifically, our model provides a decision support system that combines extensive functionality with a user-friendly design, making it highly effective for DFSS challenges. This approach not only establishes a solid foundation for data integration in data management but also enhances business competitiveness and supports sustained growth in the digital economy.},
  archive      = {J_AIR},
  author       = {Zhou, Fang and Chen, Ting-Yu},
  doi          = {10.1007/s10462-025-11128-7},
  journal      = {Artificial Intelligence Review},
  month        = {5},
  number       = {5},
  pages        = {1-54},
  shortjournal = {Artif. Intell. Rev.},
  title        = {A synergetic intuitionistic fuzzy model combining AHP, entropy, and ELECTRE for data fabric solution selection},
  volume       = {58},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A novel reinforcement learning-based multi-operator differential evolution with cubic spline for the path planning problem. <em>AIR</em>, <em>58</em>(5), 1-56. (<a href='https://doi.org/10.1007/s10462-025-11129-6'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Path planning in autonomous driving systems remains a critical challenge, requiring algorithms capable of generating safe, efficient, and reliable routes. Existing state-of-the-art methods, including graph-based and sampling-based approaches, often produce sharp, suboptimal paths and struggle in complex search spaces, while trajectory-based algorithms suffer from high computational costs. Recently, meta-heuristic optimization algorithms have shown effective performance but often lack learning ability due to their inherent randomness. This paper introduces a unified benchmarking framework, named Reda’s Path Planning Benchmark 2024 (RP2B-24), alongside two novel reinforcement learning (RL)-based path-planning algorithms: Q-Spline Multi-Operator Differential Evolution (QSMODE), utilizing Q-learning (Q-tables), and Deep Q-Spline Multi-Operator Differential Evolution (DQSMODE), based on Deep Q-networks (DQN). Both algorithms are integrated under a single framework and enhanced with cubic spline interpolation to improve path smoothness and adaptability. The proposed RP2B-24 library comprises 50 distinct benchmark problems, offering a comprehensive and generalizable testing ground for diverse path-planning algorithms. Unlike traditional approaches, RL in QSMODE/DQSMODE is not merely a parameter adjustment method but is fully utilized to generate paths based on the accumulated search experience to enhance path quality. QSMODE/DQSMODE introduces a unique self-training update mechanism for the Q-table and DQN based on candidate paths within the algorithm’s population, complemented by a secondary update method that increases population diversity through random action selection. An adaptive RL switching probability dynamically alternates between these Q-table update modes. DQSMODE and QSMODE demonstrated superior performance, outperforming 22 state-of-the-art algorithms, including the IMODEII. The algorithms ranked first and second in the Friedman test and SNE-SR ranking test, achieving scores of 99.2877 (DQSMODE) and 93.0463 (QSMODE), with statistically significant results in the Wilcoxon test. The practical applicability of the algorithm was validated on a ROS-based system using a four-wheel differential drive robot, which successfully followed the planned paths in two driving scenarios, demonstrating the algorithm’s feasibility and effectiveness for real-world scenarios. The source code for the proposed benchmark and algorithm is publicly available for further research and experimentation at: https://github.com/MohamedRedaMu/RP2B24-Benchmark and https://github.com/MohamedRedaMu/QSMODEAlgorithm .},
  archive      = {J_AIR},
  author       = {Reda, Mohamed and Onsy, Ahmed and Haikal, Amira Y. and Ghanbari, Ali},
  doi          = {10.1007/s10462-025-11129-6},
  journal      = {Artificial Intelligence Review},
  month        = {5},
  number       = {5},
  pages        = {1-56},
  shortjournal = {Artif. Intell. Rev.},
  title        = {A novel reinforcement learning-based multi-operator differential evolution with cubic spline for the path planning problem},
  volume       = {58},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Evaluation of belief entropies: From the perspective of evidential neural network. <em>AIR</em>, <em>58</em>(5), 1-34. (<a href='https://doi.org/10.1007/s10462-025-11130-z'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In Dempster-Shafer’s theory, the belief entropy for total uncertainty measure of mass function has attracted the interest of many researchers in recent years. Although various belief entropies can meet some basic requirements, how to judge the performance of belief entropies is still an open issue. This paper proposes a novel evidential neural network (ENN) classifier to evaluate different belief entropies in practical application. Driven by the least commitment principle (LCP), the maximum entropy is integrated into the traditional divergence-based loss function. The proposed loss function consists of divergence and maximum entropy parts, which considers not only the distribution difference but also the degree of approaching the maximum entropy. Some classification experiments are conducted in 7 real-world datasets to validate the effectiveness of the proposed evaluation method.},
  archive      = {J_AIR},
  author       = {Mao, Kun and Wang, Yanni and Zhou, Wen and Ye, Jiangang and Fang, Bin},
  doi          = {10.1007/s10462-025-11130-z},
  journal      = {Artificial Intelligence Review},
  month        = {5},
  number       = {5},
  pages        = {1-34},
  shortjournal = {Artif. Intell. Rev.},
  title        = {Evaluation of belief entropies: From the perspective of evidential neural network},
  volume       = {58},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). An actor-critic based recommender system with context-aware user modeling. <em>AIR</em>, <em>58</em>(5), 1-40. (<a href='https://doi.org/10.1007/s10462-025-11134-9'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recommendation systems empower users with tailored service assistance by learning about their interactions with systems and recommending items based on their preferences and interests. Typical recommender systems view the recommendation process as a static procedure disregarding the fact that users’ preferences are changed over time. Reinforcement learning (RL) approaches are the most advanced and recent techniques used by researchers to handle challenges where the user’s interest is captured by their most recent interactions with the system. However, most of the recent research on RL-based recommender systems focuses on simply the user’s recent interactions to generate the recommendations without taking into account the context of the user in which these interactions occur. The context has a great impact on users’ interests, behaviors, and ratings e.g., user mood, time, day type, companion, social circle, and location. In this paper, we propose a context-aware deep reinforcement learning-based recommender system focusing on context-specific state modeling methods. In this approach, states are designed based on the user’s most recent context. In parallel, a list-wise version of the context-aware recommender agent is also proposed, in which a list of items is recommended to users at each step of interaction based on their context. The findings of the study indicate that modeling users’ preferences in combination with contextual variables improves the performance of RL-based recommender systems. Furthermore, we evaluate the proposed method on context-based datasets in an offline environment. The performance in terms of evaluation measures optimally indicates the worth of the proposed method in comparison with existing studies. More precisely, the highest Presicion@5, MAP@10, and NDCG@10 of the context-aware recommender agent are 77%, 76%, and 74% respectively.},
  archive      = {J_AIR},
  author       = {Bukhari, Maryam and Maqsood, Muazzam and Adil, Farhan},
  doi          = {10.1007/s10462-025-11134-9},
  journal      = {Artificial Intelligence Review},
  month        = {5},
  number       = {5},
  pages        = {1-40},
  shortjournal = {Artif. Intell. Rev.},
  title        = {An actor-critic based recommender system with context-aware user modeling},
  volume       = {58},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A critical review of artificial intelligence based techniques for automatic prediction of cephalometric landmarks. <em>AIR</em>, <em>58</em>(5), 1-56. (<a href='https://doi.org/10.1007/s10462-025-11135-8'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Automatic cephalometric landmark detection has emerged as a pivotal area of research that combines medical imaging, computer vision, and orthodontics. The identification of cephalometric landmarks is of utmost importance in the field of orthodontics, as it contributes significantly to the process of diagnosing and planning treatments, as well as conducting research on craniofacial aspects. This practice holds the potential to improve clinical decision-making and ultimately increase the outcomes for patients. This work explores a wide range of strategies, encompassing both traditional edge-based methods and advanced deep learning approaches. The study leveraged various academic publication databases like IEEEXplore, ScienceDirect, arXiv, Springer and PubMed to thoroughly search for articles related to automatic cephalometric landmark detection. Additionally, other pertinent publications were acquired from credible sources like Google Scholar and Wiley databases. Screening the articles relied on three selection criteria: (a) publication titles, abstracts, literature reviews, (b) cephalometric radiograph datasets suitable for 2D landmarking, and (c) studies conducted over different time periods were employed to gain a comprehensive understanding of the evolution of methodologies used in landmark prediction to identify the most relevant papers for this review. The initial electronic database search identified 268 papers on landmark detection. A total of 118 publications were selected and incorporated in the present study after a meticulous screening process. Performance analysis was conducted on studies that reported Successful Detection Rates (SDRs) within different clinically accepted precision ranges, Mean Radial Error (MRE) with Standard Deviation (SD) between manually annotated and automated landmarks as outcomes. Bar graphs and custom combination plots were utilized to analyse the correlations among different methodologies employed and their evaluation metrics outcomes. The performance comparison results indicate that Deep Learning techniques showed superior accuracy in automating 2D cephalometric landmarks compared to other conventional and Machine Learning approaches. Recently, more advanced Deep Learning algorithms have been developed to improve the accuracy of automatic landmark prediction.},
  archive      = {J_AIR},
  author       = {Neeraja, R. and Anbarasi, L. Jani},
  doi          = {10.1007/s10462-025-11135-8},
  journal      = {Artificial Intelligence Review},
  month        = {5},
  number       = {5},
  pages        = {1-56},
  shortjournal = {Artif. Intell. Rev.},
  title        = {A critical review of artificial intelligence based techniques for automatic prediction of cephalometric landmarks},
  volume       = {58},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Quantum deep learning in neuroinformatics: A systematic review. <em>AIR</em>, <em>58</em>(5), 1-24. (<a href='https://doi.org/10.1007/s10462-025-11136-7'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Neuroinformatics involves replicating and detecting intricate brain activities through computational models, where deep learning plays a foundational role. Our systematic review explores quantum deep learning (QDL), an emerging deep learning sub-field, to assess whether quantum-based approaches outperform classical approaches in brain data learning tasks. This review is a pioneering effort to compare these deep learning domains. In addition, we survey neuroinformatics and its various subdomains to understand the current state of the field and where QDL stands relative to recent advancements. Our statistical analysis of tumor classification studies (n = 16) reveals that QDL models achieved a mean accuracy of 0.9701 (95% CI 0.9533–0.9868), slightly outperforming classical models with a mean accuracy of 0.9650 (95% CI 0.9475–0.9825). We observed similar trends across Alzheimer’s diagnosis, stroke lesion detection, cognitive state monitoring, and brain age prediction, with QDL demonstrating better performance in metrics such as F1-score, dice coefficient, and RMSE. Our findings, paired with prior documented quantum advantages, highlight QDL’s promise in healthcare applications as quantum technology evolves. Our discussion outlines existing research gaps with the intent of encouraging further investigation in this developing field.},
  archive      = {J_AIR},
  author       = {Orka, Nabil Anan and Awal, Md. Abdul and Liò, Pietro and Pogrebna, Ganna and Ross, Allen G. and Moni, Mohammad Ali},
  doi          = {10.1007/s10462-025-11136-7},
  journal      = {Artificial Intelligence Review},
  month        = {5},
  number       = {5},
  pages        = {1-24},
  shortjournal = {Artif. Intell. Rev.},
  title        = {Quantum deep learning in neuroinformatics: A systematic review},
  volume       = {58},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Leveraging generative AI synthetic and social media data for content generalizability to overcome data constraints in vision deep learning. <em>AIR</em>, <em>58</em>(5), 1-24. (<a href='https://doi.org/10.1007/s10462-025-11137-6'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Generalizing deep learning models across diverse content types is a persistent challenge in domains like facial emotion recognition (FER), where datasets often fail to reflect the wide range of emotional responses triggered by different stimuli. This study addresses the issue of content generalizability by comparing FER model performance between models trained on video data collected in a controlled laboratory environment, data extracted from a social media platform (YouTube), and synthetic data generated using Generative Adversarial Networks. The videos focus on facial reactions to advertisements, and the integration of these different data sources seeks to address underrepresented advertisement genres, emotional reactions, and individual diversity. Our FER models leverage Convolutional Neural Networks Xception architecture, which is fine-tuned using category based sampling. This ensures training and validation data represent diverse advertisement categories, while testing data includes novel content to evaluate generalizability rigorously. Precision–recall curves and ROC-AUC metrics are used to assess performance. Results indicate a 7% improvement in accuracy and a 12% increase in precision–recall AUC when combining real-world social media and synthetic data, demonstrating reduced overfitting and enhanced content generalizability. These findings highlight the effectiveness of integrating synthetic and real-world data to build FER systems that perform reliably across more diverse and representative content.},
  archive      = {J_AIR},
  author       = {Alipour, Panteha and Gallegos, Erika},
  doi          = {10.1007/s10462-025-11137-6},
  journal      = {Artificial Intelligence Review},
  month        = {5},
  number       = {5},
  pages        = {1-24},
  shortjournal = {Artif. Intell. Rev.},
  title        = {Leveraging generative AI synthetic and social media data for content generalizability to overcome data constraints in vision deep learning},
  volume       = {58},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Interval-valued intuitionistic fuzzy generator based low-light enhancement model for referenced image datasets. <em>AIR</em>, <em>58</em>(5), 1-31. (<a href='https://doi.org/10.1007/s10462-025-11138-5'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Image processing is a rapidly evolving research field with diverse applications across science and technology, including biometric systems, surveillance, traffic signal control and medical imaging. Digital images taken in low-light conditions are often affected by poor contrast and pixel detail, leading to uncertainty. Although various fuzzy based techniques have been proposed for low-light image enhancement, there remains a need for a model that can manage greater uncertainty while providing better structural information. To address this, an interval-valued intuitionistic fuzzy generator is proposed to develop an advanced low-light image enhancement model for referenced image datasets. The enhancement process involves a structural similarity index measure (SSIM) based optimization approach with respect to the parameters of the generator. For experimental validation, the Low-Light (LOL), LOLv2-Real and LOLv2-Synthetic benchmark datasets are utilized. The results are compared with several existing techniques using quality metrics such as SSIM, peak signal-to-noise ratio, absolute mean brightness error, mean absolute error, root mean squared error, blind/referenceless image spatial quality evaluator and naturalness image quality evaluator, demonstrating the superiority of the proposed model. Ultimately, the model’s performance is benchmarked against state-of-the-art methods, highlighting its enhanced efficiency.},
  archive      = {J_AIR},
  author       = {Selvam, Chithra and Sundaram, Dhanasekar},
  doi          = {10.1007/s10462-025-11138-5},
  journal      = {Artificial Intelligence Review},
  month        = {5},
  number       = {5},
  pages        = {1-31},
  shortjournal = {Artif. Intell. Rev.},
  title        = {Interval-valued intuitionistic fuzzy generator based low-light enhancement model for referenced image datasets},
  volume       = {58},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A three-way decision model for multi-granular support intuitionistic fuzzy rough sets based on overlap functions. <em>AIR</em>, <em>58</em>(5), 1-44. (<a href='https://doi.org/10.1007/s10462-025-11139-4'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Three-way decision-making provides an effective framework for addressing uncertainty, aligning closely with human cognitive decision patterns. This paper proposes a novel three-way decision model based on multi-granular support intuitionistic fuzzy rough sets, integrating n-dimensional overlap and grouping functions. The model constructs optimistic and pessimistic upper and lower approximations to optimize decision rules and introduces score and precision functions for ranking. To validate the model, a consumer decision-making algorithm was developed and applied to empirical data. The results demonstrate that the proposed model effectively narrows decision boundary regions, enhances decision-making precision, and supports decision-making in complex multi-attribute scenarios. This study not only advances rough set theory but also offers practical tools for addressing real-world uncertainty in decision-making.},
  archive      = {J_AIR},
  author       = {Yu, Peng and Zhao, Xiyue},
  doi          = {10.1007/s10462-025-11139-4},
  journal      = {Artificial Intelligence Review},
  month        = {5},
  number       = {5},
  pages        = {1-44},
  shortjournal = {Artif. Intell. Rev.},
  title        = {A three-way decision model for multi-granular support intuitionistic fuzzy rough sets based on overlap functions},
  volume       = {58},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). An assessment framework for explainable AI with applications to cybersecurity. <em>AIR</em>, <em>58</em>(5), 1-19. (<a href='https://doi.org/10.1007/s10462-025-11141-w'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Several explainable AI methods are available, but there is a lack of a systematic comparison of such methods. This paper contributes in this direction, by providing a framework for comparing alternative explanations in terms of complexity and robustness. We exemplify our proposal on a real case study in the cybersecurity domain, namely, phishing website detection. In fact, in this domain explainability is a compelling issue because of its potential benefits for the detection of fraudulent attacks and for the design of efficient security defense mechanisms. For this purpose, we apply our methodology to the machine learning models obtained by analyzing a publicly available dataset containing features extracted from malicious and legitimate web pages. The experiments show that our methodology is quite effective in selecting the explainability method which is, at the same time, less complex and more robust.},
  archive      = {J_AIR},
  author       = {Calzarossa, Maria Carla and Giudici, Paolo and Zieni, Rasha},
  doi          = {10.1007/s10462-025-11141-w},
  journal      = {Artificial Intelligence Review},
  month        = {5},
  number       = {5},
  pages        = {1-19},
  shortjournal = {Artif. Intell. Rev.},
  title        = {An assessment framework for explainable AI with applications to cybersecurity},
  volume       = {58},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Reconstructing dance movements using a mathematical model based on optimized nature-inspired machine learning. <em>AIR</em>, <em>58</em>(5), 1-27. (<a href='https://doi.org/10.1007/s10462-025-11142-9'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recording dance movements nowadays becomes problematic due to complex recording procedures and unavoidable data loss caused by some resource elements, like bodily or clothing material composition. The task of filling in the missing data for the performed motion and retrieving the sequence as a whole becomes difficult due to the characteristics of physical motion, which include cinematographic perspectives that render the movements themselves non-linear. Previous works have indicated some level of success in loss motion recovery, but only for a short span. The first two-dimensional matrix computation paradigm lacks theoretical justification for the recovery of the non-linear motion information, which is a limitation. This issue has been addressed by developing a new enhanced model called the Machine Learning 2-Dimensional Matrix-Calculation (ML-2DMC), which is presumably designed to achieve the rehabilitation and recovery of human movement and dance. The proposed procedure takes advantage of the effectiveness of the machine learning algorithms and applies 2D matrix computation methods, permitting good results across a variety of experiments. A new method called fractal-chaotic map grey wolf optimizer (FCM-GWO) is introduced to optimize the parameters of ML-2DMC. This optimization itself increases the efficiency of the ML-2DMC model when it comes to the retrieval of complex movements of the processes involving dance. The paper gives experimental results validating the efficiency of the proposed approach against other methods, such as recurrent convolutional neural networks and other more sophisticated models and approaches incorporating multi-paradigm sensors and devices such as Kinect sensors along with low-rank matrix completion methods. The study shows that the ML-2DMC-FCM-GWO method effectively tackles the complexities of non-linear human motion and dance recovery, making a significant addition to the field of motion analysis and restoration.},
  archive      = {J_AIR},
  author       = {Song, Jing and Ding, Li},
  doi          = {10.1007/s10462-025-11142-9},
  journal      = {Artificial Intelligence Review},
  month        = {5},
  number       = {5},
  pages        = {1-27},
  shortjournal = {Artif. Intell. Rev.},
  title        = {Reconstructing dance movements using a mathematical model based on optimized nature-inspired machine learning},
  volume       = {58},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). AI-based bridge maintenance management: A comprehensive review. <em>AIR</em>, <em>58</em>(5), 1-43. (<a href='https://doi.org/10.1007/s10462-025-11144-7'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Over recent decades, the implementation of Artificial Intelligence (AI) across various industrial fields from automation to cybersecurity has been transformative. Whilst the implementations of linking AI and data sciences remain complex and thus limited, they both aim to harness data for actionable insights and future predictions. A research focal point in the application of AI in maintenance is crucial for the sustainability and efficiency of assets. Typically, in the civil infrastructure, there are significant benefits to be gained from AI-driven applications. This study reviews the implementation of the AI in bridge maintenance decision-making by conducting a review of literature on major works undertaken by researchers and analysing 102 scientific articles published from 2010 to 2023. Our literature review revealed an emerging trend in recent studies, focusing on the exploration of defect prognosis in bridge maintenance. However, upon further analysis, it becomes evident that there is a notable gap in the existing literature, in the studies related to performance-based prognostic maintenance strategies for bridges. This gap presents an opportunity for future research, one that could yield valuable insights in the field of bridge maintenance and asset management. The review also reveals the focus of the existing literature on defect identification by using the bridge imagery processing. While the AI’s potential in damage detection using bridge imagery is evident, challenges persist including the computational processing and data availability. This review of the literature includes a comprehensive overview of the current implementation of AI in bridge maintenance, highlighting limitations, challenges, and prospective directions.},
  archive      = {J_AIR},
  author       = {Shahrivar, Farham and Sidiq, Amir and Mahmoodian, Mojtaba and Jayasinghe, Sanduni and Sun, Zhiyan and Setunge, Sujeeva},
  doi          = {10.1007/s10462-025-11144-7},
  journal      = {Artificial Intelligence Review},
  month        = {5},
  number       = {5},
  pages        = {1-43},
  shortjournal = {Artif. Intell. Rev.},
  title        = {AI-based bridge maintenance management: A comprehensive review},
  volume       = {58},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multi-strategy fusion mayfly algorithm on task offloading and scheduling for IoT-based fog computing multi-tasks learning. <em>AIR</em>, <em>58</em>(5), 1-46. (<a href='https://doi.org/10.1007/s10462-025-11145-6'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The rapid development of Internet of Things (IoT) technology has accumulated a large amount of data, which needs to be stored, processed and deeply analyzed to meet the specific goals and needs of users. As an emerging computing model, Fog computing can allocate a large number of computing resources reasonably. In order to solve the problem of insufficient population diversity and low algorithm efficiency, Aiming at the task scheduling problem of Bag-of-Tasks(BoT) application in cloud and fog environment, a multi-strategy fusion Mayfly Algorithm was proposed. The method of improving the individual learning coefficient and the global learning coefficient is used to significantly improve the convergence speed, local search ability, and global search ability, and then the method of improving the social positive attraction coefficient is used to balance the development and exploration ability of the algorithm and help the algorithm to get rid of the local optimum. The main goal of the logarithm Mayfly Algorithm (lMA) is to complete the tasks of the IoT task package in the fog system efficiently with low cost in terms of reducing execution time and operating costs. The improved algorithms were compared with Mayfly Algorithm (MA), Genetic Algorithm (GA), Grey Wolf Optimizer (GWO), Tyrannosaurus Optimization Algorithm (TROA), Harris Hawks Optimization (HHO), Reptile Search Algorithm (RSA) and Red-Tailed Hawk Algorithm (RTH), and the results showed that lMA was significantly improved in terms of reducing processing time and operating cost. The performance of lMA is verified, and the results show that the model can not only save transmission energy consumption but also have good convergence.},
  archive      = {J_AIR},
  author       = {Sui, Xiao-Fei and Wang, Jie-Sheng and Zhang, Shi-Hui and Zhang, Si-Wen and Zhang, Yun-Hao},
  doi          = {10.1007/s10462-025-11145-6},
  journal      = {Artificial Intelligence Review},
  month        = {5},
  number       = {5},
  pages        = {1-46},
  shortjournal = {Artif. Intell. Rev.},
  title        = {Multi-strategy fusion mayfly algorithm on task offloading and scheduling for IoT-based fog computing multi-tasks learning},
  volume       = {58},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multi-attribute decision-making using q-rung orthopair fuzzy zagreb index. <em>AIR</em>, <em>58</em>(5), 1-31. (<a href='https://doi.org/10.1007/s10462-025-11149-2'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The q-rung orthopair fuzzy set (q-ROFS), an extension of intuitionistic and Pythagorean fuzzy sets, offers greater flexibility in representing vague information with two possible outcomes, yes or no. The fuzzy Zagreb index is an important graph parameter, widely used in fields such as network theory, spectral graph theory, mathematics, and molecular chemistry. In this paper, the first and second Zagreb indices for q-rung orthopair fuzzy graphs (q-ROFGs) are introduced, and bounds for these indices are established, including their behavior in regular q-ROFGs. Additionally, it is explored, how various graph operations such as union, Cartesian product, direct product, and lexicographical product affect the first Zagreb index. Furthermore, a new approach is presented that combines Multiple-Attribute Decision-Making (MADM) with graph-based models to improve decision-making, particularly in vaccine selection. The methodology constructs a bipartite graph for each attribute, where virologists assign membership and non-membership values to vaccines. The Zagreb index is used to measure the importance of each vaccine, and a weighted aggregation technique normalizes the scores. The final ranking is derived from a computed score function. The results demonstrate the effectiveness of the approach in providing a systematic and mathematically rigorous framework for multi-attribute decision-making, with rank correlation analysis confirming its robustness compared to existing methods such as q-ROF PROMETHEE, q-ROF VICOR, q-ROF TOPSIS, q-ROFWG, and q-ROFWA.},
  archive      = {J_AIR},
  author       = {Rao, Yongsheng and Kosari, Saeed and Hameed, Saira and Yousaf, Zulqarnain},
  doi          = {10.1007/s10462-025-11149-2},
  journal      = {Artificial Intelligence Review},
  month        = {5},
  number       = {5},
  pages        = {1-31},
  shortjournal = {Artif. Intell. Rev.},
  title        = {Multi-attribute decision-making using q-rung orthopair fuzzy zagreb index},
  volume       = {58},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Csan: Cross-coupled semantic adversarial network for cross-modal retrieval. <em>AIR</em>, <em>58</em>(5), 1-27. (<a href='https://doi.org/10.1007/s10462-025-11152-7'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cross-modal retrieval aims to correlate multimedia data by bridging the heterogeneity gap. Most cross-modal retrieval approaches learn a common subspace to project the multimedia data into the subspace for directly measuring the similarity. However, the existing cross-modal retrieval frameworks cannot fully capture the semantic consistency in the limited supervision information. In this paper, we propose a Cross-coupled Semantic Adversarial Network (CSAN) for cross-modal retrieval. The main structure of this approach is mainly composed of the generative adversarial network, i.e., each modality branch is equipped with a generator and a discriminator. Besides, a cross-coupled semantic architecture is designed to fully explore the correlation of paired heterogeneous samples. To be specific, we couple a forward branch with an inverse mapping and implement a weight-sharing strategy of the inverse mapping branch to the branch of another modality. Furthermore, a cross-coupled consistency loss is introduced to minimize the semantic gap between the representations of the inverse mapping branch and the forward branch. Extensive qualitative and quantitative experiments are conducted to evaluate the performance of the proposed approach. By comparing against the previous works, the experiment results demonstrate our approach outperforms state-of-the-art works.},
  archive      = {J_AIR},
  author       = {Li, Zhuoyi and Lu, Huibin and Fu, Hao and Meng, Fanzhen and Gu, Guanghua},
  doi          = {10.1007/s10462-025-11152-7},
  journal      = {Artificial Intelligence Review},
  month        = {5},
  number       = {5},
  pages        = {1-27},
  shortjournal = {Artif. Intell. Rev.},
  title        = {Csan: Cross-coupled semantic adversarial network for cross-modal retrieval},
  volume       = {58},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A comprehensive review of artificial intelligence - Based algorithm towards fetal facial anomalies detection (2013–2024). <em>AIR</em>, <em>58</em>(5), 1-49. (<a href='https://doi.org/10.1007/s10462-025-11160-7'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This review explores the growing need for AI-based algorithms in diagnosing fetal facial anomalies, which are often difficult to detect due to limitations in current imaging techniques like ultrasound and MRI. These challenges include low resolution, motion artifacts, and insufficient annotated data, which hinder early and accurate diagnosis. AI algorithms, particularly deep learning models like Convolutional Neural Networks (CNNs) and U-Net, offers significant potential to overcome these challenges by analyzing large datasets and improving image analysis. Early diagnosis of these anomalies is crucial for enabling timely interventions, personalized treatment plans, and better prenatal care. This study adopts a systematic review approach, to assess existing research on AI-based approaches for fetal facial anomaly detection. The review includes peer-reviewed studies from key biomedical databases like PubMed, IEEE Xplore, and ScienceDirect, focusing on the last 15 years. Studies that implemented AI techniques and manual techniques for detecting anomalies in prenatal images were considered. Among all models reviewed, CNNs and U-Net architectures were found to be the most effective. CNNs excel at classifying medical images, while U-Net is particularly powerful for image segmentation. These models have demonstrated high accuracy in identifying conditions such as cleft lip, palate, and micrognathia. The use of AI in clinical settings can greatly enhance the precision and efficiency of fetal anomaly detection, addressing current limitations in medical imaging. By integrating AI, particularly deep learning models, into clinical workflows, prenatal care can be transformed, allowing for earlier and more accurate diagnosis. This can lead to more personalized care, timely interventions, and ultimately improved health outcomes for affected individuals and their families.},
  archive      = {J_AIR},
  author       = {Sriraam, Natarajan and Chinta, Babu and Seshadri, Suresh and Suresh, Sudarshan},
  doi          = {10.1007/s10462-025-11160-7},
  journal      = {Artificial Intelligence Review},
  month        = {5},
  number       = {5},
  pages        = {1-49},
  shortjournal = {Artif. Intell. Rev.},
  title        = {A comprehensive review of artificial intelligence - Based algorithm towards fetal facial anomalies detection (2013–2024)},
  volume       = {58},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Automatic brain MRI tumors segmentation based on deep fusion of weak edge and context features. <em>AIR</em>, <em>58</em>(5), 1-25. (<a href='https://doi.org/10.1007/s10462-025-11151-8'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Brain tumors pose a significant health risk to humans. The edge boundaries in brain magnetic resonance imaging (MRI) are often blurred and poorly defined, which can easily result in inaccurate segmentation of lesion areas. To address these challenges, we proposed an Automatic Brain MRI Tumor Segmentation based on deep fusion of Weak Edge and Context features (AS-WEC). First, AS-WEC introduces the Otsu Double Threshold Weak Edges Adaptive Detection (Otsu-WD), which focuses on tumor edge information and differentiates between lesion edges and normal cerebral sulci and gyri. Second, an edge branching network based on the Gated Recurrent Unit (GRU) is constructed to fully preserve the edge context information of the lesion region. Finally, a maximum index fusion mechanism has been designed to incorporate a multilayer feature map, preventing the loss of edge details during the deep feature fusion process. The experimental results demonstrate that the Otsu-WD method outperforms the Canny and TEED algorithms in detecting brain MRI tumor edges. In brain MRI tumor segmentation, AS-WEC delivers a clearer visual segmentation effect compared to the classical UNet++ network and recent models like PVT-Former. On both datasets, AS-WEC demonstrated improvements across multiple metrics. The Dice averaged 92.96%, and the mIoU reached 93.12%, effectively validating the method’s efficacy in brain MRI tumor segmentation. Code and pre-trained models are available at https://github.com/DL-Segment/AS-WEC.git .},
  archive      = {J_AIR},
  author       = {Xiao, Leyi and Zhou, Baoxian and Fan, Chaodong},
  doi          = {10.1007/s10462-025-11151-8},
  journal      = {Artificial Intelligence Review},
  month        = {5},
  number       = {5},
  pages        = {1-25},
  shortjournal = {Artif. Intell. Rev.},
  title        = {Automatic brain MRI tumors segmentation based on deep fusion of weak edge and context features},
  volume       = {58},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Semi-supervised constrained clustering: An in-depth overview, ranked taxonomy and future research directions. <em>AIR</em>, <em>58</em>(5), 1-127. (<a href='https://doi.org/10.1007/s10462-024-11103-8'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Clustering is a well-known unsupervised machine learning approach capable of automatically grouping discrete sets of instances with similar characteristics. Constrained clustering is a semi-supervised extension to this process that can be used when expert knowledge is available to indicate constraints that can be exploited. Well-known examples of such constraints are must-link (indicating that two instances belong to the same group) and cannot-link (two instances definitely do not belong together). The research area of constrained clustering has grown significantly over the years with a large variety of new algorithms and more advanced types of constraints being proposed. However, no unifying overview is available to easily understand the wide variety of available methods, constraints and benchmarks. To remedy this, this study presents in-detail the background of constrained clustering and provides a novel ranked taxonomy of the types of constraints that can be used in constrained clustering. In addition, it focuses on the instance-level pairwise constraints, and gives an overview of its applications and its historical context. Finally, it presents a statistical analysis covering 315 constrained clustering methods, categorizes them according to their features, and provides a ranking score indicating which methods have the most potential based on their popularity and validation quality. Finally, based upon this analysis, potential pitfalls and future research directions are provided.},
  archive      = {J_AIR},
  author       = {González-Almagro, Germán and Peralta, Daniel and De Poorter, Eli and Cano, José-Ramón and García, Salvador},
  doi          = {10.1007/s10462-024-11103-8},
  journal      = {Artificial Intelligence Review},
  month        = {5},
  number       = {5},
  pages        = {1-127},
  shortjournal = {Artif. Intell. Rev.},
  title        = {Semi-supervised constrained clustering: An in-depth overview, ranked taxonomy and future research directions},
  volume       = {58},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Quantum encoding whale optimization algorithm for global optimization and adaptive infinite impulse response system identification. <em>AIR</em>, <em>58</em>(5), 1-58. (<a href='https://doi.org/10.1007/s10462-025-11120-1'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The whale optimization algorithm (WOA) is motivated by the predatory nature of bubble nets and mimics dwindling and encircling, bubble net persecuting, and randomized wandering and foraging actions to locate the expansive adequate value. However, the WOA has several deficiencies: inadequate resolution accuracy, sluggish convergence speed, susceptibility to search stagnation, and insufficient localized detection efficiency. A quantum encoding WOA (QWOA) is introduced for global optimization and adaptive infinite impulse response (IIR) system identification. The quantum encoding mechanism exploits the principle of a quantum bit to encode a search agent, which manipulates the state of an essential quantum bit and amends the location data. The quantum rotation gate modulates the quantum bit’s configuration, the quantum NOT gate accomplishes bit mutation and prohibits precocious convergence. The probability amplitude of the quantum bit represents the multistate superposition state of the search agent, which enriches the population diversity, advances individualized information, broadens the detection scope, inhibits premature convergence, facilitates estimation effectiveness, and promotes solution accuracy. The QWOA not only promptly locates the search scope nearest the most appropriate solution but also computes the spiral-shaped encircling route to promote predation diversification. Twenty-three benchmark functions, eight real-world engineering layouts, and adaptive IIR system identification are utilized to assess the QWOA’s feasibility and effectiveness. The experimental results reveal that QWOA successfully equalizes exploration and exploitation to accelerate convergence speed, ameliorate calculation accuracy, and strengthen stability and robustness.},
  archive      = {J_AIR},
  author       = {Zhang, Jinzhong and Liu, Wei and Zhang, Gang and Zhang, Tan},
  doi          = {10.1007/s10462-025-11120-1},
  journal      = {Artificial Intelligence Review},
  month        = {5},
  number       = {5},
  pages        = {1-58},
  shortjournal = {Artif. Intell. Rev.},
  title        = {Quantum encoding whale optimization algorithm for global optimization and adaptive infinite impulse response system identification},
  volume       = {58},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Bridging machine learning and peptide design for cancer treatment: A comprehensive review. <em>AIR</em>, <em>58</em>(5), 1-59. (<a href='https://doi.org/10.1007/s10462-025-11148-3'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Anticancer peptides (ACPs) offer a promising alternative to traditional cancer therapies due to their specificity and reduced side effects. The development of ACPs using machine learning (ML) and deep learning (DL) follows a structured process, beginning with sequence collection from in vitro and in vivo experiments. Key features such as hydrophobicity and secondary structure are extracted, and classification models categorize peptides based on their properties. ML models predict anticancer effectiveness, followed by toxicity checks and Structure-Activity Relationship (SAR) analysis to ensure safety and efficacy, with validation tests confirming their activity. This review explores how the automated design of ACPs can be enhanced by leveraging advanced ML and DL techniques. These methods, with their ability to automate feature selection and activity prediction, have significantly improved the efficiency and accuracy of peptide discovery. This structured approach holds high potential to guide researchers in the automated design of ACPs, accelerating the discovery of effective peptides while ensuring safety. Special attention is given to new approaches such as Convolutional Neural Networks (CNNs), Recurrent Neural Networks (RNNs), and Generative Adversarial Networks (GANs), which show promise in addressing key challenges like data imbalance and computational complexity. Moreover, we examine the latest published research to compare the performance of various ML models in ACP prediction. By considering these advancements and challenges, this review outlines future opportunities for improving the scalability and reliability of ACP discovery using AI-driven techniques. This structured approach underscores the transformative impact of automation in peptide design, pushing the boundaries of modern cancer therapy development.},
  archive      = {J_AIR},
  author       = {Rezaee, Khosro and Eslami, Hossein},
  doi          = {10.1007/s10462-025-11148-3},
  journal      = {Artificial Intelligence Review},
  month        = {5},
  number       = {5},
  pages        = {1-59},
  shortjournal = {Artif. Intell. Rev.},
  title        = {Bridging machine learning and peptide design for cancer treatment: A comprehensive review},
  volume       = {58},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Application of artificial intelligence technology in the study of anthropogenic earthquakes: A review. <em>AIR</em>, <em>58</em>(5), 1-42. (<a href='https://doi.org/10.1007/s10462-025-11157-2'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Artificial intelligence (AI) has emerged as a crucial tool in the monitoring and research of anthropogenic earthquakes (AEs). Despite its utility, AEs monitoring faces significant challenges due to the intricate signal characteristics of seismic events, low signal-to-noise ratio (SNR) in data, and insufficient spatial coverage of monitoring networks, which complicate the effective deployment of AI technologies. This review systematically explores recent advancements in AI applications for identifying and classifying AEs, detecting weak signals, phase picking, event localization, and seismic risk analysis, while highlighting current issues and future developmental directions. Key challenges include accurately distinguishing specific anthropogenic seismic events due to their intricate signal patterns, limited model generalizability caused by constrained training datasets, and the lack of comprehensive models capable of handling event recognition, detection, and classification across diverse scenarios. Despite these obstacles, innovative approaches such as data-sharing platforms, transfer learning (TL), and hybrid AI models offer promising solutions to enhance AEs monitoring and improve predictive capabilities for induced seismic hazards. This review provides a scientific foundation to guide the ongoing development and application of AI technologies in AEs monitoring, forecasting, and disaster mitigation.},
  archive      = {J_AIR},
  author       = {Li, Jingwei and Zhai, Hongyu and Jiang, Changsheng and Wang, Ziang and Wang, Peng and Chang, Xu and Zhang, Yan and Wei, Yonggang and Si, Zhengya},
  doi          = {10.1007/s10462-025-11157-2},
  journal      = {Artificial Intelligence Review},
  month        = {5},
  number       = {5},
  pages        = {1-42},
  shortjournal = {Artif. Intell. Rev.},
  title        = {Application of artificial intelligence technology in the study of anthropogenic earthquakes: A review},
  volume       = {58},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Topological numbers in uniform intuitionistic fuzzy environment and their application in neural network. <em>AIR</em>, <em>58</em>(4), 1-22. (<a href='https://doi.org/10.1007/s10462-024-10965-2'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Intuitionistic Fuzzy sets combine the ideas of uniformity, membership, and non-membership grades of the elements. Similarly, Intuitionistic Fuzzy Graphs are the generalization of simple fuzzy graphs. Depending on the uniformity of the fuzzy graphs (USIF) they can be categorized in different ways via membership values. From the idea of uniform fuzzy topological indices, we have developed the concepts of uniform intuitionistic fuzzy topological indices for uniform intuitionistic fuzzy graphs. This idea provides a more adaptable and nuanced representation of structural properties in graphs or networks. According to the theory of fuzzy topological indices, the importance of topological indices changes depending on the circumstance and the specific problem at hand. When the interactions between nodes are uncertain but not always hesitant, fuzzy graph theory and its adjusted topological indices are sufficient to capture and assess the underlying structure. In such cases where uncertainty is more complicated and hesitation is a major problem then there are better ways to address by intuitionistic fuzzy graph theory and the topological indices that go along with it. This article, developed the concept of uniform intuitionistic fuzzy graphs afresh and proposed Intuitionistic Fuzzy Topological Indices. We determine these indices using the topological indices and labeling of crisp graphs, rather than relying on the degrees of intuitionistic fuzzy graphs and edge portions. This approach is then applied to find intuitionistic fuzzy topological indices. Also, we have provided the MATLAB algorithm to illustrate the concept of IF labeling of cellular neural networks of any order. An example is given to explain the idea and approach towards one kind of uniform intuitionistic fuzzy graph represented by Cellular Neural Networks and graphical plots of the indices involved are also made.},
  archive      = {J_AIR},
  author       = {Zamri, Siti Norziahidayu Amzee and Ahmad, Haseeb and Azeem, Muhammad and Almohsen, Bandar},
  doi          = {10.1007/s10462-024-10965-2},
  journal      = {Artificial Intelligence Review},
  month        = {4},
  number       = {4},
  pages        = {1-22},
  shortjournal = {Artif. Intell. Rev.},
  title        = {Topological numbers in uniform intuitionistic fuzzy environment and their application in neural network},
  volume       = {58},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). New fuzzy zeroing neural network with noise suppression capability for time-varying linear equation solving. <em>AIR</em>, <em>58</em>(4), 1-19. (<a href='https://doi.org/10.1007/s10462-024-11026-4'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, the zeroing neural network (ZNN) with continuous/discrete-time forms has realized success in solving the time-varying linear equation (TVLE). In this paper, we provide a further investigation by proposing a new fuzzy zeroing neural network (FZNN) model to solve the TVLE in noisy environment. Such a FZNN model, which has the capability of suppressing noise, is developed by using the integration enhancement and fuzzy control strategy. Then, theoretical analysis is presented to show that the proposed FZNN model can effectively solve the TVLE, even with the existence of noise. Comparative simulation results through different examples further verify the effectiveness and robustness of the proposed FZNN model on TVLE solving.},
  archive      = {J_AIR},
  author       = {Guo, Dongsheng and Zhang, Chan and Cang, Naimeng and Zhang, Xiyuan and Xiao, Lin and Sun, Zhongbo},
  doi          = {10.1007/s10462-024-11026-4},
  journal      = {Artificial Intelligence Review},
  month        = {4},
  number       = {4},
  pages        = {1-19},
  shortjournal = {Artif. Intell. Rev.},
  title        = {New fuzzy zeroing neural network with noise suppression capability for time-varying linear equation solving},
  volume       = {58},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Serial filter-wrapper feature selection method with elite guided mutation strategy on cancer gene expression data. <em>AIR</em>, <em>58</em>(4), 1-49. (<a href='https://doi.org/10.1007/s10462-024-11029-1'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Nowadays, many researchers utilize cancer gene expression data to solve the problem of cancer subtype diagnosis, but cancer gene expression data are often high-dimensional, multi-sample, and multi-classified, so a hybrid serial filter-wrapper feature selection (FS) method based on elite guided mutation strategy for cancer gene expression data is proposed. It is divided into a preliminary screening phase and a combined modeling phase. In the preliminary screening stage, the threshold values of seven filter methods are determined by the leave-one cross-validation method, and the features selected by these seven filter methods are combined to form two subsets by using the thoughts of ‘‘And’’ and ‘‘Or’’ in the logical operation. The union subset of two subsets is used in the equilibrium optimizer (EO) in the subsequent combination model stage as the reserved subset in the preliminary screening stage. The resulting hybrid framework is connected by a parallel filter method designed in the first stage with an improved EO in the second stage, which is named as SFEMEO. In order to prove the effectiveness and generalization of the proposed SFEMEO, it is compared with other 9 basic algorithms on 10 UCI data sets. It is found that the classification accuracy of the SFEMEO is improved by 0.56% ~ 20.19%, and the optimal fitness is also greatly improved. After comparing SFEMEO with other nine intelligent optimization algorithms on ten cancer gene expression data sets, it can be found that compared with most algorithms, the accuracy rate is improved by 3.73% ~ 18.13%, and the optimal fitness is relatively superior. At the same time, Wilcoxon rank sum test was used to evaluate the results of intelligent optimization algorithms such as SFEMEO, which proved the effectiveness of the proposed hybrid framework and its superiority in solving the FS problem of high-dimensional cancer gene expression data.},
  archive      = {J_AIR},
  author       = {Song, Yu-Wei and Wang, Jie-Sheng and Qi, Yu-Liang and Wang, Yu-Cai and Song, Hao-Ming and Shang-Guan, Yi-Peng},
  doi          = {10.1007/s10462-024-11029-1},
  journal      = {Artificial Intelligence Review},
  month        = {4},
  number       = {4},
  pages        = {1-49},
  shortjournal = {Artif. Intell. Rev.},
  title        = {Serial filter-wrapper feature selection method with elite guided mutation strategy on cancer gene expression data},
  volume       = {58},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Comprehensive review of single image defogging techniques: Enhancement, prior, and learning based approaches. <em>AIR</em>, <em>58</em>(4), 1-54. (<a href='https://doi.org/10.1007/s10462-024-11034-4'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the field of image processing, practical applications such as object detection, tracking, and surveillance face significant challenges, particularly in adverse weather conditions like fog. Foggy weather conditions severely reduce object visibility, thereby impeding object detection and tracking processes. To address this issue, various image defogging techniques have been proposed by researchers. The prime motive of this paper is to present a detailed analysis and summary of state-of-the-art single image defogging techniques developed over the past decade. Defogging techniques have been evaluated using both qualitative and quantitative approaches to illustrate their feasibility and effectiveness. This comprehensive review aims to provide researchers with valuable insights into existing techniques so that they can proceed in a particular direction according to their interests and applications.},
  archive      = {J_AIR},
  author       = {Pandey, Pooja and Gupta, Rashmi and Goel, Nidhi},
  doi          = {10.1007/s10462-024-11034-4},
  journal      = {Artificial Intelligence Review},
  month        = {4},
  number       = {4},
  pages        = {1-54},
  shortjournal = {Artif. Intell. Rev.},
  title        = {Comprehensive review of single image defogging techniques: Enhancement, prior, and learning based approaches},
  volume       = {58},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Advanced hybridization and optimization of DNNs for medical imaging: A survey on disease detection techniques. <em>AIR</em>, <em>58</em>(4), 1-68. (<a href='https://doi.org/10.1007/s10462-024-11049-x'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Due to the high classification accuracy and fast computational speed offered by Deep Neural Networks (DNNs), they have been widely used for the design and development of automated Artificial Intelligence (AI) tools for the detection of various diseases. These tools, which are intensive computational learning models, hold tremendous significance in healthcare for identifying various diseases. The primary goal of this review is to understand the applicability and methodology for implementing DNNs, including computational costs, for the classification of distinct diseases from disparate medical imaging datasets. This study presents an extensive survey of DNNs along with their various hybridization forms. To achieve this, the research papers surveyed have been grouped into five categories: pretrained DNNs, hyperparameter-tuned optimized DNNs, hybrid DNNs and ML classifiers, hybrid models with optimization techniques, and meta-heuristics based feature selection DNNs. The major part of this review highlights the significant role of nature-inspired meta-heuristic techniques used for hyperparameter optimization or feature selection algorithms of DNNs. Besides the frameworks and computational costs, descriptions of disparate medical image datasets and image preprocessing techniques have also been discussed under each category. Furthermore, a comparative analysis for each category has been performed on the basis of different parameters, including the type and size of datasets used, image preprocessing, methodology (as per the mentioned category), and performance (in terms of classification accuracy). This study also presents a bibliometric analysis based on the publication count of various articles related to hyperparameter-tuned optimized DNNs and meta-heuristic based feature selection DNNs. This review aims to assist potential AI researchers in choosing the most sound and appropriate DNN-based techniques for disease detection and prediction, all consolidated into a one single research paper.},
  archive      = {J_AIR},
  author       = {Bohmrah, Maneet Kaur and Kaur, Harjot},
  doi          = {10.1007/s10462-024-11049-x},
  journal      = {Artificial Intelligence Review},
  month        = {4},
  number       = {4},
  pages        = {1-68},
  shortjournal = {Artif. Intell. Rev.},
  title        = {Advanced hybridization and optimization of DNNs for medical imaging: A survey on disease detection techniques},
  volume       = {58},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A comprehensive review of optic disc segmentation methods in adult and pediatric retinal images: From conventional methods to artificial intelligence (CR-ODSeg-AP-CM2AI). <em>AIR</em>, <em>58</em>(4), 1-66. (<a href='https://doi.org/10.1007/s10462-024-11056-y'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This review, titled CR-ODSeg-AP-CM2AI (Comprehensive Review of Optic Disc Segmentation in Adult and Pediatric Retinal Images: From Conventional Methods to Artificial Intelligence), explores optic disc segmentation techniques for adult and pediatric retinal images. It emphasizes the clinical implications of these techniques in diagnosing and monitoring retinal diseases across diverse populations. We systematically categorize each segmentation method, comparing traditional approaches with advancements in artificial intelligence (AI) to highlight innovative hybrid techniques that enhance segmentation accuracy and efficiency. This review also discusses evaluation metrics and the use of larger datasets to provide insights into the effectiveness and robustness of these methods.},
  archive      = {J_AIR},
  author       = {Bansal, Avinash and Kubíček, Jan and Penhaker, Marek and Augustynek, Martin},
  doi          = {10.1007/s10462-024-11056-y},
  journal      = {Artificial Intelligence Review},
  month        = {4},
  number       = {4},
  pages        = {1-66},
  shortjournal = {Artif. Intell. Rev.},
  title        = {A comprehensive review of optic disc segmentation methods in adult and pediatric retinal images: From conventional methods to artificial intelligence (CR-ODSeg-AP-CM2AI)},
  volume       = {58},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Artificial intelligence in choroid through optical coherence tomography: A comprehensive review. <em>AIR</em>, <em>58</em>(4), 1-36. (<a href='https://doi.org/10.1007/s10462-024-11067-9'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Vision-threatening conditions, such as age-related macular degeneration (AMD) and central serous chorioretinopathy (CSCR), arise from dysfunctions in the highly vascular choroid layer in the eye’s posterior segment. Optical coherence tomography (OCT) images play a crucial role in diagnosing choroidal structural changes in clinical practice. This review emphasizes the significant efforts in developing precise detection, quantification, and automated disease classification of choroidal biomarkers. The rapid progress of artificial intelligence (AI) has triggered transformative breakthroughs across sectors including medical image analysis. Recently, the integration of AI within the diagnosis and treatment of choroidal diseases has captured significant attention. Multiple studies highlight AI’s potential to enhance diagnostic precision and optimize clinical outcomes in this context. The review provides an extensive overview of AI’s current applications in choroidal analysis using OCT imaging. It encompasses a diverse array of algorithms and techniques employed for biomarker detection, such as thickness and vascularity index, and for identifying diseases like AMD and CSCR. The overarching goal of this review is to provide an updated and comprehensive exploration of AI’s impact on the choroid, highlighting its potential, challenges, and role in driving innovation in the field.},
  archive      = {J_AIR},
  author       = {Selvam, Amrish and Driban, Matthew and Ong, Joshua and Bollepalli, Sandeep Chandra and Sahel, José-Alain and Chhablani, Jay and Vupparaboina, Kiran Kumar},
  doi          = {10.1007/s10462-024-11067-9},
  journal      = {Artificial Intelligence Review},
  month        = {4},
  number       = {4},
  pages        = {1-36},
  shortjournal = {Artif. Intell. Rev.},
  title        = {Artificial intelligence in choroid through optical coherence tomography: A comprehensive review},
  volume       = {58},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Metaheuristic optimization algorithms for multi-area economic dispatch of power systems: Part i—a comprehensive survey. <em>AIR</em>, <em>58</em>(4), 1-48. (<a href='https://doi.org/10.1007/s10462-024-11070-0'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-area economic dispatch (MAED) provides an indispensable component for the security and economic operation of contemporary power systems. Over recent years, numerous metaheuristic optimization algorithms (MOAs) have surfaced for addressing the MAED problem. However, none of the literature to date conducted a comprehensive statistical research work on the MAED problem. In part I of this series, we present a comprehensive survey on this problem. (1) We collect all eleven reported MAED cases studied over the years. These cases have different structures, scales, and constraints. We illustrate the structures of all cases and provide their corresponding system parameters. (2) We collect all the MOA solution algorithms. These algorithms are inspired by different ways, and we categorize them in detail and review them comprehensively. (3) We list the detailed applications of MOAs on different cases and count the percentage of studies on each case. (4) Finally, we summarize the current research progress and point out the future research directions in terms of MAED models and solution methods, respectively. This survey provides an extensive overview of the MAED cases and its solution methods. It can provide applicable and reference suggestions for future research on the MAED problem.},
  archive      = {J_AIR},
  author       = {Wang, Yang and Xiong, Guojiang},
  doi          = {10.1007/s10462-024-11070-0},
  journal      = {Artificial Intelligence Review},
  month        = {4},
  number       = {4},
  pages        = {1-48},
  shortjournal = {Artif. Intell. Rev.},
  title        = {Metaheuristic optimization algorithms for multi-area economic dispatch of power systems: Part i—a comprehensive survey},
  volume       = {58},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Transformer-based image and video inpainting: Current challenges and future directions. <em>AIR</em>, <em>58</em>(4), 1-45. (<a href='https://doi.org/10.1007/s10462-024-11075-9'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Image inpainting is currently a hot topic within the field of computer vision. It offers a viable solution for various applications, including photographic restoration, video editing, and medical imaging. Deep learning advancements, notably convolutional neural networks (CNNs) and generative adversarial networks (GANs), have significantly enhanced the inpainting task with an improved capability to fill missing or damaged regions in an image or a video through the incorporation of contextually appropriate details. These advancements have improved other aspects, including efficiency, information preservation, and achieving both realistic textures and structures. Recently, Vision Transformers (ViTs) have been exploited and offer some improvements to image or video inpainting. The advent of transformer-based architectures, which were initially designed for natural language processing, has also been integrated into computer vision tasks. These methods utilize self-attention mechanisms that excel in capturing long-range dependencies within data; therefore, they are particularly effective for tasks requiring a comprehensive understanding of the global context of an image or video. In this paper, we provide a comprehensive review of the current image/video inpainting approaches, with a specific focus on Vision Transformer (ViT) techniques, with the goal to highlight the significant improvements and provide a guideline for new researchers in the field of image/video inpainting using vision transformers. We categorized the transformer-based techniques by their architectural configurations, types of damage, and performance metrics. Furthermore, we present an organized synthesis of the current challenges, and suggest directions for future research in the field of image or video inpainting.},
  archive      = {J_AIR},
  author       = {Elharrouss, Omar and Damseh, Rafat and Belkacem, Abdelkader Nasreddine and Badidi, Elarbi and Lakas, Abderrahmane},
  doi          = {10.1007/s10462-024-11075-9},
  journal      = {Artificial Intelligence Review},
  month        = {4},
  number       = {4},
  pages        = {1-45},
  shortjournal = {Artif. Intell. Rev.},
  title        = {Transformer-based image and video inpainting: Current challenges and future directions},
  volume       = {58},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Transfer learning in agriculture: A review. <em>AIR</em>, <em>58</em>(4), 1-37. (<a href='https://doi.org/10.1007/s10462-024-11081-x'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The rapid growth of the global population has placed immense pressure on agriculture to enhance food production while addressing environmental and socioeconomic challenges such as biodiversity loss, water scarcity, and climate variability. Addressing these challenges requires adopting modern techniques and advancing agricultural research. Although some techniques, such as machine learning and deep learning, are increasingly used in agriculture, progress is constrained by the lack of large labelled datasets. This constraint arises because collecting data is often time-consuming, labour-intensive, and requires expert knowledge for data annotation. To mitigate data limitations, transfer learning (TL) offers a viable solution by allowing pre-trained models to be adapted for agricultural applications. Many researchers have demonstrated TL’s potential to advance agriculture. Despite its importance, there is a lack of a comprehensive review, which could be essential to guide researchers in this field. Given the significance and the lack of a review paper, this paper provides a review dedicated to TL in agriculture, offering three main contributions. First, we provide an in-depth background study on TL and its applications in agriculture. Second, we offer a comprehensive examination of TL-based agricultural applications, covering pre-trained models, dataset sources, input image types, implementation platforms, and TL approaches. Third, based on an exploration of the existing studies, we identify the challenges faced when applying TL in agriculture. Finally, to address the identified challenges, we recommend suggestions for future research directions.},
  archive      = {J_AIR},
  author       = {Hossen, Md Ismail and Awrangjeb, Mohammad and Pan, Shirui and Mamun, Abdullah Al},
  doi          = {10.1007/s10462-024-11081-x},
  journal      = {Artificial Intelligence Review},
  month        = {4},
  number       = {4},
  pages        = {1-37},
  shortjournal = {Artif. Intell. Rev.},
  title        = {Transfer learning in agriculture: A review},
  volume       = {58},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Intelligent strategy for severity scoring of skin diseases based on clinical decision-making thinking with lesion-aware transformer. <em>AIR</em>, <em>58</em>(4), 1-24. (<a href='https://doi.org/10.1007/s10462-024-11083-9'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Skin diseases are numerous in types and high in incidence, posing a serious threat to human health. Accurately assessing the severity of skin diseases helps dermatologists in making personalized treatment decisions. However, focusing solely on the skin lesion itself and ignoring the true state of the surrounding skin can lead to distorted results. Assessing the severity of the condition should be a holistic process. Specifically, dermatologists need to compare the abnormal skin with surrounding skin to conduct the diagnosis. To imitate such diagnosis practice of dermatologists, we propose LSATrans, a Transformer based framework customized for severity scoring of skin diseases. Different from the Standard Self-Attention module, we propose the Lesion-aware Self-Attention (LSA) module. LSA can capture the visual features of both lesion and normal surrounding skin areas and include their relationship in modeling. In addition to LSA, the proposed LSATrans also introduces a contrastive learning strategy for further optimization. We first evaluated the performance of LSATrans in scar, atopic dermatitis, and psoriasis scoring tasks, and it achieved mean absolute errors of 0.5895, 0.5614, and 0.5416 respectively in these three tasks. Furthermore, we conducted additional validation of LSATrans’s performance in two distinct skin disease diagnosis tasks, where it demonstrated remarkable outcomes with AUCs of 0.9774 and 0.9801, respectively, in the classification of common skin diseases and subtypes of skin diseases. These results are better than existing methods, indicating that LSATrans is expected to become a universal, accurate and objective intelligent tool for scoring the severity of skin diseases.},
  archive      = {J_AIR},
  author       = {Huang, Kai and Sun, Kai and Li, Jiayi and Wu, Zhe and Wu, Xian and Duan, Yuping and Chen, Xiang and Zhao, Shuang},
  doi          = {10.1007/s10462-024-11083-9},
  journal      = {Artificial Intelligence Review},
  month        = {4},
  number       = {4},
  pages        = {1-24},
  shortjournal = {Artif. Intell. Rev.},
  title        = {Intelligent strategy for severity scoring of skin diseases based on clinical decision-making thinking with lesion-aware transformer},
  volume       = {58},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Deep learning techniques for atmospheric turbulence removal: A review. <em>AIR</em>, <em>58</em>(4), 1-33. (<a href='https://doi.org/10.1007/s10462-024-11086-6'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Atmospheric turbulence significantly complicates the interpretation and analysis of images by distorting them, making it hard to classify and track objects within a scene using traditional methods. This distortion arises from unpredictable, spatially varying disturbances, challenging the effectiveness of standard model-based techniques. These methods often become impractical due to their complexity and high memory demands, further complicating the task of restoring scenes affected by atmospheric turbulence. Deep learning approaches offer faster operation and are capable of implementation on small devices. This paper reviews the characteristics of atmospheric turbulence and its impact on acquired imagery. It compares performances of a range of state-of-the-art deep neural networks, including Transformers, SWIN and MAMBA, when used to mitigate spatio-temporal image distortions. Furthermore, this review presents: a list of available datasets; applicable metrics for evaluation of mitigation methods; an exhaustive list of state-of-the-art and historical mitigation methods. Finally, a critical statistical analysis of a range of example models is included. This review provides a roadmap of how datasets and metrics together with currently used and newly developed deep learning methods could be used to develop the next generation of turbulence mitigation techniques.},
  archive      = {J_AIR},
  author       = {Hill, Paul and Anantrasirichai, Nantheera and Achim, Alin and Bull, David},
  doi          = {10.1007/s10462-024-11086-6},
  journal      = {Artificial Intelligence Review},
  month        = {4},
  number       = {4},
  pages        = {1-33},
  shortjournal = {Artif. Intell. Rev.},
  title        = {Deep learning techniques for atmospheric turbulence removal: A review},
  volume       = {58},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Intelligent cinematography: A review of AI research for cinematographic production. <em>AIR</em>, <em>58</em>(4), 1-53. (<a href='https://doi.org/10.1007/s10462-024-11089-3'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper offers the first comprehensive review of artificial intelligence (AI) research in the context of real camera content acquisition for entertainment purposes and is aimed at both researchers and cinematographers. Addressing the lack of review papers in the field of intelligent cinematography (IC) and the breadth of related computer vision research, we present a holistic view of the IC landscape while providing technical insight, important for experts across disciplines. We provide technical background on generative AI, object detection, automated camera calibration and 3-D content acquisition, with references to assist non-technical readers. The application sections categorize work in terms of four production types: General Production, Virtual Production, Live Production and Aerial Production. Within each application section, we (1) sub-classify work according to research topic and (2) describe the trends and challenges relevant to each type of production. In the final chapter, we address the greater scope of IC research and summarize the significant potential of this area to influence the creative industries sector. We suggest that work relating to virtual production has the greatest potential to impact other mediums of production, driven by the growing interest in LED volumes/stages for in-camera virtual effects (ICVFX) and automated 3-D capture for virtual modeling of real world scenes and actors. We also address ethical and legal concerns regarding the use of creative AI that impact on artists, actors, technologists and the general public.},
  archive      = {J_AIR},
  author       = {Azzarelli, Adrian and Anantrasirichai, Nantheera and Bull, David R.},
  doi          = {10.1007/s10462-024-11089-3},
  journal      = {Artificial Intelligence Review},
  month        = {4},
  number       = {4},
  pages        = {1-53},
  shortjournal = {Artif. Intell. Rev.},
  title        = {Intelligent cinematography: A review of AI research for cinematographic production},
  volume       = {58},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A comprehensive review of deep learning-based hyperspectral image reconstruction for agri-food quality appraisal. <em>AIR</em>, <em>58</em>(4), 1-28. (<a href='https://doi.org/10.1007/s10462-024-11090-w'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Hyperspectral imaging (HSI) has recently emerged as a promising tool for various agricultural applications. However, high equipment cost, instrumentation complexity, and data-intensive nature have limited its widespread adoption. To overcome these challenges, reconstructing hyperspectral data from simple, cost-effective color or RGB (red-green-blue) images using advanced deep learning algorithms offers a practically attractive solution for a wide range of applications in food quality control and assurance. Through advanced deep learning algorithms, it is possible to capture and reconstruct spectral information from simple, cost-effective RGB imaging to create a reliable, efficient, and scalable system with accuracy comparable to dedicated, expensive HSI systems. This review provides a comprehensive overview of recent advances in deep learning techniques for HSI reconstruction and highlights the transformative impact of deep learning-based hyperspectral image reconstruction on agricultural and food products and anticipates a future where these innovations will lead to more advanced and widespread applications in the agri-food industry.},
  archive      = {J_AIR},
  author       = {Ahmed, Md. Toukir and Monjur, Ocean and Khaliduzzaman, Alin and Kamruzzaman, Mohammed},
  doi          = {10.1007/s10462-024-11090-w},
  journal      = {Artificial Intelligence Review},
  month        = {4},
  number       = {4},
  pages        = {1-28},
  shortjournal = {Artif. Intell. Rev.},
  title        = {A comprehensive review of deep learning-based hyperspectral image reconstruction for agri-food quality appraisal},
  volume       = {58},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Machine learning for smart water distribution systems: Exploring applications, challenges and future perspectives. <em>AIR</em>, <em>58</em>(4), 1-56. (<a href='https://doi.org/10.1007/s10462-024-11093-7'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The advancements of the Internet of Things and Low-Power Wide-Area Network technology will accelerate in the next future the adoption of smart meters in water distribution systems, enabling the collection of a huge amount of fine-grained data. How to turn massive smart meter data into actionable knowledge will be the key point to limit water wastage and promote efficient and sustainable distribution. Although the collection of data worldwide is currently limited, the potential future impact of exploiting data-driven and machine learning methods is increasingly recognized in research and industry, as shown by many scientific works published in recent years. In particular, the interest in deep learning for smart water distribution systems is increasing, motivated by the ability to learn intricate patterns from big data. This work aims to provide an overview of the current research and identify challenges for future directions by conducting an application-oriented survey. Specifically, by analysing data characteristics and operational targets, we propose a new taxonomy that helps structure properly the macro-areas of water management into infrastructure analysis, demand analysis and water quality monitoring. Existing methods are discussed for each application under these three stages. In addition, we also discuss potential research directions, such as federated learning, incremental learning, probabilistic modeling and explainability and address broad issues like data availability and implications for privacy.},
  archive      = {J_AIR},
  author       = {Taloma, Redemptor Jr Laceda and Cuomo, Francesca and Comminiello, Danilo and Pisani, Patrizio},
  doi          = {10.1007/s10462-024-11093-7},
  journal      = {Artificial Intelligence Review},
  month        = {4},
  number       = {4},
  pages        = {1-56},
  shortjournal = {Artif. Intell. Rev.},
  title        = {Machine learning for smart water distribution systems: Exploring applications, challenges and future perspectives},
  volume       = {58},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Cluster-based decision making: A novel approach for handling large-scale alternatives in rooftop solar PV site selection. <em>AIR</em>, <em>58</em>(4), 1-45. (<a href='https://doi.org/10.1007/s10462-024-11097-3'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This research proposes to develop a systematic methodology for ranking a large number of alternatives in large-scale real-world problems under uncertainty, where the traditional individual ranking methods fail to provide meaningful and actionable insights. This paper introduces a novel framework for fuzzy large-scale decision-making (FLSDM) using triangular neutrosophic fuzzy numbers (TNFNs) to perform cluster-based ranking as a solution to these challenges. The proposed approach develops the Sugeno–Weber operator within the TNFN environment for data aggregation. An advanced K-means++ algorithm is designed to enable precise clustering of alternatives. Using the TOPSIS and DEA cross-efficiency model, extended for TNFNs, the clusters are ranked, and the alternatives are prioritized within each cluster. The practical use of the proposed approach is demonstrated through a real-world case study on rooftop solar photovoltaic (PV) site selection. Additionally, thorough analyses are conducted to validate its robustness and effectiveness.},
  archive      = {J_AIR},
  author       = {Sharma, Priya and Mehlawat, Mukesh Kumar and Gupta, Pankaj and Pamucar, Dragan},
  doi          = {10.1007/s10462-024-11097-3},
  journal      = {Artificial Intelligence Review},
  month        = {4},
  number       = {4},
  pages        = {1-45},
  shortjournal = {Artif. Intell. Rev.},
  title        = {Cluster-based decision making: A novel approach for handling large-scale alternatives in rooftop solar PV site selection},
  volume       = {58},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Dimensionless machine learning: Dimensional analysis to improve LSSVM and ANN models and predict bearing capacity of circular foundations. <em>AIR</em>, <em>58</em>(4), 1-53. (<a href='https://doi.org/10.1007/s10462-024-11099-1'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This research focuses on developing hybrid intelligence techniques to predict the bearing capacity of circular foundations using the most effective parameters. In this regard, a database involving 968 test circular foundations involving different rock properties, soil characteristics, and foundation radius has been prepared by laboratory tests for training and testing the new model presented in this study. For adequately considering various factors, the several parameters of depth (D) in m, density of soil (DS) in gr/cm3, internal angle of friction (IAF) in degree, cohesion of soil (CS) in kg/cm2, and foundation radius (FR) in m were considered as the input of the model and bearing capacity in kg/cm2 was considered as the target parameter. Three main strategies were addressed in this study. First, four well-known machine learning algorithms, Artificial Neural Network (ANN), Bagging Regressor (BR), Least Squares Support Vector Machine (LSSVM), and Gradient Boosting Regressor (GBR), were adopted to predict bearing capacity. Second, a novel and robust mathematical computation named dimensional analysis (DA) theorem was integrated with machine learning techniques to improve the accuracy and performance of the models by decreasing the number of inputs. Third, based on DA implementation, a rational mathematical formula using gene expression programming (GEP) was structured to anticipate bearing capacity. Furthermore, the sensitivity analysis process specified the impact of the five effective factors. The results of this study demonstrate the effectiveness of integrating DA with machine learning models to predict the BC of circular foundations. Among the developed models, the DA-LSSVM model showed superior performance, achieving an R² of 0.998 and 0.996 for training and testing, respectively, indicating high prediction accuracy. The results indicated that the IAF was the most sensitive factor with r = 0.709, while CS was the least sensitive with r = -0.087. A graphical user interface (GUI) app has been designed to apply the proposed models in this study conveniently. In the last step of this process, the GUI and the DA-based machine learning models were implemented by analyzing two examples. According to the findings, the GUI may be employed to make a reliable and speedy projection of the bearing capacity of circular foundations by considering a variety of input parameters.},
  archive      = {J_AIR},
  author       = {Li, Hongchao and Hosseini, Shahab and Gordan, Behrouz and Zhou, Jian and Ullah, Sajid},
  doi          = {10.1007/s10462-024-11099-1},
  journal      = {Artificial Intelligence Review},
  month        = {4},
  number       = {4},
  pages        = {1-53},
  shortjournal = {Artif. Intell. Rev.},
  title        = {Dimensionless machine learning: Dimensional analysis to improve LSSVM and ANN models and predict bearing capacity of circular foundations},
  volume       = {58},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A comprehensive and systematic literature review on intrusion detection systems in the internet of medical things: Current status, challenges, and opportunities. <em>AIR</em>, <em>58</em>(4), 1-88. (<a href='https://doi.org/10.1007/s10462-024-11101-w'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The increasing number of medical devices in the Internet of Medical Things (IoMT) environment has raised significant cybersecurity concerns. These devices often have weak security features, poor design, and insufficient authentication protocols, making them vulnerable to cyberattacks and intrusions. To mitigate these threats, robust security measures are essential. This includes implementing strong security protocols, ensuring continuous security monitoring, enforcing regular updates, and maintaining a constant response plan. Additionally, designing an effective Intrusion Detection System (IDS) is crucial to safeguard patient data and devices. This paper systematically studies the current state of the literature and the essential methods for intrusion detection in the IoMT. Employing a selection process, the paper identifies 28 critical studies published between 2018 and April 2024. The intrusion detection mechanisms in the IoMT are divided into five categories: IDS based on artificial intelligence models, datasets used in IoMT for IDS, fundamental security requirements, intrusion detection processes, and evaluation metrics. This paper dissects the various mechanisms within each category in a meticulous and comprehensive analysis. Finally, the paper examines the challenges and open issues in developing IDSs in IoMT. By offering a roadmap for researchers to enhance IDSs in the IoMT, this paper has the potential to significantly impact the fields of computer engineering, cybersecurity, and healthcare, thereby contributing to the advancement of these crucial fields.},
  archive      = {J_AIR},
  author       = {Naghib, Arezou and Gharehchopogh, Farhad Soleimanian and Zamanifar, Azadeh},
  doi          = {10.1007/s10462-024-11101-w},
  journal      = {Artificial Intelligence Review},
  month        = {4},
  number       = {4},
  pages        = {1-88},
  shortjournal = {Artif. Intell. Rev.},
  title        = {A comprehensive and systematic literature review on intrusion detection systems in the internet of medical things: Current status, challenges, and opportunities},
  volume       = {58},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A systematic review of multilingual numeral recognition systems. <em>AIR</em>, <em>58</em>(4), 1-27. (<a href='https://doi.org/10.1007/s10462-025-11105-0'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multilingual numeral recognition systems in online-offline environments play an essential role in several applications like banking or financial transactions, educational sectors, hospitals, etc. Several approaches have been proposed and executed for multilingual numeral recognition for various languages. This study systematically reviews eighty-four articles on the current research on multilingual numeral recognition in offline and online environments. According to the screening criteria, 489 relevant studies were retrieved from standard databases, and only 84 studies were used for further analysis based on the insertion and elimination measures. Our study investigates and analyzes the earlier approaches, datasets developed and utilized, and machine and deep learning methods applied in multilingual numeral recognition across different languages and handwritings. It also provides possible applications and challenges for future studies. Our analysis shows that some datasets are available for scientific research, but comprehensive multilingual datasets and cross-lingual models for multilingual recognition systems are urgently needed. In addition, this review finds that convolutional neural networks (CNN) and support vector machines (SVM) are mainly applied methods in multilingual numeral recognition due to their high recognition accuracy. The findings of this review will provide valuable insights for researchers directing the development of multilingual datasets and robust and effective systems for offline and online multilingual numeral recognition for several multilingual applications.},
  archive      = {J_AIR},
  author       = {Jabde, Meenal and Patil, Chandrashekhar H. and Vibhute, Amol D. and Saini, Jatinderkumar R.},
  doi          = {10.1007/s10462-025-11105-0},
  journal      = {Artificial Intelligence Review},
  month        = {4},
  number       = {4},
  pages        = {1-27},
  shortjournal = {Artif. Intell. Rev.},
  title        = {A systematic review of multilingual numeral recognition systems},
  volume       = {58},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). An experimental survey of imbalanced learning algorithms for bankruptcy prediction. <em>AIR</em>, <em>58</em>(4), 1-57. (<a href='https://doi.org/10.1007/s10462-025-11107-y'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Information about imminent bankruptcy is crucial for financial institutions, decision-making managers, and state agencies. Since bankruptcy prediction is a prevalent research topic, many new methods have been continuously proposed. Bankruptcy prediction is frequently approached as a binary classification task. Since bankruptcy datasets are inherently imbalanced, bankruptcy classification is usually performed using class imbalance learning methods. The nature of these methods is very diverse, but they can usually be categorized as ensemble, cost-sensitive, sampling, and hybrid methods. In this paper, we provide a comprehensive experimental comparison of 45 methods. These methods were selected because they cover the approaches and algorithms frequently employed for bankruptcy prediction and imbalanced learning. Extensive experiments on 15 publicly available datasets with different imbalance ratios showed that the methods based on a combination of ensemble learning and undersampling are able to handle data imbalance and achieve the best results for bankruptcy classification.},
  archive      = {J_AIR},
  author       = {Gnip, Peter and Kanász, Róbert and Zoričak, Martin and Drotár, Peter},
  doi          = {10.1007/s10462-025-11107-y},
  journal      = {Artificial Intelligence Review},
  month        = {4},
  number       = {4},
  pages        = {1-57},
  shortjournal = {Artif. Intell. Rev.},
  title        = {An experimental survey of imbalanced learning algorithms for bankruptcy prediction},
  volume       = {58},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Artificial intelligence advances in anomaly detection for telecom networks. <em>AIR</em>, <em>58</em>(4), 1-40. (<a href='https://doi.org/10.1007/s10462-025-11108-x'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Telecommunication networks are becoming increasingly dynamic and complex due to the massive amounts of data they process. As a result, detecting abnormal events within these networks is essential for maintaining security and ensuring seamless operation. Traditional methods of anomaly detection, which rely on rule-based systems, are no longer effective in today’s fast-evolving telecom landscape. Thus, making AI useful in addressing these shortcomings. This review critically examines the role of Artificial Intelligence (AI), particularly deep learning, in modern anomaly detection systems for telecom networks. It explores the evolution from early strategies to current AI-driven approaches, discussing the challenges, the implementation of machine learning algorithms, and practical case studies. Additionally, emerging AI technologies such as Generative Adversarial Networks (GANs) and Reinforcement Learning (RL) are highlighted for their potential to enhance anomaly detection. This review provides AI’s transformative impact on telecom anomaly detection, addressing challenges while leveraging 5G/6G, edge computing, and the Internet of Things (IoT). It recommends hybrid models, advanced data preprocessing, and self-adaptive systems to enhance robustness and reliability, enabling telecom operators to proactively manage anomalies and optimize performance in a data driven environment.},
  archive      = {J_AIR},
  author       = {Edozie, Enerst and Shuaibu, Aliyu Nuhu and Sadiq, Bashir Olaniyi and John, Ukagwu Kelechi},
  doi          = {10.1007/s10462-025-11108-x},
  journal      = {Artificial Intelligence Review},
  month        = {4},
  number       = {4},
  pages        = {1-40},
  shortjournal = {Artif. Intell. Rev.},
  title        = {Artificial intelligence advances in anomaly detection for telecom networks},
  volume       = {58},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Artificial intelligence in four-dimensional imaging for motion management in radiation therapy. <em>AIR</em>, <em>58</em>(4), 1-39. (<a href='https://doi.org/10.1007/s10462-025-11109-w'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Four-dimensional imaging (4D-imaging) plays a critical role in achieving precise motion management in radiation therapy. However, challenges remain in 4D-imaging such as a long imaging time, suboptimal image quality, and inaccurate motion estimation. With the tremendous success of artificial intelligence (AI) in the image domain, particularly deep learning, there is great potential in overcoming these challenges and improving the accuracy and efficiency of 4D-imaging without the need for hardware modifications. In this review, we provide a comprehensive overview of how these AI-based methods could drive the evolution of 4D-imaging for motion management. We discuss the inherent issues associated with multiple 4D modalities and explore the current research progress of AI in 4D-imaging. Furthermore, we delve into the unresolved challenges and limitations in 4D-imaging and provide insights into the future direction of this field.},
  archive      = {J_AIR},
  author       = {Yinghui, Wang and Haonan, Xiao and Jing, Wang and Lu, Wang and Wen, Li and Zhuoran, Jiang and Ge, Ren and Shaohua, Zhi and Josh, Qian and Jianrong, Dai and Kuo, Men and Lei, Ren and Xiaofeng, Yang and Tian, Li and Jing, Cai},
  doi          = {10.1007/s10462-025-11109-w},
  journal      = {Artificial Intelligence Review},
  month        = {4},
  number       = {4},
  pages        = {1-39},
  shortjournal = {Artif. Intell. Rev.},
  title        = {Artificial intelligence in four-dimensional imaging for motion management in radiation therapy},
  volume       = {58},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Comprehensive exploration of diffusion models in image generation: A survey. <em>AIR</em>, <em>58</em>(4), 1-49. (<a href='https://doi.org/10.1007/s10462-025-11110-3'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The rapid development of deep learning technology has led to the emergence of diffusion models as a promising generative model with diverse applications. These include image generation, audio and video synthesis, molecular design, and text generation. The distinctive generation mechanism and exceptional generation quality of diffusion models have made them a valuable tool in these diverse fields. However, with the extensive deployment of diffusion models in the domain of image generation, concerns pertaining to data privacy, data security, and artistic ethics have emerged with increasing prominence. Given the accelerated pace of development in the field of diffusion models, the majority of extant surveys are deficient in two respects: firstly, they fail to encompass the latest advances in diffusion-based image synthesis; and secondly, they seldom consider the potential social implications of diffusion models. In order to address these issues, this paper presents a comprehensive survey of the most recent applications of diffusion models in the field of image generation. Furthermore, it provides an in-depth analysis of the potential social impacts that may result from their use. Firstly, this paper presents a systematic survey of the background principles and theoretical foundations of diffusion models. Subsequently, this paper provides a detailed examination of the most recent applications of diffusion models across a range of image generation subfields, including style transfer, image completion, image editing, super-resolution, and beyond. Finally, we present a comprehensive examination of these social issues, addressing data privacy concerns, such as the potential for data leakage and the implementation of protective measures during model training. We also analyse the risk of malicious exploitation of the model and the defensive strategies employed to mitigate such risks. Additionally, we examine the implications of the authenticity and originality of generated images on artistic creativity and copyright protection.},
  archive      = {J_AIR},
  author       = {Chen, Hang and Xiang, Qian and Hu, Jiaxin and Ye, Meilin and Yu, Chao and Cheng, Hao and Zhang, Lei},
  doi          = {10.1007/s10462-025-11110-3},
  journal      = {Artificial Intelligence Review},
  month        = {4},
  number       = {4},
  pages        = {1-49},
  shortjournal = {Artif. Intell. Rev.},
  title        = {Comprehensive exploration of diffusion models in image generation: A survey},
  volume       = {58},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). HFA-net: Hybrid feature-aware network for large-scale point cloud semantic segmentation. <em>AIR</em>, <em>58</em>(4), 1-34. (<a href='https://doi.org/10.1007/s10462-025-11111-2'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Semantic segmentation of large-scale point clouds in 3D computer vision is a challenging problem. Existing feature extraction modules often emphasize learning local geometry while not giving adequate consideration to the integration of color information. This limitation prevents the network from thoroughly learning local features, thereby impacting segmentation accuracy. In this study, we propose three modules for robust feature extraction and aggregation, forming a novel point cloud segmentation network (HFA-Net) for large-scale point cloud semantic segmentation. First, we introduce the Hybrid Feature Extraction Component (HFEC) and the Hybrid Bilateral Enhancement Component (HBAC) to comprehensively extract and enhance the geometric, color, and semantic information of point clouds. Second, we incorporate the Ternary-Distance Attention Pooling (TDAP) module, which leverages trilateral distances to further refine the network’s focus on various features, enabling it to emphasize both locally important features and broader local neighborhoods. These modules are stacked into dense residual components to expand the network’s receptive field. Our experiments on several large-scale benchmark datasets, including Semantic3D, Toronto3D, S3DIS and LASDU demonstrate the effectiveness of HFA-Net when compared to state-of-the-art networks.},
  archive      = {J_AIR},
  author       = {Wen, Changji and Zhang, Long and Ren, Junfeng and Hong, Rundong and Li, Chenshuang and Yang, Ce and Lv, Yanfeng and Chen, Hongbing and yang, Ning},
  doi          = {10.1007/s10462-025-11111-2},
  journal      = {Artificial Intelligence Review},
  month        = {4},
  number       = {4},
  pages        = {1-34},
  shortjournal = {Artif. Intell. Rev.},
  title        = {HFA-net: Hybrid feature-aware network for large-scale point cloud semantic segmentation},
  volume       = {58},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Text analytics for co-creation in public sector organizations: A literature review-based research framework. <em>AIR</em>, <em>58</em>(4), 1-45. (<a href='https://doi.org/10.1007/s10462-025-11112-1'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The public sector faces considerable challenges that stem from increasing external and internal demands, the need for diverse and complex services, and citizens’ lack of satisfaction and trust in public sector organisations (PSOs). An alternative to traditional public service delivery is the co-creation of public services. Data analytics has been fueled by the availability of immense amounts of data, including textual data, and techniques to analyze data, so it has immense potential to foster data-driven solutions for the public sector. In the paper, we systematically review the existing literature on the application of Text Analytics (TA) techniques on textual data that can support public service co-creation. In this review, we identify the TA techniques, the public services and the co-creation phase they support, as well as envisioned public values for the stakeholder groups. On the basis of the analysis, we develop a Research Framework that helps to structure the TA-enabled co-creation process in PSOs, increases awareness among public sector organizations and stakeholders on the significant potential of TA in creating value, and provides scholars with some avenues for further research.},
  archive      = {J_AIR},
  author       = {Rizun, Nina and Revina, Aleksandra and Edelmann, Noella},
  doi          = {10.1007/s10462-025-11112-1},
  journal      = {Artificial Intelligence Review},
  month        = {4},
  number       = {4},
  pages        = {1-45},
  shortjournal = {Artif. Intell. Rev.},
  title        = {Text analytics for co-creation in public sector organizations: A literature review-based research framework},
  volume       = {58},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Review of filtering based feature selection for botnet detection in the internet of things. <em>AIR</em>, <em>58</em>(4), 1-49. (<a href='https://doi.org/10.1007/s10462-025-11113-0'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Botnets are a major security threat in the Internet of Things (IoT), posing significant risks to user privacy, network availability, and the integrity of IoT devices. With the increasing availability of large datasets that contain hundreds or even thousands of variables, selecting the right set of features can be a challenging task. Feature selection is a critical step in developing effective machine learning-based botnet detection systems, as it enables the selection of a subset of features that are most relevant for detection. This paper provides a comprehensive review of filtering based feature selection techniques for botnet detection in IoT. It examines a range of filtering based techniques and evaluates their effectiveness in addressing the challenges and limitations of botnet detection in IoT. It aims to identify the gaps in the literature and areas for future research, and discuss the broader implications of findings for the field of IoT botnet detection. This review provides valuable insights and guidance for researchers and practitioners working on botnet detection in IoT, and highlights the importance of effective feature selection in developing robust and reliable detection systems.},
  archive      = {J_AIR},
  author       = {Saied, Mohamed and Guirguis, Shawkat and Madbouly, Magda},
  doi          = {10.1007/s10462-025-11113-0},
  journal      = {Artificial Intelligence Review},
  month        = {4},
  number       = {4},
  pages        = {1-49},
  shortjournal = {Artif. Intell. Rev.},
  title        = {Review of filtering based feature selection for botnet detection in the internet of things},
  volume       = {58},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Adaptive graph diffusion networks: Compact and expressive GNNs with large receptive fields. <em>AIR</em>, <em>58</em>(4), 1-35. (<a href='https://doi.org/10.1007/s10462-025-11114-z'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Graph neural networks (GNNs) are widely used in graph-based tasks, but deep GNNs often suffer from oversmoothing. Existing effective deep GNNs have various shortcomings including redundant complexity, oversimplified architecture, or predefined parameters. To address these issues, we propose adaptive graph diffusion networks (AGDNs), a class of compact and expressive GNNs that can effectively leverage deep neighborhood information. We introduce hopwise attention and hopwise convolution with positional embeddings for learning nodewise and channelwise hop weights, respectively, which overcomes oversmoothing and ensures a powerful ability to learn arbitrary filters in the spectral domain. Our experiments demonstrate that AGDNs can effectively learn various filters on images and exhibit superior performance on diverse and challenging open graph benchmark datasets for node classification and link prediction tasks while maintaining moderate complexity and fast running time.},
  archive      = {J_AIR},
  author       = {Sun, Chuxiong and Zhang, Muhan and Hu, Jie and Gu, Hongming and Chen, Jinpeng and Yang, Mingchuan},
  doi          = {10.1007/s10462-025-11114-z},
  journal      = {Artificial Intelligence Review},
  month        = {4},
  number       = {4},
  pages        = {1-35},
  shortjournal = {Artif. Intell. Rev.},
  title        = {Adaptive graph diffusion networks: Compact and expressive GNNs with large receptive fields},
  volume       = {58},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Word embedding factor based multi-head attention. <em>AIR</em>, <em>58</em>(4), 1-21. (<a href='https://doi.org/10.1007/s10462-025-11115-y'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The natural language processing (NLP) field has made significant progress using deep learning models based on multi-head attention mechanisms, such as Transformer and BERT. However, there are two major limitations to this approach. First, the number of heads is often manually set based on empirical experience, and second, it is not clear enough in semantic understanding and interpretation. In this study, we propose a novel attention mechanism called Factor Analysis-based Multi-head (FAM) Attention, which combines the theory of explorative factor analysis and word embedding. The experimental results demonstrate that FAM Attention achieves better performance and requires fewer parameters compared to traditional methods while also having better semantic understanding ability and interpretability at the token level. This also has significant implications for current Large Language Models (LLMs), particularly in terms of effectively reducing parameter counts and enhancing performance.},
  archive      = {J_AIR},
  author       = {Li, Zhengren and Zhao, Yumeng and Zhang, Xiaohang and Han, Huawei and Huang, Cui},
  doi          = {10.1007/s10462-025-11115-y},
  journal      = {Artificial Intelligence Review},
  month        = {4},
  number       = {4},
  pages        = {1-21},
  shortjournal = {Artif. Intell. Rev.},
  title        = {Word embedding factor based multi-head attention},
  volume       = {58},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Advances in diffusion models for image data augmentation: A review of methods, models, evaluation metrics and future research directions. <em>AIR</em>, <em>58</em>(4), 1-55. (<a href='https://doi.org/10.1007/s10462-025-11116-x'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Image data augmentation constitutes a critical methodology in modern computer vision tasks, since it can facilitate towards enhancing the diversity and quality of training datasets; thereby, improving the performance and robustness of machine learning models in downstream tasks. In parallel, augmentation approaches can also be used for editing/modifying a given image in a context- and semantics-aware way. Diffusion Models (DMs), which comprise one of the most recent and highly promising classes of methods in the field of generative Artificial Intelligence (AI), have emerged as a powerful tool for image data augmentation, capable of generating realistic and diverse images by learning the underlying data distribution. The current study realizes a systematic, comprehensive and in-depth review of DM-based approaches for image augmentation, covering a wide range of strategies, tasks and applications. In particular, a comprehensive analysis of the fundamental principles, model architectures and training strategies of DMs is initially performed. Subsequently, a taxonomy of the relevant image augmentation methods is introduced, focusing on techniques regarding semantic manipulation, personalization and adaptation, and application-specific augmentation tasks. Then, performance assessment methodologies and respective evaluation metrics are analyzed. Finally, current challenges and future research directions in the field are discussed.},
  archive      = {J_AIR},
  author       = {Alimisis, Panagiotis and Mademlis, Ioannis and Radoglou-Grammatikis, Panagiotis and Sarigiannidis, Panagiotis and Papadopoulos, Georgios Th.},
  doi          = {10.1007/s10462-025-11116-x},
  journal      = {Artificial Intelligence Review},
  month        = {4},
  number       = {4},
  pages        = {1-55},
  shortjournal = {Artif. Intell. Rev.},
  title        = {Advances in diffusion models for image data augmentation: A review of methods, models, evaluation metrics and future research directions},
  volume       = {58},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Advancing cancer diagnosis and treatment: Integrating image analysis and AI algorithms for enhanced clinical practice. <em>AIR</em>, <em>58</em>(4), 1-25. (<a href='https://doi.org/10.1007/s10462-025-11117-w'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cancer screening and diagnosis with the utilization of innovative Artificial Intelligence tools improved the treatment strategies and patients’ survival. With the rapid development of imaging technologies and the rise of artificial intelligence (AI), there is a significant opportunity to improve cancer diagnostics through the combination of image analysis and AI algorithms. This article provides a comprehensive review of studies that have investigated the application of AI-assisted image processing in cancer diagnosis. We searched the Web of Science and Scopus databases to identify relevant studies published between 2014 and January 2024. The search strategy utilized targeted keywords such as cancer diagnostics, image analysis, artificial intelligence, and advanced imaging techniques. We limited the review to articles written in English and using AI-assisted image processing in cancer diagnosis. The results show that by leveraging machine learning algorithms, including deep learning, computer-aided diagnosis systems have been developed that are efficient in detecting tumors, thereby facilitating early cancer detection. Additionally, various authors have explored the integration of personalized treatment approaches and precision medicine, allowing for the development of treatment plans tailored to individual patient characteristics and needs. The review emphasizes the potential of AI-assisted image processing in revolutionizing cancer diagnostics. The insights gained from this study contribute to the current understanding of the field and pave the way for future research and development aimed at advancing cancer diagnostics using image analysis and artificial intelligence.},
  archive      = {J_AIR},
  author       = {Saeidnia, Hamid Reza and Firuzpour, Faezeh and Kozak, Marcin and majd, Hooman Soleymani},
  doi          = {10.1007/s10462-025-11117-w},
  journal      = {Artificial Intelligence Review},
  month        = {4},
  number       = {4},
  pages        = {1-25},
  shortjournal = {Artif. Intell. Rev.},
  title        = {Advancing cancer diagnosis and treatment: Integrating image analysis and AI algorithms for enhanced clinical practice},
  volume       = {58},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Tornado optimizer with coriolis force: A novel bio-inspired meta-heuristic algorithm for solving engineering problems. <em>AIR</em>, <em>58</em>(4), 1-99. (<a href='https://doi.org/10.1007/s10462-025-11118-9'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper proposes a new meta-heuristic algorithm named tornado optimizer with Coriolis force (TOC) which is applied to solve global optimization and constrained engineering problems in continuous search spaces. The fundamental concepts and ideas beyond the proposed TOC Optimizer are inspired by nature based on the observation of the cycle process of tornadoes and how thunderstorms and windstorms evolve into tornadoes using Coriolis force. The Coriolis force is applied to windstorms that directly evolve to form tornadoes based on the developed optimization method. The proposed TOC algorithm mathematically models and implements the behavioral steps of tornado formation by windstorms and thunderstorms and then dissipation of tornadoes on the ground. These steps ultimately lead to feasible solutions when applied to solve optimization problems. These behavioral steps are mathematically represented along with the Coriolis force to allow for a proper balance between exploration and exploitation during the optimization process, as well as to allow search agents to explore and exploit every possible area of the search space. The performance of the proposed TOC optimizer was thoroughly examined on a simple benchmark set of 23 test functions, and a set of 29 well-known benchmark functions from the CEC-2017 test for a variety of dimensions. A comparative study of the computational and convergence analysis results was carried out to clarify the efficacy and stability levels of the proposed TOC optimizer compared to other well-known optimizers. The TOC optimizer outperformed other comparative algorithms using the mean ranks of Friedman’s test by 20.75%, 27.248%, and 25.85% on the 10-, 30-, and 50-dimensional CEC 2017 test set, respectively. The reliability and appropriateness of the TOC optimizer were examined by solving real-world problems including eight engineering design problems and one industrial process. The proposed optimizer divulged satisfactory performance over other competing optimizers regarding solution quality and global optimality as per statistical test methods.},
  archive      = {J_AIR},
  author       = {Braik, Malik and Al-Hiary, Heba and Alzoubi, Hussein and Hammouri, Abdelaziz and Azmi Al-Betar, Mohammed and Awadallah, Mohammed A.},
  doi          = {10.1007/s10462-025-11118-9},
  journal      = {Artificial Intelligence Review},
  month        = {4},
  number       = {4},
  pages        = {1-99},
  shortjournal = {Artif. Intell. Rev.},
  title        = {Tornado optimizer with coriolis force: A novel bio-inspired meta-heuristic algorithm for solving engineering problems},
  volume       = {58},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A soft prompt learning method for medical text classification with simulated human cognitive capabilities. <em>AIR</em>, <em>58</em>(4), 1-30. (<a href='https://doi.org/10.1007/s10462-025-11121-0'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Medical text classification aims to identify the category to which a short medical text belongs. Recent research indicates that the “prompt-based learning” paradigm, especially the soft prompt, can improve the performance in text classification tasks by bridging the gap between pre-training objectives and downstream tasks. However, current research on soft prompt learning neglects the relation between the pseudo tokens and raw sentences when generating embeddings for the template. This study investigates, for the first time, how to simulate human cognitive processes in medical text classification tasks using soft prompt learning based on attention mechanisms. The proposed approach pays more attention to the parts of the raw sentence that are more relevant to the category label when generating embeddings for the pseudo tokens, resembling the reasoning process humans go through during text classification. Experiments conducted on two datasets, KUAKE-QIC and CHIP-CTC, indicate that the F1-macro scores of the proposed approach are 0.8064 and 0.8434, which outperform the benchmark and previous prompt learning approaches. In addition, the corresponding experiments also demonstrate the generalization ability and few-shot learning capability of the proposed method.},
  archive      = {J_AIR},
  author       = {Wang, Yu and Zhou, Luyao and Zhang, Weimin and Zhang, Feifan and Wang, Yuan},
  doi          = {10.1007/s10462-025-11121-0},
  journal      = {Artificial Intelligence Review},
  month        = {4},
  number       = {4},
  pages        = {1-30},
  shortjournal = {Artif. Intell. Rev.},
  title        = {A soft prompt learning method for medical text classification with simulated human cognitive capabilities},
  volume       = {58},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A review of chinese sentiment analysis: Subjects, methods, and trends. <em>AIR</em>, <em>58</em>(3), 1-37. (<a href='https://doi.org/10.1007/s10462-024-10988-9'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Sentiment analysis has emerged as a prominent research domain within the realm of natural language processing, garnering increasing attention and a growing body of literature. While numerous literature reviews have examined sentiment analysis techniques, methods, topics and applications, there remains a gap in the literature concerning thematic trends and research methodologies in sentiment analysis, particularly in the context of Chinese text. This study addresses this gap by presenting a comprehensive survey dedicated to the progression of research subjects, methods and trends in sentiment analysis of Chinese text. Employing a framework that combines keyword co-occurrence analysis with a sophisticated community detection algorithm, this survey offers a novel perspective on the landscape of Chinese sentiment analysis research. By tracing the interplay between research methodologies and emerging topics over the past two decades, our study not only facilitates a comparative analysis of their correlations but also illuminates evolving patterns, identifying significant hotspots and trends over time for Chinese language text analysis. This invaluable insight provides a roadmap for researchers seeking to navigate the intricate terrain of sentiment analysis within the context of Chinese language. Moreover, this paper extends beyond the academic realm, offering practical insights into sentiment analysis methodologies and themes while pinpointing avenues for future exploration, technical limitations, and directions for sentiment analysis of Chinese text.},
  archive      = {J_AIR},
  author       = {Wang, Zhaoxia and Huang, Donghao and Cui, Jingfeng and Zhang, Xinyue and Ho, Seng-Beng and Cambria, Erik},
  doi          = {10.1007/s10462-024-10988-9},
  journal      = {Artificial Intelligence Review},
  month        = {3},
  number       = {3},
  pages        = {1-37},
  shortjournal = {Artif. Intell. Rev.},
  title        = {A review of chinese sentiment analysis: Subjects, methods, and trends},
  volume       = {58},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Beyond one-shot explanations: A systematic literature review of dialogue-based xAI approaches. <em>AIR</em>, <em>58</em>(3), 1-37. (<a href='https://doi.org/10.1007/s10462-024-11007-7'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the last decade, there has been increasing interest in allowing users to understand how the predictions of machine-learned models come about, thus increasing transparency and empowering users to understand and potentially contest those decisions. Dialogue-based approaches, in contrast to traditional one-shot eXplainable Artificial Intelligence (xAI) methods, facilitate interactive, in-depth exploration through multi-turn dialogues, simulating human-like interactions, allowing for a dynamic exchange where users can ask questions and receive tailored, relevant explanations in real-time. This paper reviews the current state of dialogue-based xAI, presenting a systematic review of 1339 publications, narrowed down to 15 based on inclusion criteria. We explore theoretical foundations of the systems, propose key dimensions along which different solutions to dialogue-based xAI differ, and identify key use cases, target audiences, system components, and the types of supported queries and responses. Furthermore, we investigate the current paradigms by which systems are evaluated and highlight their key limitations. Key findings include identifying the main use cases, objectives, and audiences targeted by dialogue-based xAI methods, in addition to an overview of the main types of questions and information needs. Beyond discussing avenues for future work, we present a meta-architecture for these systems from existing literature and outlined prevalent theoretical frameworks.},
  archive      = {J_AIR},
  author       = {Mindlin, Dimitry and Beer, Fabian and Sieger, Leonie Nora and Heindorf, Stefan and Esposito, Elena and Ngonga Ngomo, Axel-Cyrille and Cimiano, Philipp},
  doi          = {10.1007/s10462-024-11007-7},
  journal      = {Artificial Intelligence Review},
  month        = {3},
  number       = {3},
  pages        = {1-37},
  shortjournal = {Artif. Intell. Rev.},
  title        = {Beyond one-shot explanations: A systematic literature review of dialogue-based xAI approaches},
  volume       = {58},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Topic modelling through the bibliometrics lens and its technique. <em>AIR</em>, <em>58</em>(3), 1-41. (<a href='https://doi.org/10.1007/s10462-024-11011-x'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Topic modelling (TM) is a significant natural language processing (NLP) task and is becoming more popular, especially, in the context of literature synthesis and analysis. Despite the growing volume of studies on the use of and versatility of TM, the knowledge of TM development, especially from the perspective of bibliometrics analysis is limited. To this end, this study evaluated TM research using two techniques namely, bibliometrics analysis and TM itself to provide the current status and the pathway for future studies in the TM field. For this purpose, this study used 16,941 documents collected from Scopus database from 2004 to 2023. Results indicate that the publications on TM have increased over the years, however, the citation impact has declined. Furthermore, the scientific production on TM is concentrated in two countries namely, China and the USA. Our findings showed there are several applications of TM that are understudied, for example, TM for image segmentation and classification. This paper highlighted the future research directions, most importantly, calls for increased multidisciplinary research approaches to fully deploy TM algorithms optimally and thus, increase usage in non-computer science subject areas.},
  archive      = {J_AIR},
  author       = {Ogunleye, Bayode and Lancho Barrantes, Barbara S. and Zakariyyah, Kudirat Ibilola},
  doi          = {10.1007/s10462-024-11011-x},
  journal      = {Artificial Intelligence Review},
  month        = {3},
  number       = {3},
  pages        = {1-41},
  shortjournal = {Artif. Intell. Rev.},
  title        = {Topic modelling through the bibliometrics lens and its technique},
  volume       = {58},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Artificial lemming algorithm: A novel bionic meta-heuristic technique for solving real-world engineering optimization problems. <em>AIR</em>, <em>58</em>(3), 1-113. (<a href='https://doi.org/10.1007/s10462-024-11023-7'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The advent of the intelligent information era has witnessed a proliferation of complex optimization problems across various disciplines. Although existing meta-heuristic algorithms have demonstrated efficacy in many scenarios, they still struggle with certain challenges such as premature convergence, insufficient exploration, and lack of robustness in high-dimensional, nonconvex search spaces. These limitations underscore the need for novel optimization techniques that can better balance exploration and exploitation while maintaining computational efficiency. In response to this need, we propose the Artificial Lemming Algorithm (ALA), a bio-inspired metaheuristic that mathematically models four distinct behaviors of lemmings in nature: long-distance migration, digging holes, foraging, and evading predators. Specifically, the long-distance migration and burrow digging behaviors are dedicated to highly exploring the search domain, whereas the foraging and evading predators behaviors provide exploitation during the optimization process. In addition, ALA incorporates an energy-decreasing mechanism that enables dynamic adjustments to the balance between exploration and exploitation, thereby enhancing its ability to evade local optima and converge to global solutions more robustly. To thoroughly verify the effectiveness of the proposed method, ALA is compared with 17 other state-of-the-art meta-heuristic algorithms on the IEEE CEC2017 benchmark test suite and the IEEE CEC2022 benchmark test suite. The experimental results indicate that ALA has reliable comprehensive optimization performance and can achieve superior solution accuracy, convergence speed, and stability in most test cases. For the 29 10-, 30-, 50-, and 100-dimensional CEC2017 functions, ALA obtains the lowest Friedman average ranking values among all competitor methods, which are 1.7241, 2.1034, 2.7241, and 2.9310, respectively, and for the 12 CEC2022 functions, ALA again wins the optimal Friedman average ranking of 2.1667. Finally, to further evaluate its applicability, ALA is implemented to address a series of optimization cases, including constrained engineering design, photovoltaic (PV) model parameter identification, and fractional-order proportional-differential-integral (FOPID) controller gain tuning. Our findings highlight the competitive edge and potential of ALA for real-world engineering applications. The source code of ALA is publicly available at https://github.com/StevenShaw98/Artificial-Lemming-Algorithm .},
  archive      = {J_AIR},
  author       = {Xiao, Yaning and Cui, Hao and Khurma, Ruba Abu and Castillo, Pedro A.},
  doi          = {10.1007/s10462-024-11023-7},
  journal      = {Artificial Intelligence Review},
  month        = {3},
  number       = {3},
  pages        = {1-113},
  shortjournal = {Artif. Intell. Rev.},
  title        = {Artificial lemming algorithm: A novel bionic meta-heuristic technique for solving real-world engineering optimization problems},
  volume       = {58},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Improving real estate investment trusts (REITs) time-series prediction accuracy using machine learning and technical analysis indicators. <em>AIR</em>, <em>58</em>(3), 1-47. (<a href='https://doi.org/10.1007/s10462-024-11037-1'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The primary goal of investors who include Real Estate Investment Trusts (REITs) in their portfolios is to achieve better returns while reducing the overall risk of their investments. REITs are entities responsible for owning and managing real estate properties. To achieve greater returns while reducing risk, it is essential to accurately predict future REIT prices. This study explores the predictive capability of five different machine learning algorithms used to predict REIT prices. These algorithms include Ordinary Least Squares Linear Regression, Support Vector Regression, k-Nearest Neighbours Regression, Extreme Gradient Boosting, and Long/Short-Term Memory Neural Networks. Additionally, historical REIT prices are supplemented with Technical Analysis indicators (TAIs) to aid in price predictions. While TA indicators are commonly used in stock market forecasting, their application in the context of REITs has remained relatively unexplored. The study applied these algorithms to predict future prices for 30 REITs from the United States, United Kingdom, and Australia, along with 30 stocks and 30 bonds. After obtaining our price predictions, we employ a Genetic Algorithm (GA) to optimise weights of a diversified portfolio. Our results reveal several key findings: (i) all machine learning algorithms demonstrated low average and standard deviation values in the error rate distributions, outperforming commonly used statistical benchmarks such as Holt’s Linear Trend Method (HLTM), Trigonometric Box-Cox Autoregressive Time Series (TBATS), and Autoregressive Integrated Moving Average (ARIMA); (ii) incorporating Technical Analysis indicators in the ML algorithms resulted in a significant reduction in prediction errors, up to 60% in some cases; and (iii) a multi-asset portfolio constructed using predictions that incorporated Technical Analysis indicators outperformed a portfolio based solely on predictions derived from past prices. Furthermore, this study employed Shapley Value-based techniques, specifically SHAP and SAGE, to analyse the importance of the features used in the analysis. These techniques provided additional evidence of the value added by Technical Analysis indicators in this context.},
  archive      = {J_AIR},
  author       = {Habbab, Fatim Z. and Kampouridis, Michael and Papastylianou, Tasos},
  doi          = {10.1007/s10462-024-11037-1},
  journal      = {Artificial Intelligence Review},
  month        = {3},
  number       = {3},
  pages        = {1-47},
  shortjournal = {Artif. Intell. Rev.},
  title        = {Improving real estate investment trusts (REITs) time-series prediction accuracy using machine learning and technical analysis indicators},
  volume       = {58},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A systematic review for transformer-based long-term series forecasting. <em>AIR</em>, <em>58</em>(3), 1-29. (<a href='https://doi.org/10.1007/s10462-024-11044-2'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The emergence of deep learning has yielded noteworthy advancements in time series forecasting (TSF). Transformer architectures have witnessed broad utilization and adoption in TSF tasks. Transformers have proven to be the most successful solution to extract the semantic correlations among the elements within a long sequence. Various variants have enabled Transformer architecture to effectively handle long-term time series forecasting (LTSF) tasks. In this article, we first present a comprehensive overview of Transformer architectures and their subsequent enhancements developed to address various LTSF tasks. Then, we summarize the publicly available LTSF datasets and relevant evaluation metrics. Furthermore, we provide valuable insights into the best practices and techniques for effectively training Transformers in the context of time-series analysis. Lastly, we propose potential research directions in this rapidly evolving field.},
  archive      = {J_AIR},
  author       = {Su, Liyilei and Zuo, Xumin and Li, Rui and Wang, Xin and Zhao, Heng and Huang, Bingding},
  doi          = {10.1007/s10462-024-11044-2},
  journal      = {Artificial Intelligence Review},
  month        = {3},
  number       = {3},
  pages        = {1-29},
  shortjournal = {Artif. Intell. Rev.},
  title        = {A systematic review for transformer-based long-term series forecasting},
  volume       = {58},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Tour group prioritization driven by online reviews: Using an improved EDAS-SIR method with credibility. <em>AIR</em>, <em>58</em>(3), 1-36. (<a href='https://doi.org/10.1007/s10462-024-11052-2'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Online reviews have become an important channel for people to share their experiences with tour groups. However, the complexity and uncertainty of the massive amount of online review information make it difficult for tourists to compare tour groups through personal browsing. Most studies usually focus on the semantic understanding of the review text content when extracting attributes, but they ignore the question of whether the review text content is credible. Therefore, based on 21,076 reviews of 6 tour groups from Ctrip.com, this paper proposes an online review-driven method considering credibility to assist tourists in choosing tour groups. Firstly, we use review credibility to measure the importance of words in reviews. Word2Vec and K-means are applied to cluster keywords and obtain attributes. Subsequently, sentiment analysis is conducted using Stanford CoreNLP. This analysis assesses the attitudes of reviewers towards each attribute, forming a nested probabilistic linguistic term set matrix to comprehensively retain the complex and multidimensional information present in the reviews. Finally, the EDAS-SIR method is proposed for ranking and selecting tour groups. Furthermore, comparative analysis with other methods demonstrates the good rationality and reliability of this method, which is applicable to other decision-making problems.},
  archive      = {J_AIR},
  author       = {Xu, Mengting and Wang, Xinxin and Xu, Zeshui},
  doi          = {10.1007/s10462-024-11052-2},
  journal      = {Artificial Intelligence Review},
  month        = {3},
  number       = {3},
  pages        = {1-36},
  shortjournal = {Artif. Intell. Rev.},
  title        = {Tour group prioritization driven by online reviews: Using an improved EDAS-SIR method with credibility},
  volume       = {58},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Modified LSHADE-SPACMA with new mutation strategy and external archive mechanism for numerical optimization and point cloud registration. <em>AIR</em>, <em>58</em>(3), 1-41. (<a href='https://doi.org/10.1007/s10462-024-11053-1'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Numerical optimization and point cloud registration are critical research topics in the field of artificial intelligence. The differential evolution algorithm is an effective approach to address these problems, and LSHADE-SPACMA, the winning algorithm of CEC2017, is a competitive differential evolution variant. However, LSHADE-SPACMA’s local exploitation capability can sometimes be insufficient when handling these challenges. Therefore, in this work, we propose a modified version of LSHADE-SPACMA (mLSHADE-SPACMA) for numerical optimization and point cloud registration. Compared to the original approach, this work presents three main innovations. First, we present a precise elimination and generation mechanism to enhance the algorithm’s local exploitation ability. Second, we introduce a mutation strategy based on a modified semi-parametric adaptive strategy and rank-based selective pressure, which improves the algorithm’s evolutionary direction. Third, we propose an elite-based external archiving mechanism, which ensures the diversity of the external population and can accelerate the algorithm’s convergence progress. Additionally, we utilize the CEC2014 (Dim = 10, 30, 50, 100) and CEC2017 (Dim = 10, 30, 50, 100) test suites for numerical optimization experiments, comparing our approach against: (1) 10 recent CEC winner algorithms, including LSHADE, EBOwithCMAR, jSO, LSHADE-cnEpSin, HSES, LSHADE-RSP, ELSHADE-SPACMA, EA4eig, L-SRTDE, and LSHADE-SPACMA; (2) 4 advanced variants: APSM-jSO, LensOBLDE, ACD-DE, and MIDE. The results of the Wilcoxon signed-rank test and Friedman mean rank test demonstrate that mLSHADE-SPACMA not only outperforms the original LSHADE-SPACMA but also surpasses other high-performance optimizers, except that it is inferior L-SRTDE on CEC2017. Finally, 25 point cloud registration cases from the Fast Global Registration dataset are applied for simulation analysis to demonstrate the potential of the developed mLSHADE-SPACMA technique for solving practical optimization problems. The code is available at https://github.com/ShengweiFu?tab=repositories and https://ww2.mathworks.cn/matlabcentral/fileexchange/my-file-exchange},
  archive      = {J_AIR},
  author       = {Fu, Shengwei and Ma, Chi and Li, Ke and Xie, Cankun and Fan, Qingsong and Huang, Haisong and Xie, Jiangxue and Zhang, Guozhang and Yu, Mingyang},
  doi          = {10.1007/s10462-024-11053-1},
  journal      = {Artificial Intelligence Review},
  month        = {3},
  number       = {3},
  pages        = {1-41},
  shortjournal = {Artif. Intell. Rev.},
  title        = {Modified LSHADE-SPACMA with new mutation strategy and external archive mechanism for numerical optimization and point cloud registration},
  volume       = {58},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Systematic review on neural architecture search. <em>AIR</em>, <em>58</em>(3), 1-38. (<a href='https://doi.org/10.1007/s10462-024-11058-w'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Machine Learning (ML) has revolutionized various fields, enabling the development of intelligent systems capable of solving complex problems. However, the process of manually designing and optimizing ML models is often time-consuming, labor-intensive, and requires specialized expertise. To address these challenges, Automatic Machine Learning (AutoML) has emerged as a promising approach that automates the process of selecting and optimizing ML models. Within the realm of AutoML, Neural Architecture Search (NAS) has emerged as a powerful technique that automates the design of neural network architectures, the core components of ML models. It has recently gained significant attraction due to its capability to discover novel and efficient architectures that surpass human-designed counterparts. This manuscript aims to present a systematic review of the literature on this topic published between 2017 and 2023 to identify, analyze, and classify the different types of algorithms developed for NAS. The methodology follows the guidelines of Systematic Literature Review (SLR) methods. Consequently, this study identified 160 articles that provide a comprehensive overview of the field of NAS, encompassing discussion on current works, their purposes, conclusions, and predictions of the direction of this science branch in its main core pillars: Search Space (SSp), Search Strategy (SSt), and Validation Strategy (VSt). Subsequently, the key milestones and advancements that have shaped the field are highlighted. Moreover, we discuss the challenges and open issues that remain in the field. We envision that NAS will continue to play a pivotal role in the advancement of ML, enabling the development of more intelligent and efficient ML models for a wide range of applications.},
  archive      = {J_AIR},
  author       = {Salmani Pour Avval, Sasan and Eskue, Nathan D. and Groves, Roger M. and Yaghoubi, Vahid},
  doi          = {10.1007/s10462-024-11058-w},
  journal      = {Artificial Intelligence Review},
  month        = {3},
  number       = {3},
  pages        = {1-38},
  shortjournal = {Artif. Intell. Rev.},
  title        = {Systematic review on neural architecture search},
  volume       = {58},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A systematic survey on human pose estimation: Upstream and downstream tasks, approaches, lightweight models, and prospects. <em>AIR</em>, <em>58</em>(3), 1-62. (<a href='https://doi.org/10.1007/s10462-024-11060-2'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, human pose estimation has been widely studied as a branch task of computer vision. Human pose estimation plays an important role in the development of medicine, fitness, virtual reality, and other fields. Early human pose estimation technology used traditional manual modeling methods. Recently, human pose estimation technology has developed rapidly using deep learning. This study not only reviews the basic research of human pose estimation but also summarizes the latest cutting-edge technologies. In addition to systematically summarizing the human pose estimation technology, this article also extends to the upstream and downstream tasks of human pose estimation, which shows the positioning of human pose estimation technology more intuitively. In particular, considering the issues regarding computer resources and challenges concerning model performance faced by human pose estimation, the lightweight human pose estimation models and the transformer-based human pose estimation models are summarized in this paper. In general, this article classifies human pose estimation technology around types of methods, 2D or 3D representation of outputs, the number of people, views, and temporal information. Meanwhile, classic datasets and targeted datasets are mentioned in this paper, as well as metrics applied to these datasets. Finally, we generalize the current challenges and possible development of human pose estimation technology in the future.},
  archive      = {J_AIR},
  author       = {Gao, Zheyan and Chen, Jinyan and Liu, Yuxin and Jin, Yucheng and Tian, Dingxiaofei},
  doi          = {10.1007/s10462-024-11060-2},
  journal      = {Artificial Intelligence Review},
  month        = {3},
  number       = {3},
  pages        = {1-62},
  shortjournal = {Artif. Intell. Rev.},
  title        = {A systematic survey on human pose estimation: Upstream and downstream tasks, approaches, lightweight models, and prospects},
  volume       = {58},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Adversarial regularize graph variational autoencoder based on encoder optimization strategy. <em>AIR</em>, <em>58</em>(3), 1-28. (<a href='https://doi.org/10.1007/s10462-024-11068-8'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Graph variational autoencoders (VAEs) have been widely used to address the representation problem of graph nodes. However, most existing graph VAEs focus on minimizing reconstruction loss and overlook the uncertainty in the latent distribution and the issue of posterior collapse during training. An Adversarial Regularize Graph Variational Autoencoder Based on Encoder Optimization Strategy (MCM-ARVGE) is proposed from the perspective of network structure and loss function. MCM-ARVGE introduces a Multi-dimensional Cloud Generator (MCG) that transforms the traditional encoder, expanding the Gaussian distribution into a Gaussian cloud distribution. Furthermore, MCM-ARVGE employs the idea of adversarial regularization to train the Gaussian cloud distribution, reducing the randomness of the Gaussian cloud distribution. Finally, based on the Gaussian cloud distribution, an effective uncertainty similarity measurement method for cloud distributions is introduced to address the problem of posterior collapse. Experimental results validate the universality and effectiveness of MCM-ARVGE, as it outperforms the baseline model in graph embedding tasks.},
  archive      = {J_AIR},
  author       = {Dai, Jin and Peng, Yanhui and Wang, Guoyin and Hu, Feng},
  doi          = {10.1007/s10462-024-11068-8},
  journal      = {Artificial Intelligence Review},
  month        = {3},
  number       = {3},
  pages        = {1-28},
  shortjournal = {Artif. Intell. Rev.},
  title        = {Adversarial regularize graph variational autoencoder based on encoder optimization strategy},
  volume       = {58},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Enhanced crayfish optimization algorithm with differential evolution’s mutation and crossover strategies for global optimization and engineering applications. <em>AIR</em>, <em>58</em>(3), 1-72. (<a href='https://doi.org/10.1007/s10462-024-11069-7'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Optimization algorithms play a crucial role in solving complex challenges across various fields, including engineering, finance, and data science. This study introduces a novel hybrid optimization algorithm, the Hybrid Crayfish Optimization Algorithm with Differential Evolution (HCOADE), which addresses the limitations of premature convergence and inadequate exploitation in the traditional Crayfish Optimization Algorithm (COA). By integrating COA with Differential Evolution (DE) strategies, HCOADE leverages DE’s mutation and crossover mechanisms to enhance global optimization performance. The COA, inspired by the foraging and social behaviors of crayfish, provides a flexible framework for exploring the solution space, while DE’s robust strategies effectively exploit this space. To evaluate HCOADE’s performance, extensive experiments are conducted using 34 benchmark functions from CEC 2014 and CEC 2017, as well as six engineering design problems. The results are compared with ten leading optimization algorithms, including classical COA, Particle Swarm Optimization (PSO), Grey Wolf Optimizer (GWO), Whale Optimization Algorithm (WOA), Moth-flame Optimization (MFO), Salp Swarm Algorithm (SSA), Reptile Search Algorithm (RSA), Sine Cosine Algorithm (SCA), Constriction Coefficient-Based Particle Swarm Optimization Gravitational Search Algorithm (CPSOGSA), and Biogeography-based Optimization (BBO). The average rankings and results from the Wilcoxon Rank Sum Test provide a comprehensive comparison of HCOADE’s performance, clearly demonstrating its superiority. Furthermore, HCOADE’s performance is assessed on the CEC 2020 and CEC 2022 test suites, further confirming its effectiveness. A comparative analysis against notable winners from the CEC competitions, including LSHADEcnEpSin, LSHADESPACMA, and CMA-ES, using the CEC-2017 test suite, revealed superior results for HCOADE. This study underscores the advantages of integrating DE strategies with COA and offers valuable insights for addressing complex global optimization problems.},
  archive      = {J_AIR},
  author       = {Maiti, Binanda and Biswas, Saptadeep and Ezugwu, Absalom El-Shamir and Bera, Uttam Kumar and Alzahrani, Ahmed Ibrahim and Alblehai, Fahad and Abualigah, Laith},
  doi          = {10.1007/s10462-024-11069-7},
  journal      = {Artificial Intelligence Review},
  month        = {3},
  number       = {3},
  pages        = {1-72},
  shortjournal = {Artif. Intell. Rev.},
  title        = {Enhanced crayfish optimization algorithm with differential evolution’s mutation and crossover strategies for global optimization and engineering applications},
  volume       = {58},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). QSHO: Quantum spotted hyena optimizer for global optimization. <em>AIR</em>, <em>58</em>(3), 1-57. (<a href='https://doi.org/10.1007/s10462-024-11072-y'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Spotted Hyena Optimizer (SHO) is a population-based metaheuristic algorithm inspired by the spotted hyenas’ social behavior, and it has been developed to solve global optimization problems. SHO has shown superior performance over its competitive metaheuristic algorithms in solving benchmark function optimization and engineering design problems. However, it suffers from getting stuck in local optima due to its lack of exploration while solving multi-modal optimization problems. This article proposes an improved SHO, quantum SHO (QSHO), inspired by quantum computing. The QSHO implements a quantum computing mechanism to promote its exploration ability. The novel method is tested on well-known IEEE CEC2013 and IEEE CEC2017 benchmark suits with 30 and 50 dimensions and four real-world engineering optimization problems. The results of QSHO are compared with that of Classical SHO, improved SHO (ISHO), Modified SHO (MSHO), Oppositional SHO with mutation operator (OBL-MO-SHO), SHO with space transformation search (STS-SHO), Quantum Salp Swarm Algorithm (QSSA), and Chimp Optimization Algorithm (ChOA). The results are analyzed using the Wilcoxon Signed Rank Test (WSRT) and Friedman Test. The empirical results show that QSHO statistically outperforms other compared algorithms for benchmark problem suits with 30 and 50 dimensions. According to Friedman Test statistics, the QSHO algorithm ranked first and second in solving CEC2013 30D and 50D, respectively, whereas it ranked first in both solving CEC2017 30D and 50D. In addition, we have assessed the QSHO in four real-world engineering optimization problems, and the QSHO statistically outperforms the competitive algorithms.},
  archive      = {J_AIR},
  author       = {Si, Tapas and Miranda, Péricles B. C. and Nandi, Utpal and Jana, Nanda Dulal and Maulik, Ujjwal and Mallik, Saurav and Shah, Mohd Asif},
  doi          = {10.1007/s10462-024-11072-y},
  journal      = {Artificial Intelligence Review},
  month        = {3},
  number       = {3},
  pages        = {1-57},
  shortjournal = {Artif. Intell. Rev.},
  title        = {QSHO: Quantum spotted hyena optimizer for global optimization},
  volume       = {58},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Approximate homomorphic encryption based privacy-preserving machine learning: A survey. <em>AIR</em>, <em>58</em>(3), 1-49. (<a href='https://doi.org/10.1007/s10462-024-11076-8'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Machine Learning (ML) is rapidly advancing, enabling various applications that improve people’s work and daily lives. However, this technical progress brings privacy concerns, leading to the emergence of Privacy-Preserving Machine Learning (PPML) as a popular research topic. In this work, we investigate the privacy protection topic in ML, and showcase the advantages of Homomorphic Encryption (HE) among different privacy-preserving techniques. Additionally, this work presents an introduction of approximate HE, emphasizing its advantages and providing the detail of some representative schemes. Moreover, we systematically review the related works about approximate HE based PPML schemes from the four technical applications and three advanced applications, along with their application scenarios, models and datasets. Finally, we suggest some potential future directions to guide readers in extending the research of PPML.},
  archive      = {J_AIR},
  author       = {Yuan, Jiangjun and Liu, Weinan and Shi, Jiawen and Li, Qingqing},
  doi          = {10.1007/s10462-024-11076-8},
  journal      = {Artificial Intelligence Review},
  month        = {3},
  number       = {3},
  pages        = {1-49},
  shortjournal = {Artif. Intell. Rev.},
  title        = {Approximate homomorphic encryption based privacy-preserving machine learning: A survey},
  volume       = {58},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Application of deep learning for high-throughput phenotyping of seed: A review. <em>AIR</em>, <em>58</em>(3), 1-34. (<a href='https://doi.org/10.1007/s10462-024-11079-5'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Seed quality is of great importance for agricultural cultivation. High-throughput phenotyping techniques can collect magnificent seed information in a rapid and non-destructive manner. Emerging deep learning technology brings new opportunities for effectively processing massive and diverse data from seeds and evaluating their quality. This article comprehensively reviews the principle of several high-throughput phenotyping techniques for non-destructively collection of seed information. In addition, recent research studies on the application of deep learning-based approaches for seed quality inspection are reviewed and summarized, including variety classification and grading, seed damage detection, components prediction, seed cleanliness, vitality assessment, etc. This review illustrates that the combination of deep learning and high-throughput phenotyping techniques can be a promising tool for collection of various phenotype information of seeds, which can be used for effective evaluation of seed quality in industrial practical applications, such as seed breeding, seed quality inspection and management, and seed selection as a food source.},
  archive      = {J_AIR},
  author       = {Jin, Chen and Zhou, Lei and Pu, Yuanyuan and Zhang, Chu and Qi, Hengnian and Zhao, Yiying},
  doi          = {10.1007/s10462-024-11079-5},
  journal      = {Artificial Intelligence Review},
  month        = {3},
  number       = {3},
  pages        = {1-34},
  shortjournal = {Artif. Intell. Rev.},
  title        = {Application of deep learning for high-throughput phenotyping of seed: A review},
  volume       = {58},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Advancing paleontology: A survey on deep learning methodologies in fossil image analysis. <em>AIR</em>, <em>58</em>(3), 1-63. (<a href='https://doi.org/10.1007/s10462-024-11080-y'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Understanding ancient organisms and their interactions with paleoenvironments through the study of body fossils is a central tenet of paleontology. Advances in digital image capture now allow for efficient and accurate documentation, curation, and interrogation of fossil forms and structures in two and three dimensions, extending from microfossils to larger specimens. Despite these developments, key fossil image processing and analysis tasks, such as segmentation and classification, still require significant user intervention, which can be labor-intensive and subject to human bias. Recent advances in deep learning offer the potential to automate fossil image analysis, improving throughput and limiting operator bias. Despite the emergence of deep learning within paleontology in the last decade, challenges such as the scarcity of diverse, high quality image datasets and the complexity of fossil morphology necessitate further advancement which will be aided by the adoption of concepts from other scientific domains. Here, we comprehensively review state-of-the-art deep learning based methodologies applied to fossil analysis, grouping the studies based on the fossil type and nature of the task. Furthermore, we analyze existing literature to tabulate dataset information, neural network architecture type, and key results, and provide textual summaries. Finally, we discuss novel techniques for fossil data augmentation and fossil image enhancements, which can be combined with advanced neural network architectures, such as diffusion models, generative hybrid networks, transformers, and graph neural networks, to improve body fossil image analysis.},
  archive      = {J_AIR},
  author       = {Yaqoob, Mohammed and Ishaq, Mohammed and Ansari, Mohammed Yusuf and Qaiser, Yemna and Hussain, Rehaan and Rabbani, Harris Sajjad and Garwood, Russell J. and Seers, Thomas D.},
  doi          = {10.1007/s10462-024-11080-y},
  journal      = {Artificial Intelligence Review},
  month        = {3},
  number       = {3},
  pages        = {1-63},
  shortjournal = {Artif. Intell. Rev.},
  title        = {Advancing paleontology: A survey on deep learning methodologies in fossil image analysis},
  volume       = {58},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Systematic review of recent years: Machine learning-based interactive therapy for people suffering from dementia. <em>AIR</em>, <em>58</em>(3), 1-30. (<a href='https://doi.org/10.1007/s10462-024-11084-8'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Medical advances over the last century have significantly extended life expectancy. Today, the world’s population is quite old, and will become even older in the years to come. Diseases that particularly concern the elderly are therefore more frequent, and dementia is one of them. This condition mainly affects the elderly and cannot be cured today. However, people suffering from dementia do require care, and this entails significant costs for our society. Machine learning could be useful in a context where it is difficult to find medical staff and where cost reduction is a priority. In recent years, research has been conducted to find ways of treating dementia with machine learning-based therapies in which the patient can actively participate. In this paper, a systematic literature review of these therapies is conducted: (a) paper metadata is analysed, (b) dataset characteristics are examined, (c) therapy types are compared, (d) suggested architectures are considered, (e) therapy performance is reviewed, (f) usability is discussed, and g) ethical considerations are taken into account. Twenty-three papers were selected in which various types of therapy were suggested for use with cell phones, computers, robots, or virtual reality. The results of the usability tests were very positive, both in terms of cognitive faculties evolution and patient satisfaction.},
  archive      = {J_AIR},
  author       = {Rohrer, Coralie and Ben Souissi, Souhir and Kurpicz-Briki, Mascha},
  doi          = {10.1007/s10462-024-11084-8},
  journal      = {Artificial Intelligence Review},
  month        = {3},
  number       = {3},
  pages        = {1-30},
  shortjournal = {Artif. Intell. Rev.},
  title        = {Systematic review of recent years: Machine learning-based interactive therapy for people suffering from dementia},
  volume       = {58},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Enhancing wireless sensor network lifespan and efficiency through improved cluster head selection using improved squirrel search algorithm. <em>AIR</em>, <em>58</em>(3), 1-32. (<a href='https://doi.org/10.1007/s10462-024-11088-4'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A Wireless Sensor Network (WSN) is a significant technological advancement that might contribute to the industrial revolution. The sensor nodes that are part of WSNs are battery-powered. Energy is the most crucial resource for WSNs since batteries cannot be changed or refilled. Since WSNs are a finite resource, several techniques have been devised and used throughout time to preserve them. To extend the lifespan of WSNs, this study will provide an effective method for Cluster Head (CH) selections. Many researches are employing the Swarm-based optimization algorithm to Select the optimal CH. In this study, the Squirrel Search Algorithm (SSA) is utilized to select the optimal CH Selection in WSN. The general SSA has been modified in this study to address the exact need for CH choice in WSNs. The Improved Squirrel Search Algorithm (I-SSA) integrates a series of enhancements aimed at accelerating convergence and elevating solution quality. Notably, we’ve implemented Adaptive Population Initialization, Dynamic Step Size Control, and a Local Search Algorithm to augment the exploration and exploitation capabilities of the SSA. These enhancements collectively refine the algorithm’s ability to navigate the search space effectively, resulting in more efficient convergence towards optimal solutions. The suggested formulation’s goal function takes into account the CH balance average, factor, sink distance residual energy and intra-cluster distance. The simulations are run under a variety of circumstances. The MATLAB 2021a working setting is utilised for simulation. The proposed code of conduct SSA-C is compared with the existing protocols Grey Wolf Optimization (GWO), SSA, Chernobyl Disaster Optimizer (CDO), Sperm Swarm Optimization (SSO), A Metaheuristic Optimized Cluster head selection-based Routing Algorithm for WSNs (MOCRAW), Energy-Efficient Weighted Clustering (EEWC), and Multi-agent pathfinding using Ant Colony Optimization (MAP-ACO). The ISSA-C method achieved a Packet Delivery Ratio (PDR) of 88%, outperforming GWO, SSA, and MAP-ACO. It reduced energy consumption to 210 mJ, which is lower than other methods, and showed improved bit error rates. Cluster formation and head selection times were also reduced to 82 s and 67 s, respectively.},
  archive      = {J_AIR},
  author       = {Alshammri, Ghalib H.},
  doi          = {10.1007/s10462-024-11088-4},
  journal      = {Artificial Intelligence Review},
  month        = {3},
  number       = {3},
  pages        = {1-32},
  shortjournal = {Artif. Intell. Rev.},
  title        = {Enhancing wireless sensor network lifespan and efficiency through improved cluster head selection using improved squirrel search algorithm},
  volume       = {58},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). AI evaluation of ChatGPT and human generated image/textual contents by bipolar generalized fuzzy hypergraph. <em>AIR</em>, <em>58</em>(3), 1-56. (<a href='https://doi.org/10.1007/s10462-024-11015-7'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Artificial Intelligence (AI) tools, i.e., ChatGPT (Chat Generative Pre-Trained Transformer), are positively and negatively revolutionizing the culture of industries, science, and education. The main objectives of this study are to address uncertainty and vagueness in ChatGPT systems, apply bipolarity as two-sided states of data, model generalized graph-based network with derivations, develop bipolar multi-dimensional fuzzy relation, advance entropy metrics for quantifying ambiguity, cluster entities based on level cuts, present pattern recognition in terms of statistical correlation coefficient, analyze speech recognition framework, and schedule online surgeries on the basis of blockchain technology. The outlined innovation pinpoints on the self-evaluation of ChatGPT systems, merging the bipolarity and generalized fuzzy hypergraph approach, developing the interpretation of graph-based patterns, and benchmarking the AI analysis and metrics advancement. To assess the efficiency of AI bipolar generalized fuzzy hypergraph (BGFH) model, the key conceptual benchmarks are clustering technique for detecting patterns and similar groups of data, statistical methods for the analysis of pattern recognition, and entropy metrics for quantifying the fuzziness within a system. This layout furnishes important characteristics such as union, intersection, complement, homomorphism, isomorphism, verifying the overlapping (intersection) and complement of two strong BGFHs as a strong BGFH. In addition, certain specifications of reflexive, symmetric, transitive, overlapping and integration, are defined using bipolar multi-dimensional fuzzy relation. Eleven classes are derived based on different values within $$t\in [0,1]$$ and $$s\in [-1,0],$$ classifying analogous data that aids the similarity detection of generated outputs. Through this approach, a new pattern recognition is used as a data evaluation technique to intelligently facilitate the process in terms of correlation coefficient. It is revealed that the highest magnitude of 0.145 is adopted for patterns $$C_{1}$$ and D, indicating the most positive correlation between patterns, while patterns $$C_{4}$$ and D with the value of $$-0.35$$ are negatively correlated. The results verify that the entropy measure of visual data (0.75) is higher than the entropy measure of textual data with the value of 0.68, indicating more vagueness and ambiguity in visual generated systems. The corresponding textual data $$E^{P}(U)$$ and $$E^{N}(U)$$ are, respectively, calculated as 0.62 and 0.45 for human-created contents and ChatGPT-generated contents, whilst for visual data, the entropy measures $$E^{P}(U)$$ and $$E^{N}(U)$$ are, respectively, 0.25 and 0.66, showing the higher values for the entropy measure of ChatGPT-generated visual data compared to the ChatGPT-generated textual data. In relation to the speech recognition analysis, the highest human performance degree is affiliated to word “a” (0.89), while the lowest degree belongs to word “i” (0.81). The highest AI performance degree is allocated to word “it” $$(-0.7),$$ and the lowest degree is affiliated to word “the” $$(-0.88).$$ The overall entropy measure is calculated by 0.23, and the entropy measure of AI-based data is 0.35, on the other hand, the entropy measure of human-based data is equal to 0.29, representing higher vagueness for AI-based data. According to the obtained results in surgical case scheduling, the bipolar value of $$(0.9,-0.1)$$ is allocated to the surgeon who has the highest positive performance (0.9) and the lowest negative performance $$(-0.1);$$ this indicates the superior overall performance $$(\lambda _{a}=0.8)$$ of the leader during the AI blockchain robotic colon surgery. The worst overall performance (0.22) is allotted to the surgeon, who is required to be removed from the surgery team by the leader physician. The outcomes are validated by a comparative analysis with respect to the classical bipolar fuzzy graph and bipolar fuzzy hypergraph, and NLP (natural language processing) approaches.},
  archive      = {J_AIR},
  author       = {Amini, Abbas and Firouzkouhi, Narjes and Farag, Wael and Ali, Omar and Zabalawi, Isam and Davvaz, Bijan},
  doi          = {10.1007/s10462-024-11015-7},
  journal      = {Artificial Intelligence Review},
  month        = {3},
  number       = {3},
  pages        = {1-56},
  shortjournal = {Artif. Intell. Rev.},
  title        = {AI evaluation of ChatGPT and human generated image/textual contents by bipolar generalized fuzzy hypergraph},
  volume       = {58},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A comprehensive survey on impact of applying various technologies on the internet of medical things. <em>AIR</em>, <em>58</em>(3), 1-71. (<a href='https://doi.org/10.1007/s10462-024-11063-z'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper explores the transformative impact of the Internet of Medical Things (IoMT) on healthcare. By integrating medical equipment and sensors with the internet, IoMT enables real-time monitoring of patient health, remote patient care, and individualized treatment plans. IoMT significantly improves several healthcare domains, including managing chronic diseases, patient safety, and drug adherence, resulting in better patient outcomes and reduced expenses. Technologies like blockchain, Artificial Intelligence (AI), and cloud computing further boost IoMT’s capabilities in healthcare. Blockchain enhances data security and interoperability, AI analyzes massive volumes of health data to find patterns and make predictions, and cloud computing offers scalable and cost-effective data processing and storage. Therefore, this paper provides a comprehensive review of the Internet of Things (IoT) and IoMT-based edge-intelligent smart healthcare, focusing on publications published between 2018 and 2024. The review addresses numerous studies on IoT, IoMT, AI, edge and cloud computing, security, Deep Learning, and blockchain. The obstacles facing IoMT are also covered in this paper, including interoperability issues, regulatory compliance, and privacy and data security concerns. Finally, recommendations for further studies are provided.},
  archive      = {J_AIR},
  author       = {El-deep, Shorouk E. and Abohany, Amr A. and Sallam, Karam M. and El-Mageed, Amr A. Abd},
  doi          = {10.1007/s10462-024-11063-z},
  journal      = {Artificial Intelligence Review},
  month        = {3},
  number       = {3},
  pages        = {1-71},
  shortjournal = {Artif. Intell. Rev.},
  title        = {A comprehensive survey on impact of applying various technologies on the internet of medical things},
  volume       = {58},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). AncientGlyphNet: An advanced deep learning framework for detecting ancient chinese characters in complex scene. <em>AIR</em>, <em>58</em>(3), 1-30. (<a href='https://doi.org/10.1007/s10462-024-11095-5'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Detecting ancient Chinese characters in various media, including stone inscriptions, calligraphy, and couplets, is challenging due to the complex backgrounds and diverse styles. This study proposes an advanced deep-learning framework for detecting ancient Chinese characters in complex scenes to improve detection accuracy. First, the framework introduces an Ancient Character Haar Wavelet Transform downsampling block (ACHaar), effectively reducing feature maps’ spatial resolution while preserving key ancient character features. Second, a Glyph Focus Module (GFM) is introduced, utilizing attention mechanisms to enhance the processing of deep semantic information and generating ancient character feature maps that emphasize horizontal and vertical features through a four-path parallel strategy. Third, a Character Contour Refinement Layer (CCRL) is incorporated to sharpen the edges of characters. Additionally, to train and validate the model, a dedicated dataset was constructed, named Huzhou University-Ancient Chinese Character Dataset for Complex Scenes (HUSAM-SinoCDCS), comprising images of stone inscriptions, calligraphy, and couplets. Experimental results demonstrated that the proposed method outperforms previous text detection methods on the HUSAM-SinoCDCS dataset, with accuracy improved by 1.36–92.84%, recall improved by 2.24–85.61%, and F1 score improved by 1.84–89.08%. This research contributes to digitizing ancient Chinese character artifacts and literature, promoting the inheritance and dissemination of traditional Chinese character culture. The source code and the HUSAM-SinoCDCS dataset can be accessed at https://github.com/youngbbi/AncientGlyphNet and https://github.com/youngbbi/HUSAM-SinoCDCS .},
  archive      = {J_AIR},
  author       = {Qi, Hengnian and Yang, Hao and Wang, Zhaojiang and Ye, Jiabin and Xin, Qiuyi and Zhang, Chu and Lang, Qing},
  doi          = {10.1007/s10462-024-11095-5},
  journal      = {Artificial Intelligence Review},
  month        = {3},
  number       = {3},
  pages        = {1-30},
  shortjournal = {Artif. Intell. Rev.},
  title        = {AncientGlyphNet: An advanced deep learning framework for detecting ancient chinese characters in complex scene},
  volume       = {58},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Efficient maximum iterations for swarm intelligence algorithms: A comparative study. <em>AIR</em>, <em>58</em>(3), 1-49. (<a href='https://doi.org/10.1007/s10462-024-11104-7'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A swarm intelligence algorithm usually iterates many times to approximate the optimum to obtain the solution of a problem. The maximum iteration is influenced by many factors such as the algorithm itself, problem types, as well as dimensions and search space sizes of decision variables. There are few existing studies on efficient maximum iterations, especially a large-scale study on comparison for different problem types. By dividing three CEC benchmark sets into several problem types, this study made a large-scale performance comparison of 123 common swarm intelligence algorithms from several views. The experimental results show that for low-dimensionality, wide search space, and/or simple- and medium-complex problems, about a quarter of the algorithms are concentrated in iterations of about 30 ~ 80, while most algorithms for other types of problems tend to have as many iterations as possible. By and large, for the Classical set, large iterations are beneficial for improving the performance of most algorithms, while less than half of the algorithms for CEC 2019 and CEC 2022 do so. And, the efficient iterations of excellent algorithms are about 300 on low dimensionality, wide search space and simple-complexity problems, while other types are as large as possible. In terms of algorithm speed, LSO, DE and RSA are the fastest on all the three benchmark sets, and the runtime of all algorithms is almost linearly related to the maximum iterations. Although the conclusions largely depend on the problem types, we believe that an efficient iteration is necessary to optimize algorithm performance.},
  archive      = {J_AIR},
  author       = {Si-Ma, Shen and Liu, Han-Ming and Zhan, Hong-Xiang and Liu, Zhao-Fa and Guo, Gang and Yu, Cong and Hu, Peng-Cheng},
  doi          = {10.1007/s10462-024-11104-7},
  journal      = {Artificial Intelligence Review},
  month        = {3},
  number       = {3},
  pages        = {1-49},
  shortjournal = {Artif. Intell. Rev.},
  title        = {Efficient maximum iterations for swarm intelligence algorithms: A comparative study},
  volume       = {58},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Edge deep learning in computer vision and medical diagnostics: A comprehensive survey. <em>AIR</em>, <em>58</em>(3), 1-78. (<a href='https://doi.org/10.1007/s10462-024-11033-5'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Edge deep learning, a paradigm change reconciling edge computing and deep learning, facilitates real-time decision making attuned to environmental factors through the close integration of computational resources and data sources. Here we provide a comprehensive review of the current state of the art in edge deep learning, focusing on computer vision applications, in particular medical diagnostics. An overview of the foundational principles and technical advantages of edge deep learning is presented, emphasising the capacity of this technology to revolutionise a wide range of domains. Furthermore, we present a novel categorisation of edge hardware platforms based on performance and usage scenarios, facilitating platform selection and operational effectiveness. Following this, we dive into approaches to effectively implement deep neural networks on edge devices, encompassing methods such as lightweight design and model compression. Reviewing practical applications in the fields of computer vision in general and medical diagnostics in particular, we demonstrate the profound impact edge-deployed deep learning models can have in real-life situations. Finally, we provide an analysis of potential future directions and obstacles to the adoption of edge deep learning, with the intention to stimulate further investigations and advancements of intelligent edge deep learning solutions. This survey provides researchers and practitioners with a comprehensive reference shedding light on the critical role deep learning plays in the advancement of edge computing applications.},
  archive      = {J_AIR},
  author       = {Xu, Yiwen and Khan, Tariq M. and Song, Yang and Meijering, Erik},
  doi          = {10.1007/s10462-024-11033-5},
  journal      = {Artificial Intelligence Review},
  month        = {3},
  number       = {3},
  pages        = {1-78},
  shortjournal = {Artif. Intell. Rev.},
  title        = {Edge deep learning in computer vision and medical diagnostics: A comprehensive survey},
  volume       = {58},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A taxonomy of literature reviews and experimental study of deepreinforcement learning in portfolio management. <em>AIR</em>, <em>58</em>(3), 1-46. (<a href='https://doi.org/10.1007/s10462-024-11066-w'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Portfolio management involves choosing and actively overseeing various investment assets to meet an investor’s long-term financial goals, considering their risk tolerance and desired return potential. Traditional methods, like mean–variance analysis, often lack the flexibility needed to navigate the complexities of today’s financial markets. Recently, Deep Reinforcement Learning (DRL) has emerged as a promising approach, enabling continuous adjustments to investment strategies based on market feedback without explicit price predictions. This paper presents a comprehensive literature review of DRL applications in portfolio management, aimed at finance researchers, data scientists, AI experts, FinTech engineers, and students seeking advanced portfolio optimization methodologies. We also conducted an experimental study to evaluate five DRL algorithms—Advantage Actor-Critic (A2C), Deep Deterministic Policy Gradient (DDPG), Proximal Policy Optimization (PPO), Soft Actor-Critic (SAC), and Twin Delayed DDPG (TD3)—in managing a portfolio of 30 Dow Jones Industrial Average (DJIA) stocks. Their performance is compared with the DJIA index and traditional strategies, demonstrating DRL’s potential to improve portfolio outcomes while effectively managing risk.},
  archive      = {J_AIR},
  author       = {Rezaei, Mohadese and Nezamabadi-Pour, Hossein},
  doi          = {10.1007/s10462-024-11066-w},
  journal      = {Artificial Intelligence Review},
  month        = {3},
  number       = {3},
  pages        = {1-46},
  shortjournal = {Artif. Intell. Rev.},
  title        = {A taxonomy of literature reviews and experimental study of deepreinforcement learning in portfolio management},
  volume       = {58},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Textual variations in social media text processing applications: Challenges, solutions, and trends. <em>AIR</em>, <em>58</em>(3), 1-48. (<a href='https://doi.org/10.1007/s10462-024-11071-z'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Being an informal communication source, social media text is susceptible to several intentional and unintentional textual variations. These variations lead to various out-of-vocabulary (OOV) words, making social media text processing more challenging. This work analyses and discusses such challenges by providing a detailed overview of different sources of intentional and unintentional OOV words and associated challenges. We provide a detailed survey of pre-processing techniques, including traditional and application-specific methods proposed in the literature to handle intentional and unintentional textual variations, while highlighting their pros and cons. The paper analyses the implications of text normalization (standardization) in different social media text-processing applications. Moreover, the paper provides an overview of the recent challenges and trends in handling social media textual variations, and it is expected to provide a baseline for future research.},
  archive      = {J_AIR},
  author       = {Khan, Jebran and Ahmad, Kashif and Jagatheesaperumal, Senthil Kumar and Sohn, Kyung-Ah},
  doi          = {10.1007/s10462-024-11071-z},
  journal      = {Artificial Intelligence Review},
  month        = {3},
  number       = {3},
  pages        = {1-48},
  shortjournal = {Artif. Intell. Rev.},
  title        = {Textual variations in social media text processing applications: Challenges, solutions, and trends},
  volume       = {58},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Digital forgetting in large language models: A survey of unlearning methods. <em>AIR</em>, <em>58</em>(3), 1-41. (<a href='https://doi.org/10.1007/s10462-024-11078-6'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Large language models (LLMs) have become the state of the art in natural language processing. The massive adoption of generative LLMs and the capabilities they have shown have prompted public concerns regarding their impact on the labor market, privacy, the use of copyrighted work, and how these models align with human ethics and the rule of law. As a response, new regulations are being pushed, which require developers and service providers to evaluate, monitor, and forestall or at least mitigate the risks posed by their models. One mitigation strategy is digital forgetting: given a model with undesirable knowledge or behavior, the goal is to obtain a new model where the detected issues are no longer present. Digital forgetting is usually enforced via machine unlearning techniques, which modify trained machine learning models for them to behave as models trained on a subset of the original training data. In this work, we describe the motivations and desirable properties of digital forgetting when applied to LLMs, and we survey recent works on machine unlearning. Specifically, we propose a taxonomy of unlearning methods based on the reach and depth of the modifications done on the models, we discuss and compare the effectiveness of machine unlearning methods for LLMs proposed so far, and we survey their evaluation. Finally, we describe open problems of machine unlearning applied to LLMs and we put forward recommendations for developers and practitioners.},
  archive      = {J_AIR},
  author       = {Blanco-Justicia, Alberto and Jebreel, Najeeb and Manzanares-Salor, Benet and Sánchez, David and Domingo-Ferrer, Josep and Collell, Guillem and Eeik Tan, Kuan},
  doi          = {10.1007/s10462-024-11078-6},
  journal      = {Artificial Intelligence Review},
  month        = {3},
  number       = {3},
  pages        = {1-41},
  shortjournal = {Artif. Intell. Rev.},
  title        = {Digital forgetting in large language models: A survey of unlearning methods},
  volume       = {58},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Advancements in securing federated learning with IDS: A comprehensive review of neural networks and feature engineering techniques for malicious client detection. <em>AIR</em>, <em>58</em>(3), 1-63. (<a href='https://doi.org/10.1007/s10462-024-11082-w'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Federated Learning (FL) is a technique that can learn a global machine-learning model at a central server by aggregating locally trained models. This distributed machine-learning approach preserves the privacy of local models. However, FL systems are inherently vulnerable to significant security challenges such as cyber-attacks, handling non-independent and identically distributed (non-IID) data, and data privacy concerns. This systematic literature review addresses these issues by examining advanced neural network models, feature engineering methods, and privacy-preserving techniques within intrusion detection systems (IDS) for FL environments. These are key elements for improving the security of FL systems. To the best of our knowledge, this review is among the first to comprehensively explore the combined impacts of these technologies. We analyzed 88 studies published between 2021 and October 2024. This study offers valuable insights for future research directions, including scaling FL in a real-world environment.},
  archive      = {J_AIR},
  author       = {Latif, Naila and Ma, Wenping and Ahmad, Hafiz Bilal},
  doi          = {10.1007/s10462-024-11082-w},
  journal      = {Artificial Intelligence Review},
  month        = {3},
  number       = {3},
  pages        = {1-63},
  shortjournal = {Artif. Intell. Rev.},
  title        = {Advancements in securing federated learning with IDS: A comprehensive review of neural networks and feature engineering techniques for malicious client detection},
  volume       = {58},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Deep learning and computer vision in plant disease detection: A comprehensive review of techniques, models, and trends in precision agriculture. <em>AIR</em>, <em>58</em>(3), 1-64. (<a href='https://doi.org/10.1007/s10462-024-11100-x'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Plant diseases cause significant damage to agriculture, leading to substantial yield losses and posing a major threat to food security. Detection, identification, quantification, and diagnosis of plant diseases are crucial parts of precision agriculture and crop protection. Modernizing agriculture and improving production efficiency are significantly affected by using computer vision technology for crop disease diagnosis. This technology is notable for its non-destructive nature, speed, real-time responsiveness, and precision. Deep learning (DL), a recent breakthrough in computer vision, has become a focal point in agricultural plant protection that can minimize the biases of manually selecting disease spot features. This study reviews the techniques and tools used for automatic disease identification, state-of-the-art DL models, and recent trends in DL-based image analysis. The techniques, performance, benefits, drawbacks, underlying frameworks, and reference datasets of more than 278 research articles were analyzed and subsequently highlighted in accordance with the architecture of computer vision and deep learning models. Key findings include the effectiveness of imaging techniques and sensors like RGB, multispectral, and hyperspectral cameras for early disease detection. Researchers also evaluated various DL architectures, such as convolutional neural networks, vision transformers, generative adversarial networks, vision language models, and foundation models. Moreover, the study connects academic research with practical agricultural applications, providing guidance on the suitability of these models for production environments. This comprehensive review offers valuable insights into the current state and future directions of deep learning in plant disease detection, making it a significant resource for researchers, academicians, and practitioners in precision agriculture.},
  archive      = {J_AIR},
  author       = {Upadhyay, Abhishek and Chandel, Narendra Singh and Singh, Krishna Pratap and Chakraborty, Subir Kumar and Nandede, Balaji M. and Kumar, Mohit and Subeesh, A. and Upendar, Konga and Salem, Ali and Elbeltagi, Ahmed},
  doi          = {10.1007/s10462-024-11100-x},
  journal      = {Artificial Intelligence Review},
  month        = {3},
  number       = {3},
  pages        = {1-64},
  shortjournal = {Artif. Intell. Rev.},
  title        = {Deep learning and computer vision in plant disease detection: A comprehensive review of techniques, models, and trends in precision agriculture},
  volume       = {58},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Graph pooling for graph-level representation learning: A survey. <em>AIR</em>, <em>58</em>(2), 1-50. (<a href='https://doi.org/10.1007/s10462-024-10949-2'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In graph-level representation learning tasks, graph neural networks have received much attention for their powerful feature learning capabilities. However, with the increasing scales of graph data, how to efficiently process and extract the key information has become the focus of research. The graph pooling technique, as a key step in graph neural networks, simplifies the graph structure by merging nodes or subgraphs, which significantly improves the computational efficiency and feature extraction ability of graph neural networks. Although various graph pooling methods have been proposed by numerous scholars, there is still a relative lack of systematic summaries of these works. In this paper, we comprehensively sort out the fundamentals and recent progress of graph pooling techniques in graph neural networks and discuss its wide range of application scenarios, as well as the current challenges and opportunities, which point out the direction for future research. Specifically, we first provide a detailed introduction to the basics of graph pooling, including its definition, principles, and its function in graph neural networks. Then, we categorize and summarize the research preliminaries of graph pooling, including various graph pooling methods proposed in recent years. Next, we explore the potential of graph pooling for a wide range of applications, which provides insightful insights for the promotion and practice of graph pooling technology in more fields. Furthermore, we conduct a comparative analysis of various graph pooling methods and evaluate their performance on a benchmark dataset, providing a comprehensive understanding of their strengths and weaknesses. Finally, we systematically analyze the challenges and opportunities of the current graph pooling methods and provide a prospective outlook on future research directions.},
  archive      = {J_AIR},
  author       = {Li, Zhi-Peng and Wang, Si-Guo and Zhang, Qin-Hu and Pan, Yi-Jie and Xiao, Nai-An and Guo, Jia-Yang and Yuan, Chang-An and Liu, Wen-Jian and Huang, De-Shuang},
  doi          = {10.1007/s10462-024-10949-2},
  journal      = {Artificial Intelligence Review},
  month        = {2},
  number       = {2},
  pages        = {1-50},
  shortjournal = {Artif. Intell. Rev.},
  title        = {Graph pooling for graph-level representation learning: A survey},
  volume       = {58},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Graph reduction techniques for instance selection: Comparative and empirical study. <em>AIR</em>, <em>58</em>(2), 1-42. (<a href='https://doi.org/10.1007/s10462-024-10971-4'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The surge in data generation has prompted a shift to big data, challenging the notion that “more data equals better performance” due to processing and time constraints. In this evolving artificial intelligence and machine learning landscape, instance selection (IS) has become crucial for data reduction without compromising model quality. Traditional IS methods, though efficient, often struggle with large, complex datasets in data mining. This study evaluates graph reduction techniques, grounded in graph theory, as a novel approach for instance selection. The objective is to leverage the inherent structures of data represented as graphs to enhance the effectiveness of instance selection. We evaluated 35 graph reduction techniques across 29 classification datasets. These techniques were assessed based on various metrics, including accuracy, F1 score, reduction rate, and computational times. Graph reduction methods showed significant potential in maintaining data integrity while achieving substantial reductions. Top techniques achieved up to 99% reduction while maintaining or improving accuracy. For instance, the Multilevel sampling achieved an accuracy effectiveness score of 0.8555 with 99.16% reduction on large datasets, while Leiden sampling showed high effectiveness on smaller datasets (0.8034 accuracy, 97.87% reduction). Computational efficiency varied widely, with reduction times ranging from milliseconds to minutes. This research advances the theory of graph-based instance selection and offers practical application guidelines. Our findings indicate graph reduction methods effectively preserve data quality and boost processing efficiency in large, complex datasets, with some techniques achieving up to 160-fold speedups in model training at high reduction rates.},
  archive      = {J_AIR},
  author       = {Rustamov, Zahiriddin and Zaki, Nazar and Rustamov, Jaloliddin and Zaitouny, Ayham and Damseh, Rafat},
  doi          = {10.1007/s10462-024-10971-4},
  journal      = {Artificial Intelligence Review},
  month        = {2},
  number       = {2},
  pages        = {1-42},
  shortjournal = {Artif. Intell. Rev.},
  title        = {Graph reduction techniques for instance selection: Comparative and empirical study},
  volume       = {58},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Survey: Application and analysis of generative adversarial networks in medical images. <em>AIR</em>, <em>58</em>(2), 1-81. (<a href='https://doi.org/10.1007/s10462-024-10992-z'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Generative Adversarial Networks (GANs) have shown promising prospects and achieved significant results in medical image analysis tasks. This article provides a comprehensive review of recent research on GANs and their variants in medical applications, including tasks such as image synthesis, segmentation, classification, detection, denoising, reconstruction, fusion, registration, and prediction. We summarize and analyze the reviewed literature, with a focus on model framework design,dataset sources, and performance evaluation. Our research findings are presented in the form of tables. In the end,article discusses open challenges and directions for future research.},
  archive      = {J_AIR},
  author       = {Heng, Yang and Yinghua, Ma and Khan, Fiaz Gul and Khan, Ahmad and Ali, Farman and AlZubi, Ahmad Ali and Hui, Zeng},
  doi          = {10.1007/s10462-024-10992-z},
  journal      = {Artificial Intelligence Review},
  month        = {2},
  number       = {2},
  pages        = {1-81},
  shortjournal = {Artif. Intell. Rev.},
  title        = {Survey: Application and analysis of generative adversarial networks in medical images},
  volume       = {58},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Mitigating content poisoning attacks in named data networking: A survey of recent solutions, limitations, challenges and future research directions. <em>AIR</em>, <em>58</em>(2), 1-41. (<a href='https://doi.org/10.1007/s10462-024-10994-x'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Named Data Networking (NDN) is one of the capable applicants for the future Internet architecture, where communications focus on content rather than providing content. NDN implements Information-Centric Networking (ICN) with its unique node structure and significant characteristics such as built-in mobility support, multicast support, and efficient content distribution to end-users. It has several key features, including inherent security, that protect the content rather than the communication channel. Despite the good features that NDN provides, it is nonetheless vulnerable to a variety of attacks, the most critical of them is the Content Poisoning Attack (CPA). In this survey, the existing solutions presented for the prevention of CPA in the NDN paradigm have been critically analyzed. Furthermore, we also compared the suggested schemes based on latency, communication overhead, and security. In addition, we have also shown the possibility of other possible NDN attacks on the suggested schemes. Finally, we adds some open research challanges.},
  archive      = {J_AIR},
  author       = {Ullah, Syed Sajid and Hussain, Saddam and Ali, Ihsan and Khattak, Hizbullah and Mastorakis, Spyridon},
  doi          = {10.1007/s10462-024-10994-x},
  journal      = {Artificial Intelligence Review},
  month        = {2},
  number       = {2},
  pages        = {1-41},
  shortjournal = {Artif. Intell. Rev.},
  title        = {Mitigating content poisoning attacks in named data networking: A survey of recent solutions, limitations, challenges and future research directions},
  volume       = {58},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Enhancing industrial robot selection through a hybrid novel approach: Integrating CRITIC-VIKOR method with probabilistic uncertain linguistic q-rung orthopair fuzzy. <em>AIR</em>, <em>58</em>(2), 1-38. (<a href='https://doi.org/10.1007/s10462-024-11001-z'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The increasing complexity and novel features of industrial robots have made their selection for specific applications a challenging task. Decision-makers are faced with the daunting task of navigating through various attributes and specifications, often under conditions of ambiguity and uncertainty. To assist in this complex decision-making process, this paper introduces a novel decision-making framework based on probabilistic uncertain linguistic q-rung orthopair fuzzy sets (PULq-ROFS). This framework effectively combines the strengths of probabilistic uncertain linguistic term set (PULTS) and q-rung orthopair fuzzy set (q-ROFS) to provide a more robust approach for handling ambiguity and uncertainty in the robot selection process. The proposed methodology integrates a multi-attribute group decision-making (MAGDM) approach. This approach utilizes the VIseKriterijumska Optimizacija I KOmpromisno Resenje (VIKOR) method in conjunction with the criterion importance via inter-criteria correlation (CRITIC) method. The CRITIC method determines attribute weights by analyzing both the differences and contrast intensity of criteria, thereby accounting for the relative strength and conflict among them. VIKOR is then employed to aggregate individual regret and group utility, resulting in a compromise solution that guides decision-makers toward the optimal robot selection. Proposed method provide the clarity and confidence to decision makers for choice of attributes and crediting this enhancement to the framework’s transparency and its ability to incorporate a wide range of stakeholder perspectives. This framework facilitates a more inclusive decision-making process that acknowledges differing viewpoints and preferences. This proposed approach not only directs users toward optimal selections but also encourages collaboration among decision-makers, promoting a sense of shared ownership and responsibility in the selection process. This paper select the best robot by utilizing the benefits of both CRITIC and VIKOR method. The effectiveness of this integrated approach is validated through parameter and comparative analysis. The results demonstrate the potential applicability of the proposed methodology in real-world industrial robot selection scenarios, providing decision-makers with a powerful tool to navigate the complexities of modern robotic systems.},
  archive      = {J_AIR},
  author       = {Naz, Sumera and ul Hassan, Muhammad Muneeb and Mehmood, Atif and Espitia, Gabriel Piñeres and Butt, Shariq Aziz},
  doi          = {10.1007/s10462-024-11001-z},
  journal      = {Artificial Intelligence Review},
  month        = {2},
  number       = {2},
  pages        = {1-38},
  shortjournal = {Artif. Intell. Rev.},
  title        = {Enhancing industrial robot selection through a hybrid novel approach: Integrating CRITIC-VIKOR method with probabilistic uncertain linguistic q-rung orthopair fuzzy},
  volume       = {58},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Digital phenotypes and digital biomarkers for health and diseases: A systematic review of machine learning approaches utilizing passive non-invasive signals collected via wearable devices and smartphones. <em>AIR</em>, <em>58</em>(2), 1-43. (<a href='https://doi.org/10.1007/s10462-024-11009-5'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Passive non-invasive sensing signals from wearable devices and smartphones are typically collected continuously without user input. This passive and continuous data collection makes these signals suitable for moment-by-moment monitoring of health-related outcomes, disease diagnosis, and prediction modeling. A growing number of studies have utilized machine learning (ML) approaches to predict and analyze health indicators and diseases using passive non-invasive signals collected via wearable devices and smartphones. This systematic review identified peer-reviewed journal articles utilizing ML approaches for digital phenotyping and measuring digital biomarkers to analyze, screen, identify, and/or predict health-related outcomes using passive non-invasive signals collected from wearable devices or smartphones. PubMed, PubMed with Mesh, Web of Science, Scopus, and IEEE Xplore were searched for peer-reviewed journal articles published up to June 2024, identifying 66 papers. We reviewed the study populations used for data collection, data acquisition details, signal types, data preparation steps, ML approaches used, digital phenotypes and digital biomarkers, and health outcomes and diseases predicted using these ML techniques. Our findings highlight the promising potential for objective tracking of health outcomes and diseases using passive non-invasive signals collected from wearable devices and smartphones with ML approaches for characterization and prediction of a range of health outcomes and diseases, such as stress, seizure, fatigue, depression, and Parkinson’s disease. Future studies should focus on improving the quality of collected data, addressing missing data challenges, providing better documentation on study participants, and sharing the source code of the implemented methods and algorithms, along with their datasets and methods, for reproducibility purposes.},
  archive      = {J_AIR},
  author       = {Sameh, Alireza and Rostami, Mehrdad and Oussalah, Mourad and Korpelainen, Raija and Farrahi, Vahid},
  doi          = {10.1007/s10462-024-11009-5},
  journal      = {Artificial Intelligence Review},
  month        = {2},
  number       = {2},
  pages        = {1-43},
  shortjournal = {Artif. Intell. Rev.},
  title        = {Digital phenotypes and digital biomarkers for health and diseases: A systematic review of machine learning approaches utilizing passive non-invasive signals collected via wearable devices and smartphones},
  volume       = {58},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A survey on deep learning-based automated essay scoring and feedback generation. <em>AIR</em>, <em>58</em>(2), 1-40. (<a href='https://doi.org/10.1007/s10462-024-11017-5'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep learning-based automated essay scoring (AES) models exhibit a remarkable ability to identify complex patterns within essays and then generate accurate score predictions in an end-to-end training fashion. However, these models face a critical limitation in explaining the specific patterns and features utilized for scoring, which are essential for interpreting the scores and offering constructive feedback to essay authors. Numerous studies have focused on essay scoring, with the aim of modeling prompt-specific, domain-adaptable, or trait-specific AES. While existing surveys on AES cover topics ranging from representation to scoring models, they primarily emphasize scoring models. This study addresses a crucial gap by encompassing research on feedback generation for essay assessment tasks. By delving into essay scoring and feedback generation, we synthesize several existing literature to provide readers with a comprehensive understanding of ongoing research in both deep learning-based essay scoring and automated feedback generation. We categorized the existing essay scoring studies into prompt-specific and cross-prompt AES models, noting that prompt-specific AES is extensively researched category. However, we have only come across a few studies concerning automated feedback generation, likely because of the limited availability of suitable datasets for researching such types of tasks. Moreover, this survey provides insights into approaches for essay representation, prevalent datasets, evaluation metrics, and challenges in automated essay scoring tasks. By shedding light on these aspects, our goal is to delineate the current landscape, identify key research directions, and pave the way for further advancements in automated essay assessment.},
  archive      = {J_AIR},
  author       = {Misgna, Haile and On, Byung-Won and Lee, Ingyu and Choi, Gyu Sang},
  doi          = {10.1007/s10462-024-11017-5},
  journal      = {Artificial Intelligence Review},
  month        = {2},
  number       = {2},
  pages        = {1-40},
  shortjournal = {Artif. Intell. Rev.},
  title        = {A survey on deep learning-based automated essay scoring and feedback generation},
  volume       = {58},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Advancing explainable MOOC recommendation systems: A morphological operations-based framework on partially ordered neutrosophic fuzzy hypergraphs. <em>AIR</em>, <em>58</em>(2), 1-44. (<a href='https://doi.org/10.1007/s10462-024-11018-4'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recommendation systems constitute an integral part of nearly all digital service platforms. However, the common assumption in most recommendation systems in the literature is that similar users will be interested in similar items. This assumption holds only sometimes due to the inherent inhomogeneity of user-item interactions. To address this challenge, we introduce a novel recommendation system that leverages partially ordered neutrosophic hypergraphs to model higher-order relationships among users and items. The partial ordering of nodes enables the system to develop efficient top-N recommendations with very high Normalized Discounted Cumulative Gain (NDCG). Our approach incorporates the morphological operation of dilation, applied to user clusters obtained through fuzzy spectral clustering of the hypergraph, to generate the requisite number of recommendations. Explanations for recommendations are obtained through morphological erosion applied on the dual of the embedded hypergraph. Through rigorous testing in educational and e-commerce domains, it has been proved that our method outperforms state-of-the-art techniques and demonstrates excellent performance for various evaluation parameters. The NDCG value, a measure of ranking quality, surpasses 0.10, and the Hit Ratio (HR) consistently falls within the range of 0.25 to 0.30. The Root Mean Square Error (RMSE) values are minimal, reaching as low as 0.4. These results collectively position our algorithm as a good choice for generating recommendations with proper explanations, making it a promising solution for real-world applications.},
  archive      = {J_AIR},
  author       = {Shareef, Mehbooba and Jose, Babita Roslind and Mathew, Jimson and Pruthviraja, Dayananda},
  doi          = {10.1007/s10462-024-11018-4},
  journal      = {Artificial Intelligence Review},
  month        = {2},
  number       = {2},
  pages        = {1-44},
  shortjournal = {Artif. Intell. Rev.},
  title        = {Advancing explainable MOOC recommendation systems: A morphological operations-based framework on partially ordered neutrosophic fuzzy hypergraphs},
  volume       = {58},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Genetic biomarkers and machine learning techniques for predicting diabetes: Systematic review. <em>AIR</em>, <em>58</em>(2), 1-42. (<a href='https://doi.org/10.1007/s10462-024-11020-w'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Diabetes mellitus is a long-term metabolic condition marked by high blood sugar levels due to issues with insulin production, insulin effectiveness, or a combination of both. It stands as one of the fastest-growing diseases worldwide, projected to afflict 693 million adults by 2045. The escalating prevalence of diabetes and associated health complications (kidney disease, retinopathy, and neuropathy) underscore the imperative to devise predictive models for early diagnosis and intervention. These complications contribute to increased mortality rates, blindness, kidney failure, and an overall diminished quality of life in individuals living with diabetes. While clinical risk factors and glycemic control provide valuable insights, they alone cannot reliably predict the onset of vascular complications. Genetic biomarkers and machine learning techniques have emerged as promising tools for predicting diabetes development risk and associated complications. Despite the emergence of numerous smart AI models for diabetes prediction, there is still a need for a thorough review outlining their progress and challenges. To address this gap, this paper offers a systematic review of the literature on AI-based models for diabetes identification, following the PRISMA extension for scoping reviews guidelines. Our review revealed that multimodal diabetes prediction models outperformed unimodal models. Most studies focused on classical machine learning models, with SNPs being the most used data type, followed by gene expression profiles, while lipidomic and metabolomic data were the least utilized. Moreover, some studies focused on identifying genetic determinants of diabetes complications relied on familial linkage analysis, tailored for robust effect loci. However, these approaches had limitations, including susceptibility to false positives in candidate gene studies and underpowered AI models capabilities due to sample size constraints. The landscape shifted dramatically with the proliferation of genomic datasets, fueled by the emergence of biobanks and the amalgamation of global cohorts. This surge has led to a more than twofold increase in genetic discoveries related to both diabetes and its complications using AI. Our focus here is on these genetic breakthroughs, particularly those empowered by AI models. However, we also highlight the existing gaps in research and underscore the need for further advancements to propel genomic discovery to the next level.},
  archive      = {J_AIR},
  author       = {Khan, Sulaiman and Mohsen, Farida and Shah, Zubair},
  doi          = {10.1007/s10462-024-11020-w},
  journal      = {Artificial Intelligence Review},
  month        = {2},
  number       = {2},
  pages        = {1-42},
  shortjournal = {Artif. Intell. Rev.},
  title        = {Genetic biomarkers and machine learning techniques for predicting diabetes: Systematic review},
  volume       = {58},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Enhancement of satellite images based on CLAHE and augmented elk herd optimizer. <em>AIR</em>, <em>58</em>(2), 1-75. (<a href='https://doi.org/10.1007/s10462-024-11022-8'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Satellite images often have very narrow brightness value ranges, so it is necessary to enhance the contrast and brightness, maintain the quality of visual information, and preserve pertinent details in the images before conducting additional analysis. This is because improving the brightness and contrast of images is crucial to image processing and analysis as it makes it easier for people to identify and comprehend the images. The Incomplete Beta Function (IBF) is a popular transformation function for Image Contrast Enhancement (ICE). Nevertheless, IBF has modest efficiency in parameter selection, a small set of adjustable parameters for stretching regions with high or low gray levels, and image enhancement is almost ineffective with stretching at either end. Meta-heuristic algorithms have been utilized efficiently and effectively over the past few decades to solve complicated image processing problems. This paper presents an Augmented version of the Elk Herd Optimizer (AEHO) combined with other traditional ICE techniques to improve edge details, entropy, local contrast, and local brightness of low-contrast natural and satellite images. The AEHO method employs a multi-stage strategic procedure, where its mathematical model undergoes several enhancements before being applied to ICE to allow for further exploration and exploitation of its features. This method uses a pre-established fitness criterion for the purpose of optimizing a set of parameters to rework a well-known transformation function and an effective assessment technique as an objective standard for this purpose. In the proposed image enhancement model, contrast limited adaptive histogram equalization was first applied as a prior step to ameliorate the color intensity. Then, the optimal IBF’s parameters for ICE were adaptively determined using AEHO. After that, bilateral gamma correction was used to improve the visual quality of images without sacrificing edge details or natural color quality. The proposed AEHO-based image enhancement model is tested on natural scenes, certain standard images, and publicly available satellite images. In addition to other five techniques built on based on pre-existing meta-heuristics, the performance of the proposed method was compared against other well-known state-of-the-art image enhancement algorithms. The objective evaluation of the enhancement algorithms was achieved utilizing a variety of full-reference, no-reference, and pertinent performance evaluation norms. The experimental findings illustrated that the proposed image enhancement method can successfully outperform several other algorithms that employed the same image enhancement model as AEHO in addition to other conventional image enhancement methods included for comparison. The results on ten natural and satellite color images showed that the presented method performs better than all other comparative methods in the corresponding evaluation criteria in terms of average peak signal-to-noise ratio, average universal quality index, average structural contrast-quality index, and average values of discrete entropy results, which are more than 32.30, 94.0%, 0.98.9%, and 7.4, respectively. In a nutshell, AEHO can be an efficient method that can be used to tackle several image processing problems.},
  archive      = {J_AIR},
  author       = {Braik, Malik and Al-Betar, Mohammed Azmi and Mahdi, Mohammed A. and Al-Shalabi, Mohammed and Ahamad, Shahanawaj and Saad, Sawsan A.},
  doi          = {10.1007/s10462-024-11022-8},
  journal      = {Artificial Intelligence Review},
  month        = {2},
  number       = {2},
  pages        = {1-75},
  shortjournal = {Artif. Intell. Rev.},
  title        = {Enhancement of satellite images based on CLAHE and augmented elk herd optimizer},
  volume       = {58},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Analyzing customer preferences for hydrogen cars: A characteristic objects method approach. <em>AIR</em>, <em>58</em>(2), 1-25. (<a href='https://doi.org/10.1007/s10462-024-11027-3'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As hydrogen vehicles gain popularity, car manufacturers are introducing numerous models, presenting customers with the challenge of choosing the most suitable option. To address this, Multi-Criteria Decision Analysis methods are often used to evaluate and select the best alternative. This study applies the Characteristic Objects Method (COMET) to address the practical problem of selecting the most appropriate hydrogen car for decision-makers. Using data provided by manufacturers, we evaluate ten hydrogen vehicles and create six decision models based on the preferences of three decision-makers, utilizing both the recently proposed Triad Support and Expected Solution Point-COMET algorithms. The models provide insights into how customer preferences can be extracted and represented in decision models. Moreover, we analyze local weights derived from the models to understand customer expectations for hydrogen cars better. The results of our study highlight the effectiveness of the COMET approach in capturing and comparing decision-maker preferences, offering a valuable methodology framework for future applications in similar multi-criteria decision-making problems.},
  archive      = {J_AIR},
  author       = {Shekhovtsov, Andrii and Rafiei Oskooei, Amirkia and Wątróbski, Jarosław and Sałabun, Wojciech},
  doi          = {10.1007/s10462-024-11027-3},
  journal      = {Artificial Intelligence Review},
  month        = {2},
  number       = {2},
  pages        = {1-25},
  shortjournal = {Artif. Intell. Rev.},
  title        = {Analyzing customer preferences for hydrogen cars: A characteristic objects method approach},
  volume       = {58},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Reconstructing damaged fNIRS signals with a generative deep learning model. <em>AIR</em>, <em>58</em>(2), 1-20. (<a href='https://doi.org/10.1007/s10462-024-11028-2'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Functional near-infrared spectroscopy (fNIRS) imaging offers a promising avenue for measuring brain function in both healthy and diseased cohorts. However, signal quality in fNIRS data frequently encounters challenges, such as low signal-to-noise ratio or substantial motion artifacts in one or multiple measurement channels, impeding the comprehensive exploitation of the data. Developing a valid method to improve the quality of damaged fNIRS signals is crucial, particularly given the extensive use of wearable fNIRS devices in natural settings where noise issues are even more unavoidable. Here, we proposed a generative deep learning approach to recover damaged fNIRS signals in one or more measurement channels. The model captured spatial and temporal variations in the time series of fNIRS data by integrating multiscale convolutional layers, gated recurrent units (GRUs), and linear regression analyses. We trained the model on a resting-state fNIRS dataset from healthy elderly individuals and evaluated its performance in terms of reconstruction accuracy and functional connectivity matrix similarity. Collectively, the proposed model exhbited an excellent performance for the reconstruction of damaged fNIRS time series. In individual channel-level, the model can accurately reconstruct damaged fNIRS time series (mean correlation = 0.80 ± 0.14) while preserving intervariable relationships (correlation = 0.93). In multiple channel-level, the model maintained robust reconstruction accuracy and consistency in terms of functional connectivity. Our findings underscore the potential of generative deep learning techniques in reconstructing damaged fNIRS signals, providing a novel perspective for the efficient utilization of data in clinical diagnosis and brain research.},
  archive      = {J_AIR},
  author       = {Zhi, Yingxu and Zhang, Baiqiang and Xu, Bingxin and Wan, Fei and Niu, Peisong and Niu, Haijing},
  doi          = {10.1007/s10462-024-11028-2},
  journal      = {Artificial Intelligence Review},
  month        = {2},
  number       = {2},
  pages        = {1-20},
  shortjournal = {Artif. Intell. Rev.},
  title        = {Reconstructing damaged fNIRS signals with a generative deep learning model},
  volume       = {58},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A comprehensive study on the application of machine learning in psoriasis diagnosis and treatment: Taxonomy, challenges and recommendations. <em>AIR</em>, <em>58</em>(2), 1-53. (<a href='https://doi.org/10.1007/s10462-024-11031-7'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Psoriasis is a common skin disease with complex mechanisms, and its diagnosis and treatment bring many challenges. In recent years, machine learning (ML) techniques have been proposed as a new tool to improve this disease’s diagnosis and treatment process. With the ability to learn from limited data and transfer knowledge from one field to another, these techniques have a high potential to improve diagnosis accuracy and treatment efficiency. However, using ML in diagnosing and treating psoriasis is associated with several challenges, including data limitations, the complexity of algorithms, and the need for high expertise to implement them properly. By presenting a detailed taxonomy, this article examines the applications and challenges of using ML techniques in psoriasis and analyzes the latest achievements in this field. The results of this study show that ML techniques have increased the accuracy of psoriasis diagnosis by 35% and improved treatment efficiency by 29%. In addition, these techniques reduced the data processing time by 21% and improved the overall treatment process. Also, these methods have increased the success rate of patient survival predictions by 15%. Finally, by examining the existing challenges and providing solutions to overcome these challenges, this research will help researchers and experts in this field develop new strategies to improve the diagnosis and treatment of psoriasis by better understanding ML applications.},
  archive      = {J_AIR},
  author       = {Ghorbian, Mohsen and Ghobaei-Arani, Mostafa and Ghorbian, Saeid},
  doi          = {10.1007/s10462-024-11031-7},
  journal      = {Artificial Intelligence Review},
  month        = {2},
  number       = {2},
  pages        = {1-53},
  shortjournal = {Artif. Intell. Rev.},
  title        = {A comprehensive study on the application of machine learning in psoriasis diagnosis and treatment: Taxonomy, challenges and recommendations},
  volume       = {58},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Task scheduling in cloud computing systems using multi-objective honey badger algorithm with two hybrid elite frameworks and circular segmentation screening. <em>AIR</em>, <em>58</em>(2), 1-69. (<a href='https://doi.org/10.1007/s10462-024-11032-6'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In cloud computing environment, task scheduling is the most critical problem to be solved. Two different multi-objective honey badger algorithms (MOHBA-I and MOHBA-II) based on hybrid elitist framework and circular segmentation screening are proposed for the multi-objective problem of task scheduling optimization in cloud computing systems. MOHBA-I and MOHBA-II combine the grid indexing mechanism and decomposition technique, respectively, to select better populations based on elite non-dominated sorting. A circular segmentation screening mechanism was proposed to retain the superior individuals when the regional density is too high to further maintain the diversity of the populations, and attach an external archive to preserve the uniformly diversified Pareto decomposition set. The performance of the proposed algorithms is verified by using test functions. MOHBA-I and MOHBA-II achieve the first and third rankings, respectively, compared to other classical multi-objective algorithms. Solve the cloud computing task scheduling problem using time, load and price cost as metrics, test for different task sizes, and compare MOHBA-I with algorithms such as NSGA-III, MOPSO and MOEA/D in the same experimental environment. When facing a large-scale task, MOHBA-I ranks first in HyperVolume value with 2.4449E−02 for two objectives and 9.2950E−03 for three objectives. The experimental results show that MOHBA-I finds the highest number of solutions with better convergence and coverage, obtaining a satisfactory Pareto front, which can provide more and better choices for decision makers.},
  archive      = {J_AIR},
  author       = {Zhang, Si-Wen and Wang, Jie-Sheng and Zhang, Shi-Hui and Xing, Yu-Xuan and Sui, Xiao-Fei and Zhang, Yun-Hao},
  doi          = {10.1007/s10462-024-11032-6},
  journal      = {Artificial Intelligence Review},
  month        = {2},
  number       = {2},
  pages        = {1-69},
  shortjournal = {Artif. Intell. Rev.},
  title        = {Task scheduling in cloud computing systems using multi-objective honey badger algorithm with two hybrid elite frameworks and circular segmentation screening},
  volume       = {58},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Improved artificial rabbits algorithm for global optimization and multi-level thresholding color image segmentation. <em>AIR</em>, <em>58</em>(2), 1-57. (<a href='https://doi.org/10.1007/s10462-024-11035-3'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Artificial Rabbits Optimization Algorithm is a metaheuristic optimization algorithm proposed in 2022. This algorithm has weak local search ability, which can easily lead to the algorithm falling into local optimal solutions. To overcome these limitations, this paper introduces an Improved Artificial Rabbits Optimization Algorithm (IARO) and demonstrates its effectiveness in multi-level threshold color image segmentation using the Otsu method. Initially, we apply a center-driven strategy to enhance exploration by updating the rabbit’s position during the random hiding phase. Additionally, when the algorithm stalls, a Gaussian Randomized Wandering (GRW) strategy is utilized to enable the algorithm to escape local optima and improve convergence accuracy. The performance of the IARO algorithm is evaluated using 23 standard benchmark functions and CEC2020 benchmark functions, and compared with nine other algorithms. Experimental results indicate that IARO excels in global optimization and demonstrates notable robustness. To assess its effectiveness in multi-threshold color image segmentation, the algorithm is tested on classical Berkeley images. Evaluation metrics including execution time, Peak Signal-to-Noise Ratio (PSNR), Feature Similarity (FSIM), Structural Similarity (SSIM), Boundary Displacement Error (BDE), The Probabilistic Rand Index (PRI), Variation of Information (VOI) and average fitness value are used to measure segmentation quality. The results reveal that IARO achieves high accuracy and fast segmentation speed, validating its efficiency and practical utility in real-world applications.},
  archive      = {J_AIR},
  author       = {Jia, Heming and Su, Yuanyuan and Rao, Honghua and Liang, Muzi and Abualigah, Laith and Liu, Chibiao and Chen, Xiaoguo},
  doi          = {10.1007/s10462-024-11035-3},
  journal      = {Artificial Intelligence Review},
  month        = {2},
  number       = {2},
  pages        = {1-57},
  shortjournal = {Artif. Intell. Rev.},
  title        = {Improved artificial rabbits algorithm for global optimization and multi-level thresholding color image segmentation},
  volume       = {58},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Landscape of machine learning evolution: Privacy-preserving federated learning frameworks and tools. <em>AIR</em>, <em>58</em>(2), 1-44. (<a href='https://doi.org/10.1007/s10462-024-11036-2'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Machine learning is one of the most widely used technologies in the field of Artificial Intelligence. As machine learning applications become increasingly ubiquitous, concerns about data privacy and security have also grown. The work in this paper presents a broad theoretical landscape concerning the evolution of machine learning and deep learning from centralized to distributed learning, first in relation to privacy-preserving machine learning and secondly in the area of privacy-enhancing technologies. It provides a comprehensive landscape of the synergy between distributed machine learning and privacy-enhancing technologies, with federated learning being one of the most prominent architectures. Various distributed learning approaches to privacy-aware techniques are structured in a review, followed by an in-depth description of relevant frameworks and libraries, more particularly in the context of federated learning. The paper also highlights the need for data protection and privacy addressed from different approaches, key findings in the field concerning AI applications, and advances in the development of related tools and techniques.},
  archive      = {J_AIR},
  author       = {Nguyen, Giang and Sáinz-Pardo Díaz, Judith and Calatrava, Amanda and Berberi, Lisana and Lytvyn, Oleksandr and Kozlov, Valentin and Tran, Viet and Moltó, Germán and López García, Álvaro},
  doi          = {10.1007/s10462-024-11036-2},
  journal      = {Artificial Intelligence Review},
  month        = {2},
  number       = {2},
  pages        = {1-44},
  shortjournal = {Artif. Intell. Rev.},
  title        = {Landscape of machine learning evolution: Privacy-preserving federated learning frameworks and tools},
  volume       = {58},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Revisiting non-learned operators based deep learning for image classification: A lightweight directional-aware network. <em>AIR</em>, <em>58</em>(2), 1-23. (<a href='https://doi.org/10.1007/s10462-024-11038-0'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Due to the stable feature representation capability provided by non-learned operators, the integration with deep learning models, i.e., non-learned operator based deep learning models, has become a paradigm, however, performance-wise, it is still not promising. In this paper, by revisiting non-learned operator based deep learning models, we reveal the reasons for their underperformance: lack of geometric invariance, insufficient sparsity, and neglect of directional importance. In response, we present a Lightweight Directional-Aware Network (LDAN) for image classification. Specifically, to generate sparse geometric-invariant features, we propose a ShearletNet to capture multi-directional features in three different levels. Then, a Directional-Aware module is designed to highlight the discriminative multi-directional features and generate multi-scale features. Finally, a Pointwise Convolution module is used to integrate the multi-directional features with the multi-scale ones for reducing the computational resources. Experiments on the commonly used CIFAR10, CIFAR100, Self-Taught Learning 10 (STL10), and Tiny ImageNet datasets demonstrate the efficiency and effectiveness of the proposed LDAN. Compared to the existing non-learned operator based models, LDAN reduces the parameter count by 80.83% while achieving a 6.32% increase in accuracy.},
  archive      = {J_AIR},
  author       = {Guo, Yuwei and Zhang, Wenhao and Gao, Yupeng and Jiao, Licheng and Wang, Shuo and Du, Jiabo and Liu, Fang},
  doi          = {10.1007/s10462-024-11038-0},
  journal      = {Artificial Intelligence Review},
  month        = {2},
  number       = {2},
  pages        = {1-23},
  shortjournal = {Artif. Intell. Rev.},
  title        = {Revisiting non-learned operators based deep learning for image classification: A lightweight directional-aware network},
  volume       = {58},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Ratai: Recurrent autoencoder with imputation units and temporal attention for multivariate time series imputation. <em>AIR</em>, <em>58</em>(2), 1-24. (<a href='https://doi.org/10.1007/s10462-024-11039-z'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multivariate time series is ubiquitous in real-world applications, yet it often suffers from missing values that impede downstream analytical tasks. In this paper, we introduce the Long Short-Term Memory Network based Recurrent Autoencoder with Imputation Units and Temporal Attention Imputation Model (RATAI), tailored for multivariate time series. RATAI is designed to address certain limitations of traditional RNN-based imputation methods, which often focus on predictive modeling to estimate missing values, sometimes neglecting the contextual impact of observed data at and beyond the target time step. Drawing inspiration from Kalman smoothing, which effectively integrates past and future information to refine state estimations, RATAI aims to extract feature representations from time series data and use them to reconstruct a complete time series, thus overcoming the shortcomings of existing approaches. It employs a dual-stage imputation process: the encoder utilizes temporal information and attribute correlations to predict and impute missing values, and extract feature representation of imputed time series. Subsequently, the decoder reconstructs the series from the feature representation, and the reconstructed values are used as the final imputation values. Additionally, RATAI incorporates a temporal attention mechanism, allowing the decoder to focus on highly relevant inputs during reconstruction. This model can be trained directly using data that contains missing values, avoiding the misleading effects on model training that can arise from setting initial values for missing values. Our experiments demonstrate that RATAI outperforms benchmark models in multivariate time series imputation.},
  archive      = {J_AIR},
  author       = {Lai, Xiaochen and Yao, Yachen and Mu, Jichong and Lu, Wei and Zhang, Liyong},
  doi          = {10.1007/s10462-024-11039-z},
  journal      = {Artificial Intelligence Review},
  month        = {2},
  number       = {2},
  pages        = {1-24},
  shortjournal = {Artif. Intell. Rev.},
  title        = {Ratai: Recurrent autoencoder with imputation units and temporal attention for multivariate time series imputation},
  volume       = {58},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Specification overfitting in artificial intelligence. <em>AIR</em>, <em>58</em>(2), 1-37. (<a href='https://doi.org/10.1007/s10462-024-11040-6'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Machine learning (ML) and artificial intelligence (AI) approaches are often criticized for their inherent bias and for their lack of control, accountability, and transparency. Consequently, regulatory bodies struggle with containing this technology’s potential negative side effects. High-level requirements such as fairness and robustness need to be formalized into concrete specification metrics, imperfect proxies that capture isolated aspects of the underlying requirements. Given possible trade-offs between different metrics and their vulnerability to over-optimization, integrating specification metrics in system development processes is not trivial. This paper defines specification overfitting, a scenario where systems focus excessively on specified metrics to the detriment of high-level requirements and task performance. We present an extensive literature survey to categorize how researchers propose, measure, and optimize specification metrics in several AI fields (e.g., natural language processing, computer vision, reinforcement learning). Using a keyword-based search on papers from major AI conferences and journals between 2018 and mid-2023, we identify and analyze 74 papers that propose or optimize specification metrics. We find that although most papers implicitly address specification overfitting (e.g., by reporting more than one specification metric), they rarely discuss which role specification metrics should play in system development or explicitly define the scope and assumptions behind metric formulations.},
  archive      = {J_AIR},
  author       = {Roth, Benjamin and Luz de Araujo, Pedro Henrique and Xia, Yuxi and Kaltenbrunner, Saskia and Korab, Christoph},
  doi          = {10.1007/s10462-024-11040-6},
  journal      = {Artificial Intelligence Review},
  month        = {2},
  number       = {2},
  pages        = {1-37},
  shortjournal = {Artif. Intell. Rev.},
  title        = {Specification overfitting in artificial intelligence},
  volume       = {58},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Applications of deep learning in alzheimer’s disease: A systematic literature review of current trends, methodologies, challenges, innovations, and future directions. <em>AIR</em>, <em>58</em>(2), 1-57. (<a href='https://doi.org/10.1007/s10462-024-11041-5'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Alzheimer’s Disease (AD) constitutes a significant global health issue. In the next 40 years, it is expected to affect 106 million people. Although more and more people are getting AD, there are still no effective drugs to treat it. Insightful information about how important it is to find and treat AD quickly. Recently, Deep Learning (DL) techniques have been used more and more to diagnose AD. They claim better accuracy in drug reuse, medication recognition, and labeling. This essay meticulously examines the works that have talked about using DL with Alzheimer’s disease. Some of the methods are Natural Language Processing (NLP), drug reuse, classification, and identification. Concerning these methods, we examine their pros and cons, paying special attention to how easily they can be explained, how safe they are, and how they can be used in medical situations. One important finding is that Convolutional Neural Networks (CNNs) are most often used for AD research and Python is most often used for DL issues. Some security problems, like data protection and model stability, are not looked at enough in the present research, according to us. This study thoroughly examines present methods and also points out areas that need more work, like better data integration and AI systems that can be explained. The findings should help guide more research and speed up the creation of DL-based AD identification tools in the future.},
  archive      = {J_AIR},
  author       = {Toumaj, Shiva and Heidari, Arash and Shahhosseini, Reza and Jafari Navimipour, Nima},
  doi          = {10.1007/s10462-024-11041-5},
  journal      = {Artificial Intelligence Review},
  month        = {2},
  number       = {2},
  pages        = {1-57},
  shortjournal = {Artif. Intell. Rev.},
  title        = {Applications of deep learning in alzheimer’s disease: A systematic literature review of current trends, methodologies, challenges, innovations, and future directions},
  volume       = {58},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Computational humor recognition: A systematic literature review. <em>AIR</em>, <em>58</em>(2), 1-53. (<a href='https://doi.org/10.1007/s10462-024-11043-3'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Computational humor recognition is considered to be one of the hardest tasks in natural language processing (NLP) since humor is such a particularly complex emotion. There are very few recent studies that offer an analysis of certain aspects of computational humor. However, there has been no attempt to study the empirical evidence on computational humor recognition in a systematic way. The aim of this research is to examine computational humor detection from three aspects: datasets, features and algorithms. Therefore, a Systematic Literature Review (SLR) was carried out to present in detail the computational techniques for humor identification under these aspects. After posing some research questions, a total of 106 primary papers were identified as relevant to the objectives of these questions and further detailed analysis was conducted. The study revealed that there are a great number of publicly available annotated humor datasets with many different types of humor instances. Twenty-one (21) humor features have been carefully studied, and research evidence of their use in humor computational detection is presented. Additionally, a classification of the humor detection approaches was performed, and the results are presented. Finally, the challenges of applying these techniques to humor recognition as well as promising future research directions are discussed.},
  archive      = {J_AIR},
  author       = {Kalloniatis, Antonios and Adamidis, Panagiotis},
  doi          = {10.1007/s10462-024-11043-3},
  journal      = {Artificial Intelligence Review},
  month        = {2},
  number       = {2},
  pages        = {1-53},
  shortjournal = {Artif. Intell. Rev.},
  title        = {Computational humor recognition: A systematic literature review},
  volume       = {58},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). How the internet of things technology improves agricultural efficiency. <em>AIR</em>, <em>58</em>(2), 1-26. (<a href='https://doi.org/10.1007/s10462-024-11046-0'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Meeting the rising global food demand among limited resources necessitates transformative agricultural innovations. The Internet of Things (IoT) emerges as a pivotal technology in modern agriculture, offering data-driven solutions to optimize productivity and sustainability. This review provides a focused exploration of IoT’s transformative role in agriculture, analyzing its integration with big data, real-time monitoring, and precision farming practices. Key insights include global market trends, projections for IoT adoption in agriculture by 2030, and advancements in IoT-related technologies shaping the future of agritech. The review underscores how IoT enhances agricultural efficiency by enabling precise data collection, automated decision-making, and optimized resource use, while addressing operational challenges such as interoperability, scalability, cost constraints, and regulatory hurdles. By consolidating evidence from emerging studies, this work advocates for interdisciplinary collaborations to deepen understanding and innovation in IoT-driven smart agriculture, positioning it as a cornerstone for achieving global food security.},
  archive      = {J_AIR},
  author       = {Duguma, Amenu Leta and Bai, Xiuguang},
  doi          = {10.1007/s10462-024-11046-0},
  journal      = {Artificial Intelligence Review},
  month        = {2},
  number       = {2},
  pages        = {1-26},
  shortjournal = {Artif. Intell. Rev.},
  title        = {How the internet of things technology improves agricultural efficiency},
  volume       = {58},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). An end-to-end occluded person re-identification network with smoothing corrupted feature prediction. <em>AIR</em>, <em>58</em>(2), 1-31. (<a href='https://doi.org/10.1007/s10462-024-11047-z'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Occluded person re-identification (ReID) is a challenging task as the images suffer from various obstacles and less discriminative information caused by incomplete body parts. Most current works rely on auxiliary models to infer the visible body parts and partial-level features matching to overcome the contaminated body information, which consumes extra inference time and fails when facing complex occlusions. More recently, some methods utilized masks provided from image occlusion augmentation (OA) for the supervision of mask learning. These works estimated occlusion scores for each part of the image by roughly dividing it in the horizontal direction, but cannot accurately predict the occlusion, as well as failing in vertical occlusions. To address this issue, we proposed a Smoothing Corrupted Feature Prediction (SCFP) network in an end-to-end manner for occluded person ReID. Specifically, aided by OA that simulates occlusions appearing in pedestrians and providing occlusion masks, the proposed Occlusion Decoder and Estimator (ODE) estimates and eliminates corrupted features, which is supervised by mask labels generated via restricting all occlusions into a group of patterns. We also designed an Occlusion Pattern Smoothing (OPS) to improve the performance of ODE when predicting irregular obstacles. Subsequently, a Local-to-Body (L2B) representation is constructed to mitigate the limitation of the partial body information for final matching. To investigate the performance of SCFP, we compared our model to the existing state-of-the-art methods in occluded and holistic person ReID benchmarks and proved that our method achieves superior results over the state-of-the-art methods. We also achieved the highest Rank-1 accuracies of 70.9%, 87.0%, and 93.2% in Occluded-Duke, Occluded-ReID, and P-DukeMTMC, respectively. Furthermore, the proposed SCFP generalizes well in holistic datasets, yielding accuracies of 95.8% in Market-1510 and 90.7% in DukeMTMC-reID.},
  archive      = {J_AIR},
  author       = {Zhao, Caijie and Qin, Ying and Zhang, Bob and Zhao, Yajie and Wu, Baoyun},
  doi          = {10.1007/s10462-024-11047-z},
  journal      = {Artificial Intelligence Review},
  month        = {2},
  number       = {2},
  pages        = {1-31},
  shortjournal = {Artif. Intell. Rev.},
  title        = {An end-to-end occluded person re-identification network with smoothing corrupted feature prediction},
  volume       = {58},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Models of symbol emergence in communication: A conceptual review and a guide for avoiding local minima. <em>AIR</em>, <em>58</em>(2), 1-40. (<a href='https://doi.org/10.1007/s10462-024-11048-y'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Computational simulations are a popular method for testing hypotheses about the emergence of symbolic communication. This kind of research is performed in a variety of traditions including language evolution, developmental psychology, cognitive science, artificial intelligence, and robotics. The motivations for the models are different, but the operationalisations and methods used are often similar. We identify the assumptions and explanatory targets of the most representative models and summarise the known results. We claim that some of the assumptions—such as portraying meaning in terms of mapping, focusing on the descriptive function of communication, and modelling signals with amodal tokens—may hinder the success of modelling. Relaxing these assumptions and foregrounding the interactions of embodied and situated agents allows one to systematise the multiplicity of pressures under which symbolic systems evolve. In line with this perspective, we sketch the road towards modelling the emergence of meaningful symbolic communication, where symbols are simultaneously grounded in action and perception and form an abstract system.},
  archive      = {J_AIR},
  author       = {Zubek, Julian and Korbak, Tomasz and Rączaszek-Leonardi, Joanna},
  doi          = {10.1007/s10462-024-11048-y},
  journal      = {Artificial Intelligence Review},
  month        = {2},
  number       = {2},
  pages        = {1-40},
  shortjournal = {Artif. Intell. Rev.},
  title        = {Models of symbol emergence in communication: A conceptual review and a guide for avoiding local minima},
  volume       = {58},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Artificial intelligence for geometry-based feature extraction, analysis and synthesis in artistic images: A survey. <em>AIR</em>, <em>58</em>(2), 1-47. (<a href='https://doi.org/10.1007/s10462-024-11051-3'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Artificial Intelligence significantly enhances the visual art industry by analyzing, identifying and generating digitized artistic images. This review highlights the substantial benefits of integrating geometric data into AI models, addressing challenges such as high inter-class variations, domain gaps, and the separation of style from content by incorporating geometric information. Models not only improve AI-generated graphics synthesis quality, but also effectively distinguish between style and content, utilizing inherent model biases and shared data traits. We explore methods like geometric data extraction from artistic images, the impact on human perception, and its use in discriminative tasks. The review also discusses the potential for improving data quality through innovative annotation techniques and the use of geometric data to enhance model adaptability and output refinement. Overall, incorporating geometric guidance boosts model performance in classification and synthesis tasks, providing crucial insights for future AI applications in the visual arts domain.},
  archive      = {J_AIR},
  author       = {Vijendran, Mridula and Deng, Jingjing and Chen, Shuang and Ho, Edmond S. L. and Shum, Hubert P. H.},
  doi          = {10.1007/s10462-024-11051-3},
  journal      = {Artificial Intelligence Review},
  month        = {2},
  number       = {2},
  pages        = {1-47},
  shortjournal = {Artif. Intell. Rev.},
  title        = {Artificial intelligence for geometry-based feature extraction, analysis and synthesis in artistic images: A survey},
  volume       = {58},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Predictor-based neural network control for unmanned aerial vehicles with input quantization: Design and application. <em>AIR</em>, <em>58</em>(2), 1-17. (<a href='https://doi.org/10.1007/s10462-024-11054-0'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, I design a predictor-based neural network (NN) controller for unmanned aerial vehicles (UAVs) with input quantization to address the trajectory tracking problem in the presence of time-varying disturbances caused by aerodynamics and external environment. The NN with a state predictor (SP) is employed in the controller design to improve transient performance without high-frequency oscillations and address the problem of instability caused by the time-varying disturbances. Additionally, the prediction errors from the SP are used to update the learning rate of the NN, resulting in smoother and faster learning responses. Furthermore, a hysteresis quantizer is employed to discretize signals and reduce the transmission burden on digital hardware, which can enhance the suitability of the system for practical implementation. Based on the Lyapunov method, the closed-loop system of the UAV achieves input-to-state stability (ISS). Finally, to validate and assess the performance and effectiveness of our proposed control method, I present and analyze both simulation results and experimental results from real-world applications.},
  archive      = {J_AIR},
  author       = {Wu, Di},
  doi          = {10.1007/s10462-024-11054-0},
  journal      = {Artificial Intelligence Review},
  month        = {2},
  number       = {2},
  pages        = {1-17},
  shortjournal = {Artif. Intell. Rev.},
  title        = {Predictor-based neural network control for unmanned aerial vehicles with input quantization: Design and application},
  volume       = {58},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Staying ahead of phishers: A review of recent advances and emerging methodologies in phishing detection. <em>AIR</em>, <em>58</em>(2), 1-46. (<a href='https://doi.org/10.1007/s10462-024-11055-z'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The escalating threat of phishing attacks poses significant challenges to cybersecurity, necessitating innovative approaches for detection and mitigation. This paper addresses this need by presenting a comprehensive review of state-of-the-art methodologies for phishing detection, spanning traditional machine learning techniques to cutting-edge deep learning frameworks. The review encompasses a diverse range of methods, including list-based approaches, machine learning algorithms, graph-based analysis, deep learning models, network embedding techniques, and generative adversarial networks (GANs). Each method is meticulously scrutinized, highlighting its rationale, advantages, and empirical results. For instance, deep learning models, particularly convolutional neural networks (CNNs) and recurrent neural networks (RNNs), demonstrate superior detection performance, leveraging their ability to extract complex patterns from phishing data. Ensemble learning techniques and GANs offer additional benefits by enhancing detection accuracy and resilience against adversarial attacks. The impact of this review extends beyond academic discourse, informing practitioners and policymakers about the evolving landscape of phishing detection. By elucidating the strengths and limitations of existing methods, this paper guides the development of more robust and effective cybersecurity solutions. Moreover, the insights gleaned from this review lay the groundwork for future research endeavors, such as integrating contextual information, user behavior analysis, and explainable AI techniques into phishing detection systems. Ultimately, this work contributes to the collective effort to fortify digital defenses against sophisticated phishing threats, safeguarding the integrity of online ecosystems.},
  archive      = {J_AIR},
  author       = {Kavya, S. and Sumathi, D.},
  doi          = {10.1007/s10462-024-11055-z},
  journal      = {Artificial Intelligence Review},
  month        = {2},
  number       = {2},
  pages        = {1-46},
  shortjournal = {Artif. Intell. Rev.},
  title        = {Staying ahead of phishers: A review of recent advances and emerging methodologies in phishing detection},
  volume       = {58},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Possibilistic C-means with novel image representation for image segmentation. <em>AIR</em>, <em>58</em>(2), 1-21. (<a href='https://doi.org/10.1007/s10462-024-11057-x'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Image segmentation is the process of automatically dividing an image into several parts and extracting the relevant data and information. Compared to the traditional Fuzzy C-Means algorithm, the Possibilistic C-Means (PCM) algorithm has advantages in reducing the influence of noise on cluster center estimation. However, the PCM algorithm still shows poor clustering performance under high-intensity noise, which may lead to overlapping cluster centers. Considering the impact of neighborhood information of image pixels on the image segmentation results, this paper proposes a Vector-Based Possibilistic C-Means (VBPCM) algorithm. The algorithm incorporates neighborhood information and uses a vector representation method to describe image pixels. Additionally, an adjustable distance based on an exponential function is proposed to describe the similarity between vectors. The proposed VBPCM algorithm outperforms the conventional PCM, obtaining uplifiting gains of 4%, 2%, and 9% in Pixel Accuracy, Mean Pixel Accuracy, and Mean Intersection over Union, respectively. The experimental outputs illustrate that VBPCM algorithm can achieve more satisfactory cluster effect with high-intensity noise, further perform better in image segmentation task.},
  archive      = {J_AIR},
  author       = {Cui, Hanshuai and Wang, Hongjian and Zeng, Wenyi and Liu, Yuqing and Zhao, Bo},
  doi          = {10.1007/s10462-024-11057-x},
  journal      = {Artificial Intelligence Review},
  month        = {2},
  number       = {2},
  pages        = {1-21},
  shortjournal = {Artif. Intell. Rev.},
  title        = {Possibilistic C-means with novel image representation for image segmentation},
  volume       = {58},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Faster independent vector analysis with joint pairwise updates of demixing vectors. <em>AIR</em>, <em>58</em>(2), 1-18. (<a href='https://doi.org/10.1007/s10462-024-11061-1'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To achieve more efficient blind separation of multi-channel speech signals, this paper proposes a new algorithm for blind source separation(BSS) of sound sources using auxiliary function-based independent vector analysis (AuxIVA) with joint pairwise updates of demixing vectors. This algorithm is better than AuxIVA using iterative projection with adjustment (AuxIVA-IPA) when separating multiple sources. The IPA method jointly executes iterative projection (IP) and iterative source steering (ISS) to update and updates one row and one column of the separation matrix in each iteration. On this basis, IPA is extended to jointly execute IP2 and ISS2 for updating, which can update two rows and two columns of the separation matrix in each iteration. Accordingly, this proposed method is named by IPA2. Furthermore, it can optimize the same cost function as IPA while maintaining the same time complexity. Finally, the convolutional speech separation experiments are conducted to validate the effectiveness and efficiency of the proposed method. The experimental results corroborate that compared with the state-of-the-art IP, IP2, ISS, ISS2, and IPA used in AuxIVA, the IPA2 method acquires faster convergence speed and better separation performance, enabling the cost function to reach the convergence interval faster and maintaining good separation results.},
  archive      = {J_AIR},
  author       = {Luo, Zhongqiang and Guo, Ruiming and Wang, Ling},
  doi          = {10.1007/s10462-024-11061-1},
  journal      = {Artificial Intelligence Review},
  month        = {2},
  number       = {2},
  pages        = {1-18},
  shortjournal = {Artif. Intell. Rev.},
  title        = {Faster independent vector analysis with joint pairwise updates of demixing vectors},
  volume       = {58},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Advances and challenges in learning from experience replay. <em>AIR</em>, <em>58</em>(2), 1-65. (<a href='https://doi.org/10.1007/s10462-024-11062-0'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {From the first theoretical propositions in the 1950s to its application in real-world problems, Reinforcement Learning (RL) is still a fascinating and complex class of machine learning algorithms with overgrowing literature in recent years. In this work, we present an extensive and structured literature review and discuss how the Experience Replay (ER) technique has been fundamental in making various RL methods in most relevant problems and different domains more data efficient. ER is the central focus of this review. One of its main contributions is a taxonomy that organizes the many research works and the different RL methods that use ER. Here, the focus is on how RL methods improve and apply ER strategies, demonstrating their specificities and contributions while having ER as a prominent component. Another relevant contribution is the organization in a facet-oriented way, allowing different perspectives of reading, whether based on the fundamental problems of RL, focusing on algorithmic strategies and architectural decisions, or with a view to different applications of RL with ER. Moreover, we start by presenting a detailed formal theoretical foundation of RL and some of the most relevant algorithms and bring from the recent literature some of the main trends, challenges, and advances focused on ER formal basement and how to improve its propositions to make it even more efficient in different methods and domains. Lastly, we discuss challenges and open problems and present relevant paths to feature works.},
  archive      = {J_AIR},
  author       = {Neves, Daniel Eugênio and Ishitani, Lucila and do Patrocínio Júnior, Zenilton Kleber Gonçalves},
  doi          = {10.1007/s10462-024-11062-0},
  journal      = {Artificial Intelligence Review},
  month        = {2},
  number       = {2},
  pages        = {1-65},
  shortjournal = {Artif. Intell. Rev.},
  title        = {Advances and challenges in learning from experience replay},
  volume       = {58},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Real-time speech emotion recognition using deep learning and data augmentation. <em>AIR</em>, <em>58</em>(2), 1-41. (<a href='https://doi.org/10.1007/s10462-024-11065-x'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In human–human interactions, detecting emotions is often easy as it can be perceived through facial expressions, body gestures, or speech. However, in human–machine interactions, detecting human emotion can be a challenge. To improve this interaction, Speech Emotion Recognition (SER) has emerged, with the goal of recognizing emotions solely through vocal intonation. In this work, we propose a SER system based on deep learning approaches and two efficient data augmentation techniques such as noise addition and spectrogram shifting. To evaluate the proposed system, we used three different datasets: TESS, EmoDB, and RAVDESS. We employe several algorithms such as Mel Frequency Cepstral Coefficients (MFCC), Zero Crossing Rate (ZCR), Mel spectrograms, Root Mean Square Value (RMS), and chroma to select the most appropriate vocal features that represent speech emotions. Three different deep learning models were imployed, including MultiLayer Perceptron (MLP), Convolutional Neural Network (CNN), and a hybrid model that combines CNN with Bidirectional Long-Short Term Memory (Bi-LSTM). By exploring these different approaches, we were able to identify the most effective model for accurately identifying emotional states from speech signals in real-time situation. Overall, our work demonstrates the effectiveness of the proposed deep learning model, specifically based on CNN+BiLSTM enhanced with data augmentation for the proposed real-time speech emotion recognition.},
  archive      = {J_AIR},
  author       = {Barhoumi, Chawki and BenAyed, Yassine},
  doi          = {10.1007/s10462-024-11065-x},
  journal      = {Artificial Intelligence Review},
  month        = {2},
  number       = {2},
  pages        = {1-41},
  shortjournal = {Artif. Intell. Rev.},
  title        = {Real-time speech emotion recognition using deep learning and data augmentation},
  volume       = {58},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Damage location and area measurement of aviation functional surface via neural radiance field and improved yolov8 network. <em>AIR</em>, <em>58</em>(2), 1-20. (<a href='https://doi.org/10.1007/s10462-024-11073-x'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To realize high-precision intelligent detection, location and area measurement of aviation functional surface damage, a damage location and area measurement method combining neural radiance field and improved Yolov8 network is proposed in this paper. The high-fidelity NeRF (Neural Radiance Field) and 3DGS (3D Gaussian Splatting) models are trained by acquired multi-view optical images of damaged functional surfaces. The rendered new-view images are used as a new data augmentation method to enhance the training effect of Yolov8 network. The network architecture of Yolov8 model is improved. The backbone is replaced with the latest StarNet feature extraction network, and the context feature fusion module (CFFM) proposed in this paper is used for feature fusion enhancement. The improved context-guide multi-head self-attention (CG-MHSA) is added to the detection Head. The comparison and ablation experiment results show that the improved module proposed in this paper has a good effect on the improvement of Yolov8 model, and improves the damage detection ability and location ability of the model. The application experiment results verify the effectiveness of the proposed method for calculating the damage area of a plane/camber surface, and the accuracy of the damage area measurement is high.},
  archive      = {J_AIR},
  author       = {Hu, Qichun and Xu, Haojun and Wei, Xiaolong and Cai, Yu and Yin, Yizhen and Chen, Junliang and He, Weifeng},
  doi          = {10.1007/s10462-024-11073-x},
  journal      = {Artificial Intelligence Review},
  month        = {2},
  number       = {2},
  pages        = {1-20},
  shortjournal = {Artif. Intell. Rev.},
  title        = {Damage location and area measurement of aviation functional surface via neural radiance field and improved yolov8 network},
  volume       = {58},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Deep learning for surgical instrument recognition and segmentation in robotic-assisted surgeries: A systematic review. <em>AIR</em>, <em>58</em>(1), 1-31. (<a href='https://doi.org/10.1007/s10462-024-10979-w'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Applying deep learning (DL) for annotating surgical instruments in robot-assisted minimally invasive surgeries (MIS) represents a significant advancement in surgical technology. This systematic review examines 48 studies that utilize advanced DL methods and architectures. These sophisticated DL models have shown notable improvements in the precision and efficiency of detecting and segmenting surgical tools. The enhanced capabilities of these models support various clinical applications, including real-time intraoperative guidance, comprehensive postoperative evaluations, and objective assessments of surgical skills. By accurately identifying and segmenting surgical instruments in video data, DL models provide detailed feedback to surgeons, thereby improving surgical outcomes and reducing complication risks. Furthermore, the application of DL in surgical education is transformative. The review underscores the significant impact of DL on improving the accuracy of skill assessments and the overall quality of surgical training programs. However, implementing DL in surgical tool detection and segmentation faces challenges, such as the need for large, accurately annotated datasets to train these models effectively. The manual annotation process is labor-intensive and time-consuming, posing a significant bottleneck. Future research should focus on automating the detection and segmentation process and enhancing the robustness of DL models against environmental variations. Expanding the application of DL models across various surgical specialties will be essential to fully realize this technology’s potential. Integrating DL with other emerging technologies, such as augmented reality (AR), also offers promising opportunities to further enhance the precision and efficacy of surgical procedures.},
  archive      = {J_AIR},
  author       = {Ahmed, Fatimaelzahraa Ali and Yousef, Mahmoud and Ahmed, Mariam Ali and Ali, Hasan Omar and Mahboob, Anns and Ali, Hazrat and Shah, Zubair and Aboumarzouk, Omar and Al Ansari, Abdulla and Balakrishnan, Shidin},
  doi          = {10.1007/s10462-024-10979-w},
  journal      = {Artificial Intelligence Review},
  month        = {1},
  number       = {1},
  pages        = {1-31},
  shortjournal = {Artif. Intell. Rev.},
  title        = {Deep learning for surgical instrument recognition and segmentation in robotic-assisted surgeries: A systematic review},
  volume       = {58},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Artificial intelligence-based optimization techniques for optimal reactive power dispatch problem: A contemporary survey, experiments, and analysis. <em>AIR</em>, <em>58</em>(1), 1-48. (<a href='https://doi.org/10.1007/s10462-024-10982-1'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The optimization challenge known as the optimal reactive power dispatch (ORPD) problem is of utmost importance in the electric power system owing to its substantial impact on stability, cost-effectiveness, and security. Several metaheuristic algorithms have been developed to address this challenge, but they all suffer from either being stuck in local minima, having an insufficiently fast convergence rate, or having a prohibitively high computational cost. Therefore, in this study, the performance of four recently published metaheuristic algorithms, namely the mantis search algorithm (MSA), spider wasp optimizer (SWO), nutcracker optimization algorithm (NOA), and artificial gorilla optimizer (GTO), is assessed to solve this problem with the purpose of minimizing power losses and voltage deviation. These algorithms were chosen due to the robustness of their local optimality avoidance and convergence speed acceleration mechanisms. In addition, a modified variant of NOA, known as MNOA, is herein proposed to further improve its performance. This modified variant does not combine the information of the newly generated solution with the current solution to avoid falling into local minima and accelerate the convergence speed. However, MNOA still needs further improvement to strengthen its performance for large-scale problems, so it is integrated with a newly proposed improvement mechanism to promote its exploration and exploitation operators; this hybrid variant was called HNOA. These proposed algorithms are used to estimate potential solutions to the ORPD problem in small-scale, medium-scale, and large-scale systems and are being tested and validated on the IEEE 14-bus, IEEE 39-bus, IEEE 57-bus, IEEE 118-bus, and IEEE 300-bus electrical power systems. In comparison to eight rival optimizers, HNOA is superior for large-scale systems (IEEE 118-bus and 300-bus systems) at optimizing power losses and voltage deviation; MNOA performs better for medium-scale systems (IEEE 57-bus); and MSA excels for small-scale systems (IEEE 14-bus and 39-bus systems).},
  archive      = {J_AIR},
  author       = {Abdel-Basset, Mohamed and Mohamed, Reda and Hezam, Ibrahim M. and Sallam, Karam M. and Alshamrani, Ahmad M. and Hameed, Ibrahim A.},
  doi          = {10.1007/s10462-024-10982-1},
  journal      = {Artificial Intelligence Review},
  month        = {1},
  number       = {1},
  pages        = {1-48},
  shortjournal = {Artif. Intell. Rev.},
  title        = {Artificial intelligence-based optimization techniques for optimal reactive power dispatch problem: A contemporary survey, experiments, and analysis},
  volume       = {58},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Improved sandcat swarm optimization algorithm for solving global optimum problems. <em>AIR</em>, <em>58</em>(1), 1-68. (<a href='https://doi.org/10.1007/s10462-024-10986-x'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The sand cat swarm optimization algorithm (SCSO) is a metaheuristic algorithm proposed by Amir Seyyedabbasi et al. SCSO algorithm mimics the predatory behavior of sand cats, which gives the algorithm a strong optimized performance. However, as the number of iterations of the algorithm increases, the moving efficiency of the sand cat decreases, resulting in the decline of search ability. The convergence speed of the algorithm gradually decreases, and it is easy to fall into local optimum, and it is difficult to find a better solution. In order to improve the search and movement efficiency of the sand cat, and enhance the global optimization ability and convergence performance of the algorithm, an improved sand cat Swarm Optimization (ISCSO) algorithm was proposed. In ISCSO algorithm, we propose a low-frequency noise search strategy and a spiral contraction walking strategy according to the habit of sand cat, and add random opposition-based learning and restart strategy. The frequency factor was used to control the search direction of the sand cat, and the spiral contraction hunting was carried out, which effectively improved the randomness of the population, expanded the search range of the algorithm, enhanced the moving efficiency of the sand cat, and accelerated the convergence speed of the algorithm. We use 23 standard benchmark functions and IEEE CEC2014 benchmark functions to compare ISCSO with 10 algorithms, and prove the effectiveness of the improved strategy. Finally, ISCSO was evaluated using five constrained engineering design problems. In the results of these problems, using ISCSO has 3.08%, 0.23%, 0.37%, 22.34%, 1.38% improvement compared with the original algorithm respectively, which proves the effectiveness of the improved strategy in practical application problems. The source code website for ISCSO is https://github.com/Ruiruiz30/ISCSO-s-code.},
  archive      = {J_AIR},
  author       = {Jia, Heming and Zhang, Jinrui and Rao, Honghua and Abualigah, Laith},
  doi          = {10.1007/s10462-024-10986-x},
  journal      = {Artificial Intelligence Review},
  month        = {1},
  number       = {1},
  pages        = {1-68},
  shortjournal = {Artif. Intell. Rev.},
  title        = {Improved sandcat swarm optimization algorithm for solving global optimum problems},
  volume       = {58},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Uncovering suggestions in MOOC discussion forums: A transformer-based approach. <em>AIR</em>, <em>58</em>(1), 1-23. (<a href='https://doi.org/10.1007/s10462-024-10997-8'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The field of natural language processing has experienced significant advances in recent years, but these advances have not yet resulted in improved analytics for instructors on MOOC platforms. Valuable information, such as suggestions, is generated in the comment forums of these courses, but due to their volume, manual processing is often impractical. This study examines the feasibility of fine-tuning and effectively utilizing state-of-the-art deep learning models to identify comments that contain suggestions in MOOC forums. The main challenges encountered are the lack of labeled datasets from the MOOC context for fine-tuning classification models and the soaring computational cost of this training. For this study, we manually collected and labeled 2228 comments in Spanish and English from 5 MOOCs and scraped 1.4 million MOOC reviews from 3 platforms. We fine-tuned and evaluated 4 pretrained models based on the transformer architecture and 3 traditional machine learning models to compare their effectiveness in the suggestion mining task in this domain. Transformer-based models proved to be highly effective in this task/domain combination, achieving performance levels that matched or exceeded those deemed appropriate in other contexts and were significantly greater than those achieved by traditional models. Domain adaptation led to improved linguistic understanding of the target domain; however, in this project, this approach did not translate into an observable improvement in suggestion mining. The automated identification of comments that can be labeled as suggestions can result in considerable time savings for instructors, especially considering that less than a quarter of the analyzed comments contain suggestions.},
  archive      = {J_AIR},
  author       = {Reina Sánchez, Karen and Vaca Serrano, Gonzalo and Arbáizar Gómez, Juan Pedro and Duran-Heras, Alfonso},
  doi          = {10.1007/s10462-024-10997-8},
  journal      = {Artificial Intelligence Review},
  month        = {1},
  number       = {1},
  pages        = {1-23},
  shortjournal = {Artif. Intell. Rev.},
  title        = {Uncovering suggestions in MOOC discussion forums: A transformer-based approach},
  volume       = {58},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Toward robust decision-making under multiple evaluation scenarios with a novel fuzzy ranking approach: Green supplier selection study case. <em>AIR</em>, <em>58</em>(1), 1-27. (<a href='https://doi.org/10.1007/s10462-024-11006-8'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the evolving field of decision-making, the continuous advancement of technologies and methodologies drives the pursuit of more reliable tools. Decision support systems (DSS) provide information to make informed choices and multi-criteria decision analysis (MCDA) methods are an important component of defining decision models. Despite their usefulness, there are still challenges in making robust decisions in dynamic environments due to the varying performance of different MCDA methods. It creates space for the development of techniques to aggregate conflicting results. This paper introduces a fuzzy ranking approach for aggregating results from multi-criteria assessments, specifically addressing the limitations of current result aggregation techniques. Unlike conventional methods, the proposed approach represents rankings as fuzzy sets, providing detailed insights into the robustness of decision problems. The study uses green supplier selection as a case study, examining the performance of the introduced approach and the robustness of its recommendations within the sustainability field. This study offers a new methodology for aggregating results from multiple evaluation scenarios, thereby enhancing decision-maker awareness and robustness. Through comparative analysis with traditional compromise solution methods, this paper highlights the limitations of current approaches and indicates the advantages of adopting fuzzy ranking aggregation. This study significantly advances the field of decision-making by enhancing the understanding of the stability of decision outcomes.},
  archive      = {J_AIR},
  author       = {Więckowski, Jakub and Wątróbski, Jarosław and Sałabun, Wojciech},
  doi          = {10.1007/s10462-024-11006-8},
  journal      = {Artificial Intelligence Review},
  month        = {1},
  number       = {1},
  pages        = {1-27},
  shortjournal = {Artif. Intell. Rev.},
  title        = {Toward robust decision-making under multiple evaluation scenarios with a novel fuzzy ranking approach: Green supplier selection study case},
  volume       = {58},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A review of deep learning-based stereo vision techniques for phenotype feature and behavioral analysis of fish in aquaculture. <em>AIR</em>, <em>58</em>(1), 1-61. (<a href='https://doi.org/10.1007/s10462-024-10960-7'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The industrialization, high-density, and greener aquaculture requires a more precise and intelligent aquaculture management. Phenotypic and behavioral information of fish, which can reflect fish growth and welfare status, play a crucial role in aquaculture management. Stereo vision technology, which simulates parallax perception of the human eye, can obtain the three-dimensional phenotypic characteristics and movement trajectories of fish through different types of sensors. It can overcome the limitations in dealing with fish deformation, frequent occlusions and understanding three-dimension scenes compared to the traditional two-dimensional computer vision techniques. With the deep learning development and application in aquaculture, stereo vision has become a super computer vision technology that can provide more precise and interpretable information for intelligent aquaculture management, such as size estimation, counting and behavioral analysis of fish. Hence, it is very beneficial for researchers, managers, and entrepreneurs to possess a thorough comprehension about the fast-developing stereo vision technology for modern aquaculture. This study provides a critical review of relevant topics, including the four-layer application structure of stereo vision technology in aquaculture, various deep learning-based technologies used, and specific application scenarios. The review contributes to research development by identifying the current challenges and provide valuable suggestions for future research directions. This review can serve as a useful resource for developing future studies and applications of stereo vision technology in smart aquaculture, focusing on phenotype feature extraction and behavioral analysis of fish.},
  archive      = {J_AIR},
  author       = {Zhao, Yaxuan and Qin, Hanxiang and Xu, Ling and Yu, Huihui and Chen, Yingyi},
  doi          = {10.1007/s10462-024-10960-7},
  journal      = {Artificial Intelligence Review},
  month        = {1},
  number       = {1},
  pages        = {1-61},
  shortjournal = {Artif. Intell. Rev.},
  title        = {A review of deep learning-based stereo vision techniques for phenotype feature and behavioral analysis of fish in aquaculture},
  volume       = {58},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Reinforcement learning in sentiment analysis: A review and future directions. <em>AIR</em>, <em>58</em>(1), 1-40. (<a href='https://doi.org/10.1007/s10462-024-10967-0'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Sentiment analysis in natural language processing (NLP) is used to understand the polarity of human emotions (e.g., positive and negative) and preferences (e.g., price and quality). Reinforcement learning (RL) enables a decision maker (or agent) to observe the operating environment (or the current state) and select the optimal action to receive feedback signals (or reward) from the operating environment. Deep reinforcement learning (DRL) extends RL with deep neural networks (i.e., main and target networks) to capture the state information of inputs and address the curse of dimensionality issue of RL. In sentiment analysis, RL and DRL reduce the need for a large labeled dataset and linguistic resources, increasing scalability and preserving the context and order of logical partitions. Through enhancement, the RL and DRL algorithms identify negations, enhance the quality of the generated responses, predict the logical partitions, remove the irrelevant aspects, and ultimately capture the correct sentiment polarity. This paper presents a review of RL and DRL models and algorithms with their objectives, applications, datasets, performance, and open issues in sentiment analysis.},
  archive      = {J_AIR},
  author       = {Eyu, Jer Min and Yau, Kok-Lim Alvin and Liu, Lei and Chong, Yung-Wey},
  doi          = {10.1007/s10462-024-10967-0},
  journal      = {Artificial Intelligence Review},
  month        = {1},
  number       = {1},
  pages        = {1-40},
  shortjournal = {Artif. Intell. Rev.},
  title        = {Reinforcement learning in sentiment analysis: A review and future directions},
  volume       = {58},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Sequential rough set: A conservative extension of pawlak’s classical rough set. <em>AIR</em>, <em>58</em>(1), 1-33. (<a href='https://doi.org/10.1007/s10462-024-10976-z'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Rough set theory is an important approach to deal with uncertainty in data mining. However, Pawlak’s classical rough set has low fault-tolerance on concept approximation based on knowledge granules, which may influence the classification accuracy in practical application. To address this problem, the present paper proposes a novel sequential rough-set model that is proved to be a conservative extension of Pawlak’s classical rough set. As a result, it effectively improves the fault-tolerance ability, classification accuracy and concept approximation accuracy of the latter without any additional assumption. Based on the properties and theoretical analysis of the proposed model, an algorithm is presented to automatically determine the sequential thresholds and compute the three regions for the given concept. Experiments on real data verify the validity of the algorithm, and also show the stable improvement on the two types of accuracy.},
  archive      = {J_AIR},
  author       = {Xu, Wenyan and Yan, Yucong and Li, Xiaonan},
  doi          = {10.1007/s10462-024-10976-z},
  journal      = {Artificial Intelligence Review},
  month        = {1},
  number       = {1},
  pages        = {1-33},
  shortjournal = {Artif. Intell. Rev.},
  title        = {Sequential rough set: A conservative extension of pawlak’s classical rough set},
  volume       = {58},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Revisiting streaming anomaly detection: Benchmark and evaluation. <em>AIR</em>, <em>58</em>(1), 1-24. (<a href='https://doi.org/10.1007/s10462-024-10995-w'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Anomaly detection in streaming data is an important task for many real-world applications, such as network security, fraud detection, and system monitoring. However, streaming data often exhibit concept drift, which means that the data distribution changes over time. This poses a significant challenge for many anomaly detection algorithms, as they need to adapt to the evolving data to maintain high detection accuracy. Existing streaming anomaly detection algorithms lack a unified evaluation framework that validly assesses their performance and robustness under different types of concept drifts and anomalies. In this paper, we conduct a systematic technical review of the state-of-the-art methods for anomaly detection in streaming data. We propose a new data generator, called SCAR (Streaming data generator with Customizable Anomalies and concept dRifts), that can synthesize streaming data based on synthetic and real-world datasets from different domains. Furthermore, we adapt four static anomaly detection models to the streaming setting using a generic reconstruction strategy as baselines, and then compare them systematically with 9 existing streaming anomaly detection algorithms on 76 synthesized datasets that have various types of anomalies and concept drifts. The challenges and future research directions for anomaly detection in streaming data are also presented.},
  archive      = {J_AIR},
  author       = {Cao, Yang and Ma, Yixiao and Zhu, Ye and Ting, Kai Ming},
  doi          = {10.1007/s10462-024-10995-w},
  journal      = {Artificial Intelligence Review},
  month        = {1},
  number       = {1},
  pages        = {1-24},
  shortjournal = {Artif. Intell. Rev.},
  title        = {Revisiting streaming anomaly detection: Benchmark and evaluation},
  volume       = {58},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Deep emotion recognition in textual conversations: A survey. <em>AIR</em>, <em>58</em>(1), 1-37. (<a href='https://doi.org/10.1007/s10462-024-11010-y'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Emotion Recognition in Conversations (ERC) is a key step towards successful human–machine interaction. While the field has seen tremendous advancement in the last few years, new applications and implementation scenarios present novel challenges and opportunities. These range from leveraging the conversational context, speaker, and emotion dynamics modelling, to interpreting common sense expressions, informal language, and sarcasm, addressing challenges of real-time ERC, recognizing emotion causes, different taxonomies across datasets, multilingual ERC, and interpretability. This survey starts by introducing ERC, elaborating on the challenges and opportunities of this task. It proceeds with a description of the emotion taxonomies and a variety of ERC benchmark datasets employing such taxonomies. This is followed by descriptions comparing the most prominent works in ERC with explanations of the neural architectures employed. Then, it provides advisable ERC practices towards better frameworks, elaborating on methods to deal with subjectivity in annotations and modelling and methods to deal with the typically unbalanced ERC datasets. Finally, it presents systematic review tables comparing several works regarding the methods used and their performance. Benchmarking these works highlights resorting to pre-trained Transformer Language Models to extract utterance representations, using Gated and Graph Neural Networks to model the interactions between these utterances, and leveraging Generative Large Language Models to tackle ERC within a generative framework. This survey emphasizes the advantage of leveraging techniques to address unbalanced data, the exploration of mixed emotions, and the benefits of incorporating annotation subjectivity in the learning phase.},
  archive      = {J_AIR},
  author       = {Pereira, Patrícia and Moniz, Helena and Carvalho, Joao Paulo},
  doi          = {10.1007/s10462-024-11010-y},
  journal      = {Artificial Intelligence Review},
  month        = {1},
  number       = {1},
  pages        = {1-37},
  shortjournal = {Artif. Intell. Rev.},
  title        = {Deep emotion recognition in textual conversations: A survey},
  volume       = {58},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Artificial intelligence application and high-performance work systems in the manufacturing sector: A moderated-mediating model. <em>AIR</em>, <em>58</em>(1), 1-28. (<a href='https://doi.org/10.1007/s10462-024-11013-9'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This empirical investigation examines the complex dynamics between Artificial Intelligence (AI), Potential Development (PD), Training Initiatives (TI), and High-Performance Work Systems (HPWS) within manufacturing firms to gain valuable insights into how AI technologies influence high-performance work systems through employee development and training. Using a purposive sampling technique, around two hundred employees from twenty-four manufacturing firms in the textile, automotive, steel, and pharmaceutical sectors participated in the self-administered survey. The empirical analysis of the data sets was conducted using the PLS-SEM approach. This result demonstrated positive associations between AI, PD, and HPWS, emphasizing the key role of AI in supporting employee development and improving high-performance work systems. Furthermore, training’s amplification effect on the relation between artificial intelligence and professional development highlighted the significance of employees’ upskilling for AI integration. Conversely, the mediating role of PD between AI adoption and HPWS effectiveness highlighted the significant role of employee professional development in achieving HPWS through AI integration within the systems. The study offered insight into the mediation of PD between AI and HPWS effectiveness, emphasizing its centrality in translating AI-driven advances into tangible organizational outcomes. The study findings have significant ramifications for both theory and practice. Theoretically, this research adds to an evolving dialogue surrounding AI’s effects on HR practices and organizational outcomes; practically speaking, organizations can utilize this research’s insights in strategically integrating AI technologies, designing tailored training programs for their employees, and creating an environment conducive to ongoing employee development.},
  archive      = {J_AIR},
  author       = {Zahoor, Sajjad and Chaudhry, Iffat Sabir and Yang, Shuili and Ren, Xiaoyan},
  doi          = {10.1007/s10462-024-11013-9},
  journal      = {Artificial Intelligence Review},
  month        = {1},
  number       = {1},
  pages        = {1-28},
  shortjournal = {Artif. Intell. Rev.},
  title        = {Artificial intelligence application and high-performance work systems in the manufacturing sector: A moderated-mediating model},
  volume       = {58},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Fuzzy task assignment in heterogeneous distributed multi-robot system. <em>AIR</em>, <em>58</em>(1), 1-33. (<a href='https://doi.org/10.1007/s10462-024-10977-y'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This study addresses the problem of coordination in cooperative multi-robot systems performing complex tasks. An analysis of cooperative behavior in mobile multi-robot systems in terms of task execution accuracy by heterogeneous robots is carried out. In addition, we evaluate the capacity and compatibility of tasks assigned to robots to optimize task execution without using direct communication with the robots or a central decision-making unit. A model for task selection in heterogeneous distributed multi-robot systems is proposed. It is based on two processes: the first decomposes complex tasks into elementary tasks, and the second assigns elementary tasks to mobile robots for real-time execution. The distribution of elementary tasks is NP-hard, which leads us to recommend approximate solutions. A fuzzy system called Fuzzy Decision Making in Task Selection is proposed, which uses fuzzy logic to solve this problem. This system allows robots to choose to perform any task in the future. An approach is presented that uses two cascading fuzzy systems. The first calculates the utility of the robot and then activates the second fuzzy system to calculate the utility of the task. By using the output of the fuzzy decision system in our model, each robot will be able to decide for itself which tasks to perform. The results of a simulation of mobile robots transporting goods demonstrate the effectiveness of this fuzzy decision-maker.},
  archive      = {J_AIR},
  author       = {Khelifa, Rechache and Hamza, Teggar and Fatma, Boufera},
  doi          = {10.1007/s10462-024-10977-y},
  journal      = {Artificial Intelligence Review},
  month        = {1},
  number       = {1},
  pages        = {1-33},
  shortjournal = {Artif. Intell. Rev.},
  title        = {Fuzzy task assignment in heterogeneous distributed multi-robot system},
  volume       = {58},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Extra dimension algorithm: A breakthrough for optimization and enhancing DNN efficiency. <em>AIR</em>, <em>58</em>(1), 1-35. (<a href='https://doi.org/10.1007/s10462-024-10991-0'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Proposing an efficient meta-heuristic to improve the inputs of a trainer in deep neural network (DNNs) is significant. According to the Kaluza’s theory, there exists an extra dimension in the universe. This paper proposes a novel algorithm, extra dimension algorithm (EDA), which is simulated based on this theory. The proposed algorithm utilizes the extra dimension to evaluate the current region of solutions and determine the best direction to follow for the next step of the process. Finally, EDA is used to improve inputs of DNN in the process of solving optimization test problems. The same DNN with and without EDA is used to solve extensive optimization problems, including energy-related tasks. The efficiency of EDA in DNN is assessed by solving some test problems in references, the feasibility and efficiency of solutions, within a suitable number of iterations are demonstrated according to the results. The contributions of this paper are as follows: (1) Introduction of the EDA based on Kaluza’s theory. (2) Application of EDA to enhance the performance of DNNs. (3) Demonstration of EDA’s effectiveness in solving complex optimization problems. (4) Comprehensive evaluation of EDA’s impact on energy optimization problems and other test cases. (5) EDA achieved an average improvement of 15% in optimization accuracy and reduced convergence time compared to the best-performing alternatives.},
  archive      = {J_AIR},
  author       = {Hosseini, Eghbal and Al-Ghaili, Abbas M. and Kadir, Dler Hussein and Jamil, Norziana and Deveci, Muhammet and Gunasekaran, Saraswathy Shamini and Razali, Rina Azlin},
  doi          = {10.1007/s10462-024-10991-0},
  journal      = {Artificial Intelligence Review},
  month        = {1},
  number       = {1},
  pages        = {1-35},
  shortjournal = {Artif. Intell. Rev.},
  title        = {Extra dimension algorithm: A breakthrough for optimization and enhancing DNN efficiency},
  volume       = {58},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Robustness in deep learning models for medical diagnostics: Security and adversarial challenges towards robust AI applications. <em>AIR</em>, <em>58</em>(1), 1-107. (<a href='https://doi.org/10.1007/s10462-024-11005-9'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The current study investigates the robustness of deep learning models for accurate medical diagnosis systems with a specific focus on their ability to maintain performance in the presence of adversarial or noisy inputs. We examine factors that may influence model reliability, including model complexity, training data quality, and hyperparameters; we also examine security concerns related to adversarial attacks that aim to deceive models along with privacy attacks that seek to extract sensitive information. Researchers have discussed various defenses to these attacks to enhance model robustness, such as adversarial training and input preprocessing, along with mechanisms like data augmentation and uncertainty estimation. Tools and packages that extend the reliability features of deep learning frameworks such as TensorFlow and PyTorch are also being explored and evaluated. Existing evaluation metrics for robustness are additionally being discussed and evaluated. This paper concludes by discussing limitations in the existing literature and possible future research directions to continue enhancing the status of this research topic, particularly in the medical domain, with the aim of ensuring that AI systems are trustworthy, reliable, and stable.},
  archive      = {J_AIR},
  author       = {Javed, Haseeb and El-Sappagh, Shaker and Abuhmed, Tamer},
  doi          = {10.1007/s10462-024-11005-9},
  journal      = {Artificial Intelligence Review},
  month        = {1},
  number       = {1},
  pages        = {1-107},
  shortjournal = {Artif. Intell. Rev.},
  title        = {Robustness in deep learning models for medical diagnostics: Security and adversarial challenges towards robust AI applications},
  volume       = {58},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Machine learning techniques for coffee classification: A comprehensive review of scientific research. <em>AIR</em>, <em>58</em>(1), 1-45. (<a href='https://doi.org/10.1007/s10462-024-11004-w'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the realm of agribusiness, transformative shifts are underway, propelled by the growing demands and expanding scales of grain production. This evolution calls for a critical reevaluation of the existing paradigms in coffee production and marketing paradigms, with a specific focus on integrating Artificial Intelligence (AI). This work aims to review, synthesize, and summarize the available data regarding how Machine Learning (ML) has been used to detect and classify characteristics in coffee beans and leaves. For this purpose, a comprehensive literature review of the most significant research contributions describing the application of AI for advanced classification techniques in coffee agriculture has been carried out. Our analysis suggests that implementing AI technologies allows the classification of coffee, encompassing various attributes such as maturity, roast intensity, disease identification, flavor profiles, and overall quality. More largely, this technological advancement holds the potential to revolutionize coffee farming by providing producers and agricultural specialists with sophisticated tools to enhance production efficiency, minimize costs, and improve the accuracy and confidence of their decision-making processes. The motivation for the literature review is to address the increasing global demands and evolving scales of grain production, particularly in coffee farming, by critically reevaluating existing paradigms and integrating AI techniques. This review aims to synthesize and summarize how ML has been utilized to detect and classify various characteristics of coffee beans and leaves, thereby highlighting the potential of AI to revolutionize coffee farming by enhancing production efficiency, minimizing costs, and improving decision-making accuracy. This article presents the latest studies in ML in the coffee area, observes the methodology used, and allows researchers to develop new solutions that cover gaps in the literature, open problems, challenges, and future trends, bringing a real contribution to the scientific field. Finally, this article gathers and presents the databases used in many studies, which may be useful for future ML projects.},
  archive      = {J_AIR},
  author       = {Motta, Isabela V. C. and Vuillerme, Nicolas and Pham, Huy-Hieu and de Figueiredo, Felipe A. P.},
  doi          = {10.1007/s10462-024-11004-w},
  journal      = {Artificial Intelligence Review},
  month        = {1},
  number       = {1},
  pages        = {1-45},
  shortjournal = {Artif. Intell. Rev.},
  title        = {Machine learning techniques for coffee classification: A comprehensive review of scientific research},
  volume       = {58},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A systematic literature review of recent advances on context-aware recommender systems. <em>AIR</em>, <em>58</em>(1), 1-53. (<a href='https://doi.org/10.1007/s10462-024-10939-4'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recommender systems are software mechanisms whose usage is to offer suggestions for different types of entities like products, services, or contacts that could be useful or interesting for a specific user. Other ways have been explored in the field to enhance the power of these systems by integrating the context as an additional attribute. This inclusion tries to extract the user preferences more accurately taking into account multiple components such as temporal, spatial, or social ones. Notwithstanding the magnitude of context-awareness in this area, the research community is in agreement with the lack of framework for context information and how to integrate it into recommender systems. Under this premise, this paper focuses on a comprehensive systematic literature review of the state-of-the-art recommendation techniques and their characteristics to benefit from contextual information. The following survey presents the following contributions as outcomes of our study: (i) determine a framework where multiple aspects are taken into account to have a clear definition of context representation, (ii) the techniques used to incorporate context, and (iii) the evaluation of these methods in terms of reproducibility and effectiveness. Our review also covers some crucial topics about context integration, classification of the contexts, application domains, and evaluation of the used datasets, metrics, and code implementations, where we observed clear shiftings in algorithmic and evaluation trends towards Neural Network approaches and ranking metrics, respectively. Just as importantly, future research opportunities and directions are exposed as final closure, standing out the exploitation of various data sources and the scalability and customization of existing solutions.},
  archive      = {J_AIR},
  author       = {Mateos, Pablo and Bellogín, Alejandro},
  doi          = {10.1007/s10462-024-10939-4},
  journal      = {Artificial Intelligence Review},
  month        = {1},
  number       = {1},
  pages        = {1-53},
  shortjournal = {Artif. Intell. Rev.},
  title        = {A systematic literature review of recent advances on context-aware recommender systems},
  volume       = {58},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A multi-strategy boosted bald eagle search algorithm for global optimization and constrained engineering problems: Case study on MLP classification problems. <em>AIR</em>, <em>58</em>(1), 1-50. (<a href='https://doi.org/10.1007/s10462-024-10957-2'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Bald Eagle Search (BES) algorithm is an innovative population-based method inspired by the intelligent hunting behavior of bald eagles. While BES shows promise, it faces challenges such as susceptibility to local optima and imbalances between exploration and exploitation phases. To address these limitations, this paper introduces the Multi-Strategy Boosted Bald Eagle Search (MBBES) algorithm. MBBES enhances the original BES by incorporating an adaptive parameter, two distinct mutation strategies, and replacing the swoop stage with a fall stage. We rigorously evaluate MBBES against classic and improved algorithms using the CEC2014 and CEC2017 test sets. The experimental results demonstrate that MBBES significantly improves the ability to escape local optima and achieves superior convergence accuracy. Moreover, MBBES ranks first according to the Friedman test, outperforming its counterparts in solving five practical engineering problems and three MLP classification problems, underscoring its effectiveness in real-world optimization scenarios. These findings indicate that MBBES not only surpasses BES but also sets a new benchmark in optimization performance.},
  archive      = {J_AIR},
  author       = {Zheng, Rong and Li, Ruikang and Hussien, Abdelazim G. and Hamad, Qusay Shihab and Al-Betar, Mohammed Azmi and Che, Yan and Wen, Hui},
  doi          = {10.1007/s10462-024-10957-2},
  journal      = {Artificial Intelligence Review},
  month        = {1},
  number       = {1},
  pages        = {1-50},
  shortjournal = {Artif. Intell. Rev.},
  title        = {A multi-strategy boosted bald eagle search algorithm for global optimization and constrained engineering problems: Case study on MLP classification problems},
  volume       = {58},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Improving ranking-based question answering with weak supervision for low-resource qur’anic texts. <em>AIR</em>, <em>58</em>(1), 1-33. (<a href='https://doi.org/10.1007/s10462-024-10964-3'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This work tackles the challenge of ranking-based machine reading comprehension (MRC), where a question answering (QA) system generates a ranked list of relevant answers for each question instead of simply extracting a single answer. We highlight the limitations of traditional learning methods in this setting, particularly under limited training data. To address these issues, we propose a novel ranking-inspired learning method that focuses on ranking multiple answer spans instead of single answer extraction. This method leverages lexical overlap as weak supervision to guide the ranking process. We evaluate our approach on the Qur’an Reading Comprehension Dataset (QRCD), a low-resource Arabic dataset over the Holy Qur’an. We employ transfer learning with external resources to fine-tune various transformer-based models, mitigating the low-resource challenge. Experimental results demonstrate that our proposed method significantly outperforms standard mechanisms across different models. Furthermore, we show its better alignment with the ranking-based MRC task and the effectiveness of external resources for this low-resource dataset. Our best performing model achieves a state-of-the-art partial Reciprocal Rank (pRR) score of 63.82%, surpassing the previous best-known score of 58.60%. To foster further research, we release code [GitHub repository: github.com/mohammed-elkomy/weakly-supervised-mrc ], trained models, and detailed experiments to the community.},
  archive      = {J_AIR},
  author       = {ElKoumy, Mohammed and Sarhan, Amany},
  doi          = {10.1007/s10462-024-10964-3},
  journal      = {Artificial Intelligence Review},
  month        = {1},
  number       = {1},
  pages        = {1-33},
  shortjournal = {Artif. Intell. Rev.},
  title        = {Improving ranking-based question answering with weak supervision for low-resource qur’anic texts},
  volume       = {58},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Federated learning design and functional models: Survey. <em>AIR</em>, <em>58</em>(1), 1-38. (<a href='https://doi.org/10.1007/s10462-024-10969-y'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Federated learning is a multiple device collaboration setup designed to solve machine learning problems under framework for aggregation and knowledge transfer in distributed local data. This distributed model ensures the privacy of data at each local node. Owing to its relevance, there has been extensive research activities and outcomes in federated learning with expanded applicability to different areas by the research community. As such, there is a vast research archive made available by the community with research work and articles related to the various aspects of federated learning such as applications, challenges, privacy, functionalities, and design. With respect to the function and design of federated learning, client selection, aggregation, knowledge transfer, management of distributed data (Non-IID), Incentive of data and communication cost are of paramount importance. Any effective design of federated learning requires these aspects to be well considered. There are numerous survey articles found among the available literature that focus on its application and challenges, opportunities, data privacy and protection, as well as on federated learning on internet of things, federated learning on edge computing, etc. In this paper, a review of the available literature on the various elements of design and functionalities in federated learning has been carried out with an aim to lay emphasis on the important challenges and research opportunities. More specifically, this work has endeavored to understand and summarize the various functional methods available, along with their techniques and goals. Additionally, it has strived to get a bird’s eye view of how various functions and designs of federated learning have been used in applications, and how it has helped uncover challenges and promising research directions for the future.},
  archive      = {J_AIR},
  author       = {Ayeelyan, John and Utomo, Sapdo and Rouniyar, Adarsh and Hsu, Hsiu-Chun and Hsiung, Pao-Ann},
  doi          = {10.1007/s10462-024-10969-y},
  journal      = {Artificial Intelligence Review},
  month        = {1},
  number       = {1},
  pages        = {1-38},
  shortjournal = {Artif. Intell. Rev.},
  title        = {Federated learning design and functional models: Survey},
  volume       = {58},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Escape: An optimization method based on crowd evacuation behaviors. <em>AIR</em>, <em>58</em>(1), 1-60. (<a href='https://doi.org/10.1007/s10462-024-11008-6'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Meta-heuristic algorithms, particularly those based on swarm intelligence, are highly effective for solving black-box optimization problems. However, maintaining a balance between exploration and exploitation within these algorithms remains a significant challenge. This paper introduces a useful algorithm, called Escape or Escape Algorithm (ESC), inspired by crowd evacuation behavior, to solve real-world cases and benchmark problems. The ESC algorithm simulates the behavior of crowds during the evacuation, where the population is divided into calm, herding, and panic groups during the exploration phase, reflecting different levels of decision-making and emotional states. Calm individuals guide the crowd toward safety, herding individuals imitate others in less secure areas, and panic individuals make volatile decisions in the most dangerous zones. As the algorithm transitions into the exploitation phase, the population converges toward optimal solutions, akin to finding the safest exit. The effectiveness of the ESC algorithm is validated on two adjustable problem size test suites, CEC 2017 and CEC 2022. ESC ranked first in the 10-dimensional, 30-dimensional tests of CEC 2017, and the 10-dimensional and 20-dimensional tests of CEC 2022, and second in the 50-dimensional and 100-dimensional tests of CEC 2017. Additionally, ESC performed exceptionally well, ranking first in the engineering problems of pressure vessel design, tension/compression spring design, and rolling element bearing design, as well as in two 3D UAV path planning problems, demonstrating its efficiency in solving real-world complex problems, particularly complex problems like 3D UAV path planning. Compared with 12 other high-performance, classical, and advanced algorithms, ESC exhibited superior performance in complex optimization problems. The source codes of ESC algorithm will be shared at https://aliasgharheidari.com/ESC.html and other websites.},
  archive      = {J_AIR},
  author       = {Ouyang, Kaichen and Fu, Shengwei and Chen, Yi and Cai, Qifeng and Heidari, Ali Asghar and Chen, Huiling},
  doi          = {10.1007/s10462-024-11008-6},
  journal      = {Artificial Intelligence Review},
  month        = {1},
  number       = {1},
  pages        = {1-60},
  shortjournal = {Artif. Intell. Rev.},
  title        = {Escape: An optimization method based on crowd evacuation behaviors},
  volume       = {58},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Innovative solution suggestions for financing electric vehicle charging infrastructure investments with a novel artificial intelligence-based fuzzy decision-making modelling. <em>AIR</em>, <em>58</em>(1), 1-32. (<a href='https://doi.org/10.1007/s10462-024-11012-w'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The right methods for effective financing of electric vehicle charging infrastructure investments should be identified. However, in the literature, there is no consensus on which funding source would be right for these projects. There is a need for a new study to recommend the most appropriate financing strategy for these projects. Accordingly, the purpose of this study is to identify innovative solutions for financing electric vehicle charging infrastructure investments. A novel fuzzy decision-making model is introduced to reach this objective. Firstly, the weights of experts are calculated using dimension reduction. Secondly, Spherical fuzzy decision matrix is obtained. Thirdly, the criteria in charging infrastructure for electric vehicles are weighted using Spherical fuzzy criteria importance through intercriteria correlation (CRITIC). Fourthly, innovative solutions for financing electric vehicles charging infrastructure are ranked via Spherical fuzzy ranking technique by geometric mean of similarity ratio to optimal solution (RATGOS). The main contribution of this study is that effective strategies can be identified for financing electric vehicle charging infrastructure investments by establishing a novel decision-making model. Most of the existing models in the literature could not consider the weights of the experts. This condition is criticized by different scholar because these experts can have different qualifications. To satisfy this problem, in this study, dimension reduction algorithm with machine learning is taken into consideration to compute thee weights of the experts. The findings demonstrate that the most effective criterion in the innovative financial solution for the charging infrastructure of electric vehicles is determined as “potential income”. According to the ranking results, it is also defined that the most sustainable solution among the innovative strategies for financing the charging infrastructure of electric vehicles is “blockchain technology”.},
  archive      = {J_AIR},
  author       = {Kou, Gang and Eti, Serkan and Yüksel, Serhat and Dinçer, Hasan and Ergün, Edanur and Gökalp, Yaşar},
  doi          = {10.1007/s10462-024-11012-w},
  journal      = {Artificial Intelligence Review},
  month        = {1},
  number       = {1},
  pages        = {1-32},
  shortjournal = {Artif. Intell. Rev.},
  title        = {Innovative solution suggestions for financing electric vehicle charging infrastructure investments with a novel artificial intelligence-based fuzzy decision-making modelling},
  volume       = {58},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Enhancing keratoconus detection with transformer technology and multi-source integration. <em>AIR</em>, <em>58</em>(1), 1-31. (<a href='https://doi.org/10.1007/s10462-024-11016-6'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Keratoconus is a progressive eye disease characterized by the thinning and conical distortion of the cornea, leading to visual impairment. Early and accurate detection is essential for effective management and treatment. Traditional diagnostic methods, relying primarily on corneal topography, often fail to detect early-stage keratoconus due to their subjective nature and limited scope. In this research, we present a novel multi-source detection approach utilizing transformer technology to predict keratoconus progression more accurately. By integrating and analyzing diverse data sources, including corneal topography, aberrometry, pachymetry, and biomechanical properties, our method captures subtle changes indicative of disease progression. Transformer networks, known for their capability to model complex dependencies in data, are employed to handle the multimodal datasets effectively. Experimental results demonstrate that our approach significantly outperforms existing methods, such as SVM-based, Random Forests-based, and CNN-based models, in terms of accuracy, precision, recall, and F-score. Moreover, the proposed system exhibits lower execution times, highlighting its efficiency in clinical settings. This innovative methodology holds the potential to revolutionize keratoconus management by enabling earlier and more precise interventions, ultimately enhancing patient outcomes and contributing significantly to both the medical and machine learning communities.},
  archive      = {J_AIR},
  author       = {Ismael, Osama},
  doi          = {10.1007/s10462-024-11016-6},
  journal      = {Artificial Intelligence Review},
  month        = {1},
  number       = {1},
  pages        = {1-31},
  shortjournal = {Artif. Intell. Rev.},
  title        = {Enhancing keratoconus detection with transformer technology and multi-source integration},
  volume       = {58},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Transit search algorithm based on oscillation exploitation factor and roche limit for wireless sensor network deployment optimization. <em>AIR</em>, <em>58</em>(1), 1-51. (<a href='https://doi.org/10.1007/s10462-024-10951-8'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To optimize the deployment of nodes in Wireless Sensor Networks (WSN) and effectively control network node energy consumption, thereby improving the quality of perception services, a Transit search algorithm based on oscillation exploitation factor and Roche limit is proposed. The Roche limit-inspired approach enhances the stellar phase of the algorithm, accelerating the convergence rate in the mid-to-late stages of iteration while ensuring adequate exploration of the solution space. Subsequently, five weakening oscillation development factors are introduced to refine the algorithm’s exploitation phase and improve its fine-tuning accuracy. To validate the effectiveness of these strategies, various approaches are applied to optimize the coverage, waste rate and energy consumption in two models of WSN deployment, with connectivity recorded. The comparison reveals the optimal improved algorithm, SEROTS, which enhances coverage by 1.34% in the obstacle-free model compared to the original TS algorithm, with waste and energy consumption rates reduced by 2.05% and 0.00016%, respectively. In the obstacle model, coverage increases by 1.49%, while waste and energy consumption rates decrease by 6.96% and 0.0004%, respectively. To demonstrate the efficiency of the improved algorithm in optimizing WSN deployment, SEROTS is compared with four optimization algorithms: Egret Swarm Optimization Algorithm (ESOA), Honey Badger Algorithm (HBA), Sparrow Search Algorithm (SSA) and Differential Evolution (DE). Two models are selected, integrating the three objectives into a single objective function. Simulation results indicate that SEROTS performs best in both models, with an improvement of 0.53% and 0.79% over the second-best algorithm, respectively. Furthermore, the proposed strategies are compared with simulation results from five other studies, achieving higher coverage rates by 1.57%, 3.33%, 0.87%, 3.81% and 0.21%, respectively. Finally, experiments discuss the application in large-scale scenarios, verifying the feasibility and efficiency of the SEROTS algorithm in WSN deployment optimization.},
  archive      = {J_AIR},
  author       = {Xing, Yu-Xuan and Wang, Jie-Sheng and Zhang, Si-Wen and Zhang, Shi-Hui and Ma, Xin-Ru and Zhang, Yun-Hao},
  doi          = {10.1007/s10462-024-10951-8},
  journal      = {Artificial Intelligence Review},
  month        = {1},
  number       = {1},
  pages        = {1-51},
  shortjournal = {Artif. Intell. Rev.},
  title        = {Transit search algorithm based on oscillation exploitation factor and roche limit for wireless sensor network deployment optimization},
  volume       = {58},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Scene reconstruction techniques for autonomous driving: A review of 3D gaussian splatting. <em>AIR</em>, <em>58</em>(1), 1-33. (<a href='https://doi.org/10.1007/s10462-024-10955-4'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As the latest research result of the explicit radiated field technology, 3D Gaussian Splatting (3D GS) replaces the implicit expression represented by Neural Radiated Field (NeRF) and has become the hottest research direction in 3D scene reconstruction. Given the innovative work and vigorous development of 3D GS in autonomous driving, this paper comprehensively reviews and summarizes the existing related research to showcase the evolution of the 3D GS technology and possible future development directions. First, the overall research background of 3D GS is introduced based on two aspects 3D scene reconstruction methods and 3D GS research progress. Second, the relevant knowledge points of 3D GS and the core formulas to clarify the mathematical mechanism of 3D GS are presented. Third, the primary applications of the 3D scene reconstruction technology based on 3D GS in automatic driving are presented through new perspective synthesis, scene understanding, and simultaneous localization and map building (SLAM). Finally, the research frontier directions of 3D GS in autonomous driving are described, including structure optimization, 4D scene reconstruction, and cross-domain research. This paper may provide an effective and convenient pathway for researchers to understand, explore, apply this novel research method, and promote the development and application of 3D GS in automatic driving.},
  archive      = {J_AIR},
  author       = {Zhu, Huixin and Zhang, Zhili and Zhao, Junyang and Duan, Hui and Ding, Yao and Xiao, Xiongwu and Yuan, Junsong},
  doi          = {10.1007/s10462-024-10955-4},
  journal      = {Artificial Intelligence Review},
  month        = {1},
  number       = {1},
  pages        = {1-33},
  shortjournal = {Artif. Intell. Rev.},
  title        = {Scene reconstruction techniques for autonomous driving: A review of 3D gaussian splatting},
  volume       = {58},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Integrated MADM approach based on extended MABAC method with Aczel–Alsina generalized weighted bonferroni mean operator. <em>AIR</em>, <em>58</em>(1), 1-45. (<a href='https://doi.org/10.1007/s10462-024-10980-3'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Currently, q-rung orthopair (q-ROF) set theory is one of the most effective set theories in dealing uncertainty associated with imprecise information. In complex decision-making problems, input variables can be described by q-ROF numbers to cope ambiguity. While, generalized weighted Bonferroni mean (GWBM) operator can reflect correlation among input arguments. Aczel–Alsina operations underline fair and accurate evaluation of decision-makers. Harnessing these benefits, a pioneering extension of the GWBM operator based on Aczel–Alsina operations is introduced. Simultaneously, a novel generalized distance measure is crafted, drawing inspiration from Dice and Jaccard similarities. Beside these, using stepwise weight assessment ratio analysis (SWARA) and multi-attribute border approximation area comparison (MABAC) methods, this study pioneers an integrated method, q-ROF-SWARA-MABAC for assessing and prioritizing factors and alternatives on q-ROF environment. Later, with the suggested model, a case study on high-speed rail corridor (HSRC) for India is solved, revealing Varanasi-Howrah HSRC as the most preferable choice.. Moving forward, detailed sensitive analysis of suggested model is performed to explore the pertinence and supremacy. Eventually, the outcomes manifest that novel framework is flexible, reliable, accurate and could be viable option to consider for future use.},
  archive      = {J_AIR},
  author       = {Debnath, Kaushik and Roy, Sankar Kumar and Deveci, Muhammet and Tomášková, Hana},
  doi          = {10.1007/s10462-024-10980-3},
  journal      = {Artificial Intelligence Review},
  month        = {1},
  number       = {1},
  pages        = {1-45},
  shortjournal = {Artif. Intell. Rev.},
  title        = {Integrated MADM approach based on extended MABAC method with Aczel–Alsina generalized weighted bonferroni mean operator},
  volume       = {58},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). End-to-end point cloud registration with transformer. <em>AIR</em>, <em>58</em>(1), 1-22. (<a href='https://doi.org/10.1007/s10462-024-10985-y'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the widespread application of large-scale 3D point cloud data in real-world scenarios, efficient and accurate point cloud registration has become a crucial challenge. We propose an end-to-end point cloud registration method based on the Transformer architecture. This method addresses the issues of low overlap and registration in large scenes, exhibiting strong algorithmic versatility and efficiency. We introduce a combination of dynamic position encoding and ternary angular position encoding within the Transformer, effectively enhancing the representation capability of point cloud data and algorithmic generality, thus better tackling point cloud registration challenges in large scenes. Additionally, to enhance the learning capacity of the attention mechanism, we employ an improved cross-attention mechanism that multiplies the softmax with adaptive weights, enabling the model to capture key information within the point cloud more accurately. In the decoding stage, we introduce a multi-scale feature fusion approach that fully exploits the multi-layer information in point cloud data, further improving registration accuracy and robustness. Through the fusion of multi-scale features, we effectively mitigate information loss and handle matching problems between point clouds of varying sizes. Experimental results demonstrate the excellence of our method in addressing low overlap and registration tasks in large scenes, validated across multiple datasets including 3DMatch, ModelNet, KITTI, and MVP-RG.},
  archive      = {J_AIR},
  author       = {Wang, Yong and Zhou, Pengbo and Geng, Guohua and An, Li and Zhang, Qi},
  doi          = {10.1007/s10462-024-10985-y},
  journal      = {Artificial Intelligence Review},
  month        = {1},
  number       = {1},
  pages        = {1-22},
  shortjournal = {Artif. Intell. Rev.},
  title        = {End-to-end point cloud registration with transformer},
  volume       = {58},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Deep learning-based time series forecasting. <em>AIR</em>, <em>58</em>(1), 1-67. (<a href='https://doi.org/10.1007/s10462-024-10989-8'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the advancement of deep learning algorithms and the growing availability of computational power, deep learning-based forecasting methods have gained significant importance in the domain of time series forecasting. In the past decade, there has been a rapid rise in time series forecasting approaches. This paper comprehensively reviews the advancements in deep learning-based forecasting models spanning 2014 to 2024. We provide a comprehensive examination of the capabilities of these models in capturing correlations among time steps and time series variables. Additionally, we explore methods to enhance the efficiency of long-term time series forecasting and summarize the diverse loss functions employed in these models. Moreover, this study systematically evaluates the effectiveness of these approaches in both univariate and multivariate time series forecasting tasks across diverse domains. We comprehensively discuss the strengths and limitations of various algorithms from multiple perspectives, analyze their capacity to capture different types of time series information, including trend and season patterns, and compare methods for enhancing the computational efficiency of these models. Finally, we summarize the experimental results and discuss the future directions in time series forecasting. Codes and datasets are available at https://github.com/TCCofWANG/Deep-Learning-based-Time-Series-Forecasting .},
  archive      = {J_AIR},
  author       = {Song, Xiaobao and Deng, Liwei and Wang, Hao and Zhang, Yaoan and He, Yuxin and Cao, Wenming},
  doi          = {10.1007/s10462-024-10989-8},
  journal      = {Artificial Intelligence Review},
  month        = {1},
  number       = {1},
  pages        = {1-67},
  shortjournal = {Artif. Intell. Rev.},
  title        = {Deep learning-based time series forecasting},
  volume       = {58},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Applications, technologies, and evaluation methods in smart aquaponics: A systematic literature review. <em>AIR</em>, <em>58</em>(1), 1-52. (<a href='https://doi.org/10.1007/s10462-024-11003-x'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Smart aquaponics systems are gaining popularity as they contribute immensely to sustainable food production. These systems enhance traditional farming with advanced technologies like the Internet of Things (IoT), solar energy, and Artificial Intelligence (AI) for increased proficiency and productivity. However, assessing the performance and effectiveness of these systems is challenging. A systematic literature review (SLR) was conducted to examine the applications, technologies, and evaluation methods used in smart aquaponics. The study sourced peer-reviewed publications from IEEE Xplore, Scopus, SpringerLink and Science Direct. After applying inclusion and exclusion criteria, a total of 105 primary studies were selected for the SLR. The findings show that aquaponics predictions (27%) have been under-explored compared to applications that involved monitoring or monitoring and controlling aquaponics (73%). IoT technologies have been used to create prototype aquaponic systems and collect data, while machine learning/deep learning (predictive analytics) are used for prediction, abnormality detection, and intelligent decision-making. So far, predictive analytics solutions for aquaponics yield prediction, return-on-investment (ROI) estimates, resource optimisation, product marketing, security of aquaponics systems, and sustainability assessment have received very little attention. Also, few studies (37.7%) incorporated any form of evaluation of the proposed solutions, while expert feedback and usability evaluation, which involved stakeholders and end-users of aquaponics solutions, have been rarely used for their assessment. In addition, existing smart aquaponics studies have limitations in terms of their short-term focus (monitoring and controlling of aquaponics not undertaken over a long time to assess performance and sustainability), being conducted mostly in controlled settings (which limits applicability to diverse conditions), and being focused on specific geographical contexts(which limits their generalizability). These limitations provide opportunities for future research. Generally, this study provides new insights and expands discussion on the topic of smart aquaponics.},
  archive      = {J_AIR},
  author       = {Anila, Mundackal and Daramola, Olawande},
  doi          = {10.1007/s10462-024-11003-x},
  journal      = {Artificial Intelligence Review},
  month        = {1},
  number       = {1},
  pages        = {1-52},
  shortjournal = {Artif. Intell. Rev.},
  title        = {Applications, technologies, and evaluation methods in smart aquaponics: A systematic literature review},
  volume       = {58},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Deep learning adversarial attacks and defenses in autonomous vehicles: A systematic literature review from a safety perspective. <em>AIR</em>, <em>58</em>(1), 1-53. (<a href='https://doi.org/10.1007/s10462-024-11014-8'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The integration of Deep Learning (DL) algorithms in Autonomous Vehicles (AVs) has revolutionized their precision in navigating various driving scenarios, ranging from anti-fatigue safe driving to intelligent route planning. Despite their proven effectiveness, concerns regarding the safety and reliability of DL algorithms in AVs have emerged, particularly in light of the escalating threat of adversarial attacks, as emphasized by recent research. These digital or physical attacks present formidable challenges to AV safety, relying extensively on collecting and interpreting environmental data through integrated sensors and DL. This paper addresses this pressing issue through a systematic survey that meticulously explores robust adversarial attacks and defenses, specifically focusing on DL in AVs from a safety perspective. Going beyond a review of existing research papers on adversarial attacks and defenses, the paper introduces a safety scenarios taxonomy matrix Inspired by SOTIF designed to augment the safety of DL in AVs. This matrix categorizes safety scenarios into four distinct areas and classifies attacks into those areas in three scenarios, along with two defense scenarios. Furthermore, the paper investigates the testing and evaluation measurements critical for assessing attacks in the context of DL for AVs. It further explores the dynamic landscape of datasets and simulation platforms. This contribution significantly enriches the ongoing discourse surrounding the assurance of safety and reliability in autonomous vehicles, especially in the face of continually evolving adversarial challenges.},
  archive      = {J_AIR},
  author       = {Ibrahum, Ahmed Dawod Mohammed and Hussain, Manzoor and Hong, Jang-Eui},
  doi          = {10.1007/s10462-024-11014-8},
  journal      = {Artificial Intelligence Review},
  month        = {1},
  number       = {1},
  pages        = {1-53},
  shortjournal = {Artif. Intell. Rev.},
  title        = {Deep learning adversarial attacks and defenses in autonomous vehicles: A systematic literature review from a safety perspective},
  volume       = {58},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A survey on deep 3D human pose estimation. <em>AIR</em>, <em>58</em>(1), 1-53. (<a href='https://doi.org/10.1007/s10462-024-11019-3'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {3D Human Pose Estimation (3D-HPE) is a highly active and evolving research area in computer vision with numerous applications such as extended reality, action recognition, and video surveillance. The field has significantly advanced with deep learning, public datasets, and enhanced computational power, addressing challenges like depth ambiguity, occlusion, and data scarcity. Researchers confront scenario-specific issues such as ill-posed problems in monocular setups, cross-view aggregation with camera synchronizations in multi-view systems, and inter-person occlusion in multi-person scenarios. This survey comprehensively reviews contemporary strategies covering a technological spectrum including Convolutional Neural Networks, Graph Convolutional Networks, Transformers, and their combinations employed to address these challenges. It includes scenarios such as monocular and multi-view setups, single and multi-person cases, as well as image and video inputs. The survey explores various solution paradigms, including single-stage vs 2D-to-3D lifting, absolute vs relative keypoints, pixel vs voxel vs Neural Radiance Field spaces, and deterministic, probabilistic, or diffusion-based strategies, along with top-down vs bottom-up approaches. It examines advanced learning techniques beyond supervised methods and data augmentation for diverse pose datasets. It analyzes the performance of recent methods on benchmark datasets for different scenarios. Challenges are categorized into common and scenario-specific issues, and future research directions are proposed to foster further advancements in the field. Additionally, key sections are summarized in tables or visual formats for quick understanding. This survey is a valuable resource and a solid reference for researchers in the dynamic landscape of 3D human pose estimation.},
  archive      = {J_AIR},
  author       = {Neupane, Rama Bastola and Li, Kan and Boka, Tesfaye Fenta},
  doi          = {10.1007/s10462-024-11019-3},
  journal      = {Artificial Intelligence Review},
  month        = {1},
  number       = {1},
  pages        = {1-53},
  shortjournal = {Artif. Intell. Rev.},
  title        = {A survey on deep 3D human pose estimation},
  volume       = {58},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Optimizing seismic-based reservoir property prediction: A synthetic data-driven approach using convolutional neural networks and transfer learning with real data integration. <em>AIR</em>, <em>58</em>(1), 1-27. (<a href='https://doi.org/10.1007/s10462-024-11030-8'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Reservoir characterization through seismic data analysis is essential for exploration and production in the petroleum industry. However, seismic-to-well tie discrepancies, limited availability of high-quality well data, and resolution constraints pose a reliability challenge. While previous studies offer valuable insights, they still struggle to achieve high-resolution predictions in a complex geologically environment given high reliance on well data. This study integrates synthetic data-driven techniques with real data, including convolutional neural networks (CNN) and transfer learning, to improve seismic reservoir characterization. We utilize nearby well statistics and a rock physics model (RPM) to simulate pseudo wells representing various geological scenarios. Synthetic seismic gathers are generated from these pseudo wells, which are based on RPM and local well control, to train the CNN. Transfer learning is then applied to adapt the CNN to better distinguish between real and synthetic data, enhancing reservoir predictions. A comparative analysis of P-impedance predictions from three methodologies: theory-driven Pre-Stack-Seismic-Inversion (TDSI), Deep-Neural-Network (DNN), and our CNN approach, showed that CNN achieved nearly 97% prediction accuracy with low error rates, compared to relatively lower prediction accuracy rates of DNN (86.2%) and TDSI (81.5%) with high error rates, according to robust metrics including R-square, RMSE, MSE, and MAE. These results indicate that CNN not only enhanced resolution but also closely aligned with well data and superior lateral continuity, even in blind well scenarios. This study effectively integrates synthetic data-driven techniques with CNNs and transfer learning to advance seismic reservoir property prediction, offering a robust solution to overcome limitations in traditional and DNN-based approaches.},
  archive      = {J_AIR},
  author       = {Ali, Muhammad and Changxingyue, He and Wei, Ning and Jiang, Ren and Zhu, Peimin and Hao, Zhang and Hussain, Wakeel and Ashraf, Umar},
  doi          = {10.1007/s10462-024-11030-8},
  journal      = {Artificial Intelligence Review},
  month        = {1},
  number       = {1},
  pages        = {1-27},
  shortjournal = {Artif. Intell. Rev.},
  title        = {Optimizing seismic-based reservoir property prediction: A synthetic data-driven approach using convolutional neural networks and transfer learning with real data integration},
  volume       = {58},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Data augmentation in predictive maintenance applicable to hydrogen combustion engines: A review. <em>AIR</em>, <em>58</em>(1), 1-24. (<a href='https://doi.org/10.1007/s10462-024-11021-9'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Machine-learning-based predictive maintenance models, i.e. models that predict breakdowns of machines based on condition information, have a high potential to minimize maintenance costs in industrial applications by determining the best possible time to perform maintenance. Modern machines have sensors that can collect all relevant data of the operating condition and for legacy machines which are still widely used in the industry, retrofit sensors are readily, easily and inexpensively available. With the help of this data it is possible to train such a predictive maintenance model. The main problem is that most data is obtained from normal operating conditions, whereas only limited data are from failures. This leads to highly unbalanced data sets, which makes it very difficult, if not impossible, to train a predictive maintenance model that can detect faults reliably and timely. Another issue is the lack of available real data due to privacy concerns. To address these problems, a suitable data generation strategy is needed. In this work, a literature review is conducted to identify a solution approach for a suitable data augmentation strategy that can be applied to our specific use case of hydrogen combustion engines in the automotive field. This literature review shows that, among the different state-of-the-art proposals, the most promising for the generation of reliable synthetic data are the ones based on generative models. The analysis of the different metrics used in the state of the art allows to identify the most suitable ones to evaluate the quality of generated signals. Finally, an open problem in research in this area is identified and it is the need to validate the plausibility of the data generated. The generation of results in this area will contribute decisively to the development of predictive maintenance models.},
  archive      = {J_AIR},
  author       = {Schwarz, Alexander and Rahal, Jhonny Rodriguez and Sahelices, Benjamín and Barroso-García, Verónica and Weis, Ronny and Duque Antón, Simon},
  doi          = {10.1007/s10462-024-11021-9},
  journal      = {Artificial Intelligence Review},
  month        = {1},
  number       = {1},
  pages        = {1-24},
  shortjournal = {Artif. Intell. Rev.},
  title        = {Data augmentation in predictive maintenance applicable to hydrogen combustion engines: A review},
  volume       = {58},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Generative AI model privacy: A survey. <em>AIR</em>, <em>58</em>(1), 1-47. (<a href='https://doi.org/10.1007/s10462-024-11024-6'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The rapid progress of generative AI models has yielded substantial breakthroughs in AI, facilitating the generation of realistic synthetic data across various modalities. However, these advancements also introduce significant privacy risks, as the models may inadvertently expose sensitive information from their training data. Currently, there is no comprehensive survey work investigating privacy issues, e.g., attacking and defending privacy in generative AI models. We strive to identify existing attack techniques and mitigation strategies and to offer a summary of the current research landscape. Our survey encompasses a wide array of generative AI models, including language models, Generative Adversarial Networks, diffusion models, and their multi-modal counterparts. It indicates the critical need for continued research and development in privacy-preserving techniques for generative AI models. Furthermore, we offer insights into the challenges and discuss the open problems in the intersection of privacy and generative AI models.},
  archive      = {J_AIR},
  author       = {Liu, Yihao and Huang, Jinhe and Li, Yanjie and Wang, Dong and Xiao, Bin},
  doi          = {10.1007/s10462-024-11024-6},
  journal      = {Artificial Intelligence Review},
  month        = {1},
  number       = {1},
  pages        = {1-47},
  shortjournal = {Artif. Intell. Rev.},
  title        = {Generative AI model privacy: A survey},
  volume       = {58},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Deep mining the textual gold in relation extraction. <em>AIR</em>, <em>58</em>(1), 1-69. (<a href='https://doi.org/10.1007/s10462-024-11042-4'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Relation extraction (RE) is a fundamental task in natural language processing (NLP) that seeks to identify and categorize relationships among entities referenced in the text. Traditionally, RE has relied on rule-based systems. Still, recently, a variety of deep learning approaches have been employed, including recurrent neural networks (RNNs), convolutional neural networks (CNNs), and bidirectional encoder representations from transformers (BERT). This review aims to provide a comprehensive overview of relation extraction, focusing on deep learning models. Given the complexity of the RE problem, we will present it from a multi-dimensional perspective, covering model steps, relation types, method types, benchmark datasets, and applications. We will also highlight both historical and current research in the field, identifying promising research areas for further development and emerging directions. Specifically, we will focus on potential enhancements for relation extraction from poorly labeled data and provide a detailed assessment of current shortcomings in handling complex real-world situations.},
  archive      = {J_AIR},
  author       = {Sharma, Tanvi and Emmert-Streib, Frank},
  doi          = {10.1007/s10462-024-11042-4},
  journal      = {Artificial Intelligence Review},
  month        = {1},
  number       = {1},
  pages        = {1-69},
  shortjournal = {Artif. Intell. Rev.},
  title        = {Deep mining the textual gold in relation extraction},
  volume       = {58},
  year         = {2025},
}
</textarea>
</details></li>
</ul>

</body>
</html>
