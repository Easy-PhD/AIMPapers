<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>JMUI</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="jmui">JMUI - 17</h2>
<ul>
<li><details>
<summary>
(2025). How bare-hand clicking influences eye movement behavior in virtual button selection tasks: A comparative analysis with keyboard control. <em>JMUI</em>, <em>19</em>(3), 285-304. (<a href='https://doi.org/10.1007/s12193-025-00455-2'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The flexibility of virtual reality systems has fostered the development of various interaction techniques, yet our understanding of users’ eye movement behaviors with different interaction methods remains limited. This study compares eye movement details in goal-driven and stimulus-based virtual button selection tasks, using both direct bare-hand interaction and indirect keyboard control in virtual environments. Our findings show that both interface type and interaction mode significantly affect multiple eye movement metrics, including saccades, fixations, and blinks. These insights enhance our understanding of visual attention and action execution in virtual selection tasks, offering data-driven conclusions that can serve as a foundation for developing design guidelines for interaction techniques and interface designs in virtual reality. We believe these results will contribute to the creation of more natural, efficient, and user-friendly virtual reality systems, further advancing the technology.},
  archive      = {J_JMUI},
  author       = {Du, Xiaoxi and Jia, Lesong and Wu, Jinchun and Peng, Ningyue and Zhou, Xiaozhou and Xue, Chengqi},
  doi          = {10.1007/s12193-025-00455-2},
  journal      = {Journal on Multimodal User Interfaces},
  month        = {9},
  number       = {3},
  pages        = {285-304},
  shortjournal = {J. Multimodal User Interfaces},
  title        = {How bare-hand clicking influences eye movement behavior in virtual button selection tasks: A comparative analysis with keyboard control},
  volume       = {19},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Web-based multimodal learning system to develop social communication skills. <em>JMUI</em>, <em>19</em>(3), 271-284. (<a href='https://doi.org/10.1007/s12193-025-00460-5'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Virtual agents offer a scalable and cost-effective alternative to traditional human-led social skills training, which is often limited by the availability of professional trainers. Our web-based learning system, developed following Bellack et al.’s training model, integrates speech recognition, response selection, speech synthesis, and nonverbal behavior generation to provide automated training. To evaluate its effectiveness, we conducted a four-week study with 60 Japanese participants from the general population, focusing on four key social skills. Participants completed questionnaires assessing autistic traits, social anxiety, and changes in social communication skills post-training. Results demonstrated significant improvements, with notable results in participants’ confidence in declining requests. These findings highlight the potential of web-based virtual agents for enhancing social communication skills and suggest promising applications for social communication research and intervention programs.},
  archive      = {J_JMUI},
  author       = {Tanaka, Hiroki and Lisitsyna, Alexandra},
  doi          = {10.1007/s12193-025-00460-5},
  journal      = {Journal on Multimodal User Interfaces},
  month        = {9},
  number       = {3},
  pages        = {271-284},
  shortjournal = {J. Multimodal User Interfaces},
  title        = {Web-based multimodal learning system to develop social communication skills},
  volume       = {19},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Exploring user interactions with commercial machines via real-world application logs in the lab. <em>JMUI</em>, <em>19</em>(3), 253-269. (<a href='https://doi.org/10.1007/s12193-025-00456-1'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Log analysis is an effective user experience testing method for exploring subtle user interactions with commercial machines. The rapid iteration of commercial machines and their applications calls for a more cost-effective alternative to traditional in-the-wild approaches. However, lab-based log studies typically rely on customized testing programs, which do not represent real-world usage. Therefore, this study investigates the feasibility of log analysis using real-world applications in lab settings to explore user interaction. We present a case study exploring smartphone gesture interaction in more than 50 everyday operations across 10 real-world applications. We identified and analyzed detailed gesture parameters from log data on three common gesture sets, including tapping (single and double), zooming (pinching and spreading), and swiping (horizontal and vertical). Grounding in related works, we discuss observed behavior patterns, their potential causes, and implications for future design in commercial machine interfaces. We highlight the applicability of the presented approach by discussing its advantages, direction for improvements, and potential applications. We call for more studies to explore the norms and values of this approach.},
  archive      = {J_JMUI},
  author       = {Song, Fangli and Wang, Wei and Zhou, Dasen and Bryan-Kinns, Nick and Zhang, Jun and Chen, Qi and Du, Le},
  doi          = {10.1007/s12193-025-00456-1},
  journal      = {Journal on Multimodal User Interfaces},
  month        = {9},
  number       = {3},
  pages        = {253-269},
  shortjournal = {J. Multimodal User Interfaces},
  title        = {Exploring user interactions with commercial machines via real-world application logs in the lab},
  volume       = {19},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Effects of music tempo reflecting group activity on multi-participant exercise. <em>JMUI</em>, <em>19</em>(3), 235-251. (<a href='https://doi.org/10.1007/s12193-025-00457-0'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {During training, it is common to perform certain movements repeatedly within a short period—such as squats, jumping jacks, and burpees—where the frequency and rhythm of the exercises significantly impact their effectiveness. In this study, we focused on group training sessions involving repetitive exercises—a frequent scenario in gyms, home workouts, and remote training environments. Specifically, we investigated the potential of using music tempo to enhance group training effectiveness, diverging from traditional methods that primarily rely on visual cues for guidance and synchronization. We conducted a user study in an in-person group exercise setting with 18 participants organized into six groups of three. They performed squats under conditions with and without music aligned to their average tempo. We then examined how adding group-synchronized musical feedback—on top of natural interactions such as observing each other’s movements and listening to footsteps—affected training outcomes. By analyzing skeletal data and questionnaire responses, we found that music tempo contributed to improvements in both movement synchronization and aspects of the overall training experience. We also gathered suggestions for future enhancements in music-based support, including applying the proposed method to groups with smaller physical ability differences and considering differences in users’ musical backgrounds.},
  archive      = {J_JMUI},
  author       = {Wang, Ruiyun and Jin, Yuchen and Huang, Jiayun and Takahashi, Shin},
  doi          = {10.1007/s12193-025-00457-0},
  journal      = {Journal on Multimodal User Interfaces},
  month        = {9},
  number       = {3},
  pages        = {235-251},
  shortjournal = {J. Multimodal User Interfaces},
  title        = {Effects of music tempo reflecting group activity on multi-participant exercise},
  volume       = {19},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Vis-assist: Computer vision and haptic feedback-based wearable assistive device for visually impaired. <em>JMUI</em>, <em>19</em>(2), 217-234. (<a href='https://doi.org/10.1007/s12193-025-00452-5'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Visual impairment affects millions of people worldwide, posing significant challenges in their daily lives and personal safety. While assistive technologies, both wearable and non-wearable, can help mitigate these challenges, wearable devices offer the advantage of hands-free operation. In this context, we present Vis-Assist, a novel wearable visual assistive device capable of detecting and classifying objects, measuring their distances, and providing real-time haptic feedback through a vibration motor array, all using an integrated low-cost computational unit without the need for external servers. Our study distinguishes itself by utilizing haptic feedback to convey object information, allowing visually impaired individuals to discern between 19 different object classes following a brief training period. Haptic feedback offers an alternative to audio that doesn’t block hearing and can be used alongside it, serving as a complementary solution. The performance of the developed wearable device was evaluated through two types of experiments with four participants. The results demonstrate that users can identify the location of objects and thereby prevent collisions with obstacles. The experiments conducted demonstrate that users, on average, can locate a predefined object, such as a chair, within a 40 $$\hbox {m}^{2}$$ vacant space in under 94 seconds. Furthermore, users exhibit proficiency in finding objects while navigating around obstacles in the same environment, achieving this task in less than 121 seconds on average. The system developed here has high potential to help the self-navigation of visually impaired people and make their daily lives easier. To facilitate further research in this field, the complete source code for this study has been made publicly available on GitHub.},
  archive      = {J_JMUI},
  author       = {Dede, Ibrahim and Gumus, Abdurrahman},
  doi          = {10.1007/s12193-025-00452-5},
  journal      = {Journal on Multimodal User Interfaces},
  month        = {6},
  number       = {2},
  pages        = {217-234},
  shortjournal = {J. Multimodal User Interfaces},
  title        = {Vis-assist: Computer vision and haptic feedback-based wearable assistive device for visually impaired},
  volume       = {19},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Identification of visual stimuli is improved by accompanying auditory stimuli through directing eye movement: An investigation in perceptual-cognitive skills. <em>JMUI</em>, <em>19</em>(2), 201-216. (<a href='https://doi.org/10.1007/s12193-025-00451-6'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This study investigated the effect of sound-related signals (auditory) whose onset was concurrent with the eye movement, a transparent red patch (visual), and their combination together (audio-visual) when used to orient attention toward the key areas of the body during the sport of badminton. The results revealed that each of these stimuli alone improved perceptual-cognitive skills. Eye-movement sound decreased decision times for identification of shot direction with no expense to anticipation. The sound helped participants find the main spatial locations essential for the anticipation of shot direction. Moreover, the facilitation role of the sound was observed in gaze behavior characteristics. These results were not changed by altering the test environment from a video-based to an actual situation one with existing anxiety. We assume that the sound-related signal may guide attention to main locations and may advance attentional `disengagement' which, as a result, participants can recognize the badminton shot direction with more speed and accuracy.},
  archive      = {J_JMUI},
  author       = {Khalaji, Maryam and Dehkordi, Parvaneh Shamsipour and Aghdaei, Mahin and Farsi, Alireza and Neumann, David L.},
  doi          = {10.1007/s12193-025-00451-6},
  journal      = {Journal on Multimodal User Interfaces},
  month        = {6},
  number       = {2},
  pages        = {201-216},
  shortjournal = {J. Multimodal User Interfaces},
  title        = {Identification of visual stimuli is improved by accompanying auditory stimuli through directing eye movement: An investigation in perceptual-cognitive skills},
  volume       = {19},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Effect of semantic augmented reality label on stable common ground establishment. <em>JMUI</em>, <em>19</em>(2), 169-199. (<a href='https://doi.org/10.1007/s12193-025-00454-3'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Object referencing is a critical component of effective remote instruction. However, instructors and workers often struggle with this process due to a lack of common ground-a shared understanding of how to formulate mutually understandable expressions. This study proposes semantic augmented reality (AR) labels that attach semantically meaningful content to objects to support the establishment of conceptual pacts and common ground about how to refer to the objects between remote instructors and workers. To evaluate the effectiveness of this approach, we conducted a between-participants experiment with 72 participants (18 groups per condition), who completed three trials of a remote assembly task. In the first two trials, participants performed the instruction task using either the proposed Semantic Label or a conventional method, Simple Label, which attaches alphabetical characters to objects. In the third trial, all participants performed the task without AR labels. The results showed that both AR labeling methods supported the establishment of conceptual pacts and yielded similar performance in terms of task efficiency and mental workload. However, in the third trial, only participants who had used Semantic Labels maintained high communication efficiency, suggesting that Semantic Labels also serve as a catalyst for developing lasting common ground.},
  archive      = {J_JMUI},
  author       = {Wang, Tzu-Yang and Otsuki, Mai and Kuzuoka, Hideaki},
  doi          = {10.1007/s12193-025-00454-3},
  journal      = {Journal on Multimodal User Interfaces},
  month        = {6},
  number       = {2},
  pages        = {169-199},
  shortjournal = {J. Multimodal User Interfaces},
  title        = {Effect of semantic augmented reality label on stable common ground establishment},
  volume       = {19},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Interaction-aware augmented reality interface for assisting erring social robots. <em>JMUI</em>, <em>19</em>(2), 155-167. (<a href='https://doi.org/10.1007/s12193-025-00453-4'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the future, many service jobs will be performed by socially interactive robots. When initially introduced or still learning, such robots may sometimes make incorrect decisions or perform erroneous actions, thereby causing socially inappropriate interactions with guests or customers. To prevent potential problems during interactions, a human worker could be present to help or intervene when necessary. Such assistance would occur directly in front of customers, so the interface used by the human clerk is crucial. In this work, we study the use of an augmented reality (AR) head-mounted display for human clerks to oversee the robot. With such an interface, communication between the human clerk and the robot can be largely hidden from customers, especially in comparison to a traditional screen-based interface. Nonetheless, the interface may still lead to unnatural head movements by the human clerk, potentially negatively affecting customers’ impressions of the interaction. We address this by proposing an interaction-aware gaze guidance method for the AR interface, where the AR objects are moved to ensure that the resulting head movements appear natural. A user study in a sales scenario demonstrated that this method improves both customers’ subjective experience and the usability of the interface.},
  archive      = {J_JMUI},
  author       = {Hosoda, Kyohei and Brščić, Dražen and Kanda, Takayuki},
  doi          = {10.1007/s12193-025-00453-4},
  journal      = {Journal on Multimodal User Interfaces},
  month        = {6},
  number       = {2},
  pages        = {155-167},
  shortjournal = {J. Multimodal User Interfaces},
  title        = {Interaction-aware augmented reality interface for assisting erring social robots},
  volume       = {19},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). AirWhisper: Enhancing virtual reality experience via visual-airflow multimodal feedback. <em>JMUI</em>, <em>19</em>(2), 139-154. (<a href='https://doi.org/10.1007/s12193-024-00438-9'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Virtual reality (VR) technology has been increasingly focusing on incorporating multimodal outputs to enhance the sense of immersion and realism. In this work, we developed AirWhisper, a modular wearable device that provides dynamic airflow feedback to enhance VR experiences. AirWhisper simulates wind from multiple directions around the user’s head via four micro fans and 3D-printed attachments. We applied a Just Noticeable Difference study to support the design of the control system and explore the user’s perception of the characteristics of the airflow in different directions. Through multimodal comparison experiments, we find that vision-airflow multimodality output can improve the user’s VR experience from several perspectives. Finally, we designed scenarios with different airflow change patterns and different levels of interaction to test AirWhisper’s performance in various contexts and explore the differences in users’ perception of airflow under different virtual environment conditions. Our work shows the importance of developing human-centered multimodal feedback adaptive learning models that can make real-time dynamic changes based on the user’s perceptual characteristics and environmental features.},
  archive      = {J_JMUI},
  author       = {Zhao, Fangtao and Li, Ziming and Luo, Yiming and Li, Yue and Liang, Hai-Ning},
  doi          = {10.1007/s12193-024-00438-9},
  journal      = {Journal on Multimodal User Interfaces},
  month        = {6},
  number       = {2},
  pages        = {139-154},
  shortjournal = {J. Multimodal User Interfaces},
  title        = {AirWhisper: Enhancing virtual reality experience via visual-airflow multimodal feedback},
  volume       = {19},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Does mixed reality influence joint action? impact of the mixed reality setup on users’ behavior and spatial interaction. <em>JMUI</em>, <em>19</em>(2), 119-138. (<a href='https://doi.org/10.1007/s12193-024-00445-w'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Understanding how people effectively perform actions together is fundamental when designing Collaborative Mixed Reality (CMR) applications. While most of the studies on CMR mostly considered either how users are immersed in the CMR (e.g., in virtual or augmented reality) or how the physical workspace is shared by users (i.e., distributed or collocated), little is known about how their combination could influence user’s interaction in CMR. In this paper, we present a user study (n = 46, 23 pairs) that investigates the effect of the mixed reality setup on the user’s immersion and spatial interaction during a joint-action task. Groups of two participants had to perform two types of joint actions while carrying a virtual rope to maintain a certain distance: (1) Gate, where participants had to pass through a virtual aperture together, and (2) Fruit, where participants had to use a rope to slice a virtual fruit moving in the CMR. Users were either in a distributed or collocated setup and either immersed in virtual or augmented reality. Our results showed that the immersion type and location setup altered users’ proxemics as well as the users’ subjective experience. In particular, we noticed better task performance when users were in augmented reality and more considerable distances between players while interacting in a distributed setup. These results contribute to the understanding of joint action in CMR and are discussed to improve the design of CMR applications.},
  archive      = {J_JMUI},
  author       = {Brument, Hugo and De Pace, Francesco and Podkosova, Iana},
  doi          = {10.1007/s12193-024-00445-w},
  journal      = {Journal on Multimodal User Interfaces},
  month        = {6},
  number       = {2},
  pages        = {119-138},
  shortjournal = {J. Multimodal User Interfaces},
  title        = {Does mixed reality influence joint action? impact of the mixed reality setup on users’ behavior and spatial interaction},
  volume       = {19},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Impact of communication modalities on social presence and regulation processes in a collaborative game. <em>JMUI</em>, <em>19</em>(1), 101-118. (<a href='https://doi.org/10.1007/s12193-024-00450-z'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the digital era, leveraging communication technologies to foster collaborative learning is of utmost importance. This study explores the impact of different communication modalities, such as text, audio and video, on social presence and regulation processes within a computer-supported collaborative learning (CSCL) environment. Using learning analytics, we examine the influences of these modalities on collaboration and derive recommendations for their optimized use in the design of future CSCL environments. Our findings reveal a significant impact of communication modalities on the sense of social presence and regulation of collaborative activities. Audio communication results in enhanced co-presence, psychobehavioral accessibility, and better regulation processes compared to video and text modalities, indicating that audio is the most suitable modality in collaborative virtual environments for decision-making tasks. Conversely, video communication still facilitated strategic planning and enhanced self-regulation. Chat communication showed the lowest sense of social presence, yet improvements over time suggest that participants adapt to this modality, enhancing their collaborative efficiency.},
  archive      = {J_JMUI},
  author       = {Basille, Anthony and Lavoué, Élise and Serna, Audrey},
  doi          = {10.1007/s12193-024-00450-z},
  journal      = {Journal on Multimodal User Interfaces},
  month        = {3},
  number       = {1},
  pages        = {101-118},
  shortjournal = {J. Multimodal User Interfaces},
  title        = {Impact of communication modalities on social presence and regulation processes in a collaborative game},
  volume       = {19},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Vibration feedback reduces perceived difficulty of virtualized fine motor task. <em>JMUI</em>, <em>19</em>(1), 93-99. (<a href='https://doi.org/10.1007/s12193-024-00449-6'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Virtual Reality (VR) has been increasingly used in the development or rehabilitation of sensorimotor skills as it provides a safe, personalized, repeatable, realistic, and interactive environment. However, the use of VR technology to simulate fine motor interactions is still rather limited. This study evaluated the performance and user experience of a virtualized fine motor task and the potential impact of vibration feedback to complement the VR simulation. The Nine Hole Peg test (NHPT), which is widely used in health care to assess hand motor functions, was considered. 100 healthy subjects were recruited to compare the performance of the conventional, VR-based, and VR-based with vibration feedback (VR+vibration) implementation of the NHPT. Results demonstrated a significant increase in the task execution time (about 50% increase) in VR-based and VR+vibration conditions as compared to the conventional condition (Kruskal Wallis test, Bonferroni correction, p < 0.0001). Participants reported a significant decrease in perceived difficulty of the VR+vibration condition as compared to the VR-based condition (Wilcoxon signed-rank test, p < 0.05). Another interesting finding was the gender effect - female participants spent significantly more time completing the task in VR as compared to their male counterparts. These results indicate that vibration feedback enhances the usability of virtualized fine motor tasks.},
  archive      = {J_JMUI},
  author       = {Park, Wanjoo and Jamil, Muhammad Hassan and Eid, Mohamad},
  doi          = {10.1007/s12193-024-00449-6},
  journal      = {Journal on Multimodal User Interfaces},
  month        = {3},
  number       = {1},
  pages        = {93-99},
  shortjournal = {J. Multimodal User Interfaces},
  title        = {Vibration feedback reduces perceived difficulty of virtualized fine motor task},
  volume       = {19},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Pointing gestures accelerate collaborative problem-solving on tangible user interfaces. <em>JMUI</em>, <em>19</em>(1), 75-92. (<a href='https://doi.org/10.1007/s12193-024-00448-7'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Collaboration is included in the Learning and Innovation skills of the 21st Century. Collaborative problem-solving represents the interaction of two dimensions i) complex problem-solving and ii) collaboration. Technology-based assessment of collaborative problem-solving should focus on both dimensions. We ran user studies with 66 participants at three secondary schools in Luxembourg and Belgium to observe the gestural user behaviour of triads while solving a collaborative problem on a tangible user interface (TUI). Social interactions and embodiment by using gestures are important collaboration channels. Our main objective is the relation between the usage of gestures with collaboration and complex problem-solving performance. Our apparatus to test collaborative problem-solving is a tangible tabletop and a micro-world about power plants. We analysed the videos manually and found correlations between gestures, complex problem-solving performance, and user experience. The results showed that pointing gestures and adaptors significantly correlate with response time of problem-solving. Our results on user experience showed that the use of a TUI was regarded as a novel and straightforward solution that many people could learn to use very quickly. We suggest gesture performance to be considered as one indicator of collaboration.},
  archive      = {J_JMUI},
  author       = {Anastasiou, Dimitra and Maquil, Valérie},
  doi          = {10.1007/s12193-024-00448-7},
  journal      = {Journal on Multimodal User Interfaces},
  month        = {3},
  number       = {1},
  pages        = {75-92},
  shortjournal = {J. Multimodal User Interfaces},
  title        = {Pointing gestures accelerate collaborative problem-solving on tangible user interfaces},
  volume       = {19},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Augmented conversations: AR face filters for facilitating comfortable in-person interactions. <em>JMUI</em>, <em>19</em>(1), 57-74. (<a href='https://doi.org/10.1007/s12193-024-00446-9'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Individuals with social anxiety often experience heightened anxiety during face-to-face conversations due to their fear of negative judgments when perceiving neutral facial expressions from others. This research aims to alleviate this anxiety by introducing a novel approach that involves overlaying visual effects onto the conversation partner via an augmented reality head-mounted display. We developed an AR application for HoloLens 2, allowing users to overlay either an anime-style avatar or a smiling face photo during in-person interactions. We conducted a user study where participants engaged in dyadic conversations using our AR application. 29 participants compared three conditions: control, anime-style avatar, and smiling face photo. The findings reveal two significant outcomes: (1) overlaying an anime-style avatar onto the conversation partner enhances conversational comfort, and (2) individuals with pronounced social interaction anxiety and intense fear of negative evaluation benefit from our AR-based system. This research presents possibilities for practical solutions that could improve the well-being of individuals with social anxiety during in-person conversations.},
  archive      = {J_JMUI},
  author       = {Yoneyama, Juri and Fujimoto, Yuichiro and Okazaki, Kosuke and Sawabe, Taishi and Kanbara, Masayuki and Kato, Hirokazu},
  doi          = {10.1007/s12193-024-00446-9},
  journal      = {Journal on Multimodal User Interfaces},
  month        = {3},
  number       = {1},
  pages        = {57-74},
  shortjournal = {J. Multimodal User Interfaces},
  title        = {Augmented conversations: AR face filters for facilitating comfortable in-person interactions},
  volume       = {19},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). The effects of haptic, visual and olfactory augmentations on food consumed while wearing an extended reality headset. <em>JMUI</em>, <em>19</em>(1), 37-55. (<a href='https://doi.org/10.1007/s12193-024-00447-8'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The current food production system is unsustainable, necessitating a shift towards plant-based diets. Nutritious options fulfill basic needs but may not satisfy hedonic ones. Our novel approach is to promote healthier eating habits without compromising on the pleasantness of eating by using extended reality technologies and multimodal interaction. We present a multisensory augmentation system integrating augmentations in olfaction, touch, and vision. We studied the experience of eating plant-based balls and meatballs. In an experiment with 40 participants, haptic and visual augmentations were found to have significant effects: augmented meatballs and plant-based balls were perceived as bigger and heavier compared to non-augmented versions. However, olfactory augmentation did not produce a similar effect: participants did not notice a stronger aroma with augmented balls compared to non-augmented balls, and the augmented plant-based version had a less appealing scent than its non-augmented counterpart. Moreover, the findings of the study indicate that our multisensory augmentation system had no significant effect on taste perception.},
  archive      = {J_JMUI},
  author       = {Karhu, Natalia and Rantala, Jussi and Farooq, Ahmed and Sand, Antti and Pennanen, Kyösti and Lappi, Jenni and Nayak, Mohit and Sozer, Nesli and Raisamo, Roope},
  doi          = {10.1007/s12193-024-00447-8},
  journal      = {Journal on Multimodal User Interfaces},
  month        = {3},
  number       = {1},
  pages        = {37-55},
  shortjournal = {J. Multimodal User Interfaces},
  title        = {The effects of haptic, visual and olfactory augmentations on food consumed while wearing an extended reality headset},
  volume       = {19},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Design and testing of (A)MICO: A multimodal feedback system to facilitate the interaction between cobot and human operator. <em>JMUI</em>, <em>19</em>(1), 21-36. (<a href='https://doi.org/10.1007/s12193-024-00444-x'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The present work describes the design, development and testing of a multimodal feedback system, named (A)MICO, with visual and acoustic feedback designed to facilitate the interaction of workers with collaborative robots (cobots) in production lines. The feedback is designed to make the human operator more aware of the cobot’s ongoing and future activities, and therefore gain more control over the situation. The ultimate goal is to obtain a new intuitive mode for transferring information through the combination of lights and sounds, not only to facilitate the flow of communication from the cobot to the operator, but also to make the interaction more accessible to neurodivergent groups, such as people with autism spectrum disorders. The design process focused on the evaluation of the human–robot interaction to select the situations where additional information is needed, and which is the best way to transfer messages as intuitively as possible. Potential end-users were actively involved during all stages of the design and development process. Five volunteers with high functioning autism participated in a preliminary co-design to identify the issues related to the interaction with the cobot and the logic of the multimodal signals. Then, to assess the system’s adaptability to several needs and the level of usability in providing information, validation tests were carried out involving a wider group of participants with ASD. The results suggest that the adoption of a multimodal communication strategy can be useful for making the workplace accessible and improving the well-being of all workers.},
  archive      = {J_JMUI},
  author       = {Dei, Carla and Meregalli Falerni, Matteo and Cilsal, Turgut and Redaelli, Davide Felice and Lavit Nicora, Matteo and Chiappini, Mattia and Storm, Fabio Alexander and Malosio, Matteo},
  doi          = {10.1007/s12193-024-00444-x},
  journal      = {Journal on Multimodal User Interfaces},
  month        = {3},
  number       = {1},
  pages        = {21-36},
  shortjournal = {J. Multimodal User Interfaces},
  title        = {Design and testing of (A)MICO: A multimodal feedback system to facilitate the interaction between cobot and human operator},
  volume       = {19},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Assessment of comparative evaluation techniques for signing agents: A study with deaf adults. <em>JMUI</em>, <em>19</em>(1), 1-19. (<a href='https://doi.org/10.1007/s12193-024-00442-z'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Sign languages are considered fully-fledged and complete natural languages that are utilized by individuals who are deaf or hard of hearing as a means of communication within the visual-gestural modality. The utilization of virtual avatars as virtual assistants has witnessed a notable surge over the course of the previous fifteen years. Research on sign language recognition has already shown significant potential in achieving reliable and efficient automatic sign language recognition. Nevertheless, the development of physiologically believable (naturally looking) sign language synthesis and generation techniques is currently in its nascent stages. Moreover, traditional models often are rule-based, rely on manually programmed commands, and require the expertise of proficient interpreters, whereas data-driven approaches have the potential to offer more advanced solutions. In addition to the advancement of sign language systems, scholarly investigations indicate a notable lack in the signing systems evaluation by individuals who utilize sign language (deaf signers and interpreters). In this study, we introduce a sign language interpreting avatar based on data-driven techniques. Additionally, we conduct a subjective evaluation of the avatar’s performance. This paper presents the findings of a study conducted with deaf signers, which aimed to compare three different signing agents to a highly skilled sign language human interpreter. The study utilized well-known metrics that are considered to provide valuable insights into participants’ perceptions of signing agents, also their respective advantages and limitations.},
  archive      = {J_JMUI},
  author       = {Imashev, Alfarabi and Oralbayeva, Nurziya and Baizhanova, Gulmira and Sandygulova, Anara},
  doi          = {10.1007/s12193-024-00442-z},
  journal      = {Journal on Multimodal User Interfaces},
  month        = {3},
  number       = {1},
  pages        = {1-19},
  shortjournal = {J. Multimodal User Interfaces},
  title        = {Assessment of comparative evaluation techniques for signing agents: A study with deaf adults},
  volume       = {19},
  year         = {2025},
}
</textarea>
</details></li>
</ul>

</body>
</html>
