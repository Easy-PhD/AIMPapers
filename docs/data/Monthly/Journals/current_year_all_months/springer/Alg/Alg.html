<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>Alg</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="alg">Alg - 57</h2>
<ul>
<li><details>
<summary>
(2025). On the parameterized complexity of eulerian strong component arc deletion. <em>Alg</em>, <em>87</em>(11), 1669-1709. (<a href='https://doi.org/10.1007/s00453-025-01336-6'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we study the Eulerian Strong Component Arc Deletion problem, where the input is a directed multigraph and the goal is to delete the minimum number of arcs to ensure every strongly connected component of the resulting digraph is Eulerian. This problem is a natural extension of the Directed Feedback Arc Set problem and is also known to be motivated by certain scenarios arising in the study of housing markets. The complexity of the problem, when parameterized by solution size (i.e., size of the deletion set), has remained unresolved and has been highlighted in several papers. In this work, we answer this question by ruling out (subject to the usual complexity assumptions) a fixed-parameter algorithm (FPT algorithm) for this parameter and conduct a broad analysis of the problem with respect to other natural parameterizations. We prove both positive and negative results. Among these, we demonstrate that the problem is also hard (W[1]-hard or even para-NP-hard) when parameterized by either treewidth or maximum degree alone. Complementing our lower bounds, we establish that the problem is in XP when parameterized by treewidth and FPT when parameterized either by both treewidth and maximum degree or by both treewidth and solution size. We show that on simple digraphs, these algorithms have near-optimal asymptotic dependence on the treewidth assuming the Exponential Time Hypothesis.},
  archive      = {J_Alg},
  author       = {Blažej, Václav and Jana, Satyabrata and Ramanujan, M. S. and Strulo, Peter},
  doi          = {10.1007/s00453-025-01336-6},
  journal      = {Algorithmica},
  month        = {11},
  number       = {11},
  pages        = {1669-1709},
  shortjournal = {Algorithmica},
  title        = {On the parameterized complexity of eulerian strong component arc deletion},
  volume       = {87},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). ShockHash: Near optimal-space minimal perfect hashing beyond brute-force. <em>Alg</em>, <em>87</em>(11), 1620-1668. (<a href='https://doi.org/10.1007/s00453-025-01321-z'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A minimal perfect hash function (MPHF) maps a set S of n keys to the first n integers without collisions. There is a lower bound of $$n\log _2e-\mathcal {O}(\log n) \approx 1.44n$$ bits needed to represent an MPHF. This can be reached by a brute-force algorithm that tries $$e^n$$ hash function seeds in expectation and stores the first seed that leads to an MPHF. The most space-efficient previous algorithms for constructing MPHFs all use such a brute-force approach as a basic building block. In this paper, we introduce ShockHash – Small, heavily overloaded cuckoo hash tables for minimal perfect hashing. ShockHash uses two hash functions $$h_0$$ and $$h_1$$ , hoping for the existence of a function $$f : S \rightarrow \{0,1\}$$ such that $$x \mapsto h_{f(x)}(x)$$ is an MPHF on S. It then uses a 1-bit retrieval data structure to store f using $$n + o(n)$$ bits. In graph terminology, ShockHash generates n-edge random graphs until stumbling on a pseudoforest – where each component contains as many edges as nodes. Using cuckoo hashing, ShockHash then derives an MPHF from the pseudoforest in linear time. We show that ShockHash needs to try only about $$(e/2)^n \approx 1.359^n$$ seeds in expectation. This reduces the space for storing the seed by roughly n bits (maintaining the asymptotically optimal space consumption) and speeds up construction by almost a factor of $$2^n$$ compared to brute-force. Bipartite ShockHash reduces the expected construction time again to about $$1.166^n$$ by maintaining a pool of candidate hash functions and checking all possible pairs. Using ShockHash as a building block within the RecSplit framework we obtain ShockHash-RS, which can be constructed up to 3 orders of magnitude faster than competing approaches. ShockHash-RS can build an MPHF for 10 million keys with 1.489 bits per key in about half an hour. When instead using ShockHash after an efficient k-perfect hash function, it achieves space usage similar to the best competitors, while being significantly faster to construct and query.},
  archive      = {J_Alg},
  author       = {Lehmann, Hans-Peter and Sanders, Peter and Walzer, Stefan},
  doi          = {10.1007/s00453-025-01321-z},
  journal      = {Algorithmica},
  month        = {11},
  number       = {11},
  pages        = {1620-1668},
  shortjournal = {Algorithmica},
  title        = {ShockHash: Near optimal-space minimal perfect hashing beyond brute-force},
  volume       = {87},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Achieving tight $$O(4^k)$$ runtime bounds on jumpk by proving that genetic algorithms evolve near-maximal population diversity. <em>Alg</em>, <em>87</em>(11), 1564-1619. (<a href='https://doi.org/10.1007/s00453-025-01323-x'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The $$\textsc {Jump} _k$$ benchmark was the first problem for which crossover was proven to give a speed-up over mutation-only evolutionary algorithms. Jansen and Wegener (Algorithmica 2002) proved an upper bound of $$O(\textrm{poly}(n) + 4^k/p_c)$$ for the ( $$\mu $$ +1) Genetic Algorithm (( $$\mu $$ +1) GA), but only for unrealistically small crossover probabilities $$p_c$$ . To this date, it remains an open problem to prove similar upper bounds for realistic $$p_c$$ ; the best known runtime bound, in terms of function evaluations, for $$p_c = \Omega (1)$$ is $$O((n/\chi )^{k-1})$$ , $$\chi $$ a positive constant. We provide a novel approach and analyse the evolution of the population diversity, measured as sum of pairwise Hamming distances, for a variant of the ( $$\mu $$ +1) GA on $$\textsc {Jump} _k$$ . The ( $$\mu $$ +1)- $${\lambda _c}$$ -GA creates one offspring in each generation either by applying mutation to one parent or by applying crossover $${\lambda _c}$$ times to the same two parents (followed by mutation), to amplify the probability of creating an accepted offspring in generations with crossover. We show that population diversity in the ( $$\mu $$ +1)- $${\lambda _c}$$ -GA converges to an equilibrium of near-perfect diversity. This yields an improved time bound of $$O(\mu n \log (\mu ) + 4^k)$$ function evaluations for a range of k under the mild assumptions $$p_c = O(1/k)$$ and $$\mu \in \Omega (kn)$$ . For all constant k, the restriction is satisfied for some $$p_c = \Omega (1)$$ and it implies that the expected runtime for all constant k and an appropriate $$\mu = \Theta (kn)$$ is bounded by $$O(n^2 \log n)$$ , irrespective of k. For larger k, the expected time of the ( $$\mu $$ +1)- $${\lambda _c}$$ -GA is $$\Theta (4^k)$$ , which is tight for a large class of unbiased black-box algorithms and faster than the original ( $$\mu $$ +1) GA by a factor of $$\Omega (1/p_c)$$ . We also show that our analysis can be extended to other unitation functions such as $$\textsc {Jump} _{k, \delta }$$ and Hurdle.},
  archive      = {J_Alg},
  author       = {Opris, Andre and Lengler, Johannes and Sudholt, Dirk},
  doi          = {10.1007/s00453-025-01323-x},
  journal      = {Algorithmica},
  month        = {11},
  number       = {11},
  pages        = {1564-1619},
  shortjournal = {Algorithmica},
  title        = {Achieving tight $$O(4^k)$$ runtime bounds on jumpk by proving that genetic algorithms evolve near-maximal population diversity},
  volume       = {87},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Smoothed analysis of the 2-opt heuristic for the TSP under gaussian noise. <em>Alg</em>, <em>87</em>(11), 1518-1563. (<a href='https://doi.org/10.1007/s00453-025-01335-7'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The 2-opt heuristic is a very simple local search heuristic for the traveling salesperson problem. In practice it usually converges quickly to solutions within a few percentages of optimality. In contrast to this, its running-time is exponential and its approximation performance is poor in the worst case. Englert, Röglin, and Vöcking (Algorithmica, 2014) provided a smoothed analysis in the so-called one-step model in order to explain the performance of 2-opt on d-dimensional Euclidean instances, both in terms of running-time and in terms of approximation ratio. However, translating their results to the classical model of smoothed analysis, where points are perturbed by Gaussian distributions with standard deviation $$\sigma $$ , yields only weak bounds. We prove bounds that are polynomial in n and $$1/\sigma $$ for the smoothed running-time with Gaussian perturbations. In addition, our analysis for Euclidean distances is much simpler than the existing smoothed analysis. Furthermore, we prove a smoothed approximation ratio of $$O(\log (1/\sigma ))$$ . This bound is almost tight, as we also provide a lower bound of $$\Omega (\frac{\log n}{\log \log n})$$ for $$\sigma = O(1/\sqrt{n})$$ . Our main technical novelty here is that, different from existing smoothed analyses, we do not separately analyze objective values of the global and local optimum on all inputs (which only allows for a bound of $$O(1/\sigma )$$ ), but simultaneously bound them on the same input.},
  archive      = {J_Alg},
  author       = {Künnemann, Marvin and Manthey, Bodo and Veenstra, Rianne},
  doi          = {10.1007/s00453-025-01335-7},
  journal      = {Algorithmica},
  month        = {11},
  number       = {11},
  pages        = {1518-1563},
  shortjournal = {Algorithmica},
  title        = {Smoothed analysis of the 2-opt heuristic for the TSP under gaussian noise},
  volume       = {87},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Boosting double coverage for k-server via imperfect predictions. <em>Alg</em>, <em>87</em>(11), 1477-1517. (<a href='https://doi.org/10.1007/s00453-025-01333-9'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We study the online k-server problem in a learning-augmented setting. While in the traditional online model, an algorithm has no information about the request sequence, we assume that there is given some advice (for example, machine-learned predictions) on an algorithm’s decision. There is, however, no guarantee on the quality of the prediction, and it might be far from being correct. Our main result is a learning-augmented variation of the well-known Double Coverage algorithm for k-server on the line (Chrobak et al. in SIAM J Discret Math 4(2):172–181, 1991) in which we integrate predictions as well as our trust into their quality. We give an error-dependent worst-case performance guarantee, which is a function of a user-defined confidence parameter, and which interpolates smoothly between an optimal performance in case that all predictions are correct, and the best-possible performance regardless of the prediction quality. When given good predictions, we improve upon known lower bounds for online algorithms without advice. We further show that our algorithm achieves for any k almost optimal guarantees, within a class of deterministic learning-augmented algorithms respecting local and memoryless properties. Our algorithm outperforms a previously proposed (more general) learning-augmented algorithm. It is noteworthy that the previous algorithm crucially exploits memory, whereas our algorithm is memoryless. Finally, we demonstrate in experiments the practicability and the superior performance of our algorithm on real-world data.},
  archive      = {J_Alg},
  author       = {Lindermayr, Alexander and Megow, Nicole and Simon, Bertrand},
  doi          = {10.1007/s00453-025-01333-9},
  journal      = {Algorithmica},
  month        = {11},
  number       = {11},
  pages        = {1477-1517},
  shortjournal = {Algorithmica},
  title        = {Boosting double coverage for k-server via imperfect predictions},
  volume       = {87},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Correction: On the parameterized complexity of controlling amendment and successive winners. <em>Alg</em>, <em>87</em>(10), 1474-1475. (<a href='https://doi.org/10.1007/s00453-025-01328-6'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_Alg},
  author       = {Yang, Yongjie},
  doi          = {10.1007/s00453-025-01328-6},
  journal      = {Algorithmica},
  month        = {10},
  number       = {10},
  pages        = {1474-1475},
  shortjournal = {Algorithmica},
  title        = {Correction: On the parameterized complexity of controlling amendment and successive winners},
  volume       = {87},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Interweaving real-time jobs with energy harvesting to maximize throughput. <em>Alg</em>, <em>87</em>(10), 1453-1473. (<a href='https://doi.org/10.1007/s00453-025-01331-x'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Motivated by batteryless IoT devices, we consider the following scheduling problem. The input includes n unit time jobs $$\mathcal{J}= \left\{ J_1, \ldots, J_n \right\} $$ , where each job $$J_i$$ has a release time $$r_i$$ , due date $$d_i$$ , energy requirement $$e_i$$ , and weight $$w_i$$ . We consider time to be slotted; hence, all time related job values refer to slots. Let $$T=\max _i\left\{ d_i \right\} $$ . The input also includes an h(t) value for every time slot t $$\left( 1 \le t \le T \right) $$ , which is the energy harvestable on that slot. Energy is harvested at time slots when no job is executed. The objective is to find a feasible schedule that maximizes the weight of the scheduled jobs. A schedule is feasible if for every job $$J_j$$ in the schedule and its corresponding slot $$t_j$$ , $$t_{j} \ne t_{j'}$$ if $${j} \ne {j'}$$ , $$r_j \le t_j \le d_j$$ , and the available energy before $$t_j$$ is at least $$e_j$$ . To the best of our knowledge, we are the first to consider the theoretical aspects of this problem. In this work we show the following. (1) A polynomial time algorithm when all jobs have identical $$r_i, d_i$$ and $$w_i$$ . (2) A $$\frac{1}{2}$$ -approximation algorithm when all jobs have identical $$w_i$$ but arbitrary $$r_i$$ and $$d_i$$ . (3) An FPTAS when all jobs have identical $$r_i$$ and $$d_i$$ but arbitrary $$w_i$$ . (4) Reductions showing that all the variants of the problem in which at least one of the attributes $$r_i$$ , $$d_i$$ , or $$w_i$$ are not identical for all jobs are $$\textsf{NP-Hard}$$ .},
  archive      = {J_Alg},
  author       = {Schieber, Baruch and Samineni, Bhargav and Vahidi, Soroush},
  doi          = {10.1007/s00453-025-01331-x},
  journal      = {Algorithmica},
  month        = {10},
  number       = {10},
  pages        = {1453-1473},
  shortjournal = {Algorithmica},
  title        = {Interweaving real-time jobs with energy harvesting to maximize throughput},
  volume       = {87},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). The price of hierarchical clustering. <em>Alg</em>, <em>87</em>(10), 1420-1452. (<a href='https://doi.org/10.1007/s00453-025-01327-7'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Hierarchical Clustering is a popular tool for understanding the hereditary properties of a data set. Such a clustering is actually a sequence of clusterings that starts with the trivial clustering in which every data point forms its own cluster and then successively merges two existing clusters until all points are in the same cluster. A hierarchical clustering achieves an approximation factor of $$\alpha $$ if the costs of each k-clustering in the hierarchy are at most $$\alpha $$ times the costs of an optimal k-clustering. We study as cost functions the maximum (discrete) radius of any cluster (k-center problem) and the maximum diameter of any cluster (k-diameter problem). In general, the optimal clusterings do not form a hierarchy and hence an approximation factor of 1 cannot be achieved. We call the smallest approximation factor that can be achieved for any instance the price of hierarchy. For the k-diameter problem we improve the upper bound on the price of hierarchy to $$3+2\sqrt{2}\approx 5.83$$ . Moreover we significantly improve the lower bounds for k-center and k-diameter, proving a price of hierarchy of exactly 4 and $$3+2\sqrt{2}$$ , respectively.},
  archive      = {J_Alg},
  author       = {Arutyunova, Anna and Röglin, Heiko},
  doi          = {10.1007/s00453-025-01327-7},
  journal      = {Algorithmica},
  month        = {10},
  number       = {10},
  pages        = {1420-1452},
  shortjournal = {Algorithmica},
  title        = {The price of hierarchical clustering},
  volume       = {87},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). The farthest color voronoi diagram in the plane. <em>Alg</em>, <em>87</em>(10), 1393-1419. (<a href='https://doi.org/10.1007/s00453-025-01311-1'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The farthest-color Voronoi diagram (FCVD) is defined on a set of n points in the plane, where each point is labeled with one of m colors. The colored points constitute a family $$\mathcal {P}$$ of m clusters (sets) of points in the plane whose farthest-site Voronoi diagram is the FCVD. The diagram finds applications in problems related to facility location, shape matching, data imprecision, and others. In this paper we present structural properties of the FCVD, refine its combinatorial complexity bounds, and present efficient algorithms for its construction. We show that the complexity of the diagram is $$O(n\alpha (m)+\textit{str}(\mathcal {P}))$$ , where $$\textit{str}(\mathcal {P})$$ is a parameter reflecting the number of straddles between pairs of clusters, which is $$O(m(n-m))$$ . The bound reduces to $$O(n+ \textit{str}(\mathcal {P}))$$ if the clusters are pairwise non-crossing. We also present a lower bound, establishing that the complexity of the FCVD can be $$\Omega (n+m^2)$$ , even if the clusters have pairwise disjoint convex hulls. Our algorithm runs in $$O((n+\textit{str}(\mathcal {P}))\log ^3 n)$$ -time, and in certain special cases in $$O(n\log n)$$ time.},
  archive      = {J_Alg},
  author       = {Mantas, Ioannis and Papadopoulou, Evanthia and Silveira, Rodrigo I. and Wang, Zeyu},
  doi          = {10.1007/s00453-025-01311-1},
  journal      = {Algorithmica},
  month        = {10},
  number       = {10},
  pages        = {1393-1419},
  shortjournal = {Algorithmica},
  title        = {The farthest color voronoi diagram in the plane},
  volume       = {87},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Space-efficient data structure for Next/Previous Larger/Smaller value queries. <em>Alg</em>, <em>87</em>(10), 1369-1392. (<a href='https://doi.org/10.1007/s00453-025-01325-9'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Given an array of size n from a total order, we consider the problem of constructing a data structure that supports various queries (range minimum/maximum queries with their variants and next/previous larger/smaller queries) efficiently. In the encoding model (i.e., the queries can be answered without the input array), we propose a $$(3.701n + o(n))$$ -bit data structure, which supports all these queries in $$O(\log ^{(\ell )}n)$$ time, for any positive constant integer $$\ell $$ (here, $$\log ^{(1)} n = \log n$$ , and for $$\ell > 1$$ , $$\log ^{(\ell )} n = \log ({\log ^{(\ell -1)}} n)$$ ). The space of our data structure matches the current best upper bound of Tsur (Inf. Process. Lett., 2019), which does not support the queries efficiently. Also, we show that at least $$3.16n-\Theta (\log n)$$ bits are necessary for answering all the queries. Our result is obtained by generalizing Gawrychowski and Nicholson’s $$(3n - \Theta (\log n))$$ -bit lower bound (ICALP, 15) for answering range minimum and maximum queries on a permutation of size n.},
  archive      = {J_Alg},
  author       = {Jo, Seungbum and Kim, Geunho},
  doi          = {10.1007/s00453-025-01325-9},
  journal      = {Algorithmica},
  month        = {10},
  number       = {10},
  pages        = {1369-1392},
  shortjournal = {Algorithmica},
  title        = {Space-efficient data structure for Next/Previous Larger/Smaller value queries},
  volume       = {87},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Fair repetitive interval scheduling. <em>Alg</em>, <em>87</em>(9), 1340-1368. (<a href='https://doi.org/10.1007/s00453-025-01322-y'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Fair resource allocation is undoubtedly a crucial factor in customer satisfaction in several scheduling scenarios. This is especially apparent in repetitive scheduling models where the same clients repeatedly submit jobs on a daily basis. In this paper, we aim to analyze a repetitive scheduling model involving a set of n clients and a set of m days. On every day, each client submits a request to process a job exactly within a specific time interval, which may vary from day to day, modeling the scenario where the scheduling is done Just-In-Time. The daily schedule is executed on a single machine that can process a single job at a time, therefore it is not possible to schedule jobs with intersecting time intervals. Accordingly, a feasible solution corresponds to sets of jobs with disjoint time intervals, one set per day. We define the quality of service that a client receives as the number of executed jobs over the m days period. Our objective is to provide a feasible solution where each client has at least k days where his jobs are processed. We prove that this problem is NP-hard even under various natural restrictions such as identical processing times and day-independent due dates. We also provide efficient algorithms for several special cases and analyze the parameterized tractability of the problem with respect to several parameters, providing both parameterized hardness and tractability results.},
  archive      = {J_Alg},
  author       = {Heeger, Klaus and Hermelin, Danny and Itzhaki, Yuval and Molter, Hendrik and Shabtay, Dvir},
  doi          = {10.1007/s00453-025-01322-y},
  journal      = {Algorithmica},
  month        = {9},
  number       = {9},
  pages        = {1340-1368},
  shortjournal = {Algorithmica},
  title        = {Fair repetitive interval scheduling},
  volume       = {87},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Reallocation problems with minimum completion time. <em>Alg</em>, <em>87</em>(9), 1311-1339. (<a href='https://doi.org/10.1007/s00453-025-01320-0'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Reallocation scheduling is one of the most fundamental problems in various areas such as supply chain management, logistics, and transportation science. In this paper, we introduce the reallocation problem that models the scheduling in which products are with fixed cost (e.g., transition time), non-fungible, and reallocated among warehouses in parallel, and comprehensively study the complexity of the problem under various settings of the transition time, product size, and capacities. We show that the problem can be solved in polynomial time for a fundamental setting where the product size and transition time are both uniform. We also show that the feasibility of the problem is NP-complete even for little more general settings, which implies that no polynomial-time algorithm constructs a feasible schedule of the problem unless P $$=$$ NP. We then consider to solve the problem by relaxing capacity constraints, which we call the capacity augmentation, and derive a reallocation schedule feasible with the augmentation such that the completion time is at most the optimal of the original problem. When the warehouse capacity is sufficiently large, we design constant-factor approximation algorithms. We also show the relationship between the reallocation problem and the bin packing problem when the warehouse and carry-in capacities are sufficiently large.},
  archive      = {J_Alg},
  author       = {Ishii, Toshimasa and Kawahara, Jun and Makino, Kazuhisa and Ono, Hirotaka},
  doi          = {10.1007/s00453-025-01320-0},
  journal      = {Algorithmica},
  month        = {9},
  number       = {9},
  pages        = {1311-1339},
  shortjournal = {Algorithmica},
  title        = {Reallocation problems with minimum completion time},
  volume       = {87},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). How fitness aggregation methods affect the performance of competitive CoEAs on bilinear problems. <em>Alg</em>, <em>87</em>(9), 1274-1310. (<a href='https://doi.org/10.1007/s00453-025-01313-z'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Competitive co-evolutionary algorithms (CoEAs) do not rely solely on an external function to assign fitness values to sampled solutions. Instead, they use the aggregation of outcomes from interactions between competing solutions allowing to rank solutions and make selection decisions. This makes CoEAs a useful tool for optimisation problems that have intrinsically interactive domains. Over the past decades, many ways to aggregate the outcomes of interactions have been considered. At the moment, it is unclear which of these is the best choice. Previous research is fragmented and most of the fitness aggregation methods (fitness measures) proposed have only been studied empirically. We argue that a proper understanding of the dynamics of CoEAs and their fitness measures can only be achieved through rigorous analysis of their behaviour. In this work we make a step towards this goal by using runtime analysis to study two commonly used fitness measures. We show a dichotomy in the behaviour of a $$(1, \lambda )$$ CoEA when optimising a Bilinear problem. The algorithm finds a solution near the Nash equilibrium in polynomial time with high probability if the worst interaction is used as a fitness measure but is inefficient if the average of all interactions is used instead.},
  archive      = {J_Alg},
  author       = {Hevia Fajardo, Mario Alejandro and Lehre, Per Kristian},
  doi          = {10.1007/s00453-025-01313-z},
  journal      = {Algorithmica},
  month        = {9},
  number       = {9},
  pages        = {1274-1310},
  shortjournal = {Algorithmica},
  title        = {How fitness aggregation methods affect the performance of competitive CoEAs on bilinear problems},
  volume       = {87},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Enumerating graphlets with amortized time complexity independent of graph size. <em>Alg</em>, <em>87</em>(9), 1247-1273. (<a href='https://doi.org/10.1007/s00453-025-01312-0'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Graphlets of order k in a graph G are connected subgraphs induced by k nodes (called k-graphlets) or by k edges (called edge k-graphlets). They are among the interesting subgraphs in network analysis to get insights on both the local and global structure of a network. While several algorithms exist for discovering and enumerating graphlets, the amortized time complexity of such algorithms typically depends on the size of the graph G, or its maximum degree. In real networks, even the latter can be in the order of millions, whereas k is typically required to be a small value. In this paper we provide the first algorithm to list all graphlets of order k in a graph $$G=(V,E)$$ with an amortized time complexity depending solely on the order k, contrarily to previous approaches where the cost depends also on the size of G or its maximum degree. Specifically, we show that it is possible to list k-graphlets in $$O(k^2)$$ time per solution, and to list edge k-graphlets in O(k) time per solution. Furthermore we show that, if the input graph has bounded degree, then the amortized time for listing k-graphlets is reduced to O(k). Whenever $$k = O(1)$$ , as it is often the case in practical settings, these algorithms are the first to achieve constant time per solution.},
  archive      = {J_Alg},
  author       = {Conte, Alessio and Grossi, Roberto and Kobayashi, Yasuaki and Kurita, Kazuhiro and Rucci, Davide and Uno, Takeaki and Wasa, Kunihiro},
  doi          = {10.1007/s00453-025-01312-0},
  journal      = {Algorithmica},
  month        = {9},
  number       = {9},
  pages        = {1247-1273},
  shortjournal = {Algorithmica},
  title        = {Enumerating graphlets with amortized time complexity independent of graph size},
  volume       = {87},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Coloring bridge-free antiprismatic graphs. <em>Alg</em>, <em>87</em>(9), 1223-1246. (<a href='https://doi.org/10.1007/s00453-025-01316-w'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The coloring problem is a well-researched topic and its complexity is known for several classes of graphs. However, the question of its complexity remains open for the class of antiprismatic graphs, which are the complement of prismatic graphs and one of the four remaining cases highlighted by Lozin and Malishev. In this article we focus on the equivalent question of the complexity of the clique cover problem in prismatic graphs. A graph G is prismatic if for every triangle T of G, every vertex of G not in T has a unique neighbor in T. A graph is co-bridge-free if it has no $$C_4+2K_1$$ as induced subgraph. We give a polynomial time algorithm that solves the clique cover problem in co-bridge-free prismatic graphs. It relies on the structural description given by Chudnovsky and Seymour, and on later work of Preissmann, Robin and Trotignon. We show that co-bridge-free prismatic graphs have a bounded number of disjoint triangles and that implies that the algorithm presented by Preissmann et al. applies.},
  archive      = {J_Alg},
  author       = {Robin, Cléophée and Robinson, Eileen},
  doi          = {10.1007/s00453-025-01316-w},
  journal      = {Algorithmica},
  month        = {9},
  number       = {9},
  pages        = {1223-1246},
  shortjournal = {Algorithmica},
  title        = {Coloring bridge-free antiprismatic graphs},
  volume       = {87},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Matching cuts in graphs of high girth and H-free graphs. <em>Alg</em>, <em>87</em>(8), 1199-1221. (<a href='https://doi.org/10.1007/s00453-025-01318-8'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The (Perfect) Matching Cut problem is to decide if a connected graph has a (perfect) matching that is also an edge cut. The Disconnected Perfect Matching problem is to decide if a connected graph has a perfect matching that contains a matching cut. Both Matching Cut and Disconnected Perfect Matching are NP-complete for planar graphs of girth 5, whereas Perfect Matching Cut is known to be NP-complete even for subcubic bipartite graphs of arbitrarily large fixed girth. We prove that Matching Cut and Disconnected Perfect Matching are also NP-complete for bipartite graphs of arbitrarily large fixed girth and bounded maximum degree. Our result for Matching Cut resolves a 20-year old open problem. We also show that the more general problem d-Cut, for every fixed $$d\ge 1$$ , is NP-complete for bipartite graphs of arbitrarily large fixed girth and bounded maximum degree. Furthermore, we show that Matching Cut, Perfect Matching Cut and Disconnected Perfect Matching are NP-complete for H-free graphs whenever H contains a connected component with two vertices of degree at least 3. Afterwards, we update the state-of-the-art summaries for H-free graphs and compare them with each other, and with a known and full classification of the Maximum Matching Cut problem, which is to determine a largest matching cut of a graph G. Finally, by combining existing results, we obtain a complete complexity classification of Perfect Matching Cut for $$\mathcal{H}$$ -subgraph-free graphs where $$\mathcal{H}$$ is any finite set of graphs.},
  archive      = {J_Alg},
  author       = {Feghali, Carl and Lucke, Felicia and Paulusma, Daniël and Ries, Bernard},
  doi          = {10.1007/s00453-025-01318-8},
  journal      = {Algorithmica},
  month        = {8},
  number       = {8},
  pages        = {1199-1221},
  shortjournal = {Algorithmica},
  title        = {Matching cuts in graphs of high girth and H-free graphs},
  volume       = {87},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Clustering what matters in constrained settings. <em>Alg</em>, <em>87</em>(8), 1178-1198. (<a href='https://doi.org/10.1007/s00453-025-01317-9'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Constrained clustering problems generalize classical clustering formulations, e.g., $$k$$ -median, $$k$$ -means, by imposing additional constraints on the feasibility of a clustering. There has been significant recent progress in obtaining approximation algorithms for these problems, both in the metric and the Euclidean settings. However, the outlier version of these problems, where the solution is allowed to leave out m points from the clustering, is not well understood. In this work, we give a general framework for reducing the outlier version of a constrained $$k$$ -median or $$k$$ -means problem to the corresponding outlier-free version with only $$(1+\varepsilon )$$ -loss in the approximation ratio. The reduction is obtained by mapping the original instance of the problem to $$f(k,m, \varepsilon )$$ instances of the outlier-free version, where $$f(k, m, \varepsilon ) = \left( \frac{k+m}{\varepsilon }\right) ^{O(m)}$$ . As specific applications, we get the following results: Our work generalizes the results of Bhattacharya et al. and Agrawal et al. to a larger class of constrained clustering problems. Further, our reduction works for arbitrary metric spaces and so can extend clustering algorithms for outlier-free versions in both Euclidean and arbitrary metric spaces.},
  archive      = {J_Alg},
  author       = {Jaiswal, Ragesh and Kumar, Amit},
  doi          = {10.1007/s00453-025-01317-9},
  journal      = {Algorithmica},
  month        = {8},
  number       = {8},
  pages        = {1178-1198},
  shortjournal = {Algorithmica},
  title        = {Clustering what matters in constrained settings},
  volume       = {87},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Bandwidth parameterized by cluster vertex deletion number. <em>Alg</em>, <em>87</em>(8), 1146-1177. (<a href='https://doi.org/10.1007/s00453-025-01315-x'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Given a graph G and an integer b, Bandwidth asks whether there exists a bijection $$\pi $$ from V(G) to $$\{1, \ldots , |V(G)|\}$$ such that $$\max _{\{u, v \} \in E(G)} | \pi (u) - \pi (v) | \le b$$ . This is a classical NP-complete problem, known to remain NP-complete even on very restricted classes of graphs, such as trees of maximum degree 3 and caterpillars of hair length 3. In the realm of parameterized complexity, these results imply that the problem remains NP-hard on graphs of bounded pathwidth, while it is additionally known to be W[1]-hard when parameterized by the tree-depth of the input graph. In contrast, the problem does become FPT when parameterized by the vertex cover number. In this paper we make progress in understanding the parameterized (in)tractability of Bandwidth. We first show that it is FPT when parameterized by the cluster vertex deletion number cvd plus the clique number $$\omega $$ , thus significantly strengthening the previously mentioned result for vertex cover number. On the other hand, we show that Bandwidth is W[1]-hard when parameterized only by cvd. Our results develop and generalize some of the methods of argumentation of the previous results and narrow some of the complexity gaps.},
  archive      = {J_Alg},
  author       = {Gima, Tatsuya and Kim, Eun Jung and Köhler, Noleen and Melissinos, Nikolaos and Vasilakis, Manolis},
  doi          = {10.1007/s00453-025-01315-x},
  journal      = {Algorithmica},
  month        = {8},
  number       = {8},
  pages        = {1146-1177},
  shortjournal = {Algorithmica},
  title        = {Bandwidth parameterized by cluster vertex deletion number},
  volume       = {87},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Editor’s note: Special issue on GECCO 2023. <em>Alg</em>, <em>87</em>(8), 1145. (<a href='https://doi.org/10.1007/s00453-025-01319-7'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_Alg},
  doi          = {10.1007/s00453-025-01319-7},
  journal      = {Algorithmica},
  month        = {8},
  number       = {8},
  pages        = {1145},
  shortjournal = {Algorithmica},
  title        = {Editor’s note: Special issue on GECCO 2023},
  volume       = {87},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Pareto sums of pareto sets: Lower bounds and algorithms. <em>Alg</em>, <em>87</em>(8), 1111-1144. (<a href='https://doi.org/10.1007/s00453-025-01314-y'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In bi-criteria optimization problems, the goal is typically to compute the set of Pareto-optimal solutions. Many algorithms for these types of problems rely on efficient merging or combining of partial solutions and filtering of dominated solutions in the resulting sets. In this article, we consider the task of computing the Pareto sum of two given Pareto sets A, B of size n. The Pareto sum C contains all non-dominated points of the Minkowski sum $$M = \{a+b|a \in A, b\in B\}$$ . Since the Minkowski sum has a size of $$n^2$$ , but the Pareto sum C can be much smaller, the goal is to compute C without having to compute and store all of M. We present several new algorithms for efficient Pareto sum computation, including an output-sensitive successive algorithm with a running time of $$\mathcal {O}(n \log n + nk)$$ and a space consumption of $$\mathcal {O}(n+k)$$ for $$k=|C|$$ . If the elements of C are streamed, the space consumption reduces to $$\mathcal {O}(n)$$ . For output sizes $$k \ge 2n$$ , we prove a conditional lower bound for Pareto sum computation, which excludes running times in $$\mathcal {O}(n^{2-\delta })$$ for $$\delta > 0$$ unless the (min,+)-convolution hardness conjecture fails. The successive algorithm matches this lower bound for $$k \in \Theta (n)$$ . However, for $$k \in \Theta (n^2)$$ , the successive algorithm exhibits a cubic running time. But we also present an algorithm with an output-sensitive space consumption and a running time of $$\mathcal {O}(n^2 \log n)$$ , which matches the lower bound up to a logarithmic factor even for large k. Furthermore, we describe suitable engineering techniques to improve the practical running times of our algorithms. Finally, we provide an extensive comparative experimental study on generated and real-world data. As a showcase application, we consider preprocessing-based bi-criteria route planning in road networks. Pareto sum computation is the bottleneck task in the preprocessing phase and in the query phase. We show that using our algorithms with an output-sensitive space consumption allows to tackle larger instances and reduces the preprocessing and query time compared to algorithms that fully store M.},
  archive      = {J_Alg},
  author       = {Funke, Daniel and Hespe, Demian and Sanders, Peter and Storandt, Sabine and Truschel, Carina},
  doi          = {10.1007/s00453-025-01314-y},
  journal      = {Algorithmica},
  month        = {8},
  number       = {8},
  pages        = {1111-1144},
  shortjournal = {Algorithmica},
  title        = {Pareto sums of pareto sets: Lower bounds and algorithms},
  volume       = {87},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Runtime analysis with variable cost. <em>Alg</em>, <em>87</em>(7), 1081-1110. (<a href='https://doi.org/10.1007/s00453-025-01307-x'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The usual approach in runtime analysis is to derive estimates on the number of fitness function evaluations required by a method until a suitable element of the search space is found. One justification for this is that in real applications, fitness evaluation often contributes the most computational effort. A tacit assumption in this approach is that this effort is uniform and static across the search space. However, this assumption often does not hold in practice: some candidates may be far more expensive to evaluate than others. This might occur, for example, when fitness evaluation requires running a simulation or training a machine learning model. Despite the availability of a wide range of benchmark functions coupled with various runtime performance guarantees, the runtime analysis community currently lacks a solid perspective of handling variable fitness cost. Our goal with this paper is to argue for incorporating this perspective into our theoretical toolbox. We introduce two models of handling variable cost: a simple non-adaptive model together with a more general adaptive model. We prove cost bounds in these scenarios and discuss the implications for taking into account costly regions in the search space.},
  archive      = {J_Alg},
  author       = {Lehre, Per Kristian and Sutton, Andrew M.},
  doi          = {10.1007/s00453-025-01307-x},
  journal      = {Algorithmica},
  month        = {7},
  number       = {7},
  pages        = {1081-1110},
  shortjournal = {Algorithmica},
  title        = {Runtime analysis with variable cost},
  volume       = {87},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). The tight spanning ratio of the rectangle delaunay triangulation. <em>Alg</em>, <em>87</em>(7), 1060-1080. (<a href='https://doi.org/10.1007/s00453-025-01308-w'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Spanner construction is a well-studied problem and Delaunay triangulations are among the most popular spanners. Tight bounds are known if the Delaunay triangulation is constructed using an equilateral triangle, a square, or a regular hexagon. However, all other shapes have remained elusive. In this paper, we extend the restricted class of spanners for which tight bounds are known. We prove that Delaunay triangulations constructed using rectangles with aspect ratio $$A$$ have spanning ratio at most $$\sqrt{2} \sqrt{1+A^2 + A\sqrt{A^2 + 1}}$$ , which matches the known lower bound.},
  archive      = {J_Alg},
  author       = {van Renssen, André and Sha, Yuan and Sun, Yucheng and Wong, Sampson},
  doi          = {10.1007/s00453-025-01308-w},
  journal      = {Algorithmica},
  month        = {7},
  number       = {7},
  pages        = {1060-1080},
  shortjournal = {Algorithmica},
  title        = {The tight spanning ratio of the rectangle delaunay triangulation},
  volume       = {87},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Reconfiguration of the union of arborescences. <em>Alg</em>, <em>87</em>(7), 1040-1059. (<a href='https://doi.org/10.1007/s00453-025-01310-2'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {An arborescence in a digraph is an acyclic arc subset in which every vertex except a root has exactly one incoming arc. In this paper, we show the reconfigurability of the union of k arborescences for fixed k in the following sense: for any pair of arc subsets that can be partitioned into k arborescences, one can be transformed into the other by exchanging arcs one by one so that every intermediate arc subset can also be partitioned into k arborescences. This generalizes the result by Ito et al. (2023), who showed the case with $$k=1$$ . Since the union of k arborescences can be represented as a common matroid basis of two matroids, our result gives a new non-trivial example of matroid pairs for which two common bases are always reconfigurable to each other.},
  archive      = {J_Alg},
  author       = {Kobayashi, Yusuke and Mahara, Ryoga and Schwarcz, Tamás},
  doi          = {10.1007/s00453-025-01310-2},
  journal      = {Algorithmica},
  month        = {7},
  number       = {7},
  pages        = {1040-1059},
  shortjournal = {Algorithmica},
  title        = {Reconfiguration of the union of arborescences},
  volume       = {87},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Improved smoothed analysis of 2-opt for the euclidean TSP. <em>Alg</em>, <em>87</em>(7), 1008-1039. (<a href='https://doi.org/10.1007/s00453-025-01309-9'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The 2-opt heuristic is a simple local search heuristic for the travelling salesperson problem (TSP). Although it usually performs well in practice, its worst-case running time is exponential in the number of cities. Attempts to reconcile this difference between practice and theory have used smoothed analysis, in which adversarial instances are perturbed probabilistically. We are interested in the classical model of smoothed analysis for the Euclidean TSP, in which the perturbations are Gaussian. This model was previously used by Manthey and Veenstra, who obtained smoothed complexity bounds polynomial in n, the dimension d, and the perturbation strength $$\sigma ^{-1}$$ . However, their analysis only works for $$d \ge 4$$ . The only previous analysis for $$d \le 3$$ was performed by Englert, Röglin and Vöcking, who used a different perturbation model which can be translated to Gaussian perturbations. Their model yields bounds polynomial in n and $$\sigma ^{-d}$$ , and super-exponential in d. As the fact that no direct analysis exists for Gaussian perturbations that yields polynomial bounds for all d is somewhat unsatisfactory, we perform this missing analysis. Along the way, we improve all existing smoothed complexity bounds for Euclidean 2-opt with Gaussian perturbations.},
  archive      = {J_Alg},
  author       = {Manthey, Bodo and van Rhijn, Jesse},
  doi          = {10.1007/s00453-025-01309-9},
  journal      = {Algorithmica},
  month        = {7},
  number       = {7},
  pages        = {1008-1039},
  shortjournal = {Algorithmica},
  title        = {Improved smoothed analysis of 2-opt for the euclidean TSP},
  volume       = {87},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Linear-time MaxCut in multigraphs parameterized above the poljak-turzík bound. <em>Alg</em>, <em>87</em>(7), 983-1007. (<a href='https://doi.org/10.1007/s00453-025-01306-y'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {MaxCut is a classical $$\textsf{NP}$$ -complete problem and a crucial building block in many combinatorial algorithms. The famous Edwards-Erdös bound states that any connected graph on n vertices with m edges contains a cut of size at least $$\frac{m}{2}+\frac{n-1}{4}$$ . Crowston, Jones and Mnich [Algorithmica, 2015] showed that the MaxCut problem on simple connected graphs admits an FPT algorithm, where the parameter k is the difference between the desired cut size c and the lower bound given by the Edwards-Erdös bound. This was later improved by Etscheid and Mnich [Algorithmica, 2017] to run in parameterized linear time, i.e., $$f(k)\cdot O(m)$$ . We improve upon this result in two ways: Firstly, we extend the algorithm to work also for multigraphs (alternatively, graphs with positive integer weights). Secondly, we change the parameter; instead of the difference to the Edwards-Erdös bound, we use the difference to the Poljak-Turzík bound. The Poljak-Turzík bound states that any weighted graph G has a cut of weight at least $$\frac{w(G)}{2}+\frac{w_{MSF}(G)}{4}$$ , where w(G) denotes the total weight of G, and $$w_{MSF}(G)$$ denotes the weight of its minimum spanning forest. In connected simple graphs the two bounds are equivalent, but for multigraphs the Poljak-Turzík bound can be larger and thus yield a smaller parameter k. Our algorithm also runs in parameterized linear time, i.e., $$f(k)\cdot O(m+n)$$ .},
  archive      = {J_Alg},
  author       = {Lill, Jonas and Petrova, Kalina and Weber, Simon},
  doi          = {10.1007/s00453-025-01306-y},
  journal      = {Algorithmica},
  month        = {7},
  number       = {7},
  pages        = {983-1007},
  shortjournal = {Algorithmica},
  title        = {Linear-time MaxCut in multigraphs parameterized above the poljak-turzík bound},
  volume       = {87},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Structural parameterization of cluster deletion. <em>Alg</em>, <em>87</em>(6), 961-981. (<a href='https://doi.org/10.1007/s00453-025-01303-1'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the Weighted Cluster Deletion problem we are given a graph with non-negative integral edge weights and the task is to determine, for a target value k, if there is a set of edges of total weight at most k such that its removal results in a disjoint union of cliques. It is well-known that the problem is FPT parameterized by k, the total weight of edge deletions. In scenarios in which the solution size is large, naturally one needs to drop the constraint on the solution size. Here we study Weighted Cluster Deletion where the parameter does not represent the size of the solution, but the parameter captures structural properties of the input graph. Our main contribution is to classify the parameterized complexity of Weighted Cluster Deletion with three structural parameters, namely, vertex cover number, twin cover number and neighborhood diversity. We show that the problem is FPT when parameterized by the vertex cover number, whereas it becomes paraNP-hard when parameterized by the twin cover number or the neighborhood diversity. To illustrate the applicability of our FPT result, we turn our attention to the unweighted variant of the problem, namely Cluster Deletion. We show that Cluster Deletion is FPT parameterized by the twin cover number. This is the first algorithm with single-exponential running time parameterized by the twin cover number. Interestingly, we are able to achieve an FPT result for Cluster Deletion parameterized by the neighborhood diversity that involves an ILP formulation. In fact, our results generalize the parameterized setting by the solution size, as we deduce that both parameters, twin cover number and neighborhood diversity, are linearly bounded by the number of edge deletions.},
  archive      = {J_Alg},
  author       = {Italiano, Giuseppe F. and Konstantinidis, Athanasios L. and Papadopoulos, Charis},
  doi          = {10.1007/s00453-025-01303-1},
  journal      = {Algorithmica},
  month        = {6},
  number       = {6},
  pages        = {961-981},
  shortjournal = {Algorithmica},
  title        = {Structural parameterization of cluster deletion},
  volume       = {87},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Upward pointset embeddings of planar st-graphs. <em>Alg</em>, <em>87</em>(6), 930-960. (<a href='https://doi.org/10.1007/s00453-025-01302-2'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We study upward pointset embeddings (UPSEs) of planar st-graphs. Let G be a planar st-graph and let $$S \subset \mathbb {R}^2$$ be a pointset with $$|S|= |V(G)|$$ . An UPSE of G on S is an upward planar straight-line drawing of G that maps the vertices of G to the points of S. We consider both the problem of testing the existence of an UPSE of G on S (UPSE Testing) and the problem of enumerating all UPSEs of G on S. We prove that UPSE Testing is NP-complete even for st-graphs that consist of a set of directed st-paths sharing only s and t. On the other hand, if G is an n-vertex planar st-graph whose maximum st-cutset has size k, then UPSE Testing can be solved in $$\mathcal {O}(n^{4k})$$ time with $$\mathcal {O}(n^{3k})$$ space; also, all the UPSEs of G on S can be enumerated with $$\mathcal {O}(n)$$ worst-case delay, using $$\mathcal {O}(k n^{4k} \log n)$$ space, after $$\mathcal {O}(k n^{4k} \log n)$$ set-up time. Moreover, for an n-vertex st-graph whose underlying graph is a cycle, we provide a necessary and sufficient condition for the existence of an UPSE on a given pointset, which can be tested in $$\mathcal {O}(n \log n)$$ time. Related to this result, we give an algorithm that, for a set S of n points, enumerates all the non-crossing monotone Hamiltonian cycles on S with $$\mathcal {O}(n)$$ worst-case delay, using $$\mathcal {O}(n^2)$$ space, after $$\mathcal {O}(n^2)$$ set-up time.},
  archive      = {J_Alg},
  author       = {Alegrí­a, Carlos and Caroppo, Susanna and Da Lozzo, Giordano and D’Elia, Marco and Di Battista, Giuseppe and Frati, Fabrizio and Grosso, Fabrizio and Patrignani, Maurizio},
  doi          = {10.1007/s00453-025-01302-2},
  journal      = {Algorithmica},
  month        = {6},
  number       = {6},
  pages        = {930-960},
  shortjournal = {Algorithmica},
  title        = {Upward pointset embeddings of planar st-graphs},
  volume       = {87},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Improved algorithms for distance selection and related problems. <em>Alg</em>, <em>87</em>(6), 908-929. (<a href='https://doi.org/10.1007/s00453-025-01305-z'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we propose new techniques for solving geometric optimization problems involving interpoint distances of a point set in the plane. Given a set P of n points in the plane and an integer $$1 \le k \le \left( {\begin{array}{c}n\\ 2\end{array}}\right) $$ , the distance selection problem is to find the k-th smallest interpoint distance among all pairs of points of P. The previously best deterministic algorithm solves the problem in $$O(n^{4/3} \log ^2 n)$$ time (Katz and Sharir in SIAM J Comput 26(5):1384–1408, 1997 and SoCG 1993). In this paper, we improve their algorithm to $$O(n^{4/3} \log n)$$ time. Using similar techniques, we also give improved algorithms on both the two-sided and the one-sided discrete Fréchet distance with shortcuts problem for two point sets in the plane. For the two-sided problem (resp., one-sided problem), we improve the previous work (Avraham et al. in ACM Trans Algorithms 11(4):29, 2015 and SoCG 2014) by a factor of roughly $$\log ^2(m+n)$$ (resp., $$(m+n)^{\epsilon }$$ ), where m and n are the sizes of the two input point sets, respectively. Other problems whose solutions can be improved by our techniques include the reverse shortest path problems for unit-disk graphs. Our techniques are quite general and we believe they will find many other applications in future.},
  archive      = {J_Alg},
  author       = {Wang, Haitao and Zhao, Yiming},
  doi          = {10.1007/s00453-025-01305-z},
  journal      = {Algorithmica},
  month        = {6},
  number       = {6},
  pages        = {908-929},
  shortjournal = {Algorithmica},
  title        = {Improved algorithms for distance selection and related problems},
  volume       = {87},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). On the parameterized complexity of controlling amendment and successive winners. <em>Alg</em>, <em>87</em>(6), 842-907. (<a href='https://doi.org/10.1007/s00453-025-01304-0'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The amendment procedure and the successive procedure have been widely employed in parliamentary and legislative decision making and have undergone extensive study in the literature from various perspectives. However, investigating them through the lens of computational complexity theory has not been as thoroughly conducted as for many other prevalent voting procedures heretofore. To the best of our knowledge, there is only one paper which explores the complexity of several strategic voting problems under these two procedures, prior to our current work. To provide a better understanding of to what extent the two procedures resist strategic behavior, we study the parameterized complexity of constructive/destructive control by adding/deleting voters/candidates for both procedures. To enhance the generalizability of our results, we also examine a more generalized form of the amendment procedure. Our exploration yields a comprehensive (parameterized) complexity landscape of these problems with respect to numerous parameters.},
  archive      = {J_Alg},
  author       = {Yang, Yongjie},
  doi          = {10.1007/s00453-025-01304-0},
  journal      = {Algorithmica},
  month        = {6},
  number       = {6},
  pages        = {842-907},
  shortjournal = {Algorithmica},
  title        = {On the parameterized complexity of controlling amendment and successive winners},
  volume       = {87},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Online metric matching on the line with recourse. <em>Alg</em>, <em>87</em>(6), 813-841. (<a href='https://doi.org/10.1007/s00453-025-01299-8'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In online metric matching on the line, n requests appear one by one and have to be matched immediately and irrevocably to a given set of servers, all located on the real line. The goal is to minimize the sum of distances between the requests and their assigned servers. The best known online algorithm achieves a competitive ratio of $$\Theta (\log n)$$ , leaving a gap to the best-known lower bound of $$\Omega (\sqrt{\log n})$$ . In this work, we approach the problem in a recourse model where online decisions can be partially revised, allowing for the reassignment of previously matched edges. In contrast to the traditional online setting, we show that with an amortized recourse budget of $$O(\log n)$$ , we can obtain an O(1)-competitive algorithm for online metric matching on the line. This is one of the first non-trivial results for metric matching with recourse. Additionally, for so-called alternating instances, where no more than one request lies between two servers, we achieve a near-optimal result. Specifically, we give a simple algorithm that is $$(1+\varepsilon )$$ -competitive and reassigns any request at most $$O(\frac{1}{\varepsilon ^2})$$ times. This special case is particularly noteworthy, as a lower bound of $$\Omega (\log n)$$ , constructed using such instances, applies to a broad class of online algorithms, including all deterministic algorithms studied in the literature.},
  archive      = {J_Alg},
  author       = {Megow, Nicole and Nölke, Lukas},
  doi          = {10.1007/s00453-025-01299-8},
  journal      = {Algorithmica},
  month        = {6},
  number       = {6},
  pages        = {813-841},
  shortjournal = {Algorithmica},
  title        = {Online metric matching on the line with recourse},
  volume       = {87},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Correction: Galloping in fast-growth natural merge sorts. <em>Alg</em>, <em>87</em>(5), 812. (<a href='https://doi.org/10.1007/s00453-025-01297-w'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_Alg},
  author       = {Ghasemi, Elahe and Jugé, Vincent and Khalighinejad, Ghazal and Yazdanyar, Helia},
  doi          = {10.1007/s00453-025-01297-w},
  journal      = {Algorithmica},
  month        = {5},
  number       = {5},
  pages        = {812},
  shortjournal = {Algorithmica},
  title        = {Correction: Galloping in fast-growth natural merge sorts},
  volume       = {87},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Simultaneous representation of proper and unit interval graphs. <em>Alg</em>, <em>87</em>(5), 783-811. (<a href='https://doi.org/10.1007/s00453-025-01296-x'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In a confluence of combinatorics and geometry, simultaneous representations provide a way to realize combinatorial objects that share common structure. A standard case in the study of simultaneous representations is the sunflower case where all objects share the same common structure. While the recognition problem for general simultaneous interval graphs—the simultaneous version of arguably one of the most well-studied graph classes—is NP-complete, the complexity of the sunflower case for three or more simultaneous interval graphs is currently open. In this work we settle this question for proper interval graphs. We give an algorithm to recognize simultaneous proper interval graphs in linear time in the sunflower case where we allow any number of simultaneous graphs. Simultaneous unit interval graphs are much more ‘rigid’ and therefore have less freedom in their representation. We show they can be recognized in time $$\mathcal {O}(|V|\cdot |E|)$$ for any number of simultaneous graphs in the sunflower case where $$G=(V,E)$$ is the union of the simultaneous graphs. We further show that both recognition problems are in general NP-complete if the number of simultaneous graphs is not fixed. The restriction to the sunflower case is in this sense necessary.},
  archive      = {J_Alg},
  author       = {Rutter, Ignaz and Strash, Darren and Stumpf, Peter and Vollmer, Michael},
  doi          = {10.1007/s00453-025-01296-x},
  journal      = {Algorithmica},
  month        = {5},
  number       = {5},
  pages        = {783-811},
  shortjournal = {Algorithmica},
  title        = {Simultaneous representation of proper and unit interval graphs},
  volume       = {87},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Counting temporal paths. <em>Alg</em>, <em>87</em>(5), 736-782. (<a href='https://doi.org/10.1007/s00453-025-01301-3'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This work investigates the parameterised complexity of counting temporal paths. The problem of counting temporal paths is mainly motivated by temporal betweenness computation. The betweenness centrality of a vertex v is an important centrality measure that quantifies how many optimal paths between pairs of other vertices visit v. Computing betweenness centrality in a temporal graph, in which the edge set may change over discrete timesteps, requires us to count temporal paths that are optimal with respect to some criterion. For several natural notions of optimality, including foremost or fastest temporal paths, this counting problem reduces to #Temporal Path, the problem of counting all temporal paths between a fixed pair of vertices; like the problems of counting foremost and fastest temporal paths, #Temporal Path is #P-hard in general. Motivated by the many applications of this intractable problem, we initiate a systematic study of the parameterised and approximation complexity of #Temporal Path. We show that the problem presumably does not admit an FPT-algorithm for the feedback vertex number of the static underlying graph, and that it is hard to approximate in general. On the positive side, we prove several exact and approximate FPT-algorithms for special cases.},
  archive      = {J_Alg},
  author       = {Enright, Jessica and Meeks, Kitty and Molter, Hendrik},
  doi          = {10.1007/s00453-025-01301-3},
  journal      = {Algorithmica},
  month        = {5},
  number       = {5},
  pages        = {736-782},
  shortjournal = {Algorithmica},
  title        = {Counting temporal paths},
  volume       = {87},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Enumerating minimal solution sets for metric graph problems. <em>Alg</em>, <em>87</em>(5), 712-735. (<a href='https://doi.org/10.1007/s00453-025-01300-4'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Problems from metric graph theory like Metric Dimension, Geodetic Set, and Strong Metric Dimension have recently had an impact in parameterized complexity by being the first known problems in NP to admit double-exponential lower bounds in the treewidth, and even in the vertex cover number for the latter, assuming the Exponential Time Hypothesis. We initiate the study of enumerating minimal solution sets for these problems and show that they are also of great interest in enumeration. Specifically, we show that enumerating minimal resolving sets in graphs and minimal geodetic sets in split graphs are equivalent to enumerating minimal transversals in hypergraphs (denoted Trans-Enum), whose solvability in total-polynomial time is one of the most important open problems in algorithmic enumeration. This provides two new natural examples to a question that emerged in recent works: for which vertex (or edge) set graph property $$\Pi $$ is the enumeration of minimal (or maximal) subsets satisfying $$\Pi $$ equivalent to Trans-Enum? As very few properties are known to fit within this context—namely, those related to minimal domination—our results make significant progress in characterizing such properties, and provide new angles to approach Trans-Enum. In contrast, we observe that minimal strong resolving sets can be enumerated with polynomial delay. Additionally, we consider cases where our reductions do not apply, namely graphs with no long induced paths, and show both positive and negative results related to the enumeration and extension of partial solutions.},
  archive      = {J_Alg},
  author       = {Bergougnoux, Benjamin and Defrain, Oscar and Mc Inerney, Fionn},
  doi          = {10.1007/s00453-025-01300-4},
  journal      = {Algorithmica},
  month        = {5},
  number       = {5},
  pages        = {712-735},
  shortjournal = {Algorithmica},
  title        = {Enumerating minimal solution sets for metric graph problems},
  volume       = {87},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Convergence of the number of period sets in strings. <em>Alg</em>, <em>87</em>(5), 690-711. (<a href='https://doi.org/10.1007/s00453-025-01295-y'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Consider words of length n. The set of all periods of a word of length n is a subset of $$\{0,1,2,\ldots ,n-1\}$$ . However, not every subset of $$\{0,1,2,\ldots ,n-1\}$$ can be a valid set of periods. In a seminal paper in 1981, Guibas and Odlyzko proposed encoding the set of periods of a word into a binary string of length n, called an autocorrelation, where a 1 at position i denotes the period i. They considered the question of recognizing a valid period set, and also studied the number $$\kappa _n$$ of valid period sets for strings of length n. They conjectured that $$\ln \kappa _n$$ asymptotically converges to a constant times $$(\ln n)^2$$ . Although improved lower bounds for $$\ln \kappa _n/(\ln n)^2$$ were proved in 2001, the question of a tight upper bound has remained open since Guibas and Odlyzko’s paper. Here, we exhibit an upper bound for this fraction, which implies its convergence and closes this longstanding conjecture. Moreover, we extend our result to find similar bounds for the number of correlations: a generalization of autocorrelations that encodes the overlaps between two strings.},
  archive      = {J_Alg},
  author       = {Rivals, Eric and Sweering, Michelle and Wang, Pengfei},
  doi          = {10.1007/s00453-025-01295-y},
  journal      = {Algorithmica},
  month        = {5},
  number       = {5},
  pages        = {690-711},
  shortjournal = {Algorithmica},
  title        = {Convergence of the number of period sets in strings},
  volume       = {87},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Analysis of the (1+1) EA on LeadingOnes with constraints. <em>Alg</em>, <em>87</em>(5), 661-689. (<a href='https://doi.org/10.1007/s00453-025-01298-9'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Understanding how evolutionary algorithms perform on constrained problems has gained increasing attention in recent years. In this paper, we study how evolutionary algorithms optimize constrained versions of the classical LeadingOnes problem. We first provide a run time analysis for the classical (1+1) EA on the LeadingOnes problem with a deterministic cardinality constraint, giving $$\Theta (n (n-B)\log (B) + nB)$$ as the tight bound. Our results show that the behaviour of the algorithm is highly dependent on the constraint bound of the uniform constraint. Afterwards, we consider the problem in the context of stochastic constraints and provide insights using theoretical and experimental studies on how the ( $$\mu $$ +1) EA is able to deal with these constraints in a sampling-based setting.},
  archive      = {J_Alg},
  author       = {Friedrich, Tobias and Kötzing, Timo and Neumann, Aneta and Neumann, Frank and Radhakrishnan, Aishwarya},
  doi          = {10.1007/s00453-025-01298-9},
  journal      = {Algorithmica},
  month        = {5},
  number       = {5},
  pages        = {661-689},
  shortjournal = {Algorithmica},
  title        = {Analysis of the (1+1) EA on LeadingOnes with constraints},
  volume       = {87},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Tight bounds for Chordal/Interval vertex deletion parameterized by treewidth. <em>Alg</em>, <em>87</em>(5), 621-660. (<a href='https://doi.org/10.1007/s00453-025-01293-0'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In Chordal/Interval Vertex Deletion we ask how many vertices one needs to remove from a graph to make it chordal (respectively: interval). We study these problems under the parameterization by treewidth $$\textbf{tw}$$ of the input graph G. On the one hand, we present an algorithm for Chordal Vertex Deletion with running time $$2^{\mathcal {O}(\textbf{tw})} \cdot |V(G)|$$ , improving upon the running time $$2^{\mathcal {O}(\textbf{tw}^2)} \cdot |V(G)|^{\mathcal {O}(1)}$$ by Jansen, de Kroon, and Włodarczyk (STOC’21). When a tree decomposition of width $$\textbf{tw}$$ is given, then the base of the exponent equals $$2^{\omega -1}\cdot 3 + 1$$ . Our algorithm is based on a novel link between chordal graphs and graphic matroids, which allows us to employ the framework of representative families. On the other hand, we prove that Interval Vertex Deletion cannot be solved in time $$2^{o(\textbf{tw}\log \textbf{tw})} \cdot |V(G)|^{\mathcal {O}(1)}$$ assuming the Exponential Time Hypothesis.},
  archive      = {J_Alg},
  author       = {Włodarczyk, Michał},
  doi          = {10.1007/s00453-025-01293-0},
  journal      = {Algorithmica},
  month        = {5},
  number       = {5},
  pages        = {621-660},
  shortjournal = {Algorithmica},
  title        = {Tight bounds for Chordal/Interval vertex deletion parameterized by treewidth},
  volume       = {87},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Reforming an envy-free matching. <em>Alg</em>, <em>87</em>(4), 594-620. (<a href='https://doi.org/10.1007/s00453-025-01294-z'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider the problem of reforming an envy-free matching when each agent has a strict preference over items and is assigned a single item. Given an envy-free matching, we consider an operation to exchange the item of an agent with an unassigned item preferred by the agent that results in another envy-free matching. We repeat this operation as long as we can. We prove that the resulting envy-free matching is uniquely determined up to the choice of an initial envy-free matching, and can be found in polynomial time. We call the resulting matching a reformist envy-free matching, and study a shortest sequence to obtain the reformist envy-free matching from an initial envy-free matching. We prove that a shortest sequence is computationally hard to obtain. We also give polynomial-time algorithms when each agent accepts at most three items or each item is accepted by at most two agents. Inapproximability and fixed-parameter (in)tractability are also discussed.},
  archive      = {J_Alg},
  author       = {Ito, Takehiro and Iwamasa, Yuni and Kakimura, Naonori and Kamiyama, Naoyuki and Kobayashi, Yusuke and Nozaki, Yuta and Okamoto, Yoshio and Ozeki, Kenta},
  doi          = {10.1007/s00453-025-01294-z},
  journal      = {Algorithmica},
  month        = {4},
  number       = {4},
  pages        = {594-620},
  shortjournal = {Algorithmica},
  title        = {Reforming an envy-free matching},
  volume       = {87},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Guarding polyominoes under k-hop visibility. <em>Alg</em>, <em>87</em>(4), 572-593. (<a href='https://doi.org/10.1007/s00453-024-01292-7'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We study the Art Gallery Problem under k-hop visibility in polyominoes. In this visibility model, two unit squares of a polyomino can see each other if and only if the shortest path between the respective vertices in the dual graph of the polyomino has length at most k. In this paper, we show that the VC dimension of this problem is 3 in simple polyominoes, and 4 in polyominoes with holes. Furthermore, we provide a reduction from Planar Monotone 3Sat, thereby showing that the problem is NP-complete even in thin polyominoes (i.e., polyominoes that do not a contain a $$2\times 2$$ block of cells). Complementarily, we present a linear-time 4-approximation algorithm for simple 2-thin polyominoes (which do not contain a $$3\times 3$$ block of cells) for all $$k\in {\mathbb {N}}$$ .},
  archive      = {J_Alg},
  author       = {Filtser, Omrit and Krohn, Erik and Nilsson, Bengt J. and Rieck, Christian and Schmidt, Christiane},
  doi          = {10.1007/s00453-024-01292-7},
  journal      = {Algorithmica},
  month        = {4},
  number       = {4},
  pages        = {572-593},
  shortjournal = {Algorithmica},
  title        = {Guarding polyominoes under k-hop visibility},
  volume       = {87},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Fixed parameter multi-objective evolutionary algorithms for the W-separator problem. <em>Alg</em>, <em>87</em>(4), 537-571. (<a href='https://doi.org/10.1007/s00453-024-01290-9'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Parameterized analysis provides powerful mechanisms for obtaining fine-grained insights into different types of algorithms. In this work, we combine this field with evolutionary algorithms and provide parameterized complexity analysis of evolutionary multi-objective algorithms for the W-separator problem, which is a natural generalization of the vertex cover problem. The goal is to remove the minimum number of vertices such that each connected component in the resulting graph has at most W vertices. We provide different multi-objective formulations involving two or three objectives that provably lead to fixed-parameter evolutionary algorithms with respect to the value of an optimal solution OPT and W. Of particular interest are kernelizations and the reducible structures used for them. We show that in expectation the algorithms make incremental progress in finding such structures and beyond. The current best known kernelization of the W-separator uses linear programming methods and requires non-trivial post-processing steps to extract the reducible structures. We provide additional structural features to show that evolutionary algorithms with appropriate objectives are also capable of extracting them. Our results show that evolutionary algorithms with different objectives guide the search and admit fixed parameterized runtimes to solve or approximate (even arbitrarily close) the W-separator problem.},
  archive      = {J_Alg},
  author       = {Baguley, Samuel and Friedrich, Tobias and Neumann, Aneta and Neumann, Frank and Pappik, Marcus and Zeif, Ziena},
  doi          = {10.1007/s00453-024-01290-9},
  journal      = {Algorithmica},
  month        = {4},
  number       = {4},
  pages        = {537-571},
  shortjournal = {Algorithmica},
  title        = {Fixed parameter multi-objective evolutionary algorithms for the W-separator problem},
  volume       = {87},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). The compact genetic algorithm struggles on cliff functions. <em>Alg</em>, <em>87</em>(4), 507-536. (<a href='https://doi.org/10.1007/s00453-024-01281-w'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Estimation of distribution algorithms (EDAs) are general-purpose optimizers that maintain a probability distribution over a given search space. This probability distribution is updated through sampling from the distribution and a reinforcement learning process which rewards solution components that have shown to be part of good quality samples. The compact genetic algorithm (cGA) is a non-elitist EDA able to deal with difficult multimodal fitness landscapes that are hard to solve by elitist algorithms. We investigate the cGA on the Cliff function for which it was shown recently that non-elitist evolutionary algorithms and artificial immune systems optimize it in expected polynomial time. We point out that the cGA faces major difficulties when solving the Cliff function and investigate its dynamics both experimentally and theoretically. Our experimental results indicate that the cGA requires exponential time for all values of the update strength 1/K. We show theoretically that, under sensible assumptions, there is a negative drift when sampling around the location of the cliff. Experiments further suggest that there is a phase transition for K where the expected optimization time drops from $$n^{\Theta (n)}$$ to $$2^{\Theta (n)}$$ .},
  archive      = {J_Alg},
  author       = {Neumann, Frank and Sudholt, Dirk and Witt, Carsten},
  doi          = {10.1007/s00453-024-01281-w},
  journal      = {Algorithmica},
  month        = {4},
  number       = {4},
  pages        = {507-536},
  shortjournal = {Algorithmica},
  title        = {The compact genetic algorithm struggles on cliff functions},
  volume       = {87},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). XNLP-completeness for parameterized problems on graphs with a linear structure. <em>Alg</em>, <em>87</em>(4), 465-506. (<a href='https://doi.org/10.1007/s00453-024-01274-9'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we showcase the class XNLP as a natural place for many hard problems parameterized by linear width measures. This strengthens existing W[1]-hardness proofs for these problems, since XNLP-hardness implies W[t]-hardness for all t. It also indicates, via a conjecture by Pilipczuk and Wrochna (ACM Trans Comput Theory 9:1–36, 2018), that any XP algorithm for such problems is likely to require XP space. In particular, we show XNLP-completeness for natural problems parameterized by pathwidth, linear clique-width, and linear mim-width. The problems we consider are Independent Set, Dominating Set, Odd Cycle Transversal, (q-)Coloring, Max Cut, Maximum Regular Induced Subgraph, Feedback Vertex Set, Capacitated (Red-Blue) Dominating Set, Capacitated Vertex Cover and Bipartite Bandwidth.},
  archive      = {J_Alg},
  author       = {Bodlaender, Hans L. and Groenland, Carla and Jacob, Hugo and Jaffke, Lars and Lima, Paloma T.},
  doi          = {10.1007/s00453-024-01274-9},
  journal      = {Algorithmica},
  month        = {4},
  number       = {4},
  pages        = {465-506},
  shortjournal = {Algorithmica},
  title        = {XNLP-completeness for parameterized problems on graphs with a linear structure},
  volume       = {87},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Complexity framework for forbidden subgraphs i: The framework. <em>Alg</em>, <em>87</em>(3), 429-464. (<a href='https://doi.org/10.1007/s00453-024-01289-2'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {For a set of graphs $${\mathcal {H}}$$ , a graph G is $${\mathcal {H}}$$ -subgraph-free if G does not contain any graph from $${{{\mathcal {H}}}}$$ as a subgraph. We propose general and easy-to-state conditions on graph problems that explain a large set of results for $${\mathcal {H}}$$ -subgraph-free graphs. Namely, a graph problem must be efficiently solvable on graphs of bounded treewidth, computationally hard on subcubic graphs, and computational hardness must be preserved under edge subdivision of subcubic graphs. Our meta-classification says that if a graph problem $$\Pi $$ satisfies all three conditions, then for every finite set $${{{\mathcal {H}}}}$$ , it is “efficiently solvable” on $${{{\mathcal {H}}}}$$ -subgraph-free graphs if $${\mathcal {H}}$$ contains a disjoint union of one or more paths and subdivided claws, and $$\Pi $$ is “computationally hard” otherwise. We apply our meta-classification on many well-known partitioning, covering and packing problems, network design problems and width parameter problems to obtain a dichotomy between polynomial-time solvability and NP-completeness. For distance-metric problems, we obtain a dichotomy between almost-linear-time solvability and having no subquadratic-time algorithm (conditioned on some hardness hypotheses). Apart from capturing a large number of explicitly and implicitly known results in the literature, we also prove a number of new results. Moreover, we perform an extensive comparison between the subgraph framework and the existing frameworks for the minor and topological minor relations, and pose several new open problems and research directions.},
  archive      = {J_Alg},
  author       = {Johnson, Matthew and Martin, Barnaby and Oostveen, Jelle J. and Pandey, Sukanya and Paulusma, Daniël and Smith, Siani and van Leeuwen, Erik Jan},
  doi          = {10.1007/s00453-024-01289-2},
  journal      = {Algorithmica},
  month        = {3},
  number       = {3},
  pages        = {429-464},
  shortjournal = {Algorithmica},
  title        = {Complexity framework for forbidden subgraphs i: The framework},
  volume       = {87},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). FREIGHT: Fast streaming hypergraph partitioning. <em>Alg</em>, <em>87</em>(3), 405-428. (<a href='https://doi.org/10.1007/s00453-024-01291-8'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Partitioning the vertices of a (hyper)graph into k roughly balanced blocks such that few (hyper)edges run between blocks is a key problem for large-scale distributed processing. A current trend for partitioning huge (hyper)graphs using low computational resources are streaming algorithms. In this work, we propose FREIGHT: a Fast stREamInG Hypergraph parTitioning algorithm which is an adaptation of the widely-known graph-based algorithm Fennel. By using an efficient data structure, we make the overall running of FREIGHT linearly dependent on the pin-count of the hypergraph and the memory consumption linearly dependent on the numbers of nets and blocks. The results of our extensive experimentation showcase the promising performance of FREIGHT as a highly efficient and effective solution for streaming hypergraph partitioning. Our algorithm demonstrates competitive running time with the Hashing algorithm, with a geometric mean runtime within a factor of four compared to the Hashing algorithm. Significantly, our findings highlight the superiority of FREIGHT over all existing (buffered) streaming algorithms and even the in-memory algorithm HYPE, with respect to both cut-net and connectivity measures. This indicates that our proposed algorithm is a promising hypergraph partitioning tool to tackle the challenge posed by large-scale and dynamic data processing.},
  archive      = {J_Alg},
  author       = {Eyubov, Kamal and Fonseca Faraj, Marcelo and Schulz, Christian},
  doi          = {10.1007/s00453-024-01291-8},
  journal      = {Algorithmica},
  month        = {3},
  number       = {3},
  pages        = {405-428},
  shortjournal = {Algorithmica},
  title        = {FREIGHT: Fast streaming hypergraph partitioning},
  volume       = {87},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Shared versus private randomness in distributed interactive proofs. <em>Alg</em>, <em>87</em>(3), 377-404. (<a href='https://doi.org/10.1007/s00453-024-01288-3'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In distributed interactive proofs, the nodes of a graph G interact with a powerful but untrustable prover who tries to convince them, in a small number of rounds and through short messages, that G satisfies some property. This series of rounds is followed by a phase of distributed verification, which may be either deterministic or randomized, where nodes exchange messages with their neighbors. The nature of this last verification round defines the two types of interactive protocols. We say that the protocol is of Arthur–Merlin type if the verification round is deterministic. We say that the protocol is of Merlin–Arthur type if, in the verification round, the nodes are allowed to use a fresh set of random bits. In the original model introduced by Kol, Oshman, and Saxena [PODC 2018], the randomness was private in the sense that each node had only access to an individual source of random coins. Crescenzi, Fraigniaud, and Paz [DISC 2019] initiated the study of the impact of shared randomness (the situation where the coin tosses are visible to all nodes) in the distributed interactive model. In this work, we continue that research line by showing that the impact of the two forms of randomness is very different depending on whether we are considering Arthur–Merlin protocols or Merlin–Arthur protocols. While private randomness gives more power to the first type of protocols, shared randomness provides more power to the second. We also show that there exists at most an exponential gap between the certificate size in distributed interactive proofs with respect to distributed verification protocols without any randomness.},
  archive      = {J_Alg},
  author       = {Montealegre, Pedro and Ramírez-Romero, Diego and Rapaport, Ivan},
  doi          = {10.1007/s00453-024-01288-3},
  journal      = {Algorithmica},
  month        = {3},
  number       = {3},
  pages        = {377-404},
  shortjournal = {Algorithmica},
  title        = {Shared versus private randomness in distributed interactive proofs},
  volume       = {87},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). An efficient algorithm for power dominating set. <em>Alg</em>, <em>87</em>(3), 344-376. (<a href='https://doi.org/10.1007/s00453-024-01283-8'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The problem Power Dominating Set (PDS) is motivated by the placement of phasor measurement units to monitor electrical networks. It asks for a minimum set of vertices in a graph that observes all remaining vertices by exhaustively applying two observation rules. Our contribution is twofold. First, we determine the parameterized complexity of PDS by proving it is W[P]-complete when parameterized with respect to the solution size. We note that it was only known to be W[2]-hard before. Our second and main contribution is a new algorithm for PDS that efficiently solves practical instances. Our algorithm consists of two complementary parts. The first is a set of reduction rules for PDS that can also be used in conjunction with previously existing algorithms. The second is an algorithm for solving the remaining kernel based on the implicit hitting set approach. Our evaluation on a set of power grid instances from the literature shows that our solver outperforms previous state-of-the-art solvers for PDS by more than one order of magnitude on average. Furthermore, our algorithm can solve previously unsolved instances of continental scale within a few minutes.},
  archive      = {J_Alg},
  author       = {Bläsius, Thomas and Göttlicher, Max},
  doi          = {10.1007/s00453-024-01283-8},
  journal      = {Algorithmica},
  month        = {3},
  number       = {3},
  pages        = {344-376},
  shortjournal = {Algorithmica},
  title        = {An efficient algorithm for power dominating set},
  volume       = {87},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Symmetry breaking in the plane. <em>Alg</em>, <em>87</em>(3), 321-343. (<a href='https://doi.org/10.1007/s00453-024-01286-5'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We study a fundamental question related to the feasibility of deterministic symmetry breaking in the infinite Euclidean plane for two robots that have minimal or no knowledge of the respective capabilities and “measuring instruments” of themselves and each other. Assume that two anonymous mobile robots are placed at different locations at unknown distance d from each other on the infinite Euclidean plane. Each robot knows neither the location of itself nor of the other robot. The robots cannot communicate wirelessly, but have a certain nonzero visibility radius r (with range r unknown to the robots). By rendezvous we mean that they are brought at distance at most r of each other by executing symmetric (identical) mobility algorithms. The robots are moving with unknown and constant but not necessarily identical speeds, their clocks and pedometers may be asymmetric, and their chirality inconsistent. We demonstrate that rendezvous for two robots is feasible under the studied model iff the robots have either: different speeds; or different clocks; or different orientations but equal chiralities. When the rendezvous is feasible, we provide a universal algorithm which always solves rendezvous despite the fact that the robots have no knowledge of which among their respective parameters may be different.},
  archive      = {J_Alg},
  author       = {Czyzowicz, Jurek and Gąsieniec, Leszek and Killick, Ryan and Kranakis, Evangelos},
  doi          = {10.1007/s00453-024-01286-5},
  journal      = {Algorithmica},
  month        = {3},
  number       = {3},
  pages        = {321-343},
  shortjournal = {Algorithmica},
  title        = {Symmetry breaking in the plane},
  volume       = {87},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Popular roommates in simply exponential time. <em>Alg</em>, <em>87</em>(2), 292-320. (<a href='https://doi.org/10.1007/s00453-024-01287-4'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider the popular matching problem in a roommates instance G on n vertices, i.e., G is a graph where each vertex has a strict preference order over its neighbors. A matching M is popular if there is no matching N such that the vertices that prefer N to M outnumber those that prefer M to N. It is known that it is NP-hard to decide if G admits a popular matching or not. There is no better algorithm known for this problem than the brute force algorithm that enumerates all matchings and tests each for popularity—this could take n! time. Here we show an $$O^*(k^n)$$ time algorithm for this problem, where $$k < 7.32$$ . We use the recent breakthrough result on the maximum number of stable matchings possible in a roommates instance to analyze our algorithm for the popular matching problem. We identify a natural (also, hard) subclass of popular matchings called truly popular matchings that are “popular fractional” and show an $$O^*(2^n)$$ time algorithm for the truly popular matching problem in G. We also identify a subclass of max-size popular matchings called super-dominant matchings and show a linear time algorithm for the super-dominant roommates problem.},
  archive      = {J_Alg},
  author       = {Kavitha, Telikepalli},
  doi          = {10.1007/s00453-024-01287-4},
  journal      = {Algorithmica},
  month        = {2},
  number       = {2},
  pages        = {292-320},
  shortjournal = {Algorithmica},
  title        = {Popular roommates in simply exponential time},
  volume       = {87},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Galloping in fast-growth natural merge sorts. <em>Alg</em>, <em>87</em>(2), 242-291. (<a href='https://doi.org/10.1007/s00453-024-01285-6'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We study the impact of merging routines in merge-based sorting algorithms. More precisely, we focus on the galloping routine that TimSort uses to merge monotonic sub-arrays, hereafter called runs, and on the impact on the number of element comparisons performed if one uses this routine instead of a naïve merging routine. This routine was introduced in order to make TimSort more efficient on arrays with few distinct values. Alas, we prove that, although it makes TimSort sort array with two values in linear time, it does not prevent TimSort from requiring up to $$\Theta (n \log (n))$$ element comparisons to sort arrays of length n with three distinct values. However, we also prove that slightly modifying TimSort ’s galloping routine results in requiring only $$\mathcal {O}(n + n \log (\sigma ))$$ element comparisons in the worst case, when sorting arrays of length n with $$\sigma $$ distinct values. We do so by focusing on the notion of dual runs, which was introduced in the 1990s, and on the associated dual run-length entropy. This notion is both related to the number of distinct values and to the number of runs in an array, which came with its own run-length entropy that was used to explain TimSort ’s otherwise “supernatural” efficiency. We also introduce new notions of fast- and middle-growth for natural merge sorts (i.e., algorithms based on merging runs), which are found in several sorting algorithms similar to TimSort. We prove that algorithms with the fast- or middle-growth property, provided that they use our variant of TimSort ’s galloping routine for merging runs, are as efficient as possible at sorting arrays with low run-induced or dual-run-induced complexities.},
  archive      = {J_Alg},
  author       = {Ghasemi, Elahe and Jugé, Vincent and Khalighinejad, Ghazal and Yazdanyar, Helia},
  doi          = {10.1007/s00453-024-01285-6},
  journal      = {Algorithmica},
  month        = {2},
  number       = {2},
  pages        = {242-291},
  shortjournal = {Algorithmica},
  title        = {Galloping in fast-growth natural merge sorts},
  volume       = {87},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Line intersection searching amid unit balls in 3-space. <em>Alg</em>, <em>87</em>(2), 223-241. (<a href='https://doi.org/10.1007/s00453-024-01284-7'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Let $$\mathscr {B}$$ be a set of n unit balls in $${\mathbb {R}}^3$$ . We present a linear-size data structure for storing $$\mathscr {B}$$ that can determine in $$O^*(\sqrt{n})$$ time whether a query line intersects any ball of $$\mathscr {B}$$ and report all k such balls in additional O(k) time. The data structure can be constructed in $$O(n\log n)$$ time. (The $$O^*(\cdot )$$ notation hides subpolynomial factors, e.g., of the form $$O(n^{{\varepsilon }})$$ , for arbitrarily small $${\varepsilon }> 0$$ , and their coefficients which depend on $${\varepsilon }$$ .) We also consider the dual problem: Let $$\mathscr {L}$$ be a set of n lines in $${\mathbb {R}}^3$$ . We preprocess $$\mathscr {L}$$ , in $$O^*(n^2)$$ time, into a data structure of size $$O^*(n^2)$$ that can determine in $$O(\log {n})$$ time whether a query unit ball intersects any line of $$\mathscr {L}$$ , or report all k such lines in additional O(k) time.},
  archive      = {J_Alg},
  author       = {Agarwal, Pankaj K. and Ezra, Esther},
  doi          = {10.1007/s00453-024-01284-7},
  journal      = {Algorithmica},
  month        = {2},
  number       = {2},
  pages        = {223-241},
  shortjournal = {Algorithmica},
  title        = {Line intersection searching amid unit balls in 3-space},
  volume       = {87},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Partition strategies for the Maker–Breaker domination game. <em>Alg</em>, <em>87</em>(2), 191-222. (<a href='https://doi.org/10.1007/s00453-024-01280-x'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Maker–Breaker domination game is a positional game played on a graph by two players called Dominator and Staller. The players alternately select a vertex of the graph that has not yet been chosen. Dominator wins if at some point the vertices she has chosen form a dominating set of the graph. Staller wins if Dominator cannot form a dominating set. Deciding if Dominator has a winning strategy has been shown to be a PSPACE-complete problem even when restricted to chordal or bipartite graphs. In this paper, we consider strategies for Dominator based on partitions of the graph into basic subgraphs where Dominator wins as the second player. Using partitions into cycles and edges (also called perfect [1,2]-factors), we show that Dominator always wins in regular graphs and that deciding whether Dominator has a winning strategy as a second player can be computed in polynomial time for outerplanar and block graphs. We then study partitions into subgraphs with two universal vertices, which is equivalent to considering the existence of pairing dominating sets with adjacent pairs. We show that in interval graphs, Dominator wins if and only if such a partition exists. In particular, this implies that deciding whether Dominator has a winning strategy playing second is in NP for interval graphs. We finally provide an algorithm in $$n^{k+3}$$ for interval graphs with at most k nested intervals.},
  archive      = {J_Alg},
  author       = {Bagan, Guillaume and Duchêne, Eric and Gledel, Valentin and Lehtilä, Tuomo and Parreau, Aline},
  doi          = {10.1007/s00453-024-01280-x},
  journal      = {Algorithmica},
  month        = {2},
  number       = {2},
  pages        = {191-222},
  shortjournal = {Algorithmica},
  title        = {Partition strategies for the Maker–Breaker domination game},
  volume       = {87},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Optimal algorithms for online b-matching with variable vertex capacities. <em>Alg</em>, <em>87</em>(2), 167-190. (<a href='https://doi.org/10.1007/s00453-024-01282-9'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We study the b-matching problem, which generalizes classical online matching introduced by Karp, Vazirani and Vazirani (STOC 1990). Consider a bipartite graph $$G=(S\dot{\cup }R,E)$$ . Every vertex $$s\in S$$ is a server with a capacity $$b_s$$ , indicating the number of possible matching partners. The vertices $$r\in R$$ are requests that arrive online and must be matched immediately to an eligible server. The goal is to maximize the cardinality of the constructed matching. In contrast to earlier work, we study the general setting where servers may have arbitrary, individual capacities. We prove that the most natural and simple online algorithms achieve optimal competitive ratios. As for deterministic algorithms, we give a greedy algorithm RelativeBalance and analyze it by extending the primal-dual framework of Devanur, Jain and Kleinberg (SODA 2013). In the area of randomized algorithms we study the celebrated Ranking algorithm by Karp, Vazirani and Vazirani. We prove that the original Ranking strategy, simply picking a random permutation of the servers, achieves an optimal competitiveness of $$1-1/e$$ , independently of the server capacities. Hence it is not necessary to resort to a reduction, replacing every server s by $$b_s$$ vertices of unit capacity and to then run Ranking on this graph with $$\sum _{s\in S} b_s$$ vertices on the left-hand side. Additionally, we extend this result to the vertex-weighted b-matching problem. Technically, we formulate a new configuration LP for the b-matching problem and conduct a primal-dual analysis.},
  archive      = {J_Alg},
  author       = {Albers, Susanne and Schubert, Sebastian},
  doi          = {10.1007/s00453-024-01282-9},
  journal      = {Algorithmica},
  month        = {2},
  number       = {2},
  pages        = {167-190},
  shortjournal = {Algorithmica},
  title        = {Optimal algorithms for online b-matching with variable vertex capacities},
  volume       = {87},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Better hardness results for the minimum spanning tree congestion problem. <em>Alg</em>, <em>87</em>(1), 148-165. (<a href='https://doi.org/10.1007/s00453-024-01278-5'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the spanning tree congestion problem, given a connected graph G, the objective is to compute a spanning tree T in G that minimizes its maximum edge congestion, where the congestion of an edge e of T is the number of edges in G for which the unique path in T between their endpoints traverses e. The problem is known to be $$\mathbb{N}\mathbb{P}$$ -hard, but its approximability is still poorly understood, and it is not even known whether the optimum solution can be efficiently approximated with ratio o(n). In the decision version of this problem, denoted $${\varvec{K}-\textsf {STC}}$$ , we need to determine if G has a spanning tree with congestion at most K. It is known that $${\varvec{K}-\textsf {STC}}$$ is $$\mathbb{N}\mathbb{P}$$ -complete for $$K\ge 8$$ , and this implies a lower bound of 1.125 on the approximation ratio of minimizing congestion. On the other hand, $${\varvec{3}-\textsf {STC}}$$ can be solved in polynomial time, with the complexity status of this problem for $$K\in { \left\{ 4,5,6,7 \right\} }$$ remaining an open problem. We substantially improve the earlier hardness results by proving that $${\varvec{K}-\textsf {STC}}$$ is $$\mathbb{N}\mathbb{P}$$ -complete for $$K\ge 5$$ . This leaves only the case $$K=4$$ open, and improves the lower bound on the approximation ratio to 1.2. Motivated by evidence that minimizing congestion is hard even for graphs of small constant radius, we also consider $${\varvec{K}-\textsf {STC}}$$ restricted to graphs of radius 2, and we prove that this variant is $$\mathbb{N}\mathbb{P}$$ -complete for all $$K\ge 6$$ .},
  archive      = {J_Alg},
  author       = {Luu, Huong and Chrobak, Marek},
  doi          = {10.1007/s00453-024-01278-5},
  journal      = {Algorithmica},
  month        = {1},
  number       = {1},
  pages        = {148-165},
  shortjournal = {Algorithmica},
  title        = {Better hardness results for the minimum spanning tree congestion problem},
  volume       = {87},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Euclidean maximum matchings in the Plane—Local to global. <em>Alg</em>, <em>87</em>(1), 132-147. (<a href='https://doi.org/10.1007/s00453-024-01279-4'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Let M be a perfect matching on a set of points in the plane where every edge is a line segment between two points. We say that M is globally maximum if it is a maximum-length matching on all points. We say that M is k-local maximum if for any subset $$M'=\{a_1b_1,\dots ,a_kb_k\}$$ of k edges of M it holds that $$M'$$ is a maximum-length matching on points $$\{a_1,b_1,\dots ,a_k,b_k\}$$ . We show that local maximum matchings are good approximations of global ones. Let $$\mu _k$$ be the infimum ratio of the length of any k-local maximum matching to the length of any global maximum matching, over all finite point sets in the Euclidean plane. It is known that $$\mu _k\geqslant \frac{k-1}{k}$$ for any $$k\geqslant 2$$ . We show the following improved bounds for $$k\in \{2,3\}$$ : $$\sqrt{3/7}\leqslant \mu _2< 0.93 $$ and $$\sqrt{3}/2\leqslant \mu _3< 0.98$$ . We also show that every pairwise crossing matching is unique and it is globally maximum. Towards our proof of the lower bound for $$\mu _2$$ we show the following result which is of independent interest: If we increase the radii of pairwise intersecting disks by factor $$2/\sqrt{3}$$ , then the resulting disks have a common intersection.},
  archive      = {J_Alg},
  author       = {Biniaz, Ahmad and Maheshwari, Anil and Smid, Michiel},
  doi          = {10.1007/s00453-024-01279-4},
  journal      = {Algorithmica},
  month        = {1},
  number       = {1},
  pages        = {132-147},
  shortjournal = {Algorithmica},
  title        = {Euclidean maximum matchings in the Plane—Local to global},
  volume       = {87},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Online paging with heterogeneous cache slots. <em>Alg</em>, <em>87</em>(1), 89-131. (<a href='https://doi.org/10.1007/s00453-024-01270-z'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {It is natural to generalize the online $$k$$ -Server problem by allowing each request to specify not only a point p, but also a subset S of servers that may serve it. To date, only a few special cases of this problem have been studied. The objective of the work presented in this paper has been to more systematically explore this generalization in the case of uniform and star metrics. For uniform metrics, the problem is equivalent to a generalization of Paging in which each request specifies not only a page p, but also a subset S of cache slots, and is satisfied by having a copy of p in some slot in S. We call this problem Slot-Heterogenous Paging. In realistic settings only certain subsets of cache slots or servers would appear in requests. Therefore we parameterize the problem by specifying a family $${\mathcal {S}}\subseteq 2^{[k]}$$ of requestable slot sets, and we establish bounds on the competitive ratio as a function of the cache size k and family $${\mathcal {S}}$$ : Some results for the laminar case are shown via a reduction to the generalization of Paging in which each request specifies a set $$P$$ of pages, and is satisfied by fetching any page from $$P$$ into the cache. The optimal ratios for the latter problem (with laminar family of height h) are at most hk (deterministic) and $$hH_k$$ (randomized).},
  archive      = {J_Alg},
  author       = {Chrobak, Marek and Haney, Samuel and Liaee, Mehraneh and Panigrahi, Debmalya and Rajaraman, Rajmohan and Sundaram, Ravi and Young, Neal E.},
  doi          = {10.1007/s00453-024-01270-z},
  journal      = {Algorithmica},
  month        = {1},
  number       = {1},
  pages        = {89-131},
  shortjournal = {Algorithmica},
  title        = {Online paging with heterogeneous cache slots},
  volume       = {87},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Anti-factor is FPT parameterized by treewidth and list size (but counting is hard). <em>Alg</em>, <em>87</em>(1), 22-88. (<a href='https://doi.org/10.1007/s00453-024-01265-w'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the general AntiFactor problem, a graph G and, for every vertex v of G, a set $$X_v\subseteq {\mathbb {N}}$$ of forbidden degrees is given. The task is to find a set S of edges such that the degree of v in S is not in the set $$X_v$$ . Standard techniques (dynamic programming plus fast convolution) can be used to show that if M is the largest forbidden degree, then the problem can be solved in time $$(M+2)^{{\operatorname {tw}}}\cdot n^{{\mathcal {O}}(1)}$$ if a tree decomposition of width $${\operatorname {tw}}$$ is given. However, significantly faster algorithms are possible if the sets $$X_v$$ are sparse: our main algorithmic result shows that if every vertex has at most $$x$$ forbidden degrees (we call this special case AntiFactorx), then the problem can be solved in time $$(x+1)^{{\mathcal {O}}({\operatorname {tw}})}\cdot n^{{\mathcal {O}}(1)}$$ . That is, AntiFactorx is fixed-parameter tractable parameterized by treewidth $${\operatorname {tw}}$$ and the maximum number $$x$$ of excluded degrees. Our algorithm uses the technique of representative sets, which can be generalized to the optimization version, but (as expected) not to the counting version of the problem. In fact, we show that #AntiFactor1 is already #W $$[1]$$ -hard parameterized by the width of the given decomposition. Moreover, we show that, unlike for the decision version, the standard dynamic programming algorithm is essentially optimal for the counting version. Formally, for a fixed nonempty set $$X$$ , we denote by $$X$$ -AntiFactor the special case where every vertex v has the same set $$X_v=X$$ of forbidden degrees. We show the following lower bound for every fixed set $$X$$ : if there is an $$\epsilon >0$$ such that # $$X$$ -AntiFactor can be solved in time $$(\max X+2-\epsilon )^{{\operatorname {tw}}}\cdot n^{{\mathcal {O}}(1)}$$ given a tree decomposition of width $${\operatorname {tw}}$$ , then the counting strong exponential-time hypothesis (#SETH) fails.},
  archive      = {J_Alg},
  author       = {Marx, Dániel and Sankar, Govind S. and Schepper, Philipp},
  doi          = {10.1007/s00453-024-01265-w},
  journal      = {Algorithmica},
  month        = {1},
  number       = {1},
  pages        = {22-88},
  shortjournal = {Algorithmica},
  title        = {Anti-factor is FPT parameterized by treewidth and list size (but counting is hard)},
  volume       = {87},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). On scheduling mechanisms beyond the worst case. <em>Alg</em>, <em>87</em>(1), 1-21. (<a href='https://doi.org/10.1007/s00453-024-01277-6'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The problem of scheduling unrelated machines has been studied since the inception of algorithmic mechanism design (Nisan and Ronen, Algorithmic mechanism design(extended abstract). In: Proceedings of the Thirty First Annual ACM Symposium on Theory of Computing (STOC), pp. 129–140, 1999. It is a resource allocation problem that entails assigning m tasks to n machines for execution. Machines are regarded as strategic agents who may lie about their execution costs so as to minimize their time cost. To address the situation when monetary payment is not an option to compensate the machines’ costs, Koutsoupias (Theory Comput Syst 54:375–387, 2014) devised two truthful mechanisms, K and P respectively, that achieves an approximation ratio of $$\frac{n+1}{2}$$ and n, for social cost minimization. In addition, no truthful mechanism can achieve an approximation ratio better than $$\frac{n+1}{2}$$ . Hence, mechanism K is optimal. While the approximation ratio provides a strong worst-case guarantee, it also limits us to a comprehensive understanding of mechanism performance on various inputs. This paper investigates these two scheduling mechanisms beyond the worst case. We first show that mechanism K achieves a smaller social cost than mechanism P on every input. That is, mechanism K is pointwise better than mechanism P. Next, for each task, when machines’ execution costs are independent and identically drawn from a task-specific distribution, we show that the average-case approximation ratio of mechanism K converges to a constant determined by the task-specific distribution. This bound is tight for mechanism K. For a better understanding of this distribution-dependent constant, on the one hand, we estimate its value by plugging in a few common distributions; on the other, we show that this converging bound improves a known bound (Zhang in Algorithmica 83(6):1638–1652, 2021)) which only captures the single-task setting. Last, we find that the average-case approximation ratio of mechanism P converges to the same constant.},
  archive      = {J_Alg},
  author       = {Gao, Yansong and Zhang, Jie},
  doi          = {10.1007/s00453-024-01277-6},
  journal      = {Algorithmica},
  month        = {1},
  number       = {1},
  pages        = {1-21},
  shortjournal = {Algorithmica},
  title        = {On scheduling mechanisms beyond the worst case},
  volume       = {87},
  year         = {2025},
}
</textarea>
</details></li>
</ul>

</body>
</html>
