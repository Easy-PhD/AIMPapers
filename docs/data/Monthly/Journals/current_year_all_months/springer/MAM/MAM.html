<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>MAM</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="mam">MAM - 41</h2>
<ul>
<li><details>
<summary>
(2025). Designing with uncertainty: LLM interfaces as transitional spaces for democratic revival. <em>MAM</em>, <em>35</em>(4), 1-23. (<a href='https://doi.org/10.1007/s11023-025-09736-x'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The integration of large language models (LLMs) into our conversational infrastructure presents a critical inflection point for democratic practice. While contemporary digital platforms systematically erode transitional conversational spaces—interfaces between private intuition and public deliberation where tentative thoughts can be explored—this paper argues that specialized LLM interfaces could potentially reconstruct these essential democratic environments. I propose a design framework for ‘transitional conversational spaces’ that leverages uncertainty expression not merely to prevent unwarranted epistemic confidence but to create communicative environments conducive to democratic capability development. Drawing on theories of democratic deliberation and moral perception, this paper distinguishes between epistemic uncertainty (addressable through additional information) and hermeneutic uncertainty (concerning the inherently contestable nature of interpretation). The proposed framework emphasizes ‘ensemble interfaces’ that make visible the contingent nature of value judgments by presenting outputs from multiple models trained on different datasets. The design principles outlined challenge tokenistic participation by advocating for substantive participatory infrastructure with features like ‘tinkerability’—enabling communities to experiment with system configurations—and mechanisms that counter designer-centric development models. These principles stand in contrast to conventional ‘participatory AI’ approaches that treat engagement as merely instrumental to system optimization rather than as constitutive of democratic practice. This paper does not claim to solve all challenges of democratic participation but rather identifies one valuable design direction that could potentially enhance our collective capacity for exploratory dialogue. Implementation would require institutional transformations that align technological development with democratic values beyond current procedural approaches to AI governance.},
  archive      = {J_MAM},
  author       = {Delacroix, Sylvie},
  doi          = {10.1007/s11023-025-09736-x},
  journal      = {Minds and Machines},
  month        = {12},
  number       = {4},
  pages        = {1-23},
  shortjournal = {Minds Mach.},
  title        = {Designing with uncertainty: LLM interfaces as transitional spaces for democratic revival},
  volume       = {35},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Avatars as parts: A reply to sweeney. <em>MAM</em>, <em>35</em>(3), 1-18. (<a href='https://doi.org/10.1007/s11023-025-09731-2'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper responds to Paula Sweeney’s characterization of avatars as proxies, proposing instead a framework that treats avatars as genuine parts of “hybrid persons.” Adopting a four-dimensionalist metaphysics, I argue that persons should be understood as maximal aggregates of both biological and virtual temporal parts. This approach reconceptualizes the relationship between users and their avatars as analogous to the relationship between present and past selves rather than as a proxy relationship between separate entities. While Sweeney identifies an “epistemic gap” in avatar-user relationships as problematic for responsibility attribution, I argue that this gap is not unique to virtual contexts but appears in ordinary cases of personal identity over time. The real challenge stems from what I term the “consciousness gap” - our inability to directly experience our avatar’s actions. Drawing on Parfit’s insights about the unimportance of identity in the survival of persons, I show a way to attribute responsibility across such gaps. To address potential challenges to my framework similar to Johnston’s personite problem, I adopt a priority perdurantist solution to persons, treating both biological and virtual parts as metaphysically derivative from the hybrid person.},
  archive      = {J_MAM},
  author       = {Patrone, Fabio},
  doi          = {10.1007/s11023-025-09731-2},
  journal      = {Minds and Machines},
  month        = {9},
  number       = {3},
  pages        = {1-18},
  shortjournal = {Minds Mach.},
  title        = {Avatars as parts: A reply to sweeney},
  volume       = {35},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Global health in the age of AI: Charting a course for ethical implementation and societal benefit. <em>MAM</em>, <em>35</em>(3), 1-35. (<a href='https://doi.org/10.1007/s11023-025-09730-3'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Artificial Intelligence (AI) presents unprecedented opportunities to transform healthcare worldwide, from improving diagnostic accuracy to expanding access in underserved regions. Despite this potential and growing investment, a significant gap persists between AI’s theoretical promise and its realised benefits in healthcare settings. This article examines the complex barriers impeding AI benefits realization in global health contexts, including ethical uncertainties, data infrastructure limitations, evidence quality concerns, and regulatory ambiguities. We analyze current initiatives addressing these challenges and highlight how technological solutions alone cannot resolve fundamental healthcare inequities. Drawing on the interdisciplinary perspectives and insights presented at the Global Health in the Age of AI Symposium hosted by the Cini Foundation and Yale Digital Ethics Center, we propose five core infrastructure requirements necessary for ethical AI implementation: robust data exchange; epistemic certainty with staff autonomy; actively protected healthcare values; validated outcomes with meaningful accountability; and environmental sustainability. These requirements form the foundation for a systems approach that balances technological advancement with ethical imperatives, contextual adaptability, and global equity considerations. We conclude that the successful integration of AI into healthcare demands coordinated action across sectors and borders, with careful attention to avoiding technological colonialism and ensuring AI serves as a force for health equity rather than widening existing disparities.},
  archive      = {J_MAM},
  author       = {Morley, Jessica and Hine, Emmie and Roberts, Huw and Sirbu, Renée and Ashrafian, Hutan and Blease, Charlotte and Boyd, Marisha and Chen, John L. and Filho, Alexandre Chiavegatto and Coiera, Enrico and Cohen, Glenn I. and Fiske, Amelia and Jayakumar, Nandini and Kerasidou, Angeliki and Mandreoli, Federica and McCradden, Melissa D. and Namuganza, Stella and Nsoesie, Elaine O. and Parikh, Ravi B. and Reddy, Sandeep and Sedlakova, Jana and Sunbul, Tamara and van Baalen, Sophie and van Kolfschooten, Hannah and Floridi, Luciano},
  doi          = {10.1007/s11023-025-09730-3},
  journal      = {Minds and Machines},
  month        = {9},
  number       = {3},
  pages        = {1-35},
  shortjournal = {Minds Mach.},
  title        = {Global health in the age of AI: Charting a course for ethical implementation and societal benefit},
  volume       = {35},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Distant writing: Literary production in the age of artificial intelligence. <em>MAM</em>, <em>35</em>(3), 1-26. (<a href='https://doi.org/10.1007/s11023-025-09732-1'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article introduces the concept of “distant writing”, a novel literary practice in which authors act as designers, employing Large Language Models (LLMs) to generate narratives, while retaining creative control through precise prompting and iterative refinement. Unlike Moretti’s distant reading, which uses computational analysis to interpret large corpora of existing texts, distant writing harnesses computational tools (LLMs) to author new narratives, reshaping the literary production process. By examining theoretical frameworks and practical consequences, and running an experiment in distant writing called Encounters, this article argues that distant writing represents a significant evolution in authorship, not replacing but expanding human creativity within a design paradigm. The distinction between writing (close) and “wrAIting” (distant) reveals how LLM-assisted creativity can generate narrative possibilities previously inaccessible, transforming literature’s modal space while challenging traditional notions of authorship, creativity, and literary production. This emerging practice merits critical attention as it shapes future literary landscapes and reconfigures relationships between human creativity and artificial intelligence.},
  archive      = {J_MAM},
  author       = {Floridi, Luciano},
  doi          = {10.1007/s11023-025-09732-1},
  journal      = {Minds and Machines},
  month        = {9},
  number       = {3},
  pages        = {1-26},
  shortjournal = {Minds Mach.},
  title        = {Distant writing: Literary production in the age of artificial intelligence},
  volume       = {35},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Hidden risks: Artificial intelligence and hermeneutic harm. <em>MAM</em>, <em>35</em>(3), 1-18. (<a href='https://doi.org/10.1007/s11023-025-09733-0'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The AI Ethics literature has identified many forms of harm caused, perpetuated or exacerbated by artificial intelligence (AI). One, however, has been overlooked. In this paper we argue that the increasing use of AI heightens the risk of ‘hermeneutic harm’, which occurs when people are unable to make sense of, or come to terms with, unexpected, unwelcome, or harmful events they experience. We develop several examples to support our argument that AI increases the risk of hermeneutic harm. Importantly, our argument makes no assumption of flawed design, biased training data, or misuse: hermeneutic harm can occur regardless. Explainable AI (XAI) could plausibly reduce the risk of hermeneutic harm in some cases. Thus, one respect in which this paper advances the field is that it shows the need to further broaden XAI’s understanding of the social function of explanation. Yet XAI cannot fully mitigate the risk of hermeneutic harm, which (as our choice of examples shows) would persist even if all ‘black-box’ problems of system opacity were to be solved. The paper thus highlights an important but underexplored risk posed by AI systems.},
  archive      = {J_MAM},
  author       = {Rebera, Andrew P. and Lauwaert, Lode and Oimann, Ann-Katrien},
  doi          = {10.1007/s11023-025-09733-0},
  journal      = {Minds and Machines},
  month        = {9},
  number       = {3},
  pages        = {1-18},
  shortjournal = {Minds Mach.},
  title        = {Hidden risks: Artificial intelligence and hermeneutic harm},
  volume       = {35},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Adaptive updating: Ecological rationality meets reinforcement learning. <em>MAM</em>, <em>35</em>(3), 1-23. (<a href='https://doi.org/10.1007/s11023-025-09735-y'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent work has argued for an ecological perspective on the rationality of updating, showing that non-Bayesian update rules can outperform Bayesian updating in certain environments. However, this work has left unaddressed the question of how to determine which rule to use without prior knowledge of environmental features—a challenge we term the “dependency problem.” We propose a solution that uses reinforcement learning, specifically a multi-armed bandit framework, to enable dynamic rule selection. Computer simulations indicate that the “adaptive updating” this approach results in is able to solve the dependency problem at low cost in that it performs competitively with fixed update rules across varied contexts. While adaptive updating does not universally outperform fixed rules, it offers a viable alternative when the optimal rule for a given context is unknown.},
  archive      = {J_MAM},
  author       = {Douven, Igor},
  doi          = {10.1007/s11023-025-09735-y},
  journal      = {Minds and Machines},
  month        = {9},
  number       = {3},
  pages        = {1-23},
  shortjournal = {Minds Mach.},
  title        = {Adaptive updating: Ecological rationality meets reinforcement learning},
  volume       = {35},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A counterfactual account of algorithmic robustness. <em>MAM</em>, <em>35</em>(3), 1-27. (<a href='https://doi.org/10.1007/s11023-025-09734-z'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Accuracy plays an important role in the deployment of machine learning algorithms. But accuracy is not the only epistemic property that matters. For instance, it is well-known that algorithms may perform accurately during their training phase but experience a significant drop in performance when deployed in real-world conditions. To address this gap, people have turned to the concept of algorithmic robustness. Roughly, robustness refers to an algorithm’s ability to maintain its performance across a range of real-world and hypothetical conditions. In this paper, we develop a rigorous account of algorithmic robustness grounded in Robert Nozick’s counterfactual sensitivity and adherence conditions for knowledge. By bridging insights from epistemology and machine learning, we offer a novel conceptualization of robustness that captures key instances of algorithmic brittleness while advancing discussions on reliable AI deployment. We also show how a sensitivity-based account of robustness provides notable advantages over related approaches to algorithmic brittleness, including causal and safety-based ones.},
  archive      = {J_MAM},
  author       = {Bjerring, Jens Christian and Busch, Jacob and Aastrup Munch, Lauritz},
  doi          = {10.1007/s11023-025-09734-z},
  journal      = {Minds and Machines},
  month        = {9},
  number       = {3},
  pages        = {1-27},
  shortjournal = {Minds Mach.},
  title        = {A counterfactual account of algorithmic robustness},
  volume       = {35},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Explainability through systematicity: The hard systematicity challenge for artificial intelligence. <em>MAM</em>, <em>35</em>(3), 1-39. (<a href='https://doi.org/10.1007/s11023-025-09738-9'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper argues that explainability is only one facet of a broader ideal that shapes our expectations towards artificial intelligence (AI). Fundamentally, the issue is to what extent AI exhibits systematicity—not merely in being sensitive to how thoughts are composed of recombinable constituents, but in striving towards an integrated body of thought that is consistent, coherent, comprehensive, and parsimoniously principled. This richer conception of systematicity has been obscured by the long shadow of the “systematicity challenge” to connectionism, according to which network architectures are fundamentally at odds with what Fodor and colleagues termed “the systematicity of thought.” I offer a conceptual framework for thinking about “the systematicity of thought” that distinguishes four senses of the phrase. I use these distinctions to defuse the perceived tension between systematicity and connectionism and show that the conception of systematicity that historically shaped our sense of what makes thought rational, authoritative, and scientific is more demanding than the Fodorian notion. To determine whether we have reason to hold AI models to this ideal of systematicity, I then argue, we must look to the rationales for systematization and explore to what extent they transfer to AI models. I identify five such rationales and apply them to AI. This brings into view the “hard systematicity challenge.” However, the demand for systematization itself needs to be regulated by the rationales for systematization. This yields a dynamic understanding of the need to systematize thought, which tells us how systematic we need AI models to be and when.},
  archive      = {J_MAM},
  author       = {Queloz, Matthieu},
  doi          = {10.1007/s11023-025-09738-9},
  journal      = {Minds and Machines},
  month        = {9},
  number       = {3},
  pages        = {1-39},
  shortjournal = {Minds Mach.},
  title        = {Explainability through systematicity: The hard systematicity challenge for artificial intelligence},
  volume       = {35},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Explainable AI does not provide reason explanations. <em>MAM</em>, <em>35</em>(3), 1-25. (<a href='https://doi.org/10.1007/s11023-025-09739-8'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the advent of explainable artificial intelligence (XAI) to explain the outputs of black-box machine learning models, the question arises how such explanations should be conceptualised. Specifically, assuming that XAI methods provide explanations, what types of explanations are these? Although this question is usually left implicit, informal discussion of XAI methods often suggests that XAI provides reason explanations that show a machine learning model’s reasons for its classifications. This suggestion has recently been explicitly defended by Zerilli and Baum et al. This paper argues that this idea is mistaken. Neither of the two main families of views of reason explanations—the causal/dispositional family, on the one hand, and the interpretative family, on the other—can be applied to XAI. On the first type of view, it is unclear how the causal or dispositional states of black-box models could ground the normative attitudes characteristic of motivating reasons. On the second type of view, a reason explanation should show a decision in a positive light from the decision-maker’s perspective. This means that the provider of a reason explanation should be able to (accurately) answer normative why-questions about the decision-maker. It also requires the assumption that the decision-maker has recognisable concerns and beliefs. Black-box models, and XAI methods designed to explain them, do not meet these conditions. It is concluded that other types of explanations, such as causal explanations, are more appropriate to conceptualise XAI outputs, and that a reason-based conceptualisation risks creating false expectations and possibly over-reliance in users.},
  archive      = {J_MAM},
  author       = {Graff, Joris},
  doi          = {10.1007/s11023-025-09739-8},
  journal      = {Minds and Machines},
  month        = {9},
  number       = {3},
  pages        = {1-25},
  shortjournal = {Minds Mach.},
  title        = {Explainable AI does not provide reason explanations},
  volume       = {35},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Myth, power, and agency: Rethinking artificial intelligence, geopolitics and war. <em>MAM</em>, <em>35</em>(3), 1-26. (<a href='https://doi.org/10.1007/s11023-025-09741-0'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This collection interrogates how Artificial Intelligence (AI) is reshaping war, sovereignty, and human agency by entangling technological experimentation with myth-making and geopolitical power. Drawing on vignettes from the Ukrainian battlefront to Silicon Valley boardrooms, the contributions highlight the paradox that AI-driven innovation is contingent on perpetual risk-taking, distributed agency between humans and machines, and opaque state-corporate alliances. The latter foregrounds discursive and ideological alignments between state and corporate elites that increasingly naturalise apocalyptic and utopian AI futures. Furthermore, the collection examines the emergence of a global war lab, where conflict zones become sites for the rapid prototyping of autonomous systems. Meanwhile, decision-support algorithms in security and defence redistribute responsibility in unforeseen ways, thus challenging the promise of meaningful human control. Such dynamics are increasingly facilitated by the emergence of an AI Empire, in which corporate technological giants and militaries co-produce new forms of sovereignty and power across the global stage. This equally raises essential questions about the risks of automated violence, revealing the enduring role of judgment and emotion in war, and how mythology itself teaches us a great deal about technologically mediated warfare. Together, the collection argues that AI’s apparent inevitability is sustained by powerful mythmaking and storytelling that normalise experimentation, accelerate escalation, and mask unequal structures of extraction and domination. Ultimately, this collection offers a critical step towards uncovering these processes and reclaiming space for critical reflection, democratic oversight, and accountable design in an era when AI “deities” increasingly govern the politics and practices of conflict today.},
  archive      = {J_MAM},
  author       = {Csernatoni, Raluca and Broeders, Dennis and Andersen, Lise H. and Hoijtink, Marijn and Bode, Ingvild and Lindsay, Jon R. and Schwarz, Elke},
  doi          = {10.1007/s11023-025-09741-0},
  journal      = {Minds and Machines},
  month        = {9},
  number       = {3},
  pages        = {1-26},
  shortjournal = {Minds Mach.},
  title        = {Myth, power, and agency: Rethinking artificial intelligence, geopolitics and war},
  volume       = {35},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). AI, deskilling, and the prospects for public reason. <em>MAM</em>, <em>35</em>(3), 1-27. (<a href='https://doi.org/10.1007/s11023-025-09737-w'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The relationship between democracy and artificial intelligence (AI) is attracting attention, given fast-paced developments in AI and their implications for the political public sphere. The idea of ‘public reason’ can illuminate important dimensions of this relationship. Public reason is a standard of reciprocal legitimation and justificatory practice given democratic disagreement. This paper argues that AI might threaten the prospects of public reason when applied to policy debates. On the ‘civic friendship’ conception of public reason, the practice of reasoning is grounded in embodied joint action and having shared experiences. Accordingly, public reason as a reciprocal justificatory practice requires being skilled in human capacities like justice, joint action, patience and moral attention. Yet AI tools (e.g. recommender systems, personalized AI aids and AI deliberative democracy platforms) that afford disembodied, mediated interaction, threaten to deskill humans of those capacities, by creating environments that afford less opportunities to engage in activities in which the capacities are cultivated and practiced. AI tools involved in democratic deliberations can provide efficiency, scalability, and improved understanding of policy issues among participants. However, on balance, uncritical integration of such tools could deskill public reason capacities, leading to the erosion of mutual assurance between citizens, and ultimately undermining trust in democratic deliberation.},
  archive      = {J_MAM},
  author       = {Ferdman, Avigail},
  doi          = {10.1007/s11023-025-09737-w},
  journal      = {Minds and Machines},
  month        = {9},
  number       = {3},
  pages        = {1-27},
  shortjournal = {Minds Mach.},
  title        = {AI, deskilling, and the prospects for public reason},
  volume       = {35},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). No room in the reservoir. <em>MAM</em>, <em>35</em>(3), 1-27. (<a href='https://doi.org/10.1007/s11023-025-09742-z'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {There is growing enthusiasm in some quarters for purging representation from the set of concepts necessary to understand intelligence, at least in some circumstances. But to date, the list of explicit alternatives remains thin, with few plausible examples of intelligent systems the operation of which can be clearly articulated in non-representational terms. I argue that the class of Reservoir Computer (RC) models from machine learning constitutes a rich source of such examples. While RCs can learn to forecast even highly complex dynamical systems, I argue that they cannot be said to represent those systems in any but the most trivial way. Specifically, RCs do not contain compositional representations from which may be extracted subrepresentations that correspond to portions or aspects of the targets they represent. In other words, RCs do not represent parts or properties of their targets and so do not achieve intelligent prediction by manipulating component subrepresentations. They are thus concrete examples of intelligence achieved without essential use of representation.},
  archive      = {J_MAM},
  author       = {Jantzen, Benjamin},
  doi          = {10.1007/s11023-025-09742-z},
  journal      = {Minds and Machines},
  month        = {9},
  number       = {3},
  pages        = {1-27},
  shortjournal = {Minds Mach.},
  title        = {No room in the reservoir},
  volume       = {35},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). High-tech touts: AI-aided marketing and the virtues of attention. <em>MAM</em>, <em>35</em>(3), 1-15. (<a href='https://doi.org/10.1007/s11023-025-09740-1'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This essay has three interrelated aims. First, it articulates a set of capacities I call virtues of attention—capacities for intuitive discernment, good judgment about what is worth sustained focus, and the ability to engage deeply with worthwhile things. These are eudaimonist virtues in that they help us live well, not merely act rightly. Second, the essay explores what I call poisonous persuasion: the idea that rhetorical appeals, especially those used in marketing, may not only succeed by appealing to certain desires or habits of mind but may also deepen and entrench them. Third, I bring these insights together to examine a largely neglected problem at the intersection of AI and virtue ethics: how AI-aided marketing may erode the very capacities we need to flourish. This is not primarily an essay about how AI might itself be made virtuous, nor whether it can mimic or model virtue, though those are important questions. Rather, the focus is on what AI-driven persuasion may be doing to us. The danger is not just distraction, which has been well-documented, but a subtler debilitation: an incremental erosion of the attentional capacities on which flourishing depends. These systems appeal to what works—what clicks, what sells—without regard for what they may be nurturing in us. This paper uses virtue ethics to illuminate this harm: not a spectacular or easily measured injury, but a quiet undermining of our ability to recognize, commit to, and engage with the good. If we hope to thrive rather than merely function, we must understand and resist these corrosive dynamics.},
  archive      = {J_MAM},
  author       = {Clark, Sherman J.},
  doi          = {10.1007/s11023-025-09740-1},
  journal      = {Minds and Machines},
  month        = {9},
  number       = {3},
  pages        = {1-15},
  shortjournal = {Minds Mach.},
  title        = {High-tech touts: AI-aided marketing and the virtues of attention},
  volume       = {35},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Physical programmability. <em>MAM</em>, <em>35</em>(2), 1-29. (<a href='https://doi.org/10.1007/s11023-025-09714-3'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article delivers an account of what it is for a physical system to be programmable. Despite its significance in computing and beyond, today’s philosophical discourse on programmability is impoverished. This contribution offers a novel definition of physical programmability as the degree to which the selected operations of an automaton can be reconfigured in a controlled way. The framework highlights several key insights: the constrained applicability of physical programmability to material automata, the characterization of selected operations within the neo-mechanistic framework, the understanding of controlled reconfiguration through the causal theory of interventionism, and the recognition of physical programmability as a gradual notion. The account can be used to individuate programmable (computing) systems and taxonomize concrete systems based on their programmability. The article closes by posing some open questions and offering avenues for future research in this domain.},
  archive      = {J_MAM},
  author       = {Wiggershaus, Nick},
  doi          = {10.1007/s11023-025-09714-3},
  journal      = {Minds and Machines},
  month        = {6},
  number       = {2},
  pages        = {1-29},
  shortjournal = {Minds Mach.},
  title        = {Physical programmability},
  volume       = {35},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Persons, unique value and avatars. <em>MAM</em>, <em>35</em>(2), 1-14. (<a href='https://doi.org/10.1007/s11023-025-09715-2'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {An individual human has value partly in virtue of their uniqueness. Personal avatar technology—technology which creates a digital replication of a real person—appears to have the potential to undermine that value. Here I explore if and how avatars might make humans less valuable by undermining the value that a human gains from being unique. Ultimately, I conclude that, while avatars cannot make humans no longer unique, they could significantly undermine the value that we place on human uniqueness. First, I argue that a qualitative model of uniqueness cannot account for the unique value that a person has. This leads to the significant and surprising claim that necessarily unique properties of humans cannot accommodate the value arising from human uniqueness: humans have unique value in virtue of being contingently irreplaceable. I explore how the use of personal avatars might undermine or even destroy that value. Finally, I consider further applications of the theory of unique human value, including how it might explain and accommodate our attachment to personal avatars themselves.},
  archive      = {J_MAM},
  author       = {Sweeney, Paula},
  doi          = {10.1007/s11023-025-09715-2},
  journal      = {Minds and Machines},
  month        = {6},
  number       = {2},
  pages        = {1-14},
  shortjournal = {Minds Mach.},
  title        = {Persons, unique value and avatars},
  volume       = {35},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). How do social media algorithms appear? a phenomenological response to the black box metaphor. <em>MAM</em>, <em>35</em>(2), 1-21. (<a href='https://doi.org/10.1007/s11023-025-09716-1'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article challenges the dominant ‘black box’ metaphor in critical algorithm studies by proposing a phenomenological framework for understanding how social media algorithms manifest themselves in user experience. While the black box paradigm treats algorithms as opaque, self-contained entities that exist only ‘behind the scenes’, this article argues that algorithms are better understood as genetic phenomena that unfold temporally through user-platform interactions. Recent scholarship in critical algorithm studies has already identified various ways in which algorithms manifest in user experience: through affective responses, algorithmic self-reflexivity, disruptions of normal experience, points of contention, and folk theories. Yet, while these studies gesture toward a phenomenological understanding of algorithms, they do so without explicitly drawing on phenomenological theory. This article demonstrates how phenomenology, particularly a Husserlian genetic approach, can further conceptualize these already-documented algorithmic encounters. Moving beyond both the paradigm of artifacts and static phenomenological approaches, the analysis shows how algorithms emerge as inherently relational processes that co-constitute user experience over time. By reconceptualizing algorithms as genetic phenomena rather than black boxes, this paper provides a theoretical framework for understanding how algorithmic awareness develops from pre-reflective affective encounters to explicit folk theories, while remaining inextricably linked to users’ self-understanding. This phenomenological framework contributes to a more nuanced understanding of algorithmic mediation in contemporary social media environments and opens new pathways for investigating digital technologies.},
  archive      = {J_MAM},
  author       = {Longo, Anthony},
  doi          = {10.1007/s11023-025-09716-1},
  journal      = {Minds and Machines},
  month        = {6},
  number       = {2},
  pages        = {1-21},
  shortjournal = {Minds Mach.},
  title        = {How do social media algorithms appear? a phenomenological response to the black box metaphor},
  volume       = {35},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). In honor of james moor: A grateful retrospective. <em>MAM</em>, <em>35</em>(2), 1-6. (<a href='https://doi.org/10.1007/s11023-025-09718-z'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_MAM},
  author       = {Ess, Charles M.},
  doi          = {10.1007/s11023-025-09718-z},
  journal      = {Minds and Machines},
  month        = {6},
  number       = {2},
  pages        = {1-6},
  shortjournal = {Minds Mach.},
  title        = {In honor of james moor: A grateful retrospective},
  volume       = {35},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). The quantum panopticon: A theory of surveillance for the quantum era. <em>MAM</em>, <em>35</em>(2), 1-22. (<a href='https://doi.org/10.1007/s11023-025-09723-2'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The advent of quantum computing will compromise current asymmetric cryptography. Awaiting this moment, global superpowers are routinely collecting and storing encrypted data, so as to later decrypt it once sufficiently strong quantum computers are in place. We argue that this situation gives rise to a new mode of global surveillance that we refer to as a quantum panopticon. Unlike traditional forms of panoptic surveillance, the quantum panopticon introduces a temporal axis, whereby data subjects’ future pasts can be monitored from an unknown “superposition” in the quantum future. It also introduces a new level of uncertainty, in that the future watchman’s very existence becomes a function of data subjects’ efforts to protect themselves from being monitored in the present. Encryption may work as a momentary protection, but increases the likelihood of long-term preservation for future decryption, because encrypted data is stored longer than plaintext data. To illustrate the political and ethical aspects of these features, we draw on cryptographic as well as theoretical surveillance literature and call for urgent consideration of the wider implications of quantum computing for the global surveillance landscape.},
  archive      = {J_MAM},
  author       = {Olsson, Erik and Öhman, Carl},
  doi          = {10.1007/s11023-025-09723-2},
  journal      = {Minds and Machines},
  month        = {6},
  number       = {2},
  pages        = {1-22},
  shortjournal = {Minds Mach.},
  title        = {The quantum panopticon: A theory of surveillance for the quantum era},
  volume       = {35},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Moor’s ‘Are there decisions computers should never make?’. <em>MAM</em>, <em>35</em>(2), 1-8. (<a href='https://doi.org/10.1007/s11023-025-09719-y'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {‘Are There Decisions Computers Should Never Make?’ is one of James H. Moor’s many groundbreaking papers in computer ethics, and it is one that I have thought a good deal about since its publication in 1979 and especially in recent years in relation to current discourse on AI. In this paper, I describe Jim’s analysis, reflect on its relevance to current thinking about AI, and take issue with several of his arguments. The conclusion of Jim’s paper is that computers should never choose human values and goals. I suggest that this is not possible because of the nature of values and how they are intertwined in computer decision making.},
  archive      = {J_MAM},
  author       = {Johnson, Deborah G.},
  doi          = {10.1007/s11023-025-09719-y},
  journal      = {Minds and Machines},
  month        = {6},
  number       = {2},
  pages        = {1-8},
  shortjournal = {Minds Mach.},
  title        = {Moor’s ‘Are there decisions computers should never make?’},
  volume       = {35},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Moor on ethics for emerging technologies: Some environmental considerations. <em>MAM</em>, <em>35</em>(2), 1-7. (<a href='https://doi.org/10.1007/s11023-025-09721-4'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Around the turn of this century a number of emerging technologies were in the news, raising some potentially significant ethical questions. Given that they were emerging they as yet had no, or very few, impacts, so it was not obvious how best to assess them ethically. Jim Moor addressed this issue and offered three suggestions for a better ethics for emerging technologies. His first was that ethics should be dynamic, that is, it should be an ongoing process before, during and after the technological development. Second, there should be close collaboration between the researchers and developers on the one hand, and ethicists and social scientists on the other. Finally, ethical analyses should be more sophisticated. In this paper I argue that environmental issues and the questioning of core ethical values should be a central part of the ethics of emerging technologies, using AI examples. Given the kind of beings that we are, technology and the environment are closely connected for human flourishing.},
  archive      = {J_MAM},
  author       = {Weckert, John},
  doi          = {10.1007/s11023-025-09721-4},
  journal      = {Minds and Machines},
  month        = {6},
  number       = {2},
  pages        = {1-7},
  shortjournal = {Minds Mach.},
  title        = {Moor on ethics for emerging technologies: Some environmental considerations},
  volume       = {35},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). James moor’s privacy framework: A theory in need of further exploration. <em>MAM</em>, <em>35</em>(2), 1-7. (<a href='https://doi.org/10.1007/s11023-025-09717-0'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper is intended as a tribute to the late James Moor. An esteemed Dartmouth professor, who published in many areas of philosophy, including logic, Moor is perhaps best remembered today for his pioneering work in the field of computer ethics. His seminal (and award-winning) article, “What Is Computer Ethics?” (Metaphilosophy, 1985) was highly influential both in defining and shaping the then nascent field of computer ethics. Many other computer-ethics-related papers followed over the next quarter century, in which Moor examined a range of topics – from moral responsibility to autonomy to privacy in the context of computing and emerging technologies, including nanotechnology and AI. And while the insights and frameworks put forth in many of his published works have received the acclaim they deserve, Moor’s contribution to the privacy literature remains, in my view, underappreciated. In trying to show why his privacy theory deserves much more attention than received to date, I also briefly describe the evolution of Moor’s position on privacy – from his earlier publications on that topic to a comprehensive and systematic privacy framework. I then suggest that a further exploration of his privacy theory would benefit researchers working in technology ethics in general, and AI ethics in particular. Finally, I encourage privacy scholars to take a closer look at Moor’s privacy framework to see whether they might be able to tease out and disclose some potential insights and features that may still be embedded in that robust theory of privacy.},
  archive      = {J_MAM},
  author       = {Tavani, Herman T.},
  doi          = {10.1007/s11023-025-09717-0},
  journal      = {Minds and Machines},
  month        = {6},
  number       = {2},
  pages        = {1-7},
  shortjournal = {Minds Mach.},
  title        = {James moor’s privacy framework: A theory in need of further exploration},
  volume       = {35},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). The historical development of ethics of emerging technologies. <em>MAM</em>, <em>35</em>(2), 1-9. (<a href='https://doi.org/10.1007/s11023-025-09720-5'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article traces the historical development of the ethics of emerging technologies. It argues that during the late 2000s and 2010s, the field of ethics of technology transformed from a fragmented, reactive, and methodologically underdeveloped discipline focused on mature technologies and lacking policy orientation into a more cohesive, proactive, methodologically sophisticated, and policy-focused field with a strong emphasis on emerging technologies. An agenda for this transition was set in Jim Moor’s seminal publication “Why We Need Better Ethics for Emerging Technologies”.},
  archive      = {J_MAM},
  author       = {Brey, Philip A. E.},
  doi          = {10.1007/s11023-025-09720-5},
  journal      = {Minds and Machines},
  month        = {6},
  number       = {2},
  pages        = {1-9},
  shortjournal = {Minds Mach.},
  title        = {The historical development of ethics of emerging technologies},
  volume       = {35},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Interdisciplinary perspectives on the (Un)fairness of artificial intelligence. <em>MAM</em>, <em>35</em>(2), 1-5. (<a href='https://doi.org/10.1007/s11023-025-09722-3'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_MAM},
  author       = {Starke, Christopher and Blanke, Tobias and Helberger, Natali and Smets, Sonja and de Vreese, Claes},
  doi          = {10.1007/s11023-025-09722-3},
  journal      = {Minds and Machines},
  month        = {6},
  number       = {2},
  pages        = {1-5},
  shortjournal = {Minds Mach.},
  title        = {Interdisciplinary perspectives on the (Un)fairness of artificial intelligence},
  volume       = {35},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Put it to the test: Getting serious about explanation in explainable artificial intelligence. <em>MAM</em>, <em>35</em>(2), 1-28. (<a href='https://doi.org/10.1007/s11023-025-09724-1'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Artificial intelligence (AI) has become a topic of major interest to philosophers of science. Among the issues commonly discussed is AI’s opacity. To remedy opacity, scientists have provided methods commonly subsumed under the label ‘eXplaibable Artificial Intelligence’ (XAI) that aim to make AI and its outputs ‘interpretable’ and ‘explainable’. However, there is little interaction between developments in XAI and philosophical debates on scientific explanation. We here improve on this situation and argue for a descriptive and a normative thesis: (i) When suitably embedded into scientific research processes, XAI methods’ outputs can facilitate genuine scientific understanding. (ii) In order for XAI outputs to fulfill this function, they should be made testable. We will support our theses by building on recent and long-standing ideas from philosophy of science, by comparing them to a recent framework from the XAI community, and by showcasing their applicability to case studies from the life sciences.},
  archive      = {J_MAM},
  author       = {Boge, Florian J. and Mosig, Axel},
  doi          = {10.1007/s11023-025-09724-1},
  journal      = {Minds and Machines},
  month        = {6},
  number       = {2},
  pages        = {1-28},
  shortjournal = {Minds Mach.},
  title        = {Put it to the test: Getting serious about explanation in explainable artificial intelligence},
  volume       = {35},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A remembrance of jim moor by two computer scientists. <em>MAM</em>, <em>35</em>(2), 1-3. (<a href='https://doi.org/10.1007/s11023-025-09725-0'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Instead of a scholarly paper, we decided to write a brief, personal remembrance of our friend and colleague Jim Moor. We briefly touch on his personal kindnesses to us, his significant influence on legitimizing computer ethics as an object of study, and why we think Jim was so effective in reaching scholars across disciplinary divides.},
  archive      = {J_MAM},
  author       = {Grodzinsky, Frances S. and Miller, Keith W.},
  doi          = {10.1007/s11023-025-09725-0},
  journal      = {Minds and Machines},
  month        = {6},
  number       = {2},
  pages        = {1-3},
  shortjournal = {Minds Mach.},
  title        = {A remembrance of jim moor by two computer scientists},
  volume       = {35},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Open-source AI made in the EU: Why it is a good idea. <em>MAM</em>, <em>35</em>(2), 1-10. (<a href='https://doi.org/10.1007/s11023-025-09728-x'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article examines the feasibility and strategic advantages of developing open-source (OS) foundation models within the European Union (EU). Drawing upon recent developments in AI strategies globally, particularly in China, we argue that the EU’s robust regulatory framework and commitment to ethical principles uniquely position it to produce trustworthy OS foundation models. Due to stringent regulatory compliance, EU-developed OS foundation models would inherently ensure greater trustworthiness, offering a competitive and ethically aligned alternative to proprietary and foreign models. We conclude by offering concrete recommendations to realise this strategic opportunity through EU initiatives such as the AI Factories and EuroHPC infrastructure, emphasising the EU’s potential role in shaping global AI standards.},
  archive      = {J_MAM},
  author       = {Floridi, Luciano and Buttaboni, Carlotta and Hine, Emmie and Novelli, Claudio and Schroder, Tyler and Shanklin, Grant},
  doi          = {10.1007/s11023-025-09728-x},
  journal      = {Minds and Machines},
  month        = {6},
  number       = {2},
  pages        = {1-10},
  shortjournal = {Minds Mach.},
  title        = {Open-source AI made in the EU: Why it is a good idea},
  volume       = {35},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Black-box AI and patient autonomy. <em>MAM</em>, <em>35</em>(2), 1-19. (<a href='https://doi.org/10.1007/s11023-025-09729-w'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Black-box AI cannot provide causal explanations for the decisions it makes, but medical AI has shown great promise as an accurate and reliable technology that both improves the quality of patient care and provides better access to healthcare for more patients. There is an ethical argument that to meet the informational requirements of patient autonomy, medical decision-making ought to be explainable to the patient. As such, there have been claims that black-box AI ought to be only minimally used in healthcare. This paper seeks to argue that black-box AI within the clinical context does not necessarily undermine patient autonomy, as defined in standard or relational accounts. Rather, patient autonomy is affected primarily by how AI tools are used.},
  archive      = {J_MAM},
  author       = {Prince, Sinead and Lim, James Edgar},
  doi          = {10.1007/s11023-025-09729-w},
  journal      = {Minds and Machines},
  month        = {6},
  number       = {2},
  pages        = {1-19},
  shortjournal = {Minds Mach.},
  title        = {Black-box AI and patient autonomy},
  volume       = {35},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Moor’s theory of just consequentialism. <em>MAM</em>, <em>35</em>(2), 1-9. (<a href='https://doi.org/10.1007/s11023-025-09726-z'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Jim Moor not only wrote about practical ethical issues, but also about theoretical ones. In one of his most famous articles, he presents his carefully crafted theory of just consequentialism. What is offered here is an attempt to elaborate and defend a unifying ethical model that can serve as a guide for identifying just ethical policies for different information technologies. In this paper I review this model and offer some refinements to Moor’s theory of the good which is the centerpiece of his moral analysis. We demonstrate how his order of goods is instrumental in weighing the beneficial versus harmful consequences of IT policies, and why consequentialism must be supplemented by an explicit consideration of rights and justice. Finally, we review when exceptions to moral rules that protect rights are warranted on the basis of the “blindfold of justice.”},
  archive      = {J_MAM},
  author       = {Spinello, Richard A.},
  doi          = {10.1007/s11023-025-09726-z},
  journal      = {Minds and Machines},
  month        = {6},
  number       = {2},
  pages        = {1-9},
  shortjournal = {Minds Mach.},
  title        = {Moor’s theory of just consequentialism},
  volume       = {35},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). The two times problem and igus robots. <em>MAM</em>, <em>35</em>(2), 1-14. (<a href='https://doi.org/10.1007/s11023-025-09727-y'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In their interesting papers (2020, 2022), Gruber et al. use IGUS robots in an attempt to solve the two-times problem concerning the gap between the time of everyday experience and physical time.They try to do this by identifying the veridical and physical aspects of manifest time. Their verdict, based on physics, cognitive science, and experiments conducted with the aid of IGUS robots, is that the flow of time is an illusion produced by our minds. They argue that the Block Universe and similar cosmological views—lacking a unique present and dynamics—offer veridical aspects of the “flow of time,” while its corresponding illusory aspects arise as a result of natural selection. In this paper, I attempt to demonstrate that physics without the real flow of time does not provide enough resources to explain how IGUS robots could help solve the two-times problem and that the arguments proposed by the authors are insufficient. The article also briefly examines the possibility of a realist solution to the problem of the passage of time.},
  archive      = {J_MAM},
  author       = {Gołosz, Jerzy},
  doi          = {10.1007/s11023-025-09727-y},
  journal      = {Minds and Machines},
  month        = {6},
  number       = {2},
  pages        = {1-14},
  shortjournal = {Minds Mach.},
  title        = {The two times problem and igus robots},
  volume       = {35},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Effective human oversight of AI-based systems: A signal detection perspective on the detection of inaccurate and unfair outputs. <em>MAM</em>, <em>35</em>(1), 1-30. (<a href='https://doi.org/10.1007/s11023-024-09701-0'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Legislation and ethical guidelines around the globe call for effective human oversight of AI-based systems in high-risk contexts – that is oversight that reliably reduces the risks otherwise associated with the use of AI-based systems. Such risks may relate to the imperfect accuracy of systems (e.g., inaccurate classifications) or to ethical concerns (e.g., unfairness of outputs). Given the significant role that human oversight is expected to play in the operation of AI-based systems, it is crucial to better understand the conditions for effective human oversight. We argue that the reliable detection of errors (as an umbrella term for inaccuracies and unfairness) is crucial for effective human oversight. We then propose that Signal Detection Theory (SDT) offers a promising framework for better understanding what affects people’s sensitivity (i.e., how well they are able to detect errors) and response bias (i.e., the tendency to report errors given a perceived evidence of an error) in detecting errors. Whereas an SDT perspective on the detection of inaccuracies is straightforward, we demonstrate its broader applicability by detailing the specifics for an SDT perspective on unfairness detection, including the need to choose a standard for (un)fairness. Additionally, we illustrate that an SDT perspective helps to better understand the conditions for effective error detection by showing examples of task-, system-, and person-related factors that may affect the sensitivity and response bias of humans tasked with detecting unfairness associated with the use of AI-based systems. Finally, we discuss future research directions for an SDT perspective on error detection.},
  archive      = {J_MAM},
  author       = {Langer, Markus and Baum, Kevin and Schlicker, Nadine},
  doi          = {10.1007/s11023-024-09701-0},
  journal      = {Minds and Machines},
  month        = {3},
  number       = {1},
  pages        = {1-30},
  shortjournal = {Minds Mach.},
  title        = {Effective human oversight of AI-based systems: A signal detection perspective on the detection of inaccurate and unfair outputs},
  volume       = {35},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). How ChatGPT changed the media’s narratives on AI: A semi-automated narrative analysis through frame semantics. <em>MAM</em>, <em>35</em>(1), 1-24. (<a href='https://doi.org/10.1007/s11023-024-09705-w'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We perform a mixed-method frame semantics-based analysis on a dataset of more than 49,000 sentences collected from 5846 news articles that mention AI. The dataset covers the twelve-month period centred around the launch of OpenAI’s chatbot ChatGPT and is collected from the most visited open-access English-language news publishers. Our findings indicate that during the six months succeeding the launch, media attention rose tenfold—from already historically high levels. During this period, discourse has become increasingly centred around experts and political leaders, and AI has become more closely associated with dangers and risks. A deeper review of the data also suggests a qualitative shift in the types of threat AI is thought to represent, as well as the anthropomorphic qualities ascribed to it.},
  archive      = {J_MAM},
  author       = {Ryazanov, Igor and Öhman, Carl and Björklund, Johanna},
  doi          = {10.1007/s11023-024-09705-w},
  journal      = {Minds and Machines},
  month        = {3},
  number       = {1},
  pages        = {1-24},
  shortjournal = {Minds Mach.},
  title        = {How ChatGPT changed the media’s narratives on AI: A semi-automated narrative analysis through frame semantics},
  volume       = {35},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Statistical learning theory and occam’s razor: The core argument. <em>MAM</em>, <em>35</em>(1), 1-28. (<a href='https://doi.org/10.1007/s11023-024-09703-y'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Statistical learning theory is often associated with the principle of Occam’s razor, which recommends a simplicity preference in inductive inference. This paper distills the core argument for simplicity obtainable from statistical learning theory, built on the theory’s central learning guarantee for the method of empirical risk minimization. This core “means-ends” argument is that a simpler hypothesis class or inductive model is better because it has better learning guarantees; however, these guarantees are model-relative and so the theoretical push towards simplicity is checked by our prior knowledge.},
  archive      = {J_MAM},
  author       = {Sterkenburg, Tom F.},
  doi          = {10.1007/s11023-024-09703-y},
  journal      = {Minds and Machines},
  month        = {3},
  number       = {1},
  pages        = {1-28},
  shortjournal = {Minds Mach.},
  title        = {Statistical learning theory and occam’s razor: The core argument},
  volume       = {35},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Artificial intelligence (AI) and global justice. <em>MAM</em>, <em>35</em>(1), 1-29. (<a href='https://doi.org/10.1007/s11023-024-09708-7'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper provides a philosophically informed and robust account of the global justice implications of Artificial Intelligence (AI). We first discuss some of the key theories of global justice, before justifying our focus on the Capabilities Approach as a useful framework for understanding the context-specific impacts of AI on low- to middle-income countries. We then highlight some of the harms and burdens facing low- to middle-income countries within the context of both AI use and the AI supply chain, by analyzing the extraction of materials, which includes mineral extraction and the environmental harms associated with it, and the extraction of labor, which includes unethical labor practices, low wages, and the trauma experienced by some AI workers. We then outline some of the potential harms and benefits that AI poses, how these are distributed, and what global justice implications this has for low- to middle-income countries. Finally, we articulate the global justice significance of AI by utilizing the Capabilities Approach. We argue that AI must be considered from a global justice perspective given that, globally, AI puts significant downward pressure on several elements of well-being thereby making it harder for people to achieve threshold levels of the central human capabilities needed for a life of dignity.},
  archive      = {J_MAM},
  author       = {Sahebi, Siavosh and Formosa, Paul},
  doi          = {10.1007/s11023-024-09708-7},
  journal      = {Minds and Machines},
  month        = {3},
  number       = {1},
  pages        = {1-29},
  shortjournal = {Minds Mach.},
  title        = {Artificial intelligence (AI) and global justice},
  volume       = {35},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Standards for belief representations in LLMs. <em>MAM</em>, <em>35</em>(1), 1-25. (<a href='https://doi.org/10.1007/s11023-024-09709-6'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As large language models (LLMs) continue to demonstrate remarkable abilities across various domains, computer scientists are developing methods to understand their cognitive processes, particularly concerning how (and if) LLMs internally represent their beliefs about the world. However, this field currently lacks a unified theoretical foundation to underpin the study of belief in LLMs. This article begins filling this gap by proposing adequacy conditions for a representation in an LLM to count as belief-like. We argue that, while the project of belief measurement in LLMs shares striking features with belief measurement as carried out in decision theory and formal epistemology, it also differs in ways that should change how we measure belief. Thus, drawing from insights in philosophy and contemporary practices of machine learning, we establish four criteria that balance theoretical considerations with practical constraints. Our proposed criteria include accuracy, coherence, uniformity, and use, which together help lay the groundwork for a comprehensive understanding of belief representation in LLMs. We draw on empirical work showing the limitations of using various criteria in isolation to identify belief representations.},
  archive      = {J_MAM},
  author       = {Herrmann, Daniel A. and Levinstein, Benjamin A.},
  doi          = {10.1007/s11023-024-09709-6},
  journal      = {Minds and Machines},
  month        = {3},
  number       = {1},
  pages        = {1-25},
  shortjournal = {Minds Mach.},
  title        = {Standards for belief representations in LLMs},
  volume       = {35},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Cheaper spaces. <em>MAM</em>, <em>35</em>(1), 1-21. (<a href='https://doi.org/10.1007/s11023-024-09704-x'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Similarity spaces are standardly constructed by collecting pairwise similarity judgments and subjecting those to a dimension-reduction technique such as multidimensional scaling or principal component analysis. While this approach can be effective, it has some known downsides, most notably, it tends to be costly and has limited generalizability. Recently, a number of authors have attempted to mitigate these issues through machine learning techniques. For instance, neural networks have been trained on human similarity judgments to infer the spatial representation of unseen stimuli. However, these newer methods are still costly and fail to generalize widely beyond their initial training sets. This paper proposes leveraging prebuilt semantic vector spaces as a cheap alternative to collecting similarity judgments. Our results suggest that some of those spaces can be used to approximate human similarity judgments at low cost and high speed.},
  archive      = {J_MAM},
  author       = {Moullec, Matthieu and Douven, Igor},
  doi          = {10.1007/s11023-024-09704-x},
  journal      = {Minds and Machines},
  month        = {3},
  number       = {1},
  pages        = {1-21},
  shortjournal = {Minds Mach.},
  title        = {Cheaper spaces},
  volume       = {35},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Fairness in algorithmic profiling: The AMAS case. <em>MAM</em>, <em>35</em>(1), 1-30. (<a href='https://doi.org/10.1007/s11023-024-09706-9'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We study a controversial application of algorithmic profiling in the public sector, the Austrian AMAS system. AMAS was supposed to help caseworkers at the Public Employment Service (PES) Austria to allocate support measures to job seekers based on their predicted chance of (re-)integration into the labor market. Shortly after its release, AMAS was criticized for its apparent unequal treatment of job seekers based on gender and citizenship. We systematically investigate the AMAS model using a novel real-world dataset of young job seekers from Vienna, which allows us to provide the first empirical evaluation of the AMAS model with a focus on fairness measures. We further apply bias mitigation strategies to study their effectiveness in our real-world setting. Our findings indicate that the prediction performance of the AMAS model is insufficient for use in practice, as more than 30% of job seekers would be misclassified in our use case. Further, our results confirm that the original model is biased with respect to gender as it tends to (incorrectly) assign women to the group with high chances of re-employment, which is not prioritized in the PES’ allocation of support measures. However, most bias mitigation strategies were able to improve fairness without compromising performance and thus may form an important building block in revising profiling schemes in the present context.},
  archive      = {J_MAM},
  author       = {Achterhold, Eva and Mühlböck, Monika and Steiber, Nadia and Kern, Christoph},
  doi          = {10.1007/s11023-024-09706-9},
  journal      = {Minds and Machines},
  month        = {3},
  number       = {1},
  pages        = {1-30},
  shortjournal = {Minds Mach.},
  title        = {Fairness in algorithmic profiling: The AMAS case},
  volume       = {35},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Correction: Submarine cables and the risks to digital sovereignty. <em>MAM</em>, <em>35</em>(1), 1. (<a href='https://doi.org/10.1007/s11023-024-09707-8'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_MAM},
  author       = {Ganz, Abra and Camellini, Martina and Hine, Emmie and Novelli, Claudio and Roberts, Huw and Floridi, Luciano},
  doi          = {10.1007/s11023-024-09707-8},
  journal      = {Minds and Machines},
  month        = {3},
  number       = {1},
  pages        = {1},
  shortjournal = {Minds Mach.},
  title        = {Correction: Submarine cables and the risks to digital sovereignty},
  volume       = {35},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). An app a day will (Probably not) keep the doctor away: An evidence audit of health and medical apps available on the apple app store. <em>MAM</em>, <em>35</em>(1), 1-30. (<a href='https://doi.org/10.1007/s11023-025-09710-7'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {There are more than 350,000 health apps available in public app stores. The extolled benefits of health apps are numerous and well documented. However, there are also concerns that poor-quality apps, marketed directly to consumers, threaten the tenets of evidence-based medicine and expose individuals to the risk of harm. This study addresses this issue by assessing the overall quality of evidence publicly available to support the effectiveness claims of health apps marketed directly to consumers. To assess the quality of evidence available to the public to support the effectiveness claims of health apps marketed directly to consumers, an audit was conducted of a purposive sample of apps available on the Apple App Store. We find the quality of evidence available to support the effectiveness claims of health apps marketed directly to consumers to be poor. Less than half of the 220 apps (44%) we audited state that they have evidence to support their claims of effectiveness and, of these allegedly evidence-based apps, more than 70% rely publicly on either very low or low-quality evidence. For the minority of app developers that do publish studies, significant methodological limitations are commonplace. Finally, there is a pronounced tendency for apps—particularly mental health and diagnostic apps—to either borrow evidence generated in other (typically offline) contexts or to rely exclusively on unsubstantiated, unpublished user metrics as evidence to support their effectiveness claims. Health apps represent a significant opportunity for individual consumers and healthcare systems. Nevertheless, this opportunity will be missed if the health apps market continues to be flooded by poor quality, poorly evidenced, and potentially unsafe apps. It must be accepted that a continuing lag in generating high-quality publicly available evidence of app effectiveness and safety is not inevitable: it is a choice. Just because it will be challenging to raise the quality of the evidence base publicly available to support the claims of health apps, this does not mean that the bar for evidence quality should be lowered. Innovation for innovation’s sake must not be prioritized over public health and safety.},
  archive      = {J_MAM},
  author       = {Morley, Jessica and Laitila, Joel and Ross, Joseph S. and Schamroth, Joel and Zhang, Joe and Floridi, Luciano},
  doi          = {10.1007/s11023-025-09710-7},
  journal      = {Minds and Machines},
  month        = {3},
  number       = {1},
  pages        = {1-30},
  shortjournal = {Minds Mach.},
  title        = {An app a day will (Probably not) keep the doctor away: An evidence audit of health and medical apps available on the apple app store},
  volume       = {35},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). ChatGPT-4 in the turing test. <em>MAM</em>, <em>35</em>(1), 1-10. (<a href='https://doi.org/10.1007/s11023-025-09711-6'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {There has been considerable optimistic speculation on how well ChatGPT-4 would perform in a Turing Test. However, no minimally serious implementation of the test has been reported to have been carried out. This brief note documents the results of subjecting ChatGPT-4 to 10 Turing Tests, with different interrogators and participants. The outcome is tremendously disappointing for the optimists. Despite ChatGPT reportedly outperforming 99.9% of humans in a Verbal IQ test, it falls short of passing the Turing Test. In 9 out of the 10 tests conducted, the interrogators successfully identified ChatGPT-4 and the human participant. The probability of obtaining this result from a process in which the interrogator is really no better than chance at correct identification is calculated to be less than 1%. An additional question was posed to the interrogators at the end of each test: What led them to distinguish between the human and the machine? The interrogators, who effectively filtered out ChatGPT-4 from passing the Turing Test for intelligence, stated that they could identify the machine because it, in effect, responded more intelligently than the human. Subsequently, ChatGPT-4 was tasked with differentiating syntax from semantics and self-corrected when falling for the fallacy of equivocation. The curious situation is arrived at that passing the Turing Test for intelligence remains a challenge that ChatGPT-4 has yet to overcome, precisely because, as per the interrogators, its intellectual abilities surpass those of individual humans.},
  archive      = {J_MAM},
  author       = {Restrepo Echavarría, Ricardo},
  doi          = {10.1007/s11023-025-09711-6},
  journal      = {Minds and Machines},
  month        = {3},
  number       = {1},
  pages        = {1-10},
  shortjournal = {Minds Mach.},
  title        = {ChatGPT-4 in the turing test},
  volume       = {35},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). On twelve shades of green: Assessing the levels of environmental protection in the artificial intelligence act. <em>MAM</em>, <em>35</em>(1), 1-19. (<a href='https://doi.org/10.1007/s11023-025-09713-4'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The paper examines twelve legal regimes related to the governance and regulation of both the environmental risks and opportunities brought forth by the use of AI systems and AI models in the Artificial Intelligence Act (‘AIA’) of EU law. The assessment of risks and opportunities of AI related to the environment includes the high-risk management procedures under Art. 9 of the AIA, the “fundamental rights impact assessment” of Art. 27, and the codes of conduct of Art. 95. These provisions are supplemented by further regulatory regimes, such as the proposal of EU directive on sustainable consumption and green claims, and Reg. (EU) 2023/588 on environmental and space sustainability, among others. The aim of the analysis is to specify which are the less or the more environmentally friendly regulatory regimes set up with the AIA. The claim is that Art. 9, 27 and 95 are among the less green pieces of the whole legislation.},
  archive      = {J_MAM},
  author       = {Pagallo, Ugo},
  doi          = {10.1007/s11023-025-09713-4},
  journal      = {Minds and Machines},
  month        = {3},
  number       = {1},
  pages        = {1-19},
  shortjournal = {Minds Mach.},
  title        = {On twelve shades of green: Assessing the levels of environmental protection in the artificial intelligence act},
  volume       = {35},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). The testimony gap: Machines and reasons. <em>MAM</em>, <em>35</em>(1), 1-16. (<a href='https://doi.org/10.1007/s11023-025-09712-5'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Most people who have considered the matter have concluded that machines cannot be moral agents. Responsibility for acting on the outputs of machines must always rest with a human being. A key problem for the ethical use of AI, then, is to ensure that it does not block the attribution of responsibility to humans or lead to individuals being unfairly held responsible for things over which they had no control. This is the “responsibility gap”. In this paper, we argue that the claim that machines cannot be held responsible for their actions has unacknowledged implications for the conditions under which the outputs of AI can serve as reasons for belief. Following Robert Brandom, we argue that, because the assertion of a claim is an action, moral agency is a necessary condition for the giving and evaluating of reasons in discourse. Thus, the same considerations that suggest that machines cannot be held responsible for their actions suggest that they cannot be held to account for the epistemic value — or lack of value — of their outputs. If there is a responsibility gap, there is also a “testimony gap.” An under-recognised problem with the use of AI, then, is to ensure that it does not block the attribution of testimony to human beings or lead to individuals being held responsible for claims that they have not asserted. More generally, the “assertions” of machines are only capable of serving as justifications for belief or action where one or more people accept responsibility for them.},
  archive      = {J_MAM},
  author       = {Sparrow, Robert and Flenady, Gene},
  doi          = {10.1007/s11023-025-09712-5},
  journal      = {Minds and Machines},
  month        = {3},
  number       = {1},
  pages        = {1-16},
  shortjournal = {Minds Mach.},
  title        = {The testimony gap: Machines and reasons},
  volume       = {35},
  year         = {2025},
}
</textarea>
</details></li>
</ul>

</body>
</html>
