<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>COAP</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="coap">COAP - 95</h2>
<ul>
<li><details>
<summary>
(2025). On the robust isolated calmness of a class of nonsmooth optimizations on riemannian manifolds and its applications. <em>COAP</em>, <em>92</em>(2), 709--754. (<a href='https://doi.org/10.1007/s10589-025-00712-w'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper studies the robust isolated calmness property of the KKT solution mapping of a class of nonsmooth optimization problems on Riemannian manifolds. The manifold versions of the Robinson constraint qualification, the strict Robinson constraint qualification, and the second order conditions are defined and discussed. We show that the robust isolated calmness of the KKT solution mapping is equivalent to satisfying the M-SRCQ and M-SOSC conditions. Furthermore, under the above two conditions, we show that the Riemannian augmented Lagrangian method achieves a local linear convergence rate. Finally, we verify the proposed conditions and demonstrate the convergence rate on two minimization problems over the sphere and the manifold of fixed rank matrices.},
  archive      = {J_COAP},
  author       = {Bao, Chenglong and Ding, Chao and Zhou, Yuexin},
  doi          = {10.1007/s10589-025-00712-w},
  journal      = {Computational Optimization and Applications},
  month        = {11},
  number       = {2},
  pages        = {709--754},
  shortjournal = {Comput. Optim. Appl.},
  title        = {On the robust isolated calmness of a class of nonsmooth optimizations on riemannian manifolds and its applications},
  volume       = {92},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Improved gauss-seidel type and jacobi type methods for linear complementarity problems. <em>COAP</em>, <em>92</em>(2), 683--708. (<a href='https://doi.org/10.1007/s10589-025-00714-8'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we propose two Gauss-Seidel type methods, i.e., the improved modulus-based Gauss-Seidel method and the improved projected Gauss-Seidel method, to solve linear complementarity problems with Z-matrices, in which the diagonal entries of the system matrices may be non-positive. The monotone convergence of the improved modulus-based Gauss-Seidel method is proved minutely. And by proving the equivalence between the improved projected Gauss-Seidel method and the improved modulus-based Gauss-Seidel method, the same property of the improved projected Gauss-Seidel method is also shown. In addition, we also propose the improved modulus-based Jacobi method and show its monotone convergence. These methods extend the applications of the classical modulus-based matrix splitting methods and the classical projected method for solving linear complementarity problems. Finally, our numerical experiments verify our theoretical results. We also compare the three proposed methods and an existing related method, and the results show the effectiveness of the proposed algorithms.},
  archive      = {J_COAP},
  author       = {Liu, Wentai and Wang, Yong and Huang, Zheng-Hai},
  doi          = {10.1007/s10589-025-00714-8},
  journal      = {Computational Optimization and Applications},
  month        = {11},
  number       = {2},
  pages        = {683--708},
  shortjournal = {Comput. Optim. Appl.},
  title        = {Improved gauss-seidel type and jacobi type methods for linear complementarity problems},
  volume       = {92},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). The capacitated vehicle routing problem with planned downtime. <em>COAP</em>, <em>92</em>(2), 655--681. (<a href='https://doi.org/10.1007/s10589-025-00713-9'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Capacitated Vehicle Routing Problem with Planned Downtime introduced in this work is a generalization of the classical vehicle routing problem in which two sets of routes have to be jointly designed: one set considering that all the vehicles of the fleet are available, and another set that considers the scheduled unavailability of a vehicle. Besides routing cost minimization, it is also intended that the routes for the two cases (when all vehicles are available and when one is missing) are as similar as possible in order to reduce the impact on the customers. In this paper, we formally define the problem, propose two mixed integer programming models for it, one of them based on bilevel programming, and devise exact branch-and-cut algorithms based on those models. The algorithms are first compared, and then the most efficient one is tested on different benchmark instances with up to 40 nodes and 5 vehicles.},
  archive      = {J_COAP},
  author       = {Landete, Mercedes and Leal, Marina and Monge, Juan Francisco and Rodríguez-Martín, Inmaculada},
  doi          = {10.1007/s10589-025-00713-9},
  journal      = {Computational Optimization and Applications},
  month        = {11},
  number       = {2},
  pages        = {655--681},
  shortjournal = {Comput. Optim. Appl.},
  title        = {The capacitated vehicle routing problem with planned downtime},
  volume       = {92},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Convergence of the majorized PAM method with subspace correction for low-rank composite factorization model. <em>COAP</em>, <em>92</em>(2), 617--653. (<a href='https://doi.org/10.1007/s10589-025-00708-6'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper focuses on the convergence certificates of the majorized proximal alternating minimization (PAM) method with subspace correction, proposed in (Tao et al. in SIAM J. Optim. 32, 959–988 (2022)) for the column $$\ell _{2,0}$$ -norm regularized factorization model and now extended to a class of low-rank composite factorization models from matrix completion. The convergence analysis of this PAM method becomes extremely challenging because a subspace correction step is introduced to every proximal subproblem to ensure a closed-form solution. We establish the full convergence of the iterate sequence and column subspace sequences of factor pairs generated by the PAM, under the KL property of the objective function and a condition that holds automatically for the column $$\ell _{2,0}$$ -norm function. Numerical comparison with the popular proximal alternating linearized minimization (PALM) method is conducted on one-bit matrix completion problems, which indicates that the PAM with subspace correction has an advantage in seeking lower relative error within less time.},
  archive      = {J_COAP},
  author       = {Tao, Ting and Qian, Yitian and Pan, Shaohua},
  doi          = {10.1007/s10589-025-00708-6},
  journal      = {Computational Optimization and Applications},
  month        = {11},
  number       = {2},
  pages        = {617--653},
  shortjournal = {Comput. Optim. Appl.},
  title        = {Convergence of the majorized PAM method with subspace correction for low-rank composite factorization model},
  volume       = {92},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A new dual spectral projected gradient method for log-determinant semidefinite programming with hidden clustering structures. <em>COAP</em>, <em>92</em>(2), 589--615. (<a href='https://doi.org/10.1007/s10589-025-00703-x'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper proposes a new efficient method for a sparse Gaussian graphical model with hidden clustering structures by extending a dual spectral projected gradient (DSPG) method proposed by Nakagaki et al. (Comput Opt Appl, 76(1):33–68, 2020). We establish the global convergence of the proposed method to an optimal solution, and we show that the projection onto the feasible region can be solved with low computational complexity by using the pool-adjacent-violators algorithm. Numerical experiments on synthetic data and real data demonstrate the efficiency of the proposed method. The proposed method takes 0.91 s to achieve a similar solution to the direct application of the DSPG method which takes 4361 s.},
  archive      = {J_COAP},
  author       = {Namchaisiri, Charles and Liu, Tianxiang and Yamashita, Makoto},
  doi          = {10.1007/s10589-025-00703-x},
  journal      = {Computational Optimization and Applications},
  month        = {11},
  number       = {2},
  pages        = {589--615},
  shortjournal = {Comput. Optim. Appl.},
  title        = {A new dual spectral projected gradient method for log-determinant semidefinite programming with hidden clustering structures},
  volume       = {92},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Efficient training of gaussian processes with tensor product structure. <em>COAP</em>, <em>92</em>(2), 563--587. (<a href='https://doi.org/10.1007/s10589-025-00707-7'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To determine the optimal set of hyperparameters of a Gaussian process based on a large number of training data, both a linear system and a trace estimation problem must be solved. In this paper, we focus on establishing numerical methods for the case where the covariance matrix is given as the sum of possibly multiple Kronecker products, i.e., can be identified as a tensor. As such, we will represent this operator and the training data in the tensor train format. Based on the AMEn method and Krylov subspace methods, we derive an efficient scheme for computing the matrix functions required for evaluating the gradient and the objective function in hyperparameter optimization.},
  archive      = {J_COAP},
  author       = {Koenig, Josie and Pfeffer, Max and Stoll, Martin},
  doi          = {10.1007/s10589-025-00707-7},
  journal      = {Computational Optimization and Applications},
  month        = {11},
  number       = {2},
  pages        = {563--587},
  shortjournal = {Comput. Optim. Appl.},
  title        = {Efficient training of gaussian processes with tensor product structure},
  volume       = {92},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). An alternating algorithm for structure preserving CP-decompositions of partially symmetric tensors. <em>COAP</em>, <em>92</em>(2), 529--561. (<a href='https://doi.org/10.1007/s10589-025-00705-9'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Paatero [1] and Kolda and Bader [2] pointed out that optimal solutions of the best rank-R approximation of a tensor may not exist in general. This problem is also called as the degeneration problem. In this paper, we study the issue of numerical algorithms of structure preserving rank-R approximation and structure preserving CP-decomposition of partially symmetric tensors. We prove that the rank-2 approximation optimization model based on the alternating best rank-1 approximation is non-degenerate. We then propose an alternating symmetric high-order power method (a-SHOPM) for the best structure preserving rank-R approximation problem and prove the convergence of the algorithm. Furthermore, we find that if the classic BFGS algorithm is used to obtain the initial point first, then the a-SHOPM has better computational performance. So, we then propose a BFGS-a-SHOPM algorithm. Numerical examples show that the BFGS-a-SHOPM algorithm has a better success rate and less computation time for the best structure preserving rank-R approximation or structure preserving CP-decomposition.},
  archive      = {J_COAP},
  author       = {Ni, Jie and Dai, Yuhong and Peng, Zheng},
  doi          = {10.1007/s10589-025-00705-9},
  journal      = {Computational Optimization and Applications},
  month        = {11},
  number       = {2},
  pages        = {529--561},
  shortjournal = {Comput. Optim. Appl.},
  title        = {An alternating algorithm for structure preserving CP-decompositions of partially symmetric tensors},
  volume       = {92},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Refining asymptotic complexity bounds for nonconvex optimization methods, including why steepest descent is $$o(\epsilon ^{-2})$$ rather than $$\mathcal{O}(\epsilon ^{-2})$$. <em>COAP</em>, <em>92</em>(2), 515--527. (<a href='https://doi.org/10.1007/s10589-025-00709-5'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We revisit the standard “telescoping sum” argument ubiquitous in the final steps of analyzing evaluation complexity of algorithms for smooth nonconvex optimization, and obtain a refined formulation of the resulting bound as a function of the requested accuracy $$\epsilon $$ . While bounds obtained using the standard argument typically are of the form $$\mathcal{O}(\epsilon ^{-\alpha })$$ for some positive $$\alpha $$ , the refined results are of the form $$o(\epsilon ^{-\alpha })$$ . We then explore to which known algorithms our refined bounds are applicable and finally describe an example showing how close the standard and refined bounds can be.},
  archive      = {J_COAP},
  author       = {Gratton, S. and Sim, C.-K. and Toint, Ph. L.},
  doi          = {10.1007/s10589-025-00709-5},
  journal      = {Computational Optimization and Applications},
  month        = {11},
  number       = {2},
  pages        = {515--527},
  shortjournal = {Comput. Optim. Appl.},
  title        = {Refining asymptotic complexity bounds for nonconvex optimization methods, including why steepest descent is $$o(\epsilon ^{-2})$$ rather than $$\mathcal{O}(\epsilon ^{-2})$$},
  volume       = {92},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Treatment learning with gini constraints by heaviside composite optimization and a progressive method. <em>COAP</em>, <em>92</em>(2), 471--513. (<a href='https://doi.org/10.1007/s10589-025-00706-8'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper proposes a Heaviside composite optimization approach and presents a progressive method for solving multi-treatment learning problems with non-convex constraints. A Heaviside composite function is a composite of a Heaviside function (i.e., the indicator function of either the open $$( \, 0,\infty )$$ or closed $$[ \, 0,\infty \, )$$ interval) with a possibly nondifferentiable function. Modeling-wise, we show how Heaviside composite optimization provides a rigorous mathematical formulation for learning multi-treatment rules subject to Gini constraints. A Heaviside composite function has an equivalent discrete formulation and the resulting optimization problem can in principle be solved by integer programming (IP) methods. Nevertheless, for constrained treatment learning problems with large datasets, a straightforward application of off-the-shelf IP solvers is usually ineffective in achieving global optimality. To alleviate such a computational burden, our major contribution is the proposal of the progressive method by leveraging the effectiveness of state-of-the-art IP solvers for problems of modest sizes. We provide the theoretical advantage of the progressive method with the connection to continuous optimization and show that the computed solution is locally optimal for a broad class of Heaviside composite optimization problems. The superior numerical performance of the proposed method is demonstrated by extensive computational experimentation. A brief discussion of how score-based and tree-based multi-classification problems can also be formulated as Heaviside composite optimization problems and thus treated by the same progressive method is presented in an appendix.},
  archive      = {J_COAP},
  author       = {Fang, Yue and Liu, Junyi and Pang, Jong-Shi},
  doi          = {10.1007/s10589-025-00706-8},
  journal      = {Computational Optimization and Applications},
  month        = {11},
  number       = {2},
  pages        = {471--513},
  shortjournal = {Comput. Optim. Appl.},
  title        = {Treatment learning with gini constraints by heaviside composite optimization and a progressive method},
  volume       = {92},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A proximal gradient method with an explicit line search for multiobjective optimization. <em>COAP</em>, <em>92</em>(2), 437--469. (<a href='https://doi.org/10.1007/s10589-025-00711-x'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present a proximal gradient method for solving convex multiobjective optimization problems, where each objective function is the sum of two convex functions, one of which is assumed to be continuously differentiable. The algorithm incorporates a backtracking line search procedure that requires solving only one proximal subproblem per iteration, and is exclusively applied to the differentiable part of the objective functions. Under mild assumptions, we show that the sequence generated by the method converges to a weakly Pareto optimal point of the problem. Additionally, we establish an iteration complexity bound by proving that the method finds an $$\varepsilon$$ -approximate weakly Pareto point in at most $${{{\mathcal {O}}}}(1/\varepsilon )$$ iterations. Numerical experiments illustrating the practical behavior of the method are presented.},
  archive      = {J_COAP},
  author       = {Bello-Cruz, Y. and Melo, J. G. and Prudente, L. F. and Serra, R. V. G.},
  doi          = {10.1007/s10589-025-00711-x},
  journal      = {Computational Optimization and Applications},
  month        = {11},
  number       = {2},
  pages        = {437--469},
  shortjournal = {Comput. Optim. Appl.},
  title        = {A proximal gradient method with an explicit line search for multiobjective optimization},
  volume       = {92},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Risk-averse constrained blackbox optimization under mixed aleatory/epistemic uncertainties. <em>COAP</em>, <em>92</em>(2), 375--435. (<a href='https://doi.org/10.1007/s10589-025-00704-w'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper addresses risk-averse constrained optimization problems where the objective and constraint functions can only be computed by a blackbox subject to unknown uncertainties. To handle mixed aleatory/epistemic uncertainties, the problem is transformed into a conditional value-at-risk (CVaR) constrained optimization problem. General inequality constraints are managed through Lagrangian relaxation. A convolution of the Lagrangian function with a truncated Gaussian density is used to smooth the problem. A gradient estimator of the smooth Lagrangian function is derived, possessing attractive properties: it estimates the gradient with only two outputs of the blackbox, regardless of dimension, and evaluates the blackbox only within the bound constraints. This gradient estimator is then utilized in a multi-timescale stochastic approximation algorithm to solve the smooth problem. Under mild assumptions, this algorithm almost surely converges to a feasible point of the CVaR-constrained problem whose objective function value is arbitrarily close to that of a local solution. Finally, numerical experiments are conducted to serve three purposes. Firstly, they provide insights on how to set the hyperparameter values of the algorithm. Secondly, they demonstrate the effectiveness of the algorithm when a truncated Gaussian gradient estimator is used. Lastly, they show its ability to handle mixed aleatory/epistemic uncertainties in practical applications.},
  archive      = {J_COAP},
  author       = {Audet, Charles and Bigeon, Jean and Couderc, Romain and Kokkolaras, Michael},
  doi          = {10.1007/s10589-025-00704-w},
  journal      = {Computational Optimization and Applications},
  month        = {11},
  number       = {2},
  pages        = {375--435},
  shortjournal = {Comput. Optim. Appl.},
  title        = {Risk-averse constrained blackbox optimization under mixed aleatory/epistemic uncertainties},
  volume       = {92},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A general preconditioner for a class of vertical tensor complementarity problems. <em>COAP</em>, <em>92</em>(1), 345--373. (<a href='https://doi.org/10.1007/s10589-025-00700-0'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, based on the methodology of the preconditioning technique, our objective is devoted to solving a class of vertical tensor complementarity problems (VTCP) by the preconditioned fixed point method based on tensor splitting. We firstly propose a general preconditioner based on the amount of negative components of special vector involved. Secondly, some convergence and comparison theorems of the proposed method are given. Thirdly, we analyze the influence of the parameter on the convergence rate of the proposed method. Finally, numerical examples are given to illustrate the effectiveness of the proposed method.},
  archive      = {J_COAP},
  author       = {Wu, Shi-Liang and Long, Mei and Li, Cui-Xia},
  doi          = {10.1007/s10589-025-00700-0},
  journal      = {Computational Optimization and Applications},
  month        = {9},
  number       = {1},
  pages        = {345--373},
  shortjournal = {Comput. Optim. Appl.},
  title        = {A general preconditioner for a class of vertical tensor complementarity problems},
  volume       = {92},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A unified approach for smoothing approximations to the exact $$\ell _1$$ -penalty for inequality-constrained optimization. <em>COAP</em>, <em>92</em>(1), 327--344. (<a href='https://doi.org/10.1007/s10589-025-00694-9'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In penalty methods for inequality-constrained optimization problems, the nondifferentiability of the exact $$\ell _1$$ -penalty function limits the use of efficient smooth algorithms for solving the subproblems. In light of this, a great number of smoothing techniques has been proposed in the literature. In this paper we present, in a unified manner, results and methods based on functions that smooth and approximate the exact penalty function. We show that these functions define a class of algorithms that converges to global and local minimizers. This unified approach allows us to derive sufficient conditions that guarantee the existence of local minimizers for the subproblems and to establish a linear convergence rate for this class of methods, using an error bound-type condition. Finally, numerical experiments with problems of the CUTEst collection are presented to illustrate the computational performance of some methods from the literature which can be recovered as particular cases of our unified approach.},
  archive      = {J_COAP},
  author       = {Rosa, Mariana da and Ribeiro, Ademir Alves and Karas, Elizabeth Wegner},
  doi          = {10.1007/s10589-025-00694-9},
  journal      = {Computational Optimization and Applications},
  month        = {9},
  number       = {1},
  pages        = {327--344},
  shortjournal = {Comput. Optim. Appl.},
  title        = {A unified approach for smoothing approximations to the exact $$\ell _1$$ -penalty for inequality-constrained optimization},
  volume       = {92},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Adaptive cyclic gradient methods with interpolation. <em>COAP</em>, <em>92</em>(1), 301--325. (<a href='https://doi.org/10.1007/s10589-025-00691-y'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Gradient method is an important method for solving large scale problems. In this paper, a new gradient method framework for unconstrained optimization problem is proposed, where the stepsize is updated in a cyclic way. The Cauchy step is approximated by the quadratic interpolation. And the cycle for stepsize update is adjusted adaptively. Combining with the adaptive nonmonotone line search technique, we prove the global convergence of the proposed method. Furthermore, its sublinear convergence rate for convex problems and R-linear convergence rate for problems with quadratic functional growth property are analyzed. Numerical results show that our proposed algorithm enjoys good performances in terms of both computational cost and obtained function values.},
  archive      = {J_COAP},
  author       = {Xie, Yixin and Sun, Cong and Yuan, Ya-Xiang},
  doi          = {10.1007/s10589-025-00691-y},
  journal      = {Computational Optimization and Applications},
  month        = {9},
  number       = {1},
  pages        = {301--325},
  shortjournal = {Comput. Optim. Appl.},
  title        = {Adaptive cyclic gradient methods with interpolation},
  volume       = {92},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A sequence-form differentiable path-following method to compute nash equilibria. <em>COAP</em>, <em>92</em>(1), 265--300. (<a href='https://doi.org/10.1007/s10589-025-00702-y'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The sequence-form representation has shown remarkable efficacy in computing Nash equilibria for two-player extensive-form games with perfect recall. Nonetheless, devising an efficient algorithm for n-player games using the sequence form remains a substantial challenge. To bridge this gap, we establish a necessary and sufficient condition, characterized by a polynomial system, for Nash equilibrium within the sequence-form framework. Building upon this, we develop a sequence-form differentiable path-following method for computing a Nash equilibrium. This method involves constructing an artificial logarithmic-barrier game in sequence form, where two functions of an auxiliary variable are introduced to incorporate logarithmic-barrier terms into the payoff functions and construct the strategy space. Afterward, we prove the existence of a smooth path determined by the artificial game, originating from an arbitrary totally mixed behavioral-strategy profile and converging to a Nash equilibrium of the original game as the auxiliary variable approaches zero. In addition, a convex-quadratic-penalty method and a variant of linear tracing procedure in sequence form are presented as two alternative techniques for computing a Nash equilibrium. Numerical comparisons further illuminate the effectiveness and efficiency of these methods.},
  archive      = {J_COAP},
  author       = {Hou, Yuqing and Cao, Yiyin and Dang, Chuangyin and Wang, Yong},
  doi          = {10.1007/s10589-025-00702-y},
  journal      = {Computational Optimization and Applications},
  month        = {9},
  number       = {1},
  pages        = {265--300},
  shortjournal = {Comput. Optim. Appl.},
  title        = {A sequence-form differentiable path-following method to compute nash equilibria},
  volume       = {92},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Preprocessing and valid inequalities for exact detection of critical nodes via integer programming. <em>COAP</em>, <em>92</em>(1), 215--263. (<a href='https://doi.org/10.1007/s10589-025-00698-5'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The critical nodes detection problem (CNDP) involves identifying a limited number of nodes for removal from an undirected graph, to maximize the disconnections between remaining node pairs. In this paper, we shall provide a high-efficiency algorithm for precisely solving the integer programming (IP) formulations for the CNDP. Firstly, a preprocessing procedure is introduced, which can not only reduce the size of the exponential-size IP formulation of the problem but also strengthen the linear programming relaxation. Secondly, the polyhedral properties of the polytope associated with the exponential-size IP formulation are explored, providing a flexible way to derive facet-defining inequalities for the polytope from certain projected ones. Thirdly, a family of strong valid inequalities based on clique subgraphs is developed for the polytope, with both necessary and sufficient conditions for them to be facet-defining. The complexity and algorithm of the separation problem for these inequalities are also investigated. Finally, we extend our research findings from the exponential-size IP formulation to two polynomial-size IP reformulations for the CNDP. Computational results demonstrate the efficacy of incorporating our proposed preprocessing and valid inequalities into an IP solver for solving all three CNDP formulations.},
  archive      = {J_COAP},
  author       = {Chen, Sheng-Jie and Chen, Liang and Li, Guang-Ming and Dai, Yu-Hong},
  doi          = {10.1007/s10589-025-00698-5},
  journal      = {Computational Optimization and Applications},
  month        = {9},
  number       = {1},
  pages        = {215--263},
  shortjournal = {Comput. Optim. Appl.},
  title        = {Preprocessing and valid inequalities for exact detection of critical nodes via integer programming},
  volume       = {92},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Cardinality objective nonlinear programs for facility capacity expansion. <em>COAP</em>, <em>92</em>(1), 179--214. (<a href='https://doi.org/10.1007/s10589-025-00697-6'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we consider a class of mathematical programs with inequality and equality constraints where the objective involves a cardinality penalty. The introduction of cardinality penalty can make the model automatically generate sparse solutions, but solving the cardinality objective nonlinear program is highly challenging since the objective function is discontinuous. We first give a continuous approximation and discuss its relationship with the original problem. Second, we propose a proximal augmented Lagrangian method for finding a weak directional(d)-stationary point of the continuous approximation. The proposed algorithm is a novel combination of the classical augmented Lagrangian method and proximal gradient algorithm. We prove that the proposed method globally converges to a weak d-stationary point of the continuous approximation, which is stronger than Clarke stationary point. Third, we demonstrate that the cardinality objective nonlinear program is a better model for the facility capacity expansion problem, which can generate key capacity expansion locations to avoid the high operating costs caused by expanding a large number of facilities. Finally, a systematic computational study on two capacity expansion problems is presented. The numerical results demonstrate the benefit of the cardinality objective nonlinear program and the effectiveness of the proposed algorithm.},
  archive      = {J_COAP},
  author       = {Li, Gao-Xi and Yang, Xin-Min},
  doi          = {10.1007/s10589-025-00697-6},
  journal      = {Computational Optimization and Applications},
  month        = {9},
  number       = {1},
  pages        = {179--214},
  shortjournal = {Comput. Optim. Appl.},
  title        = {Cardinality objective nonlinear programs for facility capacity expansion},
  volume       = {92},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A subspace minimization barzilai-borwein method for multiobjective optimization problems. <em>COAP</em>, <em>92</em>(1), 155--178. (<a href='https://doi.org/10.1007/s10589-025-00695-8'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Nonlinear conjugate gradient methods have recently garnered significant attention within the multiobjective optimization community. These methods maintain consistency in conjugate parameters with their single-objective optimization counterparts. However, the desirable conjugate property of search directions remains uncertain, even for quadratic cases, in multiobjective conjugate gradient methods. This loss of the conjugate property significantly limits the applicability of these methods. To elucidate the role of the last search direction, we develop a subspace minimization Barzilai-Borwein method for multiobjective optimization problems (SMBBMO). In SMBBMO, each search direction is derived by optimizing a preconditioned Barzilai-Borwein subproblem within a two-dimensional subspace generated by the last search direction and the current Barzilai-Borwein descent direction. Furthermore, to ensure the global convergence of SMBBMO, we employ a modified Cholesky factorization on a transformed scale matrix, capturing the local curvature information of the problem within the two-dimensional subspace. Under mild assumptions, we establish both global and Q-linear convergence of the proposed method. Finally, comparative numerical experiments confirm the efficiency of SMBBMO, even when tackling large-scale and ill-conditioned problems.},
  archive      = {J_COAP},
  author       = {Chen, Jian and Tang, Liping and Yang, Xinmin},
  doi          = {10.1007/s10589-025-00695-8},
  journal      = {Computational Optimization and Applications},
  month        = {9},
  number       = {1},
  pages        = {155--178},
  shortjournal = {Comput. Optim. Appl.},
  title        = {A subspace minimization barzilai-borwein method for multiobjective optimization problems},
  volume       = {92},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). The cosine measure relative to a subspace. <em>COAP</em>, <em>92</em>(1), 125--153. (<a href='https://doi.org/10.1007/s10589-025-00701-z'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The cosine measure was introduced in 2003 to quantify the richness of finite positive spanning sets of directions in the context of derivative-free directional methods. A positive spanning set is a set of vectors whose nonnegative linear combinations span the whole space. The present work extends the definition of cosine measure. In particular, the paper studies cosine measures relative to a subspace, and proposes a deterministic algorithm to compute it. The paper also studies the situation in which the set of vectors is infinite. The extended definition of the cosine measure might be useful for subspace decomposition methods.},
  archive      = {J_COAP},
  author       = {Audet, Charles and Hare, Warren and Jarry-Bolduc, Gabriel},
  doi          = {10.1007/s10589-025-00701-z},
  journal      = {Computational Optimization and Applications},
  month        = {9},
  number       = {1},
  pages        = {125--153},
  shortjournal = {Comput. Optim. Appl.},
  title        = {The cosine measure relative to a subspace},
  volume       = {92},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A bi-objective optimization based acquisition strategy for batch bayesian global optimization. <em>COAP</em>, <em>92</em>(1), 81--123. (<a href='https://doi.org/10.1007/s10589-025-00696-7'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper we deal with batch Bayesian Optimization (Bayes-Opt) problems over a box. Bayes-Opt approaches find their main applications when the objective function is very expensive to evaluate. Sometimes, given the availability of multi-processor computing architectures, function evaluation might be performed in parallel in order to lower the clock-time of the overall computation. This paper fits this situation and is devoted to the development of a novel bi-objective optimization (BOO) acquisition strategy to sample batches of points where to evaluate the objective function. The BOO problem involves the Gaussian Process posterior mean and variance functions, which, in most of the acquisition strategies from the literature, are generally used in combination, frequently through scalarization. However, such scalarization could compromise the Bayes-Opt process performance, as getting the desired trade-off between exploration and exploitation is not trivial in most cases. We instead aim to reconstruct the Pareto front of the BOO problem exploiting first order information of the posterior mean and variance, thus generating multiple trade-offs of the two functions without any a priori knowledge. The algorithm used for the reconstruction is the Non-dominated Sorting Memetic Algorithm (NSMA), recently proposed in the literature and proved to be effective in solving hard MOO problems. Finally, we present two clustering approaches, each of them operating on a different space, to select potentially optimal points from the Pareto front. We compare our methodology with well-known acquisition strategies from the literature, showing its effectiveness on a wide set of experiments.},
  archive      = {J_COAP},
  author       = {Carciaghi, Francesco and Magistri, Simone and Mansueto, Pierluigi and Schoen, Fabio},
  doi          = {10.1007/s10589-025-00696-7},
  journal      = {Computational Optimization and Applications},
  month        = {9},
  number       = {1},
  pages        = {81--123},
  shortjournal = {Comput. Optim. Appl.},
  title        = {A bi-objective optimization based acquisition strategy for batch bayesian global optimization},
  volume       = {92},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Riemannian adaptive regularized newton methods with hölder continuous hessians. <em>COAP</em>, <em>92</em>(1), 29--79. (<a href='https://doi.org/10.1007/s10589-025-00692-x'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper presents strong worst-case iteration and operation complexity guarantees for Riemannian adaptive regularized Newton methods, a unified framework encompassing both Riemannian adaptive regularization (RAR) methods and Riemannian trust region (RTR) methods. We comprehensively characterize the sources of approximation in second-order manifold optimization methods: the objective function’s smoothness, retraction’s smoothness, and subproblem solver’s inexactness. Specifically, for a function with a $$\mu $$ -Hölder continuous Hessian, when equipped with a retraction featuring a $$\nu $$ -Hölder continuous differential and a $$\theta $$ -inexact subproblem solver, both RTR and RAR with $$2\!+\!\alpha $$ regularization (where $$\alpha =\min \{\mu ,\nu ,\theta \}$$ ) locate an $$(\epsilon ,\epsilon ^{\alpha /(1+\alpha )})$$ -approximate second-order stationary point within at most $$O(\epsilon ^{-(2+\alpha )/(1+\alpha )})$$ iterations and at most $${\widetilde{O}}(\epsilon ^{- (4+3\alpha ) /(2(1+\alpha ))})$$ Hessian-vector products with high probability. These complexity results are novel and sharp, and reduce to an iteration complexity of $$O(\epsilon ^{-3 /2})$$ and an operation complexity of $${\widetilde{O}}(\epsilon ^{-7 /4})$$ when $$\alpha =1$$ .},
  archive      = {J_COAP},
  author       = {Zhang, Chenyu and Jiang, Rujun},
  doi          = {10.1007/s10589-025-00692-x},
  journal      = {Computational Optimization and Applications},
  month        = {9},
  number       = {1},
  pages        = {29--79},
  shortjournal = {Comput. Optim. Appl.},
  title        = {Riemannian adaptive regularized newton methods with hölder continuous hessians},
  volume       = {92},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A randomized feasible algorithm for optimization with orthogonal constraints. <em>COAP</em>, <em>92</em>(1), 1--27. (<a href='https://doi.org/10.1007/s10589-025-00693-w'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we propose a randomized feasible algorithm for optimization over the Stiefel manifold, where only some randomly chosen columns of the variable matrix are updated at each iteration. It is proved that the sequence of Riemannian gradients generated by the algorithm converges to zero with probability one. Numerical results show that the algorithm is efficient, especially for the problems when the matrices involved are sparse.},
  archive      = {J_COAP},
  author       = {Fei, Fan and Feng, Yuchen and Fan, Jinyan},
  doi          = {10.1007/s10589-025-00693-w},
  journal      = {Computational Optimization and Applications},
  month        = {9},
  number       = {1},
  pages        = {1--27},
  shortjournal = {Comput. Optim. Appl.},
  title        = {A randomized feasible algorithm for optimization with orthogonal constraints},
  volume       = {92},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Correction: Regularized methods via cubic model subspace minimization for nonconvex optimization. <em>COAP</em>, <em>91</em>(3), 1415--1416. (<a href='https://doi.org/10.1007/s10589-025-00681-0'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_COAP},
  author       = {Bellavia, Stefania and Palitta, Davide and Porcelli, Margherita and Simoncini, Valeria},
  doi          = {10.1007/s10589-025-00681-0},
  journal      = {Computational Optimization and Applications},
  month        = {7},
  number       = {3},
  pages        = {1415--1416},
  shortjournal = {Comput. Optim. Appl.},
  title        = {Correction: Regularized methods via cubic model subspace minimization for nonconvex optimization},
  volume       = {91},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Inner $$\delta $$ -approximation of the convex hull of finite sets. <em>COAP</em>, <em>91</em>(3), 1373--1413. (<a href='https://doi.org/10.1007/s10589-025-00682-z'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {For a given finite set X and an approximation parameter $$\delta \ge 0$$ , a convex polygon or polyhedron $$\mathcal{P}^\textrm{inner}$$ is called an inner $$\delta $$ -approximation of the convex hull $${{\,\textrm{conv}\,}}X$$ of X if $${{\,\textrm{conv}\,}}X$$ contains $$\mathcal{P}^\textrm{inner}$$ and the Hausdorff distance between them is not greater than $$\delta $$ . In this paper, two algorithms for computing inner $$\delta $$ -approximation in 2D are developed. This approximation approach can reduce the computation time. For example, if X consists of $$1,\!000,\!000$$ random points in an ellipse, the computation time can be reduced by $$11.20\%$$ if one chooses $$\delta $$ to be equal to $$10^{-4}$$ multiplied by the diameter of this ellipse. By choosing $$\delta = 0$$ , our algorithms can be applied to quickly determine the exact convex hull $${{\,\textrm{conv}\,}}X$$ . Numerical experiments confirm that their time complexity is linear in n if X consists of n random points in ellipses or rectangles. Compared to others, our Algorithm 2 is much faster than the Quickhull algorithm in the Qhull library, which is faster than all 2D convex hull functions in CGAL (Computational Geometry Algorithm Library). If X consists of $$n = 100,\!000$$ random points in an ellipse or a rectangle, Algorithm 2 is 5.17 or 18.26 times faster than Qhull, respectively. The speedup factors of our algorithms increase with n. E.g., if X consists of $$n = 46,\!200,\!000$$ random points in an ellipse or a rectangle, the speedup factors of Algorithm 2 compared to Qhull are 8.46 and 22.44, respectively.},
  archive      = {J_COAP},
  author       = {Hoang, Nam-Dũng and Linh, Nguyen Kieu and Phu, Hoang Xuan},
  doi          = {10.1007/s10589-025-00682-z},
  journal      = {Computational Optimization and Applications},
  month        = {7},
  number       = {3},
  pages        = {1373--1413},
  shortjournal = {Comput. Optim. Appl.},
  title        = {Inner $$\delta $$ -approximation of the convex hull of finite sets},
  volume       = {91},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). An efficient global optimization algorithm for the sum of linear ratios problems based on a novel adjustable branching rule. <em>COAP</em>, <em>91</em>(3), 1339--1371. (<a href='https://doi.org/10.1007/s10589-025-00679-8'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, an efficient branch and bound algorithm with a new adjustable branching rule is presented to solve the sum of linear ratios problem (SLRP). In the algorithm, problem (SLRP) is first converted into its equivalent form (ERP) whose objective function involves ( $$p-1$$ ) linear ratios via Charnes–Cooper transformation, and (ERP) is equivalently translated to problem (EP) which has a linear objective function by some variables transformation. A convex relaxation problem (CRP) for (EP) is constructed to obtain a lower bound to the optimal value of (EP). In addition, a novel adjustable branching rule is proposed to offer tight lower bounds to the optimal values of (EP) over the corresponding sub-rectangles under some certain conditions. Also, a convex combination method is designed to update the upper bound for the optimal value of (ERP). By continuously refining the initial rectangle and tackling a series of convex relaxation problems, the presented algorithm can find a global optimal solution to (ERP). Moreover, we analyze the complexity result of the proposed algorithm. Finally, the feasibility and effectiveness of the algorithm are verified by preliminary numerical experiments.},
  archive      = {J_COAP},
  author       = {Huang, Bingdi and Shen, Peiping},
  doi          = {10.1007/s10589-025-00679-8},
  journal      = {Computational Optimization and Applications},
  month        = {7},
  number       = {3},
  pages        = {1339--1371},
  shortjournal = {Comput. Optim. Appl.},
  title        = {An efficient global optimization algorithm for the sum of linear ratios problems based on a novel adjustable branching rule},
  volume       = {91},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Adaptive dipole-like parameter calibration of complex black-box continuous processes. <em>COAP</em>, <em>91</em>(3), 1309--1338. (<a href='https://doi.org/10.1007/s10589-025-00677-w'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Large-scale systems such as flexible manufacturing, chemical production facilities, and traffic networks aim to maximize measures related to profit, health and safety, throughput, and service level. Due to the complexity of such systems, the mechanism that connects the input parameters to performance outputs is often unavailable, and optimization methods based on convexity or even differentiability of the objective function may not be applicable. Since these systems are characterized by heavy costs per unit time, the system manager has to resort to black-box approaches for optimization, where a set of parameters is tuned in order to maximize an accumulated performance measure of the process. In this paper, a novel mechanism is proposed for real-time calibration of parameters in continuous search spaces. The developed algorithm seeks the global optimum by means of solution exploitation, and adapts dynamically according to environmental changes. The solution method builds a sequence of random pairs of trials, called “dipoles”, which are used to adapt online the probability density function of the unknown parameters. The proposed method is characterized by the following advantages: (1) it does not depend on subjective coefficients setting; (2) solution exploitation starts from the first iteration; (3) the algorithm is effective also for systems with high dimensionality; (4) since sampling only involves two trials, exploitation is based on recent data rather than on data that extends far back in time. Several illustrative numerical examples are provided to show the applicability and efficiency of the proposed method.},
  archive      = {J_COAP},
  author       = {Herbon, Avi and Gaggero, Mauro and Khmelnitsky, Eugene and Sanguineti, Marcello},
  doi          = {10.1007/s10589-025-00677-w},
  journal      = {Computational Optimization and Applications},
  month        = {7},
  number       = {3},
  pages        = {1309--1338},
  shortjournal = {Comput. Optim. Appl.},
  title        = {Adaptive dipole-like parameter calibration of complex black-box continuous processes},
  volume       = {91},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). On the forward–backward method with nonmonotone linesearch for infinite-dimensional nonsmooth nonconvex problems. <em>COAP</em>, <em>91</em>(3), 1263--1308. (<a href='https://doi.org/10.1007/s10589-025-00684-x'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper provides a comprehensive study of the nonmonotone forward–backward splitting (FBS) method for solving a class of nonsmooth composite problems in Hilbert spaces. The objective function is the sum of a Fréchet differentiable (not necessarily convex) function and a proper lower semicontinuous convex (not necessarily smooth) function. These problems appear, for example, frequently in the context of optimal control of nonlinear partial differential equations (PDEs) with nonsmooth sparsity-promoting cost functionals. We discuss the convergence and complexity of FBS equipped with the nonmonotone linesearch under different conditions. In particular, R-linear convergence will be derived under quadratic growth-type conditions. We also investigate the applicability of the algorithm to problems governed by PDEs. Numerical experiments are also given that justify our theoretical findings.},
  archive      = {J_COAP},
  author       = {Azmi, Behzad and Bernreuther, Marco},
  doi          = {10.1007/s10589-025-00684-x},
  journal      = {Computational Optimization and Applications},
  month        = {7},
  number       = {3},
  pages        = {1263--1308},
  shortjournal = {Comput. Optim. Appl.},
  title        = {On the forward–backward method with nonmonotone linesearch for infinite-dimensional nonsmooth nonconvex problems},
  volume       = {91},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Convergence rate of inexact augmented lagrangian method with practical relative error criterion for composite convex programming. <em>COAP</em>, <em>91</em>(3), 1227--1261. (<a href='https://doi.org/10.1007/s10589-025-00683-y'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we consider the composite convex optimization problem with a linear equality constraint. We propose a practical inexact augmented Lagrangian (IAL) framework that employs two relative error criteria. Under the first criterion, we demonstrate convergence and establish sublinear ergodic convergence rates. By incorporating the second criterion, we achieve sublinear non-ergodic convergence rates. Furthermore, we determine the total iteration complexity of the IAL framework by slightly relaxing these criteria. Numerical experiments on both synthetic and real-world problems are conducted to illustrate the efficiency of the proposed IAL method.},
  archive      = {J_COAP},
  author       = {Qu, Yunfei and Cai, Xingju and Liu, Hongying and Han, Deren},
  doi          = {10.1007/s10589-025-00683-y},
  journal      = {Computational Optimization and Applications},
  month        = {7},
  number       = {3},
  pages        = {1227--1261},
  shortjournal = {Comput. Optim. Appl.},
  title        = {Convergence rate of inexact augmented lagrangian method with practical relative error criterion for composite convex programming},
  volume       = {91},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). An extended sequential quadratic method with extrapolation. <em>COAP</em>, <em>91</em>(3), 1185--1225. (<a href='https://doi.org/10.1007/s10589-025-00680-1'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We revisit and adapt the extended sequential quadratic method (ESQM) in Auslender (J Optim Theory Appl 156:183–212, 2013) for solving a class of difference-of-convex optimization problems whose constraints are defined as the intersection of level sets of Lipschitz differentiable functions and a simple compact convex set. Particularly, for this class of problems, we develop a variant of ESQM, called ESQM with extrapolation ( $$\hbox {ESQM}_{\textrm{e}}$$ ), which incorporates Nesterov’s extrapolation techniques for empirical acceleration. Under standard constraint qualifications, we show that the sequence generated by $$\hbox {ESQM}_{\textrm{e}}$$ clusters at a critical point if the extrapolation parameters are uniformly bounded above by a certain threshold. Convergence of the whole sequence and the convergence rate are established by assuming Kurdyka-Łojasiewicz (KL) property of a suitable potential function and imposing additional differentiability assumptions on the objective and constraint functions. In addition, when the objective and constraint functions are all convex, we show that linear convergence can be established if a certain exact penalty function is known to be a KL function with exponent $$\frac{1}{2}$$ ; we also discuss how the KL exponent of such an exact penalty function can be deduced from that of the original extended objective (i.e., sum of the objective and the indicator function of the constraint set). Finally, we perform numerical experiments to demonstrate the empirical acceleration of $$\hbox {ESQM}_{\textrm{e}}$$ over a basic version of ESQM, and illustrate its effectiveness by comparing with the natural competing algorithm $$\hbox {SCP}_{\textrm{ls}}$$ from Yu et al. (SIAM J Optim 31:2024–2054, 2021).},
  archive      = {J_COAP},
  author       = {Zhang, Yongle and Pong, Ting Kei and Xu, Shiqi},
  doi          = {10.1007/s10589-025-00680-1},
  journal      = {Computational Optimization and Applications},
  month        = {7},
  number       = {3},
  pages        = {1185--1225},
  shortjournal = {Comput. Optim. Appl.},
  title        = {An extended sequential quadratic method with extrapolation},
  volume       = {91},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Riemannian optimization using three different metrics for hermitian PSD fixed-rank constraints. <em>COAP</em>, <em>91</em>(3), 1135--1184. (<a href='https://doi.org/10.1007/s10589-025-00687-8'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {For smooth optimization problems with a Hermitian positive semidefinite fixed-rank constraint, we consider three existing approaches including the simple Burer–Monteiro method, and Riemannian optimization over quotient geometry and the embedded geometry. These three methods can be all represented via quotient geometry with three Riemannian metrics $$g^i(\cdot , \cdot )$$ $$(i=1,2,3)$$ . By taking the nonlinear conjugate gradient method (CG) as an example, we show that CG in the factor-based Burer–Monteiro approach is equivalent to Riemannian CG on the quotient geometry with the Bures–Wasserstein metric $$g^1$$ . Riemannian CG on the quotient geometry with the metric $$g^3$$ is equivalent to Riemannian CG on the embedded geometry. For comparing the three approaches, we analyze the condition number of the Riemannian Hessian near a minimizer under the three different metrics. Under certain assumptions, the condition number from the Bures–Wasserstein metric $$g^1$$ is significantly worse than the other two metrics. Numerical experiments show that the Burer–Monteiro CG method has obviously slower asymptotic convergence rate either when the minimizer has a large condition number or when it is rank deficient, which is consistent with the condition number analysis.},
  archive      = {J_COAP},
  author       = {Zheng, Shixin and Huang, Wen and Vandereycken, Bart and Zhang, Xiangxiong},
  doi          = {10.1007/s10589-025-00687-8},
  journal      = {Computational Optimization and Applications},
  month        = {7},
  number       = {3},
  pages        = {1135--1184},
  shortjournal = {Comput. Optim. Appl.},
  title        = {Riemannian optimization using three different metrics for hermitian PSD fixed-rank constraints},
  volume       = {91},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Convergence analysis of a mixed logarithmic barrier-augmented lagrangian algorithm without constraint qualification. <em>COAP</em>, <em>91</em>(3), 1105--1134. (<a href='https://doi.org/10.1007/s10589-025-00690-z'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we exploit some properties of points in a neighborhood of the solution set of degenerate optimization problems. Combining these facts with the boundedness of the inverse of regularized Jacobian matrix arising in a mixed logarithmic barrier-augmented lagrangian method, we propose an updating rule for parameters of a mixed logarithmic barrier-augmented Lagrangian algorithm. The superlinear convergence of this algorithm is then proved without any constraint qualification. Numerical results on degenerate problems are also presented to confirm theoretical results.},
  archive      = {J_COAP},
  author       = {Nguyen, Tran Ngoc},
  doi          = {10.1007/s10589-025-00690-z},
  journal      = {Computational Optimization and Applications},
  month        = {7},
  number       = {3},
  pages        = {1105--1134},
  shortjournal = {Comput. Optim. Appl.},
  title        = {Convergence analysis of a mixed logarithmic barrier-augmented lagrangian algorithm without constraint qualification},
  volume       = {91},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A flexible gradient tracking algorithmic framework for decentralized optimization. <em>COAP</em>, <em>91</em>(3), 1073--1104. (<a href='https://doi.org/10.1007/s10589-025-00685-w'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In decentralized optimization over networks, each node in the network has a portion of the global objective function and the aim is to collectively optimize this function. Gradient tracking methods have emerged as a popular alternative for solving such problems due to their strong theoretical guarantees and robust empirical performance. These methods perform two operations (steps) at each iteration: (1) compute local gradients at each node, and (2) communicate local information with neighboring nodes. The complexity of these two steps can vary significantly across applications. In this work, we present a flexible gradient tracking algorithmic framework designed to balance the composition of communication and computation steps over the optimization process using a randomized scheme. The proposed framework is general, unifies gradient tracking methods, and recovers classical gradient tracking methods as special cases. We establish convergence guarantees in expectation and illustrate how the complexity of communication and computation steps can be balanced using the provided flexibility. Finally, we illustrate the performance of the proposed methods on quadratic and logistic regression problems, and compare against popular algorithms from the literature.},
  archive      = {J_COAP},
  author       = {Berahas, Albert S. and Bollapragada, Raghu and Gupta, Shagun},
  doi          = {10.1007/s10589-025-00685-w},
  journal      = {Computational Optimization and Applications},
  month        = {7},
  number       = {3},
  pages        = {1073--1104},
  shortjournal = {Comput. Optim. Appl.},
  title        = {A flexible gradient tracking algorithmic framework for decentralized optimization},
  volume       = {91},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Second order shape optimization for an interface identification problem constrained by nonlocal models. <em>COAP</em>, <em>91</em>(3), 1033--1071. (<a href='https://doi.org/10.1007/s10589-025-00678-9'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Since shape optimization methods have been proven useful for identifying interfaces in models governed by partial differential equations, we show how shape optimization techniques can also be applied to an interface identification problem constrained by a nonlocal Dirichlet problem. Here, we focus on deriving the second shape derivative of the corresponding reduced functional and we further investigate a second-order optimization algorithm.},
  archive      = {J_COAP},
  author       = {Schuster, Matthias and Schulz, Volker},
  doi          = {10.1007/s10589-025-00678-9},
  journal      = {Computational Optimization and Applications},
  month        = {7},
  number       = {3},
  pages        = {1033--1071},
  shortjournal = {Comput. Optim. Appl.},
  title        = {Second order shape optimization for an interface identification problem constrained by nonlocal models},
  volume       = {91},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A multi-precision quadratic regularization method for unconstrained optimization with rounding error analysis. <em>COAP</em>, <em>91</em>(3), 997--1031. (<a href='https://doi.org/10.1007/s10589-025-00676-x'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a multi-precision extension of the Quadratic Regularization (R2) algorithm that enables it to take advantage of low-precision computations, and by extension to decrease energy consumption during the solve. The lower the precision in which computations occur, the larger the errors induced in the objective value and gradient, as well as in all other computations that occur in the course of the iterations. The Multi-Precision R2 (MPR2) algorithm monitors the accumulation of rounding errors with two aims: to provide guarantees on the result of all computations, and to permit evaluations of the objective and its gradient in the lowest precision possible while preserving convergence properties. MPR2’s numerical results show that single precision offers enough accuracy for more than half of objective evaluations and most of gradient evaluations during the algorithm’s execution. However, MPR2 fails to converge on several problems of the test set for which double precision does not offer enough precision to ensure the convergence conditions before reaching a first order critical point. That is why we propose a practical version of MPR2 with relaxed conditions which converges for almost as many problems as R2 and potentially enables saving about 50% time and 60% energy for objective evaluation and 50% time and 70% energy for gradient evaluation.},
  archive      = {J_COAP},
  author       = {Monnet, Dominique and Orban, Dominique},
  doi          = {10.1007/s10589-025-00676-x},
  journal      = {Computational Optimization and Applications},
  month        = {7},
  number       = {3},
  pages        = {997--1031},
  shortjournal = {Comput. Optim. Appl.},
  title        = {A multi-precision quadratic regularization method for unconstrained optimization with rounding error analysis},
  volume       = {91},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Convergence of a quasi-newton method for solving systems of nonlinear underdetermined equations. <em>COAP</em>, <em>91</em>(2), 973--996. (<a href='https://doi.org/10.1007/s10589-024-00606-3'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The development and convergence analysis of a quasi-Newton method for the solution of systems of nonlinear underdetermined equations is investigated. These equations arise in many application fields, e.g., supervised learning of large overparameterised neural networks, which require the development of efficient methods with guaranteed convergence. In this paper, a new approach for the computation of the Moore–Penrose inverse of the approximate Jacobian coming from the Broyden update is presented and a semi-local convergence result for a damped quasi-Newton method is proved. The theoretical results are illustrated in detail for the case of systems of multidimensional quadratic equations, and validated in the context of eigenvalue problems and supervised learning of overparameterised neural networks.},
  archive      = {J_COAP},
  author       = {Vater, N. and Borzì, A.},
  doi          = {10.1007/s10589-024-00606-3},
  journal      = {Computational Optimization and Applications},
  month        = {6},
  number       = {2},
  pages        = {973--996},
  shortjournal = {Comput. Optim. Appl.},
  title        = {Convergence of a quasi-newton method for solving systems of nonlinear underdetermined equations},
  volume       = {91},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Convergence of ease-controlled random reshuffling gradient algorithms under lipschitz smoothness. <em>COAP</em>, <em>91</em>(2), 933--971. (<a href='https://doi.org/10.1007/s10589-025-00667-y'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this work, we consider minimizing the average of a very large number of smooth and possibly non-convex functions, and we focus on two widely used minibatch frameworks to tackle this optimization problem: incremental gradient (IG) and random reshuffling (RR). We define ease-controlled modifications of the IG/RR schemes, which require a light additional computational effort but can be proved to converge to a stationary point under weak and standard assumptions. In particular, we define two algorithmic schemes in which the IG/RR iteration is controlled by using a watchdog rule and a derivative-free linesearch that activates only sporadically to adjust the stepsize so to guarantee convergence. The two schemes differ in the watchdog rule and the linesearch, which are performed using either a monotonic or a non-monotonic rule. The two schemes allow controlling the updating of the stepsize used in the main IG/RR iteration, avoiding the use of pre-set rules that may drive the stepsize to zero too fast, reducing the effort in designing effective updating rules of the stepsize. We perform computational analysis using different deep neural architectures and a benchmark of varying-size datasets. We compare our implementation with both a full batch gradient method (i.e. L-BFGS) and a fair implementation of IG/RR methods, proving that our algorithms require a similar computational effort compared to the other online algorithms and that the control on the learning rate may allow a faster decrease of the objective function.},
  archive      = {J_COAP},
  author       = {Seccia, Ruggiero and Coppola, Corrado and Liuzzi, Giampaolo and Palagi, Laura},
  doi          = {10.1007/s10589-025-00667-y},
  journal      = {Computational Optimization and Applications},
  month        = {6},
  number       = {2},
  pages        = {933--971},
  shortjournal = {Comput. Optim. Appl.},
  title        = {Convergence of ease-controlled random reshuffling gradient algorithms under lipschitz smoothness},
  volume       = {91},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Truncated LSQR for matrix least squares problems. <em>COAP</em>, <em>91</em>(2), 905--932. (<a href='https://doi.org/10.1007/s10589-024-00629-w'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We are interested in the numerical solution of the matrix least squares problem $$\begin{aligned} \min _{X\in \mathbb {R}^{m\times m}} \Vert AXB+CXD-F\Vert _\mathcal{F} , \end{aligned}$$ with A and C full column rank, B and D full row rank, F an $$n\times n$$ matrix of low rank, and $$\Vert \cdot \Vert _\mathcal{F}$$ the Frobenius norm. We derive a matrix-oriented implementation of LSQR, and devise an implementation of the truncation step that exploits the properties of the method. Experimental comparisons with the Conjugate Gradient method applied to the normal matrix equation and with a (new) sketched implementation of matrix LSQR illustrate the competitiveness of the proposed algorithm. We also explore the applicability of our method in the context of Kronecker-based Dictionary Learning, and devise a representation of the data that seems to be promising for classification purposes.},
  archive      = {J_COAP},
  author       = {Piccinini, Lorenzo and Simoncini, Valeria},
  doi          = {10.1007/s10589-024-00629-w},
  journal      = {Computational Optimization and Applications},
  month        = {6},
  number       = {2},
  pages        = {905--932},
  shortjournal = {Comput. Optim. Appl.},
  title        = {Truncated LSQR for matrix least squares problems},
  volume       = {91},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). The indefinite proximal gradient method. <em>COAP</em>, <em>91</em>(2), 861--903. (<a href='https://doi.org/10.1007/s10589-024-00604-5'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We introduce a monotone and a non-monotone variant of the proximal gradient method in which the quadratic term is diagonal but may be indefinite, and is safeguarded by a trust region. Our method is a special case of the proximal quasi-Newton trust-region method of Aravkin et al. (SIAM J Optim 32(2):900–929, 2022). We provide closed-form solution of the step computation in certain cases where the nonsmooth term is separable and the trust region is defined in the infinity norm, so that no iterative subproblem solver is required. Our analysis expands upon that of (Aravkin et al. in SIAM J Optim 32(2):900–929, 2022) by generalizing the trust-region approach to problems with bound constraints. We provide an efficient open-source implementation of our method, named TRDH, in the Julia language in which Hessians approximations are given by diagonal quasi-Newton updates. TRDH evaluates one standard proximal operator and one indefinite proximal operator per iteration. We also analyze and implement a variant named iTRDH that performs a single indefinite proximal operator evaluation per iteration. We establish that iTRDH enjoys the same asymptotic worst-case iteration complexity as TRDH. We report numerical experience on unconstrained and bound-constrained problems, where TRDH and iTRDH are used both as standalone and subproblem solvers. Our results illustrate that, as standalone solvers, TRDH and iTRDH improve upon the quadratic regularization method R2 of (Aravkin et al. in SIAM J Optim 32(2):900–929, 2022) but also sometimes upon their quasi-Newton trust-region method, referred to here as TR-R2, in terms of smooth objective value and gradient evaluations. On challenging nonnegative matrix factorization, binary classification and data fitting problems, TRDH and iTRDH used as subproblem solvers inside TR improve upon TR-R2 for at least one choice of diagonal approximation and memory parameter.},
  archive      = {J_COAP},
  author       = {Leconte, Geoffroy and Orban, Dominique},
  doi          = {10.1007/s10589-024-00604-5},
  journal      = {Computational Optimization and Applications},
  month        = {6},
  number       = {2},
  pages        = {861--903},
  shortjournal = {Comput. Optim. Appl.},
  title        = {The indefinite proximal gradient method},
  volume       = {91},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Off-the-grid regularisation for poisson inverse problems. <em>COAP</em>, <em>91</em>(2), 827--860. (<a href='https://doi.org/10.1007/s10589-025-00688-7'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Off-the-grid regularisation has been extensively employed over the last decade in the context of ill-posed inverse problems formulated in the continuous setting of the space of Radon measures $${{\mathcal {M}}(\Omega )}$$ . These approaches enjoy convexity and counteract the discretisation biases as well the numerical instabilities typical of their discrete counterparts. In the framework of sparse reconstruction of discrete point measures (sum of weighted Diracs), a Total Variation regularisation norm in $${{\mathcal {M}}(\Omega )}$$ is typically combined with an $$L^2$$ data term modelling additive Gaussian noise. To assess the framework of off-the-grid regularisation in the presence of signal-dependent Poisson noise, we consider in this work a variational model where Total Variation regularisation is coupled with a Kullback–Leibler data term under a non-negativity constraint. Analytically, we study the optimality conditions of the composite functional and analyse its dual problem. Then, we consider an homotopy strategy to select an optimal regularisation parameter and use it within a Sliding Frank-Wolfe algorithm. Several numerical experiments on both 1D/2D/3D simulated and real 3D fluorescent microscopy data are reported.},
  archive      = {J_COAP},
  author       = {Lazzaretti, Marta and Estatico, Claudio and Melero, Alejandro and Calatroni, Luca},
  doi          = {10.1007/s10589-025-00688-7},
  journal      = {Computational Optimization and Applications},
  month        = {6},
  number       = {2},
  pages        = {827--860},
  shortjournal = {Comput. Optim. Appl.},
  title        = {Off-the-grid regularisation for poisson inverse problems},
  volume       = {91},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Variational graph p-laplacian eigendecomposition under p-orthogonality constraints. <em>COAP</em>, <em>91</em>(2), 787--825. (<a href='https://doi.org/10.1007/s10589-024-00631-2'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The p-Laplacian is a non-linear generalization of the Laplace operator. In the graph context, its eigenfunctions are used for data clustering, spectral graph theory, dimensionality reduction and other problems, as non-linearity better captures the underlying geometry of the data. We formulate the graph p-Laplacian nonlinear eigenproblem as an optimization problem under p-orthogonality constraints. The problem of computing multiple eigenpairs of the graph p-Laplacian is then approached incrementally by minimizing the graph Rayleigh quotient under nonlinear constraints. A simple reformulation allows us to take advantage of linear constraints. We propose two different optimization algorithms to solve the variational problem. The first is a projected gradient descent on manifold, and the second is an Alternate Direction Method of Multipliers which leverages the scaling invariance of the graph Rayleigh quotient to solve a constrained minimization under p-orthogonality constraints. We demonstrate the effectiveness and accuracy of the proposed algorithms and compare them in terms of efficiency.},
  archive      = {J_COAP},
  author       = {Lanza, Alessandro and Morigi, Serena and Recupero, Giuseppe},
  doi          = {10.1007/s10589-024-00631-2},
  journal      = {Computational Optimization and Applications},
  month        = {6},
  number       = {2},
  pages        = {787--825},
  shortjournal = {Comput. Optim. Appl.},
  title        = {Variational graph p-laplacian eigendecomposition under p-orthogonality constraints},
  volume       = {91},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A scaled gradient projection method for the realization of the balancing principle in TGV-based image restoration. <em>COAP</em>, <em>91</em>(2), 759--785. (<a href='https://doi.org/10.1007/s10589-025-00659-y'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the last few years, Total Generalized Variation (TGV) regularization has proved to be a valuable tool to remove blur and noise from an image while avoiding the staircase effect typical of the Total Variation (TV) and preserving the sharp edges. The TGV-regularized model depends on two regularization parameters whose values must be appropriately selected to obtain good-quality restored images. In this work, we propose the use of the Balancing Principle (BP) to formulate the TGV-based image restoration problem as a constrained minimization problem whose objective is an implicit function of the two regularization parameters depending on the image to be restored. The values of the regularization parameters, and the corresponding restored image, satisfying the optimality condition of the formulated problem guarantee that the data fidelity and regularization terms are balanced. We introduce a Scaled Gradient Projection (SGP) method specifically tailored to the BP-based optimization problem and test its effectiveness against the fixed-point iteration schemes proposed in the literature. The numerical results performed on real-life images, affected by both Gaussian and Poisson noise, show that the proposed approach can effectively restore input images corrupted by several kinds of noise and outperform the fixed-point strategies for the realization of the Balancing Principle.},
  archive      = {J_COAP},
  author       = {Landi, Germana and Viola, Marco and Zama, Fabiana},
  doi          = {10.1007/s10589-025-00659-y},
  journal      = {Computational Optimization and Applications},
  month        = {6},
  number       = {2},
  pages        = {759--785},
  shortjournal = {Comput. Optim. Appl.},
  title        = {A scaled gradient projection method for the realization of the balancing principle in TGV-based image restoration},
  volume       = {91},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Spectral stochastic gradient method with additional sampling for finite and infinite sums. <em>COAP</em>, <em>91</em>(2), 717--758. (<a href='https://doi.org/10.1007/s10589-025-00664-1'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we propose a new stochastic gradient method for numerical minimization of finite sums. We also propose a modified version of this method applicable on more general problems referred to as infinite sum problems, where the objective function is in the form of mathematical expectation. The method is based on a strategy to exploit the effectiveness of the well-known Barzilai–Borwein (BB) rules or variants of these (BB-like) rules for updating the step length in the standard gradient method. The proposed method adapts the aforementioned strategy into the stochastic framework by exploiting the same Sample Average Approximations (SAA) estimator of the objective function for several iterations. Furthermore, the sample size is controlled by an additional sampling which also plays a role in accepting the proposed iterate point. Moreover, the number of “inner” iterations with the same sample is also controlled by an adaptive rule which prevents the method from getting stuck with the same estimator for too long. Convergence results are discussed for the finite and infinite sum version, for general and strongly convex objective functions. For the strongly convex case, we provide convergence rate and worst-case complexity analysis. Numerical experiments on well-known datasets for binary classifications show very promising performance of the method, without the need to provide special values for hyperparameters on which the method depends.},
  archive      = {J_COAP},
  author       = {Krklec Jerinkić, Nataša and Ruggiero, Valeria and Trombini, Ilaria},
  doi          = {10.1007/s10589-025-00664-1},
  journal      = {Computational Optimization and Applications},
  month        = {6},
  number       = {2},
  pages        = {717--758},
  shortjournal = {Comput. Optim. Appl.},
  title        = {Spectral stochastic gradient method with additional sampling for finite and infinite sums},
  volume       = {91},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Distributed inexact newton method with adaptive step sizes. <em>COAP</em>, <em>91</em>(2), 683--715. (<a href='https://doi.org/10.1007/s10589-025-00666-z'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider two formulations for distributed optimization wherein N nodes in a generic connected network solve a problem of common interest: distributed personalized optimization and consensus optimization. A new method termed DINAS (Distributed Inexact Newton method with Adaptive step size) is proposed. DINAS employs large adaptively computed step sizes, requires a reduced global parameters knowledge with respect to existing alternatives, and can operate without any local Hessian inverse calculations nor Hessian communications. When solving personalized distributed learning formulations, DINAS achieves quadratic convergence with respect to computational cost and linear convergence with respect to communication cost, the latter rate being independent of the local functions condition numbers or of the network topology. When solving consensus optimization problems, DINAS is shown to converge to the global solution. Extensive numerical experiments demonstrate significant improvements of DINAS over existing alternatives. As a result of independent interest, we provide for the first time convergence analysis of the Newton method with the adaptive Polyak’s step size when the Newton direction is computed inexactly in centralized environment.},
  archive      = {J_COAP},
  author       = {Jakovetić, Dušan and Krejić, Nataša and Malaspina, Greta},
  doi          = {10.1007/s10589-025-00666-z},
  journal      = {Computational Optimization and Applications},
  month        = {6},
  number       = {2},
  pages        = {683--715},
  shortjournal = {Comput. Optim. Appl.},
  title        = {Distributed inexact newton method with adaptive step sizes},
  volume       = {91},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Polynomial worst-case iteration complexity of quasi-newton primal-dual interior point algorithms for linear programming. <em>COAP</em>, <em>91</em>(2), 649--681. (<a href='https://doi.org/10.1007/s10589-024-00584-6'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Quasi-Newton methods are well known techniques for large-scale numerical optimization. They use an approximation of the Hessian in optimization problems or the Jacobian in system of nonlinear equations. In the Interior Point context, quasi-Newton algorithms compute low-rank updates of the matrix associated with the Newton systems, instead of computing it from scratch at every iteration. In this work, we show that a simplified quasi-Newton primal-dual interior point algorithm for linear programming, which alternates between Newton and quasi-Newton iterations, enjoys polynomial worst-case iteration complexity. Feasible and infeasible cases of the algorithm are considered and the most common neighborhoods of the central path are analyzed. To the best of our knowledge, this is the first attempt to deliver polynomial worst-case iteration complexity bounds for these methods. Unsurprisingly, the worst-case complexity results obtained when quasi-Newton directions are used are worse than their counterparts when Newton directions are employed. However, quasi-Newton updates are very attractive for large-scale optimization problems where the cost of factorizing the matrices is much higher than the cost of solving linear systems.},
  archive      = {J_COAP},
  author       = {Gondzio, Jacek and Sobral, Francisco N. C.},
  doi          = {10.1007/s10589-024-00584-6},
  journal      = {Computational Optimization and Applications},
  month        = {6},
  number       = {2},
  pages        = {649--681},
  shortjournal = {Comput. Optim. Appl.},
  title        = {Polynomial worst-case iteration complexity of quasi-newton primal-dual interior point algorithms for linear programming},
  volume       = {91},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Exploiting effective negative curvature directions via SYMMBK algorithm, in Newton–Krylov methods. <em>COAP</em>, <em>91</em>(2), 617--647. (<a href='https://doi.org/10.1007/s10589-025-00650-7'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper we consider the issue of computing negative curvature directions, for nonconvex functions, within Newton–Krylov methods for large scale unconstrained optimization. In the last decades this issue has been widely investigated in the literature, and different approaches have been proposed. We focus on the well known SYMMBK method introduced for solving large scale symmetric possibly indefinite linear systems (Bunch and Kaufman in Math Comput 31:163–179, 2003; Chandra in Conjugate gradient methods for partial differential equations, Yale University, New Haven, 1978; Conn et al. Trust-region methods. MPS-SIAM Series on Optimization, Philadelphia, 2000; HSL 2013: A collection of Fortran codes for large scale scientific computation. http://www.hsl.rl.ac.uk/ ), and show how to exploit it to yield an effective negative curvature direction in optimization frameworks. The distinguishing feature of our proposal is that the computation of negative curvatures is basically carried out as by–product of SYMMBK procedure, without storing no more than two additional vectors. Hence, no explicit matrix factorization or matrix storage is required. An extensive numerical experimentation has been performed on CUTEst problems; the obtained results have been analyzed also through novel profiles (Quality Profiles) which highlighted the good capability of the algorithms which use negative curvature directions to determine better local minimizers.},
  archive      = {J_COAP},
  author       = {Fasano, Giovanni and Piermarini, Christian and Roma, Massimo},
  doi          = {10.1007/s10589-025-00650-7},
  journal      = {Computational Optimization and Applications},
  month        = {6},
  number       = {2},
  pages        = {617--647},
  shortjournal = {Comput. Optim. Appl.},
  title        = {Exploiting effective negative curvature directions via SYMMBK algorithm, in Newton–Krylov methods},
  volume       = {91},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Proximal-stabilized semidefinite programming. <em>COAP</em>, <em>91</em>(2), 573--616. (<a href='https://doi.org/10.1007/s10589-024-00614-3'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A regularized version of the primal-dual Interior Point Method (IPM) for the solution of Semidefinite Programming Problems (SDPs) is presented in this paper. Leveraging on the proximal point method, a novel Proximal Stabilized Interior Point Method for SDP (PS-SDP-IPM) is introduced. The method is strongly supported by theoretical results concerning its convergence: the worst-case complexity result is established for the inner regularized infeasible inexact IPM solver. The new method demonstrates an increased robustness when dealing with problems characterized by ill-conditioning or linear dependence of the constraints without requiring any kind of pre-processing. Extensive numerical experience is reported to illustrate advantages of the proposed method when compared to the state-of-the-art solver.},
  archive      = {J_COAP},
  author       = {Cipolla, Stefano and Gondzio, Jacek},
  doi          = {10.1007/s10589-024-00614-3},
  journal      = {Computational Optimization and Applications},
  month        = {6},
  number       = {2},
  pages        = {573--616},
  shortjournal = {Comput. Optim. Appl.},
  title        = {Proximal-stabilized semidefinite programming},
  volume       = {91},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Distributed tikhonov regularization for ill-posed inverse problems from a bayesian perspective. <em>COAP</em>, <em>91</em>(2), 541--572. (<a href='https://doi.org/10.1007/s10589-025-00675-y'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, we exploit the similarities between Tikhonov regularization and Bayesian hierarchical models to propose a regularization scheme that acts like a distributed Tikhonov regularization where the amount of regularization varies from component to component, and a computationally efficient numerical scheme that is suitable for large-scale problems. In the standard formulation, Tikhonov regularization compensates for the inherent ill-conditioning of linear inverse problems by augmenting the data fidelity term measuring the mismatch between the data and the model output with a scaled penalty functional. The selection of the scaling of the penalty functional is the core problem in Tikhonov regularization. If an estimate of the amount of noise in the data is available, a popular way is to use the Morozov discrepancy principle, stating that the scaling parameter should be chosen so as to guarantee that the norm of the data fitting error is approximately equal to the norm of the noise in the data. A too small value of the regularization parameter would yield a solution that fits to the noise (too weak regularization) while a too large value would lead to an excessive penalization of the solution (too strong regularization). In many applications, it would be preferable to apply distributed regularization, replacing the regularization scalar by a vector valued parameter, so as to allow different regularization for different components of the unknown, or for groups of them. Distributed Tikhonov-inspired regularization is particularly well suited when the data have significantly different sensitivity to different components, or to promote sparsity of the solution. The numerical scheme that we propose, while exploiting the Bayesian interpretation of the inverse problem and identifying the Tikhonov regularization with the maximum a posteriori estimation, requires no statistical tools. A clever combination of numerical linear algebra and numerical optimization tools makes the scheme computationally efficient and suitable for problems where the matrix is not explicitly available. Moreover, in the case of underdetermined problems, passing through the adjoint formulation in data space may lead to substantial reduction in computational complexity.},
  archive      = {J_COAP},
  author       = {Calvetti, D. and Somersalo, E.},
  doi          = {10.1007/s10589-025-00675-y},
  journal      = {Computational Optimization and Applications},
  month        = {6},
  number       = {2},
  pages        = {541--572},
  shortjournal = {Comput. Optim. Appl.},
  title        = {Distributed tikhonov regularization for ill-posed inverse problems from a bayesian perspective},
  volume       = {91},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Projection free methods on product domains. <em>COAP</em>, <em>91</em>(2), 511--540. (<a href='https://doi.org/10.1007/s10589-024-00585-5'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Projection-free block-coordinate methods avoid high computational cost per iteration, and at the same time exploit the particular problem structure of product domains. Frank–Wolfe-like approaches rank among the most popular ones of this type. However, as observed in the literature, there was a gap between the classical Frank–Wolfe theory and the block-coordinate case, with no guarantees of linear convergence rates even for strongly convex objectives in the latter. Moreover, most of previous research concentrated on convex objectives. This study now deals also with the non-convex case and reduces above-mentioned theory gap, in combining a new, fully developed convergence theory with novel active set identification results which ensure that inherent sparsity of solutions can be exploited in an efficient way. Preliminary numerical experiments seem to justify our approach and also show promising results for obtaining global solutions in the non-convex case.},
  archive      = {J_COAP},
  author       = {Bomze, Immanuel and Rinaldi, Francesco and Zeffiro, Damiano},
  doi          = {10.1007/s10589-024-00585-5},
  journal      = {Computational Optimization and Applications},
  month        = {6},
  number       = {2},
  pages        = {511--540},
  shortjournal = {Comput. Optim. Appl.},
  title        = {Projection free methods on product domains},
  volume       = {91},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Safeguarded augmented lagrangian algorithms with scaled stopping criterion for the subproblems. <em>COAP</em>, <em>91</em>(2), 491--509. (<a href='https://doi.org/10.1007/s10589-024-00572-w'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {At each iteration of the safeguarded augmented Lagrangian algorithm Algencan, a bound-constrained subproblem consisting of the minimization of the Powell–Hestenes–Rockafellar augmented Lagrangian function is considered, for which an approximate minimizer with tolerance tending to zero is sought. More precisely, a point that satisfies a subproblem first-order necessary optimality condition with tolerance tending to zero is required. In this work, based on the success of scaled stopping criteria in constrained optimization, we propose a scaled stopping criterion for the subproblems of Algencan. The scaling is done with the maximum absolute value of the first-order Lagrange multipliers approximation, whenever it is larger than one. The difference between the convergence theory of the scaled and non-scaled versions of Algencan is discussed and extensive numerical experiments are provided.},
  archive      = {J_COAP},
  author       = {Birgin, E. G. and Haeser, G. and Martínez, J. M.},
  doi          = {10.1007/s10589-024-00572-w},
  journal      = {Computational Optimization and Applications},
  month        = {6},
  number       = {2},
  pages        = {491--509},
  shortjournal = {Comput. Optim. Appl.},
  title        = {Safeguarded augmented lagrangian algorithms with scaled stopping criterion for the subproblems},
  volume       = {91},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A general framework for whiteness-based parameters selection in variational models. <em>COAP</em>, <em>91</em>(2), 457--489. (<a href='https://doi.org/10.1007/s10589-024-00615-2'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this work, we extend the residual whiteness principle, originally proposed in (Lanza et al. in Electron Trans Numer Anal 53:329–352 2020) for the selection of a single regularization parameter in variational models for inverse problems under additive white noise corruption, to much broader scenarios. More specifically, we address the problem of estimating multiple parameters for imaging inverse problems subject to both white and non-white but whitenable noise corruptions, thus covering most of the application cases. The proposed parameter selection criterion, referred to as generalized whiteness principle, is formulated as a bilevel optimization problem. To circumvent the non-smoothness of the variational models typically employed in imaging problems—the non-smoothness representing a bottleneck in the bilevel set-up—we propose to adopt a derivative-free minimization algorithm for the solution of the designed bilevel problem. We refer to this novel numerical solution paradigm as bilevel derivative-free approach. Numerical tests highlight both the ability of the proposed generalized whiteness principle to effectively select multiple parameters and the significant advantages, in terms of computational cost, of the bilevel derivative-free numerical solution framework.},
  archive      = {J_COAP},
  author       = {Bevilacqua, Francesca and Lanza, Alessandro and Pragliola, Monica and Sgallari, Fiorella},
  doi          = {10.1007/s10589-024-00615-2},
  journal      = {Computational Optimization and Applications},
  month        = {6},
  number       = {2},
  pages        = {457--489},
  shortjournal = {Comput. Optim. Appl.},
  title        = {A general framework for whiteness-based parameters selection in variational models},
  volume       = {91},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Spectral analysis of block preconditioners for double saddle-point linear systems with application to PDE-constrained optimization. <em>COAP</em>, <em>91</em>(2), 423--455. (<a href='https://doi.org/10.1007/s10589-024-00623-2'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we describe and analyze the spectral properties of a symmetric positive definite inexact block preconditioner for a class of symmetric, double saddle-point linear systems. We develop a spectral analysis of the preconditioned matrix, showing that its eigenvalues can be described in terms of the roots of a cubic polynomial with real coefficients. We illustrate the efficiency of the proposed preconditioners, and verify the theoretical bounds, in solving large-scale PDE-constrained optimization problems.},
  archive      = {J_COAP},
  author       = {Bergamaschi, Luca and Martínez, Ángeles and Pearson, John W. and Potschka, Andreas},
  doi          = {10.1007/s10589-024-00623-2},
  journal      = {Computational Optimization and Applications},
  month        = {6},
  number       = {2},
  pages        = {423--455},
  shortjournal = {Comput. Optim. Appl.},
  title        = {Spectral analysis of block preconditioners for double saddle-point linear systems with application to PDE-constrained optimization},
  volume       = {91},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Strong global convergence properties of algorithms for nonlinear symmetric cone programming. <em>COAP</em>, <em>91</em>(2), 397--421. (<a href='https://doi.org/10.1007/s10589-024-00642-z'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Sequential optimality conditions have played a major role in establishing strong global convergence properties of numerical algorithms for many classes of optimization problems. In particular, the way complementarity is handled defines different optimality conditions and is fundamental to achieving a strong condition. Typically, one uses the inner product structure to measure complementarity, which provides a general approach to conic optimization problems, even in the infinite-dimensional case. In this paper we exploit the Jordan algebraic structure of symmetric cones to measure complementarity, resulting in a stronger sequential optimality condition related to the well-known complementary approximate Karush-Kuhn-Tucker conditions in standard nonlinear programming. Our results improve some known results in the setting of semidefinite programming and second-order cone programming in a unified framework. In particular, we obtain global convergence that are stronger than those known for augmented Lagrangian and interior point methods for general symmetric cones.},
  archive      = {J_COAP},
  author       = {Andreani, R. and Haeser, G. and Ramos, A. and Santos, D. O. and Secchin, L. D. and Serranoni, A.},
  doi          = {10.1007/s10589-024-00642-z},
  journal      = {Computational Optimization and Applications},
  month        = {6},
  number       = {2},
  pages        = {397--421},
  shortjournal = {Comput. Optim. Appl.},
  title        = {Strong global convergence properties of algorithms for nonlinear symmetric cone programming},
  volume       = {91},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A nested primal–dual iterated tikhonov method for regularized convex optimization. <em>COAP</em>, <em>91</em>(2), 357--395. (<a href='https://doi.org/10.1007/s10589-024-00613-4'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Proximal–gradient methods are widely employed tools in imaging that can be accelerated by adopting variable metrics and/or extrapolation steps. One crucial issue is the inexact computation of the proximal operator, often implemented through a nested primal–dual solver, which represents the main computational bottleneck whenever an increasing accuracy in the computation is required. In this paper, we propose a nested primal–dual method for the efficient solution of regularized convex optimization problems. Our proposed method approximates a variable metric proximal–gradient step with extrapolation by performing a prefixed number of primal–dual iterates, while adjusting the steplength parameter through an appropriate backtracking procedure. Choosing a prefixed number of inner iterations allows the algorithm to keep the computational cost per iteration low. We prove the convergence of the iterates sequence towards a solution of the problem, under a relaxed monotonicity assumption on the scaling matrices and a shrinking condition on the extrapolation parameters. Furthermore, we investigate the numerical performance of our proposed method by equipping it with a scaling matrix inspired by the Iterated Tikhonov method. The numerical results show that the combination of such scaling matrices and Nesterov-like extrapolation parameters yields an effective acceleration towards the solution of the problem.},
  archive      = {J_COAP},
  author       = {Aleotti, Stefano and Bonettini, Silvia and Donatelli, Marco and Prato, Marco and Rebegoldi, Simone},
  doi          = {10.1007/s10589-024-00613-4},
  journal      = {Computational Optimization and Applications},
  month        = {6},
  number       = {2},
  pages        = {357--395},
  shortjournal = {Comput. Optim. Appl.},
  title        = {A nested primal–dual iterated tikhonov method for regularized convex optimization},
  volume       = {91},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Preface: New trends in large scale optimization. <em>COAP</em>, <em>91</em>(2), 351--356. (<a href='https://doi.org/10.1007/s10589-025-00689-6'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_COAP},
  author       = {Bellavia, Stefania and De Simone, Valentina and Morini, Benedetta},
  doi          = {10.1007/s10589-025-00689-6},
  journal      = {Computational Optimization and Applications},
  month        = {6},
  number       = {2},
  pages        = {351--356},
  shortjournal = {Comput. Optim. Appl.},
  title        = {Preface: New trends in large scale optimization},
  volume       = {91},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). On the well-posedness of tracking dirichlet data for bernoulli free boundary problems. <em>COAP</em>, <em>91</em>(1), 311--349. (<a href='https://doi.org/10.1007/s10589-025-00662-3'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The aim of this paper is to study the shape optimization method for solving the Bernoulli free boundary problem, a well-known ill-posed problem that seeks the unknown free boundary through Cauchy data. Different formulations have been proposed in the literature that differ in the choice of the objective functional. Specifically, it was shown respectively in Eppler and Harbrecht (SIAM J Control Optim 48:2901–2916, 2010, J Optim Theory Appl 145:17–35, 2010) that tracking Neumann data is well-posed but tracking Dirichlet data is not. In this paper we propose a new well-posed objective functional that tracks Dirichlet data at the free boundary. By calculating the Euler derivative and the shape Hessian of the objective functional we show that the new formulation is well-posed, i.e., the shape Hessian is coercive at the minima. The coercivity of the shape Hessian may ensure the existence of optimal solutions for the nonlinear Ritz–Galerkin approximation method and its convergence, thus is crucial for the formulation. As a summary, we conclude that tracking Dirichlet or Neumann data in their energy norm is not sufficient, but tracking them in a half an order higher norm will be well-posed. To support our theoretical results we carry out extensive numerical experiments.},
  archive      = {J_COAP},
  author       = {Gong, Wei and Liu, Le},
  doi          = {10.1007/s10589-025-00662-3},
  journal      = {Computational Optimization and Applications},
  month        = {5},
  number       = {1},
  pages        = {311--349},
  shortjournal = {Comput. Optim. Appl.},
  title        = {On the well-posedness of tracking dirichlet data for bernoulli free boundary problems},
  volume       = {91},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Speeding up L-BFGS by direct approximation of the inverse hessian matrix. <em>COAP</em>, <em>91</em>(1), 283--310. (<a href='https://doi.org/10.1007/s10589-025-00665-0'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {L-BFGS is one of the widely used quasi-Newton methods. Instead of explicitly storing an approximation H of the inverse Hessian, L-BFGS keeps a limited number of vectors that can be used for computing the product of H by the gradient. However, this computation is sequential, each step depending on the outcome of the previous step. To solve this problem, we propose the Direct L-BFGS (DirL-BFGS) method that, seeing H as a linear operator, directly stores a low-rank plus diagonal (LRPD) representation of H. Employing the LRPD representation enables us to leverage the benefits of vector processing, leading to accelerating and parallelizing the calculations in the form of single instruction, multiple data. We evaluate our proposed method on different quadratic optimization problems and several regression and classification tasks with neural networks. Numerical results show that DirL-BFGS is faster overall than L-BFGS.},
  archive      = {J_COAP},
  author       = {Sadeghi-Lotfabadi, Ashkan and Ghiasi-Shirazi, Kamaledin},
  doi          = {10.1007/s10589-025-00665-0},
  journal      = {Computational Optimization and Applications},
  month        = {5},
  number       = {1},
  pages        = {283--310},
  shortjournal = {Comput. Optim. Appl.},
  title        = {Speeding up L-BFGS by direct approximation of the inverse hessian matrix},
  volume       = {91},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). The $$\omega $$ -condition number: Applications to preconditioning and low rank generalized jacobian updating. <em>COAP</em>, <em>91</em>(1), 235--282. (<a href='https://doi.org/10.1007/s10589-025-00669-w'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Preconditioning is essential in iterative methods for solving linear systems. It is also the implicit objective in updating approximations of Jacobians in optimization methods, e.g., in quasi-Newton methods. We study a nonclassic matrix condition number, the $$\omega $$ -condition number, $$\omega $$ for short. $$\omega $$ is the ratio of: the arithmetic and geometric means of the singular values, rather than the largest and smallest for the classical $$\kappa $$ -condition number. The simple functions in $$\omega $$ allow one to exploit first order optimality conditions. We use this fact to derive explicit formulae for (i) $$\omega $$ -optimal low rank updating of generalized Jacobians arising in the context of nonsmooth Newton methods; and (ii) $$\omega $$ -optimal preconditioners of special structure for iterative methods for linear systems. In the latter context, we analyze the benefits of $$\omega $$ for (a) improving the clustering of eigenvalues; (b) reducing the number of iterations; and (c) estimating the actual condition of a linear system. Moreover we show strong theoretical connections between the $$\omega $$ -optimal preconditioners and incomplete Cholesky factorizations, and highlight the misleading effects arising from the inverse invariance of $$\kappa $$ . Our results confirm the efficacy of using the $$\omega $$ -condition number compared to the $$\kappa $$ -condition number.},
  archive      = {J_COAP},
  author       = {Jung, Woosuk L. and Torregrosa-Belén, David and Wolkowicz, Henry},
  doi          = {10.1007/s10589-025-00669-w},
  journal      = {Computational Optimization and Applications},
  month        = {5},
  number       = {1},
  pages        = {235--282},
  shortjournal = {Comput. Optim. Appl.},
  title        = {The $$\omega $$ -condition number: Applications to preconditioning and low rank generalized jacobian updating},
  volume       = {91},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Inertial accelerated stochastic mirror descent for large-scale generalized tensor CP decomposition. <em>COAP</em>, <em>91</em>(1), 201--233. (<a href='https://doi.org/10.1007/s10589-025-00668-x'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The majority of classic tensor CP decomposition models are designed for squared loss, utilizing Euclidean distance as a local proximal term. However, the Euclidean distance is unsuitable for the generalized loss function applicable to diverse types of real-world data, such as integer and binary data. Consequently, algorithms developed under the squared loss are not easily adaptable to handle these generalized losses, partially due to the absence of the gradient Lipschitz continuity. This paper explores generalized tensor CP decomposition, employing the Bregman distance as the proximal term and introducing an inertial accelerated block randomized stochastic mirror descent algorithm (iTableSMD). Within a broader multi-block variance reduction and inertial acceleration framework, we demonstrate the sublinear convergence rate for the subsequential sequence produced by the iTableSMD algorithm. We further show that iTableSMD requires at most $$\mathcal {O}(\varepsilon ^{-2})$$ iterations in expectation to attain an $$\varepsilon $$ -stationary point and establish the global convergence of the sequence. Numerical experiments on real datasets demonstrate that our proposed algorithm is efficient and achieves better performance than the existing state-of-the-art methods.},
  archive      = {J_COAP},
  author       = {Liu, Zehui and Wang, Qingsong and Cui, Chunfeng and Xia, Yong},
  doi          = {10.1007/s10589-025-00668-x},
  journal      = {Computational Optimization and Applications},
  month        = {5},
  number       = {1},
  pages        = {201--233},
  shortjournal = {Comput. Optim. Appl.},
  title        = {Inertial accelerated stochastic mirror descent for large-scale generalized tensor CP decomposition},
  volume       = {91},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). On a family of relaxed gradient descent methods for strictly convex quadratic minimization. <em>COAP</em>, <em>91</em>(1), 173--200. (<a href='https://doi.org/10.1007/s10589-025-00670-3'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper studies the convergence properties of a family of Relaxed $$\ell $$ -Minimal Gradient Descent methods for quadratic optimization; the family includes the omnipresent Steepest Descent method, as well as the Minimal Gradient method. Simple proofs are provided that show, in an appropriately chosen norm, the gradient and the distance of the iterates from optimality converge linearly, for all members of the family. Moreover, the function values decrease linearly, and iteration complexity results are provided. All theoretical results hold when (fixed) relaxation is employed. It is also shown that, given a fixed overhead and storage budget, every Relaxed $$\ell $$ -Minimal Gradient Descent method can be implemented using exactly one matrix vector product. Numerical experiments are presented that illustrate the benefits of relaxation across the family.},
  archive      = {J_COAP},
  author       = {MacDonald, Liam and Murray, Rua and Tappenden, Rachael},
  doi          = {10.1007/s10589-025-00670-3},
  journal      = {Computational Optimization and Applications},
  month        = {5},
  number       = {1},
  pages        = {173--200},
  shortjournal = {Comput. Optim. Appl.},
  title        = {On a family of relaxed gradient descent methods for strictly convex quadratic minimization},
  volume       = {91},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Finding search directions in quasi-newton methods for minimizing a quadratic function subject to uncertainty. <em>COAP</em>, <em>91</em>(1), 145--171. (<a href='https://doi.org/10.1007/s10589-025-00661-4'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We investigate quasi-Newton methods for minimizing a strongly convex quadratic function which is subject to errors in the evaluation of the gradients. In particular, we focus on computing search directions for quasi-Newton methods that all give identical behavior in exact arithmetic, generating minimizers of Krylov subspaces of increasing dimensions, thereby having finite termination. The BFGS quasi-Newton method may be seen as an ideal method in exact arithmetic and is empirically known to behave very well on a quadratic problem subject to small errors. We investigate large-error scenarios, in which the expected behavior is not so clear. We consider memoryless methods that are less expensive than the BFGS method, in that they generate low-rank quasi-Newton matrices that differ from the identity by a symmetric matrix of rank two. In addition, a more advanced model for generating the search directions is proposed, based on solving a chance-constrained optimization problem. Our numerical results indicate that for large errors, such a low-rank memoryless quasi-Newton method may perform better than a BFGS method. In addition, the results indicate a potential edge by including the chance-constrained model in the memoryless quasi-Newton method.},
  archive      = {J_COAP},
  author       = {Peng, Shen and Canessa, Gianpiero and Ek, David and Forsgren, Anders},
  doi          = {10.1007/s10589-025-00661-4},
  journal      = {Computational Optimization and Applications},
  month        = {5},
  number       = {1},
  pages        = {145--171},
  shortjournal = {Comput. Optim. Appl.},
  title        = {Finding search directions in quasi-newton methods for minimizing a quadratic function subject to uncertainty},
  volume       = {91},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Quadratic convex reformulations for a class of complex quadratic programming problems. <em>COAP</em>, <em>91</em>(1), 125--144. (<a href='https://doi.org/10.1007/s10589-025-00672-1'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We investigate a class of complex quadratic programming problems characterized by unit-modulus and discrete argument constraints. This problem can be reformulated as a mixed-integer quadratic programming problem, which could be addressed using a commercial solver such as Gurobi. However, the solver’s efficiency is often unsatisfying if the problem formulation is inadequately designed. In this paper, we introduce several quadratic convex reformulations aimed at enhancing the solver’s performance. We extend the classical diagonal perturbation-based reformulation technique to this problem. Additionally, by leveraging the unique structure of the problem, we derive a new quadratic convex reformulation that provides a tighter continuous relaxation compared to the diagonal perturbation-based approach. The numerical tests on random instances and the unimodular code design problem demonstrate the superiority of the newly proposed reformulation.},
  archive      = {J_COAP},
  author       = {Lu, Cheng and Kang, Gaojian and Qu, Guangtai and Deng, Zhibin},
  doi          = {10.1007/s10589-025-00672-1},
  journal      = {Computational Optimization and Applications},
  month        = {5},
  number       = {1},
  pages        = {125--144},
  shortjournal = {Comput. Optim. Appl.},
  title        = {Quadratic convex reformulations for a class of complex quadratic programming problems},
  volume       = {91},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). On a globally convergent semismooth* newton method in nonsmooth nonconvex optimization. <em>COAP</em>, <em>91</em>(1), 67--124. (<a href='https://doi.org/10.1007/s10589-025-00658-z'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper we present GSSN, a globalized SCD semismooth $$^{*}$$ Newton method for solving nonsmooth nonconvex optimization problems. The global convergence properties of the method are ensured by the proximal gradient method, whereas locally superlinear convergence is established via the SCD semismooth $$^{*}$$ Newton method under quite weak assumptions. The Newton direction is based on the SC (subspace containing) derivative of the subdifferential mapping and can be computed by the (approximate) solution of an equality-constrained quadratic program. Special attention is given to the efficient numerical implementation of the overall method.},
  archive      = {J_COAP},
  author       = {Gfrerer, Helmut},
  doi          = {10.1007/s10589-025-00658-z},
  journal      = {Computational Optimization and Applications},
  month        = {5},
  number       = {1},
  pages        = {67--124},
  shortjournal = {Comput. Optim. Appl.},
  title        = {On a globally convergent semismooth* newton method in nonsmooth nonconvex optimization},
  volume       = {91},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Proximal gradient method for convex multiobjective optimization problems without lipschitz continuous gradients. <em>COAP</em>, <em>91</em>(1), 27--66. (<a href='https://doi.org/10.1007/s10589-025-00663-2'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper analyzes a proximal gradient method for nondifferentiable convex multiobjective optimization problems, where the components of the objective function are the sum of a proper lower semicontinuous function and a continuously differentiable function. By adopting a typical line search procedure, it is found that without a Lipschitz continuity of the gradients of the smooth part of the objective function, the proposed method is able to generate sequences that converge to weakly Pareto optimal points. The convergence rate of the method is found to be $$\mathcal {O}(1/k)$$ . Further, if the smooth component in the objective function is strongly convex, then the convergence rate is $$\mathcal {O}(r^k)$$ for some $$r\in (0,1)$$ . Moreover, in the absence of a strong convexity assumption, we also consider the accelerated version of the proposed approach based on the Nesterov step strategy. We obtain the improved convergence rate of $$\mathcal {O}(1/k^2)$$ , which is measured by a merit function. Numerical implementation strategies and performance profiles of the proposed methods on the considered problem involving $$\ell _1$$ -norm and indicator function are also provided.},
  archive      = {J_COAP},
  author       = {Zhao, Xiaopeng and Raushan, Ravi and Ghosh, Debdas and Yao, Jen-Chih and Qi, Min},
  doi          = {10.1007/s10589-025-00663-2},
  journal      = {Computational Optimization and Applications},
  month        = {5},
  number       = {1},
  pages        = {27--66},
  shortjournal = {Comput. Optim. Appl.},
  title        = {Proximal gradient method for convex multiobjective optimization problems without lipschitz continuous gradients},
  volume       = {91},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A symmetric version of the generalized chambolle-pock-he-yuan method for saddle point problems. <em>COAP</em>, <em>91</em>(1), 1--26. (<a href='https://doi.org/10.1007/s10589-025-00671-2'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A primal-dual method for solving convex-concave saddle point problems was initially proposed by Chambolle and Pock, and its convergence was first proved by He and Yuan later. This primal-dual method (“CPHY" for short) reduces to the Arrow–Hurwicz method (as well as the primal-dual hybrid gradient method) when the combination parameter $$\theta =0$$ , and to a special case of the proximal point algorithm when $$\theta =1$$ , both of which have been well-studied. However, for $$\theta \in (0,1)$$ , some theoretical aspects are not yet well-understood, particularly regarding the convergence behavior without imposing strong assumptions. Additionally, although saddle point problems inherently exhibit symmetry between primal and dual variables, the CPHY does not fully exploit this symmetry, as only one variable is updated using an extrapolated step. In this work, we propose a symmetric version of the CPHY by incorporating both symmetry and extrapolation techniques. The resulting algorithm guarantees convergence for $$\theta \in (-1,1)$$ and ensures symmetric updates for both primal and dual variables. Numerical experiments on LASSO, TV image inpainting, and graph cuts demonstrate the algorithm’s improved efficiency.},
  archive      = {J_COAP},
  author       = {Ma, Feng and Li, Si and Zhang, Xiayang},
  doi          = {10.1007/s10589-025-00671-2},
  journal      = {Computational Optimization and Applications},
  month        = {5},
  number       = {1},
  pages        = {1--26},
  shortjournal = {Comput. Optim. Appl.},
  title        = {A symmetric version of the generalized chambolle-pock-he-yuan method for saddle point problems},
  volume       = {91},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). An arc-search interior-point algorithm for nonlinear constrained optimization. <em>COAP</em>, <em>90</em>(3), 969--995. (<a href='https://doi.org/10.1007/s10589-025-00648-1'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper proposes a new arc-search interior-point algorithm for the nonlinear constrained optimization problem. The proposed algorithm uses the second-order derivatives to construct a search arc that approaches the optimizer. Because the arc stays in the interior set longer than any straight line, it is expected that the scheme will generate a better new iterate than a line search method. The computation of the second-order derivatives requires to solve the second linear system of equations, but the coefficient matrix of the second linear system of equations is the same as the first linear system of equations. Therefore, the matrix decomposition obtained while solving the first linear system of equations can be reused. In addition, most elements of the right-hand side vector of the second linear system of equations are already computed when the coefficient matrix is assembled. Therefore, the computation cost for solving the second linear system of equations is insignificant and the benefit of having a better search scheme is well justified. The convergence of the proposed algorithm is established. Some preliminary test results are reported to demonstrate the merit of the proposed algorithm.},
  archive      = {J_COAP},
  author       = {Yang, Yaguang},
  doi          = {10.1007/s10589-025-00648-1},
  journal      = {Computational Optimization and Applications},
  month        = {4},
  number       = {3},
  pages        = {969--995},
  shortjournal = {Comput. Optim. Appl.},
  title        = {An arc-search interior-point algorithm for nonlinear constrained optimization},
  volume       = {90},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Improving the stochastically controlled stochastic gradient method by the bandwidth-based stepsize. <em>COAP</em>, <em>90</em>(3), 941--968. (<a href='https://doi.org/10.1007/s10589-025-00651-6'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Stepsize plays an important role in the stochastic gradient method. The bandwidth-based stepsize allows us to adjust the stepsize within a banded region determined by some boundary functions. Based on the bandwidth-based stepsize, we propose a new method, namely SCSG-BD, for smooth non-convex finite-sum optimization problems. For the boundary functions 1/t, $$1/(t\log (t + 1))$$ and $$1/t^p$$ ( $$p\in (0,1)$$ ), SCSG-BD converges sublinearly to a stationary point at a faster rate than the stochastically controlled stochastic gradient (SCSG) method under certain conditions. Moreover, SCSG-BD is able to converge linearly to the solution if the objective function satisfies the Polyak–Łojasiewicz condition. We also introduce the 1/t-Barzilai–Borwein stepsize for practical computation. Numerical experiments demonstrate that SCSG-BD performs better than SCSG and its variants.},
  archive      = {J_COAP},
  author       = {Liu, Chenchen and Huang, Yakui and Wang, Dan},
  doi          = {10.1007/s10589-025-00651-6},
  journal      = {Computational Optimization and Applications},
  month        = {4},
  number       = {3},
  pages        = {941--968},
  shortjournal = {Comput. Optim. Appl.},
  title        = {Improving the stochastically controlled stochastic gradient method by the bandwidth-based stepsize},
  volume       = {90},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A second-order sequential optimality condition for nonlinear second-order cone programming problems. <em>COAP</em>, <em>90</em>(3), 911--939. (<a href='https://doi.org/10.1007/s10589-025-00649-0'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the last two decades, the sequential optimality conditions, which do not require constraint qualifications and allow improvement on the convergence assumptions of algorithms, had been considered in the literature. It includes the work by Andreani et al. (IMA J Numer Anal 37:1902–1929, 2017), with a sequential optimality condition for nonlinear programming, that uses the second-order information of the problem. More recently, Fukuda et al. (Set-Valued Var Anal 31:15, 2023) analyzed the conditions that use second-order information, in particular for nonlinear second-order cone programming problems (SOCP). However, such optimality conditions were not defined explicitly. In this paper, we propose an explicit definition of approximate-Karush-Kuhn-Tucker 2 (AKKT2) and complementary-AKKT2 (CAKKT2) conditions for SOCPs. We prove that the proposed AKKT2/CAKKT2 conditions are satisfied at local optimal points of the SOCP without any constraint qualification. We also present two algorithms that are based on augmented Lagrangian and sequential quadratic programming methods and show their global convergence to points satisfying the proposed conditions.},
  archive      = {J_COAP},
  author       = {Fukuda, Ellen H. and Okabe, Kosuke},
  doi          = {10.1007/s10589-025-00649-0},
  journal      = {Computational Optimization and Applications},
  month        = {4},
  number       = {3},
  pages        = {911--939},
  shortjournal = {Comput. Optim. Appl.},
  title        = {A second-order sequential optimality condition for nonlinear second-order cone programming problems},
  volume       = {90},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Understanding the Douglas–Rachford splitting method through the lenses of moreau-type envelopes. <em>COAP</em>, <em>90</em>(3), 881--910. (<a href='https://doi.org/10.1007/s10589-024-00646-9'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We analyze the Douglas–Rachford splitting method for weakly convex optimization problems, by the token of the Douglas–Rachford envelope, a merit function akin to the Moreau envelope. First, we use epi-convergence techniques to show that this artifact approximates the original objective function via epigraphs. Secondly, we present how global convergence and local linear convergence rates for Douglas–Rachford splitting can be obtained using such envelope, under mild regularity assumptions. The keystone of the convergence analysis is the fact that the Douglas–Rachford envelope satisfies a sufficient descent inequality alongside the generated sequence, a feature that allows us to use arguments usually employed for descent methods. We report numerical experiments that use weakly convex penalty functions, which are comparable with the known behavior of the method in the convex case.},
  archive      = {J_COAP},
  author       = {Atenas, Felipe},
  doi          = {10.1007/s10589-024-00646-9},
  journal      = {Computational Optimization and Applications},
  month        = {4},
  number       = {3},
  pages        = {881--910},
  shortjournal = {Comput. Optim. Appl.},
  title        = {Understanding the Douglas–Rachford splitting method through the lenses of moreau-type envelopes},
  volume       = {90},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A convex combined symmetric alternating direction method of multipliers for separable optimization. <em>COAP</em>, <em>90</em>(3), 839--880. (<a href='https://doi.org/10.1007/s10589-025-00647-2'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Alternating Direction Method of Multipliers (ADMM) is a powerful first-order method used in many practical separable optimization problems. In this paper, we propose a new variant of the symmetric ADMM, called the Convex Combined Symmetric ADMM (CcS-ADMM), by integrating a convex combination technique. CcS-ADMM retains all the favorable features of ADMM, including the ability to take full advantage of problem structures and global convergence under relaxed parameter conditions. Furthermore, using the moderate assumptions and primal-dual gap, we analyze the convergence and the O(1/N) ergodic convergence rate of the algorithm with convex setting. Additionally, we propose the convergence of the CcS-ADMM with nonconvex setting in Euclidean space under so called Kurdyka–Lojasiewicz property and some widely used assumptions, and we establish the pointwise iteration-complexity of CcS-ADMM with respect to the augmented Lagrangian function and the primal-dual residuals. Finally, we present the results from preliminary numerical experiments to demonstrate the performance of the proposed algorithms.},
  archive      = {J_COAP},
  author       = {Wang, Xiaoquan and Shao, Hu and Wu, Ting},
  doi          = {10.1007/s10589-025-00647-2},
  journal      = {Computational Optimization and Applications},
  month        = {4},
  number       = {3},
  pages        = {839--880},
  shortjournal = {Comput. Optim. Appl.},
  title        = {A convex combined symmetric alternating direction method of multipliers for separable optimization},
  volume       = {90},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Regularized methods via cubic model subspace minimization for nonconvex optimization. <em>COAP</em>, <em>90</em>(3), 801--837. (<a href='https://doi.org/10.1007/s10589-025-00655-2'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Adaptive cubic regularization methods for solving nonconvex problems need the efficient computation of the trial step, involving the minimization of a cubic model. We propose a new approach in which this model is minimized in a low dimensional subspace that, in contrast to classic approaches, is reused for a number of iterations. Whenever the trial step produced by the low-dimensional minimization process is unsatisfactory, we employ a regularized Newton step whose regularization parameter is a by-product of the model minimization over the low-dimensional subspace. We show that the worst-case complexity of classic cubic regularized methods is preserved, despite the possible regularized Newton steps. We focus on the large class of problems for which (sparse) direct linear system solvers are available and provide several experimental results showing the very large gains of our new approach when compared to standard implementations of adaptive cubic regularization methods based on direct linear solvers. Our first choice as projection space for the low-dimensional model minimization is the polynomial Krylov subspace; nonetheless, we also explore the use of rational Krylov subspaces in case where the polynomial ones lead to less competitive numerical results.},
  archive      = {J_COAP},
  author       = {Bellavia, Stefania and Palitta, Davide and Porcelli, Margherita and Simoncini, Valeria},
  doi          = {10.1007/s10589-025-00655-2},
  journal      = {Computational Optimization and Applications},
  month        = {4},
  number       = {3},
  pages        = {801--837},
  shortjournal = {Comput. Optim. Appl.},
  title        = {Regularized methods via cubic model subspace minimization for nonconvex optimization},
  volume       = {90},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). On the convergence of the gradient descent method with stochastic fixed-point rounding errors under the polyak–Łojasiewicz inequality. <em>COAP</em>, <em>90</em>(3), 753--799. (<a href='https://doi.org/10.1007/s10589-025-00656-1'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the training of neural networks with low-precision computation and fixed-point arithmetic, rounding errors often cause stagnation or are detrimental to the convergence of the optimizers. This study provides insights into the choice of appropriate stochastic rounding strategies to mitigate the adverse impact of roundoff errors on the convergence of the gradient descent method, for problems satisfying the Polyak–Łojasiewicz inequality. Within this context, we show that a biased stochastic rounding strategy may be even beneficial in so far as it eliminates the vanishing gradient problem and forces the expected roundoff error in a descent direction. Furthermore, we obtain a bound on the convergence rate that is stricter than the one achieved by unbiased stochastic rounding. The theoretical analysis is validated by comparing the performances of various rounding strategies when optimizing several examples using low-precision fixed-point arithmetic.},
  archive      = {J_COAP},
  author       = {Xia, Lu and Massei, Stefano and Hochstenbach, Michiel E.},
  doi          = {10.1007/s10589-025-00656-1},
  journal      = {Computational Optimization and Applications},
  month        = {4},
  number       = {3},
  pages        = {753--799},
  shortjournal = {Comput. Optim. Appl.},
  title        = {On the convergence of the gradient descent method with stochastic fixed-point rounding errors under the polyak–Łojasiewicz inequality},
  volume       = {90},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). All saddle points for polynomial optimization. <em>COAP</em>, <em>90</em>(3), 721--752. (<a href='https://doi.org/10.1007/s10589-025-00657-0'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we study how to compute all saddle points for the constrained and unconstrained polynomial optimization, respectively. For the constrained polynomial optimization, a scalar-type semidefinite relaxation algorithm is proposed based on the Karush-Kuhn-Tucker conditions. While for the unconstrained polynomial optimization, a matrix-type semidefinite relaxation algorithm is proposed based on the second-order optimality conditions. Both algorithms can detect the nonexistence of saddle points or find all of them if there are finitely many ones. The finite convergence of the algorithms can also be obtained under some genericity conditions.},
  archive      = {J_COAP},
  author       = {Zhou, Anwa and Yin, Shiqian and Fan, Jinyan},
  doi          = {10.1007/s10589-025-00657-0},
  journal      = {Computational Optimization and Applications},
  month        = {4},
  number       = {3},
  pages        = {721--752},
  shortjournal = {Comput. Optim. Appl.},
  title        = {All saddle points for polynomial optimization},
  volume       = {90},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). On the use of restriction of the right-hand side in spatial branch-and-bound algorithms to ensure termination. <em>COAP</em>, <em>90</em>(3), 691--720. (<a href='https://doi.org/10.1007/s10589-025-00652-5'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Spatial branch-and-bound algorithms for global minimization of non-convex problems require both lower and upper bounding procedures that finally converge to a globally optimal value in order to ensure termination of these methods. Whereas convergence of lower bounds is commonly guaranteed for standard approaches in the literature, this does not always hold for upper bounds. For this reason, different so-called convergent upper bounding procedures are proposed. These methods are not always used in practice, possibly due to their additional complexity or possibly due to increasing runtimes on average problems. For that reason, in this article we propose a refinement of classical branch-and-bound methods that is simple to implement and comes with marginal overhead. We prove that this small improvement already leads to convergent upper bounds, and thus show that termination of spatial branch-and-bound methods is ensured under mild assumptions.},
  archive      = {J_COAP},
  author       = {Kirst, Peter and Füllner, Christian},
  doi          = {10.1007/s10589-025-00652-5},
  journal      = {Computational Optimization and Applications},
  month        = {4},
  number       = {3},
  pages        = {691--720},
  shortjournal = {Comput. Optim. Appl.},
  title        = {On the use of restriction of the right-hand side in spatial branch-and-bound algorithms to ensure termination},
  volume       = {90},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Parabolic optimal control problems with combinatorial switching constraints, part III: Branch-and-bound algorithm. <em>COAP</em>, <em>90</em>(3), 649--689. (<a href='https://doi.org/10.1007/s10589-025-00654-3'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present a branch-and-bound algorithm for globally solving parabolic optimal control problems with binary switches that have bounded variation and possibly need to satisfy further combinatorial constraints. More precisely, for a given tolerance $$\varepsilon >0$$ , we show how to compute in finite time an $$\varepsilon $$ -optimal solution in function space, independently of any prior discretization. The main ingredients in our approach are an appropriate branching strategy in infinite dimension, an a posteriori error estimation in order to obtain safe dual bounds, and an adaptive refinement strategy in order to allow arbitrary switching points in the limit. The performance of our approach is demonstrated by extensive experimental results.},
  archive      = {J_COAP},
  author       = {Buchheim, Christoph and Grütering, Alexandra and Meyer, Christian},
  doi          = {10.1007/s10589-025-00654-3},
  journal      = {Computational Optimization and Applications},
  month        = {4},
  number       = {3},
  pages        = {649--689},
  shortjournal = {Comput. Optim. Appl.},
  title        = {Parabolic optimal control problems with combinatorial switching constraints, part III: Branch-and-bound algorithm},
  volume       = {90},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Cardinality constrained mean-variance portfolios: A penalty decomposition algorithm. <em>COAP</em>, <em>90</em>(3), 631--648. (<a href='https://doi.org/10.1007/s10589-025-00653-4'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The cardinality-constrained mean-variance portfolio problem has garnered significant attention within contemporary finance due to its potential for achieving low risk while effectively managing transaction costs. Instead of solving this problem directly, many existing methods rely on regularization and approximation techniques, which hinder investors’ ability to precisely specify a portfolio’s desired cardinality level. Moreover, these approaches typically include more hyper-parameters and increase the problem’s dimensionality. To address these challenges, we propose a customized penalty decomposition algorithm. We demonstrate that this algorithm not only does it converge to a local minimizer of the cardinality-constrained mean-variance portfolio problem, but is also computationally efficient. Our approach leverages a sequence of penalty subproblems, each tackled using Block Coordinate Descent (BCD). We show that the steps within BCD yield closed-form solutions, allowing us to identify a saddle point of the penalty subproblems. Finally, by applying our penalty decomposition algorithm to real-world datasets, we highlight its efficiency and its superiority over state-of-the-art methods across several performance metrics.},
  archive      = {J_COAP},
  author       = {Mousavi, Ahmad and Michailidis, George},
  doi          = {10.1007/s10589-025-00653-4},
  journal      = {Computational Optimization and Applications},
  month        = {4},
  number       = {3},
  pages        = {631--648},
  shortjournal = {Comput. Optim. Appl.},
  title        = {Cardinality constrained mean-variance portfolios: A penalty decomposition algorithm},
  volume       = {90},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Inter-DS: A cost saving algorithm for expensive constrained multi-fidelity blackbox optimization. <em>COAP</em>, <em>90</em>(3), 607--629. (<a href='https://doi.org/10.1007/s10589-024-00645-w'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This work introduces Inter-DS, a blackbox optimization algorithmic framework for computationally expensive constrained multi-fidelity problems. When applying a direct search method to such problems, the scarcity of feasible points may lead to numerous costly evaluations spent on infeasible points. Our proposed algorithm addresses this issue by leveraging multi-fidelity information, allowing for premature interruption of an evaluation when a point is estimated to be infeasible. These estimations are controlled by a biadjacency matrix, for which we propose a construction. The proposed method acts as an intermediary component bridging any non multi-fidelity direct search solver and a multi-fidelity blackbox problem, giving the user freedom of choice for the solver. A series of computational tests are conducted to validate the approach. The results show a significant improvement in solution quality when an initial feasible starting point is provided. When this condition is not met, the outcomes are contingent upon specific properties of the blackbox.},
  archive      = {J_COAP},
  author       = {Alarie, Stéphane and Audet, Charles and Diago, Miguel and Digabel, Sébastien Le and Lebeuf, Xavier},
  doi          = {10.1007/s10589-024-00645-w},
  journal      = {Computational Optimization and Applications},
  month        = {4},
  number       = {3},
  pages        = {607--629},
  shortjournal = {Comput. Optim. Appl.},
  title        = {Inter-DS: A cost saving algorithm for expensive constrained multi-fidelity blackbox optimization},
  volume       = {90},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A branch-and-price algorithm for the hyper-rectangular clustering problem with axis-parallel clusters and outliers. <em>COAP</em>, <em>90</em>(2), 583--605. (<a href='https://doi.org/10.1007/s10589-024-00637-w'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We address the problem of clustering a set of points in $$\mathbb {R}^d$$ with axis-parallel clusters, while allowing to discard a pre-specified number of points, thus declared to be outliers. We present an integer programming formulation for this problem which employs exponentially-many variables and we develop an exact algorithm to solve it, based on branch-and-price techniques. We solve the linear relaxation of the proposed model by applying column generation and we propose two different integer programming formulations to tackle the associated pricing subproblem. For the branch-and-price algorithm, we propose a branching rule specifically tailored for the problem. Finally, we provide computational experiments showing that the proposed approach is effective in practice.},
  archive      = {J_COAP},
  author       = {Delle Donne, Diego and Marenco, Javier},
  doi          = {10.1007/s10589-024-00637-w},
  journal      = {Computational Optimization and Applications},
  month        = {3},
  number       = {2},
  pages        = {583--605},
  shortjournal = {Comput. Optim. Appl.},
  title        = {A branch-and-price algorithm for the hyper-rectangular clustering problem with axis-parallel clusters and outliers},
  volume       = {90},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). An accelerated first-order regularized momentum descent ascent algorithm for stochastic nonconvex-concave minimax problems. <em>COAP</em>, <em>90</em>(2), 557--582. (<a href='https://doi.org/10.1007/s10589-024-00638-9'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Stochastic nonconvex minimax problems have attracted wide attention in machine learning, signal processing and many other fields in recent years. In this paper, we propose an accelerated first-order regularized momentum descent ascent algorithm (FORMDA) for solving stochastic nonconvex-concave minimax problems. The iteration complexity of the algorithm is proved to be $$\tilde{\mathcal {O}}(\varepsilon ^{-6.5})$$ to obtain an $$\varepsilon $$ -stationary point, which achieves the best-known complexity bound for single-loop algorithms to solve the stochastic nonconvex-concave minimax problems under the stationarity of the objective function.},
  archive      = {J_COAP},
  author       = {Zhang, Huiling and Xu, Zi},
  doi          = {10.1007/s10589-024-00638-9},
  journal      = {Computational Optimization and Applications},
  month        = {3},
  number       = {2},
  pages        = {557--582},
  shortjournal = {Comput. Optim. Appl.},
  title        = {An accelerated first-order regularized momentum descent ascent algorithm for stochastic nonconvex-concave minimax problems},
  volume       = {90},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Rank-one approximation of a higher-order tensor by a riemannian trust-region method. <em>COAP</em>, <em>90</em>(2), 515--556. (<a href='https://doi.org/10.1007/s10589-024-00634-z'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we consider a rank-one approximation problem of a higher-order tensor. We treat the problem as an optimization model on a Cartesian product of manifolds and solve this model by using a Riemannian optimization method. We derive the action of the Riemannian Hessian of the objective function on tangent vectors to the Cartesian product of manifolds. A Riemannian trust-region method with block-diagonal Hessian is used to solve this model, and the subproblem is solved by the truncated conjugate gradient method. The convergence analysis of the Riemannian trust-region method has been established in the literature with certain assumptions. We verify those assumptions for the rank-one approximation problem. Numerical experiments illustrate that the proposed model with the method is feasible and effective.},
  archive      = {J_COAP},
  author       = {Chen, Jianheng and Huang, Wen},
  doi          = {10.1007/s10589-024-00634-z},
  journal      = {Computational Optimization and Applications},
  month        = {3},
  number       = {2},
  pages        = {515--556},
  shortjournal = {Comput. Optim. Appl.},
  title        = {Rank-one approximation of a higher-order tensor by a riemannian trust-region method},
  volume       = {90},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Convergence analysis for a nonlocal gradient descent method via directional gaussian smoothing. <em>COAP</em>, <em>90</em>(2), 481--513. (<a href='https://doi.org/10.1007/s10589-024-00641-0'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We analyze the convergence of a nonlocal gradient descent method for minimizing a class of high-dimensional non-convex functions, where a directional Gaussian smoothing (DGS) is proposed to define the nonlocal gradient (also referred to as the DGS gradient). The method was first proposed in [Zhang et al., Enabling long-range exploration in minimization of multimodal functions, UAI 2021], in which multiple numerical experiments showed that replacing the traditional local gradient with the DGS gradient can help the optimizers escape local minima more easily and significantly improve their performance. However, a rigorous theory for the efficiency of the method on nonconvex landscape is lacking. In this work, we investigate the scenario where the objective function is composed of a convex function, perturbed by deterministic oscillating noise. We provide a convergence theory under which the iterates exponentially converge to a tightened neighborhood of the solution, whose size is characterized by the noise wavelength. We also establish a correlation between the optimal values of the Gaussian smoothing radius and the noise wavelength, thus justifying the advantage of using moderate or large smoothing radii with the method. Furthermore, if the noise level decays to zero when approaching the global minimum, we prove that DGS-based optimization converges to the exact global minimum with linear rates, similarly to standard gradient-based methods in optimizing convex functions. Several numerical experiments are provided to confirm our theory and illustrate the superiority of the approach over those based on the local gradient.},
  archive      = {J_COAP},
  author       = {Tran, Hoang and Du, Qiang and Zhang, Guannan},
  doi          = {10.1007/s10589-024-00641-0},
  journal      = {Computational Optimization and Applications},
  month        = {3},
  number       = {2},
  pages        = {481--513},
  shortjournal = {Comput. Optim. Appl.},
  title        = {Convergence analysis for a nonlocal gradient descent method via directional gaussian smoothing},
  volume       = {90},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). An inexact ADMM for separable nonconvex and nonsmooth optimization. <em>COAP</em>, <em>90</em>(2), 445--479. (<a href='https://doi.org/10.1007/s10589-024-00643-y'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {An inexact alternating direction method of multiplies (I-ADMM) with an expansion linesearch step was developed for solving a family of separable minimization problems subject to linear constraints, where the objective function is the sum of a smooth but possibly nonconvex function and a possibly nonsmooth nonconvex function. Global convergence and linear convergence rate of the I-ADMM were established under proper conditions while inexact relative error criterion was used for solving the subproblems. In addition, a unified proximal gradient (UPG) method with momentum acceleration was proposed for solving the smooth but possibly nonconvex subproblem. This UPG method guarantees global convergence and will automatically reduce to an optimal accelerated gradient method when the smooth function in the objective is convex. Our numerical experiments on solving nonconvex quadratic programming problems and sparse optimization problems from statistical learning show that the proposed I-ADMM is very effective compared with other state-of-the-art algorithms in the literature.},
  archive      = {J_COAP},
  author       = {Bai, Jianchao and Zhang, Miao and Zhang, Hongchao},
  doi          = {10.1007/s10589-024-00643-y},
  journal      = {Computational Optimization and Applications},
  month        = {3},
  number       = {2},
  pages        = {445--479},
  shortjournal = {Comput. Optim. Appl.},
  title        = {An inexact ADMM for separable nonconvex and nonsmooth optimization},
  volume       = {90},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Convergence of a stochastic variance reduced Levenberg–Marquardt method. <em>COAP</em>, <em>90</em>(2), 417--444. (<a href='https://doi.org/10.1007/s10589-024-00639-8'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we study the empirical residual optimization problem with a least squares loss function. A stochastic variance reduced Levenberg–Marquardt method is proposed for solving it. It is shown that both the estimates and the models of the objective function are probabilistically weak accurate if the sample size is chosen appropriately. Moreover, the method converges to a stationary point of the problem almost surely under certain conditions.},
  archive      = {J_COAP},
  author       = {Shao, Weiyi and Fan, Jinyan},
  doi          = {10.1007/s10589-024-00639-8},
  journal      = {Computational Optimization and Applications},
  month        = {3},
  number       = {2},
  pages        = {417--444},
  shortjournal = {Comput. Optim. Appl.},
  title        = {Convergence of a stochastic variance reduced Levenberg–Marquardt method},
  volume       = {90},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Adversarial perturbations of physical signals. <em>COAP</em>, <em>90</em>(2), 395--415. (<a href='https://doi.org/10.1007/s10589-024-00636-x'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We investigate the vulnerability of computer-vision-based signal classifiers to adversarial perturbations of their inputs, where the signals and perturbations are subject to physical constraints. We consider a scenario in which a source and interferer emit signals that propagate as waves to a detector, which attempts to classify the source by analyzing the spectrogram of the signal it receives using a pre-trained neural network. By solving PDE-constrained optimization problems, we construct interfering signals that cause the detector to misclassify the source even though the perturbations to the spectrogram of the received signal are nearly imperceptible. Though such problems can have millions of decision variables, we introduce methods to solve them efficiently. Our experiments demonstrate that one can compute effective and physically realizable adversarial perturbations for a variety of machine learning models under various physical conditions.},
  archive      = {J_COAP},
  author       = {Bassett, Robert L. and Van Dellen, Austin and Austin, Anthony P.},
  doi          = {10.1007/s10589-024-00636-x},
  journal      = {Computational Optimization and Applications},
  month        = {3},
  number       = {2},
  pages        = {395--415},
  shortjournal = {Comput. Optim. Appl.},
  title        = {Adversarial perturbations of physical signals},
  volume       = {90},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Solving polynomial variational inequality problems via lagrange multiplier expressions and moment-SOS relaxations. <em>COAP</em>, <em>90</em>(2), 361--394. (<a href='https://doi.org/10.1007/s10589-024-00635-y'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper focuses on the development of numerical methods for solving variational inequality problems (VIPs) with involved mappings and feasible sets characterized by polynomial functions. We propose a numerical algorithm for computing solutions to polynomial VIPs based on Lagrange multiplier expressions and the Moment-SOS hierarchy of semidefinite relaxations. Building upon this algorithm, we also extend to finding more or even all solutios to polynomial VIPs. This algorithm can find solutions to polynomial VIPs or determine their nonexistence within a finite number of steps, under some general assumptions. Moreover, it is demonstrated that if the VIP is represented by generic polynomial functions, a finite number of Karush–Kuhn–Tucker (KKT) points exist, and all solutions to the polynomial VIP are KKT points. The paper establishes that in such cases, the method is guaranteed to terminate within a finite number of iterations, with an upper bound for the number of KKT points determined using intersection theory. Finally, even when algorithms lack finite convergence, the paper demonstrates asymptotic convergence under specific continuity assumptions. Numerical experiments are conducted to illustrate the efficiency of the proposed methods.},
  archive      = {J_COAP},
  author       = {Nie, Jiawang and Sun, Defeng and Tang, Xindong and Zhang, Min},
  doi          = {10.1007/s10589-024-00635-y},
  journal      = {Computational Optimization and Applications},
  month        = {3},
  number       = {2},
  pages        = {361--394},
  shortjournal = {Comput. Optim. Appl.},
  title        = {Solving polynomial variational inequality problems via lagrange multiplier expressions and moment-SOS relaxations},
  volume       = {90},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A mixed-integer PDE-constrained optimization formulation for constructing electromagnetic cloaks with multiple materials. <em>COAP</em>, <em>90</em>(2), 337--360. (<a href='https://doi.org/10.1007/s10589-024-00644-x'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We study the design of an electromagnetic cloak from multiple materials with an additional constraint on the mass of the cloak. Our problem is an example of a topology optimization problem, and we formulate this problem as a mixed-integer partial-differential equation constrained optimization (MIPDECO) problem, where Maxwell’s equation models the propagation of the wave through the cloak and surrounding medium. We use binary variables to model the assignment of the different materials, and their relevant properties (permittivity and density). The mass constraint adds a nontrivial constraint to this problem. We propose a two-phase strategy to solve this problem. In the first phase, we solve a continuous relaxation, and then propose a new variant of the feasibility pump that exploits the structure of the PDE to obtain an initial integral solution candidate. In the second phase, we use a trust-region approach to improve this incumbent. We also consider a continuation or mesh-sequencing approach to find better solutions faster on consecutively finer meshes. We present detailed numerical results to illustrate the effectiveness of our approaches for constructing multi-material cloaks with a mass constraint.},
  archive      = {J_COAP},
  author       = {Vogt, Ryan H. and Leyffer, Sven and Munson, Todd},
  doi          = {10.1007/s10589-024-00644-x},
  journal      = {Computational Optimization and Applications},
  month        = {3},
  number       = {2},
  pages        = {337--360},
  shortjournal = {Comput. Optim. Appl.},
  title        = {A mixed-integer PDE-constrained optimization formulation for constructing electromagnetic cloaks with multiple materials},
  volume       = {90},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). On a minimization problem of the maximum generalized eigenvalue: Properties and algorithms. <em>COAP</em>, <em>90</em>(1), 303--336. (<a href='https://doi.org/10.1007/s10589-024-00621-4'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We study properties and algorithms of a minimization problem of the maximum generalized eigenvalue of symmetric-matrix-valued affine functions, which is nonsmooth and quasiconvex, and has application to eigenfrequency optimization of truss structures. We derive an explicit formula of the Clarke subdifferential of the maximum generalized eigenvalue and prove the maximum generalized eigenvalue is a pseudoconvex function, which is a subclass of a quasiconvex function, under suitable assumptions. Then, we consider smoothing methods to solve the problem. We introduce a smooth approximation of the maximum generalized eigenvalue and prove the convergence rate of the smoothing projected gradient method to a global optimal solution in the considered problem. Also, some heuristic techniques to reduce the computational costs, acceleration and inexact smoothing, are proposed and evaluated by numerical experiments.},
  archive      = {J_COAP},
  author       = {Nishioka, Akatsuki and Toyoda, Mitsuru and Tanaka, Mirai and Kanno, Yoshihiro},
  doi          = {10.1007/s10589-024-00621-4},
  journal      = {Computational Optimization and Applications},
  month        = {1},
  number       = {1},
  pages        = {303--336},
  shortjournal = {Comput. Optim. Appl.},
  title        = {On a minimization problem of the maximum generalized eigenvalue: Properties and algorithms},
  volume       = {90},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A new branch-and-cut algorithm for linear sum-of-ratios problem based on SLO method and LO relaxation. <em>COAP</em>, <em>90</em>(1), 257--301. (<a href='https://doi.org/10.1007/s10589-024-00622-3'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider a linear sum-of-ratios fractional programming problem that arises from a broad range of applications and is known to be NP-hard. In this paper, we first develop a successive linear optimization (SLO) method for the linear sum-of-ratios problem and show that it converges to a KKT point of the underlying problem. Second, we propose a new branch-and-cut algorithm for globally solving the linear sum-of-ratios fractional program by integrating the SLO method, the linear optimization (LO) relaxation, branch-and-bound framework and branch-and-cut rule. We establish the global convergence of the algorithm and estimate its complexity. Numerical results are reported to illustrate the effectiveness of the proposed algorithm in finding a global optimal solution to large-scale instances of linear sum-of-ratios problem.},
  archive      = {J_COAP},
  author       = {Luo, Hezhi and Xu, Youmin and Wu, Huixian and Wang, Guoqiang},
  doi          = {10.1007/s10589-024-00622-3},
  journal      = {Computational Optimization and Applications},
  month        = {1},
  number       = {1},
  pages        = {257--301},
  shortjournal = {Comput. Optim. Appl.},
  title        = {A new branch-and-cut algorithm for linear sum-of-ratios problem based on SLO method and LO relaxation},
  volume       = {90},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Approximate bregman proximal gradient algorithm for relatively smooth nonconvex optimization. <em>COAP</em>, <em>90</em>(1), 227--256. (<a href='https://doi.org/10.1007/s10589-024-00618-z'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we propose the approximate Bregman proximal gradient algorithm (ABPG) for solving composite nonconvex optimization problems. ABPG employs a new distance that approximates the Bregman distance, making the subproblem of ABPG simpler to solve compared to existing Bregman-type algorithms. The subproblem of ABPG is often expressed in a closed form. Similarly to existing Bregman-type algorithms, ABPG does not require the global Lipschitz continuity for the gradient of the smooth part. Instead, assuming the smooth adaptable property, we establish the global subsequential convergence under standard assumptions. Additionally, assuming that the Kurdyka–Łojasiewicz property holds, we prove the global convergence for a special case. Our numerical experiments on the $$\ell _p$$ regularized least squares problem, the $$\ell _p$$ loss problem, and the nonnegative linear system show that ABPG outperforms existing algorithms especially when the gradient of the smooth part is not globally Lipschitz or even locally Lipschitz continuous.},
  archive      = {J_COAP},
  author       = {Takahashi, Shota and Takeda, Akiko},
  doi          = {10.1007/s10589-024-00618-z},
  journal      = {Computational Optimization and Applications},
  month        = {1},
  number       = {1},
  pages        = {227--256},
  shortjournal = {Comput. Optim. Appl.},
  title        = {Approximate bregman proximal gradient algorithm for relatively smooth nonconvex optimization},
  volume       = {90},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Efficient proximal subproblem solvers for a nonsmooth trust-region method. <em>COAP</em>, <em>90</em>(1), 193--226. (<a href='https://doi.org/10.1007/s10589-024-00628-x'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In [R. J. Baraldi and D. P. Kouri, Mathematical Programming, (2022), pp. 1-40], we introduced an inexact trust-region algorithm for minimizing the sum of a smooth nonconvex and nonsmooth convex function. The principle expense of this method is in computing a trial iterate that satisfies the so-called fraction of Cauchy decrease condition—a bound that ensures the trial iterate produces sufficient decrease of the subproblem model. In this paper, we expound on various proximal trust-region subproblem solvers that generalize traditional trust-region methods for smooth unconstrained and convex-constrained problems. We introduce a simplified spectral proximal gradient solver, a truncated nonlinear conjugate gradient solver, and a dogleg method. We compare algorithm performance on examples from data science and PDE-constrained optimization.},
  archive      = {J_COAP},
  author       = {Baraldi, Robert J. and Kouri, Drew P.},
  doi          = {10.1007/s10589-024-00628-x},
  journal      = {Computational Optimization and Applications},
  month        = {1},
  number       = {1},
  pages        = {193--226},
  shortjournal = {Comput. Optim. Appl.},
  title        = {Efficient proximal subproblem solvers for a nonsmooth trust-region method},
  volume       = {90},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Fast convergence of the primal-dual dynamical system and corresponding algorithms for a nonsmooth bilinearly coupled saddle point problem. <em>COAP</em>, <em>90</em>(1), 151--192. (<a href='https://doi.org/10.1007/s10589-024-00626-z'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper studies the convergence rate of a second-order dynamical system associated with a nonsmooth bilinearly coupled convex-concave saddle point problem, as well as the convergence rate of its corresponding discretizations. We derive the convergence rate of the primal-dual gap for the second-order dynamical system with asymptotically vanishing damping term. Based on an implicit discretization scheme, we propose a primal-dual algorithm and provide a non-ergodic convergence rate under a general setting for the inertial parameters when one objective function is continuously differentiable and convex and the other is a proper, convex and lower semicontinuous function. For this algorithm we derive a $$O\left( 1/k^2 \right) $$ convergence rate under three classical rules proposed by Nesterov, Chambolle-Dossal and Attouch-Cabot without assuming strong convexity, which is compatible with the results of the continuous-time dynamic system. For the case when both objective functions are continuously differentiable and convex, we further present a primal-dual algorithm based on an explicit discretization. We provide a corresponding non-ergodic convergence rate for this algorithm and show that the sequence of iterates generated weakly converges to a primal-dual optimal solution. Finally, we present numerical experiments that indicate the superior numerical performance of both algorithms.},
  archive      = {J_COAP},
  author       = {Ding, Ke-wei and Fliege, Jörg and Vuong, Phan Tu},
  doi          = {10.1007/s10589-024-00626-z},
  journal      = {Computational Optimization and Applications},
  month        = {1},
  number       = {1},
  pages        = {151--192},
  shortjournal = {Comput. Optim. Appl.},
  title        = {Fast convergence of the primal-dual dynamical system and corresponding algorithms for a nonsmooth bilinearly coupled saddle point problem},
  volume       = {90},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Solving convex optimization problems via a second order dynamical system with implicit hessian damping and tikhonov regularization. <em>COAP</em>, <em>90</em>(1), 113--149. (<a href='https://doi.org/10.1007/s10589-024-00620-5'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper deals with a second order dynamical system with a Tikhonov regularization term in connection to the minimization problem of a convex Fréchet differentiable function. The fact that beside the asymptotically vanishing damping we also consider an implicit Hessian driven damping in the dynamical system under study allows us, via straightforward explicit discretization, to obtain inertial algorithms of gradient type. We show that the value of the objective function in a generated trajectory converges rapidly to the global minimum of the objective function and depending the Tikhonov regularization parameter the generated trajectory converges weakly to a minimizer of the objective function or the generated trajectory converges strongly to the element of minimal norm from the $$\mathop {\text {argmin}}$$ set of the objective function. We also obtain the fast convergence of the velocities towards zero and some integral estimates. Our analysis reveals that the Tikhonov regularization parameter and the damping parameters are strongly correlated, there is a setting of the parameters that separates the cases when weak convergence of the trajectories to a minimizer and strong convergence of the trajectories to the minimal norm minimizer can be obtained.},
  archive      = {J_COAP},
  author       = {László, Szilárd Csaba},
  doi          = {10.1007/s10589-024-00620-5},
  journal      = {Computational Optimization and Applications},
  month        = {1},
  number       = {1},
  pages        = {113--149},
  shortjournal = {Comput. Optim. Appl.},
  title        = {Solving convex optimization problems via a second order dynamical system with implicit hessian damping and tikhonov regularization},
  volume       = {90},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). The sparse(st) optimization problem: Reformulations, optimality, stationarity, and numerical results. <em>COAP</em>, <em>90</em>(1), 77--112. (<a href='https://doi.org/10.1007/s10589-024-00625-0'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider the sparse optimization problem with nonlinear constraints and an objective function, which is given by the sum of a general smooth mapping and an additional term defined by the $$ \ell _0 $$ -quasi-norm. This term is used to obtain sparse solutions, but difficult to handle due to its nonconvexity and nonsmoothness (the sparsity-improving term is even discontinuous). The aim of this paper is to present two reformulations of this program as a smooth nonlinear program with complementarity-type constraints. We show that these programs are equivalent in terms of local and global minima and introduce a problem-tailored stationarity concept, which turns out to coincide with the standard KKT conditions of the two reformulated problems. In addition, a suitable constraint qualification as well as second-order conditions for the sparse optimization problem are investigated. These are then used to show that three Lagrange–Newton-type methods are locally fast convergent. Numerical results on different classes of test problems indicate that these methods can be used to drastically improve sparse solutions obtained by some other (globally convergent) methods for sparse optimization problems.},
  archive      = {J_COAP},
  author       = {Kanzow, Christian and Schwartz, Alexandra and Weiß, Felix},
  doi          = {10.1007/s10589-024-00625-0},
  journal      = {Computational Optimization and Applications},
  month        = {1},
  number       = {1},
  pages        = {77--112},
  shortjournal = {Comput. Optim. Appl.},
  title        = {The sparse(st) optimization problem: Reformulations, optimality, stationarity, and numerical results},
  volume       = {90},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A real moment-HSOS hierarchy for complex polynomial optimization with real coefficients. <em>COAP</em>, <em>90</em>(1), 53--75. (<a href='https://doi.org/10.1007/s10589-024-00617-0'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper proposes a real moment-HSOS hierarchy for complex polynomial optimization problems with real coefficients. We show that this hierarchy provides the same sequence of lower bounds as the complex analogue, yet is much cheaper to solve. In addition, we prove that global optimality is achieved when the ranks of the moment matrix and certain submatrix equal two in case that a sphere constraint is present, and as a consequence, the complex polynomial optimization problem has either two real optimal solutions or a pair of conjugate optimal solutions. A simple procedure for extracting a pair of conjugate optimal solutions is given in the latter case. Various numerical examples are presented to demonstrate the efficiency of this new hierarchy, and an application to polyphase code design is also provided.},
  archive      = {J_COAP},
  author       = {Wang, Jie and Magron, Victor},
  doi          = {10.1007/s10589-024-00617-0},
  journal      = {Computational Optimization and Applications},
  month        = {1},
  number       = {1},
  pages        = {53--75},
  shortjournal = {Comput. Optim. Appl.},
  title        = {A real moment-HSOS hierarchy for complex polynomial optimization with real coefficients},
  volume       = {90},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Generating representative sets for multiobjective discrete optimization problems with specified coverage errors. <em>COAP</em>, <em>90</em>(1), 27--51. (<a href='https://doi.org/10.1007/s10589-024-00627-y'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present a new approach to generate representations with a coverage error quality guarantee for multiobjective discrete optimization problems with any number of objectives. Our method is based on an earlier exact algorithm that finds the entire nondominated set using $$\varepsilon $$ -constraint scalarizations. The representation adaptation requires the search to be conducted over a p-dimensional parameter space instead of the $$(p-1)$$ -dimensional one of the exact version. The algorithm uses rectangles as search elements and for each rectangle, two-stage mathematical programs are solved to obtain efficient solutions. The representation algorithm implements a modified search procedure and is designed to eliminate a rectangle if it can be verified that it is not of interest given a particular coverage error requirement. Since computing the coverage error is a computationally demanding task, we propose a method to compute an upper bound on this quantity in polynomial time. The algorithm is tested on multiobjective knapsack and assignment problem instances with different error tolerance levels. We observe that our representation algorithm provides significant savings in computational effort even with relatively low levels of coverage error tolerance values for problems with three objective functions. Moreover, computational effort decreases almost linearly when coverage error tolerance increases. This makes it possible to obtain good quality representations for larger problem instances. An analysis of anytime performance on two selected problem instances demonstrates that the algorithm puts together a diverse representation starting from the early iterations.},
  archive      = {J_COAP},
  author       = {Kirlik, Gokhan and Sayın, Serpil},
  doi          = {10.1007/s10589-024-00627-y},
  journal      = {Computational Optimization and Applications},
  month        = {1},
  number       = {1},
  pages        = {27--51},
  shortjournal = {Comput. Optim. Appl.},
  title        = {Generating representative sets for multiobjective discrete optimization problems with specified coverage errors},
  volume       = {90},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Optimization schemes on manifolds for structured matrices with fixed eigenvalues. <em>COAP</em>, <em>90</em>(1), 1--26. (<a href='https://doi.org/10.1007/s10589-024-00630-3'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Several manifold optimization schemes are presented and analyzed for solving a specialized inverse structured symmetric matrix problem with prescribed spectrum. Some entries in the desired matrix are assigned in advance and cannot be altered. The rest of the entries are free, some of them preferably away from zero. The reconstructed matrix must satisfy these requirements and its eigenvalues must be the given ones. This inverse eigenvalue problem is related to the problem of determining the graph, with weights on the undirected edges, of the matrix associated with its sparse pattern. Our optimization schemes are based on considering the eigenvector matrix as the only unknown and iteratively moving on the manifold of orthogonal matrices, forcing the additional structural requirements through a change of variables and a convenient differentiable objective function in the space of square matrices. We propose Riemannian gradient-type methods combined with two different well-known retractions, and with two well-known constrained optimization strategies: penalization and augmented Lagrangian. We also present a block alternating technique that takes advantage of a proper separation of variables. Convergence properties of the penalty alternating approach are established. Finally, we present initial numerical results to demonstrate the effectiveness of our proposals.},
  archive      = {J_COAP},
  author       = {Chehab, Jean-Paul and Oviedo, Harry and Raydan, Marcos},
  doi          = {10.1007/s10589-024-00630-3},
  journal      = {Computational Optimization and Applications},
  month        = {1},
  number       = {1},
  pages        = {1--26},
  shortjournal = {Comput. Optim. Appl.},
  title        = {Optimization schemes on manifolds for structured matrices with fixed eigenvalues},
  volume       = {90},
  year         = {2025},
}
</textarea>
</details></li>
</ul>

</body>
</html>
