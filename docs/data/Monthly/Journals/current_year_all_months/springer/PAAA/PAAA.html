<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>PAAA</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="paaa">PAAA - 192</h2>
<ul>
<li><details>
<summary>
(2025). ViTGuard: A synergistic approach to malware detection using vision transformers and genetic algorithms optimization. <em>PAAA</em>, <em>28</em>(4), 1-20. (<a href='https://doi.org/10.1007/s10044-025-01516-8'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the realm of cybersecurity, malware detection stands at the forefront of defense against malicious software. This study introduces an innovative strategy to tackle the ever-evolving cyber threats that characterize the current landscape, transcending traditional methodologies. We present a hybridized approach that combines the advanced capabilities of Vision Transformer (ViT) model, genetic algorithms, and cutting-edge deep learning techniques, marking a new era in cybersecurity. The proposed process begins by transforming complex malware source code into grayscale images, effectively bridging the gap between linear code analysis and spatial image recognition. These grayscale images are analyzed using the ViT_b16 model, renowned for its exceptional ability to uncover subtle intricacies within images. The next steps involve leveraging deep learning to scrutinize the features identified by the ViT, facilitating precise detection of malicious code. To enhance the efficiency of the proposed deep learning model, a genetic algorithm is employed for end-to-end hyperparameter optimization for both ViT and deep learning phases. this process aims at calibrating essential parameters such as the Image Size, Number of Attention Heads, Hidden Size (Embedding Dimension), MLP (Feedforward) Dimension, activation function, architectural depth, neuron count, optimizers, initializers, dropout layers, batch normalization, and learning rates of the ViT_b16 and deep learning models. After extensive training on a dataset comprising 25 diverse malware families, the proposed model exhibits remarkable performance, consistently achieving an accuracy rate exceeding 99% in differentiating among these malware variants. A comprehensive evaluation and benchmarking against both state-of-the-art malware detection methodologies and widely used baseline models, including CNNs and traditional machine learning algorithms, demonstrating superior detection performance across all metrics.},
  archive      = {J_PAAA},
  author       = {Bakır, Halit and Bakır, Rezan and Alkhaldi, Tareq and Darem, Abdulbasit A. and Alhashmi, Asma A. and Alqhatani, Abdulmajeed},
  doi          = {10.1007/s10044-025-01516-8},
  journal      = {Pattern Analysis and Applications},
  month        = {12},
  number       = {4},
  pages        = {1-20},
  shortjournal = {Pattern Anal. Appl.},
  title        = {ViTGuard: A synergistic approach to malware detection using vision transformers and genetic algorithms optimization},
  volume       = {28},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). MDUA-YOLO: An advanced deep learning approach for PCB surface defect detection. <em>PAAA</em>, <em>28</em>(4), 1-14. (<a href='https://doi.org/10.1007/s10044-025-01526-6'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The surface defects of printed circuit boards (PCB) that occur during the manufacturing process seriously affect product quality. So it is important to detect PCB surface defects quickly and accurately. However, existing defect detection methods still have room to be improved for PCB surface defect detection. This paper proposes an advanced model, MDUA-YOLO, based on YOLOv5 to increase the detection accuracy of defects on PCB surface. Firstly, we introduce the C3 Mobile Vision Transformer (C3MobileViT) module in the backbone of YOLOv5, improving the feature extraction capability of the model. Secondly, the Deformable Convolutional-Receptive Field Block (DC-RFB) module is incorporated into the neck of YOLOv5 to dynamically expands the receptive field and more accurately capture the location information of small defects. Additionally, we design the Union Attention Block (UAB) module in the neck of YOLOv5 to optimize the fusion of low-level and high-level feature maps. Finally, an extra prediction head and new feature fusion layer are also added to enhance the ability of the model to detect small defects. On the benchmark PKU-PCB and DeepPCB datasets, numerous experimental results show that the MDUA-YOLO surpasses other comparison state-of-the-art models and meets the real-time detection requirement of industrial environment.},
  archive      = {J_PAAA},
  author       = {Liu, Xiaowei and Wang, Haichao},
  doi          = {10.1007/s10044-025-01526-6},
  journal      = {Pattern Analysis and Applications},
  month        = {12},
  number       = {4},
  pages        = {1-14},
  shortjournal = {Pattern Anal. Appl.},
  title        = {MDUA-YOLO: An advanced deep learning approach for PCB surface defect detection},
  volume       = {28},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Enhancing video salient object detection via SAM-based multimodal energy prompting. <em>PAAA</em>, <em>28</em>(4), 1-13. (<a href='https://doi.org/10.1007/s10044-025-01531-9'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Video Salient Object Detection (VSOD) aims to identify the most visually conspicuous objects in videos and extract key information from complex visual scenes. Recent studies combine optical flow (OF) and depth for complementary feature extraction. However, suboptimal fusion strategies often treat these modalities merely as extensions of the RGB stream, failing to fully leverage their unique semantic contributions. To address this limitation, we propose a novel SAM-based Multimodal Energy Prompting Network (MEPNet), which utilizes implicit prompts derived from OF and depth within a pre-trained Segment Anything Model (SAM). This approach enhances VSOD by effectively integrating the complementary dynamic and structural information from these modalities. Particularly, we introduce a Spectrogram Energy Generator to extract Spectrogram Energy from OF and depth. These energy-driven prompts fine-tune SAM via the Modality Energy Adapter, effectively mitigating noise interference and improving segmentation accuracy. In addition, we propose a Circular High-frequency Filter to enhance RGB modality details using an adaptive circular mask. Extensive experiments on five VSOD benchmark datasets demonstrate that our MEPNet outperforms state-of-the-art approaches. Furthermore, our MEPNet generalizes effectively to the Video-Camouflaged Object Detection task and achieves competitive results. The module and predicted maps are publicly available at https://github.com/TOMMYWHY/MEPNet .},
  archive      = {J_PAAA},
  author       = {Jiang, Tao and Wang, Yi and Hou, Feng and Liu, Li-li},
  doi          = {10.1007/s10044-025-01531-9},
  journal      = {Pattern Analysis and Applications},
  month        = {12},
  number       = {4},
  pages        = {1-13},
  shortjournal = {Pattern Anal. Appl.},
  title        = {Enhancing video salient object detection via SAM-based multimodal energy prompting},
  volume       = {28},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Personalized federated learning with adaptive aggregation within clusters. <em>PAAA</em>, <em>28</em>(4), 1-14. (<a href='https://doi.org/10.1007/s10044-025-01533-7'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Federated learning allows clients to collaboratively train models while keeping client data local. Initially, it trains a global model to serve all clients, but when the distribution of each local dataset differs significantly from the global dataset, the local objectives of each client may diverge from the globally optimal values, leading to drift in local updates. This phenomenon greatly impacts model performance. The primary purpose of client participation in federated learning is to obtain personalized models with better local performance. In order to solve this problem, this paper proposes a new federated learning algorithm - Federated Learning with Adaptive Intra - cluster Aggregation (FedACC). This algorithm utilizes the inference correlation of client-uploaded models to the server and divides clients with similar data distributions into clusters. During the weighted aggregation of models within each cluster, we introduce an adaptive weight learning algorithm and use the obtained weights to perform the weighted aggregation of cluster models. The algorithm can cluster clients with similar data distributions and utilize adaptive weight learning within the cluster to obtain optimal aggregation weights, enabling more efficient and personalized federated learning through a weighted aggregation cluster model. Our experiments are conducted on three public image datasets, namely MNIST, Fashion - MNIST, and CIFAR − 10, and in a data - heterogeneous environment. Compared with three baseline algorithms, the Federated Averaging algorithm (FedAvg), the Federated Proximal algorithm (FedProx), and the Federated Learning with Intra - cluster Similarity algorithm (FLIS), the global model of the FedACC algorithm proposed in this paper converges faster and has a higher accuracy. On the Fashion - MNIST dataset, compared with FedAvg, FedProx, and FLIS algorithms, the accuracy of FedACC is improved by 11.6%, 10.5%, and 3.0% respectively, which proves the effectiveness of the FedACC algorithm.},
  archive      = {J_PAAA},
  author       = {Ding, Shiyuan and Liu, Yanhong and Shi, Haobin and Gao, Yingying},
  doi          = {10.1007/s10044-025-01533-7},
  journal      = {Pattern Analysis and Applications},
  month        = {12},
  number       = {4},
  pages        = {1-14},
  shortjournal = {Pattern Anal. Appl.},
  title        = {Personalized federated learning with adaptive aggregation within clusters},
  volume       = {28},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A novel hybrid approach for retinal vessel segmentation with dynamic long-range dependency and multi-scale retinal edge fusion enhancement. <em>PAAA</em>, <em>28</em>(4), 1-19. (<a href='https://doi.org/10.1007/s10044-025-01534-6'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Accurate retinal vessel segmentation is crucial for ophthalmic image analysis, providing key structural information for diagnosis and treatment planning. However, existing methods struggle with multi-scale vessel variability, complex curvatures, and ambiguous boundaries. CNNs, Transformer, and Mamba-based approaches have shown promise, yet still struggle to maintain vascular continuity and accurately delineate fine vessel boundaries, especially in thin or tortuous regions, leading to structural discontinuities and edge ambiguity. To address these limitations, we propose a novel hybrid framework that synergistically integrates CNNs and Mamba for high-precision retinal vessel segmentation. Our approach introduces three key innovations: (1) The proposed High-Resolution Edge Fuse Network is a high-resolution preserving hybrid segmentation framework that enhances edge features to ensure accurate and robust vessel segmentation. (2) The Dynamic Snake Visual State Space block is designed to adaptively capture vessel curvature details and long-range dependencies. An improved eight-directional 2D Snake-Selective Scan mechanism and a dynamic weighting strategy enhance the perception of complex vascular topologies. (3) The MREF module enhances boundary precision through multi-scale edge feature aggregation, suppressing noise while emphasizing critical vessel structures across scales. Experiments on DRIVE, STARE, and CHASE_DB1 demonstrate the robust and effective performance of our method. Specifically, our approach attains Dice scores of 82.14%, 76.29%, and 80.46%; clDice scores of 82.40%, 80.30%, and 82.93%; and AUC values of 98.56%, 98.05%, and 98.78%, respectively. This work provides a robust method for clinical applications requiring accurate retinal vessel analysis. The code is available at https://github.com/frank-oy/HREFNet .},
  archive      = {J_PAAA},
  author       = {Ouyang, Yihao and Kuang, Xunheng and Xiong, Mengjia and Wang, Zhida and Wang, Yuanquan},
  doi          = {10.1007/s10044-025-01534-6},
  journal      = {Pattern Analysis and Applications},
  month        = {12},
  number       = {4},
  pages        = {1-19},
  shortjournal = {Pattern Anal. Appl.},
  title        = {A novel hybrid approach for retinal vessel segmentation with dynamic long-range dependency and multi-scale retinal edge fusion enhancement},
  volume       = {28},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Adversarial translucent patch: A robust physical attack technique against object detectors. <em>PAAA</em>, <em>28</em>(4), 1-16. (<a href='https://doi.org/10.1007/s10044-025-01535-5'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The increasing use of computer vision-based autonomous driving technology in daily life necessitates further evaluation of its safety. Current physical attack techniques, using non-transparent stickers as perturbations, lack stealth and therefore may not be effective in real-world applications. Some studies use translucent patches on camera lenses to attack deep neural networks (DNNs), but accessing a victim’s camera is impractical. Light-based attacks also struggle to achieve robust effects under varying environmental conditions. To address these issues within the domain of applied pattern recognition, we propose Adversarial Translucent Patch (AdvTP). This method utilizes translucent color patches optimized with a differential evolution algorithm to create effective physical perturbations. These patches are applied to target objects for black-box attacks on object detectors, a key area in computer vision and image processing. Extensive experiments validate the method’s effectiveness, stealth, and robustness. The proposed method achieves a 91.04% success rate in digital attacks and a 100% success rate in most physical attack cases. It demonstrates superior stealth compared to baseline methods and achieves an average attack success rate of 94.04% against advanced object detectors. We also analyze the method’s generalization capabilities across different pattern recognition tasks, including attacking image classifiers and vehicle detectors, as well as its performance in transfer attacks and against adversarial defenses. Given the significant security threats posed by this method to vision-based applications, which are critical in various applied domains, we believe this work will attract considerable attention from the pattern recognition community. The code can be found in https://github.com/kalbinur90/AdvTP.git .},
  archive      = {J_PAAA},
  author       = {Tiliwalidi, Kalibinuer and Hu, Chengyin and Shi, Weiwen and Lu, Guangxi and Wu, Hao},
  doi          = {10.1007/s10044-025-01535-5},
  journal      = {Pattern Analysis and Applications},
  month        = {12},
  number       = {4},
  pages        = {1-16},
  shortjournal = {Pattern Anal. Appl.},
  title        = {Adversarial translucent patch: A robust physical attack technique against object detectors},
  volume       = {28},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). High-capacity reversible data hiding in encrypted HDR images with multiple data hiders. <em>PAAA</em>, <em>28</em>(4), 1-14. (<a href='https://doi.org/10.1007/s10044-025-01536-4'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This study proposes a reversible data hiding algorithm for encrypted high-dynamic-range images, with the potential to significantly impact the field. The algorithm supports high embedding capacity, high embedding rate, and allows for multiple data hiders. It begins by using a median edge detector to predict the value of each processing pixel, and determines the embedding capacity through multi-MSB prediction and leading zero count prediction strategies. Multiple label maps are generated and encoded using Huffman coding to reduce transmission overhead. Furthermore, secret sharing is employed to generate several image shares containing pre-embedded label maps, which are distributed to participants who can independently embed secret message. The proposed algorithm enhances the confidentiality of high-dynamic-range images through secret sharing and improves robustness by using multiple image shares, preventing a single image attack from compromising message recovery. Overall, the algorithm significantly increases embedding capacity. Experimental results demonstrate that this approach makes substantial contributions to reversible data hiding in encrypted high-dynamic-range images.},
  archive      = {J_PAAA},
  author       = {Lin, Alfrindo and Lin, Yun-Ting and Jao, Wen-Ting and Tsai, Yuan-Yu},
  doi          = {10.1007/s10044-025-01536-4},
  journal      = {Pattern Analysis and Applications},
  month        = {12},
  number       = {4},
  pages        = {1-14},
  shortjournal = {Pattern Anal. Appl.},
  title        = {High-capacity reversible data hiding in encrypted HDR images with multiple data hiders},
  volume       = {28},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Boostis:boosting image semi-supervised learning through pseudo-label quality assessment. <em>PAAA</em>, <em>28</em>(4), 1-13. (<a href='https://doi.org/10.1007/s10044-025-01537-3'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, significant progress has been made in semi-supervised learning methods that leverage pseudo-labels and consistency regularization. However, two key issues with existing approaches have been identified. Firstly, these methods often focus on maintaining inter-class consistency between strong and weak augmentations, but they tend to overlook intra-class differences. This oversight results in a waste of valuable intra-class information. Secondly, the selection of a fixed high threshold for pseudo-label confidence restricts the quantity and utilization of pseudo-labels. On the contrary, using an initial low threshold introduces a large number of erroneous pseudo-labels, resulting in a degradation of the model’s performance. To address these issues, we propose a novel semi-supervised learning framework that combines contrastive learning and feature discrepancy loss. Our approach introduces a new loss function that facilitates intra-class discrimination by emphasizing inter-class differences. Additionally, we tackle the threshold problem in pseudo-label consistency loss by introducing dynamic weighting coefficients. These coefficients help balance the impact of both the quantity and quality of pseudo-labels on the model. Our experimental results demonstrate that our method effectively harnesses the feature differences among samples with different perturbations. This enhancement boosts the model’s feature generation capability while mitigating the negative effects of erroneous pseudo-labels on model’s performance. Overall, our proposed framework provides a more comprehensive and effective solution to semi-supervised learning in classification applications by addressing the issues of intra-class differences and the selection of pseudo-label thresholds.},
  archive      = {J_PAAA},
  author       = {Liu, Pingping and Chen, Pengfei and Liu, Xiaofeng and Zhou, Qiuzhan},
  doi          = {10.1007/s10044-025-01537-3},
  journal      = {Pattern Analysis and Applications},
  month        = {12},
  number       = {4},
  pages        = {1-13},
  shortjournal = {Pattern Anal. Appl.},
  title        = {Boostis:boosting image semi-supervised learning through pseudo-label quality assessment},
  volume       = {28},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Enhancing multi-view deep image clustering via contrastive learning for global and local consistency. <em>PAAA</em>, <em>28</em>(4), 1-14. (<a href='https://doi.org/10.1007/s10044-025-01538-2'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-view clustering (MVC) is a data clustering method with many applications, including but not limited to image and video analysis, text and language processing, bioinformatics, and signal processing. The objective of multi-view deep clustering is to enhance the efficacy of clustering algorithms by integrating data from disparate views. However, discrepancies and inconsistencies between different views frequently reduce the precision of the clustering outcomes. In the recent popular contrastive learning, it has been observed that the processing of positive and negative samples does not consider the multi-view consistency information, ultimately resulting in a decline in clustering accuracy. In this paper, we put forth a global and local consistency-based contrastive learning framework to enhance the efficacy of multi-view deep clustering. First, a global consistency constraint is designed to ensure that the global representations of different views can be aligned to capture the data’s main features. Secondly, we introduce a local consistency mechanism, which aims to preserve the unique local information in each view and obtain efficient, positive samples to improve the complementarity and robustness of the inter-view representations through contrastive learning. The experimental results demonstrate that the proposed method markedly enhances the clustering performance on several real benchmark datasets, mainly when dealing with multi-view data with incompleteness.},
  archive      = {J_PAAA},
  author       = {Shi, Fuhao and Lu, Hu},
  doi          = {10.1007/s10044-025-01538-2},
  journal      = {Pattern Analysis and Applications},
  month        = {12},
  number       = {4},
  pages        = {1-14},
  shortjournal = {Pattern Anal. Appl.},
  title        = {Enhancing multi-view deep image clustering via contrastive learning for global and local consistency},
  volume       = {28},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Edge-preserving image smoothing via sparse gradient enhancement. <em>PAAA</em>, <em>28</em>(4), 1-21. (<a href='https://doi.org/10.1007/s10044-025-01539-1'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Edge-preserving image smoothing is fundamental in the fields of computer vision and image processing. The primary challenge is to smooth low-amplitude details while retaining critical structural information. Existing global filtering methods typically incorporate a data fidelity term and a gradient smoothness term. However, preserving the full semantic information of an image solely through data fidelity remains difficult. To address this, we propose a novel generalized smoothing model that integrates a data fidelity term, a structural fidelity term, and a sparse smoothing term to enhance edge-preserving smoothing performance. The structural fidelity term is designed to ensure that the gradient of the output image closely matches the preprocessed gradient of the input image, thereby achieving structural fidelity. Simultaneously, the smoothing term with sparse regularity constraints is employed to smooth detailed information while preserving significant structural elements. Extensive experimental validation demonstrates that our proposed method outperforms existing techniques and is applicable across various fields, including image smoothing, detail enhancement, edge extraction, HDR tone mapping, clip-art compression artifact removal, and image abstraction. The source code is available at: https://github.com/kxZhang1016/EPSGEF .},
  archive      = {J_PAAA},
  author       = {Long, Jianwu and Zhang, Kaixin and Liu, Yuanqin and Chen, Shuang and Luo, Qi},
  doi          = {10.1007/s10044-025-01539-1},
  journal      = {Pattern Analysis and Applications},
  month        = {12},
  number       = {4},
  pages        = {1-21},
  shortjournal = {Pattern Anal. Appl.},
  title        = {Edge-preserving image smoothing via sparse gradient enhancement},
  volume       = {28},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Stain normalization of pathological images using self-attention based cycleGAN with grey scale consistency loss. <em>PAAA</em>, <em>28</em>(4), 1-19. (<a href='https://doi.org/10.1007/s10044-025-01540-8'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The appearance of stains in digital pathological images is prone to be affected by variations in imaging protocols, dyes, scanners, and illumination conditions. This inconsistency will prevent robustness and generalization of computer-aided diagnostic algorithms. Thus, many researchers have proposed efficient methods to normalize different stained images, among which the CycleGAN method prevails. In practice, we have found that this method can cause the background region to be mistaken for the foreground region or can result in the nucleus being considered cytoplasm, a phenomenon we refer to as Stain Region Inversion (SRI). To address the problem and improve its structure-preserving performance in stain normalization tasks, this paper proposes a novel stain normalization method called Structure-Preserving Self-Attention CycleGAN (SPSA-CycleGAN), which enhances the performance of CycleGAN in processing histological and cytological images. We demonstrate how to utilize multi-head self-attention to capture local features and use grey-scaled images to address the issue of SRI, enhancing the pixel-level structure-preserving capability of the original CycleGAN model. Our method is then verified in five experiments and compared with six other state-of-the-art stain normalization methods. The experimental results demonstrated that our SPSA-CycleGAN has better or comparable performance compared to all the other methods. Code available at: https://github.com/Smile-We/SPSA-CycleGAN},
  archive      = {J_PAAA},
  author       = {Chen, Zheng and Jiang, Peng and Duan, Wensi and Wang, Lang and Li, Cheng and Wang, Junfeng and Liu, Juan},
  doi          = {10.1007/s10044-025-01540-8},
  journal      = {Pattern Analysis and Applications},
  month        = {12},
  number       = {4},
  pages        = {1-19},
  shortjournal = {Pattern Anal. Appl.},
  title        = {Stain normalization of pathological images using self-attention based cycleGAN with grey scale consistency loss},
  volume       = {28},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Balanced group convolution: An improved group convolution based on approximability estimates. <em>PAAA</em>, <em>28</em>(4), 1-13. (<a href='https://doi.org/10.1007/s10044-025-01542-6'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The performance of neural networks has been significantly improved by increasing the number of channels in convolutional layers. However, this increase in performance comes with a higher computational cost, resulting in numerous studies focused on reducing it. One promising approach to address this issue is group convolution, which effectively reduces the computational cost by grouping channels. However, to the best of our knowledge, there has been no theoretical analysis on how well the group convolution approximates the standard convolution. In this paper, we mathematically analyze the approximation of the group convolution to the standard convolution with respect to the number of groups. Furthermore, we propose a novel variant of the group convolution called balanced group convolution, which shows a higher approximation with a small additional computational cost. We provide experimental results that validate our theoretical findings and demonstrate the superior performance of the balanced group convolution over other variants of group convolution.},
  archive      = {J_PAAA},
  author       = {Lee, Youngkyu and Park, Jongho and Lee, Chang-Ock},
  doi          = {10.1007/s10044-025-01542-6},
  journal      = {Pattern Analysis and Applications},
  month        = {12},
  number       = {4},
  pages        = {1-13},
  shortjournal = {Pattern Anal. Appl.},
  title        = {Balanced group convolution: An improved group convolution based on approximability estimates},
  volume       = {28},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Dual-stream network with coordinate attention for multi-view micro-expression recognition using 3D face reconstruction. <em>PAAA</em>, <em>28</em>(4), 1-12. (<a href='https://doi.org/10.1007/s10044-025-01499-6'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Micro-expression recognition (MER) is a challenging task due to the subtle and local movements of facial muscles. To get rid of redundant video frames, studies have been conducted on the use of apex frames for MER. However, these studies mainly focused on 2D frontal apex frames, ignoring side face information, which can result in insufficient extraction of features?. To address this issue, we propose a novel dual-stream network with coordinate attention (DSNCA) framework for MER, which comprises a multi-view coordinate attention module (MVCAM) and an apex frame coordinate attention module (AFCAM). The MVCAM initially employs 3D face reconstruction to acquire unobstructed multi-view images that are rich in micro-expression information and enhanced with geometric details. Subsequently, it adaptively learns micro-expression features from multiple angle views with coordinate attention, thus leveraging supplementary face information. In contrast, the AFCAM aims to adaptively locate key regions where micro-expressions occur, thereby minimizing redundant information. The proposed method achieved UF1 and UAR of 76.45, 75.16, 59.21, 59.6, 62.77, 62.06, 66.74, and 69.31 on the Chinese Academy of Sciences Micro-expression DatabaseII (CASMEII), the Spontaneous Micro-expression Corpus (SMIC), the Spontaneous Actions and Micro-Movements (SAMM), and the composite databases, respectively. The code is available for research purposes at https://github.com/pennypppp/DSNCA .},
  archive      = {J_PAAA},
  author       = {Ma, Pianpian and Chen, Jingying and Liu, Xu and Liu, Xiaodi},
  doi          = {10.1007/s10044-025-01499-6},
  journal      = {Pattern Analysis and Applications},
  month        = {12},
  number       = {4},
  pages        = {1-12},
  shortjournal = {Pattern Anal. Appl.},
  title        = {Dual-stream network with coordinate attention for multi-view micro-expression recognition using 3D face reconstruction},
  volume       = {28},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A new multi-feature fusion trajectory prediction method. <em>PAAA</em>, <em>28</em>(4), 1-17. (<a href='https://doi.org/10.1007/s10044-025-01541-7'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Pedestrian trajectory prediction is critical in fields such as autonomous driving and robot navigation. It enables autonomous vehicles and robots to make proactive decisions, avoid collisions with pedestrians, and ensure pedestrian safety. Some methods utilize Generative Adversarial Networks (GAN) for trajectory prediction. However, there are still two major drawbacks that hinder the improvement of trajectory prediction accuracy. 1) The complex interactions among multiple pieces of information have not been fully captured, preventing the information from being fully utilized. 2) The uncertainty of network outputs has not been fully modeled and processed. To address these challenges and enhance the model’s adaptability to unknown data, especially to provide an effective solution for managing complex tasks, a novel multi-feature fusion trajectory prediction method is introduced. Firstly, this method designs a new feature fusion submodule. It considers the correlation between multi-feature information through the Power Average (P-A) operator. It also enables the model to better learn the differences among multiple feature information and effectively capture the complex interactions among them. Then, the uncertainty of the network’s output is modeled by incorporating a conservative output state through triangular fuzzy numbers, enhancing the flexibility of the network. Evidence theory is crucial in practical applications due to its effectiveness in representing and processing uncertain information. Therefore, we adopt the belief measure of evidence theory as the uncertainty output of the network. Finally, considering that evidence distance can accurately assess the differences between uncertain information, we embed the evidence distance formula into the loss function of the model as a loss term. This loss term is used to address the discrepancy between the model’s predicted output and the actual distribution, thereby optimizing the information loss of the model and enhancing prediction accuracy. Experiments conducted on the public ETH/UCY datasets reveal that the proposed method achieves higher accuracy, particularly for trajectory prediction in complex situations. Additionally, we have not significantly reduced the model’s efficiency.},
  archive      = {J_PAAA},
  author       = {Yang, Tian and Wang, Gang and Lai, Jian and Wang, Yang},
  doi          = {10.1007/s10044-025-01541-7},
  journal      = {Pattern Analysis and Applications},
  month        = {12},
  number       = {4},
  pages        = {1-17},
  shortjournal = {Pattern Anal. Appl.},
  title        = {A new multi-feature fusion trajectory prediction method},
  volume       = {28},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Modeling hierarchical functional brain networks via $$\:{\mathbf{L}}_{2}$$ -normalized attention fully convolutional recurrent autoencoder for multi-task fMRI data. <em>PAAA</em>, <em>28</em>(4), 1-16. (<a href='https://doi.org/10.1007/s10044-025-01543-5'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Modeling functional brain networks is essential for revealing the functional mechanisms of the human brain. Deep Neural Network (DNN) models have been widely employed for extracting multi-scale spatiotemporal features from functional magnetic resonance imaging (fMRI) data. However, current approaches face two fundamental challenges. Firstly, existing deep neural network-based approaches exhibit significant limitations in learning cross-task common representations when dealing with the variable sequence length characteristics inherent in task-fMRI multi-task data. Secondly, existing approaches often neglect the dynamic variability of neural activity across different time points in fMRI data. To overcome these challenges, a novel framework based on $$\:{\mathbf{L}}_{2}$$ -Normalized Attention Fully Convolutional Recurrent Autoencoder ( $$\:{\mathbf{L}}_{2}$$ -FCRAAE) is proposed for modeling hierarchical functional brain networks (FBNs). Specifically, the $$\:{\mathbf{L}}_{2}$$ -FCRAAE is trained in an unsupervised manner, where the autoencoder architecture guides the attention modules to focus on task-activated regions. The architecture incorporates two synergistic design principles: First, its fully convolutional recurrent structure inherently adapts to variable-length tfMRI time series while effectively capturing long-range temporal dynamics and recognizing brain state transitions. Second, the integrated $$\:{\mathbf{L}}_{2}$$ -normalized temporal-channel attention module weights task-relevant neural activation patterns, substantially enhancing representational capacity. Comprehensive experiments demonstrate that the proposed $$\:{\mathbf{L}}_{2}$$ -FCRAAE exhibits superior capability and generalizability in characterizing spatial and temporal patterns of FBNs in a hierarchical manner. Overall, this study presents a novel approach for understanding the hierarchical organization of functional brain architecture. The code for this paper is available at: https://github.com/beiweizai111/FCAAE .},
  archive      = {J_PAAA},
  author       = {Liu, Huan and Cui, Puwang and Zhang, Minye and Li, Li and Han, Fei},
  doi          = {10.1007/s10044-025-01543-5},
  journal      = {Pattern Analysis and Applications},
  month        = {12},
  number       = {4},
  pages        = {1-16},
  shortjournal = {Pattern Anal. Appl.},
  title        = {Modeling hierarchical functional brain networks via $$\:{\mathbf{L}}_{2}$$ -normalized attention fully convolutional recurrent autoencoder for multi-task fMRI data},
  volume       = {28},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Deep hashing with prototype-aware hardness-weighted for supervised cross-modal retrieval. <em>PAAA</em>, <em>28</em>(4), 1-13. (<a href='https://doi.org/10.1007/s10044-025-01546-2'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep hashing technology has gained widespread attention due to its low storage cost and high retrieval efficiency. Although existing cross-modal hashing methods have achieved good retrieval results, there is still the problem of facing modal heterogeneity leading to semantic separation. Therefore, this paper proposes a deep hashing method based on prototype-aware hardness-weighted, which uses the CLIP model after fine-tuning the large language model as a feature extractor to better obtain the feature information of the two modalities. And a set of loss functions are designed to better handle difficult samples by prototype-aware hardness-weighted loss to ensure that the data can be embedded in the appropriate location, bringing relevant data closer and pulling irrelevant data away. After conducting experiments on three datasets and comparing with some advanced cross-modal retrieval methods in recent years, it can be shown that our proposed DPHS method has excellent performance. The code and datasets used in this article can be obtained from https://github.com/chmy180/dphs-main .},
  archive      = {J_PAAA},
  author       = {Ge, Bin and Cheng, Mengyan and Xia, Chenxing},
  doi          = {10.1007/s10044-025-01546-2},
  journal      = {Pattern Analysis and Applications},
  month        = {12},
  number       = {4},
  pages        = {1-13},
  shortjournal = {Pattern Anal. Appl.},
  title        = {Deep hashing with prototype-aware hardness-weighted for supervised cross-modal retrieval},
  volume       = {28},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Few-shot segmentation network based on class-aware prototype fusion. <em>PAAA</em>, <em>28</em>(4), 1-13. (<a href='https://doi.org/10.1007/s10044-025-01547-1'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Most existing few-shot segmentation methods based on the support-query matching framework suffer from insufficient support information, where the limited number and coverage of annotated samples often produce prototypes that are incomplete or contaminated by background features, leading to incomplete activation of target regions in the query image and false activation of irrelevant areas. To address this issue, we propose a class-aware prototype fusion network (CAPFN) for few-shot segmentation, comprising a class-aware module (CAM) and a prototype fusion module (PFM). The CAM extracts class-specific information by jointly utilizing support features, masks, and preliminary query predictions, thereby guiding the model to attend precisely to target regions. To further mitigate semantic gap between the support and query domains, the PFM constructs a hybrid prototype by fusing support-derived and query-derived prototypes based on initial predictions. This fusion enhances prototype quality, reduces information loss, and improves the discriminative capacity of query activation. Extensive experiments on benchmark datasets (PASCAL-5 $${}^i$$ and COCO-20 $${}^i$$ ) demonstrate the superiority of our approach. Notably, with ResNet50 as the backbone, our model achieves a 2.32% improvement in mIoU over the baseline on the COCO-20 $${}^i$$ dataset. The code is available at https://github.com/ShuoWang011/CAPFN.git .},
  archive      = {J_PAAA},
  author       = {Yang, Aiping and Wang, Shuo and Sang, Zijia and Zhou, Yaran},
  doi          = {10.1007/s10044-025-01547-1},
  journal      = {Pattern Analysis and Applications},
  month        = {12},
  number       = {4},
  pages        = {1-13},
  shortjournal = {Pattern Anal. Appl.},
  title        = {Few-shot segmentation network based on class-aware prototype fusion},
  volume       = {28},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Localised ensemble learning (LEL) – A localised approach to class imbalance. <em>PAAA</em>, <em>28</em>(4), 1-25. (<a href='https://doi.org/10.1007/s10044-025-01545-3'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Class imbalance is a persistent challenge in machine learning, often degrading model performance by skewing predictions toward majority classes. Traditional approaches typically focus on global correction strategies, which may overlook important Localised irregularities within the data. These global methods may fail to adapt to the varying characteristics of individual samples, limiting their effectiveness in complex real-world scenarios.To address these limitations, we propose Localised Ensemble Learning (LEL), a novel framework that incorporates local structural information into the learning process. LEL begins by applying K-Nearest Neighbors (KNN) to assign each sample a Sample Type based on specific rules that capture neighbourhood distribution, distance-based imbalance, and sample quality. This Sample Type feature is then used to partition the dataset into distinct subsets, each of which is treated using tailored imbalance mitigation strategies. Individual models are trained on these subsets, and their predictions are integrated into a final ensemble, allowing LEL to address different forms of localised imbalance in a principled and modular way. The effectiveness of LEL is validated through a comprehensive evaluation against global correction strategies namely SMOTE, NOGAN, Cost-Sensitive Learning (CSL) and Ensemble methods, across multiple metrics including Recall, Precision, F1 Score, Kappa, G Mean and quantitatively. Statistical significance of the results is assessed using paired T-tests and Wilcoxon signed rank test. SHAP (SHapley Additive exPlanations) values are employed to analyze feature contributions, revealing the Sample Type feature as a critical determinant of model performance. Additionally, an ablation study highlights the impact of key parameters, such as the $$ k $$ value in KNN providing further insights into the robustness and adaptability of the LEL framework. Experimental results demonstrate that LEL consistently outperforms global approaches across all tested classifiers, including Random Forest, Decision Tree, XGBoost, and Naive Bayes. LEL achieves statistically significant improvements in recall and precision, underscoring its ability to handle localised forms of imbalance effectively which translates to global imbalances. The findings emphasize the importance of addressing localised data defects and leveraging features like Sample Type, which capture complex relationships and enhance predictive accuracy.},
  archive      = {J_PAAA},
  author       = {Olabisi, Olayemi and Maurya, Lalit and Bader-El-Den, Mohamed},
  doi          = {10.1007/s10044-025-01545-3},
  journal      = {Pattern Analysis and Applications},
  month        = {12},
  number       = {4},
  pages        = {1-25},
  shortjournal = {Pattern Anal. Appl.},
  title        = {Localised ensemble learning (LEL) – A localised approach to class imbalance},
  volume       = {28},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Semi-supervised graph convolutional deep embedded clustering. <em>PAAA</em>, <em>28</em>(4), 1-16. (<a href='https://doi.org/10.1007/s10044-025-01548-0'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Clustering is an important field of study in data analysis, and semi-supervised clustering, as a sub-category of clustering, has received extensive attention for significantly improving performance with only a small amount of prior information introduced. In recent years, many studies have incorporated semi-supervision into their field. Although good results have been obtained, most of these methods, when introducing additional information (for example, multi-view information), could not make full use of the given knowledge to mine intra-class similarity, and achieved suboptimal performance. For this situation, we put forward a new method of Semi-supervised Graph Convolutional Deep Embedding Clustering (SGDEC), which is a single-view algorithm. Specifically, an autoencoder network (implemented by linear layers) is pre-trained. We further propose to employ a Graph Convolutional Network (GCN) to learn the data similarity based on prior pairwise information, and make clear the training direction of the latent variable. Clustering label assignment and feature representation can be optimized by clustering loss alone. Unlike previous methods that only use pairwise information as a constraint in neural network, this architecture can make full use of pairwise information and capture similarities from it, optimizing the entire network for better clustering performance. We conduct many experiments on four popular benchmark datasets of images using the same set of learning rates, and it is shown that our SGDEC achieves a significant improvement over the new single-view algorithms, even better than the latest multi-view methods.},
  archive      = {J_PAAA},
  author       = {Cao, Chao and Li, Mengli and Li, Chungui},
  doi          = {10.1007/s10044-025-01548-0},
  journal      = {Pattern Analysis and Applications},
  month        = {12},
  number       = {4},
  pages        = {1-16},
  shortjournal = {Pattern Anal. Appl.},
  title        = {Semi-supervised graph convolutional deep embedded clustering},
  volume       = {28},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). TSDFSA: A two-stage dynamic feature selection method based on attention mechanism for multi-sensor data. <em>PAAA</em>, <em>28</em>(4), 1-14. (<a href='https://doi.org/10.1007/s10044-025-01550-6'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In multi-sensor behavior recognition, data redundancy and irrelevant features severely impair model training and optimization, leading to degraded performance. Feature selection enhances model performance by reducing dimensionality and retaining representative features. Existing dynamic selection methods iteratively update feature weights via forward propagation but are sensitive to initial parameters, which may result in early selections that reflect only local feature importance. This issue hinders the characterization of global contributions, undermines subset representativeness, and impairs model generalization. To address these challenges, we propose a Two-Stage Dynamic Feature Selection Approach (TSDFSA) based on attention mechanisms. TSDFSA adopts a two-stage feature selection strategy comprising a pre-training stage and a dynamic selection stage. The pre-training stage alleviates the impact of initial parameter settings on the feature selection process. In the dynamic selection stage, a soft–hard coupling gating mechanism is employed to adaptively adjust feature weights. This mechanism progressively refines the feature subset and avoids the limitations of one-off selection. The Hadamard product-based weighting strategy further improves the discrimination of feature contributions. Subsequently, a binary feature mask is constructed using one-hot encoding to eliminate irrelevant features. Experimental results on two multi-sensor datasets demonstrate that TSDFSA significantly enhances behavior recognition accuracy, achieving 94.10 and 91.01, respectively. Compared to other deep learning-based feature selection methods, TSDFSA achieves superior performance, confirming its effectiveness in both feature selection and behavior recognition.},
  archive      = {J_PAAA},
  author       = {Zhang, Guozhi and Dai, Gaoyang and Liu, Zixuan and Shi, Jiajia and Lin, Yuanzhe},
  doi          = {10.1007/s10044-025-01550-6},
  journal      = {Pattern Analysis and Applications},
  month        = {12},
  number       = {4},
  pages        = {1-14},
  shortjournal = {Pattern Anal. Appl.},
  title        = {TSDFSA: A two-stage dynamic feature selection method based on attention mechanism for multi-sensor data},
  volume       = {28},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Internal–external networks for image quality improvement. <em>PAAA</em>, <em>28</em>(4), 1-12. (<a href='https://doi.org/10.1007/s10044-025-01551-5'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The advent of deep learning has led to a notable improvement in super resolution (SR) performance in recent years. Because supervised deep learning SR methods often rely on training data generated by an assumed or predetermined degradation model, they perform exceptionally well in the ideal scenario, where the degradation model of a test low-resolution image does, in fact, conform with the assumed model (e.g., bicubic down-scaling) without unknown parameters like sensor noise, non-ideal point spread function (PSF), etc. However, this ideal setting is rarely suitable for actual low-resolution photographs. In this paper, we propose a hybrid method for the SR problem that considers both internal information to a given image and exterior information obtained by a pretrained SR network. Consequently, a hybrid network for SR is produced, which utilizes both information channels and adapts to the given image (and possibly its specific degradation). According to the experimental results on the standard dataset, the proposed approach has improved PSNR and SSIM. The maximum values of the two PSNR and SSIM measurements in the unknown downscaling kernel case are 29.50 and 0.93 respectively. The results of the experiments indicate that the proposed approach outperforms the state-of-the-art methods, especially when the degradation model is ambiguous or not ideal. Our source code is available at https://github.com/cvloc/FusionNet_SR .},
  archive      = {J_PAAA},
  author       = {Loc, Cu Vinh and Hai, Nguyen Thanh and Viet, Truong Xuan and Viet, Tran Hoang and Thao, Le Hoang and Viet, Nguyen Hoang},
  doi          = {10.1007/s10044-025-01551-5},
  journal      = {Pattern Analysis and Applications},
  month        = {12},
  number       = {4},
  pages        = {1-12},
  shortjournal = {Pattern Anal. Appl.},
  title        = {Internal–external networks for image quality improvement},
  volume       = {28},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Dile: A distribution-based incremental learning approach. <em>PAAA</em>, <em>28</em>(4), 1-18. (<a href='https://doi.org/10.1007/s10044-025-01552-4'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Image classification has thrived with the arrival of Deep Learning. However, the learned models are mostly static (cannot handle new classes), require a large number of images and long training times; and, in general, heavily depend on the user who defines the classes and provides labeled training data. Recently, class incremental learning has gained popularity by removing the need to retrain a model once data from new classes is available. Most incremental systems know when new classes are given and focus on not decreasing their global performance as these become available. In this paper, we describe DILE, an incremental learner based on distributions of embedded features and their comparisons using the Fréchet distance. DILE uses a limited amount of memory per class by using statistics of these feature vectors. We also introduce an incremental tree-based representation that is gradually built as more classes are known to the system, and which significantly reduces the classification times while placing semantically closer classes in the same branches of the tree. We tested the proposed approach on several databases with very competitive performance against state-of-the-art systems. Additional experiments were performed to see how DILE can work with a single image per class and automatically obtaining additional data from the Internet, and how well DILE can work if it has no knowledge if the examples are from a known or unknown class.},
  archive      = {J_PAAA},
  author       = {Morales, Eduardo F. and Herrera-Vega, Javier and Serrano-Cuevas, Jonathan and Aburto, Yareli and Sucar, L. Enrique and Escalante, Hugo J.},
  doi          = {10.1007/s10044-025-01552-4},
  journal      = {Pattern Analysis and Applications},
  month        = {12},
  number       = {4},
  pages        = {1-18},
  shortjournal = {Pattern Anal. Appl.},
  title        = {Dile: A distribution-based incremental learning approach},
  volume       = {28},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Exacter set-level nonlocal attention mechanism based on learning weights for image denoising. <em>PAAA</em>, <em>28</em>(4), 1-15. (<a href='https://doi.org/10.1007/s10044-025-01553-3'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, deep learning models based on non-local attention (NLA) have achieved remarkable success in image denoising. The success of NLA stems from its ability to capture long-range interactions guided by both the data itself and learnable parameters within the attention mechanism. Building on the classical insight from traditional non-local filtering methods, it has been observed that more accurate calculation of interactions or similarities between tokens can significantly improve denoising performance. However, existing non-local attention networks designed for denoising have failed to consider the trustworthiness of these interaction computations. In this paper, motivated by the statistic principles underlying similarity calculation, we propose a exacter set-level non-local attention based on the learning weights (ESNLA). This approach leverages similar tokens within the local neighborhood to mitigate the impact of noise and enhance the robustness of similarity calculations under noisy conditions. Designed as an efficient modular component, ESNLA can be seamlessly integrated into any deep convolutional architecture for image denoising. Ablation studies validate the superiority of ESNLA over existing NLA modules. By embedding multiple ESNLA blocks into a ResNet backbone, we further construct a deep ESNLA Network (ESNLANet). Extensive experiments demonstrate that ESNLANet achieves highly competitive denoising performance, outperforming a wide range of state-of-the-art methods.},
  archive      = {J_PAAA},
  author       = {Chen, Dai-Qiang and Chen, Ying-Chun},
  doi          = {10.1007/s10044-025-01553-3},
  journal      = {Pattern Analysis and Applications},
  month        = {12},
  number       = {4},
  pages        = {1-15},
  shortjournal = {Pattern Anal. Appl.},
  title        = {Exacter set-level nonlocal attention mechanism based on learning weights for image denoising},
  volume       = {28},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). An efficient framework for text-to-image retrieval using complete feature aggregation and cross-knowledge conversion. <em>PAAA</em>, <em>28</em>(4), 1-15. (<a href='https://doi.org/10.1007/s10044-025-01554-2'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Text-driven person retrieval aims to find a particular person within an array of images using a natural language description of that individual. The main challenge in this task lies in calculating a similarity score that measures the likeness between the person’s image and the given description. This procedure involves uncovering the intricate, underlying connections between various levels of image sub-regions and textual phrases. Previous efforts have sought to tackle this issue by utilizing individual pre-trained models for visual and textual data to extract features. However, these methods lack essential alignment capabilities regarding efficient multimodal data matching. Moreover, these approaches rely on prior information to investigate explicit part alignments, potentially causing the disruption of information within the same modality. To increase the performance of existing state-of-the-art text-to-image retrieval systems, we proposed a novel technique aimed at enhancing the effectiveness of the text-image pedestrian matching pipeline, referred to as Complete Feature Aggregation and Cross-Knowledge Conversion (CFA-CKC), which integrates diverse supervisory signals from various textual and visual perspectives. Our proposed algorithm achieves new state-of-the-art results on all three public datasets, including CUHK-PEDES, ICFG-PEDES, and RSTPReid. Experimental results show that our CFA-CKC outperforms the state-of-the-art methods by up to 6.0% in rank-1 accuracy. The code and pre-trained models are available at this link https://github.com/AIVIETNAMResearch/Pedestrian-Matching .},
  archive      = {J_PAAA},
  author       = {Ngo, Bach Hoang and An, Minh-Hung and Anh, Khoa Nguyen Tho and Bui, Minh-Duc and Dinh, Quang-Vinh and Nguyen, Vinh Dinh},
  doi          = {10.1007/s10044-025-01554-2},
  journal      = {Pattern Analysis and Applications},
  month        = {12},
  number       = {4},
  pages        = {1-15},
  shortjournal = {Pattern Anal. Appl.},
  title        = {An efficient framework for text-to-image retrieval using complete feature aggregation and cross-knowledge conversion},
  volume       = {28},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Future occupancy guidance under multi-view collaboration for trajectory prediction. <em>PAAA</em>, <em>28</em>(4), 1-15. (<a href='https://doi.org/10.1007/s10044-025-01555-1'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Vehicle trajectory prediction is a critical component of autonomous driving technology, aiming to predict plausible future trajectories for surrounding agents in dynamic traffic scenarios. However, recent approaches merely fit certain elements of interest using the agent’s local poses, which is insufficient for effectively perceiving large-scale static maps. Meanwhile, some models rely on historical or current map states, assigning excessive attention to traversed road segments. This leads to severe limitations in vehicles’ autonomous decision-making capabilities. Furthermore, single-stage prediction fails to evaluate intersecting future spatial distributions, consequently inducing hazardous behaviors. To address these challenges, this paper proposes a Future Occupancy Guidance(FOG) framework based on multi-view collaboration. Firstly, the model leverages edge relationships to construct spatio-temporal interactions among elements across multiple viewpoint graphs. Then, we generate future occupancy markers along the driving direction and adaptively shift the map query perspective to accommodate the distance disparities caused by agents traveling at different speeds, thereby providing road-level guidance. Finally, we design a refinement module to further correct the rough predicted trajectories with collision risks through multi-head attention. The experimental results on Argoverse 1 and Argoverse 2 motion forecasting benchmarks demonstrate that our FOG achieves outstanding performance in reducing both FDE and MR metrics. The code implementation is available at: https://github.com/alien-HL/FOG.git},
  archive      = {J_PAAA},
  author       = {Sang, Haifeng and Huang, Lai},
  doi          = {10.1007/s10044-025-01555-1},
  journal      = {Pattern Analysis and Applications},
  month        = {12},
  number       = {4},
  pages        = {1-15},
  shortjournal = {Pattern Anal. Appl.},
  title        = {Future occupancy guidance under multi-view collaboration for trajectory prediction},
  volume       = {28},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Point-pad: Point cloud upsampling with kernel representation and attention. <em>PAAA</em>, <em>28</em>(4), 1-13. (<a href='https://doi.org/10.1007/s10044-025-01558-y'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Sensor generated point clouds are often noisy and sparse, which pose challenges in various downstream tasks. Recent approaches have tackled the problem through various upsampling approaches. Existing methods suffer from outlier and variable point cloud density leading to loss in object geometry. This work introduces Point-PAD, an architecture which utilizes kernel points and positional encodings in order to encode the local geometry, which is then processed by a multi-head attention block for adjusting predicted points with the entire architecture supported by a density aware loss function. The architecture, when tested on the PU-GAN dataset, achieves state-of-the-art results. The model outperformed the state-of-the-art by 3.22% and was able to upsample points even at sharp curves while preserving the geometry. We also report results on one of the first outdoor dataset comprising real life LiDAR (Light Detection and Ranging) scans of outdoor scenes for this task. Our model outperforms the existing work, proving its generalization, which is crucial for applications related to digital twins and autonomous systems. The code is available at: https://github.com/geoai4cities/PointPAD},
  archive      = {J_PAAA},
  author       = {Verma, Sameer and Kumar, Vaibhav},
  doi          = {10.1007/s10044-025-01558-y},
  journal      = {Pattern Analysis and Applications},
  month        = {12},
  number       = {4},
  pages        = {1-13},
  shortjournal = {Pattern Anal. Appl.},
  title        = {Point-pad: Point cloud upsampling with kernel representation and attention},
  volume       = {28},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Feature selection based on multimodal multi-objective particle swarm optimization and prior information. <em>PAAA</em>, <em>28</em>(4), 1-26. (<a href='https://doi.org/10.1007/s10044-025-01556-0'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Due to the conflicting objectives of classification accuracy and selected features size, feature selection is typically approached as a multi-objective optimization problem. However, traditional methods often overlook the inherent multimodal nature of feature selection. Additionally, these methods might ignore the importance of filter-based prior knowledge in forming equivalent feature subsets, weakening the ability to search for such subsets. An improved feature selection algorithm, named NRMOPSO, is proposed in this study, which is based on multimodal multi-objective particle swarm optimization and integrates a niche method with ReliefF. Initially, the Incrementally Expanding Niche Strategy (IENS) adjusts niche size for comprehensive initial exploration. Subsequently, the ReliefF algorithm evaluates feature importance, incorporating ReliefF-based prior information into the particle search to include significant unselected features while retaining essential ones. Experimental results on 14 UCI datasets indicate that the proposed algorithm effectively identifies multiple equivalent feature subsets and, on high-dimensional datasets, achieves smaller feature subsets without compromising classification accuracy when compared with five classical and advanced multimodal multi-objective optimization algorithms.},
  archive      = {J_PAAA},
  author       = {Liu, Wenkai and Ling, Qinghua and Han, Fei and Han, Henry and Shi, Jinlong},
  doi          = {10.1007/s10044-025-01556-0},
  journal      = {Pattern Analysis and Applications},
  month        = {12},
  number       = {4},
  pages        = {1-26},
  shortjournal = {Pattern Anal. Appl.},
  title        = {Feature selection based on multimodal multi-objective particle swarm optimization and prior information},
  volume       = {28},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). TrafficDiff: Diffusion model based adversarial traffic scenario controllable generation for autonomous driving robust evaluation. <em>PAAA</em>, <em>28</em>(4), 1-18. (<a href='https://doi.org/10.1007/s10044-025-01561-3'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As foundation models (FMs) are increasingly applied in safety-critical domains such as autonomous driving, their ability to handle rare, ambiguous, or adversarial conditions becomes essential for ensuring cognitive robustness. Generative AI offers a promising path for testing such capabilities by synthesizing diverse and realistic traffic scenarios. As a prominent class of generative models, diffusion models are known for their strong diversity, yet controllable generation remains a key challenge. To address this, we propose a controllable scenario generation framework based on diffusion models. First, a dynamic spatiotemporal fusion encoding mechanism integrates contextual factors (e.g., road layout, vehicle types) to enhance realism. To enhance diversity, we introduce a global–local optimizer that guides scenario generation while preserving physical and statistical consistency. To generate safety-critical long-tail scenarios, we design an adversarial induction method that enhances scenario criticality, while a system dynamics model improves long-tail scenario generation. Finally, a mechanism-based scenario filter ensures the safety and compliance of generated scenarios by eliminating unrealistic samples. We validate our method on benchmark datasets and real-vehicle tests. Compared to existing SOTA methods, traffic scenario diversity is enhanced by 6–8 times on average. In real-vehicle evaluations, TrafficDiff increase the collision rate by 25.5% and leads much mission failure, effectively challenging system robustness. This approach provides a scalable solution for virtual scenario validation, driving advancements in autonomous driving safety assessment.Our code of TrafficDiff is available at https://github.com/Moresweet/TrafficDiff .},
  archive      = {J_PAAA},
  author       = {Bai, Xuesong and Li, Hongbo and Dong, Peng and Tian, Changhang and Fei, Yang and Zhang, Jinchuan and Ren, Yilong and Li, Aoyong},
  doi          = {10.1007/s10044-025-01561-3},
  journal      = {Pattern Analysis and Applications},
  month        = {12},
  number       = {4},
  pages        = {1-18},
  shortjournal = {Pattern Anal. Appl.},
  title        = {TrafficDiff: Diffusion model based adversarial traffic scenario controllable generation for autonomous driving robust evaluation},
  volume       = {28},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Lightweight multi-scale vehicle object detection algorithm based on optimized YOLOv8. <em>PAAA</em>, <em>28</em>(4), 1-24. (<a href='https://doi.org/10.1007/s10044-025-01562-2'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In intelligent transportation surveillance scenarios, existing high-precision vehicle detection models commonly suffer from insufficient real-time performance due to their large parameter sizes and high computational complexity. Conversely, while lightweight models exhibit lower computational demands, they often fail to meet the accuracy requirements of practical applications. To overcome this fundamental trade-off between accuracy and efficiency, this paper proposes an optimized lightweight multi-scale vehicle detection model based on YOLOv8, termed SGS-YOLOv8. Its core innovation lies in the synergistic integration of multiple lightweight techniques into the YOLOv8 architecture, specifically tailored for vehicle detection tasks: (1) Within the backbone network, we innovatively integrate and optimize the SCDown module alongside an improved GCSPR module, significantly enhancing the parameter efficiency of multi-scale feature extraction. (2) In the neck structure, we creatively design a cascaded lightweight architecture composed of GSConv and VoV-GSCSP, effectively mitigating the performance degradation in feature fusion typically encountered during traditional neck lightweighting. (3) Within the detection head, we introduce the computationally efficient Multi-Head Self-Attention (MHSA) mechanism combined with feature screening and weighting strategies, enabling adaptive focus on critical target features and enhancing discriminative capability in complex scenes. Comprehensive experiments conducted on three benchmark datasets—BIT-Vehicle, UA-Detrac, and BDD100K—demonstrate that the proposed SGS-YOLOv8 model achieves significant reductions of 36.90% in parameters, 45.68% in computational load (GFLOPs), and 33.39% in model size compared to the baseline YOLOv8 model, while maintaining average precision with a marginal decrease of no more than 0.2%. These results effectively validate the superiority of the proposed synergistic lightweighting strategy in resolving the accuracy-efficiency trade-off dilemma, offering an efficient solution for real-time applications on edge computing platforms.},
  archive      = {J_PAAA},
  author       = {Duan, Hanqing and Zhang, Yixuan and Zhu, Songhao and Liang, Zhiwei},
  doi          = {10.1007/s10044-025-01562-2},
  journal      = {Pattern Analysis and Applications},
  month        = {12},
  number       = {4},
  pages        = {1-24},
  shortjournal = {Pattern Anal. Appl.},
  title        = {Lightweight multi-scale vehicle object detection algorithm based on optimized YOLOv8},
  volume       = {28},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Integrating spatial and frequency information for under-display camera image restoration. <em>PAAA</em>, <em>28</em>(4), 1-16. (<a href='https://doi.org/10.1007/s10044-025-01549-z'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Under-Display Camera (UDC) houses a digital camera lens under a display panel. However, UDC introduces complex degradations such as noise, blur, decrease in transmittance, and flare. Despite the remarkable progress, previous research on UDC mainly focuses on eliminating diffraction in the spatial domain and rarely explores its potential in the frequency domain. In this paper, we revisit the UDC degradations in the Fourier space and figure out intrinsic frequency priors that imply the presence of the flares. Based on these observations, we propose SFIM, a novel multi-level deep neural network that efficiently restores UDC-distorted images by integrating local and global (the collective contribution of all points in the image) information. SFIM uses CNNs to capture fine-grained local details and FFT-based models to extract global patterns. The network comprises a spatial domain block (SDB), a frequency domain block (FDB), and an attention-based multi-level integration block (AMIB). Specifically, SDB focuses more on detailed textures such as noise and blur, FDB emphasizes irregular texture loss in extensive areas such as flare, and AMIB employs cross-domain attention to selectively integrate complementary spatial and frequency features across multiple levels, enhancing detail recovery and mitigating irregular degradations like flare. SFIM’s superior performance over state-of-the-art approaches is demonstrated through rigorous quantitative and qualitative assessments. Our source code is publicly available at: https://github.com/mcrl/SFIM .},
  archive      = {J_PAAA},
  author       = {Ahn, Kyusu and Kim, Jinpyo and Park, Chanwoo and Kim, JiSoo and Lee, Jaejin},
  doi          = {10.1007/s10044-025-01549-z},
  journal      = {Pattern Analysis and Applications},
  month        = {12},
  number       = {4},
  pages        = {1-16},
  shortjournal = {Pattern Anal. Appl.},
  title        = {Integrating spatial and frequency information for under-display camera image restoration},
  volume       = {28},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). LightAudioCNN: A novel deep neural network for audio-based parkinson’s disease recognition and subtype differentiation. <em>PAAA</em>, <em>28</em>(4), 1-21. (<a href='https://doi.org/10.1007/s10044-025-01524-8'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This study introduces a Deep Neural Network architecture called LightAudioCNN. Its main purpose is to examine cord vibration patterns to improve the diagnosis of Parkinsons’ disease (PD) and differentiate it from similar conditions. LightAudioCNN represents a step in developing more objective and precise diagnostic tools, especially crucial in the early stages of PD, unlike the conventional symptom-based methods known for their arbitrary and unreliable nature. By analyzing vowel sounds (“a” and “i”) from a dataset of 83 participants, this study evaluates LightAudioCNN’s effectiveness while ensuring the reliability of its outcomes using a patient separation method. LightAudioCNN demonstrates high diagnostic accuracy and efficiency, achieving an Area Under the Curve (AUC) score of 0.99 in binary classification tasks and 0.96 in multiclass classification tasks with corresponding accuracy rates of 95% and 81%. These results were obtained through comparisons with Deep Neural Networks trained on Mel Spectrograms and contemporary transformer models processing Mel spectrograms or raw audio data. Additionally, the application of LightAudioCNN to the Italian Parkinson Speech dataset further substantiates its high diagnostic capability. On this dataset, LightAudioCNN achieved a mean accuracy of 97.69%, a precision of 97.88%, and an AUC score of 0.9873, illustrating its ability to capture complex speech patterns associated with Parkinson’s disease. The model’s performance was in line with the other deep learning models. Furthermore, the study highlights the versatility of LightAudioCNN beyond Parkinsons’ disease by proving its superiority in identifying COVID-19 by analyzing breath patterns and cough sounds. In this comparison, LightAudioCNN surpasses deep learning and traditional machine learning models by achieving a mean accuracy of 78.81% in the same scenarios. This proves the model’s potential for quickly and accurately diagnosing COVID-19, demonstrating its relevance across conditions. The model also has a small footprint of about 3.1 M parameters, which is about 7 times less than standard computer vision architectures such as ResNet50, allowing the integration of this technology locally into smartphone applications with the aim of managing and treating not just Parkinson’s’ Disease but also emerging health threats, like COVID-19.},
  archive      = {J_PAAA},
  author       = {Dentamaro, Vincenzo and Gattulli, Vincenzo and Impedovo, Donato},
  doi          = {10.1007/s10044-025-01524-8},
  journal      = {Pattern Analysis and Applications},
  month        = {12},
  number       = {4},
  pages        = {1-21},
  shortjournal = {Pattern Anal. Appl.},
  title        = {LightAudioCNN: A novel deep neural network for audio-based parkinson’s disease recognition and subtype differentiation},
  volume       = {28},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A hybrid marker extraction method by gradient and spectral features for marker-controlled watershed segmentation. <em>PAAA</em>, <em>28</em>(4), 1-21. (<a href='https://doi.org/10.1007/s10044-025-01557-z'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Image segmentation is fundamental in various domains, including medical imaging, satellite analysis, and microscopic studies. The watershed transform is a widely adopted method for delineating object boundaries; however, its susceptibility to noise and over-segmentation limits its effectiveness. This study proposes two enhanced strategies for marker generation in marker-controlled watershed segmentation. The first, Dual-Channel Morphological and Gradient Spectral (DMGS), integrates morphological and spectral features to generate more informative markers and reduce over-segmentation. Building upon this, the second method, Hybrid Marker Extraction by Gradient and Spectral features (HMEGS), refines the process by eliminating redundant morphological markers and introducing an adaptive thresholding mechanism based on gradient analysis. HMEGS further enhances boundary precision and segmentation consistency by preserving dominant edge structures and improving region homogeneity. Experimental evaluations on the BSDS500 dataset demonstrate that HMEGS achieves a 0.17% reduction in Variation of Information (VI), a 1.4% increase in Covering (CV), and a 0.5% improvement in the Probabilistic Rand Index (PRI) compared to state-of-the-art methods. These results highlight the robustness and accuracy of the proposed framework across diverse imaging scenarios.},
  archive      = {J_PAAA},
  author       = {Hossaini, S. B. and Mousavi, S. M. and Bavafa Toosi, A.},
  doi          = {10.1007/s10044-025-01557-z},
  journal      = {Pattern Analysis and Applications},
  month        = {12},
  number       = {4},
  pages        = {1-21},
  shortjournal = {Pattern Anal. Appl.},
  title        = {A hybrid marker extraction method by gradient and spectral features for marker-controlled watershed segmentation},
  volume       = {28},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Attribute graph clustering via dual-driven network with dynamic structure optimization and prototype contrast. <em>PAAA</em>, <em>28</em>(4), 1-19. (<a href='https://doi.org/10.1007/s10044-025-01560-4'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep attributed graph clustering aims to group nodes into semantically meaningful clusters by leveraging both node attributes and complex topological structures. However, most existing methods rely on static adjacency matrices and are limited by the local aggregation scope of graph neural networks. This leads to noisy structural features that fail to capture global cluster structures, which in turn degrades clustering performance. To address these challenges, we propose a dual-driven network that enhances the robustness and discriminability of graph clustering through dynamic structure optimization and prototype contrastive learning. The model adopts a co-training dual-branch framework that integrates attribute representations from an autoencoder with structural features from a graph autoencoder, achieving unified optimization from low-level features to high-order topology. Meanwhile, the graph structure is iteratively updated based on node features and current topology, enabling adaptive reconstruction and noise suppression. A momentum-based prototype contrastive module is further introduced to continuously align cluster prototypes in the latent space, improving intra-cluster compactness and inter-cluster separability. The entire model is trained under a unified self-supervised objective. Finally, traditional clustering algorithms are applied to the learned embeddings for node partitioning. Experiments on five benchmark datasets show that our method consistently outperforms 13 state-of-the-art clustering approaches. In particular, it achieves a 2% clustering accuracy improvement over the best existing method on the DBLP dataset. Our code is available at https://github.com/aaaaaaa1wqdqw/DynStruct-Cluster .},
  archive      = {J_PAAA},
  author       = {Tan, Shuqiu and Li, Huibing},
  doi          = {10.1007/s10044-025-01560-4},
  journal      = {Pattern Analysis and Applications},
  month        = {12},
  number       = {4},
  pages        = {1-19},
  shortjournal = {Pattern Anal. Appl.},
  title        = {Attribute graph clustering via dual-driven network with dynamic structure optimization and prototype contrast},
  volume       = {28},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A variational autoencoder for probabilistic non-negative matrix factorisation. <em>PAAA</em>, <em>28</em>(4), 1-10. (<a href='https://doi.org/10.1007/s10044-025-01563-1'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We introduce and demonstrate the variational autoencoder (VAE) for probabilistic non-negative matrix factorisation (PAE-NMF). We design a network which can perform non-negative matrix factorisation (NMF) and add in aspects of a VAE to make the coefficients of the latent space probabilistic. By restricting the weights in the final layer of the network to be non-negative and using the non-negative Weibull distribution we produce a probabilistic form of NMF which allows us to generate new data and find a probability distribution that effectively links the latent and input variables. Our approach uses a minimum description length methodology to provide a method for achieving automatic regularisation; as it is designed using neural networks it can leverage deep learning frameworks for automatic differentiation, fast gradient descent algorithms and GPU support. We demonstrate the effectiveness of PAE-NMF on three heterogeneous datasets: images, financial time series and genomic.},
  archive      = {J_PAAA},
  author       = {Squires, Steven and Prügel Bennett, Adam and Niranjan, Mahesan},
  doi          = {10.1007/s10044-025-01563-1},
  journal      = {Pattern Analysis and Applications},
  month        = {12},
  number       = {4},
  pages        = {1-10},
  shortjournal = {Pattern Anal. Appl.},
  title        = {A variational autoencoder for probabilistic non-negative matrix factorisation},
  volume       = {28},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Adaptive multi-scale framework for incremental semantic segmentation. <em>PAAA</em>, <em>28</em>(4), 1-18. (<a href='https://doi.org/10.1007/s10044-025-01564-0'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Class-incremental semantic segmentation (CISS) aims to progressively learn new categories or adapt to novel environments for semantic segmentation tasks without requiring full model retraining, while ensuring that segmentation performance on previously learned classes is preserved. Most existing class-incremental semantic segmentation methods mitigate the issues of catastrophic forgetting and background shift through strategies such as pseudo-labeling and knowledge distillation. Although existing methods have achieved certain success, they still have some limitations: (1) they ignore contextual relationships between tasks, resulting in a lack of holistic learning; (2) knowledge from the old model is directly transferred to the new model, even though not all of it is beneficial for the new model. This paper proposes a novel Adaptive Multi-scale Incremental Semantic Segmentation (AMIS) framework that incorporates a Global Attention Block (GAB), designed to capture contextual information across different tasks and address these challenges. In addition, this work employs an Adaptive Multi-scale Distillation (AMD) module to perform multi-scale pooling and fusion on features extracted by the decoder, enabling the model to adaptively focus on informative representations. Moreover, a Background Compensation Strategy (BCS) is applied to enhance the model’s ability to distinguish ambiguous boundaries between background and target classes. Extensive experiments on the Pascal VOC and ADE20K datasets demonstrate that the proposed method effectively mitigates catastrophic forgetting and background shift, outperforming state-of-the-art approaches in most scenarios. The code is available at https://github.com/ZXCV-7/AMIS .},
  archive      = {J_PAAA},
  author       = {Qu, ShaoJun and Xie, DongLin},
  doi          = {10.1007/s10044-025-01564-0},
  journal      = {Pattern Analysis and Applications},
  month        = {12},
  number       = {4},
  pages        = {1-18},
  shortjournal = {Pattern Anal. Appl.},
  title        = {Adaptive multi-scale framework for incremental semantic segmentation},
  volume       = {28},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Fpa-yolov8s: An efficient small object detection algorithm for drone aerial imagery. <em>PAAA</em>, <em>28</em>(4), 1-16. (<a href='https://doi.org/10.1007/s10044-025-01566-y'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Object detection in drone (UAV) aerial imagery remains challenging due to small target sizes, dense distributions, varied viewpoints, and complex backgrounds, leading to the limited effectiveness of existing algorithms. To address these issues, we propose FPA-YOLOv8s (Feature Patch-Aware YOLOv8-small), an enhanced lightweight detector based on YOLOv8s. FPA-YOLOv8s introduces three key improvements: (i) a C2f_PPA module that strengthens local and global feature extraction through Patch-Aware Attention and improves multi-scale feature fusion; (ii) a MRSF-Neck that incorporates high-resolution feature layers and reinforced feature aggregation to better adapt to complex scenes and multi-scale targets; and (iii) an Adaptive DySample Module (ADM) for efficient upsampling, leveraging lightweight channel attention and range constraints to enhance small-object perception. Extensive experiments on two public UAV datasets demonstrate the effectiveness of the proposed approach. On VisDrone2019, FPA-YOLOv8s achieves a 7% increase in detection accuracy compared to YOLOv8s with negligible parameter growth. On UAVDT, it obtains 3.6% higher mAP@50 and 4.1% higher mAP@50–95, while maintaining competitive model size and computational cost. These results confirm that FPA-YOLOv8s achieves a favorable balance between accuracy and efficiency, offering a practical solution for small-object detection in UAV applications. The source code is available at https://github.com/lc1768266731/FPA-Yolov8s/tree/main .},
  archive      = {J_PAAA},
  author       = {Nan, Hai and Li, Cheng},
  doi          = {10.1007/s10044-025-01566-y},
  journal      = {Pattern Analysis and Applications},
  month        = {12},
  number       = {4},
  pages        = {1-16},
  shortjournal = {Pattern Anal. Appl.},
  title        = {Fpa-yolov8s: An efficient small object detection algorithm for drone aerial imagery},
  volume       = {28},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Image-text relationship-based feature interaction networks for multimodal aspect-based sentiment analysis. <em>PAAA</em>, <em>28</em>(4), 1-16. (<a href='https://doi.org/10.1007/s10044-025-01568-w'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The multimodal aspect-based sentiment analysis (MABSA) task aims to determine the sentiment polarity associated with each specific aspect term mentioned in the text by integrating multimodal information, such as textual and visual data. This task is of significant importance for deeper comprehension of user sentiments and opinions. However, existing research suffers from the following limitation: (1) Although a correspondence exists between visual regions and words, current methods struggle to accurately learn the correct region-word alignment. (2) 2D images contain numerous features irrelevant to sentiment analysis, and existing methods lack an effective denoising mechanism. To tackle the mentioned issues, this paper provides a novel image-text relationship-based feature interaction network (ITRIN). Specifically, this paper introduces a cross-modal alignment correction module to learn the correspondence between region-level visual features and textual tokens and employs an adaptive gating mechanism to mitigate the adverse effects of misaligned region-word pairs. Furthermore, by incorporating global contextual information, we design a gating mechanism based on the probability scores of the image-text relationship (BRGate) in the final feature fusion layer, which enables deep filtering and effective fusion of multimodal features. The proposed model, ITRIN, achieves state-of-the-art performance on two benchmark datasets, Twitter-2015 and Twitter-2017. Extensive ablation studies further validate its superiority and effectiveness. The source code is publicly released at https://github.com/MHTransKanba/ITRIN .},
  archive      = {J_PAAA},
  author       = {Ma, Hao and Huan, Hai},
  doi          = {10.1007/s10044-025-01568-w},
  journal      = {Pattern Analysis and Applications},
  month        = {12},
  number       = {4},
  pages        = {1-16},
  shortjournal = {Pattern Anal. Appl.},
  title        = {Image-text relationship-based feature interaction networks for multimodal aspect-based sentiment analysis},
  volume       = {28},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). CMPFNet: Semantic segmentation network for cross-modal phased fusion in extreme light scenes. <em>PAAA</em>, <em>28</em>(4), 1-21. (<a href='https://doi.org/10.1007/s10044-025-01569-9'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Reliable perception systems are crucial for the stable operation of applications such as autonomous driving and night-time security under extreme lighting conditions, including darkness and low light. The environmental perception modules within these systems typically rely on the precise segmentation of single-frame images. However, Existing RGB-infrared segmentation models often overlook modality discrepancies in feature fusion and suffer from detail loss due to repeated downsampling. To address these issues, we propose CMPFNet (Cross-modal Phased Fusion Net), a semantic segmentation model designed for extreme lighting conditions. CMPFNet employs a two-branch structure to capture local and global features, enhancing the segmentation accuracy of small targets. We introduce a feature rectification and fusion module to mitigate inter-modality gaps and improve feature interaction via multimodal rectification and attention mechanisms. A feature recovery module is introduced to restore fine details lost in deep layers, while a multi-scale decoding module reduce noise interference across scales. Experiments on MFNet and PST900 datasets demonstrate that CMPFNet outperforms mainstream methods, achieving a 60.6% mIoU on MFNet, a 1.7% improvement over the previous best method, validating its effectiveness. This approach provides a technical foundation for robust environmental perception in the above-mentioned scenarios. Code is available at https://github.com/tzh0831/CMPFNet .},
  archive      = {J_PAAA},
  author       = {Han, Jing and Tian, Zihe and Gao, Koukou and Lyu, Xueqiang},
  doi          = {10.1007/s10044-025-01569-9},
  journal      = {Pattern Analysis and Applications},
  month        = {12},
  number       = {4},
  pages        = {1-21},
  shortjournal = {Pattern Anal. Appl.},
  title        = {CMPFNet: Semantic segmentation network for cross-modal phased fusion in extreme light scenes},
  volume       = {28},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Enhancement comparison of laplace kernelized piecewise regression-based progressive generative adversarial network for latent fingerprint. <em>PAAA</em>, <em>28</em>(3), 1-20. (<a href='https://doi.org/10.1007/s10044-025-01491-0'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Latent fingerprints are partial or incomplete fingerprints typically found at crime scenes. This enhancement is a critical step in identifying individuals from fingerprints left at crime scenes. It involves applying various image processing algorithms to improve the quality and contrast of the fingerprint patterns, making them more distinguishable from the background. Many deep learning and machine learning techniques have been applied to this problem due to their ability to automatically learn complex patterns and features from latent fingerprint images. However, accurate matching and detection with minimal time consumption remain major challenges. To enhance the latent fingerprint image, a novel Laplace Kernelized Piecewise Regression-based Progressive Generative Adversarial Network (LKPR-PGAN) is employed in five different processes, namely image acquisition, preprocessing, region of interest (ROI) segmentation, minutiae feature extraction, and matching. First, latent fingerprint images are collected from the dataset during the image acquisition phase. The images then undergo preprocessing to remove noise using a Laplace kernelized enhanced frost filtering technique. Following this, ROI segmentation is performed to extract the fingerprint image using the Rand index diagonal proximity method. Next, the Progressive Generative Adversarial Network (GAN) is employed for minutiae feature extraction and matching. Piecewise regression is used to extract different minutiae features from the image. The Hamann similarity coefficient is used for minutiae feature matching with the ground truth. The accurate matching results significantly improve latent fingerprint identification performance. Experimental results are described that the LKPR-PGAN method achieves higher accuracy in latent fingerprint identification with minimal time consumption compared to existing methods.},
  archive      = {J_PAAA},
  author       = {Muthusamy, Dharmalingam and Muniyappan, Saritha},
  doi          = {10.1007/s10044-025-01491-0},
  journal      = {Pattern Analysis and Applications},
  month        = {9},
  number       = {3},
  pages        = {1-20},
  shortjournal = {Pattern Anal. Appl.},
  title        = {Enhancement comparison of laplace kernelized piecewise regression-based progressive generative adversarial network for latent fingerprint},
  volume       = {28},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Real-time person re-identification and tracking on edge devices with distributed optimization. <em>PAAA</em>, <em>28</em>(3), 1-22. (<a href='https://doi.org/10.1007/s10044-025-01492-z'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper presents an efficient real-time person re-identification (ReID) and pedestrian tracking solution optimized for resource-constrained edge devices in multi-camera surveillance. Our key contribution is a hybrid distributed architecture that offloads lightweight detection tasks (using YOLOv10n) to edge devices, while a centralized server handles advanced feature extraction (OSNet) and robust identity tracking (ByteTrack). To improve efficiency, we integrate adaptive frame skipping on edge devices and parallel batch processing on the server. Semantic-enhanced embeddings and a memory-based retrieval mechanism improve ReID performance in crowded scenes. Additionally, we employ Apache Kafka for efficient load balancing and video stream management. Experimental results on CUHK03 and Penn-Fudan demonstrated high accuracy while maintaining real-time performance on limited-resource hardware (2 vCPU, 4 GB RAM, and Jetson Nano). These results make our approach a practical solution for real-world surveillance applications in crowded environments. Our code is available at: https://github.com/2uanDM/reid-pipeline .},
  archive      = {J_PAAA},
  author       = {Dang, Tuan Linh and Hoang, Minh Hoang and Ngo, Viet Anh and Duong, Minh Quan and Ha, Hoang Hiep and Nguyen, The An and Le, Hoang},
  doi          = {10.1007/s10044-025-01492-z},
  journal      = {Pattern Analysis and Applications},
  month        = {9},
  number       = {3},
  pages        = {1-22},
  shortjournal = {Pattern Anal. Appl.},
  title        = {Real-time person re-identification and tracking on edge devices with distributed optimization},
  volume       = {28},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Cross-attention-based hybrid ViT-CNN fusion network for action recognition in visible and infrared videos. <em>PAAA</em>, <em>28</em>(3), 1-15. (<a href='https://doi.org/10.1007/s10044-025-01493-y'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Human action recognition (HAR) in videos is a critical task in computer vision, but traditional methods relying solely on visible (RGB) data face challenges in low-light or occluded scenarios. Infrared (IR) imagery offers robustness in such conditions, yet effectively fusing IR and visible modalities remains an open problem. To address this, we propose HVCCA-Net, Hybrid ViT-CNN Cross-Attention Network that integrates the strengths of both modalities. Our framework consists of three key modules: (1) a video pre-processing (VPP) module that extracts IR/visible frames, stacked dense flow, and residual images; (2) an intra-modality spatio-temporal feature learning (ISTFL) module combining Inflated 3D CNN (I3D), Group Propagation Vision Transformer (GPViT), and Bi-directional Long Short-Term Memory (BiLSTM) to capture local and global features; and (3) a cross-modality multi-head attention fusion (CMHAF) module that dynamically aligns and fuses complementary features. Experiments on the Infrared-Visible dataset demonstrate state-of-the-art performance (96.0% accuracy), outperforming existing methods. The results highlight the effectiveness of our cross-attention mechanism in leveraging multimodal data for robust action recognition. The code and datasets of the proposed method are available at https://github.com/jvdgit/IR-Vis-Action-Recognition.git},
  archive      = {J_PAAA},
  author       = {Imran, Javed and Gupta, Himanshu},
  doi          = {10.1007/s10044-025-01493-y},
  journal      = {Pattern Analysis and Applications},
  month        = {9},
  number       = {3},
  pages        = {1-15},
  shortjournal = {Pattern Anal. Appl.},
  title        = {Cross-attention-based hybrid ViT-CNN fusion network for action recognition in visible and infrared videos},
  volume       = {28},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Reversible data hiding in encrypted images using chaos theory and chinese remainder theorem. <em>PAAA</em>, <em>28</em>(3), 1-25. (<a href='https://doi.org/10.1007/s10044-025-01495-w'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Reversible data hiding in encrypted images (RDHEI) has recently garnered attention for embedding secret data within anonymized images, overcoming the spatial-correlation dependency of traditional RDH methods. In particular, this paper introduces a new RDHEI scheme based on chaos theory and Chinese Remainder Theorem (CRT). Initially, the host image is encrypted using a chaos-based symmetric cipher that adheres to strong cryptographic principles and demonstrates robust security features. Next, bits from the secret image are XORed with a random bit sequence generated by a chaotic function, initiated by a data-hiding key. Subsequently, a new data embedding method, based on pixel fusion and the CRT, is introduced. This data-hiding and encoding process, which incorporates two encrypted image bytes and one secret image bit, maintains a consistent embedding capacity of 0.5 bits per pixel (BPP), independent of the host image's complexity, while ensuring reversibility. At the receiving end, both the original host image and the embedded secret data can be flawlessly recovered. Furthermore, the proposed scheme enables extraction of the secret image based solely on the data-hiding key, without necessitating the decryption key, thereby achieving the method's semi-separability property. Experimental results indicate that the proposed scheme, which offers a fixed embedding capacity and induces only a minimal increase in the watermarked-to-original image size ratio (WOSR), upholds essential theoretical values necessary for encryption and withstands common cryptographic attacks. Moreover, as a fully reversible and semi-separable technique, the proposed scheme demonstrates both feasibility and efficiency in data hiding, surpassing several state-of-the-art methods in certain performance metrics.},
  archive      = {J_PAAA},
  author       = {Yavuz, Erdem},
  doi          = {10.1007/s10044-025-01495-w},
  journal      = {Pattern Analysis and Applications},
  month        = {9},
  number       = {3},
  pages        = {1-25},
  shortjournal = {Pattern Anal. Appl.},
  title        = {Reversible data hiding in encrypted images using chaos theory and chinese remainder theorem},
  volume       = {28},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Contour-aware contrastive learning for 3D knee segmentation from MR images. <em>PAAA</em>, <em>28</em>(3), 1-14. (<a href='https://doi.org/10.1007/s10044-025-01494-x'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Automatic segmentation of knee MR images plays an important role in the diagnosis and treatment of knee osteoarthritis. Existing deep learning-based methods usually require considerable annotated samples, and manual labeling of knee MR images is tedious and time-consuming. To address the above problem, we propose a novel semi-supervised method, named as Contour-Aware Contrastive Learning Network (CACL-Net), for segmentation of femoral cartilage, tibial internal cartilage, and tibial external cartilage in knee MR images. The CACL-Net takes an encoder-decoder structure similar to the V-Net as backbone, and adopts a novel contrastive learning auxiliary task called Progressive Encoding Module, which can maintain the key high-level semantic information of the image and attenuate the irrelevant low-level semantic information. We also coin a contour-based self-attention module to prompt the network to pay more attention to edge details during the decoding process, thereby obtaining accurate segmentation results. Extensive experimental results demonstrate the proposed CACL-Net outperforms some other semi-supervised methods for knee MR image segmentation, and shows the potential usage of CACL-Net in the domain of semi-supervised segmentation problems. Our code is available at https://github.com/ldcdm/CLASS-Net .},
  archive      = {J_PAAA},
  author       = {Dong, Xianda and Zhang, Lei and Zhao, Xing and Zhou, Shoujun and Wang, Yuanquan and Xia, Jun and Zhang, Tao},
  doi          = {10.1007/s10044-025-01494-x},
  journal      = {Pattern Analysis and Applications},
  month        = {9},
  number       = {3},
  pages        = {1-14},
  shortjournal = {Pattern Anal. Appl.},
  title        = {Contour-aware contrastive learning for 3D knee segmentation from MR images},
  volume       = {28},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Context-aware network with enhanced local information for medical image segmentation. <em>PAAA</em>, <em>28</em>(3), 1-13. (<a href='https://doi.org/10.1007/s10044-025-01496-9'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the field of medical image segmentation, architectures such as Unet and APFormer based on CNN (Convolutional Neural Network) and Transformer have made significant progress. However, they still face some challenges when dealing with complex datasets, such as high computational complexity and insufficient integration of contextual information. To address these issues, we propose a hybrid model for medical image segmentation that combines CNN, attention mechanism and R-MLP. Specifically, based on the coding stage of the U-shaped architecture, we introduce the attention mechanism,and propose the MFP module, which aims to compensate for the loss of global information after the encoder, and to reduce the semantic gap between the low-level features and the high-level features during the encoding and decoding process. In particular, we also propose the MCE module, which further extracts local information after capturing global information at the bottleneck layer. We conducted experiments on BUSI, DDTI, and PH2 datasets, and the results show that our model performs well in terms of F1, IoU, HD95, and ASD metrics, reaching 80.13%, 72.13%, 15.8642 mm, and 5.0325 mm on the BUSI dataset, respectively.Thus, compared with the state-of-the-art approaches, the proposed model shows a significant improvement compared to the state-of-the-art methods. The code is available at https://github.com/wang-xiang223/MARM-UNet .},
  archive      = {J_PAAA},
  author       = {Liu, Shangwang and Wang, Hongwei and Lin, Yinghai and Jin, Xianglian and Wang, Yusen and Cheng, Yulin},
  doi          = {10.1007/s10044-025-01496-9},
  journal      = {Pattern Analysis and Applications},
  month        = {9},
  number       = {3},
  pages        = {1-13},
  shortjournal = {Pattern Anal. Appl.},
  title        = {Context-aware network with enhanced local information for medical image segmentation},
  volume       = {28},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Animal re-identification in video through track clustering. <em>PAAA</em>, <em>28</em>(3), 1-14. (<a href='https://doi.org/10.1007/s10044-025-01497-8'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Monitoring a group of animals would greatly benefit from automated animal re-identification from video. Multiple Object Tracking alone does not provide a sufficiently good re-identification, hence we propose to augment the process by further clustering the output tracks. Unlike datasets for person and vehicle identification, existing animal datasets are not substantial enough to train an advanced model for conventional clustering. In this paper, we present a Classification-Based Clustering method (CBC) which employs track labels and temporal constraints to train a bespoke model for each video dataset. Our proposed method works better than using the tracks alone as animal identities. It also outperforms 13 alternative clustering methods applied to the tracking results.},
  archive      = {J_PAAA},
  author       = {Williams, Francis J. and Hennessey, Samuel L. and Kuncheva, Ludmila I.},
  doi          = {10.1007/s10044-025-01497-8},
  journal      = {Pattern Analysis and Applications},
  month        = {9},
  number       = {3},
  pages        = {1-14},
  shortjournal = {Pattern Anal. Appl.},
  title        = {Animal re-identification in video through track clustering},
  volume       = {28},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Tgcpn: Two-level grid context propagation network for 3D small object detection. <em>PAAA</em>, <em>28</em>(3), 1-15. (<a href='https://doi.org/10.1007/s10044-025-01498-7'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {3D small objects detection is a challenging task. Most existing approaches based on self-attention mechanism to capture context features from grid representation, thereby improving the detection performance of small objects. However, it is difficult to obtain representative object features with a single grid representation. To address this problem, we propose a new 3D backbone, named Two-level Grid Context Propagation Network (TGCPN), which integrates two grid representations into a single framework. TGCPN consists of a Contextual Grid Grouping Module (CGGM) and a Grid-Point Dual Key attention (G-PDK). Specifically, we first dynamically partition voxel grids and pillar grids with CGGM to enable subsequent parallel processing. Then, G-PDK is used to capture the contextual information in the grids and the fine-grained details in the point clouds. Finally, feature propagation is performed on voxel grids and pillar grids to integrate features at different levels. Extensive experiments demonstrate that our TGCPN-based method exhibits excellent performance in 3D small object detection. In addition, our single-stage detector built on TGCPN exhibits excellent real-time performance compared with some mainstream 3D backbones. Our implementation is available at https://github.com/user-vv/TGCPN .},
  archive      = {J_PAAA},
  author       = {Zhou, Yan and Pu, Lei and Xu, Xuemiao and Yi, Chang’an and He, Xinwei and Zhou, Yuexia and Xu, Yewen},
  doi          = {10.1007/s10044-025-01498-7},
  journal      = {Pattern Analysis and Applications},
  month        = {9},
  number       = {3},
  pages        = {1-15},
  shortjournal = {Pattern Anal. Appl.},
  title        = {Tgcpn: Two-level grid context propagation network for 3D small object detection},
  volume       = {28},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Unified semantic annotation of vessel behaviors via embeddings on topic model. <em>PAAA</em>, <em>28</em>(3), 1-12. (<a href='https://doi.org/10.1007/s10044-025-01500-2'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A vast amount of vessel trajectory (VT) data is generated during vessel movements. This data contains rich movement patterns that are essential for understanding vessel behaviors. Annotating semantics for VTs aids in analyzing these behaviors and supports maritime safety management. However, existing methods often focus on either individual or interactive behaviors in isolation, failing to address both simultaneously. Additionally, they are not optimized for large VT datasets and struggle to extract meaningful semantic information effectively. To address these challenges, this paper proposes a method called Semantic Embedding-Enhanced Topic Modeling for Vessel Behaviors (SeeToM). SeeToM integrates the semantics of both single and multiple vessels to construct VT documents. Subsequently, it employs an initialized topic dispersal method to select initial topic centers, thereby improving topic coverage and making SeeToM more suitable for large-scale trajectory data. Furthermore, SeeToM incorporates neural networks into the topic model using Weibull distribution and bidirectional transfer loss, enabling deeper exploration of potential semantic information. Experiments conducted on real datasets from the Zhoushan Archipelago and Sanya demonstrate that SeeToM can effectively extract various vessel semantics, such as acceleration, turn, and collision avoidance. Compared to state-of-the-art baselines, our method achieves improvements of 3.3% in Purity and 3.5% in Normalized Mutual Information (NMI).},
  archive      = {J_PAAA},
  author       = {Tao, Zhiyuan and Zhang, Rui and Zhang, Yongchang and Wu, Xiaolie and Lei, Tao and Xiao, Zhu and Liu, Kezhong},
  doi          = {10.1007/s10044-025-01500-2},
  journal      = {Pattern Analysis and Applications},
  month        = {9},
  number       = {3},
  pages        = {1-12},
  shortjournal = {Pattern Anal. Appl.},
  title        = {Unified semantic annotation of vessel behaviors via embeddings on topic model},
  volume       = {28},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Subject-independent multi-channel voting for EEG-based emotion recognition using wavelet scattering deep network and advanced signal metrics. <em>PAAA</em>, <em>28</em>(3), 1-29. (<a href='https://doi.org/10.1007/s10044-025-01501-1'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Electroencephalography (EEG) signals, reflecting human brain activity, hold potential beyond medical diagnosis, particularly in emotion recognition. Despite the development of machine learning models utilizing EEG data for this purpose, achieving good enough accuracy remains a challenge due to signals complexity and non-stationary nature, especially in extracting effective features that encapsulate temporal and frequency information. This paper introduces a novel hand-crafted feature extraction technique that avoids conventional signal segmentation and analyzes the entire length of EEG signals. This method builds a convolutional network utilizing Wavelet Scattering Transform (WST) blocks, followed by deriving a comprehensive 17-feature set from the raw EEG data and WST scattering coefficients. This integrative set takes advantage of the WST’s ability to produce a signal representation that is stable against noise, invariant to time shifts, and captures both temporal and frequency components while also leveraging the intrinsic properties of the raw data, offering an alternative to the computational deep models. The integration of Linear Discriminant Analysis for dimensionality reduction and the K-Nearest Neighbors algorithm for classification, further refined by a majority voting mechanism across all channels, results in a robust classification framework. The proposed method is evaluated across GAMEEMO and DEAP datasets with two and four emotional classes, using Leave-One-Subject-Out validation, achieving classification accuracy exceeding 97%. The findings support the effectiveness of this approach in EEG-based emotion recognition. Furthermore, an ablation study on the two datasets is implemented to assess each component’s impact, revealing insights into the model’s effectiveness and improvement areas.},
  archive      = {J_PAAA},
  author       = {Elrefaiy, Ahmed and Tawfik, Nahed and Zayed, Nourhan and Elhenawy, Ibrahim},
  doi          = {10.1007/s10044-025-01501-1},
  journal      = {Pattern Analysis and Applications},
  month        = {9},
  number       = {3},
  pages        = {1-29},
  shortjournal = {Pattern Anal. Appl.},
  title        = {Subject-independent multi-channel voting for EEG-based emotion recognition using wavelet scattering deep network and advanced signal metrics},
  volume       = {28},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). LTGS: An optical remote sensing tiny ship detection model. <em>PAAA</em>, <em>28</em>(3), 1-23. (<a href='https://doi.org/10.1007/s10044-025-01503-z'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In optical remote sensing (ORS) ship detection tasks based on deep learning, traditional models have made significant progress in detecting small ships, but still face challenges such as insufficient accuracy and missed detection for tiny ships. To address this problem, we propose LTGS (Lightweight Tiny-target Guided Sensing), a lightweight and optimized detection model based on YOLOv8n, specifically designed for tiny-object detection in ORS imagery. The design of LTGS is guided by a novel architectural framework termed the Detection Path Optimizer (DPO), which redefines the detection process as a coordinated multi-path optimization system to improve detection performance and efficiency. LTGS incorporates three key components. First, the Lightweight Cascaded Perception Module (LCPM) replaces the original C2f in the backbone to enhance feature extraction and gradient propagation through hierarchical cascaded convolutions and identity mappings, while significantly reducing parameters. Second, the Global Spatial Pyramid Pooling-Fast (GSPPF) module replaces the traditional SPPF to facilitate global-local semantic fusion via adaptive pooling, improving robustness in cluttered environments. Third, the Tiny Target Detection Head (TD) introduces guided upsampling and multi-path fusion to restore fine-grained details lost during downsampling, thereby boosting localization accuracy for tiny targets. Experiments on the public datasets HRSC2016 and MASATI v2, along with the Self-managed and Height-managed datasets, verified the model’s effectiveness. The results show that the improved model increased mAP@50 by 2.1%, reduced parameters by 0.3 M, and reduced the weight file size by 0.3 MB, with particularly superior performance in tiny ship detection tasks.},
  archive      = {J_PAAA},
  author       = {Yan, Chunman and Qi, Ningning},
  doi          = {10.1007/s10044-025-01503-z},
  journal      = {Pattern Analysis and Applications},
  month        = {9},
  number       = {3},
  pages        = {1-23},
  shortjournal = {Pattern Anal. Appl.},
  title        = {LTGS: An optical remote sensing tiny ship detection model},
  volume       = {28},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). NEONATE: A bi-directional search strategy for non-overlapping pattern matching with gap constraint. <em>PAAA</em>, <em>28</em>(3), 1-20. (<a href='https://doi.org/10.1007/s10044-025-01505-x'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Pattern matching calculates support or occurrence numbers of a pattern in information retrieval and has derived kinds of constraints. Among the variety of constraint-matching conditions, plenty of research has illustrated the significant non-overlapping condition, which means that any two occurrences cannot use the identical character of the sequence in the same position of the pattern. Although a corresponding completeness proof has been given in state-of-the-art works, there is still room to improve the matching process. Inspired by the literature on strict matching under the non-overlapping condition, this study proposes a Nettree-based approach to matching given patterns called NEONATE. It adopts a bi-directional search strategy and pruning operation to realize the non-overlapping pattern matching under the gap and length constraints. NEONATE first locates the horizontal layer with the fewest available nodes in the Nettree. Then, it iteratively seeks the rightmost parent and rightmost child node until an occurrence is obtained in which the nodes in the Nettree path can strictly satisfy the pattern constraint. This study has not only demonstrated the completeness of NEONATE but also analyzed the complexity of the algorithm. The correctness and efficiency of NEONATE have been proved via extensive experiments on biological and musical sequences.},
  archive      = {J_PAAA},
  author       = {Shen, Cong and Li, Yongyi},
  doi          = {10.1007/s10044-025-01505-x},
  journal      = {Pattern Analysis and Applications},
  month        = {9},
  number       = {3},
  pages        = {1-20},
  shortjournal = {Pattern Anal. Appl.},
  title        = {NEONATE: A bi-directional search strategy for non-overlapping pattern matching with gap constraint},
  volume       = {28},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). DermoMamba: A cross-scale mamba-based model with guide fusion loss for skin lesion segmentation in dermoscopy images. <em>PAAA</em>, <em>28</em>(3), 1-26. (<a href='https://doi.org/10.1007/s10044-025-01506-w'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, the application of artificial intelligence in medical image segmentation has garnered significant attention, particularly in the development of deep learning models aimed at improving accuracy and efficiency. Skin lesion segmentation is one of the most essential tasks in healthcare, serving as a crucial step in aiding the early detection of skin cancer, allowing physicians to develop appropriate and effective treatment plans for patients. In this research, we introduce a new compact U-shaped network design that integrates the local information extraction capability of Convolutional Neural Networks (CNNs) with the long-range dependency capturing ability of Mamba. Specifically, we introduce the Cross-Scale Mamba Block, a sequential architecture that effectively combines the ability to capture global features with an expanded receptive field and a bottleneck structure, which is enhanced with an optimized multiaxial Mamba mechanism for comprehensive spatial information aggregation. Additionally, the Convolutional Block Attention Module in the skip connections helps preserve information and enhance attention to important details. Furthermore, we introduce a new loss function, Guide Fusion Loss, which introduces an innovative attention map calculation to enhance segmentation accuracy at boundary regions in complex images. The proposed model, namely DermoMamba, is assessed using two datasets of dermoscopic skin lesion images, ISIC 2018 and PH2, achieving superior performance compared to advanced methods utilizing CNNs, Transformers and Mamba, while using fewer than 5 million parameters and less than 1 G floating point operations per second. This significant reduction in computational cost is achieved without compromising accuracy. Based on the experimental results, our model stands as an effective solution, striking a balance between accuracy and compactness. The code is made available at: https://github.com/hnkhai25/Segment .},
  archive      = {J_PAAA},
  author       = {Hoang, Ngoc-Khai and Nguyen, Dinh-Hieu and Tran, Thi-Thao and Pham, Van-Truong},
  doi          = {10.1007/s10044-025-01506-w},
  journal      = {Pattern Analysis and Applications},
  month        = {9},
  number       = {3},
  pages        = {1-26},
  shortjournal = {Pattern Anal. Appl.},
  title        = {DermoMamba: A cross-scale mamba-based model with guide fusion loss for skin lesion segmentation in dermoscopy images},
  volume       = {28},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Elastic matching through the lens of probability and divergence in time series prediction. <em>PAAA</em>, <em>28</em>(3), 1-19. (<a href='https://doi.org/10.1007/s10044-025-01507-9'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Time series analysis is in predictive modeling and decision-making across numerous disciplines. A key challenge involves identifying similar intervals that reveal critical patterns and inform strategic decisions. This paper introduces the Finding Best Similar (FBS) method, a framework for time series comparison that prioritizes distribution-based similarity over traditional point-to-point alignment. FBS employs Kernel Density Estimation (KDE) to model the underlying statistical distributions of time series segments and evaluates their differences using measures such as Kullback–Leibler Divergence, Rényi’s Divergence, Jensen-Shannon Divergence, and Hellinger Distance. By focusing on probability distributions rather than time-indexed values, FBS offers robustness against noise, scaling, and temporal misalignment. Utilizing a sliding window approach, FBS efficiently detects intervals with similar distributions, enabling both short- and long-term insights. Experimental results demonstrate that FBS achieves the highest future correlation of 0.898, signifying a 1.4% improvement over other methods, with statistically significant results underscoring its reliability in practical applications. In particular, the concept of Predictive Fidelity highlights how certain stocks exhibit consistent recurring patterns: FBS excels at uncovering these resemblances and correlating them with future price movements. By bypassing the need for elastic alignment, FBS remains computationally scalable and suitable for large datasets. Its adaptability is further underscored by diverse applications, including anomaly detection, pattern recognition, forecasting, and financial time series. Together, these findings establish FBS as a robust, domain-agnostic grounded solution for extracting meaningful insights from temporal data while offering a direct measure of how effectively a series replicates-and potentially predicts-its own past.},
  archive      = {J_PAAA},
  author       = {Forouzan, Ali and Yazdi, Hadi Sadoghi},
  doi          = {10.1007/s10044-025-01507-9},
  journal      = {Pattern Analysis and Applications},
  month        = {9},
  number       = {3},
  pages        = {1-19},
  shortjournal = {Pattern Anal. Appl.},
  title        = {Elastic matching through the lens of probability and divergence in time series prediction},
  volume       = {28},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Contextual xLSTM-based multimodal fusion for conversational emotion recognition. <em>PAAA</em>, <em>28</em>(3), 1-18. (<a href='https://doi.org/10.1007/s10044-025-01508-8'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In real-world dialogue systems, the ability to understand user emotions and engage in human-like interactions is of great significance. Emotion Recognition in Conversation (ERC) is one of the key approaches to achieving this goal and has attracted increasing attention. However, existing methods for ERC often fail to effectively model contextual information and exploit the complementarity of multimodal information. Few approaches are capable of fully capturing the complex correlations and mapping relationships between different modalities. Additionally, classifying minority classes and semantically similar classes remains a significant challenge. To address these issues, this paper proposes an attention-based contextual modeling and multimodal fusion network. The proposed method efficiently combines an extended LSTM network (xLSTM) with an attention mechanism to thoroughly model the contextual information generated during conversations. xLSTM is an enhanced LSTM unit featuring matrix memory and exponential gating mechanisms, which can better capture long-range dependencies and improve recognition performance. Furthermore, a Transformer-based modality encoder is employed to map features from different modalities into a shared feature space, enabling alignment and mutual enhancement among modalities. This facilitates both intra-modal and inter-modal information interaction, thereby maximizing the complementary advantages of multimodal data. A multimodal fusion module based on bidirectional multi-head cross-attention layers is then used to capture cross-modal mapping relationships among text, audio, and visual modalities, effectively integrating multimodal information. Extensive experiments conducted on two benchmark ERC datasets, IEMOCAP and MELD, demonstrate that the proposed method achieves weighted F1 scores of 75.21 and 69.78, respectively, outperforming the current state-of-the-art methods by 2.2 and 3.3 points. It also achieves accuracy rates of 80.59% and 69.16% on the two datasets, representing improvements of 6.64% and 1.11%, respectively. The codes and models are available at: https://github.com/rhoqwomda/MFCRE},
  archive      = {J_PAAA},
  author       = {Qi, Yupeng and Ibrayim, Mayire and Tohti, Turdi},
  doi          = {10.1007/s10044-025-01508-8},
  journal      = {Pattern Analysis and Applications},
  month        = {9},
  number       = {3},
  pages        = {1-18},
  shortjournal = {Pattern Anal. Appl.},
  title        = {Contextual xLSTM-based multimodal fusion for conversational emotion recognition},
  volume       = {28},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). ConUDiff: Diffusion model with contrastive pretraining and uncertain region optimization for segmentation of left ventricle from echocardiography. <em>PAAA</em>, <em>28</em>(3), 1-11. (<a href='https://doi.org/10.1007/s10044-025-01509-7'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Accurate segmentation of the left ventricle (LV) in echocardiograms plays a crucial role in the diagnosis and treatment of cardiovascular diseases. However, manual segmentation of the left ventricle is time-consuming and subject to inter-observer variability. It is crucial to develop an accurate and automatic segmentation method. In this paper, we propose a novel diffusion-based model, called ConUDiff in short, for LV segmentation in echocardiography. The proposed ConUDiff is based on the denoising diffusion probabilistic model and two modules are introduced, i.e., a contrastive pretrained ResNet-50 encoder and an uncertain region optimization module (UROM). The contrastive pretrained ResNet-50 encoder is employed to extract rich feature representations from the original image and enhance the semantic information contained in the feature maps. The UROM module is designed to optimize uncertain regions in the feature maps. We evaluate our method on two public datasets, i.e., the EchoNet-Dynamic dataset and the EchoNet-Pediatric dataset. The experimental results demonstrate that the proposed ConUDiff outperforms some popular networks, achieving a Dice score of 92.68% on the EchoNet-Dynamic dataset and a Dice score of 90.69% on the EchoNet-Pediatric dataset. Our method shows the potential for echocardiographic left ventricle segmentation.},
  archive      = {J_PAAA},
  author       = {Zhang, Guohuan and Zhang, Lei and Fu, Xuetong and Wang, Yuanquan and Zhou, Shoujun and Wei, Jin and Zhao, Di},
  doi          = {10.1007/s10044-025-01509-7},
  journal      = {Pattern Analysis and Applications},
  month        = {9},
  number       = {3},
  pages        = {1-11},
  shortjournal = {Pattern Anal. Appl.},
  title        = {ConUDiff: Diffusion model with contrastive pretraining and uncertain region optimization for segmentation of left ventricle from echocardiography},
  volume       = {28},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Effective dual stage fusion model for single image dehazing. <em>PAAA</em>, <em>28</em>(3), 1-25. (<a href='https://doi.org/10.1007/s10044-025-01510-0'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Haze is an atmospheric phenomenon that greatly decreases the visibility of images captured by digital camera. The aim of the image dehazing is to restore the scene radiance from the hazy image. This paper proposes dual stage fusion model for an effective dehazing. In the proposed method, hazy image is first pre-processed using Laplacian filter and Dark Channel Prior (DCP). To capture the edge information and reduce the noise in the input image, the Laplacian filtered image is combined with the dark channel image. Then, the output image obtained from DCP algorithm (without soft matting) is enhanced through dual stage fusion technique. The first stage fusion is the linear fusion of the two images obtained by applying AGCWD and WAAHE, and the second stage fusion is the DWT based fusion of the first stage fusion image and ACMHE image. Proposed work is implemented on various datasets such as O-HAZE, BeDDE and NH-HAZE. To see the effectiveness of proposed work, different images of same size with varied haze dense are used. AGCWD and WAAHE in dual stage fusion preserves small details and colors. Moreover, ACMHE gives more enhanced image in terms of contrast and sharpness. Both qualitative and quantitative analysis have been done on several images and compared with some existing dehaze methods. Results show that the technique proposed in the paper provides satisfactory result for dense hazy image also.},
  archive      = {J_PAAA},
  author       = {Varshney, Vikas and Panda, Jeebananda and Gupta, Rashmi},
  doi          = {10.1007/s10044-025-01510-0},
  journal      = {Pattern Analysis and Applications},
  month        = {9},
  number       = {3},
  pages        = {1-25},
  shortjournal = {Pattern Anal. Appl.},
  title        = {Effective dual stage fusion model for single image dehazing},
  volume       = {28},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Selecting sampling ratios in imbalanced datasets through class complexity. <em>PAAA</em>, <em>28</em>(3), 1-17. (<a href='https://doi.org/10.1007/s10044-025-01511-z'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Data characteristics that reveal the complexity of the dataset, such as overlap, lack of density, the presence of noisy points, etc. are key factors for the deterioration of the classification task of imbalanced datasets. The class size imbalance is not the main problem, but its combination with the above mentioned characteristics. Despite this, when applying sampling methods for preprocessing imbalanced data, the sampling ratio is selected based on the class sizes. In this paper, we propose a methodology to select the sampling ratio by seeking for a balance in the complexity of the classes instead of in their sizes. Our proposed methodology, called Hostility-Aware Ratio for Sampling (HARS), tracks how the complexity of the classes changes when a sampling method is applied for different ratios of minority to majority instances, and recommends the ratio for which there is a balance between the class complexities. Complexity is gauged through the hostility measure, a complexity measure that estimates the probability of misclassifying an instance, a class, or the entire dataset. The proposal is assessed on a total of 66 real datasets and compared with the state-of-the-art (SOTA) methods providing satisfactory classification results that validate the use of the complexity for the task of choosing the sampling ratio and that a balance in complexity favors a more balanced learning process of classifiers.},
  archive      = {J_PAAA},
  author       = {Lancho, Carmen and Cuesta, Marina and De Diego, Isaac Martín and Aceña, Víctor and Moguerza, Javier M.},
  doi          = {10.1007/s10044-025-01511-z},
  journal      = {Pattern Analysis and Applications},
  month        = {9},
  number       = {3},
  pages        = {1-17},
  shortjournal = {Pattern Anal. Appl.},
  title        = {Selecting sampling ratios in imbalanced datasets through class complexity},
  volume       = {28},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Statistical framework for butterfly species recognition using raw spatio-hyperspectral images. <em>PAAA</em>, <em>28</em>(3), 1-26. (<a href='https://doi.org/10.1007/s10044-025-01513-x'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Butterfly species recognition is essential for biodiversity monitoring and pest management. However, traditional approaches based on RGB images are limited by their reliance on color and shape features, making them sensitive to visual ambiguities such as similar color patterns across species and variations in wing shape or appearance during flight. While hyperspectral imaging (HSI) provides rich spectral information, its conventional use relies on the reconstruction and analysis of an HSI datacube, which is computationally intensive and unsuitable for real-time applications. We address this limitation by proposing a framework that bypasses the need for datacube reconstruction. Instead, it operates directly on raw spatio-spectral images captured by a compact HSI camera before reconstruction, resulting in limited spectral data for the detected butterfly. Assuming that the spectral features of butterfly species follow Gaussian distributions, we observe overlaps between some distributions due to spectral similarities among some species. To overcome this, we introduce a classification approach based on a convex combination of Gaussian Naive Bayes and Z-score methods. This approach proves more effective, yielding better results when butterflies are detected in the near-infrared wavelengths. We achieve an accuracy of 88.75% using near-infrared information from a single spatio-spectral image, reaching up to 97.5% when aggregating spectral information from a sequence of four spatio-spectral images.},
  archive      = {J_PAAA},
  author       = {Adjé, Erick A. and Delmaire, Gilles and Ahouandjinou, Arnaud S. R. M. and Puigt, Matthieu and Roussel, Gilles},
  doi          = {10.1007/s10044-025-01513-x},
  journal      = {Pattern Analysis and Applications},
  month        = {9},
  number       = {3},
  pages        = {1-26},
  shortjournal = {Pattern Anal. Appl.},
  title        = {Statistical framework for butterfly species recognition using raw spatio-hyperspectral images},
  volume       = {28},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A hybrid unsupervised-weakly supervised method for video anomaly detection. <em>PAAA</em>, <em>28</em>(3), 1-18. (<a href='https://doi.org/10.1007/s10044-025-01515-9'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Video anomaly detection has recently become a critical field of research because of its potential applications in autonomous surveillance systems. We propose Hybrid-ConvATFM, a novel method that uniquely operates in both unsupervised and weakly supervised settings, making several key contributions. First, it alleviates the human annotation problem by employing a GMM-based divisive clustering algorithm to generate video-level pseudo-labels, enabling fully unsupervised operation when frame-level or video-level annotations are unavailable. Second, Hybrid-ConvATFM excels at capturing the complex temporal dynamics of anomalous events through its innovative architecture that combines convolutional autoencoders with attention modules, extracting enhanced local and global temporal dependencies of video segments. Third, our approach incorporates a temporal feature magnitude learning technique that effectively identifies and classifies abnormal segments with high precision. The versatility and effectiveness of Hybrid-ConvATFM is demonstrated through extensive experimentation on benchmark datasets. In the weakly supervised setting, our method achieves area under the curve scores of 97.70% on ShanghaiTech, 84.91% on UCF-Crime, and 83.51% on XD-Violence. In the fully unsupervised setting, Hybrid-ConvATFM attains 89.50% on ShanghaiTech, 79.61% on UCF-Crime, and 78.43% on XD-Violence, establishing new performance benchmarks in video anomaly detection across multiple operational scenarios.},
  archive      = {J_PAAA},
  author       = {Rakhmonov, Akhrorjon Akhmadjon Ugli and Varnousefaderani, Bahar Amirian and Kim, Jeonghong},
  doi          = {10.1007/s10044-025-01515-9},
  journal      = {Pattern Analysis and Applications},
  month        = {9},
  number       = {3},
  pages        = {1-18},
  shortjournal = {Pattern Anal. Appl.},
  title        = {A hybrid unsupervised-weakly supervised method for video anomaly detection},
  volume       = {28},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Domain diversity based meta learning for continual person re-identification. <em>PAAA</em>, <em>28</em>(3), 1-14. (<a href='https://doi.org/10.1007/s10044-025-01502-0'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Continual person re-identification (CReID) aims to learn a unified model to cope with changing scenarios (e.g., from malls to streets and stations, etc). However, deploying the CReID model directly on unseen scenarios (which cannot be foreseen) outside the continual pipeline leads to a drop in generalization performance due to distribution shifts. In this paper, we design a meta learning paradigm with the seen and unseen domains to adapt the model to distribution shifts, where the unseen domains are produced by our carefully designed Domain Diversity (DD). Considering the knowledge encompassing both seen and unseen domains can fight against forgetting as well as improve generalization, we accumulate the learned and future knowledge corresponding to seen and unseen domains respectively through graph attention networks. Subsequently, we integrate the accumulated knowledge into meta learning steps to guide the training of the model, ensuring less forgetting and better generalization. Extensive experiments conducted on twelve datasets demonstrate the effectiveness of our method with superior performance in generalization and anti-forgetting. The code is available at https://github.com/DFLAG-NEU/MetaDD .},
  archive      = {J_PAAA},
  author       = {Liu, Zhaoshuo and Feng, Chaolu and Yu, Kun and Song, Jiangdian and Li, Wei},
  doi          = {10.1007/s10044-025-01502-0},
  journal      = {Pattern Analysis and Applications},
  month        = {9},
  number       = {3},
  pages        = {1-14},
  shortjournal = {Pattern Anal. Appl.},
  title        = {Domain diversity based meta learning for continual person re-identification},
  volume       = {28},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Affine locality-sensitive nonnegative representation based image classification. <em>PAAA</em>, <em>28</em>(3), 1-17. (<a href='https://doi.org/10.1007/s10044-025-01504-y'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Representation-based classification methods (RBCM) have garnered significant attention in the field of image classification over the past decade. However, the effectiveness of $$\ell _1$$ or $$\ell _2$$ -regularization in improving classification performance remains ambiguous. To address this issue, the nonnegative representation-based classification (NRC) method proposes a constructive approach that emphasizes strong positive correlations between samples from the same classes while treating samples from different classes as irrelevant. Despite its merits, NRC overlooks some critical aspects. Firstly, it fails to incorporate an affine constraint, which is necessary for ensuring that all samples reside in a specific affine space. Additionally, the absence of local information among samples results in reduced discriminability, as NRC relies solely on global representation. Lastly, the instability of the solution to representation coefficients can be attributed to the lack of regularization terms in NRC’s objective function, leading to increased misclassification probabilities. To address these limitations, we introduce the affine locality-sensitive nonnegative representation (ALNR) model as a novel approach for image classification. More specifically, ALNR enforces an affine constraint on representation coefficients and introduces a regularization term involving a locality-sensitive matrix in the objective function. Extensive experiments conducted on diverse datasets demonstrate the strong competitiveness of our proposed method. Habitually, the ALNR’s source code can be made publicly accessible on my profile page at https://github.com/li-zi-qi/ALNR .},
  archive      = {J_PAAA},
  author       = {Li, Ziqi and Song, Hongcheng and Guo, Tingting and Yin, Hefeng and Zhang, Yonghong and Zhu, Linglong and Sun, Jun},
  doi          = {10.1007/s10044-025-01504-y},
  journal      = {Pattern Analysis and Applications},
  month        = {9},
  number       = {3},
  pages        = {1-17},
  shortjournal = {Pattern Anal. Appl.},
  title        = {Affine locality-sensitive nonnegative representation based image classification},
  volume       = {28},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Metric learning for recommendation with conditional generative adversarial networks based hard negative samples. <em>PAAA</em>, <em>28</em>(3), 1-15. (<a href='https://doi.org/10.1007/s10044-025-01512-y'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Tag recommendations designed to automatically assign the most relevant tags to items to improve recommendation results. Research has shown metric learning that satisfies triangular inequalities is more likely to capture similarity relationships between users, items, and tags compared to inner product computation. However, existing metric learning-based tag recommendation methods face critical challenges: random negative sampling strategies result in excessive proximity between negative and positive samples in low-dimensional metric spaces, forming ambiguous decision boundaries. Such hard negative samples hinder models from distinguishing semantically similar candidate tags, leading to misclassification and performance degradation. To address these issues, this paper proposes MLGAN, a Metric Learning for Recommendation with Conditional Generative Adversarial Networks based Hard Negative Samples. The framework first embeds users, items, and tags into a low-dimensional embedding space through metric learning, establishing distance relationships among original samples. Subsequently, an adversarial metric learning architecture incorporates a hard sample generation module, where a conditional generative adversarial network (CGAN) synthesizes hard negative tags that users would unlikely select. This process reveals latent user-item-tag relationships from original samples, while a novel difficulty consistency loss enables precise control over the hardness level of generated negative tags through a two-pronged approach. Finally, adversarial training jointly optimizes metric space parameters for both original and generated samples, enhancing the model’s discriminative capability to improve recommendation accuracy and robustness. We conducted extensive experiments on both LastFm and Movielens datasets to analyze the impact of different parameters and intrinsically different components on the performance, and experimental results show that the MLGAN evaluation metrics outperform the most competitive baselines.},
  archive      = {J_PAAA},
  author       = {Fei, Zhengshun and Qin, Xiangyu and Chen, Jianxin and Wen, Shuangquan and Xiang, Xinjian},
  doi          = {10.1007/s10044-025-01512-y},
  journal      = {Pattern Analysis and Applications},
  month        = {9},
  number       = {3},
  pages        = {1-15},
  shortjournal = {Pattern Anal. Appl.},
  title        = {Metric learning for recommendation with conditional generative adversarial networks based hard negative samples},
  volume       = {28},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). High-order tensor based multi-view clustering via enhanced adaptive graph propagation. <em>PAAA</em>, <em>28</em>(3), 1-17. (<a href='https://doi.org/10.1007/s10044-025-01517-7'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-view clustering is a significant technique within the realm of machine learning, designed to uncover the underlying structure of data by conducting an integrated analysis of multi-source data. However, current multi-view clustering methods exhibit three principal shortcomings. Firstly, existing approaches seldom apply filters to smooth data representations. Secondly, traditional multi-view clustering methods struggle with the effective integration of information from diverse views, leading to inadequate utilization of information and challenges in addressing incompleteness and inconsistency between views. Thirdly, most methods do not fully exploit high-order structural information among data. To address these limitations, this paper proposes a novel method termed high-order tensor based multi-view clustering via enhanced adaptive graph propagation. Specifically, the method utilizes Gaussian filters to smooth data representations, thereby mitigating noise and enhancing the quality of the data. An enhanced adaptive graph propagation mechanism is then implemented to intensify interaction and information exchange among nodes, and facilitates the collaboration and fusion of information across different views. Simultaneously, high-order tensors are utilized to integrate multi-view data, accurately reveal the underlying structure of the data, and fully exploit the similarity information of high-order structures among the data. Experimental results demonstrate the superiority of our method over other clustering methods across seven real-world datasets.},
  archive      = {J_PAAA},
  author       = {Chen, Muke and Cai, Yongming and Huang, Zhanpeng},
  doi          = {10.1007/s10044-025-01517-7},
  journal      = {Pattern Analysis and Applications},
  month        = {9},
  number       = {3},
  pages        = {1-17},
  shortjournal = {Pattern Anal. Appl.},
  title        = {High-order tensor based multi-view clustering via enhanced adaptive graph propagation},
  volume       = {28},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). AESPA: Automated essay scoring using polished argument feature weights. <em>PAAA</em>, <em>28</em>(3), 1-11. (<a href='https://doi.org/10.1007/s10044-025-01518-6'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Automated Essay Scoring (AES) is a task in which a model automatically assigns a score to a given essay instead of a human grader. Recent AES studies are increasingly utilizing argument structure as a distinctive feature. However, these approaches require high feature extraction costs due to the use of handcrafted features and often consider only limited evaluation traits, making it difficult to provide detailed feedback to writers. To address these limitations, we propose Automated Essay Scoring using Polished Argument Feature Weight (AESPA), a model that uses only argument labels and can assess a wide range of writing traits. AESPA introduces a novel mechanism called trait attention, which allows the model to automatically learn how much the argument structure contributes to each evaluation trait-such as content, organization, or language use-without manually designing features. This enables AESPA to flexibly adapt to traits where argument structure is helpful, and ignore it where it is not. Experiments show that AESPA improves scoring performance across multiple traits and mitigates performance drops in traits where argument structure may otherwise introduce noise.},
  archive      = {J_PAAA},
  author       = {Jang, Junseo and Lee, Joosang and Lee, Yejin and Jeong, Seokwon and Kim, Harksoo},
  doi          = {10.1007/s10044-025-01518-6},
  journal      = {Pattern Analysis and Applications},
  month        = {9},
  number       = {3},
  pages        = {1-11},
  shortjournal = {Pattern Anal. Appl.},
  title        = {AESPA: Automated essay scoring using polished argument feature weights},
  volume       = {28},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Guest editorial: Special issue on pedestrian attribute recognition and person re-identification. <em>PAAA</em>, <em>28</em>(3), 1-2. (<a href='https://doi.org/10.1007/s10044-025-01525-7'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_PAAA},
  author       = {Greco, Antonio and Castrillón-Santana, Modesto and Vento, Bruno},
  doi          = {10.1007/s10044-025-01525-7},
  journal      = {Pattern Analysis and Applications},
  month        = {9},
  number       = {3},
  pages        = {1-2},
  shortjournal = {Pattern Anal. Appl.},
  title        = {Guest editorial: Special issue on pedestrian attribute recognition and person re-identification},
  volume       = {28},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A novel similarity measure for single-valued neutrosophic sets based on the inner product and its applications in pattern recognition and medical diagnosis. <em>PAAA</em>, <em>28</em>(3), 1-18. (<a href='https://doi.org/10.1007/s10044-025-01445-6'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the era of ambiguity and vagueness, single-valued neutrosophic sets (SVNSs) are an eminent tool for handling indeterminate and uncertain information. SVNSs reduce information loss by considering three different aspects of an object. A similarity measure is an efficient tool used in various applications, including medical diagnosis, decision-making, and pattern recognition. Although many similarity measures have been established for SVNSs in the past, some do not fit the axiomatic definition of a similarity measure or exhibit issues that lead to inconsistent results. To overcome the shortcomings of the existing similarity measures, a new measure has been developed based on a novel definition of the inner product. The applicability and effectiveness of the proposed similarity measure are demonstrated through its application to medical diagnosis and pattern recognition problems. An algorithm for the face recognition problem is proposed using the proposed measure, and the maximum spanning tree (MST) technique is extended to the single-valued neutrosophic environment to offer a clustering analysis approach based on the proposed measure. Examples are provided to illustrate the algorithms, and their performances are compared with existing methods for addressing face recognition and clustering analysis problems. Experimental results verify that the proposed similarity measure produces reliable outcomes, addresses the problems found in existing similarity measures, and outperforms them in pattern recognition and medical diagnosis.},
  archive      = {J_PAAA},
  author       = {Bisht, Garima and Pal, Arun Kumar},
  doi          = {10.1007/s10044-025-01445-6},
  journal      = {Pattern Analysis and Applications},
  month        = {9},
  number       = {3},
  pages        = {1-18},
  shortjournal = {Pattern Anal. Appl.},
  title        = {A novel similarity measure for single-valued neutrosophic sets based on the inner product and its applications in pattern recognition and medical diagnosis},
  volume       = {28},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Starg2n: A spatial-temporal relevance analysis graph neural network for traffic prediction. <em>PAAA</em>, <em>28</em>(3), 1-21. (<a href='https://doi.org/10.1007/s10044-025-01519-5'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Traffic prediction is a critical technology in intelligent transportation systems, which can provide guidance services for traffic participants and decision-making services for managers. However, traffic data is generated by the movement of road users on urban roads, which makes it constrained by road networks’ topology structures and change over time. Therefore, traffic data usually has complex spatial and temporal features, and current prediction methods have limitations in extracting these features. To this, we propose a spatial-temporal relevance analysis graph neural network, StarG2N. StarG2N constructs local spatial-temporal graphs, which are used to extract spatial-temporal features. In this process, StarG2N applies a sliding window mechanism to ensure the integrity and flexibility of feature extraction. Meanwhile, StarG2N constructs a deep neural network using K-order Chebyshev graph convolution, Gate Recurrent Unit and Multi-head Self-attention mechanism. This framework can make full use of the information of neighboring road nodes, and establish a good combination of local feature extraction and global feature analysis. StarG2N can extract spatial-temporal dependency without using different networks to model temporal and spatial dependencies separately. Extensive experiments are conducted on four real-word datasets which are all obtained from the Caltrans Performance Measurement System (PeMS). Experimental results show that StarG2N can greatly improve prediction performance compared with baselines such as T-GCN, STSGCN and OGCRNN. In terms of Mean Absolute Error (MAE), Root Mean Squared Error (RMSE), and Mean Absolute Percentage Error (MAPE), the maximum performance improvement of StarG2N is 5.54%, 9.14%, and 7.03% respectively.},
  archive      = {J_PAAA},
  author       = {Zhao, Jinbo and Xu, Xiaolong},
  doi          = {10.1007/s10044-025-01519-5},
  journal      = {Pattern Analysis and Applications},
  month        = {9},
  number       = {3},
  pages        = {1-21},
  shortjournal = {Pattern Anal. Appl.},
  title        = {Starg2n: A spatial-temporal relevance analysis graph neural network for traffic prediction},
  volume       = {28},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Affective annotation of videos from EEG-based crowdsourcing. <em>PAAA</em>, <em>28</em>(3), 1-19. (<a href='https://doi.org/10.1007/s10044-025-01476-z'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Enriching multimedia content with affective metadata unlocks opportunities for innovative and enhanced user experiences in recommendation systems, multimedia retrieval, and many other applications. However, it is really challenging to accurately decode human affect. On the one hand, manual procedures for affective annotation are labour-intensive and hardly scalable. On the other hand, content-based approaches rely on the assumption that affective experiences (1) are directly related to the content, (2) are homogeneous for a particular content, and (3) ignore the subjective nature of affective responses. Recent advancements in brain-computer interfacing (BCI) signifies the prospect of partially automating affective annotation by decoding natural affective experiences toward the content. We consider affective annotation of videos based on brainsourcing: crowdsourced affective reactions from brain signals, recorded while participants were watching videos. Our experiments are based on two popular datasets (DEAP and SEED) and three crowdsourcing models. Crowdsourcing models can support affective annotation for all videos in the SEED dataset and most videos in the DEAP dataset. For both datasets and crowdsourcing models, the performance of affective annotation increases with the crowd size and shows increased confidence of the classifiers with larger crowd sizes. For example, the average classification accuracy for binary valence in DEAP is less than 60% for the individual predictions, but increases up to about 70% for a crowd size of six participants and get to about 80% for fourteen participants. Our findings open avenues for utilizing data captured via BCI for understanding and annotating content according to its users’ affective experiences.},
  archive      = {J_PAAA},
  author       = {Moreno-Alcayde, Yoelvis and Ruotsalo, Tuukka and Leiva, Luis A. and Traver, V. Javier},
  doi          = {10.1007/s10044-025-01476-z},
  journal      = {Pattern Analysis and Applications},
  month        = {9},
  number       = {3},
  pages        = {1-19},
  shortjournal = {Pattern Anal. Appl.},
  title        = {Affective annotation of videos from EEG-based crowdsourcing},
  volume       = {28},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multi-stage bayesian prototype refinement with feature weighting for few-shot classification. <em>PAAA</em>, <em>28</em>(3), 1-14. (<a href='https://doi.org/10.1007/s10044-025-01520-y'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Few-shot classification endeavors to recognize a query sample by leveraging a limited amount of support data, with prototype classifiers being frequently applied. While the prototype classifier is simple and non-parametric, it fails to fully leverage the prior information from the support samples, resulting in prototype bias. To address this, we introduce the Multi-stage Bayesian Prototype Refinement with Feature Weighting (MBPRFW). In our approach, we begin by implementing a feature weighting module to adjust the influence of each support sample. The adjusted features are then used as prior information to build the Bayesian prototype classifier, enabling the model to place greater emphasis on the most important aspects of the data. Ultimately, we incorporate a multi-stage inference strategy, in which the most distant support samples are filtered out at each stage. Through sample filtering, prototype representations and their associated classification scores undergo systematic recalibration. Therefore, we implement multi-stage Bayesian inference to effectively optimize conventional prototype classifiers. Comprehensive experiments corroborate the effectiveness of our strategy, demonstrating substantial improvement of the model’s discriminative capability in few-shot classification scenarios. The source code of this study is available at: https://github.com/CharlesXu2004/MBPRFW .},
  archive      = {J_PAAA},
  author       = {Xu, Wei and Zhou, Xiaocong and Xu, Shengxiang and Liu, Fan and Zhang, Chuanyi and Li, Feifan and Cai, Wenwen and Zhou, Jun},
  doi          = {10.1007/s10044-025-01520-y},
  journal      = {Pattern Analysis and Applications},
  month        = {9},
  number       = {3},
  pages        = {1-14},
  shortjournal = {Pattern Anal. Appl.},
  title        = {Multi-stage bayesian prototype refinement with feature weighting for few-shot classification},
  volume       = {28},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Lyolo: A lightweight object detection algorithm integrating label enhancement for high-quality prediction boxes. <em>PAAA</em>, <em>28</em>(3), 1-12. (<a href='https://doi.org/10.1007/s10044-025-01528-4'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the field of object detection, most researchers overlook the relationship between predicted bounding boxes and ground truth boxes. Moreover, the downsampling of conventional convolution reduces image resolution, often sacrificing some details and edge information, impacting the precise determination of object positions. Meanwhile, the feature extraction capability of the backbone network in enhancement algorithms is crucial for the detection performance of the entire model. To address these issues, this paper proposes a high-quality prediction box based object detection algorithm LYOLO. It suppresses low-quality prediction boxes and enhances high-quality ones, devising a Label Enhancement (LE) strategy to effectively adjust the weights of positive and negative samples. Meanwhile, a lightweight downsampling method (Down) and a lightweight Feature Enhancement (FE) mechanism are designed. The former enlarges the receptive field to improve the model’s ability to determine object positions, and the latter further allocates feature weights to generate stronger feature representations for the backbone network. Experimental results on the VOC and COCO datasets demonstrate that LYOLO, across all sizes, performs exceptionally well. It achieves the highest accuracy with the lowest number of parameters and computational complexity while maintaining low latency. For example, LYOLOn achieves an $${mAP}_{0.5}$$ of 82.0% on the VOC dataset with only 2.28M parameters. Compared to the baseline model YOLO11n, it reduces the number of parameters by 11.9% while improving $${mAP}_{0.5}$$ by 3.0%. In comparison with YOLOv8n, YOLOv9t, and YOLOv10n, LYOLOn achieves $${mAP}_{0.5}$$ improvements of 3.4%, 2.3%, and 3.1%, respectively. The code and datasets used in this article can be obtained from https://github.com/lingzhiy/LYOLO .},
  archive      = {J_PAAA},
  author       = {Gao, Ruxin and Ling, Zhiyong and Wang, Chengyang and Li, Xiang and She, Jianmin and Liu, Qunpo},
  doi          = {10.1007/s10044-025-01528-4},
  journal      = {Pattern Analysis and Applications},
  month        = {9},
  number       = {3},
  pages        = {1-12},
  shortjournal = {Pattern Anal. Appl.},
  title        = {Lyolo: A lightweight object detection algorithm integrating label enhancement for high-quality prediction boxes},
  volume       = {28},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Hierarchical windowed graph attention transformer encoder and a large scale dataset for indian sign language recognition. <em>PAAA</em>, <em>28</em>(3), 1-20. (<a href='https://doi.org/10.1007/s10044-025-01529-3'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Sign Language (SL) recognition is a crucial task in computer vision research to identify signs from the articulation of hand shapes, hand gestures, body movements and facial expressions. Building robust SL recognition systems requires large-scale datasets with accurate labels. However, such datasets are scarce for Indian Sign Language (ISL). This paper addresses this gap by introducing a large-scale dataset of isolated ISL signs. The dataset encompasses 2002 frequently used words in the deaf community, signed by 20 deaf adult signers and contains 40033 isolated videos. We split the dataset into train, validation and test sets with no signer intersection for benchmarking models. We follow the keypoint-based approach and report the results of some existing models on this dataset. Traditional architectures process spatial and temporal features sequentially. We introduce a novel keypoint-based model called the Hierarchical Windowed Graph Attention Transformer Encoder (HWGATE) that simultaneously aggregates spatial and temporal features using a unified spatio-temporal graph. Additionally, it enhances robustness through keypoint grouping. This model achieves an accuracy of $$95.67\%$$ on the presented dataset. We pre-trained the proposed model on the new dataset and then fine-tune it on the smaller ISL INCLUDE dataset, which yields state-of-the-art results of $$98.28\%$$ accuracy, beating its baseline by 12.68 percentage points. The presented dataset and the model implementation code are available at https://cs.rkmvu.ac.in/~isl .},
  archive      = {J_PAAA},
  author       = {Patra, Suvajit and Maitra, Arkadip and Tiwari, Megha and Kumaran, K. and Prabhu, Swathy and Punyeshwarananda, Swami and Samanta, Soumitra},
  doi          = {10.1007/s10044-025-01529-3},
  journal      = {Pattern Analysis and Applications},
  month        = {9},
  number       = {3},
  pages        = {1-20},
  shortjournal = {Pattern Anal. Appl.},
  title        = {Hierarchical windowed graph attention transformer encoder and a large scale dataset for indian sign language recognition},
  volume       = {28},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Fast maritime horizon line tracking with on-off region-of-interest control. <em>PAAA</em>, <em>28</em>(3), 1-11. (<a href='https://doi.org/10.1007/s10044-025-01527-5'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Detecting and tracking the horizon line in maritime videos is crucial for various applications, such as navigation and situational awareness. Many existing algorithms for horizon line detection are highly accurate and perform well in maritime environments. However, most of these algorithms require substantial computational resources, making real-time processing impractical, particularly for large video frames. This paper presents a fast horizon line tracking algorithm that changes the region-of-interest (ROI) with an on-off control approach in each new frame. To enhance tracking accuracy, a new error metric is introduced and its Z-score is calculated to determine whether to use the full frame or ROI. The proposed method demonstrates significant improvement in processing speed while preserving accuracy. Specifically, in the Singapore On-board dataset videos, the proposed method doubled the processing speed while achieving a median positional and angular error of 1.03 pixels and 0.15 degrees, respectively. In the more challenging Buoy dataset, the total processing speed is increased by 27.1% compared to full-frame processing, with a low median positional and angular error of 0.99 pixels and 0.71 degrees, respectively. These results underscore the effectiveness of the adaptive ROI-based approach in achieving fast and accurate horizon line tracking in maritime environments. Additionally, this tracking method can be integrated with any horizon line detection algorithm, enhancing its applicability across different maritime and non-maritime environments. The result videos, showcasing the outcomes of this research, are available at https://doi.org/10.5281/zenodo.15369479 .},
  archive      = {J_PAAA},
  author       = {Agaoglu, Ahmet and Topaloglu, Nezih},
  doi          = {10.1007/s10044-025-01527-5},
  journal      = {Pattern Analysis and Applications},
  month        = {9},
  number       = {3},
  pages        = {1-11},
  shortjournal = {Pattern Anal. Appl.},
  title        = {Fast maritime horizon line tracking with on-off region-of-interest control},
  volume       = {28},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Learning a geometric deep representation to classify parkinson smooth pursuit patterns. <em>PAAA</em>, <em>28</em>(3), 1-14. (<a href='https://doi.org/10.1007/s10044-025-01514-w'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Parkinson’s disease (PD) is characterized by the degeneration of dopaminergic neurotransmitters, leading to motor disturbances that are subtle in the prodromal stages but become more pronounced as the disease progresses. These disturbances report variations regarding manifestation scale and patient phenotyping. Currently, Smooth Pursuit Eye Movement (SPEM) analysis has been suggested to be a potential biomarker for PD. However, conventional recording SPEM methods involve intrusive procedures, specialized protocols, and mainly provide information based on a single global displacement trajectory. We hypothesize that SPEM patterns encompass a diverse range of movement, characterized by intricate spatio-temporal relationships, which may be related to PD, even at early stages. This work introduces a novel end-to-end deep learning representation model that encodes spatio-temporal SPEM patterns and captures geometric second-order relationships to differentiate between PD and control subjects. The geometric learning scheme considers a Riemannian manifold structure from the spatio-temporal deep activations resulting from 3D volumetric convolutions of a set of video recordings. Following a non-intrusive video-based recording protocol, the proposed approach achieved an excellent AUC-ROC score across several SPEM task configurations, with a total of 22 subjects (11 control and 11 PD patients) participating in the study. The geometrical learning encodes spatio-temporal SPEM relationships to support the classification between PD patients and control subjects.},
  archive      = {J_PAAA},
  author       = {Celis, Luis Fernando and Olmos, Juan and Manzanera, Antoine and Martínez, Fabio},
  doi          = {10.1007/s10044-025-01514-w},
  journal      = {Pattern Analysis and Applications},
  month        = {9},
  number       = {3},
  pages        = {1-14},
  shortjournal = {Pattern Anal. Appl.},
  title        = {Learning a geometric deep representation to classify parkinson smooth pursuit patterns},
  volume       = {28},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Sentiment aware kernel mapping recommender system for amazon product recommendations. <em>PAAA</em>, <em>28</em>(3), 1-23. (<a href='https://doi.org/10.1007/s10044-025-01521-x'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recommender systems help address information overload in domains such as e-commerce, entertainment, travel, education, and social media. Their major objective is to generate personalized recommendations that help users find relevant items. However, cold start and data sparsity issues hinder the model’s ability to understand user preferences accurately. Moreover, quantitative ratings alone do not capture the full spectrum of user sentiment, hence, to increase the reliability of recommender systems, analyzing user-generated textual reviews can provide deeper insights into user opinions. From this perspective, we have exploited user sentiment analysis in kernel mapping recommender systems referred to as KSR. Specifically, we propose a sentiment-based kernel mapping recommender that applies sentiment analysis to user reviews. Instead of traditional rating-based matrices, we construct user and item-level kernels over sentiment-enriched user-product matrices. The proposed KSR learns the multi-linear mapping between encoded vectors of user-product associations and the probability density function that determines the user’s sentiments toward products. In the second stage, we integrate quantitative ratings with sentiment information using both additive and multiplicative modeling approaches within the KSR. The proposed model is validated on three different product datasets from Amazon including Food, Software, and Music Reviews. The findings indicate that the proposed sentiment-based KSR model outperforms the original KMR method by achieving the best MAE error of 0.3306 and precision@20 score of 75.44% respectively.},
  archive      = {J_PAAA},
  author       = {Bukhari, Maryam and Maqsood, Muazzam and Ghazanfar, Mustansar Ali and Sattar, Asma},
  doi          = {10.1007/s10044-025-01521-x},
  journal      = {Pattern Analysis and Applications},
  month        = {9},
  number       = {3},
  pages        = {1-23},
  shortjournal = {Pattern Anal. Appl.},
  title        = {Sentiment aware kernel mapping recommender system for amazon product recommendations},
  volume       = {28},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A new criterion based on the distribution of cluster onsets for interpreting acoustic emission data signals: Three case studies in structural and process monitoring. <em>PAAA</em>, <em>28</em>(3), 1-19. (<a href='https://doi.org/10.1007/s10044-025-01523-9'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Structural health monitoring relies on non-destructive techniques such as acoustic emission that generate large amounts of data over the lifespan of systems. Clustering methods are used to interpret these data and gain insights into damage progression and mechanisms. Conventional methods for evaluating clustering results utilise clustering validity indices (CVI) that prioritise compact and separable clusters. This paper introduces a novel approach based on the temporal sequence of cluster onsets, indicating the initial appearance of potential damage and allowing for early detection of defect initiation. The proposed CVI is based on the Kullback–Leibler divergence and can incorporate prior information about damage onsets when available. Three experiments on real-world datasets validate the effectiveness of the proposed method. The first benchmark focuses on detecting the loosening of bolted plates under vibration, where the onset-based CVI outperforms the conventional approach in both cluster quality and the accuracy of bolt loosening detection. The results demonstrate not only superior cluster quality but also unmatched precision in identifying cluster onsets, whether during uniform or accelerated damage growth. The two additional applications stem from industrial contexts. The first focuses on micro-drilling of hard materials using electrical discharge machining, demonstrating, for the first time, that the proposed criterion can effectively retrieve electrode progression to the reference depth, thus validating the setting of the machine to ensure structural integrity. The final application involves damage understanding in a composite/metal hybrid joint structure, where the cluster timeline is used to establish a scenario leading to critical failure due to slippage.},
  archive      = {J_PAAA},
  author       = {Ramasso, Emmanuel and Nkogo, Martin Mbarga and Chandarana, Neha and Bourbon, Gilles and Le Moal, Patrice and Lefèbvre, Quentin and Personeni, Martial and Soutis, Constantinos and Gresil, Matthieu and Thibaud, Sébastien},
  doi          = {10.1007/s10044-025-01523-9},
  journal      = {Pattern Analysis and Applications},
  month        = {9},
  number       = {3},
  pages        = {1-19},
  shortjournal = {Pattern Anal. Appl.},
  title        = {A new criterion based on the distribution of cluster onsets for interpreting acoustic emission data signals: Three case studies in structural and process monitoring},
  volume       = {28},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). SCSP-YOLO: A method for detecting surface defects on strip steel. <em>PAAA</em>, <em>28</em>(3), 1-14. (<a href='https://doi.org/10.1007/s10044-025-01530-w'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Reliable defect detection plays a vital role in preserving the quality and safety of strip steel. Traditional manual inspection is time-consuming, labor-intensive, and prone to inconsistencies, while many existing automated approaches suffer from high false detection rate, high miss rate, and slow processing speed. To address the challenges, this paper proposes SCSP-YOLO, an improved YOLOv5s-based algorithm for strip surface defect detection. First, a CSPDConv module has been introduced to substitute the C3 module in YOLOv5’s backbone feature extraction, improving performance, which enhances the feature extraction capability in low-resolution images. Second, the improved spatial pyramid pooling-fast structure broadens the sensory field through multi-gradient flow and effectively preserves semantic information across different scales, thereby improving the performance of multiscale feature fusion. Lastly, designed to address poor-quality anchor frames, the Wise-IoU loss function improves both stability and accuracy in detection tasks. On the NEU-DET dataset, SCSP-YOLO demonstrates an average precision of 80.1%, marking a 5.2% improvement over YOLOv5s.The frames per second reaches 104, which balances between the accuracy and speed of the algorithm. Therefore, the proposed algorithm can better satisfy the needs of practical industrial inspection.},
  archive      = {J_PAAA},
  author       = {Jiang, Shiqi and Xu, Yuebing and Lu, Hainan and Wang, Shuai},
  doi          = {10.1007/s10044-025-01530-w},
  journal      = {Pattern Analysis and Applications},
  month        = {9},
  number       = {3},
  pages        = {1-14},
  shortjournal = {Pattern Anal. Appl.},
  title        = {SCSP-YOLO: A method for detecting surface defects on strip steel},
  volume       = {28},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Explaining and visualizing black-box models through counterfactual paths. <em>PAAA</em>, <em>28</em>(3), 1-19. (<a href='https://doi.org/10.1007/s10044-025-01532-8'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Explainable AI (XAI) is an increasingly important area of machine learning research, which aims to make black-box models transparent and interpretable. In this paper, we propose a novel approach to XAI that uses the so-called counterfactual paths for model-agnostic global explanations. The algorithm measures feature importance by identifying sequential permutations of features that most influence changes in model predictions. It is particularly suitable for generating explanations based on counterfactual paths in knowledge graphs incorporating domain knowledge. Counterfactual paths introduce an additional graph dimension to current XAI methods in both explaining and visualizing black-box models. Experiments with synthetic and bio-medical data demonstrate the practical applicability of our approach.},
  archive      = {J_PAAA},
  author       = {Pfeifer, Bastian and Krzyzinski, Mateusz and Baniecki, Hubert and Holzinger, Andreas and Biecek, Przemyslaw},
  doi          = {10.1007/s10044-025-01532-8},
  journal      = {Pattern Analysis and Applications},
  month        = {9},
  number       = {3},
  pages        = {1-19},
  shortjournal = {Pattern Anal. Appl.},
  title        = {Explaining and visualizing black-box models through counterfactual paths},
  volume       = {28},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Novel construction methods for picture fuzzy divergence measures with applications in pattern recognition, MADM, and clustering analysis. <em>PAAA</em>, <em>28</em>(2), 1-28. (<a href='https://doi.org/10.1007/s10044-024-01383-9'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Divergence measure of picture fuzzy sets is a valuable tool to study the problems related to decision-making, pattern classification, and clustering analysis. However, existing divergence/distance measures are sometimes ineffective in capturing the intricacies of uncertainty and imprecision inherent in picture fuzzy sets. In view of the theoretical and experimental weaknesses of existing picture fuzzy divergence/distance measures, this article introduces novel construction methods for deriving picture fuzzy divergence measures. The first one is inductive which utilizes existing intuitionistic fuzzy divergence and the second is based on forming picture fuzzy divergence measures from existing picture fuzzy divergence measures. Additionally, we have shown that restriction on the neutrality degree in picture fuzzy divergence is an intuitionistic fuzzy divergence. Moreover, we suggested a new picture fuzzy divergence measure utilizing the proposed approach and applied it to solve practical problems concerned with decision-making, pattern classification, and clustering analysis. The performance indices “Degree of Confidence” and “Cluster Validity Index” in the picture fuzzy framework further appreciated the advantages of the proposed measures. Comparative studies with existing picture fuzzy distance/divergence measures demonstrated the effectiveness and superiority of the proposed divergence measures.},
  archive      = {J_PAAA},
  author       = {Singh, Surender and Singh, Koushal},
  doi          = {10.1007/s10044-024-01383-9},
  journal      = {Pattern Analysis and Applications},
  month        = {6},
  number       = {2},
  pages        = {1-28},
  shortjournal = {Pattern Anal. Appl.},
  title        = {Novel construction methods for picture fuzzy divergence measures with applications in pattern recognition, MADM, and clustering analysis},
  volume       = {28},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Integrated multi-local and global dynamic perception structure for sign language recognition. <em>PAAA</em>, <em>28</em>(2), 1-14. (<a href='https://doi.org/10.1007/s10044-024-01403-8'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Current sign language recognition methods often focus on whole-body features, neglecting the detailed dynamic information of key body parts. We propose an integrated multi-local and global dynamic perception structure based on a hybrid feature fusion approach that fuses features from multiple local paths into the global path, aiming to benefit from the diverse local information available in videos. First, the multi-local dynamic perception module is designed to extract multiple sign language-related spatial features with fine-grained information on local body dynamics. This module is achieved by expanding multi-local features in the channel dimension, permitting the processing of different perspectives independently for multiple local feature inputs. Moreover, we design a multi-local to global fusion module that generates multi-local fusion representations encompassing both temporal and spatial dimensions. This module integrates the fusion of deep features from multiple local dynamics, to be integrated with shallow features of the global module, achieving a match between the deep features of the multi-local to global fusion module and the shallow features of the global dynamic perception module. Finally, extensive experiments based on several sign language recognition benchmarks demonstrate that our integrated multi-local and global dynamic perception structure effectively improves performance of sign language recognition models, and significantly outperforms a number of competitive baselines.},
  archive      = {J_PAAA},
  author       = {Liang, Siyu and Li, Yunan and Shi, Yuanyuan and Chen, Huizhou and Miao, Qiguang},
  doi          = {10.1007/s10044-024-01403-8},
  journal      = {Pattern Analysis and Applications},
  month        = {6},
  number       = {2},
  pages        = {1-14},
  shortjournal = {Pattern Anal. Appl.},
  title        = {Integrated multi-local and global dynamic perception structure for sign language recognition},
  volume       = {28},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Enhancing semantic audio-visual representation learning with supervised multi-scale attention. <em>PAAA</em>, <em>28</em>(2), 1-14. (<a href='https://doi.org/10.1007/s10044-025-01414-z'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, breakthroughs in large models such as GPT and Transformers have demonstrated extraordinary versatility and power in various fields and tasks. However, despite significant progress in areas such as natural language processing, these models still face some unique challenges when processing multimodal data. Data from different modalities often contain significantly different characteristics, and the heterogeneity gap between different modalities makes it difficult to fuse these data to extract valuable information. To integrate and align semantic meanings between audio-visual modalities, this paper proposes a novel supervised multi-scale attention for enhancing semantic audio-visual representation learning from multimedia data, utilizing the audio-visual attention mechanism to combine multi-scale features. Specifically, we explore multi-scale feature extraction and audio-visual attention architecture, which computes cross-attention weights based on the correlation between joint feature representations and single-modal representations. In addition, the model is guided to learn powerful discriminative features by minimizing intra-modal and inter-modal discriminative losses and maximizing cross-modal correlations. With the widely used VEGAS and AVE benchmark datasets, our model demonstrates competitive experimental results. Extensive experiments verify that the proposed method significantly outperforms the state-of-the-art cross-modal retrieval methods.},
  archive      = {J_PAAA},
  author       = {Zhang, Jiwei and Yu, Yi and Tang, Suhua and Qi, GuoJun and Wu, Haiyuan and Hachiya, Hirotaka},
  doi          = {10.1007/s10044-025-01414-z},
  journal      = {Pattern Analysis and Applications},
  month        = {6},
  number       = {2},
  pages        = {1-14},
  shortjournal = {Pattern Anal. Appl.},
  title        = {Enhancing semantic audio-visual representation learning with supervised multi-scale attention},
  volume       = {28},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multitask learning of adversarial-contrastive graph for recommendation. <em>PAAA</em>, <em>28</em>(2), 1-13. (<a href='https://doi.org/10.1007/s10044-025-01417-w'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recommendation systems play a critical role in our daily lives. Despite great progress, existing graph-based recommendation methods still suffer from challenges including skewed data distribution, vulnerability to noises, and sparse supervision signal. We attribute the inferior performance to the limited discriminative ability of the learned representations. To remedy this, in this paper, we develop a framework termed Multi-ACG by introducing self-supervised learning, adversarial learning, and multitask learning to learn representations with higher discrimination. Specifically, self-supervised learning and adversarial learning are first employed to synthesize hard samples for training. Meanwhile, multi-task learning is adopted to balance different loss terms for optimization. Experiments are conducted on benchmark datasets and the results have demonstrated the state-of-the-art performance of the proposed method against previous ones. The code is at https://github.com/xiaoma666123/Multi-ACG.},
  archive      = {J_PAAA},
  author       = {Ma, Xingyu and Wang, Chuanxu},
  doi          = {10.1007/s10044-025-01417-w},
  journal      = {Pattern Analysis and Applications},
  month        = {6},
  number       = {2},
  pages        = {1-13},
  shortjournal = {Pattern Anal. Appl.},
  title        = {Multitask learning of adversarial-contrastive graph for recommendation},
  volume       = {28},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Pre-trained noise based unsupervised GAN for fruit disease classification in imbalanced datasets. <em>PAAA</em>, <em>28</em>(2), 1-19. (<a href='https://doi.org/10.1007/s10044-025-01418-9'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Early disease diagnosis in edible fruits and vegetables is crucial for sustainable economic agricultural production. Recently, deep neural networks have explicitly shown exceptional performance in early disease recognition. However, an insufficient and scarce dataset is a critical issue for training the neural network, which makes dataset acquisition a fundamental obstacle in enhancing the performance of deep network models. A considerable amount of dataset acquisition necessitates an additional, expensive effort owing to time constraints and expert requirements. To mitigate this challenge, a novel data augmentation method, FruitGAN, exploiting the generative adversarial architecture, has been developed. The variational autoencoder transforms the random Gaussian noise into a pre-trained noise vector, which is then input into the proposed FruitGAN method. The FruitGAN method is equipped with a self-attention, residual block, and super-resolution module to maintain tiny lesions, structural integrity, and perceptual quality in the generated fruit images. Moreover, a new real-field eggplant dataset containing the four pathogens and one healthy category, aggregating 1325 samples, has been collected from on-field farms. The proposed FruitGAN method is leveraged to generate real-like synthetic images of the eggplant to avoid class imbalance problems. The effectiveness of FruitGAN is tested on the eggplant dataset in terms of FID and SSIM scores, and the results are compared with seven other State-Of-The-Art GAN models. Furthermore, the classification performance of the FruitGAN-generated dataset has also been tested against nine pre-trained deep networks namely, AlexNet, VGG16, VGG19, ResNet50, ResNet101, DenseNet101, InceptionV3, Xception, and MobileNetV2 and four hybrid networks, namely, InceptionV3 + VGG16, SVM + VGG19, CNN + SVM, and MobileNet + Xception using transfer learning and evaluating the performance by test data. The experimental results affirmed that the developed FruitGAN outperformed all other considered GAN models by achieving 112.88 FID and 0.94 SSIM scores. Moreover, the classification accuracy of the FruitGAN augmented dataset was recorded as 95.74%, which is the highest among other considered GAN models. The code and datasets of the proposed method are available at https://github.com/ersachingupta11/FruitGAN},
  archive      = {J_PAAA},
  author       = {Gupta, Sachin and Tripathi, Ashish Kumar and Lewis, Nkenyereye},
  doi          = {10.1007/s10044-025-01418-9},
  journal      = {Pattern Analysis and Applications},
  month        = {6},
  number       = {2},
  pages        = {1-19},
  shortjournal = {Pattern Anal. Appl.},
  title        = {Pre-trained noise based unsupervised GAN for fruit disease classification in imbalanced datasets},
  volume       = {28},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Bidirectional feature fusion via cross-attention transformer for chrysanthemum classification. <em>PAAA</em>, <em>28</em>(2), 1-16. (<a href='https://doi.org/10.1007/s10044-025-01419-8'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Chrysanthemums hold significant ornamental, economic, and medicinal value, with their quality and economic worth heavily influenced by geographic origin. Accurate classification of chrysanthemums is crucial for ensuring product authenticity, boosting consumer trust, and promoting sustainable industry growth. Traditional classification methods, however, suffer from inefficiency and high costs. To address these challenges, we propose a novel chrysanthemum classification method utilizing a bidirectional feature fusion approach via cross-attention and two-stream network fusion. Our method preprocesses front and back images of chrysanthemums from diverse regions, employing the powerful Swin Transformer as the backbone to extract features. The cross-attention mechanism effectively integrates features from both image sides, and a secondary training strategy further enhances the model’s generalization capabilities. Experimental results demonstrate that our method achieves higher accuracy, precision, recall, and F1 score compared to state-of-the-art models, highlighting its potential for accurate chrysanthemum origin tracing. The code and datasets are openly available at https://github.com/dart-into/CCMCAM , ensuring transparency and reproducibility of our findings.},
  archive      = {J_PAAA},
  author       = {Chen, Yifan and Yang, Xichen and Yan, Hui and Liu, Jia and Jiang, Jian and Mao, Zhongyuan and Wang, Tianshu},
  doi          = {10.1007/s10044-025-01419-8},
  journal      = {Pattern Analysis and Applications},
  month        = {6},
  number       = {2},
  pages        = {1-16},
  shortjournal = {Pattern Anal. Appl.},
  title        = {Bidirectional feature fusion via cross-attention transformer for chrysanthemum classification},
  volume       = {28},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Remote sensing image change detection network with multi-scale feature information mining and fusion. <em>PAAA</em>, <em>28</em>(2), 1-15. (<a href='https://doi.org/10.1007/s10044-025-01420-1'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Change detection (CD) aims to predict the pixels that have changed in an image by comparing images from different times. CNN is excellent at local feature extraction, while Transformer is excellent at extracting global features. However, simple CD networks that extract fused local and global feature information have limitations in their discriminative ability. This is due to the underutilization of local and global information for multi-scale features. For this reason, this paper proposes a remote sensing image change detection network for multi-scale feature information mining and fusion (MSFIMF-RSCDNet). Firstly, based on the hierarchical features displaying different levels of information, we design a selective convolutional attention module (SCBAM) to improve the distinguishability of multi-scale features. Subsequently, a cascaded cross-self-attention module (CCSAM) is proposed to refine the global information of the multi-scale features, and finally, a high-level feature-guided multi-scale feature fusion module (HFGFFM) is utilized to improve the discriminability of the model for objects of different sizes. We show through experiments on three public optical remote sensing image CD datasets, LEVIR-CD (Chen and Shi in Remote Sens 12(10):1662, 2020), WHU-CD (Ji et al. in Trans Geosci Remote Sens 57(1):574–586, 2018), and CDD (Lebedev et al. in Int Arch Photogramm Remote Sens Spat Inf Sci 42:565–571, 2018), that stronger change detection CD performance is achieved than other commonly used methods.},
  archive      = {J_PAAA},
  author       = {Xue, Songdong and Zhang, Minming and Qiao, Gangzhu and Zhang, Chaofan and Wang, Bin},
  doi          = {10.1007/s10044-025-01420-1},
  journal      = {Pattern Analysis and Applications},
  month        = {6},
  number       = {2},
  pages        = {1-15},
  shortjournal = {Pattern Anal. Appl.},
  title        = {Remote sensing image change detection network with multi-scale feature information mining and fusion},
  volume       = {28},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). TabMixer: Advancing tabular data analysis with an enhanced MLP-mixer approach. <em>PAAA</em>, <em>28</em>(2), 1-17. (<a href='https://doi.org/10.1007/s10044-025-01423-y'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Tabular data, prevalent in relational databases and spreadsheets, is fundamental across fields like healthcare, engineering, and finance. Despite significant advances in tabular data learning, critical challenges remain: handling missing values, addressing class imbalance, enabling transfer learning, and facilitating feature incremental learning beyond traditional supervised paradigms. We introduce TabMixer, an innovative model that enhances the multilayer perceptron (MLP) mixer architecture to address these challenges. TabMixer incorporates a self-attention mechanism, making it versatile across various learning scenarios including supervised learning, transfer learning, and feature incremental learning. Extensive experiments on eight public datasets demonstrate TabMixer’s superior performance over existing state-of-the-art methods. Notably, TabMixer achieved substantial improvements in ANOVA AUC across all scenarios: a 4% increase in supervised learning (0.840 to 0.881), 8% in transfer learning (0.803 to 0.872), and 4% in feature incremental learning (0.806 to 0.843). TabMixer demonstrates high computational efficiency and scalability through reduced floating-point operations and learnable parameters. Moreover, it exhibits strong resilience to missing values and class imbalances through both its architectural design and optional preprocessing enhancements. These results establish TabMixer as a promising model for tabular data analysis and a valuable tool for diverse applications.},
  archive      = {J_PAAA},
  author       = {Eslamian, Ali and Cheng, Qiang},
  doi          = {10.1007/s10044-025-01423-y},
  journal      = {Pattern Analysis and Applications},
  month        = {6},
  number       = {2},
  pages        = {1-17},
  shortjournal = {Pattern Anal. Appl.},
  title        = {TabMixer: Advancing tabular data analysis with an enhanced MLP-mixer approach},
  volume       = {28},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). LMFR-net: Lightweight multi-scale feature refinement network for retinal vessel segmentation. <em>PAAA</em>, <em>28</em>(2), 1-15. (<a href='https://doi.org/10.1007/s10044-025-01424-x'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Retinal vessel segmentation is a crucial step in analyzing fundus images and plays a vital role in the early detection, diagnosis, and treatment of various diseases. To make the segmentation model more applicable to actual medical scenarios, A Lightweight Multi-scale Feature Refinement Network (LMFR-Net) based on dual-decoding structure is proposed for efficient retinal vessel segmentation. Using a dual-decoding structure to reduce information loss, an Improved Convolution Block (ICB) is proposed to enhance the ability to extract basic features. In addition, a Lightweight Multi-scale Attention Feature Fusion (LMAFF) module is designed to extract the multi-scale spatial structure features. A Feature Refinement Module (FRM) with dense connections is proposed to optimize detailed features and comprehensively improve network segmentation capability. Comparative experiments were conducted on the DRIVE, CHASEDB1, and STARE datasets to verify that LMFR-Net achieved the highest F1-score and Recall of 82.91% and 86.85%, respectively, with only 366kb of parameters. More refined segmentation results have also been achieved in the visualization comparison of segmented images, and the overall segmentation effect is well. This indicates that LMFR-Net achieves efficient retinal vessel segmentation with a significantly reduced computational complexity, making it well-suited for practical medical applications. The code is available at https://github.com/MCloud31/LMFR-Net .},
  archive      = {J_PAAA},
  author       = {Zhang, WenHao and Qu, ShaoJun and Feng, YueWen},
  doi          = {10.1007/s10044-025-01424-x},
  journal      = {Pattern Analysis and Applications},
  month        = {6},
  number       = {2},
  pages        = {1-15},
  shortjournal = {Pattern Anal. Appl.},
  title        = {LMFR-net: Lightweight multi-scale feature refinement network for retinal vessel segmentation},
  volume       = {28},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A fast and accurate 3D lung tumor segmentation algorithm. <em>PAAA</em>, <em>28</em>(2), 1-14. (<a href='https://doi.org/10.1007/s10044-025-01425-w'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, we propose a lung tumor segmentation algorithm based on the Allen–Cahn (AC) energy equation. The novelty lies in the fact that, when extracting the energy matrix using the AC energy equation, we employ a sliding window algorithm for feature extraction on the data without neglecting local features. After obtaining the energy matrix, we construct constraint conditions based on the minimum and maximum values in the matrix, forming an arithmetic progression. Due to the flexibility in setting the sliding window size and constraint conditions, we can achieve segmentation results according to different requirements. In the numerical experiments, we conduct segmentation experiments of varying difficulty in both two-dimensional (2D) and three-dimensional (3D) spaces to verify the effectiveness of the proposed method. When addressing the lung tumor segmentation problem, we compare the maximum diameter of 3D lung tumors segmented by our proposed segmentation algorithm with the maximum diameter of lung tumors in the original 2D CT images to validate the segmentation accuracy and significance of the proposed method. By conducting more detailed and precise measurements and segmentations of tumors in 3D space, this approach contributes to advancements in medical science and enhances patient treatment outcomes. We also conduct tumor segmentation experiments on the MSD and LIDC-IDRI datasets, setting up comparison metrics to further verify the method’s effectiveness.},
  archive      = {J_PAAA},
  author       = {Wang, Jian and Han, Ziwei and Chen, Xinlei and Kim, Junseok},
  doi          = {10.1007/s10044-025-01425-w},
  journal      = {Pattern Analysis and Applications},
  month        = {6},
  number       = {2},
  pages        = {1-14},
  shortjournal = {Pattern Anal. Appl.},
  title        = {A fast and accurate 3D lung tumor segmentation algorithm},
  volume       = {28},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Visual emotion analysis using skill-based multi-teacher knowledge distillation. <em>PAAA</em>, <em>28</em>(2), 1-15. (<a href='https://doi.org/10.1007/s10044-025-01426-9'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The biggest challenge in visual emotion analysis (VEA) is bridging the affective gap between the features extracted from an image and the emotion it expresses. It is therefore essential to rely on multiple cues to have decent predictions. Recent approaches use deep learning models to extract rich features in an automated manner, through complex frameworks built with multi-branch convolutional neural networks and fusion or attention modules. This paper explores a different approach, by introducing a three-step training scheme and leveraging knowledge distillation (KD), which reconciles effectiveness and simplicity, and thus achieves promising performances despite using a very basic CNN. KD is involved in the first step, where a student model learns to extract the most relevant features on its own, by reproducing those of several teachers specialized in different tasks. The proposed skill-based multi-teacher knowledge distillation (SMKD) loss also ensures that for each instance, the student focuses more or less on the teachers depending on their capacity to obtain a good prediction, i.e. their relevance. The two remaining steps serve respectively to train the student’s classifier and to fine-tune the whole model, both for the VEA task. Experiments on two VEA databases demonstrate the gain in performance offered by our approach, where the students consistently outperform their teachers, and also state-of-the-art methods.},
  archive      = {J_PAAA},
  author       = {Cladière, Tristan and Alata, Olivier and Ducottet, Christophe and Konik, Hubert and Legrand, Anne-Claire},
  doi          = {10.1007/s10044-025-01426-9},
  journal      = {Pattern Analysis and Applications},
  month        = {6},
  number       = {2},
  pages        = {1-15},
  shortjournal = {Pattern Anal. Appl.},
  title        = {Visual emotion analysis using skill-based multi-teacher knowledge distillation},
  volume       = {28},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Single-stage convolutional neural radiance fields. <em>PAAA</em>, <em>28</em>(2), 1-12. (<a href='https://doi.org/10.1007/s10044-025-01427-8'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Novel view synthesis captured from multiple images is a critical research topic in computer vision and computational photography due to its wide range of applications. Neural radiance fields significantly improve performance by optimizing continuous volumetric scene functions using a multi-layer perceptron. Although neural radiance fields and their modifications provide high-quality scenes, they have limitation in optimizing exact radiance fields due to their hierarchical architecture comprising coarse and fine networks. They also require numerous parameters and generally do not consider local and global relationships between samples on a ray. This paper proposes a unified single-stage paradigm that jointly learns the relative position of three-dimensional rays and their relative color and density for complex scenes using a convolutional neural network to reduce noise and irrelevant features and prevent overfitting. Experimental results including ablation tests verify the proposed approach’s superior robustness to current state-of-the-art models for synthesizing novel views. The code is available at https://github.com/xkdytk/scorf .},
  archive      = {J_PAAA},
  author       = {Lee, Yoonjae and Yoon, Gang-Joon and Song, Jinjoo and Yoon, Sang Min},
  doi          = {10.1007/s10044-025-01427-8},
  journal      = {Pattern Analysis and Applications},
  month        = {6},
  number       = {2},
  pages        = {1-12},
  shortjournal = {Pattern Anal. Appl.},
  title        = {Single-stage convolutional neural radiance fields},
  volume       = {28},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Tsi-cnn-net: Truly shift-invariant convolutional neural network for indian sign language recognition system. <em>PAAA</em>, <em>28</em>(2), 1-19. (<a href='https://doi.org/10.1007/s10044-025-01428-7'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The majority of Indian sign language (ISL) recognition systems applied convolutional neural network (CNN) based deep neural networks. However, the output of CNN image classifiers may vary significantly with a little shift in input images. This shortcoming can be partially addressed by data augmentation, anti-aliasing, or blurring that do not work with different input patterns the network trained on and non-linear activation functions like ReLU, respectively. To deal with this short-coming, an ISL recognition approach has been presented using truly shift-invariant CNN. A sub-sampling strategy i.e. adaptive polyphase sampling (APS) has been applied to allow CNN truly shift-invariant. The proposed system is completely consistent to classification task. Furthermore, it offers significantly outstanding classification accuracy not only on Indian sign language datasets but also on datasets of other sign languages.},
  archive      = {J_PAAA},
  author       = {Ghorai, Anudyuti and Nandi, Utpal and Singh, Moirangthem Marjit and Changdar, Chiranjit and Paul, Bachchu and Chowdhuri, Partha and Pal, Pabitra},
  doi          = {10.1007/s10044-025-01428-7},
  journal      = {Pattern Analysis and Applications},
  month        = {6},
  number       = {2},
  pages        = {1-19},
  shortjournal = {Pattern Anal. Appl.},
  title        = {Tsi-cnn-net: Truly shift-invariant convolutional neural network for indian sign language recognition system},
  volume       = {28},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multi-scale adaptive detail enhancement dehazing network for autonomous driving perception images. <em>PAAA</em>, <em>28</em>(2), 1-14. (<a href='https://doi.org/10.1007/s10044-025-01430-z'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In hazy weather conditions, a significant accumulation of haze poses a severe challenge to the quality of image capture for autonomous driving systems, thereby heightening safety risks for autonomous vehicles. To solve this problem, we propose the multi-scale adaptive detail enhancement dehazing network, an innovative architecture comprising the initial feature extraction module, the multi-scale adaptive feature module, and the terminal detail enhancement module, specifically designed to eradicate haze with precision. To enhance the extraction of multi-scale features, the multi-scale adaptive feature module employs the squeeze-excitation residual dense block (SRD). It not only learns the intricate multi-scale features of the image but also adaptively recalibrates the feature response of each feature map, ultimately bolstering the network’s performance and resilience. The terminal detail enhancement module, crafted with the dilation refinement block (DRB), serves as a compensatory measure for any detail loss or pseudo-artifacts that might arise from the multi-scale adaptive feature module’s operations. By incorporating the terminal detail enhancement module, the overall dehazing effect is further optimized. Empirical evaluations reveal that the proposed multi-scale adaptive detail enhancement dehazing network achieves impressive results, with a PSNR value of 30.82, an SSIM value of 0.967, and an LPIPS value of 0.033. These figures indicate that the network is adept at removing haze from images while preserving intricate details, ensuring the efficacy and reliability of autonomous driving systems in hazy environments. Code is available at https://github.com/murong-carl/Adaptive-multi-scale-detail-enhancement-dehazing-network .},
  archive      = {J_PAAA},
  author       = {Wang, Juan and Wang, Sheng and Wu, Minghu and Yang, Hao and Cao, Ye and Hu, Shuyao and Shao, Jixiang and Zeng, Chunyan},
  doi          = {10.1007/s10044-025-01430-z},
  journal      = {Pattern Analysis and Applications},
  month        = {6},
  number       = {2},
  pages        = {1-14},
  shortjournal = {Pattern Anal. Appl.},
  title        = {Multi-scale adaptive detail enhancement dehazing network for autonomous driving perception images},
  volume       = {28},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Interactive image segmentation combining global seeding and sparse local reconstruction. <em>PAAA</em>, <em>28</em>(2), 1-23. (<a href='https://doi.org/10.1007/s10044-025-01432-x'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Seed segmentation methods are highly regarded for their effectiveness in processing complex images, user-friendliness, and compatibility with graph-based representations. However, these methods often depend on intricate computational tools, leading to issues such as poor image contour adherence and incomplete seed propagation. To address these limitations, this paper proposes an interactive framework that integrates global seed information with sparse local linear reconstruction regularization (GSSR). In this framework, a Gaussian mixture model is firstly employed to construct a flow of global seed information, establishing connections between pixel points and yielding more complete segmented objects. Additionally, the $$L_{p}(0 < p \le 1)$$ norm is utilized to constrain the sparse local reconstruction term, facilitating the generation of sparse boundaries. An iterative process based on the Alternating Direction Method of Multipliers (ADMM) is developed to solve the $$L_1$$ regularization term, which is then generalized for the $$L_p$$ problem through reweighting. We conduct a comprehensive comparison on the BSD dataset, CVC-ClinicDB datasets and two publicly available MSRC datasets with different labeling schemes. Extensive experimental validation demonstrates that the proposed method outperforms existing results.The source code and datasets are openly available at: https://github.com/choppy-water/GSSR .},
  archive      = {J_PAAA},
  author       = {Long, Jianwu and Liu, Yuanqin and Zhang, Kaixin and Chen, Shuang and Luo, Qi},
  doi          = {10.1007/s10044-025-01432-x},
  journal      = {Pattern Analysis and Applications},
  month        = {6},
  number       = {2},
  pages        = {1-23},
  shortjournal = {Pattern Anal. Appl.},
  title        = {Interactive image segmentation combining global seeding and sparse local reconstruction},
  volume       = {28},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). MSDBPN: Multi-column smoothed dilated convolution based back projection network for stereo image super-resolution. <em>PAAA</em>, <em>28</em>(2), 1-11. (<a href='https://doi.org/10.1007/s10044-025-01433-w'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Fully exploiting the parallax information of stereo images for super-resolution (SR) can obtain remarkable performance. The most challenging issue for stereo image SR is how to capture complementary correlation information between the stereo image pair to accurately guide reconstruction. In this paper, we propose a multi-column smoothed dilated convolution based back projection network (MSDBPN) for stereo SR by explicitly learning and exploiting the parallax information. In particular, we incorporate adaptive weighted multi-column smoothed dilated convolutions to rapidly expand the receptive field while maintaining excellent inter-pixel correlation. Meanwhile, we reweight different column feature with adaptive learnable parameter to distinguish contributions. Furthermore, we employ a deep back projection mechanism to calculate projection error and implement self-correction to guide precise reconstruction. Extensive experiments on benchmark datasets demonstrate that our proposed method outperforms other state-of-the-art approaches on both quantitative and qualitative evaluations.},
  archive      = {J_PAAA},
  author       = {Zhou, Zihao and Wang, Yongfang and Lian, Junjie},
  doi          = {10.1007/s10044-025-01433-w},
  journal      = {Pattern Analysis and Applications},
  month        = {6},
  number       = {2},
  pages        = {1-11},
  shortjournal = {Pattern Anal. Appl.},
  title        = {MSDBPN: Multi-column smoothed dilated convolution based back projection network for stereo image super-resolution},
  volume       = {28},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Dynamic prototype-guided structural information maintaining for unsupervised domain adaptation. <em>PAAA</em>, <em>28</em>(2), 1-14. (<a href='https://doi.org/10.1007/s10044-025-01435-8'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Unsupervised Domain Adaptation (UDA) intends to transfer the knowledge learned from labeled source domain to unlabeled target domain. Most existing methods employ domain adversarial training to align the feature space distributions of two domains. However, these methods may destroy the discriminative structural information. In this paper, we propose a Dynamic Prototype-guided Structural Information Maintaining (DPSIM) approach to preserve the structural information of the target domain based on pairwise semantic similarity. Specifically, we propose a dynamic prototype learning module to learn the categorical intrinsic representation of the source domain and then to predict the similarity of pairwise samples of the target domain. Finally, a structural information maintaining module is proposed to restrict the target domain by discriminating structural information. Extensive experiments on both image classification and object detection tasks demonstrate the effectiveness of our method.},
  archive      = {J_PAAA},
  author       = {Li, Deng and Li, Peng and Liu, Jian and Han, Yahong},
  doi          = {10.1007/s10044-025-01435-8},
  journal      = {Pattern Analysis and Applications},
  month        = {6},
  number       = {2},
  pages        = {1-14},
  shortjournal = {Pattern Anal. Appl.},
  title        = {Dynamic prototype-guided structural information maintaining for unsupervised domain adaptation},
  volume       = {28},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Source screen identification using difference image mask obtained from images recaptured through screenshots based on spatial rich features. <em>PAAA</em>, <em>28</em>(2), 1-16. (<a href='https://doi.org/10.1007/s10044-025-01442-9'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As images are integral to many sectors in the digital age, it is essential to ensure their authenticity and integrity. However, the ease of digital image creation and sharing also exposes them to manipulation and misrepresentation, heightening concerns about privacy and misinformation. The process of recapturing, a prevalent anti-forensic technique, poses challenges to tampering detection methods, necessitating effective countermeasures to uphold image credibility. Monitor-screenshots, facilitated by the simplicity of capturing screenshots of Original images displayed on monitors, pose unique challenges in source identification. Addressing this, we propose a novel approach to unveil the screen fingerprint, capturing distinctive irregularities associated with blur exist in Monitor-screenshots for accurate source identification. Leveraging image registration, difference image masking, and sophisticated feature extraction techniques, our method enables precise identification of specific screens, enhancing the authentication of digital content. By scrutinizing screen-specific characteristics and artifacts left during recapture, proposed model can verify the claimed origin of Screenshots, tested on a Screenshot dataset using SVM classifier, offers a robust framework to authenticate digital content and trace its source with precision and reliability, mitigating risks associated with image manipulation and misrepresentation in the digital domain.},
  archive      = {J_PAAA},
  author       = {Anjum, Areesha and Islam, Saiful and Saleem, Mahreen and Siddiqui, Nadia},
  doi          = {10.1007/s10044-025-01442-9},
  journal      = {Pattern Analysis and Applications},
  month        = {6},
  number       = {2},
  pages        = {1-16},
  shortjournal = {Pattern Anal. Appl.},
  title        = {Source screen identification using difference image mask obtained from images recaptured through screenshots based on spatial rich features},
  volume       = {28},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). HCT: Image super-resolution restoration using hierarchical convolution transformer networks. <em>PAAA</em>, <em>28</em>(2), 1-11. (<a href='https://doi.org/10.1007/s10044-025-01413-0'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the computer vision domain, image super-resolution (SR) technology, which restores high-resolution details from low-resolution images, plays a vital role in practical applications such as medical imaging, public safety, and remote sensing. Traditional methods employ convolutional neural networks to address these issues, while Visual Transformers show potential performance in high-level vision tasks. However, compared to typical CNN architecture networks, Visual Transformers exhibit weaker reliance on high-frequency information in images, leading to blurred details and residual artifacts. To solve this issue, we use a hierarchical network structure, which allows for a more flexible feeling field for our approach. Firstly, our method complements lost spatial features using a Convolutional Swin Transformer Layer incorporating a Convolutional Feed Forward Network. This allows for the retrieval of missing spatial information and enhances the model’s representational capabilities. Next, deep feature extraction is performed by combining multiple layers into a Residual Convolutional Swin Transformer Block. Finally, we employ a hierarchical-type structure to combine the features of each branch. Experiments validate the effectiveness of the proposed method in generating images with greater detail aligned with human perception. Based on the experiments, our method is effective on SR tasks with magnification factors of 2, 3, and 4. Our method can reconstruct a clear and complete edge structure. We provide code at https://github.com/Q88392/HCT .},
  archive      = {J_PAAA},
  author       = {Guo, Ying and Tian, Chang and Wang, Han and Liu, Jie and Di, Chong and Ning, Keqing},
  doi          = {10.1007/s10044-025-01413-0},
  journal      = {Pattern Analysis and Applications},
  month        = {6},
  number       = {2},
  pages        = {1-11},
  shortjournal = {Pattern Anal. Appl.},
  title        = {HCT: Image super-resolution restoration using hierarchical convolution transformer networks},
  volume       = {28},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). PCDPose: Enhancing the lightweight 2D human pose estimation model with pose-enhancing attention and context broadcasting. <em>PAAA</em>, <em>28</em>(2), 1-18. (<a href='https://doi.org/10.1007/s10044-025-01431-y'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {2D human pose estimation is an important domain in computer vision. In recent years, the lightweight 2D human pose estimation (2DTLHPE) models based on vision transformer (ViT) have attracted extensive attention due to the fewer parameters and the lower computational requirements. However, these models are also facing the challenges of cluttered and occluded background. It results in the errors of locating keypoints. Therefore, this paper proposes the pose-enhanced contextual distillation for pose estimation model (PCDPose) to alleviate the influence of the challenges. Firstly, PCDPose introduces the pose-enhancing attention (PEA) module which highlights the foreground information in the feature map. It alleviates the influence caused by the cluttered background. Moreover, PCDPose introduces the context broadcasting (CB) module, which builds the long-range dependencies between the keypoints in occluded regions and the neighboring keypoints by broadcasting the context to each vision token (VT). It alleviates the influence caused by the occluded background. Experimental results show that PCDPose achieves a 73.5% average precision (AP) on the COCO2017 dataset, and it has a 1% performance improvement over the state-of-the-art (SOTA) model. On the CrowdPose dataset, PCDPose achieves a 71.3% AP, and it has a 5.9% performance improvement over the SOTA model.},
  archive      = {J_PAAA},
  author       = {Tian, Zhenyuan and Fu, Weina and Woźniak, Marcin and Liu, Shuai},
  doi          = {10.1007/s10044-025-01431-y},
  journal      = {Pattern Analysis and Applications},
  month        = {6},
  number       = {2},
  pages        = {1-18},
  shortjournal = {Pattern Anal. Appl.},
  title        = {PCDPose: Enhancing the lightweight 2D human pose estimation model with pose-enhancing attention and context broadcasting},
  volume       = {28},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). MPFCNet: Multi-scale parallel feature fusion convolutional network for 3D knee segmentation from MR images. <em>PAAA</em>, <em>28</em>(2), 1-15. (<a href='https://doi.org/10.1007/s10044-025-01437-6'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Accurate and automatic segmentation of knee magnetic resonance (MR) images plays a vital role in the diagnosis of osteoarthritis and knee bone diseases. However, the anatomical structure of the knee joint is complex, it is difficult to segment knee joints accurately and efficiently. This paper proposes a knee joint segmentation model from MR image, which is named a multi-scale parallel feature fusion convolutional network (MPFCNet). A Large Kernel Attention (LKA) module is coined in the MPFCNet, which effectively increases the receptive field and preserves detail textures, resulting in better feature extraction. To further utilize complementary information at various scales in both spatial and channel dimensions, a Multi-Scale Fusion (MSF) module is established. A Hybrid Feedforward Attention (HFA) module is proposed to establish long-range dependencies. Experiments and comparisons with state-of-the-art methods were conducted on the publicly available dataset OAI-ZIB. The results show that the MPFCNet achieved excellent segmentation results on the knee joint segmentation task, improving the average dice similarity coefficient.},
  archive      = {J_PAAA},
  author       = {Zhang, Hanzheng and Wu, Qing and Zhao, Xing and Wang, Yuanquan and Zhou, Shoujun and Zhang, Lei and Zhang, Tao},
  doi          = {10.1007/s10044-025-01437-6},
  journal      = {Pattern Analysis and Applications},
  month        = {6},
  number       = {2},
  pages        = {1-15},
  shortjournal = {Pattern Anal. Appl.},
  title        = {MPFCNet: Multi-scale parallel feature fusion convolutional network for 3D knee segmentation from MR images},
  volume       = {28},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Fedpartwhole: Federated domain generalization via consistent part-whole hierarchies. <em>PAAA</em>, <em>28</em>(2), 1-15. (<a href='https://doi.org/10.1007/s10044-025-01439-4'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Federated Domain Generalization (FedDG) aims to address the challenge of generalizing to unseen domains at test time while adhering to data privacy constraints that prevent centralized data storage from various client domains. Existing approaches can be broadly classified into domain alignment, data manipulation, learning strategies, and optimization of model aggregation weights. This paper introduces a novel approach to FedDG that focuses on the backbone model architecture. The key insight is that objects, even under substantial domain shifts and appearance variations, retain a consistent hierarchical structure of parts and wholes. For example, a photograph and a sketch of a dog share the same structural organization, comprising a head, body, limbs, and so on. Our architecture explicitly integrates a feature representation for the image parse tree, enabling robust generalization across domains. To the best of our knowledge, this is the first work to approach FedDG from a model architecture perspective. We compared the performance of our proposed backbone against a comparable-sized CNN-based backbone (MobileNet) for 5 different algorithms on standard benchmark datasets (PACS and VLCS), and the results showed an average improved performance of up to 17.3%. Additionally, our approach marginally outperforms the Vision Transformer (ViT-Small) on average, despite utilizing approximately 5x fewer parameters. Unlike conventional convolutional neural networks, our method is inherently interpretable, fostering trust in its predictions-a critical asset in federated learning scenarios.},
  archive      = {J_PAAA},
  author       = {Radwan, Ahmed and Shehata, Mohamed},
  doi          = {10.1007/s10044-025-01439-4},
  journal      = {Pattern Analysis and Applications},
  month        = {6},
  number       = {2},
  pages        = {1-15},
  shortjournal = {Pattern Anal. Appl.},
  title        = {Fedpartwhole: Federated domain generalization via consistent part-whole hierarchies},
  volume       = {28},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Dynamic metric memory network for long-term tracking with spatial-temporal region proposal method. <em>PAAA</em>, <em>28</em>(2), 1-18. (<a href='https://doi.org/10.1007/s10044-025-01441-w'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Fully mining target information is critical to cope with the recovery of lost targets in long-term tracking scenarios. However, most existing trackers focus on either temporal or spatial information during tracking and do not utilize this information effectively simultaneously. Therefore, we propose a dynamic metric memory network for long-term tracking with spatial-temporal region proposals. First, we present a spatio-temporal region proposal method, in which temporal memory is utilized to construct dynamic templates that represent the variations in the historical appearance of the target. Meanwhile, spatial attention focuses on the geometric information of the target to enhance the perceptual capabilities of the model. This interactive use of spatio-temporal information makes the Regional Proposal Network (RPN) generate higher-quality object-oriented proposals. Second, a dynamic metric memory network encompassing writing and reading mechanisms is designed. The former includes a metric learning judgment strategy to maintain temporal consistency and dynamically memorize significant variations. The latter reads out the entire memory to verify the quality of the candidate region and infer the optimal candidate, in which the short-term memory is used to update the template. The designed network enhances the tracker’s adaptive capability to target changes. Finally, we employ an online refinement network to rectify the prediction results to further improve the tracking performance, which updates the memory pool and switches the local–global search strategy. our experimental results on benchmarks such as VOT-LT2018 and others demonstrate that our proposed tracker is on par with the current state-of-the-art tracking algorithms.},
  archive      = {J_PAAA},
  author       = {Zhang, Huanlong and Fu, Weiqiang and Yang, Xiangbo and Qi, Rui and Wang, Xin and Zhang, Chunjie},
  doi          = {10.1007/s10044-025-01441-w},
  journal      = {Pattern Analysis and Applications},
  month        = {6},
  number       = {2},
  pages        = {1-18},
  shortjournal = {Pattern Anal. Appl.},
  title        = {Dynamic metric memory network for long-term tracking with spatial-temporal region proposal method},
  volume       = {28},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Vehicle and license plate recognition with novel dataset for toll collection. <em>PAAA</em>, <em>28</em>(2), 1-16. (<a href='https://doi.org/10.1007/s10044-025-01443-8'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose an automatic toll tax collection framework designed for challenging conditions, consisting of three sequential steps: vehicle type recognition, license plate localization, and license plate reading. Traditional decorations on vehicle fronts often introduce significant intra-class variations, severe background clutter, and partial occlusions, complicating both license plate detection and reading. In addition, non-uniform license plate positions-particularly on trucks-and variations in font styles, sizes, and partially occluded characters further challenge the process. To address these issues, we leverage advanced deep learning architectures along with a novel dataset of 10k images covering six vehicle types. Each image is manually annotated with the vehicle type and the alphanumeric characters of its license plate. We evaluate our framework using state-of-the-art YOLO models, from the initial to the latest versions: Yolov2, Yolov3, Yolov4, YOLOv5, YOLOv8, and YOLOv11, and assess their lightweight (Nano) variants for real-time deployment on a Raspberry Pi. Our experimental results demonstrate that the large variants of YOLOv5, YOLOv8, and YOLOv11 consistently achieve a top mean average precision (mAP@0.5) of 99% across all tasks, while their Nano versions attain peak mAP values of 98%, 97%, and 98% for vehicle type recognition, license plate detection, and character recognition, respectively. The code, trained models, and test images are available at https://github.com/usama-x930/VT-LPR .},
  archive      = {J_PAAA},
  author       = {Usama, Muhammad and Anwar, Hafeez and Anwar, Saeed},
  doi          = {10.1007/s10044-025-01443-8},
  journal      = {Pattern Analysis and Applications},
  month        = {6},
  number       = {2},
  pages        = {1-16},
  shortjournal = {Pattern Anal. Appl.},
  title        = {Vehicle and license plate recognition with novel dataset for toll collection},
  volume       = {28},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). MSGGSA: A multi-strategy-guided gravitational search algorithm for gene selection in cancer classification. <em>PAAA</em>, <em>28</em>(2), 1-30. (<a href='https://doi.org/10.1007/s10044-025-01446-5'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Microarray gene expression data, widely employed in the field of cancer research, provides valuable insights for distinguishing between various types of cancers. However, the inherent challenges associated with microarray data, such as limited sample size and high dimensionality, complicate the process of cancer classification. To effectively address these challenges, swarm intelligence optimization techniques have been employed for gene selection from microarray gene expression datasets. Nevertheless, accurately classifying cancer types remains a formidable challenge. The present study presents a gene selection method for cancer classification, employing a multi-strategy-guided gravitational search algorithm (MSGGSA). In MSGGSA, the traditional gravitational search algorithm (GSA) is enhanced by incorporating strategies such as segmented population initialization, inertia stagnation, dynamic update of gravitational constant, and relearning strategy for elite individuals. The proposed enhancements effectively address the limitations associated with excessive randomness in the initial population, premature convergence susceptibility, and vulnerability to local optima encountered by traditional GSA. Moreover, this enhancement effectively achieves a balance between exploration and exploitation within algorithm. The superior performance of the proposed algorithm on high-dimensional data is demonstrated through rigorous testing on 12 publicly available microarray datasets, highlighting its superiority over other popular swarm intelligence algorithms and current state-of-the-art methods.},
  archive      = {J_PAAA},
  author       = {Li, Min and Jin, Chen and Cai, Yuheng and Deng, Shaobo and Wang, Lei},
  doi          = {10.1007/s10044-025-01446-5},
  journal      = {Pattern Analysis and Applications},
  month        = {6},
  number       = {2},
  pages        = {1-30},
  shortjournal = {Pattern Anal. Appl.},
  title        = {MSGGSA: A multi-strategy-guided gravitational search algorithm for gene selection in cancer classification},
  volume       = {28},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Conversion-aware forecasting of alzheimer’s disease via featurewise attention. <em>PAAA</em>, <em>28</em>(2), 1-15. (<a href='https://doi.org/10.1007/s10044-025-01447-4'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Alzheimer’s disease (AD) is a neurodegenerative disorder that leads to cerebral atrophy, impacting memory and cognitive abilities. A precursor to AD known as Mild Cognitive Impairment (MCI) shows subtle symptoms that do not overwhelm the patients’ daily activities. MCI patients might eventually progress to AD in later stages. Early detection of the conversion is a vital step in preventative treatment planning. However, conversion detection is challenging due to the rarity of conversion visits in public datasets and the unknown nature of the conversion. This study aims to improve conversion detection with an attention-based architecture designed to encode input biomarkers and time into a shared space where time and attribute embeddings are fused with attention. Temporal information is incorporated as a separate modality with time embeddings to capture the correlation between time and feature significance for the model’s predictions. Experiments with widely used public databases (TADPOLE and NACC) show encouraging performance in conversion detection. In TADPOLE, a conversion recall of 74.3%, significantly outperforming baseline models such as logistic regression (36.9%) and Long Short-Term Memory networks (62.3%), is reported while maintaining an area under the curve (AUC) score of 82.0%. In NACC, our model demonstrates a competitive conversion recall of 71.6% and an AUC of 82.6%. The experimental results highlight the contribution of the attention between time and attributes to MCI-AD conversion recall. The experimental analyses hold promise for assisting physicians in designing targeted preventative treatment strategies for at-risk individuals. The implementation of the proposed method is available at https://github.com/ALLab-Boun/FATE-Net .},
  archive      = {J_PAAA},
  author       = {Karasu, Elvan and Baytaş, İnci M.},
  doi          = {10.1007/s10044-025-01447-4},
  journal      = {Pattern Analysis and Applications},
  month        = {6},
  number       = {2},
  pages        = {1-15},
  shortjournal = {Pattern Anal. Appl.},
  title        = {Conversion-aware forecasting of alzheimer’s disease via featurewise attention},
  volume       = {28},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Dmvae: A dual-stream multi-modal variational autoencoder for multi-task fake news detection. <em>PAAA</em>, <em>28</em>(2), 1-12. (<a href='https://doi.org/10.1007/s10044-025-01412-1'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The proliferation of fake news on social media platforms, facilitated by the development of the Internet, has become a pressing social issue, intensifying the urgency of detecting its diverse multi-modal forms. However, current methods are unable to verify the validity of the extracted multimodal features, ignore the problem of interaction between multimodal content, and fail to learn valid cross-modal features. In this paper, we took a look into new multi-modal learning methods for representation and fusion in fake news detection. A two-branch adversarial network is designed to extract different levels of event-irrelevant features, while inter-modal information interaction and intra-modal information enhancement are followed to improve the richness of the features. To improve the interpretability of the model, a multi-task learning methodology based on the variational autoencoder structure is proposed in detail, which redesigns a general loss function to balance competitive submodules, and verifies the effectiveness of the multi-modal features in turn. Finally, by comparing and analyzing the experimental results of different methods, it is demonstrated that the multimodal fake news detection model proposed in this paper can effectively improve the effectiveness of fake news detection.},
  archive      = {J_PAAA},
  author       = {Guo, Ying and Hu, Shuting and Li, Yao and Di, Chong and Liu, Jie},
  doi          = {10.1007/s10044-025-01412-1},
  journal      = {Pattern Analysis and Applications},
  month        = {6},
  number       = {2},
  pages        = {1-12},
  shortjournal = {Pattern Anal. Appl.},
  title        = {Dmvae: A dual-stream multi-modal variational autoencoder for multi-task fake news detection},
  volume       = {28},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Uieanything: Zero-shot underwater image enhancement via advanced depth estimation, white balance models, and improved sea-thru. <em>PAAA</em>, <em>28</em>(2), 1-25. (<a href='https://doi.org/10.1007/s10044-025-01422-z'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Underwater image enhancement is fundamental for marine applications yet remains challenging due to complex light-water interactions that degrade image quality through wavelength-dependent absorption and scattering effects. Existing methods often require extensive paired training data and struggle to generalize across diverse underwater conditions. We propose UIEAnything, a novel zero-shot underwater image enhancement framework that integrates automatic white balance preprocessing, physics-guided depth estimation, and an improved restoration algorithm based on underwater light transport theory. Our approach introduces three key innovations: (1) a domain adaptation strategy that bridges the gap between underwater and natural images via physically motivated white balance correction, enabling effective utilization of pre-trained models; (2) an improved Sea-thru algorithm incorporating nonlinear backscatter modeling and adaptive attenuation estimation, accurately capturing the depth-dependent nature of underwater light propagation; and (3) a unified framework that eliminates the need for task-specific training while maintaining physical consistency. Extensive experiments on seven benchmark datasets demonstrate that UIEAnything consistently outperforms state-of-the-art methods, achieving average improvements of 15.3% in PSNR and 12.8% in SSIM. Furthermore, without additional training, our framework demonstrates remarkable generalization capability by successfully addressing other challenging vision tasks involving scattering media, such as image dehazing and sandstorm removal. These results establish UIEAnything as a significant advancement in physics-guided zero-shot learning for image enhancement in complex optical environments.},
  archive      = {J_PAAA},
  author       = {Shao, Jinxin and Zhang, Haosu and Miao, Jianming},
  doi          = {10.1007/s10044-025-01422-z},
  journal      = {Pattern Analysis and Applications},
  month        = {6},
  number       = {2},
  pages        = {1-25},
  shortjournal = {Pattern Anal. Appl.},
  title        = {Uieanything: Zero-shot underwater image enhancement via advanced depth estimation, white balance models, and improved sea-thru},
  volume       = {28},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Accumulating global channel-wise patterns via deformed-bottleneck recalibration for image classification. <em>PAAA</em>, <em>28</em>(2), 1-16. (<a href='https://doi.org/10.1007/s10044-025-01429-6'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Embedding attention modules into deep convolutional neural networks (CNNs) is currently one of the common deliberations to enhance their learning ability of feature representation. In previous works, the global channel-wise patterns of a given tensor are computed and squeezed into CNN-based models through an attention mechanism. Squeezing different kinds of these features can lead to the less fusion of attentive information due to the independent operations of channel-wise recalibration. To deal with this issue, an efficient attention module of accumulated features ( $$\textrm{MAF}$$ ) is proposed by accumulating these diverse squeezes for a unitary recalibrating perceptron as follows. Firstly, we take advantage of average and deviation calculations to produce correspondingly statistical patterns of a given tensor for aggregating the global channel information. An adaptative perceptron of deformed-bottleneck recalibration ( $$\textrm{DBR}$$ ) is then presented to cohere the resultant features. Finally, the robust $$\textrm{DBR}$$ -based lightweights will be utilized to weight the concerning tensor. Additionally, to exploit more spatial-wise information, we address $$\textrm{MAF}$$ for an effective alternative of the channel-wise component in two critical attention units to form two corresponding modules that will be then inspected to indicate which integration is good for real applications. We adapt the MAF-based modules to MobileNets for further enhancement investigation. Experiments on benchmark datasets for image classification have proved the efficacy of our proposals. The code of the MAF module is available at https://github.com/nttbdrk25/MAFAttention .},
  archive      = {J_PAAA},
  author       = {Nguyen, Thanh Tuan and Nguyen, Thanh Phuong and Nguyen, Vincent},
  doi          = {10.1007/s10044-025-01429-6},
  journal      = {Pattern Analysis and Applications},
  month        = {6},
  number       = {2},
  pages        = {1-16},
  shortjournal = {Pattern Anal. Appl.},
  title        = {Accumulating global channel-wise patterns via deformed-bottleneck recalibration for image classification},
  volume       = {28},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). DPPCN: Density and position-based point convolution network for point cloud segmentation. <em>PAAA</em>, <em>28</em>(2), 1-10. (<a href='https://doi.org/10.1007/s10044-025-01436-7'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A point cloud can usually describe the outline and spatial location of an object. Due to the disorder and uneven density of the point cloud, it is a difficult task to fully obtain the local features and spatial context information of the point cloud. In this paper, we propose a point cloud segmentation network based on the encoding–decoding structure of point convolution, which extracts the local features of point clouds by density-position adaptive convolution, which integrates density information and positional relationships between points. To obtain the density information of center points, we design an auto-adjusted bandwidth and integrate it into adaptive kernel density estimation. In addition, to obtain the context of the point cloud to a greater extent, we design an encoding layer that carries the contextual information. In order to verify the effectiveness of our method, experiments were carried out on S3DIS and a self-built dataset. The experimental results verify the validity of our proposed method.},
  archive      = {J_PAAA},
  author       = {Li, Yaqian and Zhang, Ze and Li, Haibin and Zhang, Wenming},
  doi          = {10.1007/s10044-025-01436-7},
  journal      = {Pattern Analysis and Applications},
  month        = {6},
  number       = {2},
  pages        = {1-10},
  shortjournal = {Pattern Anal. Appl.},
  title        = {DPPCN: Density and position-based point convolution network for point cloud segmentation},
  volume       = {28},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Adaptive deep CNN: An effective alzheimer’s affected MRI image registration using heuristic-aided deep learning model and patch-based level fusion. <em>PAAA</em>, <em>28</em>(2), 1-24. (<a href='https://doi.org/10.1007/s10044-025-01438-5'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Aligning multiple images is referred to as image registration. Image registration methods aim to find the best adjustment to align the important elements in the images.Registering techniques can be modified to fit specific objectives by balancing rapidity with precision. Magnetic Resonance Imaging (MRI) methods are becoming increasingly significant in Alzheimer’s Disease (AD) in themedical sector. Several intriguing applications for machine learning approaches in clinical imaging exist, including the recognition of AD using MRI scans of the brain. Numerous preprocessing procedures, such as image registration, are often performed on these scans. However, the impact of registration of image approaches on machine learning classification effectiveness is little understood. To address these challenges, a new deep learning approach is suggested for registering images ofAD using MRI images. Originally, the images are garnered from the standard datasets. The patch-based label fusion method is suggested from the collected images, where the patches and their weights are estimated by the similarity measures among the patches using labeling and intensity-based distances. Subsequently, the patches are considered for hippocampal segmentation using anatlas-based segmentation model.The hippocampus is a critical brain structure involved in memory and learning, and it is one of the first regions to exhibit damage in AD. As Alzheimer’s grows, the hippocampus undergoes significant atrophy, leading to the characteristic memory loss and cognitive decline associated with the condition. Hippocampal segmentation, which involves isolating and analyzing the hippocampus from MRI scans, is essential for detecting these changes early. Accurately measuring the volume and structure of the hippocampus helps to identify signs of Alzheimer’s before other symptoms become apparent. Furthermore, precise hippocampal segmentation helps differentiate Alzheimer’s from other neurological conditions, ensuring more accurate diagnoses and tailored treatment strategies.Finally, the “Adaptive Deep Registration” is newly proposed to register the images by utilizing anAdaptive Deep Convolutional Neural Network (DCNN), in which the hyper-parameters are optimized using the Modified Gannet Optimization Algorithm (MGOA). The term “adaptive” in Adaptive DCNN reflects the model’s ability to dynamically optimize its parameters. In the context of image registration, this adaptiveness allows the MGOAto fine-tune DCNN’s parameters, such as hidden neuron count, epoch count, and step per epoch, in response to the complexity and variability of the input images. This ensures that the network can effectively learn from diverse data, improving its accuracy in aligning and registering images. The adaptive nature of the DCNN, further enhanced by the MGOA, allows the network to continuously tune its performance, adjusting hyperparameters on the fly to achieve the best possible outcomes in image registration tasks. This adaptability is crucial in handling the intricate patterns and subtle differences found in medical images, such as those of the hippocampus in Alzheimer’s disease, leading to more accurate and consistent results.At last, the efficacy of the model is examined using divergent measurements. Rather than existing methods, the proposed model acquires impressive results of performance enhancement.},
  archive      = {J_PAAA},
  author       = {Deshmukh, Vaidehi and Chapadgaonkar, Shilpa and Kowdiki, Manisha and Khaparde, Arti},
  doi          = {10.1007/s10044-025-01438-5},
  journal      = {Pattern Analysis and Applications},
  month        = {6},
  number       = {2},
  pages        = {1-24},
  shortjournal = {Pattern Anal. Appl.},
  title        = {Adaptive deep CNN: An effective alzheimer’s affected MRI image registration using heuristic-aided deep learning model and patch-based level fusion},
  volume       = {28},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Structured regularization with object size selection using mathematical morphology. <em>PAAA</em>, <em>28</em>(2), 1-18. (<a href='https://doi.org/10.1007/s10044-025-01444-7'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a novel way to incorporate morphology operators through structured regularization of machine learning models. Specifically, we introduce a feature map in the models that performs structured variable selection. The feature map is automatically processed by approximate morphology operators and is learned together with the model coefficients. Experiments were conducted with linear regression on both synthetic data, demonstrating that the proposed methods are effective in selecting groups of parameters with much less noise than baseline models, and on three-dimensional T1-weighted brain magnetic resonance images (MRI) for age prediction, demonstrating that the proposed methods enforce sparsity and select homogeneous regions of non-zero and relevant regression coefficients. The proposed methods improve interpretability in pattern analysis. The minimum size of features in the structured variable selection can be controlled by adjusting the structuring element in the approximate morphology operator, tailored to the specific study of interest. With these added benefits, the proposed methods still perform on par with commonly used variable selection and structured variable selection methods in terms of the coefficient of determination and the Pearson correlation coefficient.},
  archive      = {J_PAAA},
  author       = {Lin, Disi and Hägg, Linus and Wadbro, Eddie and Berggren, Martin and Löfstedt, Tommy},
  doi          = {10.1007/s10044-025-01444-7},
  journal      = {Pattern Analysis and Applications},
  month        = {6},
  number       = {2},
  pages        = {1-18},
  shortjournal = {Pattern Anal. Appl.},
  title        = {Structured regularization with object size selection using mathematical morphology},
  volume       = {28},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A novel fusion approach with a robust ParallelNet model for diabetic retinopathy diagnosis. <em>PAAA</em>, <em>28</em>(2), 1-17. (<a href='https://doi.org/10.1007/s10044-025-01448-3'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Diabetic Retinopathy (DR) is a serious diabetes-related complication that can lead to significant retinal damage and irreversible vision loss if not detected and treated early. While numerous deep learning algorithms have recently been developed for DR diagnosis, however they often focus on specific symptoms like exudates, vessels, or hemorrhages, overlooking a comprehensive analysis of all relevant indicators. Though, previous studies have shown high performance on benchmark public datasets but have struggled with real-time data. This paper introduces a diagnostic system that systematically incorporates all detectable symptoms of diabetic retinopathy and has demonstrated reliable performance on 108 test images from Lahore General Hospital, showcasing its robustness in real-world scenarios. Additionally, a novel algorithm for extracting retinal exudates is proposed, outperforming existing methods. The study categorizes retinal fundus images into both 2-class and multi-class diabetic retinopathy. Evaluation of current models on a local hospital dataset shows significant accuracy improvements. We also present ParallelNet, a model for classifying Diabetic Retinopathy stages: No DR, NPDR, PDR. ParallelNet outperforms established models, achieving 96% accuracy on the APTOS dataset and 90.16% on the local dataset for binary classification. For multi-classification, it achieves 90% accuracy on the APTOS dataset and 87.05% on the local dataset. These results highlight the improved performance achieved by combining our extraction algorithms with the ParallelNet model, demonstrating robustness across both public and local real-time hospital datasets.},
  archive      = {J_PAAA},
  author       = {Mahmood, Haroon and Ather, Saad and Wali, Aamir and Ali, Arshad and Malik, Tayyaba Gul and Kafeel, Wardah},
  doi          = {10.1007/s10044-025-01448-3},
  journal      = {Pattern Analysis and Applications},
  month        = {6},
  number       = {2},
  pages        = {1-17},
  shortjournal = {Pattern Anal. Appl.},
  title        = {A novel fusion approach with a robust ParallelNet model for diabetic retinopathy diagnosis},
  volume       = {28},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Dual stream deep attention networks for annual population projection. <em>PAAA</em>, <em>28</em>(2), 1-13. (<a href='https://doi.org/10.1007/s10044-025-01451-8'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Accurate population projections are essential for local and state governments planning and decision-making processes, as they directly influence the development of local infrastructure and services. Recent advancements in time series forecasting, particularly through machine learning applied to diverse social and economic factors, have improved predictive accuracy, enabling more informed decision-making at local and state levels. Therefore, researchers have explored various statistical, machine learning, and deep learning methods. However, these approaches often rely on standalone models or simple feature integration, which results need to be improved. Furthermore, the existing methods lack hybrid methods with attention mechanisms to effectively capture rich intricate features. To tackle these challenges, we designed a hybrid method, a dual-stream deep attention network namely Deep Population Network (DPNET) for annual population forecasting. The first stream employs convolutional layers to capture spatial patterns while the second stream extracts the temporal information. The output of these two streams is then concatenated and fed to the self-attention module for feature refinement followed by a fully connected layer. The DPNET showcases promising performance in small-area population projection across multiple datasets collected from Korea, Australia, and New Zealand for 10-year and 5-year forecasting periods. Compared to existing approaches, DPNET significantly reduced the error rate, achieving the lowest Mean Absolute Percentage Error (MAPE) of 5.30% and Median Absolute Percentage Error (MedAPE) of 3.03% for the Korean dataset, 6.02% MAPE and 4.09% MedAPE for the Australian dataset, and 3.99% MedAPE for the New Zealand dataset.},
  archive      = {J_PAAA},
  author       = {Hussain, Adnan and Yar, Hikmat and Khan, Noman and Khan, Zulfiqar Ahmad and Kim, Min Je and Baik, Sung Wook},
  doi          = {10.1007/s10044-025-01451-8},
  journal      = {Pattern Analysis and Applications},
  month        = {6},
  number       = {2},
  pages        = {1-13},
  shortjournal = {Pattern Anal. Appl.},
  title        = {Dual stream deep attention networks for annual population projection},
  volume       = {28},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Texture-driven pose-guided human image synthesis. <em>PAAA</em>, <em>28</em>(2), 1-16. (<a href='https://doi.org/10.1007/s10044-025-01452-7'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the rapid development of computer vision and artificial intelligence, significant breakthroughs have been made in the field of character image synthesis. Although existing methods can synthesize target pose images, there are still limitations in handling complex textures and pose alignment, such as texture distortion, pose misalignment, and missing information. To address this, this paper proposes a pose-guided human image synthesis method called Human Pose Transfer Generative Adversarial Network (HPT-GAN). The model significantly improves the quality and efficiency of synthetic images by introducing ResBlocks module, designing a Texture Transfer Module (TTM) and a ToRGB module. Specifically, ResBlocks enhance gradient stability while preserving context information, TTM efficiently aligns textures through a multi-head attention mechanism, and the ToRGB module optimizes the fusion of multi-resolution features. HPT-GAN has a small number of parameters while achieving faster processing speed than similar methods. Moreover, it has achieved good results on the DeepFashion and Market-1501 datasets.},
  archive      = {J_PAAA},
  author       = {Wei, Wei and Qin, Chao and Duan, Xiaodong},
  doi          = {10.1007/s10044-025-01452-7},
  journal      = {Pattern Analysis and Applications},
  month        = {6},
  number       = {2},
  pages        = {1-16},
  shortjournal = {Pattern Anal. Appl.},
  title        = {Texture-driven pose-guided human image synthesis},
  volume       = {28},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multicriteria decision making analysis based on fermatean fuzzy distance measure and entropy measure with application in COVID-19. <em>PAAA</em>, <em>28</em>(2), 1-32. (<a href='https://doi.org/10.1007/s10044-024-01358-w'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi Criteria Decision Making (MCDM) is a crucial tool for addressing uncertainty in complex problems owing to its transparent, flexible, and technological approach to evaluating alternatives based on conflicting criteria. Fermatean Fuzzy Set is an efficient method for exhibiting expert argumentation information in complex decision-making issues. In this study, we propose a novel Fermatean Fuzzy Distance Measure to quantify the inconsistency between two Fermatean Fuzzy Sets by employing the cross-evaluation factor. Based on the introduced Distance measure, we propose a novel Fermatean Fuzzy Entropy Measure. Furthermore, we demonstrated that the recently presented Distance and Entropy measure correspond to the axiomatic representation anticipated by an Fermatean Fuzzy Distance and Entropy measure, respectively. Several propositions are derived from the proposed distance measure. In addition, the proposed measures are employed to address MCDM problems on COVID-19 in an Fermatean Fuzzy environment that produces commendable outcomes. The findings of the investigation indicate that the proposed measure demonstrates better capabilities in real-world applications within an FF environment, generating more robust and reliable information compared to existing Distance and Entropy measures on IFS, PFS, and FFS. In the future, the proposed approaches could potentially be applied to pattern recognition, image processing, and other MCDM fields of study.},
  archive      = {J_PAAA},
  author       = {Dutta, Palash and Konwar, Alakananda},
  doi          = {10.1007/s10044-024-01358-w},
  journal      = {Pattern Analysis and Applications},
  month        = {6},
  number       = {2},
  pages        = {1-32},
  shortjournal = {Pattern Anal. Appl.},
  title        = {Multicriteria decision making analysis based on fermatean fuzzy distance measure and entropy measure with application in COVID-19},
  volume       = {28},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). “A priori” shapley data value estimation. <em>PAAA</em>, <em>28</em>(2), 1-19. (<a href='https://doi.org/10.1007/s10044-025-01454-5'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Distributed machine learning approaches are required when training data cannot be collected in a central location, due to storage, transmission or privacy/security constraints. An important task in any distributed machine learning context, and Federated Learning is no exception, is data value estimation or credit allocation, where the goal is to reward each participant proportionally to their contribution to the final performance of the machine learning model. However, all existing data value estimation techniques require that training be completed before the data values are obtained, and in this sense they can be considered as “a posteriori” approaches. Thus, all potential contributors must participate in the training process, regardless of the quality of their data or the final reward they can obtain. Here we present an “a priori” Shapley data value estimation technique in which, based on some statistical measures provided by the participants, the central counterpart or aggregator can obtain reasonably accurate data value estimates before actually starting the distributed learning process. To the best of our knowledge, this is the first “a priori” data value estimation approach proposed in the literature, and it can be used for the pre-selection of participants or to implement new pricing schemes. The introduced algorithms have been benchmarked using a variety of datasets and a logistic regression model, and we show that our “a priori” estimates are very accurate, compared to the centralized Shapley data values.},
  archive      = {J_PAAA},
  author       = {Navia-Vázquez, Angel and Cid-Sueiro, Jesús and Vázquez, Manuel A.},
  doi          = {10.1007/s10044-025-01454-5},
  journal      = {Pattern Analysis and Applications},
  month        = {6},
  number       = {2},
  pages        = {1-19},
  shortjournal = {Pattern Anal. Appl.},
  title        = {“A priori” shapley data value estimation},
  volume       = {28},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multi-view clustering via diversity induction and multi-layer concept factorization. <em>PAAA</em>, <em>28</em>(2), 1-18. (<a href='https://doi.org/10.1007/s10044-025-01455-4'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To investigate complicated nonlinear interactions and underlying hierarchical structure in multi-view data, provide more useful data representation, and boost the multi-view learning approach’s recognition power, we develop a multi-view clustering via diversity induction and multi-layer concept factorization (MCDMCF) framework. This framework employs the Hilbert-Schmidt independence criterion (HSIC) to model the projection matrix corresponding to each view to strengthen the complementarity between different views. Additionally, the proposed approach imposes multiple graph regularization constraints on each view to explore essential information in the data and adopts the concept of deep learning (DL) to investigate the hierarchical structure hidden in various views. We also used kernel technology to perform concept factorization (CF) in a high-dimensional space, which enables the MCDMCF to effectively distinguish data points. Furthermore, we designed an alternating iterative optimization approach to address the MCDMCF approach and analyze its convergence. Extensive tests reveal that the proposed method’s clustering ability on several datasets are superior to those of numerous recent multi-view clustering approaches.},
  archive      = {J_PAAA},
  author       = {Liu, Guoqing and Ge, Hongwei and Li, Ting and Su, Shuzhi and Gao, Penglian},
  doi          = {10.1007/s10044-025-01455-4},
  journal      = {Pattern Analysis and Applications},
  month        = {6},
  number       = {2},
  pages        = {1-18},
  shortjournal = {Pattern Anal. Appl.},
  title        = {Multi-view clustering via diversity induction and multi-layer concept factorization},
  volume       = {28},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Strengthen contrastive semantic consistency for fine-grained image classification. <em>PAAA</em>, <em>28</em>(2), 1-14. (<a href='https://doi.org/10.1007/s10044-025-01456-3'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Fine-grained Visual Classification (FGVC) refers to the divisions of sub-classes from the given object categories only with the supervision of the image-level labels. While numerous efforts have improved the recognition accuracy of fine-grained images by strengthening the discriminative feature between different subtle inter-classes, we argue most recent approaches still suffer from the challenge of high intra-class variances in FGVC, i.e., objects belonging to the same sub-class present huge visual differences to output different recognition results. To suppress the intra-class variances, in this paper, we capture the semantic consistency for the visual changes of intra-class images and propose a novel contrastive fine-grained visual classification network (CFGVC-Net). We first embed discriminative parts to distinguish different sub-classes based on the spatial attention map. We then design the semantic consistency enhancement module by applying several transformation strategies to the training images and further matching the discriminative features of the generated image pairs based on the center loss and contrast loss, which can improve the model learning tolerance to the distribution diversity of the intra-class images. Extensive experiments on five benchmark datasets show that the proposed CFGVC-Net can significantly enhance FGVC performance, demonstrating its effectiveness across diverse fine-grained classification tasks. The code is available at https://github.com/WangYuPeng1/CFGVC-Net .},
  archive      = {J_PAAA},
  author       = {Wang, Yupeng and Wang, Yongli and Ye, Qiaolin and Lang, Wenxi and Xu, Can},
  doi          = {10.1007/s10044-025-01456-3},
  journal      = {Pattern Analysis and Applications},
  month        = {6},
  number       = {2},
  pages        = {1-14},
  shortjournal = {Pattern Anal. Appl.},
  title        = {Strengthen contrastive semantic consistency for fine-grained image classification},
  volume       = {28},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). AdvWave: Adversarial examples generation with wavelet feature fusion mechanisms. <em>PAAA</em>, <em>28</em>(2), 1-16. (<a href='https://doi.org/10.1007/s10044-025-01458-1'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Though deep neural networks (DNNs) have demonstrated remarkable success in the community, it is very vulnerable to adversarial examples. Frequency domain transformation-based methods have been successfully utilized in adversarial attacks, such as AdvDrop. Nonetheless, AdvDrop cannot accurately drop the detail information of images due to the limitation of discrete cosine transform (DCT) to extract precise frequency information. In addition, adversarial examples generated by AdvDrop present noticeable blocking artifacts that considerably impair their quality caused by block-based image coding. Towards these ends, we propose a novel fusion-based frequency adversarial attack method named AdvWave, which generates adversarial examples using discrete wavelet transform (DWT) to extract more detailed frequency information from input images. AdvWave can effectively extract sub-band information which is discriminative for DNNs. Moreover, it adds elaborate perturbations to low-frequency information and drops the discriminative information in the process of quantization. By fusing the processed low-frequency and high-frequency information, the reconstructed images generated by AdvWave can easily fool the DNNs. Extensive experiments on several benchmark datasets demonstrate the effectiveness of AdvWave in improving the attack success rate and the quality of generated adversarial examples. The attack success rate of AdvWave on ImageNet surpasses the state-of-the-art method (AdvDrop) by an average of 1.5 $$\%$$ , and it achieves an improvement of 19.7 $$\%$$ on CIFAR-10. The Mean Squared Error (MSE) of adversarial examples generated by AdvWave is 0.00011, and the Structural Similarity Index (SSIM) is 0.981, both of which outperform AdvDrop. The code and data are available on the website https://github.com/dongfangbai11/AdvWave .},
  archive      = {J_PAAA},
  author       = {Hu, Cong and Wan, Peng and Wu, Xiaojun},
  doi          = {10.1007/s10044-025-01458-1},
  journal      = {Pattern Analysis and Applications},
  month        = {6},
  number       = {2},
  pages        = {1-16},
  shortjournal = {Pattern Anal. Appl.},
  title        = {AdvWave: Adversarial examples generation with wavelet feature fusion mechanisms},
  volume       = {28},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Kernelized multi-view graph clustering via graph structure preserving and consensus affinity graph learning. <em>PAAA</em>, <em>28</em>(2), 1-12. (<a href='https://doi.org/10.1007/s10044-024-01408-3'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, kernelized multi-view clustering has garnered significant attention due to its powerful ability to effectively cluster multi-view data characterized by non-linear structures. However, most existing methods focus more on kernel learning while neglecting graph structure learning in the kernel space, resulting in the creation of an affinity graph that is suboptimal for clustering purposes. To address this issue, this paper proposes a novel kernelized multi-view graph clustering method via graph structure preserving and consensus affinity graph learning. The proposed method first designs a predefined kernel matrix for each individual view. Subsequently, it learns a view-specific candidate affinity graph that preserves both the local and global structures of the input data within the kernel space. Next, a reliable and robust consensus affinity graph, which accurately captures the underlying cluster structure and is resistant to noise, is jointly learned from all the candidate affinity graphs and their shared latent representation. Finally, we integrate graph structure learning in kernel space, shared latent representation learning, and consensus affinity graph learning into a unified framework, enabling them to mutually reinforce each other during iterative optimization. Experiments conducted on benchmark datasets have demonstrated that the proposed method outperforms some state-of-the-art multi-view clustering methods.},
  archive      = {J_PAAA},
  author       = {Gui, Zhongyan and Yang, Jing and Xie, Zhiqiang and Ye, Cuicui},
  doi          = {10.1007/s10044-024-01408-3},
  journal      = {Pattern Analysis and Applications},
  month        = {6},
  number       = {2},
  pages        = {1-12},
  shortjournal = {Pattern Anal. Appl.},
  title        = {Kernelized multi-view graph clustering via graph structure preserving and consensus affinity graph learning},
  volume       = {28},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). DFFnet: Delay feature fusion network for efficient content-based image retrieval. <em>PAAA</em>, <em>28</em>(2), 1-19. (<a href='https://doi.org/10.1007/s10044-025-01449-2'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Due to advancement of affordable imaging devices, a huge number of images are generated for different applications. An efficient method for retrieving the appropriate images corresponding to the query image from a huge repository is still awaited. Thus, content-based image retrieval (CBIR) systems have been developed. One of the issues that directly threatens the effectiveness of CBIR system is a semantic gap. In this paper, we introduce a Delay Feature Fusion Network (DFFnet) in the framework of SqueezeNet architecture. Our proposed model fuses the past layer’s features with the current layer’s features by utilizing a transpose convolution operation followed by depth-concatenation. This integration preserves the crucial information that may be lost during the forward pass. After extracting image features, we apply the t-SNE (t-Distributed Stochastic Neighbor Embedding) method. This technique allows us to project the high-dimensional image features into a lower-dimensional space, enabling compact image indexing and potentially improving the overall performance of CBIR system. Notably, we observed that as the number of retrieval rates increases, our proposed method experiences minimal impact. By leveraging the DFFnet and employing t-SNE, our approach aims to enhance image indexing and achieve improved performance for image retrieval tasks. The performance of DFFnet with t-SNE and without t-SNE are evaluated on benchmark datasets—Corel, Kadid, and ImageNet. Our proposed DFFnet with t-SNE gives a significant improvement in terms of performance metrics: precision, recall, and F1-score, in comparison to other state-of-the-arts.},
  archive      = {J_PAAA},
  author       = {Kumar, Suneel and Ruchilekha and Singh, Manoj Kumar and Mishra, Manoj Kumar},
  doi          = {10.1007/s10044-025-01449-2},
  journal      = {Pattern Analysis and Applications},
  month        = {6},
  number       = {2},
  pages        = {1-19},
  shortjournal = {Pattern Anal. Appl.},
  title        = {DFFnet: Delay feature fusion network for efficient content-based image retrieval},
  volume       = {28},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). One-dimensional matrix-product neural networks. <em>PAAA</em>, <em>28</em>(2), 1-11. (<a href='https://doi.org/10.1007/s10044-025-01457-2'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As an alternative model of the convolutional neural network (CNN), the matrix-product neural network (MPNN) constructed on account of two-dimensional discrete matrix-product operation (TDDMPO) is not only better than the CNN in recognition performance, but also smaller in calculation and faster in convergence speed than the CNN. In order to further perfect the MPNN, compared with the discrete convolutional operation and CNN, this paper proposes one-dimensional discrete matrix-product operation (ODDMPO) and its corresponding one-dimensional matrix-product neural network (ODMPNN). Experimental results on MNIST, Fashion_MNIST, CIFAR10, and FLOWER17 datasets show that ODMPNNs improve the performance by 0.62 $$-$$ 7.38% over the corresponding one-dimensional convolutional neural networks (ODCNNs), and the calculation amount of one-dimensional matrix-product layers of ODMPNNs obtains 5 $$\times$$ -79 $$\times$$ less than that of the corresponding one-dimensional convolutional layers of ODCNNs. Therefore, it shows again that the MPNN is a potentially important model to replace the CNN.},
  archive      = {J_PAAA},
  author       = {Shan, Chuanhui and Li, Hu and Han, Chao},
  doi          = {10.1007/s10044-025-01457-2},
  journal      = {Pattern Analysis and Applications},
  month        = {6},
  number       = {2},
  pages        = {1-11},
  shortjournal = {Pattern Anal. Appl.},
  title        = {One-dimensional matrix-product neural networks},
  volume       = {28},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Occlusion-aware appearance and shape learning for occluded cloth-changing person re-identification. <em>PAAA</em>, <em>28</em>(2), 1-17. (<a href='https://doi.org/10.1007/s10044-025-01459-0'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, Person Re-Identification (Re-ID) has seen remarkable progress in addressing the issue of clothing changes. However, in real-world scenarios, Re-ID is often further challenged by occlusions, while very little research has been conducted to explicitly tackle these two challenges simultaneously. To this end, we propose a method for Occluded Cloth-Changing Person Re-ID (OCCRe-ID) termed “OASL: Occlusion-aware Appearance and Shape Learning”. OASL introduces a plug-and-play occlusion handling strategy which can be seamlessly integrated into existing Re-ID methods, enabling them to reason discriminative appearance and shape features under occlusions. Specifically, our approach leverages occlusion type information to achieve two key objectives for occlusion-awareness: (1) guide the backbone to focus on extracting identity-aware appearance features from non-occluded image regions and reason features from occluded ones, and (2) recover pose keypoints from occluded regions for mitigating occlusions in shape encoding. Additionally, we construct E-PRCC, the first dataset for OCCRe-ID, with the aim of facilitating further research in this practical domain. Extensive experiments conducted on E-PRCC, LTCC, Occluded-REID, DeepChange, and Market-1501 datasets demonstrate that OASL achieves state-of-the-art performance, offering a robust solution to the dual challenges of occlusions and clothing changes in Person Re-ID.},
  archive      = {J_PAAA},
  author       = {Nguyen, Vuong D. and Mantini, Pranav and Shah, Shishir K.},
  doi          = {10.1007/s10044-025-01459-0},
  journal      = {Pattern Analysis and Applications},
  month        = {6},
  number       = {2},
  pages        = {1-17},
  shortjournal = {Pattern Anal. Appl.},
  title        = {Occlusion-aware appearance and shape learning for occluded cloth-changing person re-identification},
  volume       = {28},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Improving underwater semantic segmentation with underwater image quality attention and muti-scale aggregation attention. <em>PAAA</em>, <em>28</em>(2), 1-12. (<a href='https://doi.org/10.1007/s10044-025-01460-7'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Underwater image understanding is crucial for both submarine navigation and seabed exploration. However, the low illumination in underwater environments degrades the imaging quality, which in turn seriously deteriorates the performance of underwater semantic segmentation, particularly for outlining the object region boundaries. To tackle this issue, we present UnderWater SegFormer (UWSegFormer), a transformer-based framework for semantic segmentation of low-quality underwater images. Firstly, we propose the Underwater Image Quality Attention (UIQA) module. This module enhances the representation of high-quality semantic information in underwater image feature channels through a channel self-attention mechanism. In order to address the issue of loss of imaging details due to the underwater environment, the Multi-scale Aggregation Attention (MAA) module is proposed. This module aggregates sets of semantic features at different scales by extracting discriminative information from high-level features, thus compensating for the semantic loss of detail in underwater objects. Finally, during training, we introduce Edge Learning Loss (ELL) in order to enhance the model’s learning of underwater object edges and improve the model’s prediction accuracy. Experiments conducted on the SUIM and DUT-USEG (DUT) datasets have demonstrated that the proposed method has advantages in terms of segmentation completeness, boundary clarity, and subjective perceptual details when compared to SOTA methods. In addition, the proposed method achieves the highest mIoU of 82.12 and 71.41 on the SUIM and DUT datasets, respectively. Code will be available at https://github.com/SAWRJJ/UWSegFormer .},
  archive      = {J_PAAA},
  author       = {Zuo, Xin and Jiang, Jiaran and Shen, Jifeng and Yang, Wankou},
  doi          = {10.1007/s10044-025-01460-7},
  journal      = {Pattern Analysis and Applications},
  month        = {6},
  number       = {2},
  pages        = {1-12},
  shortjournal = {Pattern Anal. Appl.},
  title        = {Improving underwater semantic segmentation with underwater image quality attention and muti-scale aggregation attention},
  volume       = {28},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Dynamic fire and smoke detection module with enhanced feature integration and attention mechanisms. <em>PAAA</em>, <em>28</em>(2), 1-17. (<a href='https://doi.org/10.1007/s10044-025-01461-6'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Effective fire and smoke detection mechanisms are essential to early fire warning systems. The need for annotated datasets, the complexity of fire environments, the unique characteristics of fire and smoke, and the presence of noise in images necessitate further enhancements despite the optimistic results of object detection-based technologies. We propose the Dynamic Fire and Smoke Detection Model (DFDM), an optimized YOLOv7-tiny architecture to address these challenges. Our model incorporates an asymptotic feature pyramid network (AFPN) to bridge semantic gaps and a cross-level dual attention (CDA) mechanism to improve the detection of critical fire and smoke features. Additionally, we developed a novel partial selective block (PSB) that enhances parameter efficiency and reduces redundant information. Extensive experiments on two datasets, DFS and UMA, validate the effectiveness of DFDM in diverse environments. DFDM achieves a significant mAP improvement, reaching 0.240 on the DFS dataset and 0.669 on the UMA dataset while maintaining a low parameter count of 4.34M and FLOPs of 5.697G. Furthermore, the model excels in real-time performance, processing frames at 153.8 FPS with an inference time of 6.5 milliseconds, making it ideal for real-world applications requiring fast and accurate detection. Visualizations confirm that DFDM reduces background noise and provides a wider field of view compared to baseline models, demonstrating its robustness in complex fire and smoke detection scenarios.},
  archive      = {J_PAAA},
  author       = {Amjad, Ammar and Huroon, Aamer Mohamed and Chang, Hsien-Tsung and Tai, Li-Chia},
  doi          = {10.1007/s10044-025-01461-6},
  journal      = {Pattern Analysis and Applications},
  month        = {6},
  number       = {2},
  pages        = {1-17},
  shortjournal = {Pattern Anal. Appl.},
  title        = {Dynamic fire and smoke detection module with enhanced feature integration and attention mechanisms},
  volume       = {28},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Complex image inpainting of cultural relics integrating multi-stage structural features and spatial textures. <em>PAAA</em>, <em>28</em>(2), 1-18. (<a href='https://doi.org/10.1007/s10044-025-01434-9'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Aiming at the shortcomings of the existing image inpainting algorithms in the inpainting of cultural relics images, an improved two-stage image inpainting model is proposed, which integrates multi-stage structural features and spatial textures. The first stage of structural restoration was carried out using lightweight axial attention and transformer structures to extract the structural features of the relic images. Through the structural coding module GLFSE, the global and local fusion of coherent structural features of the one-stage restoration is promoted. In the second stage, a texture restoration structure is introduced, utilizing the multi-scale sensing space denoising module MFDAF to enhance the reconstruction of fine textures. In view of the limited digital image data set of mural paintings in China, a new data set of Thangka mural paintings named Tangka was built by ourselves. The proposed method was tested on two cultural relic image data sets, Chinese landscape painting and Thangka mural painting, and Places2 realistic scene data set. We used seven metrics, namely PSNR, SSIM, FID, LPIPS, PIQE, NIQE, and BRISQUE, for objective evaluation and conducted subjective evaluations under five different mask ratios. The results confirm the effectiveness of the proposed method. The source code is publicly available at https://github.com/heart1128/Inpainting-CIOCR.},
  archive      = {J_PAAA},
  author       = {Ning, Tao and Huang, Guowei and Li, Jiaxin and Huang, Shan},
  doi          = {10.1007/s10044-025-01434-9},
  journal      = {Pattern Analysis and Applications},
  month        = {6},
  number       = {2},
  pages        = {1-18},
  shortjournal = {Pattern Anal. Appl.},
  title        = {Complex image inpainting of cultural relics integrating multi-stage structural features and spatial textures},
  volume       = {28},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). TVFace: Towards large-scale unsupervised face recognition in video streams. <em>PAAA</em>, <em>28</em>(2), 1-21. (<a href='https://doi.org/10.1007/s10044-025-01464-3'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent advances in deep learning have led to significant improvements in face recognition systems, but face clustering, particularly in video streams, remains a challenging problem. Current video face clustering approaches are primarily tailored for short-form content, such as movies and television shows, that features a limited number of face images and individuals. The few existing large-scale face datasets are derived from web images and do not effectively capture the complexities of the video domain. In view of these limitations, we present TVFace, the first large-scale dataset of face images extracted from long-form video content. TVFace has been sourced from public live streams of international news channels and contains a total of 2.6 million face images of 33 thousand individuals. To address the challenge of identity annotation in unstructured video streams, we design a semi-automatic annotation framework that combines unsupervised face clustering with human validation, ensuring scalable and high-quality labeling. TVFace is well suited to evaluate and advance face representation and identity classification components of face recognition systems across both image and video domains. We also demonstrate the effectiveness of TVFace in evaluating real-time person retrieval systems using a novel tree-search-based Hierarchical Retrieval Index tailored for online face clustering. In conclusion, our work centers around the preparation of TVFace, a dataset poised to reshape the landscape of face recognition in the video domain, making it a crucial resource for the research community. The dataset and code are available at https://github.com/Vision-At-SEECS/streamface .},
  archive      = {J_PAAA},
  author       = {Khurshid, Atif and Khan, Bostan and Shahzad, Muhammad and Fraz, Muhammad Moazam},
  doi          = {10.1007/s10044-025-01464-3},
  journal      = {Pattern Analysis and Applications},
  month        = {6},
  number       = {2},
  pages        = {1-21},
  shortjournal = {Pattern Anal. Appl.},
  title        = {TVFace: Towards large-scale unsupervised face recognition in video streams},
  volume       = {28},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Diffusion-based data augmentation and hierarchical CLIP for real estate image annotation. <em>PAAA</em>, <em>28</em>(2), 1-13. (<a href='https://doi.org/10.1007/s10044-025-01465-2'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The critical role of real estate search engines in the economic domain underscores the necessity for a robust methodology in annotating the luxury level of room images. Contemporary challenges include inadequacies in room quality assessment, underutilization of deep network capacities, and insufficient annotated house images. This paper presents an innovative real estate image annotation model employing a multi-stage approach that integrates diffusion models with contrastive language-image pretraining (CLIP) networks. Initially, the diffusion network serves as a data augmentation technique, generating supplementary real estate images for training. Next, a hierarchical CLIP model categorizes images into different room types. Subsequently, multiple CLIP models assess the condition of each room, annotating them as either contemporary or standard. The final and most significant stage involves transferring knowledge from the larger CLIP model to a smaller, more efficient model using soft labeling. This approach enhances inferencing speed while maintaining high performance. Experimental results on a newly gathered real estate image dataset demonstrate the superior performance of the proposed approach compared to existing house image classification algorithms. Our demonstration code and checkpoints are available at https://huggingface.co/strollingorange/roomLuxuryAnnotater .},
  archive      = {J_PAAA},
  author       = {Deng, Haojin and Zhang, Wandong and Yang, Yimin and Nejad, Eman},
  doi          = {10.1007/s10044-025-01465-2},
  journal      = {Pattern Analysis and Applications},
  month        = {6},
  number       = {2},
  pages        = {1-13},
  shortjournal = {Pattern Anal. Appl.},
  title        = {Diffusion-based data augmentation and hierarchical CLIP for real estate image annotation},
  volume       = {28},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Privacy-preserving people detection in the wild. <em>PAAA</em>, <em>28</em>(2), 1-21. (<a href='https://doi.org/10.1007/s10044-025-01470-5'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Detecting people and their attributes in natural conditions is one of the basic tasks of computer vision systems. Unfortunately, all known algorithms do not provide even the slightest protection of data privacy. In this paper, we fill this gap by proposing a system for detecting people while maintaining data privacy thanks to the learnable perceptual encryption of an image immediately after its acquisition and before detection. A novelty is our proposed hierachical image scrambling technique and modified YOLO detector, equipped with original input layers that enable effective detection in encrypted images. This is possible thanks to the use of the Lipschitz cost function and original attention modules. The experimental results using images from various everyday situations, obtained both in the visible spectrum and in the thermal band, prove the effectiveness of our method. Our code is available on GitHub.},
  archive      = {J_PAAA},
  author       = {Knapik, Mateusz and Cyganek, Bogusław},
  doi          = {10.1007/s10044-025-01470-5},
  journal      = {Pattern Analysis and Applications},
  month        = {6},
  number       = {2},
  pages        = {1-21},
  shortjournal = {Pattern Anal. Appl.},
  title        = {Privacy-preserving people detection in the wild},
  volume       = {28},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Masked cosine similarity prediction for self-supervised skeleton-based action representation learning. <em>PAAA</em>, <em>28</em>(2), 1-17. (<a href='https://doi.org/10.1007/s10044-025-01472-3'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Skeleton-based human action recognition faces challenges owing to the limited availability of annotated data, which constrains the performance of supervised methods in learning representations of skeleton sequences. To address this issue, researchers have introduced self-supervised learning as a method of reducing the reliance on annotated data. This approach exploits the intrinsic supervisory signals embedded within the data itself. In this study, we demonstrate that considering relative positional relationships between joints, rather than relying on joint coordinates as absolute positional information, yields more effective representations of skeleton sequences. Based on this, we introduce the Masked Cosine Similarity Prediction (MCSP) framework, which takes randomly masked skeleton sequences as input and predicts the corresponding cosine similarity between masked joints. Comprehensive experiments show that the proposed MCSP self-supervised pre-training method effectively learns representations in skeleton sequences, improving model performance while decreasing dependence on extensive labeled datasets. After pre-training with MCSP, a vanilla transformer architecture is employed for fine-tuning in action recognition. The results obtained from six subsets of the NTU-RGB+D 60, NTU-RGB+D 120 and PKU-MMD datasets show that our method achieves significant performance improvements on five subsets. Compared to training from scratch, performance improvements are 9.8%, 4.9%, 13%, 11.5%, and 3.6%, respectively, with top-1 accuracies of 92.9%, 97.3%, 89.8%, 91.2%, and 96.1% being achieved. Furthermore, our method achieves comparable results on the PKU-MMD Phase II dataset, achieving a top-1 accuracy of 51.5%. These results are competitive without the need for intricate designs, such as multi-stream model ensembles or extreme data augmentation. The source code of our MOSP is available at https://github.com/skyisyourlimit/MCSP.},
  archive      = {J_PAAA},
  author       = {Ren, Ziliang and Liu, Ronggui and Qin, Yong and Gao, Xiangyang and Zhang, Qieshi},
  doi          = {10.1007/s10044-025-01472-3},
  journal      = {Pattern Analysis and Applications},
  month        = {6},
  number       = {2},
  pages        = {1-17},
  shortjournal = {Pattern Anal. Appl.},
  title        = {Masked cosine similarity prediction for self-supervised skeleton-based action representation learning},
  volume       = {28},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). YOLO-RACE: Reassembly and convolutional block attention for enhanced dense object detection. <em>PAAA</em>, <em>28</em>(2), 1-17. (<a href='https://doi.org/10.1007/s10044-025-01471-4'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Object detection automatically identifies and locates specific objects within images or videos, and plays a critical role in various fields. Among the many approaches, the YOLO model has significantly improved the accuracy and speed of object detection, thereby garnering attention for real-time processing. However, it exhibits limitations in detecting small or densely packed objects. This study introduces YOLO-RACE (You Only Look Once with ReAssembly and Convolutional block attEntion), an improved model based on YOLOv8 by incorporating the CARAFE upsampling technique, ResBlock, and the Convolutional Block Attention Module. YOLOv8 was selected from among various YOLO models because of its ability to deliver high accuracy in complex object detection tasks while maintaining its lightweight architecture and overall efficiency. The proposed model was evaluated using the VisDrone 2019, Pascal VOC, and SKU-110 K datasets. The experimental results demonstrated a precision of 0.419, recall of 0.321, mAP50 of 0.316, mAP50@95 of 0.18, and F1-score of 0.364 on the VisDrone 2019 dataset. On the Pascal VOC dataset, the model achieved a precision of 0.764, recall of 0.699, mAP50 of 0.773, mAP50@95 of 0.559, and F1-score of 0.73. Furthermore, on the SKU-110 K dataset, the model attained a precision of 0.904, recall of 0.841, mAP50 of 0.902, mAP50@95 of 0.575, and F1-score of 0.871. Based on these comparative results, the proposed model achieved superior performance compared with YOLOv8 and its variants, as well as with other state-of-the-art models such as YOLOv9, YOLOv10, YOLOv11, GELAN, and EfficientDet. This code is available at https://github.com/AnBLab-BAE/YOLO-RACE.git .},
  archive      = {J_PAAA},
  author       = {Bae, Myeong-Hun and Park, Sung-Wook and Park, Jun and Jung, Se-Hoon and Sim, Chun-Bo},
  doi          = {10.1007/s10044-025-01471-4},
  journal      = {Pattern Analysis and Applications},
  month        = {6},
  number       = {2},
  pages        = {1-17},
  shortjournal = {Pattern Anal. Appl.},
  title        = {YOLO-RACE: Reassembly and convolutional block attention for enhanced dense object detection},
  volume       = {28},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). End-to-end model compression via pruning and knowledge distillation for lightweight image super resolution. <em>PAAA</em>, <em>28</em>(2), 1-18. (<a href='https://doi.org/10.1007/s10044-025-01450-9'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Obtaining lightweight models is crucial for effectively addressing image super-resolution (SR), particularly for resource-constrained devices. Pruning and knowledge distillation (KD) are two commonly employed methods for compressing SR models. However, previous approaches have typically focused on applying these methods either individually or sequentially, starting with pruning followed by KD. This sequential, pipeline approach tends to compartmentalize the techniques, failing to fully leverage the knowledge from the teacher model to guide the selection of redundant channels, often resulting in suboptimal pruning outcomes. To address these issues, we propose an end-to-end compression strategy named Pruning While Knowledge Distillation (PWKD) that successfully integrates pruning and KD into a single training process, significantly enhancing training efficiency. Furthermore, This novel integrated compression method allows the teacher model to provide guidance during the pruning process, instead of after pruning has been completed. By leveraging knowledge distillation from the teacher model to guide the pruning channel selection, it resolves the suboptimal channel selection issues commonly encountered in sequential training methods. This integration not only streamlines the training process for greater efficiency but also improves performance by ensuring more informed and effective pruning decisions. In addition, for pruning, we design an auto-pruning module that utilizes feature information to adaptively learn pruning masks, enabling automated pruning without the need for manually defined criteria. Moreover, we design a backward-differentiable gating function ensures the auto-pruning module remains differentiable during backpropagation. Furthermore, to address the challenges of KD, we introduce the Multiscale Wavelet Refine Module (MVRM). Designed to enhance the processing of image edges and intricate textures, MVRM significantly boosts the student model’s capability to accurately replicate the teacher model’s proficiency in restoring high-frequency information. Our integrated approach has been tested across multiple datasets and has consistently demonstrated significant performance improvements.},
  archive      = {J_PAAA},
  author       = {Wang, Yanzhe and Wang, Yizhen and Rohra, Avinash and Yin, Baoqun},
  doi          = {10.1007/s10044-025-01450-9},
  journal      = {Pattern Analysis and Applications},
  month        = {6},
  number       = {2},
  pages        = {1-18},
  shortjournal = {Pattern Anal. Appl.},
  title        = {End-to-end model compression via pruning and knowledge distillation for lightweight image super resolution},
  volume       = {28},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). GGNN: Group-guided nearest neighbors for efficient image matching. <em>PAAA</em>, <em>28</em>(2), 1-12. (<a href='https://doi.org/10.1007/s10044-025-01462-5'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The widely adopted image matching approach remains dependent on exhaustive matching of local features across images. Existing methods aiming to improve efficiency either approximate nearest neighbor (NN) search, compromising accuracy, or apply filtering only after establishing tentative matches, which restricts potential efficiency gains. We challenge the assumption that exhaustive NN search is necessary by proposing a more efficient hierarchical approach that maintains matching accuracy without relying on full-scale NN search. Our key insight is that efficiently identifying sufficiently similar, geometrically meaningful feature matches-rather than the most similar but geometrically random ones-can improve or maintain performance at a lower computational cost. We propose a novel method, Group-Guided Nearest Neighbors (GGNN), which matches groups of features first and then matches individual features only within these matched groups. This hierarchical pipeline reduces the computational complexity of feature matching from $$\theta (n^2)$$ to $$\theta (n \sqrt{n})$$ , significantly improving efficiency. Experimental results on homography estimation demonstrate that GGNN outperforms standard NN search while achieving performance comparable to state-of-the-art methods. Additionally, we formulate GGNN as a general framework, where conventional NN search is a special case with a single global feature group. This formulation provides a continuum of feature matching methods with varying computational costs, enabling automatic selection based on a given time budget.},
  archive      = {J_PAAA},
  author       = {Çine, Ersin and Baştanlar, Yalın and Özuysal, Mustafa},
  doi          = {10.1007/s10044-025-01462-5},
  journal      = {Pattern Analysis and Applications},
  month        = {6},
  number       = {2},
  pages        = {1-12},
  shortjournal = {Pattern Anal. Appl.},
  title        = {GGNN: Group-guided nearest neighbors for efficient image matching},
  volume       = {28},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Swin transformer with attention mechanism: A novel framework for person re-identification. <em>PAAA</em>, <em>28</em>(2), 1-14. (<a href='https://doi.org/10.1007/s10044-025-01466-1'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Vision Transformers have successfully captured long-range dependencies across various computer vision tasks, including Person Re-Identification (PReID). This paper proposed SwinReID, a novel PReID framework based on the Swin Transformer (ST) architecture. Unlike traditional convolutional neural networks (CNNs) that process entire images, SwinReID divides images into small, non-overlapping patches. This patch-based strategy enables a comprehensive understanding of global image context while preserving fine-grained spatial details, addressing challenges such as occlusions and appearance variations in PReID tasks. Conventional CNNs struggle with long-range dependencies due to their localized receptive fields and downsampling operations, limiting their effectiveness in complex scenarios. SwinReID overcomes these limitations by integrating an advanced transformer-based backbone and a lightweight channel attention (CA) module, which selectively improves discriminative feature channels. This refinement leads to better feature representation and robustness, particularly in challenging environments. SwinReID achieves state-of-the-art performance in benchmark datasets, with rank-1 accuracies of 95.5% in Market1501, 89.9% in DukeMTMC-reID, 64.1% in Occluded-Duke, and 91.63% in MSMT17, demonstrating its effectiveness in various and challenging PReID scenarios.},
  archive      = {J_PAAA},
  author       = {Arain, Tariq Ali and Zhang, Pengcheng and Meng, Qing and Muhammad, Abdullahi Uwaisu},
  doi          = {10.1007/s10044-025-01466-1},
  journal      = {Pattern Analysis and Applications},
  month        = {6},
  number       = {2},
  pages        = {1-14},
  shortjournal = {Pattern Anal. Appl.},
  title        = {Swin transformer with attention mechanism: A novel framework for person re-identification},
  volume       = {28},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). RS-GAN: Unsupervised running script font generation via disentangled representation learning and contextual transformer. <em>PAAA</em>, <em>28</em>(2), 1-12. (<a href='https://doi.org/10.1007/s10044-025-01468-z'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the rapid development of deep learning, calligraphy font generation has received more and more attention. Existing methods for generating calligraphy fonts usually generate regular script, which has horizontal and vertical strokes and less variation in character shape. However, the difference between running script and regular script is huge, the regular script generation model is not suitable for generating running script fonts. To address this problem, this paper proposes a novel Generative Adversarial Network model for running script font generation, called RS-GAN, which can handle the large geometric changes between different fonts. In RS-GAN, we decoupled the input images into content space and style space to separate accurate content and style representations from the limited authentic work of calligraphers. In addition, by combining the Contextual Transformer (CoT) block with Generative Adversarial Network, the model can fully capitalize on the contextual information between the input keys. The representative ability of the output feature map is enhanced, and the ability to learn complex structural features of the running script (e.g. special simple and continuous stroke techniques) is improved. The proposed RS-GAN is verified on running script data with structural differences. These data come from the “Three Greatest Running Scripts in the World”, namely Wang Xizhi’s “Orchid Pavilion Preface”, Su Shi’s “Cold Food Observance”, and Yan Zhenqing’s “Requiem to My Nephew”. The experimental results show that the proposed RS-GAN successfully generates the running script fonts with different structures and outperforms the existing models, which verifies the effectiveness and robustness of the RS-GAN.},
  archive      = {J_PAAA},
  author       = {Wang, Xuanhong and Li, Cong and Sun, Zengguo and Hui, Luying},
  doi          = {10.1007/s10044-025-01468-z},
  journal      = {Pattern Analysis and Applications},
  month        = {6},
  number       = {2},
  pages        = {1-12},
  shortjournal = {Pattern Anal. Appl.},
  title        = {RS-GAN: Unsupervised running script font generation via disentangled representation learning and contextual transformer},
  volume       = {28},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Adaptive graph learning for enhanced incomplete multi-view clustering. <em>PAAA</em>, <em>28</em>(2), 1-9. (<a href='https://doi.org/10.1007/s10044-025-01469-y'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Incomplete multi-view clustering (IMVC) aims to explore complementary and consistent information across incomplete views, yet existing IMVC methods overlook the potential hidden relationships between missing data. This paper proposes an adaptive graph learning approach for incomplete multi-view inferring and clustering (IMIC-AGL), which utilizes observable information between missing views to model and infer missing data. Specifically, missing data is modeled using robust principal component analysis, and adaptive neighborhood graph learning with rank constraints is employed to effectively fill in missing data. Experiments on benchmark datasets with varying missing rates demonstrate the effectiveness of IMIC-AGL, achieving state-of-the-art clustering performance, particularly under high missing ratios.},
  archive      = {J_PAAA},
  author       = {Hong, Rui and Chen, Xiao-ping and Zhou, Yan and Liu, Hui and Wan, Tiancai and Bai, Taili},
  doi          = {10.1007/s10044-025-01469-y},
  journal      = {Pattern Analysis and Applications},
  month        = {6},
  number       = {2},
  pages        = {1-9},
  shortjournal = {Pattern Anal. Appl.},
  title        = {Adaptive graph learning for enhanced incomplete multi-view clustering},
  volume       = {28},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Enhancing out-of-distribution learning in computer vision through dominant feature masking. <em>PAAA</em>, <em>28</em>(2), 1-30. (<a href='https://doi.org/10.1007/s10044-025-01474-1'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Out-of-distribution (OOD) learning presents a major challenge in machine learning as models must effectively generalize to previously unseen data. This challenge is prevalent in deep learning models, which tend to focus on the most dominant features in images. This narrow focus impedes OOD learning, where critical features are concealed or absent during testing, leading to reduced prediction accuracy. To address this issue, we introduce a novel data augmentation approach termed Dominant Feature Masking (DFM), inspired by human visual holistic processing. DFM strategically conceals and reveals the most prominent features within images, allowing neural networks to simultaneously capture both dominant and non-dominant attributes, thereby enhancing adaptability to OOD data. We evaluated DFM using a novel set of learning challenges termed Versatile Evaluation Benchmark (VEB), which assesses model performance on three distinct tasks: (i) augmented MNIST images to test resilience against diverse transformations; (ii) a novel dataset of unseen image classes to examine performance on new instances within familiar categories; and (iii) a dataset created by DALL-E to challenge class differentiation with artificially mixed features. Our results demonstrate that DFM significantly improves OOD generalization compared to traditional augmentation techniques, achieving marked enhancements across various conditions without compromising in-distribution testing accuracy. These findings underscore the potential of DFM to improve the performance of computer vision systems in various real-world scenarios, making them more robust and adaptable to unexpected data variations. By leveraging VEB, researchers will gain a deeper understanding of their models’ generalization performance, ensuring that CNNs are well-equipped to handle the complexities of real-world applications. The source code and VEB datasets are available at https://github.com/DeepVisionary/DFM .},
  archive      = {J_PAAA},
  author       = {Pilzak, Artem and Thivierge, Jean-Philippe},
  doi          = {10.1007/s10044-025-01474-1},
  journal      = {Pattern Analysis and Applications},
  month        = {6},
  number       = {2},
  pages        = {1-30},
  shortjournal = {Pattern Anal. Appl.},
  title        = {Enhancing out-of-distribution learning in computer vision through dominant feature masking},
  volume       = {28},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Temporal focal modulation networks for sleep stage scoring. <em>PAAA</em>, <em>28</em>(2), 1-19. (<a href='https://doi.org/10.1007/s10044-025-01475-0'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Accurate sleep stage scoring is crucial for diagnosing and treating sleep disorders, yet traditional manual methods are time-consuming and susceptible to variability. While recent advancements in machine learning and deep learning have enhanced automated sleep stage detection, many approaches still rely on handcrafted features and encounter limitations when processing full-night data. In this paper, we introduce a novel many-to-many classification framework that leverages a temporal focal modulation network for efficient and accurate sleep stage scoring. Our model, SleepFocalNet, processes full-night single-channel EEG signals and predicts sleep stages for all epochs simultaneously. SleepFocalNet is composed of three key components: a convolution block for local feature extraction, a focal modulation block for long-range temporal modeling, and a classification block for final predictions. We evaluated SleepFocalNet on Sleep Heart Health Study (SHHS), SleepEDF-20, and SleepEDF-78 datasets, achieving state-of-the-art performance. On SHHS, SleepFocalNet attained an accuracy of 0.888 and an F1-score of 0.815. On SleepEDF-20, it obtained an accuracy of 0.885 and an F1-score of 0.836. On SleepEDF-78, it outperformed other models with an accuracy of 0.855 and an F1-score of 0.800. This study represents the first application of temporal focal modulation networks in sleep stage scoring. Additionally, we conducted an extensive analysis of various network configurations to assess the impact of different architectural choices on performance. The results validate the potential of our approach to enhance the reliability and scalability of automated sleep stage scoring, offering a robust alternative to existing methods.},
  archive      = {J_PAAA},
  author       = {Zan, Hasan},
  doi          = {10.1007/s10044-025-01475-0},
  journal      = {Pattern Analysis and Applications},
  month        = {6},
  number       = {2},
  pages        = {1-19},
  shortjournal = {Pattern Anal. Appl.},
  title        = {Temporal focal modulation networks for sleep stage scoring},
  volume       = {28},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Robustness of unsupervised methods for image surface-anomaly detection. <em>PAAA</em>, <em>28</em>(2), 1-12. (<a href='https://doi.org/10.1007/s10044-025-01477-y'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Surface-anomaly detection is a critical challenge in ensuring product quality, as defects can pose safety risks and diminish product lifespan. A significant challenge in this domain is the limited availability of anomalous samples which makes training supervised models impractical. In response, unsupervised deep-learning-based methods have attracted significant attention in recent years, as they do not require anomalous samples for training. Such methods assume that during dataset curation all anomalous samples can be identified and subsequently removed from the training set. In practice, however, identifying all anomalous samples without any false negatives is rarely possible, either due to the human errors or due to the ambiguity in what is considered a defect and what is not. In this paper, we address the need to measure the robustness of the unsupervised surface-anomaly detection methods as one of the most important performance metrics. To this end, we propose a robustness measure that describes the sensitivity of an unsupervised method to the presence of anomalous data in the training set. We extensively evaluate seven well established unsupervised methods that follow different anomaly detection paradigms on four diverse datasets and analyze the results. We show that most of the analyzed methods are fairly robust to low percentages of anomalous samples in the training set, with some of them retaining the near-baseline performance even when that percentage grows fairly large.},
  archive      = {J_PAAA},
  author       = {Božič, Jakob and Fučka, Matic and Zavrtanik, Vitjan and Skočaj, Danijel},
  doi          = {10.1007/s10044-025-01477-y},
  journal      = {Pattern Analysis and Applications},
  month        = {6},
  number       = {2},
  pages        = {1-12},
  shortjournal = {Pattern Anal. Appl.},
  title        = {Robustness of unsupervised methods for image surface-anomaly detection},
  volume       = {28},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Enhancing part-based gait recognition via ensemble learning and feature fusion. <em>PAAA</em>, <em>28</em>(2), 1-19. (<a href='https://doi.org/10.1007/s10044-025-01478-x'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Gait, a behavior-based biometric feature, has gained increasing popularity in human identification, particularly in surveillance systems, due to its ability to function without physical contact or explicit consent. Traditional silhouette-based methods have demonstrated that different body parts exhibit distinct movement patterns during walking, thereby enhancing recognition accuracy. In this study, we propose an improved part-based gait recognition approach by leveraging ensemble learning on local body regions. The Gait Energy Image (GEI) is segmented into five horizontal parts, and ensemble learning is applied to the convolutional neural network (CNN) responsible for their processing. A separate MetaModel is trained for each body part to integrate the part-based features obtained from ensemble learning and synthesize the most discriminative ones. Additionally, a part-removal process is introduced to mitigate the effects of appearance-based variations by analyzing absolute differences between images with and without variations. The aggregated most distinctive features contribute to robust recognition. We evaluate our proposed approach on the CASIA-B, CASIA-C, and Outdoor-Gait datasets, and experimental results indicate that ensemble learning significantly enhances part-based gait recognition performance under various appearance variations, outperforming several state-of-the-art methods. The datasets and source code are available at https://github.com/busrakckugurlu/Enhancing-Part-based-Gait-Recognition-via-Ensemble-Learning-and-Feature-Fusion/tree/main .},
  archive      = {J_PAAA},
  author       = {Yaprak, Büşranur and Gedikli, Eyüp},
  doi          = {10.1007/s10044-025-01478-x},
  journal      = {Pattern Analysis and Applications},
  month        = {6},
  number       = {2},
  pages        = {1-19},
  shortjournal = {Pattern Anal. Appl.},
  title        = {Enhancing part-based gait recognition via ensemble learning and feature fusion},
  volume       = {28},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). FedPG: A privacy-friendly and universal method for solving non-IID data in federated learning. <em>PAAA</em>, <em>28</em>(2), 1-14. (<a href='https://doi.org/10.1007/s10044-025-01453-6'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Federated learning (FL) is a privacy-preserving distributed learning framework which could harness the potential of decentralized multimedia data. However, a significant hurdle lies in the non-uniform distribution of data among clients, leading to slow convergence and subpar accuracy in the global model. Although several approaches have been proposed to address this challenge, two key limitations remain. First, these methods frequently require access to information about local data or even the raw data, which raises significant privacy concerns for clients. Second, these methods struggle to perform well in a common non-IID scenario: class missingness, and they often fail to fully resolve the issue of client drift. In response, in this paper, we propose a privacy-friendly and universal method FedPG to solve non-IID data in FL. The core idea behind FedPG is to leverage homogeneous virtual data to alleviate both data heterogeneity and client drift. Specifically, FedPG introduces a novel image generation method based on prototype loss, which does not require any additional privacy-sensitive information. This approach generates synthetic datasets aligned with the global distribution to effectively assist local training. Besides, we also design a local training method that is suitable for scenarios involving class missingness, enabling both feature adaptation and classifier de-biasing. The comprehensive experiments demonstrate the efficacy of our FedPG framework. In the majority of cases, FedPG not only achieves superior accuracy but also exhibits accelerated convergence rates compared to alternative approaches.},
  archive      = {J_PAAA},
  author       = {Xue, Baolu and Zhang, Jiale and Chen, Bing and Meng, Weizhi},
  doi          = {10.1007/s10044-025-01453-6},
  journal      = {Pattern Analysis and Applications},
  month        = {6},
  number       = {2},
  pages        = {1-14},
  shortjournal = {Pattern Anal. Appl.},
  title        = {FedPG: A privacy-friendly and universal method for solving non-IID data in federated learning},
  volume       = {28},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). MDAAF: Masked domain adversarial adaptation framework for unsupervised domain adaptive semantic segmentation. <em>PAAA</em>, <em>28</em>(2), 1-24. (<a href='https://doi.org/10.1007/s10044-025-01473-2'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Unsupervised domain adaptation (UDA) is an important task that transfers learned knowledge from the source domain to the unseen target domain. Domain shift is the major challenge faced by UDA methods for aerial data, which is caused by differences in appearance, distribution, decision boundary, sensor platforms, capturing conditions, etc. Mix representations combine inputs of both domains, which are helpful for the generation of domain-consistent features using self-training. Domain alignment at multiple levels is required for effective adaptive segmentation between domains. In this work, we proposed a Masked Domain Adversarial Adaptation Framework (MDAAF), which consists of the Masked Domain Adversarial Adaptation Network (MDAANet), Masked Domain Dual Adaptation (MDDA) approach, Joint Adversarial Alignment (JAA), Consistency Enforcement (CE), and Feature Dissimilarity-based Alignment (FDA) for effective UDA. The proposed MDAAF achieves the all-level adaptation (input, feature, and output) to handle domain shift issues using multi-level features generated by MDAANet with MDDA. FDA increased the inter-class feature dissimilarities, CE attained input-consistent output adaptation, and JAA achieved combined adaptation at the input–output level. MDAAF achieved state-of-the-art results on six benchmark domain adaptation tasks with performance improvement in the $$\sim $$ 1.5–10% range from previous methods. The trained model weights and implementation code are available at https://github.com/chouhan-avinash/MDAAF/tree/master/ .},
  archive      = {J_PAAA},
  author       = {Chouhan, Avinash and Sur, Arijit and Chutia, Dibyajyoti and Aggarwal, Shiv Prasad},
  doi          = {10.1007/s10044-025-01473-2},
  journal      = {Pattern Analysis and Applications},
  month        = {6},
  number       = {2},
  pages        = {1-24},
  shortjournal = {Pattern Anal. Appl.},
  title        = {MDAAF: Masked domain adversarial adaptation framework for unsupervised domain adaptive semantic segmentation},
  volume       = {28},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). PointFEA: Beyond simple points, unveiling the power of raw positional features. <em>PAAA</em>, <em>28</em>(2), 1-9. (<a href='https://doi.org/10.1007/s10044-025-01481-2'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Rapid advancements in 3D acquisition technologies have significantly advanced the widespread application of point clouds in robotics and autonomous systems. However, the inherent unordered and unstructured nature of point clouds poses significant challenges for neural network design. Most methods neglect the potential of the features represented by the original positional information of the points. To capture comprehensive feature information to enhance local feature aggregation, we propose the Local Feature Aggregation Module (LFAM), which incorporates the original positional information and integrates it with edge features. To further enhance the classification accuracy and practical applicability of the model, we propose a novel point feature enhancement architecture, PointFEA, which initially employs LFAM and subsequently incorporates a channel-cluster-based attention mechanism. Our method optimally extracts local features while effectively integrating global contextual information. Extensive experimental evaluations demonstrate that PointFEA excels in point cloud classification, particularly on the ScanObjectNN dataset, achieving a 2.4% improvement over the baseline method.The code is available at https://github.com/2779705737/PointFEA/tree/master .},
  archive      = {J_PAAA},
  author       = {Ren, Leyan and Wang, Jianming and Sun, Yukuan and Li, Xiuyan},
  doi          = {10.1007/s10044-025-01481-2},
  journal      = {Pattern Analysis and Applications},
  month        = {6},
  number       = {2},
  pages        = {1-9},
  shortjournal = {Pattern Anal. Appl.},
  title        = {PointFEA: Beyond simple points, unveiling the power of raw positional features},
  volume       = {28},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Person re-identification in video surveillance systems by feature replacement of occluded parts of human figures. <em>PAAA</em>, <em>28</em>(2), 1-12. (<a href='https://doi.org/10.1007/s10044-025-01482-1'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Тhe algorithm for re-identifying people in intelligent video surveillance systems is proposed. It is based on the construction of a compound neural network descriptor and replacing features of occluded parts of a human figure. Composite descriptors are generated for all images in a gallery and recorded in a table. They characterize the global and local features of each person, considering his individual parts’ visibility. Detection and selection of areas of interest for the formation of local descriptors is carried out based on detecting key points of the human body. If a person is partially occluded by other people or objects, then the corresponding region is classified as invisible, and the compound descriptor of respective component will be invalid and equal to zero. For images whose feature vector has zero components, the feature table is ranked according to the cosine similarity metric for each visible local fragment. Based on the feature table rankings, the k-nearest neighbors are determined and the k1-best are selected from them. The corresponding k1-nearest neighbors’ component average value of the feature vector is used to replace the zero descriptor components. The feature table is then updated for the generated vectors and ranking is performed according to the query using the cosine similarity metric. ResNet-50 and DenseNet-121 were used as backbone CNNs for feature extraction, and testing was performed using Market-1501, DukeMTMC-ReID, Occluded-Duke, MSMT17, and PolReID1077 datasets.},
  archive      = {J_PAAA},
  author       = {Ye, Shiping and Bohush, R. and Ihnatsyeva, S. and Ablameyko, S.},
  doi          = {10.1007/s10044-025-01482-1},
  journal      = {Pattern Analysis and Applications},
  month        = {6},
  number       = {2},
  pages        = {1-12},
  shortjournal = {Pattern Anal. Appl.},
  title        = {Person re-identification in video surveillance systems by feature replacement of occluded parts of human figures},
  volume       = {28},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multiscope topology learning with conditional updating for airway segmentation. <em>PAAA</em>, <em>28</em>(2), 1-15. (<a href='https://doi.org/10.1007/s10044-025-01480-3'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Airway segmentation is essential in computer-assisted diagnosis and screening of bronchial diseases due to the inherent difficulty in obtaining a direct and clear visualization of airway trees from raw CT images. Although medical image segmentation technology is gradually maturing and beginning to be applied in clinics, challenges like breakages and leakages remain in airway segmentation. We propose a novel framework that enhances UNet3D with large-kernel attention for improved global and local feature extraction. A multitask prediction head across voxel, neighborhood, and surface scopes is introduced to better capture airway topology, supervised by customized loss functions. Additionally, a conditional updating strategy leverages a shared encoder and dual decoders to improve segmentation of thin branches by balancing over- and under-segmentation. Specifically, one decoder is optimized with hard examples to encourage over-segmentation, and the other refines results for accurate segmentation using all samples. Our model is quantitatively evaluated on the Binary Airway Segmentation dataset, achieving a Dice score of 0.904, precision of 0.954, tree detection rate of 0.950, and branch detection rate of 0.915, outperforming several recent methods in topological accuracy. In the Airway Tree Modeling Challenge 2022 validation set, our method ranks second overall by a mean position score across all metrics. Our future work aims to enhance prediction confidence and adaptability in ambiguous regions and improve generalizability and interpretability across diverse clinical datasets.},
  archive      = {J_PAAA},
  author       = {Hu, Yan and Meijering, Erik and Song, Yang},
  doi          = {10.1007/s10044-025-01480-3},
  journal      = {Pattern Analysis and Applications},
  month        = {6},
  number       = {2},
  pages        = {1-15},
  shortjournal = {Pattern Anal. Appl.},
  title        = {Multiscope topology learning with conditional updating for airway segmentation},
  volume       = {28},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Efficient error minimization in kernel k-means clustering. <em>PAAA</em>, <em>28</em>(2), 1-25. (<a href='https://doi.org/10.1007/s10044-025-01463-4'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Kernel k-means extends the k-means algorithm to identify non-linearly separable clusters but is inherently sensitive to cluster initialization. To address this challenge, we first formulate the kernel k-means ++ method, which conveys the efficient center initialization strategy of k-means++ from Euclidean to kernel space. Building on this, we propose global kernel k-means ++ ( $$\text {GK}k\text {M}$$ ++), a novel clustering algorithm designed to balance clustering error minimization with reduced computational cost. $$\text {GK}k\text {M}$$ ++ extends the well-established global kernel k-means algorithm by incorporating the stochastic initialization strategy of kernel k-means++. This approach significantly reduces computational complexity while preserving superior clustering error minimization capabilities akin to traditional global kernel k-means. The experimental results on synthetic, real, and graph datasets indicate that $$\text {GK}k\text {M}$$ ++ consistently outperforms both kernel k-means with random initialization and kernel k-means++, while achieving solutions comparable to those provided by the exhaustive and computational intensive global kernel k-means method.},
  archive      = {J_PAAA},
  author       = {Vardakas, Georgios and Papakostas, Ioannis and Likas, Aristidis},
  doi          = {10.1007/s10044-025-01463-4},
  journal      = {Pattern Analysis and Applications},
  month        = {6},
  number       = {2},
  pages        = {1-25},
  shortjournal = {Pattern Anal. Appl.},
  title        = {Efficient error minimization in kernel k-means clustering},
  volume       = {28},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Region-aligned single-stage point cloud object detector with direct feature compression and cross-semantic attention mechanism. <em>PAAA</em>, <em>28</em>(2), 1-14. (<a href='https://doi.org/10.1007/s10044-025-01467-0'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Lidar has become increasingly crucial for perception in autonomous driving due to its indispensable advantages. Current voxel-based single-stage detector (SSD) employs a method that utilizes 3D sparse backbone and 2D Bird’s Eye View (BEV) backbone for predicting targets in the point cloud. However, the sparse-to-dense layer between these two processes not only brings inconvenience in model design, but also deforms the height structure in feature representation, thus limiting construction of downstream backbone and ability of object perception. Therefore, we propose a Directly Sparse Feature Compression(DSFC) Block to better utilize 3D features and transform them into 2D features. Additionally, to address the weak correlation between regression and semantic features and the lack of abilities to extract global features which limits the detection performance of voxel-based SSD, we propose a Cross-semantic Cross-dimension Multi-head Attention(CDMHA) Block to better utilize regression features to enhance the ability of semantic branches. Experiments on the KITTI dataset demonstrate that our DSFC Block is more effective compared to the vanilla approach. The Cross-semantic CDMHA Block, designed using the CDMHA mechanism, enhances the object detection capability of various mainstream voxel-based SSD. We designed a network named RA-SSD to demonstrate the compatibility of our proposed methods. Experiments show that RA-SSD achieves excellent improvement in all categories compared with the baseline model.},
  archive      = {J_PAAA},
  author       = {Chen, Zhuo and Pan, Shuguo and Guo, Peng and Gao, Wang},
  doi          = {10.1007/s10044-025-01467-0},
  journal      = {Pattern Analysis and Applications},
  month        = {6},
  number       = {2},
  pages        = {1-14},
  shortjournal = {Pattern Anal. Appl.},
  title        = {Region-aligned single-stage point cloud object detector with direct feature compression and cross-semantic attention mechanism},
  volume       = {28},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Unsupervised pedestrian intention estimation through deep neural embeddings and spatio-temporal graph convolutional networks. <em>PAAA</em>, <em>28</em>(2), 1-14. (<a href='https://doi.org/10.1007/s10044-025-01483-0'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A deep understanding of pedestrian intention and crossing behaviors is crucial in applications like pedestrian attribute recognition and autonomous driving. While vehicles need to predict the movements of pedestrians accurately for safety, the recognition and re-identification systems rely on behavioral cues that help them enhance identity tracking and attribute analysis. Traditional trajectory-based methods for pedestrian intention estimation evaluate the future positions of pedestrians based on their past movements but may fail to capture their true intentions. A more effective approach will anticipate actions by analyzing underlying intent, improving the precision of pedestrian recognition and the motion prediction. Current research on estimating pedestrian intentions primarily depends on supervised learning methods. In contrast, this work introduces an unsupervised learning approach to learn intention representations. This method is based on the idea that similar intentions lead to comparable behaviors among pedestrians, and, therefore, they can be clustered. To achieve this, this paper introduces UnPIE, an unsupervised method for predicting pedestrian intentions. It utilizes Spatio-Temporal Graph Convolutional Networks to encode intentions from videos and map them into a D-dimensional latent space. The training phase incorporates Instance Recognition to increase separation between embeddings from different classes and Local Aggregation to form soft clusters of related embeddings. A supervised non-parametric classifier is used to evaluate the performance of the method. The results demonstrate that UnPIE has comparable performance with respect to supervised approaches and even surpasses them, achieving a higher Precision by about 7% on the Pedestrian Intention Estimation dataset.},
  archive      = {J_PAAA},
  author       = {Scaccia, Simone and Pro, Francesco and Amerini, Irene},
  doi          = {10.1007/s10044-025-01483-0},
  journal      = {Pattern Analysis and Applications},
  month        = {6},
  number       = {2},
  pages        = {1-14},
  shortjournal = {Pattern Anal. Appl.},
  title        = {Unsupervised pedestrian intention estimation through deep neural embeddings and spatio-temporal graph convolutional networks},
  volume       = {28},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Solving jigsaw puzzles with vision transformers. <em>PAAA</em>, <em>28</em>(2), 1-13. (<a href='https://doi.org/10.1007/s10044-025-01484-z'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Puzzle-solving is a problem having applications for instance in archaeology and cultural heritage. Proposed solutions often suffer from a performance loss when the pieces are eroded, a characteristic that is pervasive across various use—cases such as frescoes reconstruction. Most approaches divide the problem into two fundamental phases: discriminating and then positioning the pieces. We focus on the case of puzzles with square pieces, without any missing or extraneous pieces, and we introduce the first two-step deep learning solution capable of efficiently solving puzzles, from discrimination to piece placement, while remaining robust to erosion. In the context of permutation learning, we propose to use transformers to determine the correct placement of the pieces and an encoder that uses the information at the edge of the pieces. This method sets a new state of the art, achieving a significant performance gain, and introduces a new approach for learning similarity functions in the context of puzzle solving.},
  archive      = {J_PAAA},
  author       = {Heck, Gaël and Lermé, Nicolas and Le Hégarat-Mascle, Sylvie},
  doi          = {10.1007/s10044-025-01484-z},
  journal      = {Pattern Analysis and Applications},
  month        = {6},
  number       = {2},
  pages        = {1-13},
  shortjournal = {Pattern Anal. Appl.},
  title        = {Solving jigsaw puzzles with vision transformers},
  volume       = {28},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). MCT-net: A multi-branch hybrid CNN-transformer model for medical image segmentation. <em>PAAA</em>, <em>28</em>(2), 1-21. (<a href='https://doi.org/10.1007/s10044-025-01487-w'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, neural networks have demonstrated substantial progress in medical image segmentation. However, accurately segmenting objects in medical images is often restricted by edge blurring, which complicates the delineation of structures of interest. The purpose of this study is to develop a multi-branch hybrid CNN-transformer model to address the issue of edge blurring in medical image segmentation. This approach enables more precise edge detection and segmentation. Integrating hybrid CNN and transformer decoders facilitates detailed recovery while preserving global structures, thus minimizing errors in regions with blurred edges. Additionally, we incorporate a hybrid attention mechanism at the model’s bottleneck to enhance feature transfer between the encoder and decoder. The model was trained using the ACDC, Synapse, and polyp datasets, with a focus on improving segmentation accuracy in regions with blurry edges. Experimental results reveal that the proposed model surpasses existing state-of-the-art methods on the ACDC, Synapse, and polyp datasets, demonstrating its superior performance and effectiveness. These results suggest that the proposed model offers a significant improvement in handling edge blurring, with potential applications in enhancing medical image analysis and diagnostics.},
  archive      = {J_PAAA},
  author       = {Shen, Longfeng and Diao, Liangjin and Peng, Rui and Chen, Jiacong and Lu, Zhengtian and Ge, Fangzhen},
  doi          = {10.1007/s10044-025-01487-w},
  journal      = {Pattern Analysis and Applications},
  month        = {6},
  number       = {2},
  pages        = {1-21},
  shortjournal = {Pattern Anal. Appl.},
  title        = {MCT-net: A multi-branch hybrid CNN-transformer model for medical image segmentation},
  volume       = {28},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). DCRnet: An effective 2D and 3D lane detection method on highway. <em>PAAA</em>, <em>28</em>(2), 1-12. (<a href='https://doi.org/10.1007/s10044-025-01488-9'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Lane detection is a crucial component in autonomous driving systems, yet challenges persist in achieving accurate detection and lightweight deployment of 3D lane detection on highways. To address these issues, we propose a novel striped lane representation that more closely reflects the real-world characteristics of lane lines. Additionally, we introduce a double-branch cross-layer refinement network, designed to enhance model robustness while accelerating training convergence. To further enhance detection performance, we develop the stripes IOU loss function, specifically tailored for evaluating striped lane representations. To simplify deployment, we implement a prior-based spatial projection correction mechanism, effectively mapping 2D detection results into 3D space. Our algorithm achieves a 97% accuracy on the TuSimple and an average F1-score of 79.9 on the CULane, all while maintaining remarkable runtime efficiency. Furthermore, on the 3D OpenLane, it delivers robust detection performance in critical adjacent regions while sustaining high detection efficiency. These results offer substantial support for the effectiveness and practical feasibility of our approach on highway.},
  archive      = {J_PAAA},
  author       = {Zhao, Long and Liu, Xiaoye and Li, Linxiang},
  doi          = {10.1007/s10044-025-01488-9},
  journal      = {Pattern Analysis and Applications},
  month        = {6},
  number       = {2},
  pages        = {1-12},
  shortjournal = {Pattern Anal. Appl.},
  title        = {DCRnet: An effective 2D and 3D lane detection method on highway},
  volume       = {28},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Ghost-HRNet: A lightweight high-resolution network for efficient human pose estimation with enhanced multi-scale feature fusion. <em>PAAA</em>, <em>28</em>(2), 1-15. (<a href='https://doi.org/10.1007/s10044-025-01440-x'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Human pose estimation (HPE) is a critical task in computer vision, with applications spanning human-computer interaction, intelligent surveillance, behavior analysis, virtual reality, and medical diagnosis. However, existing high-resolution networks (HRNet) face challenges due to their large parameter sizes and low computational efficiency, limiting their real-time applicability. To address these issues, this paper introduces Ghost-HRNet, a lightweight HPE network that integrates the efficient feature extraction capabilities of the Ghost module with the multi-scale feature fusion strengths of HRNet. By incorporating depthwise separable convolution and the convolutional block attention module (CBAM), Ghost-HRNet achieves significant reductions in parameter count and computational load while maintaining high accuracy. Experimental results on the COCO and MPII datasets demonstrate that Ghost-HRNet achieves average accuracies of 66% and 87.26%, respectively, while reducing the parameter size by 71.3% and computational load by 79.0% compared to HRNet. This combination of efficiency and accuracy makes Ghost-HRNet particularly suitable for real-time applications, underscoring its potential to advance HPE technology.},
  archive      = {J_PAAA},
  author       = {Zheng, Xiaoyu and Zhuang, Liping and Chen, Dewang},
  doi          = {10.1007/s10044-025-01440-x},
  journal      = {Pattern Analysis and Applications},
  month        = {6},
  number       = {2},
  pages        = {1-15},
  shortjournal = {Pattern Anal. Appl.},
  title        = {Ghost-HRNet: A lightweight high-resolution network for efficient human pose estimation with enhanced multi-scale feature fusion},
  volume       = {28},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Split-form nonnegative matrix factorization for high-quality local features and contour information extraction in image clustering. <em>PAAA</em>, <em>28</em>(2), 1-25. (<a href='https://doi.org/10.1007/s10044-025-01479-w'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Nonnegative matrix factorization and its variants are extensively employed for data dimensionality reduction in applications such as clustering, pattern analysis, and feature extraction. The heterogeneity of the gathered data results in the nonnegative matrix factorization method generating a significant gap between the product of the two nonnegative factor matrices and the original data matrix. This worsens the impact of data dimensionality reduction and feature extraction. To address this issue, we provide a new nonnegative matrix factorization model that integrates the split-form method. The novel approach divides the data into two components: local features information and contour information. The strategy more effectively mitigates the impact of data heterogeneity and noise on dimensionality reduction. Additionally, considering the geometric configuration of the data, we present split-form nonnegative matrix factorization methods with single-constraint and multi-constraint graph Laplacian, respectively. We theoretically validate the convergence of the algorithms and perform a complexity analysis. We employ the multiplication update technique to derive an approximate solution for the models. Finally, we conduct comparative experiments on ten real datasets to verify the effectiveness of the proposed split-form algorithms.},
  archive      = {J_PAAA},
  author       = {Liu, Qilong and Liang, Kun and An, Liyuan and Jiao, Han},
  doi          = {10.1007/s10044-025-01479-w},
  journal      = {Pattern Analysis and Applications},
  month        = {6},
  number       = {2},
  pages        = {1-25},
  shortjournal = {Pattern Anal. Appl.},
  title        = {Split-form nonnegative matrix factorization for high-quality local features and contour information extraction in image clustering},
  volume       = {28},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). FORT-RAJ: A fisheye-optimized deep learning model for real-time trajectory prediction. <em>PAAA</em>, <em>28</em>(2), 1-20. (<a href='https://doi.org/10.1007/s10044-025-01485-y'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Fisheye cameras, renowned for their ultra-wide $$180^\circ $$ panoramic field of view, have become essential tools in applications requiring extensive area monitoring. However, their unique optical properties introduce significant image distortions, rendering traditional trajectory prediction algorithms ineffective when applied to fisheye imagery. To address this challenge, we introduce Fisheye-Optimized Real-Time Trajectory prediction (FORT-RAJ), a novel hybrid artificial intelligence approach capable of both adapting to fisheye distortions and accurately predicting pedestrian trajectories in real time. From a technical perspective, FORT-RAJ combines the strengths of two state-of-the-art models: Fisheye Online Realtime Tracking (FORT), a distortion-aware model designed for fisheye cameras but limited to pedestrian tracking, and Graph- and Attention-based multi-agent Trajectory prediction model (GATraj), a powerful prediction framework not originally compatible with fisheye distortions. By integrating the fisheye adaptation capabilities of FORT with the predictive power of GATraj, FORT-RAJ overcomes their individual limitations and provides an end-to-end solution that is both distortion-aware and capable of accurate future movement prediction. The effectiveness of the proposed approach was rigorously evaluated on two distinct datasets, namely HABBOF and Caplogy. Experimental results revealed the superiority of FORT-RAJ, achieving an Average Displacement Error of 0.39 ms and 0.38 ms, and a Final Displacement Error of 0.43 ms and 0.42 ms for HABBOF and Caplogy, respectively. These results underscore the model’s high precision in predicting pedestrian trajectories, even under challenging conditions.},
  archive      = {J_PAAA},
  author       = {Bouzayane, Sarra and Kahouadji, Mouad and Magnier, Baptiste},
  doi          = {10.1007/s10044-025-01485-y},
  journal      = {Pattern Analysis and Applications},
  month        = {6},
  number       = {2},
  pages        = {1-20},
  shortjournal = {Pattern Anal. Appl.},
  title        = {FORT-RAJ: A fisheye-optimized deep learning model for real-time trajectory prediction},
  volume       = {28},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Knowledge emerging trend detection using relevance-based dynamic thin topic model. <em>PAAA</em>, <em>28</em>(2), 1-17. (<a href='https://doi.org/10.1007/s10044-025-01486-x'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Detecting knowledge emerging trends has received increasing attention. It can help researchers understand the history of the discipline and predict future research hotspots. Dynamic topic models can be used to identify knowledge emerging trends from academic papers. However, traditional dynamic topic models have some shortcomings, such as over-assumptions, insufficient topic distinction, and high computational cost. To address this problem, we propose a relevance-based dynamic thin topic model (RBDTTM). We model topic evolution with a Gaussian process and adopt a relevance-based mechanism on topic-word distributions. Under this assumption, only words relevant to a certain topic can be represented. This relevance-based mechanism can not only decrease the number of parameters to be estimated but also achieve more prominent and focused topics. We evaluate the estimation performance of RBDTTM using a series of experiments on synthetic data. Results show that RBDTTM has greater interpretability and generalization than its competitors. Finally, we take the statistics discipline as an example and apply RBDTTM to two corpora of journal articles and a Chinese graduation thesis to explore the emerging statistical knowledge trend in the past two decades.},
  archive      = {J_PAAA},
  author       = {Wang, Feifei and Yuan, Xueqiong and Lu, Xiaoling},
  doi          = {10.1007/s10044-025-01486-x},
  journal      = {Pattern Analysis and Applications},
  month        = {6},
  number       = {2},
  pages        = {1-17},
  shortjournal = {Pattern Anal. Appl.},
  title        = {Knowledge emerging trend detection using relevance-based dynamic thin topic model},
  volume       = {28},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). WintN-CSG: A weakly supervised semantic segmentation network based on basic multimodal large-scale pre-trained models. <em>PAAA</em>, <em>28</em>(2), 1-13. (<a href='https://doi.org/10.1007/s10044-025-01489-8'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Weakly supervised semantic segmentation (WSSS), training segmentation models via image-level labels, has the advantage of low manual annotation cost compared with fully supervised semantic segmentation. However, the masks generated by the currently fashionable methods based on the Class Activation Map (CAM) still have the defects of low target segmentation accuracy, many noise pixels and incorrectly activated target pixels. To handle these shortages, the paper proposes a novel WSSS integration network (denoted as WintN-CSG) by adequately fusing the merits of scalability and versatility of multimodal pre-trained basic models CLIP, SAM as well as Grounding-DINO. The superiority of this network with CLIP as a basic framework also benefits from the creative development of Attention Selection Class-aware Attention-based Affinity (AS-CAA), Box Mask Denoising (BMD), and SAM Mask Selection and Fusion (SMSF) modules. Specifically, the proposed AS-CAA can effectively select the representative attention weight maps in Multi-Head Self-Attention (MHSA) to preliminarily remove noise pixels and modify incorrectly activated pixels. Subsequently, the designed BMD combined with Grounding-DINO can shield all noise pixels outside the bounding box, and accurately refine the isolated pixels inside the bounding box, improving the integrity and accuracy of the mask. Furthermore, the deployed SMSF screens out the most suitable masks among many superior mask candidates generated by SAM and makes up for the missing target pixels with the help of fusion and activation algorithms. Finally, experiments with only image-level labels on the PASCAL VOC 2012, MS COCO 2014 and CitySpace datasets show that our scheme achieves excellent performance in the efficiency of mask generation and segmentation accuracy.},
  archive      = {J_PAAA},
  author       = {Wen, Haotian and Ding, Derui and Liang, Wei and Sun, Ying},
  doi          = {10.1007/s10044-025-01489-8},
  journal      = {Pattern Analysis and Applications},
  month        = {6},
  number       = {2},
  pages        = {1-13},
  shortjournal = {Pattern Anal. Appl.},
  title        = {WintN-CSG: A weakly supervised semantic segmentation network based on basic multimodal large-scale pre-trained models},
  volume       = {28},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Image aesthetic assessment with weighted multi-region aggregation based on information theory. <em>PAAA</em>, <em>28</em>(2), 1-15. (<a href='https://doi.org/10.1007/s10044-025-01490-1'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Image aesthetic assessment is a hot issue in current research. It will be very important to find regions that affect the aesthetic assessment of the image, for which we propose a weighted multi-region aesthetic assessment network WRMA-Net, which consists of three modules: information theory-based image segmentation module uses information theory to segment images; in the feature extraction module, we connect Convolutional Neural Network(CNN) and Graph Neural Network(GNN) in tandem, using CNN to obtain shallow detail information of the image and GNN to obtain deep semantic information of the image, which can retain feature information at each level, and subsequently fuse shallow and deep features to predict aesthetic assessment scores by the region weighting module; the weighted multi-region aggregation module assigns different weights to each region adaptively to adjust the prediction results and find high-quality aesthetic regions. The network can analyze image aesthetics from multiple regions and provide constructive regional aesthetic suggestions. The experimental results show that our WMRA-Net achieves good results in some aesthetic assessment metrics.},
  archive      = {J_PAAA},
  author       = {Wang, Yin and Guo, Jing and Ke, Yongzhen and Wang, Kai and Yang, Shuai and Chen, Liming},
  doi          = {10.1007/s10044-025-01490-1},
  journal      = {Pattern Analysis and Applications},
  month        = {6},
  number       = {2},
  pages        = {1-15},
  shortjournal = {Pattern Anal. Appl.},
  title        = {Image aesthetic assessment with weighted multi-region aggregation based on information theory},
  volume       = {28},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Plant leaf image segmentation in natural scenes: A multi-layer graph queries propagation approach. <em>PAAA</em>, <em>28</em>(1), 1-23. (<a href='https://doi.org/10.1007/s10044-024-01380-y'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Accurate leaf segmentation is crucial for optimizing plant recognition and enhancing leaf identification precision. However, leaf segmentation encounters challenges when working with images captured in natural scenes. These images often contain intricate backgrounds with soil artifacts, overlapping leaves, plant elements, shadows, and variations in lighting. To address these issues, we propose an approach for segmenting leaf images using a multi-layer graph-based propagation method. The process begins with spatial localization of the leaf, aiding in detecting the foreground template, describing the central area of the leaf. Subsequently, a multi-level decomposition of the image into homogeneous regions is accomplished to capture image details at different scales. We then construct a graph based on this structure, connecting each region to its neighbors with weighted edges based on shared areas or edges across different resolutions. This graph is used to rank regional similarities to the leaf by propagating ranking scores from the foreground template to the image boundaries. As a result, we obtain a saliency map, which is used to extract the leaf from its surroundings. Finally, the resulting binary mask is refined using random forests to achieve optimal separation between the leaf and the background. Experiments conducted on a widely used dataset demonstrate that our method outperforms several state-of-the-art segmentation methods.},
  archive      = {J_PAAA},
  author       = {Lyasmine, Adada and Idir, Filali and Samia, Bouzefrane},
  doi          = {10.1007/s10044-024-01380-y},
  journal      = {Pattern Analysis and Applications},
  month        = {3},
  number       = {1},
  pages        = {1-23},
  shortjournal = {Pattern Anal. Appl.},
  title        = {Plant leaf image segmentation in natural scenes: A multi-layer graph queries propagation approach},
  volume       = {28},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A partitioning incremental algorithm using adaptive mahalanobis fuzzy clustering and identifying the most appropriate partition. <em>PAAA</em>, <em>28</em>(1), 1-14. (<a href='https://doi.org/10.1007/s10044-024-01360-2'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper deals with the problem of determining the most appropriate number of clusters in a fuzzy Mahalanobis partition. First, a new fuzzy Mahalanobis incremental algorithm is constructed to search for an optimal fuzzy Mahalanobis partition with $$2,\,3,\ldots$$ clusters. Among these partitions, selecting the one with the most appropriate number of clusters is based on appropriately modified existing fuzzy indexes. In addition, the Fuzzy Mahalanobis Minimal Distance index is defined as a natural extension of the recently proposed Mahalanobis Minimal Distance index for non-fuzzy clustering. The new fuzzy Mahalanobis incremental algorithm was tested on several artificial data sets and the color image segmentation problems from real-world applications: art images, nature photography images, and medical images. The algorithm includes multiple usage of the global optimization algorithm DIRECT. But unlike previously known fuzzy Mahalanobis indexes, the proposed Fuzzy Mahalanobis Minimal Distance index ensures accurate results even when applied to complex real-world applications. A possible disadvantage could be the need for longer CPU time. Furthermore, besides effective identification of the partition with the most appropriate number of clusters, it is shown how to use the proposed Fuzzy Mahalanobis Minimal Distance index to search for an acceptable partition, which proved particularly useful in the above-mentioned real-world applications.},
  archive      = {J_PAAA},
  author       = {Scitovski, Rudolf and Sabo, Kristian and Grahovac, Danijel and Martínez-Álvarez, Francisco and Ungar, Sime},
  doi          = {10.1007/s10044-024-01360-2},
  journal      = {Pattern Analysis and Applications},
  month        = {3},
  number       = {1},
  pages        = {1-14},
  shortjournal = {Pattern Anal. Appl.},
  title        = {A partitioning incremental algorithm using adaptive mahalanobis fuzzy clustering and identifying the most appropriate partition},
  volume       = {28},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Adaptive optimization of low rank decomposition and its application on fabric defect detection. <em>PAAA</em>, <em>28</em>(1), 1-15. (<a href='https://doi.org/10.1007/s10044-024-01363-z'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In practical applications of fabric defect detection, low-rank decomposition is the effective method. Sparse matrices represent defect results, so sparse terms are the focus of this application. Because the characteristics of each observation matrix differ, the weight of sparse term also differ. Therefore, this paper proposes adaptive weight for the model, allowing it to find suitable weight for different observation matrices and thereby improving the accuracy of model. During the matrix separation process of the model, elements that should belong to the sparse matrix may be separated into the noise matrix. To address this, this paper establishes new constraints to achieve a deeper separation between the two. While establishing the corresponding algorithmic framework, this paper also considers the fluctuations in the model’s solution process and proposes a new definition for the penalty factors. This aims to improve algorithm efficiency and reduce CPU time. This paper also provides a convergence analysis of the proposed method. In the dataset of fabric defects, it was shown that the star and dot types had the best results in TPR and F-measure, with TPR of 85.15% and 81.56%, and f-measure of 70.51% and 65.40%, respectively. Indicating that the method proposed in this paper has the fastest calculation speed.},
  archive      = {J_PAAA},
  author       = {Shi, Wenya and Chen, Zhixiang and Liang, Jiuzhen and Jiang, Daihong},
  doi          = {10.1007/s10044-024-01363-z},
  journal      = {Pattern Analysis and Applications},
  month        = {3},
  number       = {1},
  pages        = {1-15},
  shortjournal = {Pattern Anal. Appl.},
  title        = {Adaptive optimization of low rank decomposition and its application on fabric defect detection},
  volume       = {28},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Diverse embeddings learning for multi-view clustering. <em>PAAA</em>, <em>28</em>(1), 1-12. (<a href='https://doi.org/10.1007/s10044-024-01364-y'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-view clustering, which improves clustering performance by exploring complementarity and consistency among multiple distinct feature sets, is attracting more and more researchers due to its wide applications in various fields e.g., pattern recognition and data mining. Traditional approaches usually explore above characteristics by mapping different views to a unified embedding through view-specific mapping matrices or neural networks. Then the unified embedding is fed into conventional single view clustering algorithms for final clustering results. However, a unified embedding is not enough to model distinct or even conflict multiple view characteristics due to their diverse representation abilities. Moreover, clustering and embedding learning are divided into two separate parts, which may bring in a gap between the class label and the learned embedding. To alleviate above problems, both unified and view-specific embeddings are learned, and a shared operator tensor and view-specific latent variables are introduced for their relationship modeling. Besides, a Kullback-Liebler divergence based objective is developed as a clustering oriented constraint, which leads to more clustering friendly embedding learned. Extensive experiments are conducted on six widely used datasets, achieving better results compared with several state-of-the-art approaches.},
  archive      = {J_PAAA},
  author       = {Li, Yongzhen and Liao, Husheng},
  doi          = {10.1007/s10044-024-01364-y},
  journal      = {Pattern Analysis and Applications},
  month        = {3},
  number       = {1},
  pages        = {1-12},
  shortjournal = {Pattern Anal. Appl.},
  title        = {Diverse embeddings learning for multi-view clustering},
  volume       = {28},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). SA-DETR: Saliency attention-based DETR for salient object detection. <em>PAAA</em>, <em>28</em>(1), 1-11. (<a href='https://doi.org/10.1007/s10044-024-01379-5'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Researches on the Salient Object Detection (SOD) task have made many advances based on deep learning methods. However, most methods have focused on predicting a fine mask rather than finding the most salient objects. Most datasets for the SOD task also focus on evaluating pixel-wise accuracy rather than “saliency”. In this study, we used the Salient Objects in Clutter (SOC) dataset to conduct research that focuses more on the saliency of objects. We propose a architecture that extends the cross-attention mechanism of Transformer to the DETR architecture to learn the relationship between the global image semantics and the objects. We extended module with Saliency Attention (SA) to the network, namely SA-DETR, to detect salient objects based on object-level saliency. Our proposed method with cross- and saliency-attentions shows superior results in detecting salient objects among multiple objects compared to other methods. We demonstrate the effectiveness of our proposed method by showing that it outperforms the state-of-the-art performance of the existing SOD method by 4.7% and 0.2% in MAE and mean E-measure, respectively.},
  archive      = {J_PAAA},
  author       = {Nam, Kwangwoon and Kim, Jeeheon and Kim, Heeyeon and Chung, Minyoung},
  doi          = {10.1007/s10044-024-01379-5},
  journal      = {Pattern Analysis and Applications},
  month        = {3},
  number       = {1},
  pages        = {1-11},
  shortjournal = {Pattern Anal. Appl.},
  title        = {SA-DETR: Saliency attention-based DETR for salient object detection},
  volume       = {28},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Stgcn-pad: A spatial-temporal graph convolutional network for detecting abnormal pedestrian motion patterns at grade crossings. <em>PAAA</em>, <em>28</em>(1), 1-17. (<a href='https://doi.org/10.1007/s10044-024-01382-w'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper presents a Spatial-Temporal Graph Convolutional Network-based Pedestrians’ behaviors Anomaly Detection system (STGCN-PAD) for grade crossings. The behaviors of pedestrians are represented in a structured manner by skeleton trajectories that are generated using a pose estimation model. The ST-GCN components are sequentially applied to capture the spatial dependencies between skeleton key points within a single video frame and the temporal relationships for each of them. Based on these features, the system reconstructs input trajectories with a constant sliding window size, and the reconstruction error is used to distinguish abnormal behaviors from those normal. To accelerate the processing of extracted multi-dimensional feature maps, an MLP-Mixer model-based reconstruction network is developed as an alternative to the traditional convolution neural network. Only trajectories of normal walking behavior are included for model training. Anomalies, such as lingering and squatting activities, can be identified as outliers by observing the magnitude of reconstruction errors. The case studies demonstrate the salient feasibility and efficiency of the proposed system, which achieves at least comparable performance (approximately 88% in the AUC evaluation metric) with several state-of-the-art approaches while using the MLP-Mixer model accelerates model inference by 10× relative to our previous effort (Song et al. in Appl Intell 53:21676–21691, 2023).},
  archive      = {J_PAAA},
  author       = {Song, Ge and Qian, Yu and Wang, Yi},
  doi          = {10.1007/s10044-024-01382-w},
  journal      = {Pattern Analysis and Applications},
  month        = {3},
  number       = {1},
  pages        = {1-17},
  shortjournal = {Pattern Anal. Appl.},
  title        = {Stgcn-pad: A spatial-temporal graph convolutional network for detecting abnormal pedestrian motion patterns at grade crossings},
  volume       = {28},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Appropriateness of distances in nearest neighbour classification: A monometric perspective. <em>PAAA</em>, <em>28</em>(1), 1-23. (<a href='https://doi.org/10.1007/s10044-024-01373-x'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Among the non-parametric classification methods, the nearest neighbour classifier (NNC) holds a pre-eminent position. Given a training or sample set $${\mathcal {S}}$$ the choice one needs to make is on the value of k and the distance function d to be employed. Towards improving the efficacy of an NNC, there are many works—both theoretical and empirical—that help in choosing a suitable value of k. However, works that deal with the appropriateness of a distance d for a given $${\mathcal {S}}$$ are largely empirical. In this work, we address the following two posers for a given $${\mathcal {S}}$$ : (1) How to identify a potentially appropriate distance d? (2) What qualities should an appropriate d possess? Our investigations show that every distance function d determines a landscape on the underlying data space and only if the class boundaries align with this landscape can this d be appropriate. In view of this, we construct a relational graph $${\mathcal {G}}_{{\mathcal {S}},d}$$ , in fact, a poset, on the given $${\mathcal {S}}$$ using d. With the help of $${\mathcal {G}}_{{\mathcal {S}},d}$$ , we choose a $${\mathcal {T}} \subset {\mathcal {S}}$$ to be used in a condensed-NN algorithm. Terming it the NEN algorithm, firstly, we show empirically that the training error of this NEN algorithm is reflective of the appropriateness of d. Towards providing a theoretical justification to our claims based on empiricism, we investigate the problem of classification in the setting of monometric spaces, wherein it emerges that the suitability of d is essentially related to the embeddability of $${\mathcal {G}}_{{\mathcal {S}},d}$$ in the monometric space ( $${\mathcal {X}}, \preceq _d,d$$ ).},
  archive      = {J_PAAA},
  author       = {Gupta, Megha and Jayaram, Balasubramaniam},
  doi          = {10.1007/s10044-024-01373-x},
  journal      = {Pattern Analysis and Applications},
  month        = {3},
  number       = {1},
  pages        = {1-23},
  shortjournal = {Pattern Anal. Appl.},
  title        = {Appropriateness of distances in nearest neighbour classification: A monometric perspective},
  volume       = {28},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Class preserving projections and data augmentation for appearance-based face recognition. <em>PAAA</em>, <em>28</em>(1), 1-12. (<a href='https://doi.org/10.1007/s10044-024-01388-4'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Computer Vision and Biometrics benefit from the recent advances in Pattern Recognition and Artificial Intelligence, which tends to make model-based face recognition more efficient. Also, deep learning combined with data augmentation tends to enrich the training sets used for learning tasks. Nevertheless, face recognition still is challenging, especially because of imaging issues that occur in practice, such as changes in lighting, appearance, head posture and facial expression. In order to increase the reliability of face recognition, we propose a novel supervised appearance-based face recognition method which creates a low-dimensional orthogonal subspace that enforces the face class separability. The proposed approach uses data augmentation to mitigate the problem of training sample scarcity. Unlike most face recognition approaches, the proposed approach is capable of handling efficiently grayscale and color face images, as well as low and high-resolution face images. Moreover, proposed supervised method presents better class structure preservation than typical unsupervised approaches, and also provides better data preservation than typical supervised approaches as it obtains an orthogonal discriminating subspace that is not affected by the singularity problem that is common in such cases. Furthermore, a soft margins Support Vector Machine classifier is learnt in the low-dimensional subspace and tends to be robust to noise and outliers commonly found in practical face recognition. To validate the proposed method, an extensive set of face identification experiments was conducted on three challenging public face databases, comparing the proposed method with methods representative of the state-of-the-art. The proposed method tends to present higher recognition rates in all databases. In addition, the experiments suggest that data augmentation also plays an essential role in the appearance-based face recognition, and that the CIELAB color space (L*a*b) is generally more efficient than RGB for face recognition as it attenuates lighting variations.},
  archive      = {J_PAAA},
  author       = {Soldera, John and Scharcanski, Jacob},
  doi          = {10.1007/s10044-024-01388-4},
  journal      = {Pattern Analysis and Applications},
  month        = {3},
  number       = {1},
  pages        = {1-12},
  shortjournal = {Pattern Anal. Appl.},
  title        = {Class preserving projections and data augmentation for appearance-based face recognition},
  volume       = {28},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). EdgeFormer: Local patch-based edge detection transformer on point clouds. <em>PAAA</em>, <em>28</em>(1), 1-13. (<a href='https://doi.org/10.1007/s10044-024-01386-6'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Edge points on 3D point clouds can clearly convey 3D geometry and surface characteristics, therefore, edge detection is widely used in many vision applications with high industrial and commercial demands. However, the fine-grained edge features are difficult to detect effectively as they are generally densely distributed or exhibit small-scale surface gradients. To address this issue, we present a learning-based edge detection network, named EdgeFormer, which mainly consists of two stages. Based on the observation that spatially neighboring points tend to exhibit high correlation, forming the local underlying surface, we convert the edge detection of the entire point cloud into a point classification based on local patches. Therefore, in the first stage, we construct local patch feature descriptors that describe the local neighborhood around each point. In the second stage, we classify each point by analyzing the local patch feature descriptors generated in the first stage. Due to the conversion of the point cloud into local patches, the proposed method can effectively extract the finer details. The experimental results show that our model demonstrates competitive performance compared to six baselines.},
  archive      = {J_PAAA},
  author       = {Xie, Yifei and Tu, Zhikun and Yang, Tong and Zhang, Yuhe and Zhou, Xinyu},
  doi          = {10.1007/s10044-024-01386-6},
  journal      = {Pattern Analysis and Applications},
  month        = {3},
  number       = {1},
  pages        = {1-13},
  shortjournal = {Pattern Anal. Appl.},
  title        = {EdgeFormer: Local patch-based edge detection transformer on point clouds},
  volume       = {28},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Contrastive dual-branch network for long-tailed visual recognition. <em>PAAA</em>, <em>28</em>(1), 1-19. (<a href='https://doi.org/10.1007/s10044-024-01387-5'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Over the past decade, deep learning techniques have been widely applied to visual tasks, leading to remarkable breakthroughs. However, data in real-world scenarios often exhibit a long-tailed distribution, where head classes contain significantly more samples than tail classes. Models trained on such imbalanced data tend to bias the head classes, resulting in poor performance on tail classes. Moreover, due to the scarcity of samples in the tail classes, it is challenging for models to learn robust representations for those classes. Inspired by the success of contrastive learning in representation learning, we propose a Contrastive Dual-Branch Network (CDBN) for long-tailed visual recognition. CDBN integrates an imbalance learning branch and a contrastive learning branch to address the challenges of imbalanced data. The imbalance learning branch leverages traditional methods to address data imbalance, while the contrastive learning branch follows the principles of contrastive learning. Specifically, it uses two distinct data augmentation techniques to process the same batch of samples, generating positive sample pairs for enhanced learning. A contrastive auxiliary loss is then introduced to minimize the distance between these pairs in the normalized embedding space. Furthermore, we propose a Cumulative Fusion Strategy (CFS) to guide the model in progressively prioritizing tail classes throughout training. We conducted extensive experiments on the CIFAR10-LT, CIFAR100-LT, and ImageNet-LT datasets and compared our method with various advanced algorithms. The results demonstrate that our method substantially enhances performance across all datasets, achieving state-of-the-art results on several benchmarks. Our code is available at https://github.com/mmzbyxx/CDBN .},
  archive      = {J_PAAA},
  author       = {Miao, Jie and Zhai, Junhai and Han, Ling},
  doi          = {10.1007/s10044-024-01387-5},
  journal      = {Pattern Analysis and Applications},
  month        = {3},
  number       = {1},
  pages        = {1-19},
  shortjournal = {Pattern Anal. Appl.},
  title        = {Contrastive dual-branch network for long-tailed visual recognition},
  volume       = {28},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Exploiting optimized forgery representation space for general fake face detection. <em>PAAA</em>, <em>28</em>(1), 1-16. (<a href='https://doi.org/10.1007/s10044-024-01391-9'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Face forgery has become more realistic with deep learning in computer vision, posing a significant challenge to trustworthy face identification. Existing works have achieved considerable accuracy within the dataset by formulating the detection as a binary classification problem. These methods attempt to amplify the category differences between real and fake faces but ignore the optimization of representation space for learning the specific forgery information within samples, which results in the intra-class distribution collapse and poor generalization in unseen domains. To mitigate this issue, we propose a novel forgery detection framework that combines contrastive learning with supervised learning, named Contrastive Learning Against face Forgery (CLAF). Specifically, a dual branch learning framework is involved in extracting the consistent forgery feature distribution first. Then, we consider the similarity, variance, and covariance constraint term for the representation space, which can better preserve the specific forgery information within each sample for generalization detection. The generalization performance is confirmed on FaceForensics++, Celeb-DF, and DFDC. Extensive experiment results demonstrate the effectiveness of our framework in improving generalization.},
  archive      = {J_PAAA},
  author       = {Yang, Gaoming and Zuo, Bang and Fang, Xianjin and Zhang, Ji},
  doi          = {10.1007/s10044-024-01391-9},
  journal      = {Pattern Analysis and Applications},
  month        = {3},
  number       = {1},
  pages        = {1-16},
  shortjournal = {Pattern Anal. Appl.},
  title        = {Exploiting optimized forgery representation space for general fake face detection},
  volume       = {28},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Improving distantly supervised named entity recognition by emphasizing uncertain examples. <em>PAAA</em>, <em>28</em>(1), 1-12. (<a href='https://doi.org/10.1007/s10044-024-01392-8'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Distantly supervised named entity recognition (DS-NER) aims to acquire knowledge from noisy labels. Recently, label re-weighting and label correction based frameworks have been recognized as promising approaches for DS-NER. These methods mainly handle easy or hard examples, yet neglect the impact of uncertain examples that are predicted correctly sometimes and incorrectly some other times during optimization. In this paper, we propose UE-NER, an Uncertainty Estimation method for DS-NER, which estimates the uncertainty of training examples and emphasizes uncertain ones, thus leads to more accurate and robust performance. To enable uncertainty reasoning, we formulate DS-NER as a span-level classification problem and the variance in predicted probability of the correct class across iterations of minibatch SGD is taken as the uncertainty measure. We further design an enhanced encoder to combine the power of the named entity and other spans in the sentence to boost recognition performance. Experimental results on two benchmark datasets demonstrate the superiority of the proposed UE-NER over existing DS-NER methods.},
  archive      = {J_PAAA},
  author       = {Nie, Binling and Shao, Yiming and Wang, Yigang},
  doi          = {10.1007/s10044-024-01392-8},
  journal      = {Pattern Analysis and Applications},
  month        = {3},
  number       = {1},
  pages        = {1-12},
  shortjournal = {Pattern Anal. Appl.},
  title        = {Improving distantly supervised named entity recognition by emphasizing uncertain examples},
  volume       = {28},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Efficient multi-target vehicle trajectory prediction based on multi-scale graph convolution. <em>PAAA</em>, <em>28</em>(1), 1-17. (<a href='https://doi.org/10.1007/s10044-024-01396-4'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-target vehicle trajectory prediction holds significant importance in the field of autonomous driving. Accurate trajectory prediction can enhance the safety and efficiency of autonomous vehicles, reducing traffic accidents and congestion. However, existing methods often fall short when dealing with complex traffic scenarios. Traditional approaches typically rely on single-scale spatiotemporal feature extraction, which struggles to fully capture the complex dynamics of traffic across different temporal and spatial scales, especially in high-density traffic environments. To address these challenges, this thesis proposes a multi-target vehicle trajectory prediction method based on a Multi-Scale Graph Convolutional Network (MSGCN). This method integrates high-definition semantic maps and employs a spatiotemporal multi-head attention mechanism alongside an adaptive dynamic weighting module to achieve efficient multi-target vehicle trajectory prediction. Specifically, this thesis constructs a dynamic feature repository using vehicle subgraphs and lane subgraphs to stabilize model weight fluctuations, thereby more accurately reflecting actual traffic conditions. Experimental results on the Argoverse dataset demonstrate the effectiveness of our method. Specifically, our approach reduces the average displacement error (mADE) by 7% and enhances the final displacement error (mFDE) by 19% when compared to existing state-of-the-art models. Our code is made available at https://github.com/Garegreen/EfficientMSGCN .},
  archive      = {J_PAAA},
  author       = {Gu, Xiang and Wang, Jing and Cheng, Dengyang and Li, Chao and Huang, Qiwei},
  doi          = {10.1007/s10044-024-01396-4},
  journal      = {Pattern Analysis and Applications},
  month        = {3},
  number       = {1},
  pages        = {1-17},
  shortjournal = {Pattern Anal. Appl.},
  title        = {Efficient multi-target vehicle trajectory prediction based on multi-scale graph convolution},
  volume       = {28},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Learning learning curves. <em>PAAA</em>, <em>28</em>(1), 1-13. (<a href='https://doi.org/10.1007/s10044-024-01394-6'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Learning curves depict how a model’s expected performance changes with varying training set sizes, unlike training curves, showing a gradient-based model’s performance with respect to training epochs. Extrapolating learning curves can be useful for determining the performance gain with additional data. Parametric functions, that assume monotone behaviour of the curves, are a prevalent methodology to model and extrapolate learning curves. However, learning curves do not necessarily follow a specific parametric shape: they can have peaks, dips, and zigzag patterns. These unconventional shapes can hinder the extrapolation performance of commonly used parametric curve-fitting models. In addition, the objective functions for fitting such parametric models are non-convex, making them initialization-dependent and brittle. In response to these challenges, we propose a convex, data-driven approach that extracts information from available learning curves to guide the extrapolation of another targeted learning curve. Our method achieves this through using a learning curve database. Using the initial segment of the observed curve, we determine a group of similar curves from the database and reduce the dimensionality via Functional Principle Component Analysis FPCA. These principal components are used in a semi-parametric kernel ridge regression (SPKR) model to extrapolate targeted curves. The solution of the SPKR can be obtained analytically and does not suffer from initialization issues. To evaluate our method, we create a new database of diverse learning curves that do not always adhere to typical parametric shapes. Our method performs better than parametric non-parametric learning curve-fitting methods on this database for the learning curve extrapolation task.},
  archive      = {J_PAAA},
  author       = {Turan, O. Taylan and Tax, David M. J. and Viering, Tom J. and Loog, Marco},
  doi          = {10.1007/s10044-024-01394-6},
  journal      = {Pattern Analysis and Applications},
  month        = {3},
  number       = {1},
  pages        = {1-13},
  shortjournal = {Pattern Anal. Appl.},
  title        = {Learning learning curves},
  volume       = {28},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Semi-supervised learning with deep laplacian support vector machine. <em>PAAA</em>, <em>28</em>(1), 1-13. (<a href='https://doi.org/10.1007/s10044-024-01395-5'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep learning is a rapidly growing field that can effectively extract latent features from data and use them to make predictions based on the learned features, but most models just sum the loss of each sample without considering the relationship between samples. On the other hand, the traditional Laplacian Support Vector Machine (LapSVM) can effectively utilize samples and the relationship between samples by constructing a Laplacian graph, and performs well on semi-supervised data. In this paper, we combine LapSVM and deep learning and propose Deep Laplacian Support Vector Machine. Our approach is to first use a Deep Neural Network to extract the latent features from the image, then based on the extracted feature information and a small amount of original label information, we use LapSVM for classification, build a loss function, and finally iteratively update the two parts together. We evaluate our method on several benchmark datasets and demonstrate that it outperforms other semi-supervised learning methods.},
  archive      = {J_PAAA},
  author       = {Chen, Hangyu and Xie, Xijiong and Li, Di},
  doi          = {10.1007/s10044-024-01395-5},
  journal      = {Pattern Analysis and Applications},
  month        = {3},
  number       = {1},
  pages        = {1-13},
  shortjournal = {Pattern Anal. Appl.},
  title        = {Semi-supervised learning with deep laplacian support vector machine},
  volume       = {28},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A robust approach for outlier detection based on the ratio of number of reverse neighbors to neighbors. <em>PAAA</em>, <em>28</em>(1), 1-30. (<a href='https://doi.org/10.1007/s10044-024-01372-y'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Outlier detection is an important issue in data mining, which has a wide range of applications in medicine, economics, video search, and credit card fraud detection. Many outlier detection methods have recently been developed. Most of the existing methods act based on the distance or density. Since each of these methods has its inherent disadvantage, we proposed a method which has the advantages of both distance-based and density-based methods. The proposed method is inspired by the basic idea that outliers are usually more distant neighbors to their nearest neighbors. The proposed method consists of three different parts. Each of these parts considers the distance, density, or location of objects, and finally we reach an optimal and efficient algorithm by combining these parts. Our algorithm is based on k nearest neighbor; in addition, we also use another kind of adaptive and extended neighborhood in order to provide more accurate results. Furthermore, the proposed method is robust and has little sensitivity to changes in parameter k. Numerical experiments and comparing with well-known algorithms are performed on both synthetic and real datasets in order to prove the efficiency and robustness of the proposed method.},
  archive      = {J_PAAA},
  author       = {Heydari-Gharaei, Reza and Sharifi, Rasoul and Kashef, Shima and Nezamabadi-pour, Hossein},
  doi          = {10.1007/s10044-024-01372-y},
  journal      = {Pattern Analysis and Applications},
  month        = {3},
  number       = {1},
  pages        = {1-30},
  shortjournal = {Pattern Anal. Appl.},
  title        = {A robust approach for outlier detection based on the ratio of number of reverse neighbors to neighbors},
  volume       = {28},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). MSFM-UNET: Enhancing medical image segmentation with multi-scale and multi-view frequency fusion. <em>PAAA</em>, <em>28</em>(1), 1-15. (<a href='https://doi.org/10.1007/s10044-024-01384-8'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Medical image segmentation benefits greatly from accurate and efficient models. Although CNNs and Transformer-based models are widely regarded as foundational methods in the realm of medical image segmentation, each has inherent drawbacks: Convolutional Neural Networks (CNNs) frequently face challenges when it comes to accurately capturing long-range relationships because of their limited receptive fields. Conversely, Transformers excel at capturing long-range relationships but come with a high computational cost. To address these challenges, State Space Models (SSMs) like Mamba have emerged as a promising alternative, providing an effective method to represent long-range interactions while maintaining a linear complexity. In this study, we present the Multi-Scale and Multi-View Frequency Mamba UNet (MSFM-UNet), a model specifically designed to leverage Mamba’s unique strengths for improving medical image segmentation. Additionally, the Multi-Scale Feature Aggregation (MSFA) effectively merges the feature outputs generated by each encoder block with those from the decoder. Furthermore, the Multi-View Frequency Enhancement (MVFA) is employed to simultaneously capture global and local perspectives, combining frequency domain attributes to improve the representation of features across multiple scales. We performed a comprehensive evaluation of MSFM-UNet on four widely recognized public datasets: ISIC17, ISIC18, Synapse, and ACDC. The experimental results clearly demonstrate that MSFM-UNet outperforms the current leading models in medical image segmentation. The code is made publicly available at https://github.com/qczggaoqiang/MSFM-UNet .},
  archive      = {J_PAAA},
  author       = {Gao, Qiang and Wang, Yi and Zhou, Feiyan and Wen, Jing and Li, Yong and Fang, Bin and Chen, Peng and Du, Lan and Chen, Cunjian},
  doi          = {10.1007/s10044-024-01384-8},
  journal      = {Pattern Analysis and Applications},
  month        = {3},
  number       = {1},
  pages        = {1-15},
  shortjournal = {Pattern Anal. Appl.},
  title        = {MSFM-UNET: Enhancing medical image segmentation with multi-scale and multi-view frequency fusion},
  volume       = {28},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Latent sparse subspace learning and visual domain classification via balanced distribution alignment and Hilbert–Schmidt metric. <em>PAAA</em>, <em>28</em>(1), 1-20. (<a href='https://doi.org/10.1007/s10044-024-01390-w'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Getting machine learning (ML) to perform accurate prediction needs a sufficient number of labeled samples. However, due to the either lack or small number of labeled samples in most domains, it is often beneficial to use domain adaptation (DA) and transfer learning (TL) to leverage a related auxiliary source domain to optimize the performance on target domain. In fact, the purpose of TL and DA is to use the labeled sample information (i.e., samples and the corresponding labels) for training the classifier to categorize the unlabeled samples. In this paper, we aim to propose a novel semi-supervised transfer learning method entitled “Latent Sparse subspace learning and visual domain classification via Balanced distribution alignment and Hilbert–Schmidt metric (LSBH)”. LSBH uses the latent sparse domain transfer learning for visual adaptation (LSDT) to adapt the samples with different distributions or feature spaces across domains and prevent the creation of local common subspace for source and target domains via the simultaneous learning of latent space and sparse reconstruction. LSBH proposes a novel robust classifier which maintains performance and accuracy even when faced with variations across the source and target domains. To this end, it utilizes the following two criteria in the optimization problem: maximum mean discrepancy and Hilbert–Schmidt independence criterion to reduce the marginal and conditional distribution disparities of domains and increase the dependency between samples and labels at the classification step. LSBH obtains the optimal coefficients for the classifier, which results in the minimum error in the loss function by solving the optimization problem. Thus, the error minimizing of the loss function is a part of the optimization problem. Also, to maintain the geometric structure of data in the classification step, the neighborhood graph of samples is used. The efficiency of the proposed method has been evaluated on different visual datasets and has been compared with new and prominent methods of domain adaptation and transfer learning. The results induce the superior performance of LSBH compared to the other state-of-the-art methods in label prediction.},
  archive      = {J_PAAA},
  author       = {Noori Saray, Shiva and Balafar, Mohammad-Ali and Tahmoresnezhad, Jafar},
  doi          = {10.1007/s10044-024-01390-w},
  journal      = {Pattern Analysis and Applications},
  month        = {3},
  number       = {1},
  pages        = {1-20},
  shortjournal = {Pattern Anal. Appl.},
  title        = {Latent sparse subspace learning and visual domain classification via balanced distribution alignment and Hilbert–Schmidt metric},
  volume       = {28},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Gated normalization unit for image restoration. <em>PAAA</em>, <em>28</em>(1), 1-19. (<a href='https://doi.org/10.1007/s10044-024-01393-7'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Image restoration has been an integral part of image processing research with the goal of converting degraded images into clear ones. While some networks have achieved state-of-the-art results through architecture and module design, little attention has been paid to the adaptation of normalization methods in image restoration tasks. Normalization methods are crucial in deep learning. In this work, we attempt to combine gating mechanisms with normalization methods. Gated mechanisms are popular in feature extraction and information filtering, and combining them with normalization methods has potential for designing image restoration algorithms. Firstly, we propose a Simple Gated Attention Unit (SGAU), a block using a simple gating mechanism to validate the potential of gating mechanisms. Then, we propose a new normalization block, Gated Instance Normalization (GIN), and introduce a new normalization method, Global Response Normalization (GRN), for image restoration tasks. Both GIN and GRN combine gating mechanisms with normalization methods for feature extraction, fusion, and integration. Finally, we propose a two-stage network, Gated Normalization Network (GNNet), utilizing GIN and GRN as blocks to effectively extract and filter information. Deep separable convolutions are used in the deep layers to reduce parameters while preserving spatial information, improving local feature perception. An improved cross-stage feature fusion (ICSFF) block is used for feature information transfer between stages, and a supervised attention module (SAM) is used as input to the second stage network from the first stage output. Through various image restoration tasks, we achieve 32.93 dB PSNR on GoPro, 30.42 dB PSNR on HIDE for image deblurring, 39.94 dB PSNR on SIDD for real-world denoising, and good performance in Gaussian white noise denoising and image deraining tasks. Moreover, the GIN and GRN only generated a small number of gated weight and bias parameters, and compared to other multi-stage networks, the model size is reduced, and computational complexity is well balanced.},
  archive      = {J_PAAA},
  author       = {Wang, Qingyu and Wang, Haitao and Zang, Luyang and Jiang, Yi and Wang, Xinyao and Liu, Qiang and Huang, Dehai and Hu, Binding},
  doi          = {10.1007/s10044-024-01393-7},
  journal      = {Pattern Analysis and Applications},
  month        = {3},
  number       = {1},
  pages        = {1-19},
  shortjournal = {Pattern Anal. Appl.},
  title        = {Gated normalization unit for image restoration},
  volume       = {28},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multi-state perception consistency constraints network for person re-identification. <em>PAAA</em>, <em>28</em>(1), 1-15. (<a href='https://doi.org/10.1007/s10044-024-01398-2'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Person re-identification (Re-ID) remains challenging due to pose variations and scale changes across non-overlapping camera views. In this work, we propose a Multi-state Perception Consistency Constraints Network (MPCC-Net) that extracts discriminative and robust features for person Re-ID. MPCC-Net consists of three primary components. First, a multi-state fused backbone network processes multi-scale and multi-view information. Second, perception consistency constraints enhance feature stability. Third, partition attention modules focus on different body parts to improve local discrimination. Comprehensive experiments on benchmark datasets demonstrate MPCC-Net’s competitive performance, effectively addressing pose and scale variations for accurate person Re-ID. Our source code will also be publicly available at: https://github.com/sesamecandy/MPCC-Net},
  archive      = {J_PAAA},
  author       = {Zhou, Mengting and Lian, Guoyun and Ouyang, Xinyu and Du, Jingyu and Song, Qiqi and Yang, Jinfeng},
  doi          = {10.1007/s10044-024-01398-2},
  journal      = {Pattern Analysis and Applications},
  month        = {3},
  number       = {1},
  pages        = {1-15},
  shortjournal = {Pattern Anal. Appl.},
  title        = {Multi-state perception consistency constraints network for person re-identification},
  volume       = {28},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Self-weighted subspace clustering via adaptive rank constrained graph embedding. <em>PAAA</em>, <em>28</em>(1), 1-22. (<a href='https://doi.org/10.1007/s10044-024-01405-6'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent years, subspace clustering methods have attracted wide attention in partitioning high-dimensional data from a union of underlying subspaces, in which the data distribution is mainly explored to compensate for the absence of label information. However, for practical applications, subspace clustering still suffers from redundant and noisy features, which brings about disturbed reconstruction loss and restricts trustworthy graph learning. In this paper, we propose a robust subspace clustering framework via Self-weighted feature learning and adaptive rank constrained graph embedding (SWARG) to address the limitations of existing graph-based subspace clustering models. Specifically, a feature self-weighted learning term is introduced to the sparse subspace clustering framework to alleviate the disturbed contributions from the noisy and redundant features. As such, a few discriminative features will act as remarkable contributions in representing data samples. Meanwhile, the profile-based graph embedding term further preserving the contribution behavior information of data samples that distributed around the same subspace. Moreover, the adaptive rank-constraint graph embedding method is considered to guarantee discriminative structure for different components of representation matrix with flexible entropy-based similarity preserving. To solve the proposed model, we then develop an efficient alternative direction updating algorithm, together with convergence and complexity analysis. Finally, experimental results on toy databases and benchmark databases demonstrate the effectiveness of the proposed SWARG model compared to a series of state-of-the-art models. Our code is available at http://github.com/ty-kj/SAWRG .},
  archive      = {J_PAAA},
  author       = {Jiang, Kun and Yang, Zhihai and Sun, Qindong},
  doi          = {10.1007/s10044-024-01405-6},
  journal      = {Pattern Analysis and Applications},
  month        = {3},
  number       = {1},
  pages        = {1-22},
  shortjournal = {Pattern Anal. Appl.},
  title        = {Self-weighted subspace clustering via adaptive rank constrained graph embedding},
  volume       = {28},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). ST-HViT: Spatial-temporal hierarchical vision transformer for action recognition. <em>PAAA</em>, <em>28</em>(1), 1-13. (<a href='https://doi.org/10.1007/s10044-024-01407-4'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Human action recognition (HAR) is an important task in the field of computer vision, and its primary goal is to analyze and understand human activities in videos. In addition to containing the spatial information of static images, videos also contain unique temporal information, which makes the information contained in the videos even richer. However, the training cost required to fully learn the spatial-temporal information of the videos is quite expensive for a model. In light of this, we propose a novel two-stream network structure to effectively capture the spatial-temporal information in video data. We perform masked autoencoders (MAE) pre-training, aiming to reduce the training burden of the model through this asymmetric encoder-decoder pre-training method. In addition, we propose a new multi-scale decoder component that combines transposed convolutional upsampling and convolutional downsampling. It fully utilizes the multi-scale features of the encoder to achieve excellent performance. On two challenging video datasets, Kinetics 400 (K400) and Something-Something-v2 (SSv2), we achieve state-of-the-art performance with 85.9 $$\%$$ and 75.3 $$\%$$ Top-1 accuracy, respectively.},
  archive      = {J_PAAA},
  author       = {Xia, Limin and Fu, Weiye},
  doi          = {10.1007/s10044-024-01407-4},
  journal      = {Pattern Analysis and Applications},
  month        = {3},
  number       = {1},
  pages        = {1-13},
  shortjournal = {Pattern Anal. Appl.},
  title        = {ST-HViT: Spatial-temporal hierarchical vision transformer for action recognition},
  volume       = {28},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Bidirectional spatio-temporal generative adversarial network for video super-resolution. <em>PAAA</em>, <em>28</em>(1), 1-11. (<a href='https://doi.org/10.1007/s10044-024-01409-2'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Adversarial and periodic training method plays an essential role in video super-resolution, which can generate spatial high frequency detail and temporal consistency relation. However, this approach is based on unidirectional loop whose first frame can only use its own feature information, while the last frame can use all feature information of whole sequence. The biggest problem caused by this information imbalance is that early video frames are poorly reconstructed. To address these issues, we propose a novel video super-resolution model, called Bidirectional Spatio-Temporal Generative Adversarial Network (Bi-STGAN), to generate fine detail and temporal consistency video by explicitly introducing the backward branch. Specifically, Bi-STGAN adopts an elaborately designed bidirectional branch structure so that the high-resolution frames estimated from front to back can be used as input for subsequent iterations from back to front. The advantage of Bi-STGAN is to enhance information gathering by utilizing information from past and future frames which can be cyclically passed through the time series. The experimental results show that compared with the baselines, Bi-STGAN achieves competitive improvement of 15.20% for LPIPS and 2.87dB for PSNR on the REDS4 dataset, thereby demonstrating the superiority of our state-of-the-art model on video super-resolution.},
  archive      = {J_PAAA},
  author       = {Yang, Peng and Chen, Zhangquan and Sun, Yuankang and Hu, Zhongjian and Li, Bing},
  doi          = {10.1007/s10044-024-01409-2},
  journal      = {Pattern Analysis and Applications},
  month        = {3},
  number       = {1},
  pages        = {1-11},
  shortjournal = {Pattern Anal. Appl.},
  title        = {Bidirectional spatio-temporal generative adversarial network for video super-resolution},
  volume       = {28},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Balanced clustering contrastive learning for long-tailed visual recognition. <em>PAAA</em>, <em>28</em>(1), 1-11. (<a href='https://doi.org/10.1007/s10044-025-01410-3'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Real-world deep learning training data often follow a long-tailed (LT) distribution, where a few classes (head classes) have the most samples and many classes (tail classes) have very few samples. Models trained on LT datasets typically achieve high accuracy on head classes, but suffer from poor performance on tail classes. To address this challenge, strategies based on supervised contrastive learning have been explored. However, existing methods often focus on either reducing the dominance of head class features or expanding the feature space of tail classes, but rarely achieve a balanced feature distribution across both. In this paper, we propose Balanced clustering contrastive learning (BCCL) to balance the feature space between the head and tail classes more effectively. The proposed approach introduces two main components. First, we employ queue-based clustering to extract multiple centroids. This addresses the intra-minibatch class absence issue and maintains intra-class balance. Second, we expand the feature space of tail classes based on class frequency to enhance their expressiveness. An evaluation of four LT datasets, CIFAR-10-LT, CIFAR-100-LT, ImageNet-LT, and iNaturalist 2018, demonstrates that BCCL consistently outperforms the existing methods. These results establish the ability of BCCL to maintain a balanced feature space in diverse environments. Our code is available at https://github.com/GGTINE/BCCL .},
  archive      = {J_PAAA},
  author       = {Kim, Byeong-il and Ko, Byoung Chul},
  doi          = {10.1007/s10044-025-01410-3},
  journal      = {Pattern Analysis and Applications},
  month        = {3},
  number       = {1},
  pages        = {1-11},
  shortjournal = {Pattern Anal. Appl.},
  title        = {Balanced clustering contrastive learning for long-tailed visual recognition},
  volume       = {28},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Separability and scatteredness (S&S) ratio-based efficient SVM regularization parameter, kernel, and kernel parameter selection. <em>PAAA</em>, <em>28</em>(1), 1-13. (<a href='https://doi.org/10.1007/s10044-025-01411-2'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Support Vector Machine (SVM) is a robust machine learning algorithm with broad applications in classification, regression, and outlier detection. SVM requires tuning a regularization parameter (RP) which controls the model capacity and the generalization performance. Conventionally, the optimum RP is found by comparison of a range of values through the Cross-Validation (CV) procedure. In addition, for non-linearly separable data, the SVM uses kernels. In this case a set of kernels, each with a set of parameters, denoted as a grid of kernels, are considered. The optimal choice of RP and the grid of kernels is through various forms of deterministic or probabilistic grid-search. The existing methods rely heavily on exhaustive searches and provide very limited insight into the underlying data characteristics, resulting in excessive computational complexity. This work addresses this issue by proposing a statistical framework that directly relates the dataset’s separability and scatteredness to the choice of optimal hyperparameters. By stochastically analyzing the behavior of the regularization parameter, the method shows that the SVM performance can be modeled as a function of the newly defined separability and scatteredness (S&S) ratio of the data. The Separability is a measure of the distance between classes, and the scatteredness is the ratio of the spread of data points. In particular, for the hinge loss cost function, an S&S ratio-based table provides the optimum RP. The data S&S ratio is a powerful value that can automatically evaluate linear or non-linear separability before using the SVM algorithm. The provided lookup S&S ratio-based table can also provide the optimum kernel and its parameters before using the SVM algorithm. Consequently, the computational complexity of the CV grid-search is reduced to only the computational complexity of one-time use of the SVM. The simulation results on the real dataset confirm the superiority of the proposed approach in the sense of efficiency and computational complexity over the grid-search methods. The method performs better or comparable to the existing state of-the-art methods with a significantly reduced computational cost.},
  archive      = {J_PAAA},
  author       = {Shamsi, Mahdi and Beheshti, Soosan},
  doi          = {10.1007/s10044-025-01411-2},
  journal      = {Pattern Analysis and Applications},
  month        = {3},
  number       = {1},
  pages        = {1-13},
  shortjournal = {Pattern Anal. Appl.},
  title        = {Separability and scatteredness (S&S) ratio-based efficient SVM regularization parameter, kernel, and kernel parameter selection},
  volume       = {28},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Ultra-fast computation of fractal dimension for RGB images. <em>PAAA</em>, <em>28</em>(1), 1-22. (<a href='https://doi.org/10.1007/s10044-025-01415-y'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The fractal dimension (FD) is a quantitative parameter widely used to analyze digital images in many application fields such as image segmentation, feature extraction, object recognition, texture analysis, and image compression and denoising, among many others. A variety of algorithms have been previously proposed for estimating the FD, however most of them are limited to binary or gray-scale images only. In recent years, several authors have proposed algorithms for computing the FD of color images. Nevertheless, almost all these methods are computationally inefficient when analyzing large images. Nowadays, color images can be very large in size, and there is a growing trend toward even larger datasets. This implies that the time required to calculate the FD of such datasets can become extremely long. In this paper we present a very efficient GPU algorithm, implemented in CUDA, for computing the FD of RGB color images. Our solution is an extension to RGB of the differential box-counting (DBC) algorithm for gray-scale images. Our implementation simplifies the box-counting computation to very simple operations which are easily combined across iterations. We evaluated our algorithm on two distinct hardware/software platforms using a set of images of increasing size. The performance of our method was compared against two recent FD algorithms for RGB images: a fast box-merging GPU algorithm, and the most advanced approach based on extending the DBC method. The results showed that our GPU algorithm performed very well and achieved speedups of up to 7.9× and 6172.6× regarding these algorithms, respectively. In addition, our algorithm achieved average error rates similar to those obtained by the two reference algorithms when estimating the FD for synthetic images with known FD values, and even outperformed them when processing large images. These results suggest that our GPU algorithm offers a highly reliable and ultra-fast solution for estimating the FD of color images.},
  archive      = {J_PAAA},
  author       = {Ruiz de Miras, Juan and Li, Yurong and León, Alejandro and Arroyo, Germán and López, Luis and Torres, Juan Carlos and Martín, Domingo},
  doi          = {10.1007/s10044-025-01415-y},
  journal      = {Pattern Analysis and Applications},
  month        = {3},
  number       = {1},
  pages        = {1-22},
  shortjournal = {Pattern Anal. Appl.},
  title        = {Ultra-fast computation of fractal dimension for RGB images},
  volume       = {28},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). DE-NAF: Decoupled neural attenuation fields for sparse-view CBCT reconstruction. <em>PAAA</em>, <em>28</em>(1), 1-16. (<a href='https://doi.org/10.1007/s10044-025-01416-x'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cone-beam computed tomography (CBCT) acquires three-dimensional internal images, particularly effective for high-mineral density structures like bones. However, especially in sparse view scenarios, its ability to visualize low-density soft tissues is limited, restricting its clinical applications. To address this problem, this study proposes a method called Decoupled Neural Attenuation Fields (DE-NAF). Specifically, DE-NAF utilizes an Adaptive Hybrid Encoder that includes both hash encoding and 3D feature grid encoding methods. This approach decouples the CBCT reconstruction into two components. Hash encoding is used for high-mineral density structures, such as bones, owing to its superior encoding quality and ability to retain features of high-mineral density structures during hash conflict resolution. The 3D feature grid is employed for low-density soft tissues, such as muscles, as it effectively preserves the feature information of low-density soft tissues. The Adaptive Hybrid Encoder extracts these features, which are then decoded by a multilayer perceptron (MLP) decoder to predict X-ray attenuation values for precise reconstruction. In addition, a loss of structural perception was introduced to enhance tissue contrast and detail, further aiding CBCT reconstruction. Extensive experiments demonstrated that DE-NAF effectively addresses the limitations of CBCT in imaging low-density soft tissues, maintaining complete structural integrity and exceeding other methods in reconstruction quality.},
  archive      = {J_PAAA},
  author       = {Zhao, Tianning and Ding, Guoping and Liu, Zhenyang and Hu, Peng and Wei, Hangping and Tan, Min and Ding, Jiajun},
  doi          = {10.1007/s10044-025-01416-x},
  journal      = {Pattern Analysis and Applications},
  month        = {3},
  number       = {1},
  pages        = {1-16},
  shortjournal = {Pattern Anal. Appl.},
  title        = {DE-NAF: Decoupled neural attenuation fields for sparse-view CBCT reconstruction},
  volume       = {28},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multi-task model with attribute-specific heads for person re-identification. <em>PAAA</em>, <em>28</em>(1), 1-13. (<a href='https://doi.org/10.1007/s10044-025-01421-0'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Person re-identification (ReID) has become an important task in digital surveillance for enhancing security, efficient monitoring, and enabling various applications in smart cities and public safety systems. Person ReID with attributes is a challenging task due to different camera views create significant difficulties in capturing each person’s unique identity and detailed attributes. In this work, we propose a multi-task model that not only performs unique person ReID but also simultaneously predicts attributes. Our model jointly utilizes a shared backbone network, which can be either ResNet50 or EfficientNet, along with generalized mean (GeM) pooling to achieve efficient feature extraction. It also applies attribute-specific heads to predict various characteristics such as gender, age, type of clothes, color, and alongside the ReID classification. This multi-task approach utilizes the shared features across tasks, gives comprehensive attribute predictions, and may further contribute to identification in surveillance scenarios. We evaluate our model on two commonly used publicly available datasets, Market1501 and DukeMTMC-reID, demonstrating how our approach can improve both in ReID accuracy and give reliable attribute predictions. These results reveal that our multi-task model can be competitive, providing a holistic solution for practical applications in surveillance where both identification and attributes are important. The approach has shown the potential of unifying ReID with attribute prediction to develop more robust and advanced surveillance systems. The code of this experiment is publicly accessible at https://github.com/TripleTheGreatDali/ReIDMTMASH .},
  archive      = {J_PAAA},
  author       = {Ahmed, Md Foysal and Oyshee, Adiba An Nur},
  doi          = {10.1007/s10044-025-01421-0},
  journal      = {Pattern Analysis and Applications},
  month        = {3},
  number       = {1},
  pages        = {1-13},
  shortjournal = {Pattern Anal. Appl.},
  title        = {Multi-task model with attribute-specific heads for person re-identification},
  volume       = {28},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A new method for tuning the CNN pre-trained models as a feature extractor for malware detection. <em>PAAA</em>, <em>28</em>(1), 1-19. (<a href='https://doi.org/10.1007/s10044-024-01381-x'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Despite significant advancements in Android malware detection, current approaches face notable challenges, particularly in handling obfuscation techniques, achieving high detection accuracy, and maintaining computational efficiency. Traditional static and dynamic analysis methods often struggle with evolving malware tactics and providing lightweight execution, which necessitates models that can dynamically adapt to these challenges. To address these needs, this study presents TuneDroid, a novel approach that optimizes CNN model configurations for both improved detection rates and resilience to obfuscation. By leveraging image-based visualization of code, TuneDroid enables CNNs to recognize high-level visual patterns that remain consistent even with code modifications, thereby enhancing robustness against common evasion tactics. TuneDroid utilizes Bayesian optimization for dynamically tuning pre-trained Convolutional Neural Network (CNN) models. This optimization process selects optimal pre-trained models, layer configurations, and positions, significantly enhancing detection performance. Using a dataset of 3000 benign and 3000 malicious apps, where DEX code is converted into images, TuneDroid achieved accuracy rates of 99.44% on the validation set and 98.00% on the testing set. In comparison, static end-to-end models without hyperparameter tuning yielded lower accuracies, not exceeding 90.50% and 91.17%. The robustness of TuneDroid’s performance is demonstrated through extensive experiments, including precision, recall, F1-score, and comparisons with baseline models. These results highlight the importance of dynamic tuning in maximizing the effectiveness of CNN-based malware detection. This work stands out by focusing on the dynamic tuning of deep learning models for Android app security, demonstrating substantial improvements in detection accuracy and showcasing the potential of Bayesian optimization in this context.},
  archive      = {J_PAAA},
  author       = {Bakır, Halit},
  doi          = {10.1007/s10044-024-01381-x},
  journal      = {Pattern Analysis and Applications},
  month        = {3},
  number       = {1},
  pages        = {1-19},
  shortjournal = {Pattern Anal. Appl.},
  title        = {A new method for tuning the CNN pre-trained models as a feature extractor for malware detection},
  volume       = {28},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). MSFFNet: Multi-scale feature fusion network with semantic optimization for crowd counting. <em>PAAA</em>, <em>28</em>(1), 1-16. (<a href='https://doi.org/10.1007/s10044-024-01385-7'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, the crowd-counting system has strong strength to ensure safety in public places with the practical necessity of dense crowd analysis. However, until now, it has been difficult to obtain high-quality density maps due to complex background interference, congested crowd and large-scale variations. To address this issue, this paper presents a multi-scale feature fusion network (MSFFNet) based on CNN (convolution neural network), which is capable of detecting enough semantic features to understand crowds in sparse and highly congested scenes. In this method, a large majority of encoded features are fused adaptively rather than separated extraction components. Therefore, it enhances the ability to extend the range of receptive field sizes and reduce computation cost. MSSFNet is consists of three modules: grouped feature extractor, fusion block and decoder. The feature extractor is based on first 13 convolution layers of VGG16, which extract low-level features from crowd images. The fusion block computes the weights in each group from the contrast features and average them from convolutional layers later concatenated pooling layer with a feature map. The decoder capably extracts relevant information while enduring spatial resolution. Additionally, we designed two-stream module and semantic optimization module (SOM) with decoder which instantaneously enhance the crowd head positions and reduce background noises by re-weighting features. Extensive experiments on four public datasets (ShanghaiTech Part_A and Part_B, UCF_CC_50, UCF_QNRF and JHU-Crowd++), validate that MSFFNet can perform efficiently in complex background noises and capture head sizes in sparse, congested and various weather situation.},
  archive      = {J_PAAA},
  author       = {Rohra, Avinash and Yin, Baoqun and Bilal, Hazrat and Kumar, Aakash and Ali, Munawar and Li, Yang},
  doi          = {10.1007/s10044-024-01385-7},
  journal      = {Pattern Analysis and Applications},
  month        = {3},
  number       = {1},
  pages        = {1-16},
  shortjournal = {Pattern Anal. Appl.},
  title        = {MSFFNet: Multi-scale feature fusion network with semantic optimization for crowd counting},
  volume       = {28},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A lightweight multidimensional feature network for small object detection on UAVs. <em>PAAA</em>, <em>28</em>(1), 1-24. (<a href='https://doi.org/10.1007/s10044-024-01389-3'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {UAV small object detection has essential applications in the military, search and rescue, and smart cities, providing critical support for target recognition in complex environments. However, the existing UAV small object detection models usually have many parameters and high computational complexity, limiting their deployment and application in practical scenarios to some extent. In this study, we propose a UAV detector with Lightweight Multidimensional Feature Network (LMF-UAV), aiming to reduce the number of parameters and computation of the model while guaranteeing accuracy, which constructs the Lightweight Multidimensional Feature Network (LMF-Net) for lightweight feature extraction, and Efficient Expressive Network (EENet) for efficient feature fusion. Neural architecture search utilizes the Dual-branch Cross-stage Universal Inverted Bottleneck to enable the network to select the most suitable structure at different layers according to requirements, thereby improving the computational efficiency of LMF-Net while maintaining performance. EENet uses the Channel-wise Partial Convolution Stage to reduce redundant computation and memory access and fuse spatial features more effectively. First, LMF-Net extracts features from the images collected by UAV and obtains three multi-scale feature maps. Second, EENet performs feature fusion on three feature maps of different scales to obtain three feature representatives. Finally, the decoupled head detects the feature map and outputs the final result. The bounding box regression loss function uses Wasserstein distance to evaluate box similarity and enhance the model’s sensitivity to small targets. The experimental results demonstrate that on the VisDrone dataset, mAP50-95 of LMF-UAV reaches 24.6%, while parameters are only 14.7M, FLOPs are only 61.8G, showing a good balance between performance and efficiency.},
  archive      = {J_PAAA},
  author       = {Yang, Wenyuan and He, Qihan and Li, Zhongxu},
  doi          = {10.1007/s10044-024-01389-3},
  journal      = {Pattern Analysis and Applications},
  month        = {3},
  number       = {1},
  pages        = {1-24},
  shortjournal = {Pattern Anal. Appl.},
  title        = {A lightweight multidimensional feature network for small object detection on UAVs},
  volume       = {28},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Utilize trajectory information for small target classification. <em>PAAA</em>, <em>28</em>(1), 1-9. (<a href='https://doi.org/10.1007/s10044-024-01397-3'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Small target recognition has always been challenging in image processing systems. When targets are far from the camera, target features tend to have low quality, limiting the amount of useful information for detection systems. Consequently, classic Detection Before Tracking (DBT) algorithms find great difficulty in separating targets from their background based on their visual properties. In this study, we proposed a Track Before Detect (TBD) approach that tracks potential targets in multiple frames, reducing the false alarm rate and enhancing the detection robustness to clutter. Then, we utilize target trajectory information to distinguish actual targets from any background noise. The proposed approach reframes the classic target image classification challenge to a multivariate time series classification problem, using target trajectory coordinates (x, y) as features. The proposed approach achieved a remarkable 97% accuracy in classifying targets from noise using only ten data points (half a second of tracking). Furthermore, it successfully classified targets into specific categories (airplane, drone, bird) with a 96% accuracy rate over a 1.5 s window (30 data points).},
  archive      = {J_PAAA},
  author       = {Alkentar, Saad and Assalem, Abdulkarim and Alsahwa, Bassem},
  doi          = {10.1007/s10044-024-01397-3},
  journal      = {Pattern Analysis and Applications},
  month        = {3},
  number       = {1},
  pages        = {1-9},
  shortjournal = {Pattern Anal. Appl.},
  title        = {Utilize trajectory information for small target classification},
  volume       = {28},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). An improved nearest neighbour classifier. <em>PAAA</em>, <em>28</em>(1), 1-16. (<a href='https://doi.org/10.1007/s10044-024-01399-1'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A windowed version of the Nearest Neighbour (WNN) classifier for images is described. While its construction is inspired by the architecture of Artificial Neural Networks, the underlying theoretical framework is based on approximation theory. We illustrate WNN on the datasets MNIST and EMNIST of images of handwritten digits. In order to calibrate the parameters of WNN, we first study it on MNIST. We then apply WNN with these parameters to EMNIST resulting in an error rate of 0.76% which significantly outperforms traditional classification methods like Support Vector Machines. By expansions of the training set, an error rate down to 0.42% is achieved.},
  archive      = {J_PAAA},
  author       = {Setterqvist, Eric and Kruglyak, Natan and Forchheimer, Robert},
  doi          = {10.1007/s10044-024-01399-1},
  journal      = {Pattern Analysis and Applications},
  month        = {3},
  number       = {1},
  pages        = {1-16},
  shortjournal = {Pattern Anal. Appl.},
  title        = {An improved nearest neighbour classifier},
  volume       = {28},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Texture discrimination via hilbert curve path based information quantifiers. <em>PAAA</em>, <em>28</em>(1), 1-18. (<a href='https://doi.org/10.1007/s10044-024-01400-x'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The analysis of the spatial arrangement of colors and roughness/smoothness of figures is relevant due to its wide range of applications. This paper proposes a texture characterization method that extracts data from images using the Hilbert curve. Three information theory quantifiers are then computed: permutation entropy, permutation complexity, and Fisher information measure. The proposal exhibits some important properties: (i) it allows discrimination between figures according to varying degrees of correlations (as measured by the Hurst exponent), (ii) it is invariant to rotation and symmetry transformations, (iii) it is invariant to image scaling, (iv) it can be used for both black and white and color images. Validations have been performed not only using synthetic images but also using the well-known Brodatz image database.},
  archive      = {J_PAAA},
  author       = {Bariviera, Aurelio F. and Hansen, Roberta and Pastor, Verónica E.},
  doi          = {10.1007/s10044-024-01400-x},
  journal      = {Pattern Analysis and Applications},
  month        = {3},
  number       = {1},
  pages        = {1-18},
  shortjournal = {Pattern Anal. Appl.},
  title        = {Texture discrimination via hilbert curve path based information quantifiers},
  volume       = {28},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A light-weight backbone to adapt with extracting grouped dilation features. <em>PAAA</em>, <em>28</em>(1), 1-16. (<a href='https://doi.org/10.1007/s10044-024-01401-w'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Addressing grouped dilation features (GDFs) improved the learning ability of MobileNetV1 in image representation. However, the computational complexity is still at a high level, while the performance is a modest degree. This expensive cost is principally caused by the backbone of MobileNetV1 taking deep feature maps in several latest layers. To mitigate these issues, we propose a light-weight network (called CGDF-Net) with an adaptative architecture to effectively extract grouped dilation features. CGDF-Net is structured by two main contributions: (i) Its backbone is improved by simply replacing several latest layers of MobileNetV1 with a pointwise convolutional layer for reducing the computational complexity; (ii) Embedding an attention mechanism into the GDF block to form a completed GDF perceptron (CGDF) that directs the learning process into the significant properties of objects in images instead of the trivial ones. Experimental results on benchmark datasets for image recognition have validated that the proposed CGDF-Net network obtained good performance with a small computational cost in comparison with MobileNets and other light-weight models. For instance, CGDF-Net obtained 60.86% with 3.53M learnable parameters on Stanford Dogs, up to 6% better than MoblieNetV1-GDF (54.9%, 3.39M) and 9% versus MoblieNetV1 (51.6%, 3.33M). Meantime, the performance of CGDF-Net on ImageNet-100 is 85.22%, about 6% $$\sim$$ 8% higher than MobileNetV1-GDF’s (79.14%) and MobileNetV1’s (77.01%), respectively. The code of CGDF-Net is available at https://github.com/nttbdrk25/CGDFNet .},
  archive      = {J_PAAA},
  author       = {Nguyen, Thanh Tuan and Pham, Hoang Anh and Nguyen, Thanh Phuong},
  doi          = {10.1007/s10044-024-01401-w},
  journal      = {Pattern Analysis and Applications},
  month        = {3},
  number       = {1},
  pages        = {1-16},
  shortjournal = {Pattern Anal. Appl.},
  title        = {A light-weight backbone to adapt with extracting grouped dilation features},
  volume       = {28},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). IMIHCT: Improved multi-stage image inpainting with hybrid CNN and transformer. <em>PAAA</em>, <em>28</em>(1), 1-14. (<a href='https://doi.org/10.1007/s10044-024-01402-9'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Aiming at problems such as the poor effect of large-hole image inpainting, the limitation of local information reconstruction of the convolutional neural network, and a surge in parameters caused by stacking a large number of convolutional modules in the network model. We make full use of the advantages of convolutional neural network and transformer and propose an improved multi-stage inpainting method with hybrid CNN and transformer. The method achieves a balance between performance and parameters. Specifically, rough results are first generated using a coarse inpainting network with skip connections and lightweight Taylor transformer modules. Then, a local refinement network with coordinate attention is used to perform local refinement, optimizing local details while weakening the influence of unreasonable content in the distance. Finally, to compensate for the inability of local refinement networks to refine the overall pattern over long distances, global refinement is performed using a network that is consistent with the structure of the coarse inpainting network to make the reconstructed image more realistic and natural. Results show that the method outperforms the state of the arts on three publicly available datasets. The code is made available at https://github.com/Sheeran2000/IMIHCT .},
  archive      = {J_PAAA},
  author       = {Ning, Tao and Wang, Xingfang and Ding, Hongwei},
  doi          = {10.1007/s10044-024-01402-9},
  journal      = {Pattern Analysis and Applications},
  month        = {3},
  number       = {1},
  pages        = {1-14},
  shortjournal = {Pattern Anal. Appl.},
  title        = {IMIHCT: Improved multi-stage image inpainting with hybrid CNN and transformer},
  volume       = {28},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). UMAM-NET: Ultrasound thyroid nodule malignancy grading network based on multi-subnet attention mechanism. <em>PAAA</em>, <em>28</em>(1), 1-16. (<a href='https://doi.org/10.1007/s10044-024-01404-7'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Background and Objectives: Thyroid nodules are one of the most common thyroid diseases, and their incidence has been on the rise in recent years. Ultrasound imaging, due to its low cost and no ionizing radiation, has become the preferred method for imaging thyroid nodules. Accurate assessment of the malignancy grade of thyroid nodules is crucial to ensure the accuracy of subsequent examination and treatment. Texture and shape are key features for determining the nature of thyroid nodules. Despite the excellent performance of convolutional neural networks (CNNs) in image feature extraction and aggregation, the low resolution and high noise characteristics of ultrasound images still pose challenges for existing CNN models in identifying texture and shape. Methods: To address this challenge, we propose a thyroid nodule malignancy grading network based on a multi-subnet attention mechanism (UMAM-NET). In the feature extraction stage, we innovatively introduce the multi-subnet attention module. The module designs two parallel subnets, aiming to enhance the model’s attention to the texture and shape of thyroid nodules. Results: Compared to other deep learning models, the proposed UMAM-NET performs better in the malignant grading task of thyroid nodules. It demonstrates excellent performance on public datasets, achieving the best results in Recall (93.1%), F1-score (95.4%), and Accuracy (98.4%). Similarly, it also shows outstanding performance on the sub-collected dataset, with Recall (91.8%), F1-score (92.0%), and Accuracy (94.4%). Conclusion: Our proposed UMAM-NET, based on multi-subnet attention mechanism, provides a promising approach for accurate assessment of thyroid nodule malignancy grade, which can be of great value in clinical practice.},
  archive      = {J_PAAA},
  author       = {Bi, Hui and Wang, Fan and Xiong, YuHao and Dong, ZhaoHui and Jiang, Yibo and Zhao, Tong and Zheng, Yineng},
  doi          = {10.1007/s10044-024-01404-7},
  journal      = {Pattern Analysis and Applications},
  month        = {3},
  number       = {1},
  pages        = {1-16},
  shortjournal = {Pattern Anal. Appl.},
  title        = {UMAM-NET: Ultrasound thyroid nodule malignancy grading network based on multi-subnet attention mechanism},
  volume       = {28},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). PSFE-YOLO: A traffic sign detection algorithm with pixel-wise spatial feature enhancement. <em>PAAA</em>, <em>28</em>(1), 1-15. (<a href='https://doi.org/10.1007/s10044-024-01406-5'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Traffic sign detection is essentially important for intelligent driving. Existing detection algorithms typically incorporate self-attention mechanisms to model the dependencies among image elements, such as patches or pixels. When using a patch as a token, the positional information within the patch could be lost. When using a pixel as a token, an increase in the number of tokens can lead to a significant increase in computational complexity. To balance these two extreme situations, a pixel only needs to focus on pixels from the surrounding area. Therefore, we propose a local attention module termed Pixel-wise Spatial Feature Enhancement (PSFE), which uses pixels as tokens to enhance the spatial information of feature maps, and each pixel’s self-attention only acts on a local region to reduce computational complexity. Furthermore, we design a Bidirectional Res2Net (BR) module that generates multiple feature maps with different channel numbers from an input feature map, and then restores them to one feature map with the original input size through bidirectional fusion, greatly enriching the receptive field information contained in the feature map. We conducted experiments on the GTSDB, TT100K, and CCTSDB 2021 datasets to comprehensively evaluate our method, and the experimental results showed that our method has superior performance.},
  archive      = {J_PAAA},
  author       = {Zhang, Jianming and Wang, Zulou and Yi, Yao and Kuang, Li-Dan and Zhang, Jin},
  doi          = {10.1007/s10044-024-01406-5},
  journal      = {Pattern Analysis and Applications},
  month        = {3},
  number       = {1},
  pages        = {1-15},
  shortjournal = {Pattern Anal. Appl.},
  title        = {PSFE-YOLO: A traffic sign detection algorithm with pixel-wise spatial feature enhancement},
  volume       = {28},
  year         = {2025},
}
</textarea>
</details></li>
</ul>

</body>
</html>
