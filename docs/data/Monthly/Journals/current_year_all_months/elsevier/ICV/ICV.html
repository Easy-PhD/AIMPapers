<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>ICV</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="icv">ICV - 6</h2>
<ul>
<li><details>
<summary>
(2026). ABENet: Attention-based bidirectional enhancement network for collaborative camouflaged object detection. <em>ICV</em>, <em>165</em>, 105817. (<a href='https://doi.org/10.1016/j.imavis.2025.105817'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Camouflaged object detection (COD) aims to identify targets that are highly similar in appearance or color to their background, presenting an extremely challenging task. Existing methods primarily focus on single object detection, neglecting the potential for collaborative camouflaged behavior between targets, resulting in inadequate performance in scenarios with multiple target interferences and complex backgrounds. To address this, we propose a novel collaborative camouflaged object detection (CoCOD) framework—Attention-based Bidirectional Enhancement Network (ABENet). Specifically, we design two core modules: the Shifted Cooperative Attention Module (SCAM) and the Gated Bidirectional Enhancement Module (GBEM). The SCAM introduces a shifted window attention mechanism to facilitate efficient feature interaction between local and global contexts, combined with multi-scale receptive fields and channel selectors to enhance the response capability to critical camouflaged regions, and the GBEM employs a gated mechanism to achieve bidirectional enhancement between high- and low-level features, integrating contextual semantic information to enhance the effectiveness and robustness of collaborative camouflaged object detection tasks. Extensive experiments on the CoCOD8K dataset demonstrate that our proposed ABENet outperforms 10 state-of-the-art COD methods, 7 co-salient object detection (CoSOD) models, and 7 CoCOD approaches, as evaluated by six widely used metrics.},
  archive      = {J_ICV},
  author       = {Cong Zhang and Shiyuan Li and Yulin Zeng and Yuhui Gao and Hongbo Bi},
  doi          = {10.1016/j.imavis.2025.105817},
  journal      = {Image and Vision Computing},
  month        = {1},
  pages        = {105817},
  shortjournal = {Image Vis. Comput.},
  title        = {ABENet: Attention-based bidirectional enhancement network for collaborative camouflaged object detection},
  volume       = {165},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). DBaP-net: Deep network for image defogging based on physical properties prior. <em>ICV</em>, <em>165</em>, 105803. (<a href='https://doi.org/10.1016/j.imavis.2025.105803'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the realm of computer vision, high-quality image information serves as the foundation for downstream tasks. Nevertheless, elements such as foggy weather, suboptimal lighting circumstances, and atmospheric impurities frequently deteriorate image quality, posing a considerable research challenge in effectively restoring these low-quality images. Existing defogging approaches mainly rely on constraints and physical priors; however, they have demonstrated limited efficacy, especially when dealing with extensive fog-affected areas. To tackle this issue, a deep trainable de-fog network named DBaP-net is proposed in this paper. By leveraging convolutional neural networks, this network integrates diverse filters to extract physical priors from images. Through the construction of a sophisticated deep network architecture, DBaP-net precisely estimates the transmission map and efficiently facilitates the restoration of haze-free images. Additionally, we design a spatial transformation layer customized for physical prior features and adopt a multi-kernel fusion extraction technique to further enhance the model’s feature extraction capabilities and spatial adaptability, thereby laying a solid foundation for subsequent visual tasks. Experimental validation indicates that DBaP-net not only effectively eliminates haze from images but also significantly enhances their overall quality. In both quantitative and qualitative evaluations, DBaP-net surpasses other comparison algorithms in terms of efficiency and usability. As a result, this study offers a novel solution to the image defogging problem within computer vision frameworks, enabling the precise restoration of low-quality images and providing robust support for research endeavors and downstream applications in related fields.},
  archive      = {J_ICV},
  author       = {Bo Yu and Hanting Wei and Chenghong Zhang and Wei Wang},
  doi          = {10.1016/j.imavis.2025.105803},
  journal      = {Image and Vision Computing},
  month        = {1},
  pages        = {105803},
  shortjournal = {Image Vis. Comput.},
  title        = {DBaP-net: Deep network for image defogging based on physical properties prior},
  volume       = {165},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Automated recognition of humerus anomalies with convolutional neural networks. <em>ICV</em>, <em>165</em>, 105799. (<a href='https://doi.org/10.1016/j.imavis.2025.105799'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Humerus anomalies are a problem that requires rapid and accurate diagnosis to ensure immediate and efficient treatment. In this context, the main goal of this paper is to develop and analyze well-known Convolutional Neural Network models for the automatic recognition of humeral fractures, with the aim of proposing a useful tool for healthcare personnel. Specifically, three distinct architectures were implemented and compared: a three-layer untrained neural network, a network based on the ResNet18 architecture and one based on the DenseNet121 model, both of which were trained. The performance analysis highlighted a trade-off between accuracy and generalization ability, showing better accuracy in the pre-trained models - in particular, the DenseNet121 model achieved optimal accuracy across multiple runs of 85%. - which however proved more prone to suffer from overfitting compared to the non-pre-trained model. As a result, this study aims to propose the integration of deep learning tools in medical practice, laying important foundations for future developments, with the hope of improving the efficiency and accuracy of orthopedic diagnoses.},
  archive      = {J_ICV},
  author       = {Gea Viozzi and Fabio Persia and Daniela D’Auria},
  doi          = {10.1016/j.imavis.2025.105799},
  journal      = {Image and Vision Computing},
  month        = {1},
  pages        = {105799},
  shortjournal = {Image Vis. Comput.},
  title        = {Automated recognition of humerus anomalies with convolutional neural networks},
  volume       = {165},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). LSBE-net: Semantic segmentation of large-scale point cloud scenes via local boundary feature and spatial attention aggregation. <em>ICV</em>, <em>165</em>, 105798. (<a href='https://doi.org/10.1016/j.imavis.2025.105798'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {3D point cloud semantic segmentation plays a pivotal role in comprehending 3D scenes and facilitating environmental perception. Existing studies predominantly emphasize the extraction of local geometric structures, but they often overlook the incorporation of local boundary cues and long-range spatial relationships. This limitation hampers precise delineation of object boundaries and impairs the distinction of long distance instances. To address these challenges, we propose LSBE-Net, a novel segmentation algorithm designed to extract local boundary features and integrate spatial context features. The Local Surface Representation (LSR) module is introduced to capture local geometric shapes by encoding both surface and positional features, thereby providing critical structural information. The Local Boundary Enhancement (LBE) module extracts boundary features and fuses them with geometric and semantic features through a transformer mechanism within local neighborhoods, enabling the learning of contextual relationships and refinement of boundary delineation. These features are aggregated through the Spatial Encoding Attention (SEA) module, which facilitates the learning of long-range dependencies and spatial relationship across the point cloud. The proposed LSBE-Net is extensively evaluated on three large-scale benchmark datasets: S3DIS, Toronto3D, and Semantic3D. Our method achieves competitive mean Intersection over Union (mIoU) scores of 66.1%, 82.3%, and 78.0%, respectively, demonstrating its effectiveness and robustness in diverse real-world scenarios.},
  archive      = {J_ICV},
  author       = {Hailang Wang and Keke Duan and Mingzi Zhang and Li Ma},
  doi          = {10.1016/j.imavis.2025.105798},
  journal      = {Image and Vision Computing},
  month        = {1},
  pages        = {105798},
  shortjournal = {Image Vis. Comput.},
  title        = {LSBE-net: Semantic segmentation of large-scale point cloud scenes via local boundary feature and spatial attention aggregation},
  volume       = {165},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). EmbryoVision AI: An explainable deep learning framework for enhanced blastocyst selection in assisted reproductive technologies. <em>ICV</em>, <em>165</em>, 105795. (<a href='https://doi.org/10.1016/j.imavis.2025.105795'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Accurate embryo selection is a key factor in improving implantation success rates in Assisted Reproductive Technologies. This study presents a deep learning framework, EmbryoVision AI , designed to enhance blastocyst assessment using Time-Lapse Imaging and eXplainable AI techniques. A customized convolutional neural network was developed to capture both morphological and temporal dynamics, enabling a precise classification of the embryo. To ensure transparency, Gradient-weighted Class Activation Mapping was integrated, allowing visualization of decision-critical embryonic structures and ensuring clinical alignment. The model demonstrated strong predictive performance across different embryo grades, achieving an accuracy of 91.5% for Grade AA, 88.4% for Grade AB, and 79.3% for Grade BC. The AUC-ROC values were 0.95, 0.90, and 0.81 for Grade AA, AB, and BC, respectively, indicating strong discriminatory capabilities. The findings suggest that AI-driven embryo selection can enhance objectivity, reduce human variability, and improve ART outcomes. However, the results also underscore the need to refine AI models to better handle morphological variability in lower-quality embryos, highlighting the importance of improving generalization and strengthening clinical integration.},
  archive      = {J_ICV},
  author       = {Alessia Auriemma Citarella and Pietro Battistoni and Chiara Coscarelli and Fabiola De Marco and Luigi Di Biasi and Mengyuan Wang},
  doi          = {10.1016/j.imavis.2025.105795},
  journal      = {Image and Vision Computing},
  month        = {1},
  pages        = {105795},
  shortjournal = {Image Vis. Comput.},
  title        = {EmbryoVision AI: An explainable deep learning framework for enhanced blastocyst selection in assisted reproductive technologies},
  volume       = {165},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). CMDiff: Clip-guided multi-dimension mamba diffusion model for low light image enhancement. <em>ICV</em>, <em>165</em>, 105787. (<a href='https://doi.org/10.1016/j.imavis.2025.105787'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Significant progress has been made in low-light image enhancement techniques, but unstable image quality recovery and suboptimal visual perception remain challenges. To address these issues, we propose a robust low-light image enhancement method, CLIP-guided multi-dimension mamba diffusion (CMDiff). Our framework utilizes the CLIP model for scene information guidance via feature modulation to enhance the model’s perception of scene details. We combine the multidimension Mamba block with UNet to construct a denoising network that leverages Mamba’s strengths in processing long sequences and global contextual information. The multi-dimension Mamba module integrates key information from multiple image dimensions with linear time complexity, thus facilitating effective and comprehensive feature learning. Extensive experiments on real-world benchmarks show that our approach outperforms state-of-the-art methods in terms of image quality and noise rejection.},
  archive      = {J_ICV},
  author       = {Chengbo Yu and Dengshi Li and Jia Wei and Yulin Wu and AoLei Chen},
  doi          = {10.1016/j.imavis.2025.105787},
  journal      = {Image and Vision Computing},
  month        = {1},
  pages        = {105787},
  shortjournal = {Image Vis. Comput.},
  title        = {CMDiff: Clip-guided multi-dimension mamba diffusion model for low light image enhancement},
  volume       = {165},
  year         = {2026},
}
</textarea>
</details></li>
</ul>

</body>
</html>
