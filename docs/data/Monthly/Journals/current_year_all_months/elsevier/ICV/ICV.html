<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>ICV</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="icv">ICV - 311</h2>
<ul>
<li><details>
<summary>
(2025). Test-time adaptation for object detection via dynamic dual teaching. <em>ICV</em>, <em>163</em>, 105740. (<a href='https://doi.org/10.1016/j.imavis.2025.105740'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Test-Time Adaptation (TTA) is a practical setting in real-world applications, which aims to adapt a source-trained model to target domains with online unlabeled test data streams. Current approaches often rely on self-training, utilizing supervision signals from the source-trained model, suffering from poor adaptation due to diverse domain shifts. In this paper, we propose a novel test-time adaptation method for object detection guided by dual teachers, termed D ynamic D ual T eaching ( DDT ). Inspired by the generalization potentials of Vision-Language Models (VLMs), we integrate the VLM as an additional language-driven instructor. This integration exploits the domain-robustness of language prompts to mitigate domain shifts, collaborating with the teacher of source information within the teacher–student framework. Firstly, we utilize an ensemble prompt to guide the prediction process of the language-driven instructor. Secondly, a dynamic fusion strategy of the dual teachers is designed to generate high-quality pseudo-labels for student learning. Moreover, we incorporate a dual prediction consistency regularization to further mitigate the sensitivity of the adapted detector to domain shifts. Experiments on diverse domain adaptation benchmarks demonstrate that the proposed DDT method achieves state-of-the-art performance on both online and offline domain adaptation settings.},
  archive      = {J_ICV},
  author       = {Siqi Zhang and Lu Zhang and Zhiyong Liu},
  doi          = {10.1016/j.imavis.2025.105740},
  journal      = {Image and Vision Computing},
  month        = {11},
  pages        = {105740},
  shortjournal = {Image Vis. Comput.},
  title        = {Test-time adaptation for object detection via dynamic dual teaching},
  volume       = {163},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Mining fine-grained attributes for vision–semantics integration in few-shot learning. <em>ICV</em>, <em>163</em>, 105739. (<a href='https://doi.org/10.1016/j.imavis.2025.105739'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent advancements in Few-Shot Learning (FSL) have been significantly driven by leveraging semantic descriptions to enhance feature discrimination and recognition performance. However, existing methods, such as SemFew, often rely on verbose or manually curated attributes and apply semantic guidance only to the support set, limiting their effectiveness in distinguishing fine-grained categories. Inspired by human visual perception, which emphasizes crucial features for accurate recognition, this study introduces concise, fine-grained semantic attributes to address these limitations. We propose a Visual Attribute Enhancement (VAE) mechanism that integrates enriched semantic information into visual features, enabling the model to highlight the most relevant visual attributes and better distinguish visually similar samples. This module enhances visual features by aligning them with semantic attribute embeddings through a cross-attention mechanism and optimizes this alignment using an attribute-based cross-entropy loss. Furthermore, to mitigate the performance degradation caused by methods that supply semantic information exclusively to the support set, we propose a semantic attribute reconstruction (SAR) module. This module predicts and integrates semantic features for query samples, ensuring balanced information distribution between the support and query sets. Specifically, SAR enhances query representations by aligning and reconstructing semantic and visual attributes through regression and optimal transport losses to ensure semantic–visual consistency. Experiments on five benchmark datasets, including both general datasets and more challenging fine-grained Few-Shot datasets consistently demonstrate that our proposed method outperforms state-of-the-art methods in both 5-way 1-shot and 5-way 5-shot settings.},
  archive      = {J_ICV},
  author       = {Juan Zhao and Lili Kong and Deshang Sun and Deng Xiong and Jiancheng Lv},
  doi          = {10.1016/j.imavis.2025.105739},
  journal      = {Image and Vision Computing},
  month        = {11},
  pages        = {105739},
  shortjournal = {Image Vis. Comput.},
  title        = {Mining fine-grained attributes for vision–semantics integration in few-shot learning},
  volume       = {163},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Explainable deepfake detection across different modalities: An overview of methods and challenges. <em>ICV</em>, <em>163</em>, 105738. (<a href='https://doi.org/10.1016/j.imavis.2025.105738'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The increasing use of deepfake technology enables the creation of realistic and deceptive content, raising concerns about several serious issues, including biometric authentication, misinformation, politics, privacy, and trust. Many Deepfake Detection (DD) models are entering the market to combat the misuse of deepfakes. With these developments, one primary issue occurs in ensuring the explainability of the proposed detection models to understand the rationale of the decision. This paper aims to investigate the state-of-the-art explainable DD models across multiple modalities, including image, video, audio, and text. Unlike existing surveys that focus on detection methodologies with minimal attention to explainability and limited modality coverage, this paper directly focuses on these gaps. It offers a comprehensive analysis of advanced explainability techniques, including Grad-CAM, LIME, SHAP, LRP, Saliency Maps, and Anchors, for detecting deceptive content across the modalities. It identifies the strengths and limitations of existing models and outlines research directions to enhance explainability and interpretability in future works. By exploring these models, we aim to enhance transparency, provide deeper insights into model decisions, and bridge the gap between detection accuracy with explainability in DD models.},
  archive      = {J_ICV},
  author       = {MD Sarfaraz Momin and Abu Sufian and Debaditya Barman and Marco Leo and Cosimo Distante and Naser Damer},
  doi          = {10.1016/j.imavis.2025.105738},
  journal      = {Image and Vision Computing},
  month        = {11},
  pages        = {105738},
  shortjournal = {Image Vis. Comput.},
  title        = {Explainable deepfake detection across different modalities: An overview of methods and challenges},
  volume       = {163},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). MITS: A large-scale multimodal benchmark dataset for intelligent traffic surveillance. <em>ICV</em>, <em>163</em>, 105736. (<a href='https://doi.org/10.1016/j.imavis.2025.105736'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {General-domain large multimodal models (LMMs) have achieved significant advances in various image-text tasks. However, their performance in the Intelligent Traffic Surveillance (ITS) domain remains limited due to the absence of dedicated multimodal datasets. To address this gap, we introduce MITS (Multimodal Intelligent Traffic Surveillance), the first large-scale multimodal benchmark dataset specifically designed for ITS. MITS includes 170,400 independently collected real-world ITS images sourced from traffic surveillance cameras, annotated with eight main categories and 24 subcategories of ITS-specific objects and events under diverse environmental conditions. Additionally, through a systematic data generation pipeline, we generate high-quality image captions and 5 million instruction-following visual question-answer pairs , addressing five critical ITS tasks : object and event recognition, object counting, object localization, background analysis, and event reasoning. To demonstrate MITS’s effectiveness, we fine-tune mainstream LMMs on this dataset, enabling the development of ITS-specific applications. Experimental results show that MITS significantly improves LMM performance in ITS applications, increasing LLaVA-1.5’s performance from 0.494 to 0.905 (+83.2%), LLaVA-1.6’s from 0.678 to 0.921 (+35.8%), Qwen2-VL’s from 0.584 to 0.926 (+58.6%), and Qwen2.5-VL’s from 0.732 to 0.930 (+27.0%). We release the dataset, code, and models as open-source , providing high-value resources to advance both ITS and LMM research.},
  archive      = {J_ICV},
  author       = {Kaikai Zhao and Zhaoxiang Liu and Peng Wang and Xin Wang and Zhicheng Ma and Yajun Xu and Wenjing Zhang and Yibing Nan and Kai Wang and Shiguo Lian},
  doi          = {10.1016/j.imavis.2025.105736},
  journal      = {Image and Vision Computing},
  month        = {11},
  pages        = {105736},
  shortjournal = {Image Vis. Comput.},
  title        = {MITS: A large-scale multimodal benchmark dataset for intelligent traffic surveillance},
  volume       = {163},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). UNIR-net: A novel approach for restoring underwater images with non-uniform illumination using synthetic data. <em>ICV</em>, <em>163</em>, 105734. (<a href='https://doi.org/10.1016/j.imavis.2025.105734'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Restoring underwater images affected by non-uniform illumination (NUI) is essential to improve visual quality and usability in marine applications. Conventional methods often fall short in handling complex illumination patterns, while learning-based approaches face challenges due to the lack of targeted datasets. To address these limitations, the Underwater Non-uniform Illumination Restoration Network (UNIR-Net) is proposed. UNIR-Net integrates multiple components, including illumination enhancement, attention mechanisms, visual refinement, and contrast correction, to effectively restore underwater images affected by NUI. In addition, the Paired Underwater Non-uniform Illumination (PUNI) dataset is introduced, specifically designed for training and evaluating models under NUI conditions. Experimental results on PUNI and the large-scale real-world Non-Uniform Illumination Dataset (NUID) show that UNIR-Net achieves superior performance in both quantitative metrics and visual outcomes. UNIR-Net also improves downstream tasks such as underwater semantic segmentation, highlighting its practical relevance. The code is available at https://github.com/xingyumex/UNIR-Net .},
  archive      = {J_ICV},
  author       = {Ezequiel Pérez-Zarate and Chunxiao Liu and Oscar Ramos-Soto and Diego Oliva and Marco Pérez-Cisneros},
  doi          = {10.1016/j.imavis.2025.105734},
  journal      = {Image and Vision Computing},
  month        = {11},
  pages        = {105734},
  shortjournal = {Image Vis. Comput.},
  title        = {UNIR-net: A novel approach for restoring underwater images with non-uniform illumination using synthetic data},
  volume       = {163},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). FFENet: A frequency fusion and enhancement network for camouflaged object detection. <em>ICV</em>, <em>163</em>, 105733. (<a href='https://doi.org/10.1016/j.imavis.2025.105733'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The goal of camouflaged object detection (COD) is to accurately find camouflaged objects hidden in their surroundings. Although most of the existing frequency-domain based COD models can boost the performance of COD to a certain extent by utilizing the frequency domain information, the frequency feature fusion strategies they adopt tend to ignore the complementary effects between high-frequency features and low-frequency features. In addition, most of the existing frequency-domain based COD models also do not consider enhancing camouflaged objects using low-level frequency-domain features. In order to solve these problems, we present a frequency fusion and enhancement network (FFENet) for camouflaged object detection, which mainly includes three stages. In the frequency feature extraction stage, we design a frequency feature learning module (FLM) to extract corresponding high-frequency features and low-frequency features. In the frequency feature fusion stage, we design a frequency feature fusion module (FFM) that can increase the representation ability of the fused features by adaptively assigning weights to the high-frequency features and the low-frequency features using a cross-attention mechanism. In the frequency feature guidance information enhancement stage, we design a frequency feature guidance information enhancement module (FGIEM) to enhance the contextual information and detail information of camouflaged objects in the fused features under the guidance of the low-level frequency features. Extensive experimental results on the COD10K, CHAMELEON, NC4K and CAMO datasets show that our model is superior to most existing COD models.},
  archive      = {J_ICV},
  author       = {Haishun Du and Wenzhe Zhang and Sen Wang and Zhengyang Zhang and Linbing Cao},
  doi          = {10.1016/j.imavis.2025.105733},
  journal      = {Image and Vision Computing},
  month        = {11},
  pages        = {105733},
  shortjournal = {Image Vis. Comput.},
  title        = {FFENet: A frequency fusion and enhancement network for camouflaged object detection},
  volume       = {163},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). UpAttTrans: Upscaled attention based transformer for facial image super-resolution. <em>ICV</em>, <em>163</em>, 105731. (<a href='https://doi.org/10.1016/j.imavis.2025.105731'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Image super-resolution (SR) aims to reconstruct high-quality images from low-resolution inputs, a task particularly challenging in face-related applications due to extreme degradations and modality differences (e.g., visible, low-resolution, near-infrared). Conventional convolutional neural networks (CNNs) and GAN-based approaches have achieved notable success; however, they often struggle with preserving identity and fine structural details at high upscaling factors. In this work, we introduce UpAttTrans, a novel attention mechanism that connects original and upsampled features for better detail recovery based on vision transformer for SR. The core generator leverages a custom UpAttTrans module that translates input image patches into embeddings, processes them through transformer layers enhanced with connector-up attention, and reconstructs high-resolution outputs with improved detail retention. We evaluate our model on the CelebA dataset across multiple upscaling factors ( 4 × , 8 × , 16 × , 32 × , and 64 × ). UpAttTrans achieves a 24.63% increase in PSNR, 21.56% in SSIM, and 19.61% reduction in FID for 4 × and 8 × SR, outperforming state-of-the-art baselines. Additionally, for higher magnification levels, our model maintains strong performance, with average gains of 6.20% in PSNR and 21.49% in SSIM, indicating its robustness in extreme SR settings. These findings suggest that UpAttTrans holds significant promise for real-world applications such as face recognition in surveillance, forensic image enhancement, and cross-spectral matching, where high-quality reconstruction from severely degraded inputs is critical.},
  archive      = {J_ICV},
  author       = {Neeraj Baghel and Shiv Ram Dubey and Satish Kumar Singh},
  doi          = {10.1016/j.imavis.2025.105731},
  journal      = {Image and Vision Computing},
  month        = {11},
  pages        = {105731},
  shortjournal = {Image Vis. Comput.},
  title        = {UpAttTrans: Upscaled attention based transformer for facial image super-resolution},
  volume       = {163},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Novel extraction of discriminative fine-grained feature to improve retinal vessel segmentation. <em>ICV</em>, <em>163</em>, 105729. (<a href='https://doi.org/10.1016/j.imavis.2025.105729'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Retinal vessel segmentation is a vital early detection method for several severe ocular diseases. Despite significant progress in retinal vessel segmentation with the advancement of Neural Networks, there are still challenges to overcome. Specifically, retinal vessel segmentation aims to predict the class label for every pixel within a fundus image, with a primary focus on intra-image discrimination, making it vital for models to extract more discriminative features. Nevertheless, existing methods primarily focus on minimizing the difference between the output from the decoder and the label, but ignore fully using feature-level fine-grained representations from the encoder. To address these issues, we propose a novel Attention U-shaped Kolmogorov–Arnold Network named AttUKAN along with a novel Label-guided Pixel-wise Contrastive Loss for retinal vessel segmentation. Specifically, we implement Attention Gates into Kolmogorov–Arnold Networks to enhance model sensitivity by suppressing irrelevant feature activations and model interpretability by non-linear modeling of KAN blocks. Additionally, we also design a novel Label-guided Pixel-wise Contrastive Loss to supervise our proposed AttUKAN to extract more discriminative features by distinguishing between foreground vessel-pixel pairs and background pairs. Experiments are conducted across four public datasets including DRIVE, STARE, CHASE_DB1, HRF and our private dataset. AttUKAN achieves F1 scores of 82.50%, 81.14%, 81.34%, 80.21% and 80.09%, along with MIoU scores of 70.24%, 68.64%, 68.59%, 67.21% and 66.94% in the above datasets, which are the highest compared to 11 networks for retinal vessel segmentation. Quantitative and qualitative results show that our AttUKAN achieves state-of-the-art performance and outperforms existing retinal vessel segmentation methods. Our code will be available at https://github.com/stevezs315/AttUKAN .},
  archive      = {J_ICV},
  author       = {Shuang Zeng and Chee Hong Lee and Micky C. Nnamdi and Wenqi Shi and J. Ben Tamo and Hangzhou He and Xinliang Zhang and Qian Chen and May D. Wang and Lei Zhu and Yanye Lu and Qiushi Ren},
  doi          = {10.1016/j.imavis.2025.105729},
  journal      = {Image and Vision Computing},
  month        = {11},
  pages        = {105729},
  shortjournal = {Image Vis. Comput.},
  title        = {Novel extraction of discriminative fine-grained feature to improve retinal vessel segmentation},
  volume       = {163},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Artificial intelligence content detection techniques using watermarking: A survey. <em>ICV</em>, <em>163</em>, 105728. (<a href='https://doi.org/10.1016/j.imavis.2025.105728'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The rapid advancement in AI-generated content has catalyzed artistic creation, advertising, and media dissemination. Despite their widespread applications across several domains, AI-generated content inherently poses risks of identity fraud, copyright violation and unauthorized use. Watermarking has emerged as a critical tool for copyright protection, allowing embedding of identification information in AI-generated content, and enhances traceability and verification without hurting user experience. In this study, we provide a systematic literature review of the technique for detecting AI content, especially text and images, using watermarking, spanning studies from 2010 to 2025. Studies included in this review were peer-reviewed articles that applied watermarking to effectively distinguish AI-generated content from real or human-written content. We report strong past and current approaches to detecting watermarking-based AI content, especially text and images. This includes an analysis of how watermarking methods are used on AI-generated content, their role in enhancing performance, and a detail comparative analysis of notable techniques. Furthermore, we discuss how these methods have been evaluated, identify the research gaps and potential solutions. Our findings provide valuable insights for future watermarking-based AI content detection researchers, applications and organizations seeking to implement watermarking solutions in potential applications. To the best of our knowledge, we are the first to explore the detection of AI content, especially text and image, detection using watermarking.},
  archive      = {J_ICV},
  author       = {Nishant Kumar and Amit Kumar Singh},
  doi          = {10.1016/j.imavis.2025.105728},
  journal      = {Image and Vision Computing},
  month        = {11},
  pages        = {105728},
  shortjournal = {Image Vis. Comput.},
  title        = {Artificial intelligence content detection techniques using watermarking: A survey},
  volume       = {163},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Your image generator is your new private dataset. <em>ICV</em>, <em>163</em>, 105727. (<a href='https://doi.org/10.1016/j.imavis.2025.105727'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Generative diffusion models have emerged as powerful tools to synthetically produce training data, offering potential solutions to data scarcity and reducing labelling costs for downstream supervised deep learning applications. However, existing approaches for synthetic dataset generation face significant limitations: previous methods like Knowledge Recycling rely on label-conditioned generation with models trained from scratch, limiting flexibility and requiring extensive computational resources, while simple class-based conditioning fails to capture the semantic diversity and intra-class variations found in real datasets. Additionally, effectively leveraging text-conditioned image generation for building classifier training sets requires addressing key issues: constructing informative textual prompts, adapting generative models to specific domains, and ensuring robust performance. This paper proposes the Text-Conditioned Knowledge Recycling (TCKR) pipeline to tackle these challenges. TCKR combines dynamic image captioning, parameter-efficient diffusion model fine-tuning, and Generative Knowledge Distillation techniques to create synthetic datasets tailored for image classification. The pipeline is rigorously evaluated on ten diverse image classification benchmarks. The results demonstrate that models trained solely on TCKR-generated data achieve classification accuracies on par with (and in several cases exceeding) models trained on real images. Furthermore, the evaluation reveals that these synthetic-data-trained models exhibit substantially enhanced privacy characteristics: their vulnerability to Membership Inference Attacks is significantly reduced, with the membership inference AUC lowered by 5.49 points on average compared to using real training data, demonstrating a substantial improvement in the performance-privacy trade-off. These findings indicate that high-fidelity synthetic data can effectively replace real data for training classifiers, yielding strong performance whilst simultaneously providing improved privacy protection as a valuable emergent property. The code and trained models are available in the accompanying open-source repository .},
  archive      = {J_ICV},
  author       = {Nicolò Francesco Resmini and Eugenio Lomurno and Cristian Sbrolli and Matteo Matteucci},
  doi          = {10.1016/j.imavis.2025.105727},
  journal      = {Image and Vision Computing},
  month        = {11},
  pages        = {105727},
  shortjournal = {Image Vis. Comput.},
  title        = {Your image generator is your new private dataset},
  volume       = {163},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Assessing the noise robustness of class activation maps: A framework for reliable model interpretability. <em>ICV</em>, <em>163</em>, 105717. (<a href='https://doi.org/10.1016/j.imavis.2025.105717'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Class Activation Maps (CAMs) are one of the important methods for visualizing regions used by deep learning models. Yet their robustness to different noise remains underexplored. In this work, we evaluate and report the resilience of various CAM methods for different noise perturbations across multiple architectures and datasets. By analyzing the influence of different noise types on CAM explanations, we assess the susceptibility to noise and the extent to which dataset characteristics may impact explanation stability. The findings highlight considerable variability in noise sensitivity for various CAMs. We propose a robustness metric for CAMs that captures two key properties: consistency and responsiveness. Consistency reflects the ability of CAMs to remain stable under input perturbations that do not alter the predicted class, while responsiveness measures the sensitivity of CAMs to changes in the prediction caused by such perturbations. The metric is evaluated empirically across models, different perturbations, and datasets along with complementary statistical tests to exemplify the applicability of our proposed approach.},
  archive      = {J_ICV},
  author       = {Syamantak Sarkar and Revoti P. Bora and Bhupender Kaushal and Sudhish N. George and Kiran Raja},
  doi          = {10.1016/j.imavis.2025.105717},
  journal      = {Image and Vision Computing},
  month        = {11},
  pages        = {105717},
  shortjournal = {Image Vis. Comput.},
  title        = {Assessing the noise robustness of class activation maps: A framework for reliable model interpretability},
  volume       = {163},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). InceptionWTMNet: A hybrid network for alzheimer’s disease detection using wavelet transform convolution and mixed local channel attention on finely fused multimodal images. <em>ICV</em>, <em>163</em>, 105693. (<a href='https://doi.org/10.1016/j.imavis.2025.105693'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multimodal fusion has emerged as a critical technique for the diagnosis of Alzheimer’s Disease (AD), with the aim of effectively extracting and utilising complementary information from diverse modalities. Current fusion methods frequently cause the precise alignment of source images and do not adequately address parallax issues. This oversight can result in artifacts during the fusion process when images are misaligned. In response to this challenge, we propose a refined registration fusion technique, termed MURF, which integrates multimodal image registration and fusion within a cohesive framework. The Vision Transformer (ViT) has inspired the application of large-kernel convolutions in the diagnosis of Alzheimer’s disease (AD) because of its ability to model long-range dependencies. This approach aims to expand the receptive field and enhance the performance of diagnostic models. Despite requiring a minimal number of floating-point operations (FLOPs), these deep operators encounter challenges associated with over-parameterisation because of high memory access costs, which ultimately compromises computational efficiency. By utilising wavelet transform convolutions (WTConv), we decompose large-kernel depth-wise convolutions into four parallel branches. One branch employs a wavelet-transform convolution with square kernels, while the other two branches incorporate orthogonal wavelet-transform kernels with an identity mapping. This innovative method, with a Mixed Local Channel Attention mechanism, has facilitated the development of the InceptionWTConvolutions network. This network maintains a receptive field comparable to that of large-kernel convolutions, while concurrently minimising over-parameterisation and enhancing computational efficiency. InceptionWTMNet classified AD, MCI, and NC using MRI and PET data from ADNI dataset with 98.69% accuracy, 98.65% recall, 98.70% F1-score, and 98.98% AUC. and provide Graphical abstract in correct format.},
  archive      = {J_ICV},
  author       = {Zenan Xu and Zhengyao Bai and Han Ma and Mingqiang Xu and Qiqin Huang and Tao Lin},
  doi          = {10.1016/j.imavis.2025.105693},
  journal      = {Image and Vision Computing},
  month        = {11},
  pages        = {105693},
  shortjournal = {Image Vis. Comput.},
  title        = {InceptionWTMNet: A hybrid network for alzheimer’s disease detection using wavelet transform convolution and mixed local channel attention on finely fused multimodal images},
  volume       = {163},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Deep learning enhanced monocular visual odometry: Advancements in fusion mechanisms and training strategies. <em>ICV</em>, <em>162</em>, 105732. (<a href='https://doi.org/10.1016/j.imavis.2025.105732'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent advances in deep learning have revolutionized robotic applications such as 3D mapping, visual navigation and autonomous control. Monocular Visual Odometry (MVO) represents a critical advancement in autonomous systems, particularly drones, utilizing single-camera setups to navigate complex environments effectively. This review explores MVO’s evolution from traditional methods to its integration with cutting-edge technologies like deep learning and semantic understanding. In this study, we explore the latest training strategies, innovations in model architecture, and advanced fusion techniques used in hybrid models that combine depth and semantic information. A comprehensive literature review traces the evolution of MVO techniques, highlighting key datasets and performance metrics. Section 2 outlines the problem, while Section 3 reviews the studies, charting the evolution of MVO techniques predating the advent of deep learning. Section 4 details the methodology, focusing on cutting-edge training strategies, advancements in architectural designs, and fusion techniques in hybrid models integrating depth and semantic information. Finally, Section 5 summarizes findings, discusses implications, and suggests future research directions.},
  archive      = {J_ICV},
  author       = {E. Simsek and B. Ozyer},
  doi          = {10.1016/j.imavis.2025.105732},
  journal      = {Image and Vision Computing},
  month        = {10},
  pages        = {105732},
  shortjournal = {Image Vis. Comput.},
  title        = {Deep learning enhanced monocular visual odometry: Advancements in fusion mechanisms and training strategies},
  volume       = {162},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Explicit semantic alignment network for RGB-T salient object detection with hierarchical cross-modal fusion. <em>ICV</em>, <em>162</em>, 105730. (<a href='https://doi.org/10.1016/j.imavis.2025.105730'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Existing RGB-T salient object detection methods primarily rely on the learning mechanism of neural networks to perform implicit cross-modal feature alignment, aiming to achieve complementary fusion of modal features. However, this implicit feature alignment method has two main limitations: first, it is prone to causing loss of the salient object’s structural information; second, it may lead to abnormal activation responses that are not related to the object. To address the above issues, we propose the innovative Explicit Semantic Alignment (ESA) framework and design the Explicit Semantic Alignment Network for RGB-T Salient Object Detection with Hierarchical Cross-Modal Fusion (ESANet). Specifically, we design a Saliency-Aware Refinement Module (SARM), which fuses high-level semantic features with mid-level spatial details through cross-aggregation and the dynamic integration module to achieve bidirectional interaction and adaptive fusion of cross-modal features. It also utilizes a cross-modal multi-head attention mechanism to generate fine-grained shared semantic information. Subsequently, the Cross-Modal Feature Alignment Module (CFAM) introduces a window-based attention propagation mechanism, which enforces consistency in scene understanding between RGB and thermal modalities by using shared semantics as an alignment constraint. Finally, the Semantic-Guided Edge Sharpening Module (SESM) combines shared semantics with a weight enhancement strategy to optimize the consistency of shallow cross-modal feature distributions. Experimental results demonstrate that ESANet significantly outperforms existing state-of-the-art RGB-T salient object detection methods on three public datasets, validating its excellent performance in salient object detection tasks. Our code will be released at https://github.com/whklearn/ESANet.git .},
  archive      = {J_ICV},
  author       = {Hongkuan Wang and Qingxi Yu and Zhenguang Di and Gang Yang},
  doi          = {10.1016/j.imavis.2025.105730},
  journal      = {Image and Vision Computing},
  month        = {10},
  pages        = {105730},
  shortjournal = {Image Vis. Comput.},
  title        = {Explicit semantic alignment network for RGB-T salient object detection with hierarchical cross-modal fusion},
  volume       = {162},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Integrating explainable AI with synthetic biometric data for enhanced image synthesis and privacy in computer vision systems. <em>ICV</em>, <em>162</em>, 105726. (<a href='https://doi.org/10.1016/j.imavis.2025.105726'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Integrating Explainable AI (XAI) with synthetic biometric data improves image synthesis and privacy in computer vision systems by generating high-quality images while ensuring interpretability. This integration enhances trust and transparency in AI-driven biometric applications. However, traditional biometric data collection methods face challenges such as privacy risks, data scarcity, biases, and regulatory constraints, limiting their effectiveness in authentication and identity verification. To address these limitations, we propose a Generative Adversarial Networks with Explainable AI (GAN-EAI) framework for privacy-preserving biometric image synthesis. This framework utilizes GANs to generate high-fidelity synthetic biometric images while incorporating XAI techniques to interpret and validate the generated outputs, ensuring fairness, robustness, and bias mitigation. The proposed method enables secure, privacy-conscious biometric image synthesis, making it suitable for applications in authentication, healthcare, and identity verification. By leveraging explainability, it ensures that the model's decision-making process is interpretable, reducing the risk of biased or adversarial outputs. Experimental results demonstrate that GAN-EAI achieves superior image quality, enhances privacy protection, and reduces bias in synthetic biometric datasets, making it a reliable solution for real-world biometric applications. This research highlights the potential of integrating explainability with generative models to advance privacy-preserving AI in computer vision.},
  archive      = {J_ICV},
  author       = {Hamad Aldawsari and Saad Alammar},
  doi          = {10.1016/j.imavis.2025.105726},
  journal      = {Image and Vision Computing},
  month        = {10},
  pages        = {105726},
  shortjournal = {Image Vis. Comput.},
  title        = {Integrating explainable AI with synthetic biometric data for enhanced image synthesis and privacy in computer vision systems},
  volume       = {162},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Self-attention enhanced dynamic semantic multi-scale graph convolutional network for skeleton-based action recognition. <em>ICV</em>, <em>162</em>, 105725. (<a href='https://doi.org/10.1016/j.imavis.2025.105725'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Skeleton-based action recognition has attracted increasing attention due to its efficiency and robustness in modeling human motion. However, existing graph convolutional approaches often rely on predefined topologies and struggle to capture high-level semantic relations and long-range dependencies. Meanwhile, transformer-based methods, despite their effectiveness in modeling global dependencies, typically overlook local continuity and impose high computational costs. Moreover, current multi-stream fusion strategies commonly ignore low-level complementary cues across modalities. To address these limitations, we propose SAD-MSNet, a Self-Attention enhanced Multi-Scale dynamic semantic graph convolutional network. SAD-MSNet integrates a region-aware multi-scale skeleton simplification strategy to represent actions at different levels of abstraction. It employs a semantic-aware spatial modeling module that constructs dynamic graphs based on node types, edge types, and topological priors, further refined by channel-wise attention and adaptive fusion. For temporal modeling, the network utilizes a six-branch structure that combines standard causal convolution, dilated joint-guided temporal convolutions with varying dilation rates, and a global pooling branch, enabling it to effectively capture both short-term dynamics and long-range temporal semantics. Extensive experiments on NTU RGB+D, NTU RGB+D 120, and N-UCLA demonstrate that SAD-MSNet achieves superior performance compared to state-of-the-art methods, while maintaining a compact and interpretable architecture.},
  archive      = {J_ICV},
  author       = {Shihao Liu and Cheng Xu and Songyin Dai and Nuoya Li and Weiguo Pan and Bingxin Xu and Liu Hongzhe},
  doi          = {10.1016/j.imavis.2025.105725},
  journal      = {Image and Vision Computing},
  month        = {10},
  pages        = {105725},
  shortjournal = {Image Vis. Comput.},
  title        = {Self-attention enhanced dynamic semantic multi-scale graph convolutional network for skeleton-based action recognition},
  volume       = {162},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Breast tumor detection in ultrasound images with anatomical prior knowledge. <em>ICV</em>, <em>162</em>, 105724. (<a href='https://doi.org/10.1016/j.imavis.2025.105724'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Breast tumor detection is an important step in the procedure of computer-aided diagnosis. In clinical practice, computer-aided diagnosis system not only processes lesion images but also processes normal images without lesions. However, normal images are often overlooked. In this study, we additionally collected numerous normal images to evaluate object detection algorithms. We found that similarities between tumors and hypoechoic regions have led to false positive lesions, and the frequency of false positive lesions in normal images is higher than it in lesion images. To address this issue, we incorporate anatomical prior knowledge of breast tumors to propose a novel breast tumor detection method. Our method consists of a preprocessing method and a novel breast tumor detection network. The preprocessing method automatically extracts breast regions as anatomical constraints and utilizes channel fusion to combine images of breast regions with original images. The proposed breast tumor detection network is based on programmable gradient information and large-kernel convolution. The programmable gradient information is applied by an auxiliary branch which provides more comprehensive gradient information for backpropagation, while large-kernel convolution expands the receptive field of neurons. As a result, our method achieves the best false positive lesion rate of 3.30% and gets a reduction by at least 5.67% over other compared algorithms in normal images, with the best precision of 90.91%, sensitivity of 88.57%, f1-score of 89.75%, and mean average precision of 93.07% in lesion images. Experimental results suggest promising application potential.},
  archive      = {J_ICV},
  author       = {Liangduan Wu and Yan Zhuang and Guoliang Liao and Lin Han and Zhan Hua and Rui Wang and Ke Chen and Jiangli Lin},
  doi          = {10.1016/j.imavis.2025.105724},
  journal      = {Image and Vision Computing},
  month        = {10},
  pages        = {105724},
  shortjournal = {Image Vis. Comput.},
  title        = {Breast tumor detection in ultrasound images with anatomical prior knowledge},
  volume       = {162},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Few-sample video captioning using pre-trained language model with gated bidirectional fusion. <em>ICV</em>, <em>162</em>, 105723. (<a href='https://doi.org/10.1016/j.imavis.2025.105723'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Video captioning generates sentences for describing the video content. Previous works mostly rely on a large number of video samples for training the model, but annotating them is very costly, thus limiting the widespread application of video captioning. This motivates us to explore the way of using only a few labeled samples to describe the video, and propose a few-sample video captioning method by adopting the P re-trained language model with G ated B idirectional F usion (PGBF). In particular, we design a triple dynamic gating module that dynamically adjusts the contributions of appearance, motion, and text information to leverage the linguistic knowledge from pre-trained language model. Meanwhile, we develop a bidirectional fusion module to fuse appearance-text features and motion-text features to learn better cross-modal features. Moreover, we introduce a semantic contrastive loss to minimize the gap between visual features (i.e., appearance, motion, and the fused one) and text features (i.e., parsed nouns, verbs and whole sentence). Extensive experiments on three popular benchmarks demonstrate that our method achieves promising video captioning performance by using only a few training samples.},
  archive      = {J_ICV},
  author       = {Tao Wang and Ping Li and Zeyu Pan and Hao Wang},
  doi          = {10.1016/j.imavis.2025.105723},
  journal      = {Image and Vision Computing},
  month        = {10},
  pages        = {105723},
  shortjournal = {Image Vis. Comput.},
  title        = {Few-sample video captioning using pre-trained language model with gated bidirectional fusion},
  volume       = {162},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Flexible disentangled representation learning with soft-splitting for multi-view data. <em>ICV</em>, <em>162</em>, 105722. (<a href='https://doi.org/10.1016/j.imavis.2025.105722'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-view representation learning has gained significant attention in the machine learning and computer vision communities. However, existing approaches often fail to fully exploit the complementary part among different views during the fusion process, which may lead to representation entanglement and consequently degrade the performance for downstream tasks. To this end, we propose a novel Flexible Disentangled Representation Learning for Multi-View data in this paper. Specifically, the representation learning is performed by an adaptive soft-splitting multi-view gated fusion auto-encoder network, namely ASS-MVGFAE, which aims at separating the complementary and consistency parts in a soft way, rather than hard-splitting in the traditional methods. And then the decoupled common features are fed into a Gated Fusion Unit (GFU) to be aligned and fused, such that the shared latent representation is achieved for downstream clustering. Extensive experiments on several real-world datasets demonstrate that our method outperforms the state-of-the-art in terms of several evaluation metrics.},
  archive      = {J_ICV},
  author       = {Xunzhan Yao and Ming Yin and Yonghua Wang and Yi Guo},
  doi          = {10.1016/j.imavis.2025.105722},
  journal      = {Image and Vision Computing},
  month        = {10},
  pages        = {105722},
  shortjournal = {Image Vis. Comput.},
  title        = {Flexible disentangled representation learning with soft-splitting for multi-view data},
  volume       = {162},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). DNLN: Image super-resolution with deformable non-local attention and multi-branch weighted feature fusion. <em>ICV</em>, <em>162</em>, 105721. (<a href='https://doi.org/10.1016/j.imavis.2025.105721'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Single image super-resolution (SISR) aims to recover a high-resolution image from a low-resolution input. Despite recent advancements, existing methods often fail to fully exploit self-similarities across image scales. In this paper, we introduce the Deformable Non-Local (D-NL) attention module, integrated into a recurrent neural network. The D-NL attention mechanism leverages deformable convolutions to better capture pixel-wise correlations and long-range self-similarities. Additionally, we propose a Multi-Scale Channel Attention Module (MS-CAM) and a Multi-Branch Weighted Feature Fusion (MWFF) cell to enhance feature fusion, effectively identifying and combining features with distinct semantics and scales. Experimental results on benchmark datasets demonstrate that our approach, DNLN, significantly outperforms state-of-the-art methods, underscoring the effectiveness of exploiting long-range self-similarities for SISR.},
  archive      = {J_ICV},
  author       = {Jiaxin Chen and Dong Xing and Mohammad Shabaz and Yongpei Zhu and Yong Wang and Xianxun Zhu},
  doi          = {10.1016/j.imavis.2025.105721},
  journal      = {Image and Vision Computing},
  month        = {10},
  pages        = {105721},
  shortjournal = {Image Vis. Comput.},
  title        = {DNLN: Image super-resolution with deformable non-local attention and multi-branch weighted feature fusion},
  volume       = {162},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Edge-guided semantic-aware network for camouflaged object detection with PVTv2. <em>ICV</em>, <em>162</em>, 105720. (<a href='https://doi.org/10.1016/j.imavis.2025.105720'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Camouflaged object detection (COD) attempts to identify and segment objects visually blended into their surroundings, presenting significant challenges in complex real-world scenarios. Despite growing attention, existing COD methods often yield unsatisfactory performance, primarily due to their inadequate integration of edge information and semantic context—a critical shortcoming when handling intricate scenes. To this end, we propose a novel Edge-guided Semantic-aware Network (ESNet) that explicitly leverages the synergy between edge cues and multi-scale semantics. Our framework incorporates two key components: a Context-Aware Aggregation with Edge Guidance (CAEG) module, which utilizes edge information to refine object boundaries and enhance feature representation across scales, and a Cross-layer Semantic-Refinement Fusion (CSF) module, designed to aggregate and reinforce multi-level semantic context for richer feature characterization. Numerous experiments on three challenging benchmark datasets demonstrate that the proposed ESNet outperforms 17 state-of-the-art algorithms, achieving new standards in detection accuracy and robustness.},
  archive      = {J_ICV},
  author       = {Hongbo Bi and Jianing Yu and Disen Mo and Shiyuan Li and Cong Zhang},
  doi          = {10.1016/j.imavis.2025.105720},
  journal      = {Image and Vision Computing},
  month        = {10},
  pages        = {105720},
  shortjournal = {Image Vis. Comput.},
  title        = {Edge-guided semantic-aware network for camouflaged object detection with PVTv2},
  volume       = {162},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A dual-branch progressive network with spatial-frequency constraint for image fusion. <em>ICV</em>, <em>162</em>, 105709. (<a href='https://doi.org/10.1016/j.imavis.2025.105709'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Image fusion aims to integrate complementary information from source images to enhance the quality of fused representations. Most existing methods primarily impose pixel-level constraints in the spatial domain, which limits their ability to preserve frequency domain information. Furthermore, single-branch networks typically process source image features uniformly, which hinders cross-modal feature consideration. To address these challenges, we propose a Dual-branch Progressive Network (DPNet) for image fusion. First, a global feature fusion branch is constructed to enhance the extraction of long-range dependencies. This branch promotes global feature interaction through a Global Context Awareness (GCA) module. Subsequently, a local feature fusion branch is designed to extract local information from source images, which comprises multiple Local Feature Attention (LFA) modules to capture valuable local features. Additionally, to preserve both frequency and spatial domain information, we integrate two loss functions that jointly optimize feature retention in both domains. Experimental results on five datasets demonstrate that DPNet surpasses state-of-the-art fusion models both qualitatively and quantitatively. These findings validate its effectiveness for practical applications in military surveillance, environmental monitoring and medical imaging. The code is available at https://github.com/zenghui11/DPNet .},
  archive      = {J_ICV},
  author       = {Zenghui Wang and Wenhao Song and Xuening Xing and Lina Liu and Xianxun Zhu and Mingliang Gao},
  doi          = {10.1016/j.imavis.2025.105709},
  journal      = {Image and Vision Computing},
  month        = {10},
  pages        = {105709},
  shortjournal = {Image Vis. Comput.},
  title        = {A dual-branch progressive network with spatial-frequency constraint for image fusion},
  volume       = {162},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A review of breast cancer histopathology image analysis with deep learning: Challenges, innovations, and clinical integration. <em>ICV</em>, <em>162</em>, 105708. (<a href='https://doi.org/10.1016/j.imavis.2025.105708'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Breast cancer (BC) is the most frequently diagnosed cancer among women and a leading cause of cancer-related mortality globally. Accurate and timely diagnosis is essential for improving patient outcomes. However, traditional histopathological assessments are labor-intensive and subjective, leading to inter-observer variability and diagnostic inconsistencies, especially in resource-limited settings. Furthermore, variability in tissue staining, limited availability of standardized annotated datasets, and subtle morphological patterns complicate the consistent characterization of tumors. Deep learning (DL) has recently emerged as a transformative technology in breast cancer pathology, providing automated and objective solutions for cancer detection, classification, and segmentation from histopathological images. This review systematically evaluates advanced deep learning (DL) architectures, including convolutional neural networks (CNNs), generative adversarial networks (GANs), autoencoders, deep belief networks (DBNs), extreme learning machines (ELMs), and transformer-based models such as Vision Transformers (ViTs) as well as transfer learning, attention-based explainable AI techniques, and multimodal integration to address these diagnostic challenges. Analyzing 199 references, including 182 peer-reviewed studies published between 2014 and 2025 and 17 reputable online sources (websites, databases, etc.), we identify key innovations, limitations, and opportunities for future research. Furthermore, we explore the critical roles of synthetic data augmentation, explainable AI (XAI), and multimodal integration to enhance clinical trust, model interpretability, and diagnostic precision, ultimately facilitating personalized and efficient patient care.},
  archive      = {J_ICV},
  author       = {Inayatul Haq and Zheng Gong and Haomin Liang and Wei Zhang and Rashid Khan and Lei Gu and Roland Eils and Yan Kang and Bingding Huang},
  doi          = {10.1016/j.imavis.2025.105708},
  journal      = {Image and Vision Computing},
  month        = {10},
  pages        = {105708},
  shortjournal = {Image Vis. Comput.},
  title        = {A review of breast cancer histopathology image analysis with deep learning: Challenges, innovations, and clinical integration},
  volume       = {162},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). TPWGAN: Wavelet-aware text prior guided super-resolution for scene text images. <em>ICV</em>, <em>162</em>, 105707. (<a href='https://doi.org/10.1016/j.imavis.2025.105707'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Scene text image super-resolution (STISR) is crucial for improving the readability and recognition accuracy of low-resolution text images. Many previous methods have incorporated text prior information, such as character sequences or recognition features, into super-resolution frameworks. However, existing methods struggle to recover fine-grained text structures, often introducing artifacts or blurry edges due to insufficient high-frequency (HF) modeling and suboptimal use of text priors. Although some recent approaches incorporate wavelet-domain losses into the generator, they typically retain RGB-domain losses during adversarial training, limiting their ability to distinguish authentic text details from artifacts. To address this, we propose TPWGAN, a GAN-based STISR framework that introduces wavelet-domain losses in both the generator and discriminator. The generator is trained with fidelity losses on the HF wavelet subbands to enhance sensitivity to stroke-level variations, while the discriminator processes HF wavelet subbands fused with binary text region masks via a spatial attention mechanism, enabling semantically guided frequency-aware discrimination. Experiments on the TextZoom dataset and several real-world benchmarks show that TPWGAN achieves consistent improvements in visual quality and text recognition, particularly for challenging text instances with distortions or low resolution.},
  archive      = {J_ICV},
  author       = {Shengkai Liu and Jun Miao and Yuanhua Qiao and Hainan Wang},
  doi          = {10.1016/j.imavis.2025.105707},
  journal      = {Image and Vision Computing},
  month        = {10},
  pages        = {105707},
  shortjournal = {Image Vis. Comput.},
  title        = {TPWGAN: Wavelet-aware text prior guided super-resolution for scene text images},
  volume       = {162},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). TSGaussian: Semantic and depth-guided target-specific gaussian splatting from sparse views. <em>ICV</em>, <em>162</em>, 105706. (<a href='https://doi.org/10.1016/j.imavis.2025.105706'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent advances in Gaussian Splatting have significantly advanced the field, achieving both panoptic and interactive segmentation of 3D scenes. However, existing methodologies often overlook the critical need for reconstructing specified targets with complex structures from sparse views. To address this issue, we introduce TSGaussian, a framework that combines semantic constraints with depth priors to avoid geometry degradation in challenging novel view synthesis tasks. Our approach prioritizes computational resources on designated targets while minimizing background allocation. Bounding boxes from YOLOv9 serve as prompts for Segment Anything Model to generate 2D mask predictions, ensuring semantic accuracy and cost efficiency. TSGaussian effectively clusters 3D gaussians by introducing a compact identity encoding for each Gaussian ellipsoid and incorporating 3D spatial consistency regularization. Leveraging these modules, we propose a pruning strategy to effectively reduce redundancy in 3D gaussians. Extensive experiments demonstrate that TSGaussian outperforms state-of-the-art methods on three standard datasets and a self-built dataset, achieving superior results in novel view synthesis of specific objects. Code is available at: https://github.com/leon2000-ai/TSGaussian .},
  archive      = {J_ICV},
  author       = {Liang Zhao and Zehan Bao and Yi Xie and Hong Chen and Yaohui Chen and Weifu Li},
  doi          = {10.1016/j.imavis.2025.105706},
  journal      = {Image and Vision Computing},
  month        = {10},
  pages        = {105706},
  shortjournal = {Image Vis. Comput.},
  title        = {TSGaussian: Semantic and depth-guided target-specific gaussian splatting from sparse views},
  volume       = {162},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). LDH-net: Luminance-based deep hybrid network for document image de-shadowing. <em>ICV</em>, <em>162</em>, 105705. (<a href='https://doi.org/10.1016/j.imavis.2025.105705'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Existing deep learning-based Document Image De-shadowing (DID) methods face two key challenges. First, they struggle with complex shadows due to insufficient use of auxiliary information, such as shadow locations and illumination details. Second, they fail to effectively balance global relationships across the entire image with local feature learning to restore texture details in shadowed regions. To address these limitations, we propose a dual-branch de-shadowing network, called LDH-Net, which integrates luminance information as an auxiliary information for de-shadowing. The first branch extracts shadow-distorted features by estimating a shadow luminance map, while the second branch uses them to locate shadow regions and guide the de-shadowing. Both branches employ a hybrid feature learning mechanism to capture local and global information efficiently with lower complexity. This mechanism includes two key modules: Horizon-Vertical Attention (HVA) and Dilated Convolution Mamba (DCM). HVA models long-range pixel dependencies to propagate contextual information across the entire image to ensure global coherence and consistency. DCM utilizes dilated convolution within the State Space Model (SSM) to capture extensive contextual information and preserve local image details. Additionally, we introduce a luminance map loss to provide accurate optimization for reconstruction. Experiments on RDD, Kligler’s, Jung’s, and OSR demonstrate that LDH-Net outperforms previous state-of-the-art methods. Specifically, LDH-Net achieves the best PSNR/SSIM/LPIPS scores across all datasets, with up to 37.76 PSNR/0.981 SSIM/0.005 LPIPS on RDD datasets and consistent improvements on other benchmarks, confirming its superior performance on both visual quality and structural preservation.},
  archive      = {J_ICV},
  author       = {Fan Yang and Kunchi Li and Nanfeng Jiang and Yun Wu and Ziyu Li and Da-Han Wang},
  doi          = {10.1016/j.imavis.2025.105705},
  journal      = {Image and Vision Computing},
  month        = {10},
  pages        = {105705},
  shortjournal = {Image Vis. Comput.},
  title        = {LDH-net: Luminance-based deep hybrid network for document image de-shadowing},
  volume       = {162},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A non-local adaptive hypothesis propagation for multi-view stereo. <em>ICV</em>, <em>162</em>, 105704. (<a href='https://doi.org/10.1016/j.imavis.2025.105704'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Hypothesis propagation is a central component of PatchMatch-based multi-view stereo and significantly impacts the reconstruction performance. However, current propagation methods rely on photometric consistency to guide hypothesis propagation within a local area. When the centroid is located in a low-textured area with reflective or refractive properties, high chromatic aberration may cause the multi-view matching to fall into a local optimum that fails to provide reliable hypotheses, leading to reconstruction errors. To address this problem, we propose a non-local adaptive hypothesis propagation scheme. First, we evenly distribute sampling points in eight directions on the checkerboard to quickly determine reliable initial hypotheses. Then, starting from the initial hypotheses generated in the eight directions of the checkerboard, the hypotheses are adaptively propagated to non-checkerboard areas based on matching cost, reducing interference from unreliable photometric consistency and improving reconstruction performance in challenging areas. The test results on large-scale benchmarks show that the proposed scheme has significant advantages in reconstructing challenging areas. It can significantly improve the completeness of point clouds from current state-of-the-art methods and outperform existing propagation schemes.},
  archive      = {J_ICV},
  author       = {Yufeng Yin and Xiaoyan Liu and Qing Fan and Zichao Zhang},
  doi          = {10.1016/j.imavis.2025.105704},
  journal      = {Image and Vision Computing},
  month        = {10},
  pages        = {105704},
  shortjournal = {Image Vis. Comput.},
  title        = {A non-local adaptive hypothesis propagation for multi-view stereo},
  volume       = {162},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). SAMUNet: Enhancing pillar-based 3D object detection in autonomous driving with shape-aware mini-unet. <em>ICV</em>, <em>162</em>, 105703. (<a href='https://doi.org/10.1016/j.imavis.2025.105703'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Pillar-based 3D object detection methods outperform traditional point-based and voxel-based methods in terms of speed. However, existing methods struggle with accurately detecting large objects in complex environments due to the limitations in capturing global spatial dependencies. To address these issues, this paper proposes Shape-aware Mini-Unet Network (SAMUNet), a simple yet effective hierarchical 3D object detection network. SAMUNet incorporates multiple Sparse Mini-Unet blocks and a Shape-aware Center Head. Concretely, after converting the original point cloud into pillars, we first progressively reduce the spatial distance between distant features through downsampling in the Sparse Mini-Unet block. Then, we recover lost details through multi-scale feature fusion, enhancing the model’s ability to detect various objects. Unlike other methods, the upsampling operation in the Sparse Mini-Unet block only processes the effective feature coverage area of the intermediate feature map, significantly reducing computational costs. Finally, to further improve the accuracy of bounding box regression, we introduce Shape-aware Center Head, which models the geometric information of the bounding box’s offset direction and scale using 3D Shape-aware IoU. Extensive experiments on the nuScenes and Waymo datasets demonstrate that SAMUNet excels in detecting large objects and overall outperforms current state-of-the-art detectors, achieving 72.0% NDS and 67.7% mAP.},
  archive      = {J_ICV},
  author       = {Liping Zhu and Xuan Li and Bohui Li and Chengyang Li and Bingyao Wang and XianXiang Chang},
  doi          = {10.1016/j.imavis.2025.105703},
  journal      = {Image and Vision Computing},
  month        = {10},
  pages        = {105703},
  shortjournal = {Image Vis. Comput.},
  title        = {SAMUNet: Enhancing pillar-based 3D object detection in autonomous driving with shape-aware mini-unet},
  volume       = {162},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Phase shift guided dynamic view synthesis from monocular video. <em>ICV</em>, <em>162</em>, 105702. (<a href='https://doi.org/10.1016/j.imavis.2025.105702'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper endeavors to address the challenge of synthesizing novel views from monocular videos featuring moving objects, particularly in complex scenes with non-rigid deformations. Existing implicit representations rely on motion estimation in the spatial domain, which often struggle to capture correct temporal dynamics under such conditions. To mitigate the drawback, we propose dynamic positional encoding to represent temporal dynamics as learnable phase shifts and leverage the implicit neural representation (INR) network for iterative optimization. Utilizing optimized phase shifts as guidance enhances the representational capability of the dynamic radiance field, thereby alleviating motion ambiguity and reducing artifacts around moving objects in novel views. This paper also introduces a rational evaluation metric, referred to as “dynamic only+”, for the quantitative assessment of the rendering quality in novel views, focusing on dynamic objects and surrounding regions impacted by motion. Experimental results on multiple challenging datasets demonstrate the favorable performance of the proposed approach over state-of-the-art dynamic view synthesis methods.},
  archive      = {J_ICV},
  author       = {Chuyue Zhao and Xin Huang and Xue Wang and Guoqing Zhou and Qing Wang},
  doi          = {10.1016/j.imavis.2025.105702},
  journal      = {Image and Vision Computing},
  month        = {10},
  pages        = {105702},
  shortjournal = {Image Vis. Comput.},
  title        = {Phase shift guided dynamic view synthesis from monocular video},
  volume       = {162},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). DoseNet: Dose-adaptive prediction of the parotid glands deformation for radiotherapy planning. <em>ICV</em>, <em>162</em>, 105701. (<a href='https://doi.org/10.1016/j.imavis.2025.105701'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Parotid glands (PGs) toxicity caused by radiation-induced anatomy deformation occurs among a significant amount of patients with nasopharyngeal carcinoma treated with radiotherapy. Early prediction of PGs deformation is critical, as it can facilitate the design of treatment plans to reduce radiation-induced anatomical change in an adaptive radiotherapy workflow. Previous studies used CT images to model anatomical variation in radiotherapy. However, they did not consider the radiation dose received by the PGs which is correlated to the PGs volumetric change and can influence the anatomical variation. To address this issue, we propose DoseNet, a dose-adaptive PGs deformation prediction deep neural network, which utilizes the radiation dose and CT images to generate different anatomy predictions accommodating to the changing dose. Specifically, we use parted dose input and multi-scale cross attention to reinforce the integration of PGs anatomy and the dose received by PGs, and present a novel data augmentation method to remedy the shortcoming of the skewed data distribution of the radiation dose. Besides, to help design improved treatment plans, a novel metric termed dose volume variation (DVV) curve is developed to visualize the predicted volumetric change in respect to the dose variation of the PGs. We verify the effectiveness of our method on a dataset collected from a collaborative hospital. The experiment results show the proposed DoseNet outperforms the state-of-the-arts on the dataset and attains a Dice coefficient of 82.2% and a relative volume difference of 12.2%. The code is available at https://github.com/mkdermo/DoseNet .},
  archive      = {J_ICV},
  author       = {Bohan Yang and Yong Luo and Bo Du and Dongjing Shan and Chuan Cheng and Gang Liu and Jun Zhang and Jingnan Liu},
  doi          = {10.1016/j.imavis.2025.105701},
  journal      = {Image and Vision Computing},
  month        = {10},
  pages        = {105701},
  shortjournal = {Image Vis. Comput.},
  title        = {DoseNet: Dose-adaptive prediction of the parotid glands deformation for radiotherapy planning},
  volume       = {162},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Codebook prior-guided hybrid attention dehazing network. <em>ICV</em>, <em>162</em>, 105700. (<a href='https://doi.org/10.1016/j.imavis.2025.105700'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Transformers have been widely used in image dehazing tasks due to their powerful self-attention mechanism for capturing long-range dependencies. However, directly applying Transformers often leads to coarse details during image reconstruction, especially in complex real-world hazy scenarios. To address this problem, we propose a novel Hybrid Attention Encoder (HAE). Specifically, a channel-attention-based convolution block is integrated into the Swin-Transformer architecture. This design enhances the local features at each position through an overlapping block-wise spatial attention mechanism while leveraging the advantages of channel attention in global information processing to strengthen the network’s representation capability. Moreover, to adapt to various complex hazy environments, a high-quality codebook prior encapsulating the color and texture knowledge of high-resolution clear scenes is introduced. We also propose a more flexible Binary Matching Mechanism (BMM) to better align the codebook prior with the network, further unlocking the potential of the model. Extensive experiments demonstrate that our method consistently outperforms the second-best methods by a margin of 8% to 19% across multiple metrics on the RTTS and URHI datasets. The source code has been released at https://github.com/HanyuZheng25/HADehzeNet .},
  archive      = {J_ICV},
  author       = {Liqin Huang and Hanyu Zheng and Lin Pan and Zhipeng Su and Qiang Wu},
  doi          = {10.1016/j.imavis.2025.105700},
  journal      = {Image and Vision Computing},
  month        = {10},
  pages        = {105700},
  shortjournal = {Image Vis. Comput.},
  title        = {Codebook prior-guided hybrid attention dehazing network},
  volume       = {162},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). One-step diffusion for real-world image super-resolution via degradation removal and text prompts. <em>ICV</em>, <em>162</em>, 105699. (<a href='https://doi.org/10.1016/j.imavis.2025.105699'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Pre-trained Text-to-Image (T2I) diffusion models have shown remarkable progress in Real-world Image Super-Resolution (Real-ISR) by leveraging powerful latent space priors. However, these models typically require tens or even hundreds of diffusion steps for high-quality reconstruction, posing two critical challenges: (1) excessive computational overhead, hindering practical deployment; and (2) inherent stochasticity, leading to output uncertainty. To overcome these limitations, we propose a One-Step Diffusion framework for Real-ISR via Degradation Removal and Text Prompts (OSD-DRTP). Specifically, the proposed OSD-DRTP comprises two principal components: (1) a Degradation Removal Module (DRM), which eliminates complex real-world image degradations to restore fidelity; and (2) a Detail Enhancement Module (DEM), which integrates a fine-tuned diffusion model with text prompts from a large language model to enhance perceptual quality. In addition, we introduce Variational Score Distillation (VSD) in the latent space to ensure high-fidelity reconstruction across diverse degradation patterns. To further exploit the latent capacity of the VAE decoder, we employ a hybrid loss combining mean squared error (MSE) and perceptual loss (LPIPS), enabling accurate texture restoration without auxiliary modules. Extensive experiments demonstrate that the proposed OSD-DRTP outperforms state-of-the-art methods in both perceptual quality and computational efficiency.},
  archive      = {J_ICV},
  author       = {Yaohui Guo and Luanyuan Dai and Xinwei Gan and Yuting Huang and Miaohua Ruan and Detian Huang},
  doi          = {10.1016/j.imavis.2025.105699},
  journal      = {Image and Vision Computing},
  month        = {10},
  pages        = {105699},
  shortjournal = {Image Vis. Comput.},
  title        = {One-step diffusion for real-world image super-resolution via degradation removal and text prompts},
  volume       = {162},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Structure-aware contrastive learning for glomerulus segmentation in renal pathology. <em>ICV</em>, <em>162</em>, 105698. (<a href='https://doi.org/10.1016/j.imavis.2025.105698'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Accurate segmentation of glomeruli in renal pathology is challenging due to the difficulty in distinguishing glomeruli from surrounding tissues and their indistinct boundaries. Traditional methods often struggle with local receptive fields, primarily capturing texture rather than the overall shape of these structures. To address this issue, this paper presents a structure-aware contrastive learning strategy for precise glomerular segmentation. We implement a superpixel consistency constraint, dividing pathological images into regions of local consistency to ensure that pixels within the same area maintain feature similarity, thereby capturing structural cues of various renal tissues. The introduced loss function applies shape constraints, enabling the model to better represent the complex morphology of glomeruli against challenging backgrounds. To enhance shape consistency within glomeruli while ensuring discriminability from external tissues, we develop a contrastive learning approach that utilizes extracted structural cues. This encourages the network to effectively learn internal shape constraints and differentiate between distinct regions in feature space. Finally, we implement a multi-scale convolutional attention mechanism that integrates spatial and channel attention, improving the capture of structural features across scales. Experimental results demonstrate that our method significantly enhances segmentation accuracy across multiple public datasets, showcasing the potential of contrastive learning in renal pathology.},
  archive      = {J_ICV},
  author       = {Yuanqing Wang and Tao Wang and Xiangbo Shu and Yuhui Zheng and Jin Ding and Xianghui Fu and Zhaohui Zheng},
  doi          = {10.1016/j.imavis.2025.105698},
  journal      = {Image and Vision Computing},
  month        = {10},
  pages        = {105698},
  shortjournal = {Image Vis. Comput.},
  title        = {Structure-aware contrastive learning for glomerulus segmentation in renal pathology},
  volume       = {162},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). ECNet: An edge-guided and cross-image perception network for collaborative camouflaged object detection. <em>ICV</em>, <em>162</em>, 105697. (<a href='https://doi.org/10.1016/j.imavis.2025.105697'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Traditional camouflaged object detection (COD) methods typically focus on individual images, ignoring the contextual information from multiple related images. However, objects are often captured in multiple images or from different viewpoints in real scenarios. Leveraging collaborative information from multiple images can achieve more robust and accurate detection. This collaborative approach, known as “Collaborative Camouflaged Object Detection (CoCOD)”, addresses the limitations of single-image methods by exploiting complementary information from multiple images, enhancing detection performance. Recent advancements in CoCOD have shown notable progress. However, challenges remain in effectively extracting multi-scale features and facilitating cross-attention feature interactions. To address these limitations, we propose a novel framework, named the Edge-Guided and Cross-Image Perception Network (ECNet). The ECNet consists of two core components: the edge-guided scale module (EGSM) and the cross-image perception enhancement module (CPEM). Specifically, EGSM enhances feature extraction by integrating edge-aware guidance with multi-scale asymmetric convolutions. Meanwhile, CPEM strengthens cross-image feature interaction by introducing collaborative attention, which reinforces semantic consistency among correlated targets and suppresses distracting background information. By integrating edge-aware features across multiple spatial scales and cross-image semantic consistency, ECNet effectively addresses the challenges of camouflage detection in visually complex scenarios. Extensive experiments on the CoCOD8K dataset demonstrate that our proposed ECNet outperforms 18 state-of-the-art COD methods, 11 co-salient object detection (CoSOD) models, and 4 CoCOD approaches, as evaluated by six widely used metrics.},
  archive      = {J_ICV},
  author       = {Shiyuan Li and Hongbo Bi and Disen Mo and Cong Zhang and Yue Li},
  doi          = {10.1016/j.imavis.2025.105697},
  journal      = {Image and Vision Computing},
  month        = {10},
  pages        = {105697},
  shortjournal = {Image Vis. Comput.},
  title        = {ECNet: An edge-guided and cross-image perception network for collaborative camouflaged object detection},
  volume       = {162},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Deepfake detection using optimized VGG16-based framework enhanced with LIME for secure digital content. <em>ICV</em>, <em>162</em>, 105696. (<a href='https://doi.org/10.1016/j.imavis.2025.105696'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The rapid evolution of technologies to manipulate facial images, namely Generative Adversarial Networks (GANs) and those based on Stable Diffusion, has increased the need for effective deepfake detection mechanisms to mitigate their misuse. In this paper, the critical challenge of detecting deepfake images is addressed through a new deep learning-based approach that uses the VGG16 model after applying all necessary preprocessing steps. The VGG16 architecture was chosen for its deep structure and strong ability to capture intricate facial patterns when classifying facial images as real or manipulated. A robust preprocessing pipeline — including normalization, augmentation, facial alignment, and noise reduction — was implemented to optimize input data, improving the detection of subtle manipulations. Additionally, Explainable AI (XAI) techniques, such as the Local Interpretable Model-agnostic Explanations (LIME) framework, were integrated to provide transparent, visual explanations of the model’s predictions, enhancing interpretability and user trust. To further assess generalizability, the evaluation was extended beyond the initial dataset by incorporating three additional benchmark datasets: FaceForensics++, Celeb-DF (v2), and the DFDC Preview Set. These datasets contain a range of manipulation techniques, allowing for comprehensive testing of the model’s robustness across different scenarios. The proposed method outperformed baselines with exceptional performance metrics (accuracy, precision, recall, and F1-score up to 0.99), and maintained strong results across different datasets. These findings demonstrate that combining XAI approaches with a VGG16 model and thorough preprocessing effectively counters advanced deepfake generation techniques, such as StyleGAN2. This research contributes to a safer digital landscape by improving the detection and understanding of manipulated content, providing a practical way to confront the growing threat of deepfakes.},
  archive      = {J_ICV},
  author       = {Asma Aldrees and Nihal Abuzinadah and Muhammad Umer and Dina Abdulaziz AlHammadi and Shtwai Alsubai and Raed Alharthi},
  doi          = {10.1016/j.imavis.2025.105696},
  journal      = {Image and Vision Computing},
  month        = {10},
  pages        = {105696},
  shortjournal = {Image Vis. Comput.},
  title        = {Deepfake detection using optimized VGG16-based framework enhanced with LIME for secure digital content},
  volume       = {162},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Synergistic-aware cascaded association and trajectory refinement for multi-object tracking. <em>ICV</em>, <em>162</em>, 105695. (<a href='https://doi.org/10.1016/j.imavis.2025.105695'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-object tracking (MOT) is a pivotal research area in computer vision. Effectively tracking objects in scenarios with frequent occlusions and crowded scenes has become a key challenge in MOT tasks. Existing tracking-by-detection (TbD) methods often rely on simple two-frame association techniques. However, in situations involving scale transformation or requiring long-term association, frequent occlusion between objects can lead to ID switches, especially in scenes with dense or highly intersecting objects. Therefore, we propose a synergistic-aware cascaded association and trajectory refinement method (SCTrack) for multi-object tracking. In the data association stage, we propose a synergistic-aware cascaded association method to construct a multi-perception affinity matrix for object association, and introduce the multi-frame collaborative distance calculation to enhance the robustness. To address the problem of trajectory fragmentation, we propose a dynamic confidence-driven trajectory refinement post-processing method. This method integrates confidence and feature information to calculate trajectory association, repair fragmented trajectories, and improve the overall robustness of the tracking algorithm. Extensive experiments on the MOT17, MOT20, and DanceTrack datasets validate SCTrack’s competitive performance.},
  archive      = {J_ICV},
  author       = {Hui Li and Su Qin and Saiyu Li and Ying Gao and Yanli Wu},
  doi          = {10.1016/j.imavis.2025.105695},
  journal      = {Image and Vision Computing},
  month        = {10},
  pages        = {105695},
  shortjournal = {Image Vis. Comput.},
  title        = {Synergistic-aware cascaded association and trajectory refinement for multi-object tracking},
  volume       = {162},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Enhancing cross-domain generalization in retinal image segmentation via style randomization and style normalization. <em>ICV</em>, <em>162</em>, 105694. (<a href='https://doi.org/10.1016/j.imavis.2025.105694'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Retinal image segmentation is a crucial procedure for automatically diagnosing ophthalmic diseases. However, existing deep learning-based segmentation models suffer from the domain shift issue, i.e., the segmentation accuracy decreases significantly when the test and training images are sampled from different distributions. To overcome this issue, we focus on the challenging single-source domain generalization scenario, where we expect to train a well-generalized segmentation model on unseen test domains with only access to one domain during training. In this paper, we present a style randomization method, which performs random scaling transformation to the LAB components of the training image, to enrich the style diversity. Furthermore, we present a style normalization method to effectively normalize style information while preserving content by channel-wise feature standardization and dynamic feature affine transformation. Our approach is evaluated on four types of retinal image segmentation tasks, including retinal vessel, optic cup, optic disc, and hard exudate. Experimental results demonstrate that our method achieves competitive or superior performance compared to state-of-the-art approaches. Specifically, it outperforms the second-best method by 3.9%, 2.6%, and 4.8% on vessel, optic cup, and hard exudate segmentation tasks, respectively. Our code will be released at https://github.com/guomugong/SRN .},
  archive      = {J_ICV},
  author       = {Song Guo},
  doi          = {10.1016/j.imavis.2025.105694},
  journal      = {Image and Vision Computing},
  month        = {10},
  pages        = {105694},
  shortjournal = {Image Vis. Comput.},
  title        = {Enhancing cross-domain generalization in retinal image segmentation via style randomization and style normalization},
  volume       = {162},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A deep learning approach for contrast-agent-free breast lesion detection and classification using adversarial synthesis of contrast-enhanced mammograms. <em>ICV</em>, <em>162</em>, 105692. (<a href='https://doi.org/10.1016/j.imavis.2025.105692'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Contrast-enhanced digital mammography (CEDM) has emerged as a promising complementary imaging modality for breast cancer diagnosis, offering enhanced lesion visualization and improved diagnostic accuracy, particularly for patients with dense breast tissues. However, the reliance of CEDM on contrast agents poses challenges to patient safety and accessibility. To overcome those challenges, this paper introduces a deep learning methodology for improved breast lesion detection and classification. In particular, an image-to-image translation model based on cycle-consistent generative adversarial networks (CycleGAN) is utilized to generate synthetic CEDM (SynCEDM) images from full-field digital mammography in order to enhance visual contrast perception without the need for contrast agents. A new dataset of 3958 pairs of low-energy (LE) and CEDM images was collected from 2908 female subjects to train the CycleGAN model to generate SynCEDM images. Thus, we trained different You-Only-Look-Once (YOLO) architectures on CEDM and SynCEDM images for breast lesion detection and classification. SynCEDM images were generated with a structural similarity index (SSIM) of 0.94 ± 0.02. A YOLO lesion detector trained on original CEDM images led to a 91.34% accuracy, a 90.37% sensitivity, and a 92.06% specificity. In comparison, a detector trained on the SynCEDM images exhibited a comparable accuracy of 91.20%, a marginally higher sensitivity of 91.44%, and a slightly lower specificity of 91.30%. This approach not only aims to mitigate contrast agent risks but also to improve breast cancer detection and characterization using mammography.},
  archive      = {J_ICV},
  author       = {Manar N. Amin and Muhammad A. Rushdi and Rasha Kamal and Amr Farouk and Mohamed Gomaa and Noha M. Fouad and Ahmed M. Mahmoud},
  doi          = {10.1016/j.imavis.2025.105692},
  journal      = {Image and Vision Computing},
  month        = {10},
  pages        = {105692},
  shortjournal = {Image Vis. Comput.},
  title        = {A deep learning approach for contrast-agent-free breast lesion detection and classification using adversarial synthesis of contrast-enhanced mammograms},
  volume       = {162},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Spatiotemporal XAI: Explaining video regression models in echocardiography videos for ejection fraction prediction. <em>ICV</em>, <em>162</em>, 105691. (<a href='https://doi.org/10.1016/j.imavis.2025.105691'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep learning has showcased unprecedented success in automating echocardiography analysis. However, most of the deep learning algorithms are hindered at clinical translation due to their black-box nature. This paper aims to tackle this issue by quantitatively evaluating video regression models’ focus on the left ventricle (LV) for ejection fraction (EF) prediction task spatiotemporally in apical 4 chamber (A4C) echocardiograms using a gradient-based saliency method. We performed a quantitative evaluation to assess the ratio of how many of the maximum absolute gradient values of the deep learning models fall on the left ventricle for the video regression task of ejection fraction prediction. Then, we extend the experiment and pick the most important gradients as the segmentation size and check the ratio of intersection. Finally, we picked temporally aligned sub-clips from end diastole to end systole and calculated the expected accuracies of the mentioned metrics in time. All tests are performed in 3 different models with different architectures and results are examined quantitatively. The filtered test set includes 1209 A4C echo videos of with mean EF of 55.5%. Trained models showed 0.73 to 0.83 Pointing Game scores, where it was 0.09 for the baseline random model. m G T intersection score was 0.46 to 0.50 for the trained models, whereas the random model’s score was 0.18. Models have higher pointing game scores on the end diastole and end systole compared to intermediate frames. Transformer based models’ m G T intersection scores were negatively correlated with their error rate. All models located the left ventricle successfully and their localization performance was generally better in semantically important frames rather than the larger target area. This observation from the spatiotemporal analysis suggests possible clinical relevance to model reasoning.},
  archive      = {J_ICV},
  author       = {Yakup Abrek Er and Arda Guler and Mehmet Cagri Demir and Hande Uysal and Gamze Babur Guler and Ilkay Oksuz},
  doi          = {10.1016/j.imavis.2025.105691},
  journal      = {Image and Vision Computing},
  month        = {10},
  pages        = {105691},
  shortjournal = {Image Vis. Comput.},
  title        = {Spatiotemporal XAI: Explaining video regression models in echocardiography videos for ejection fraction prediction},
  volume       = {162},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). InpaintingPose: Enhancing human pose transfer by image inpainting. <em>ICV</em>, <em>162</em>, 105690. (<a href='https://doi.org/10.1016/j.imavis.2025.105690'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Human pose transfer involves transforming a human subject in a reference image from a source pose to a target pose while maintaining consistency in both appearance and background. Most existing methods treat the appearance and background in the reference image as a unified entity, which causes the background to be disrupted by pose transformations and prevents the model from focusing on the complex relationship between appearance and pose. In this paper, we propose InpaintingPose, a novel human pose transfer framework based on image inpainting, which enables precise pose control without affecting the background. InpaintingPose separates the background from the appearance, applying transformations only where necessary. This strategy prevents the background from being affected by pose transformations and allows the model to focus on the coupling between appearance and pose. Additionally, we introduce an appearance control mechanism to ensure appearance consistency between the generated images and the reference images. Finally, we propose an initial noise optimization strategy to address the instability in generating human images with extremely bright backgrounds. By decoupling appearance and background, InpaintingPose can also recombine the appearance and background from different reference images to produce realistic human images. Extensive experiments demonstrate the effectiveness of our method, achieving state-of-the-art FID scores of 4.74 and 26.74 on DeepFashionv2 and TikTok datasets, respectively, significantly outperforming existing approaches.},
  archive      = {J_ICV},
  author       = {Wei Zhang and Chenglin Zhou and Xuekang Peng and Zhichao Lian},
  doi          = {10.1016/j.imavis.2025.105690},
  journal      = {Image and Vision Computing},
  month        = {10},
  pages        = {105690},
  shortjournal = {Image Vis. Comput.},
  title        = {InpaintingPose: Enhancing human pose transfer by image inpainting},
  volume       = {162},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Advancements in basketball action recognition: Datasets, methods, explainability, and synthetic data applications. <em>ICV</em>, <em>162</em>, 105689. (<a href='https://doi.org/10.1016/j.imavis.2025.105689'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Basketball Action Recognition (BAR) has received increasing attention in the fields of computer vision and artificial intelligence, serving as a fundamental component in performance evaluation, automated game annotation, tactical analysis, and referee decision-making support. Despite notable advancements driven by deep learning approaches, BAR remains a challenging task due to the inherent complexity of basketball movements, frequent occlusions, and limited availability of standardized benchmark datasets. This survey provides a comprehensive and structured synthesis of current developments in BAR research, encompassing four principal dimensions: dataset curation, computational methodologies, synthetic data generation, and model explainability. A critical analysis of publicly available basketball-specific datasets is presented, delineating their modalities, annotation strategies, action taxonomies, and representational scope. Furthermore, the survey offers a structured classification of state-of-the-art action recognition methodologies, ranging from video-based and skeleton-based models to sensor-driven and multimodal fusion approaches, emphasizing architectural characteristics, evaluation protocols, and task-specific adaptations. The role of synthetic data is systematically examined as a means to address data scarcity, reduce annotation noise, and enhance model generalization through controlled variability and simulation-based augmentation. In parallel, the integration of explainable artificial intelligence (XAI) techniques is also analyzed, with a focus on post-hoc attribution methods, probabilistic reasoning models, and interpretable neural architectures, aimed at improving the transparency and accountability of decision-making processes. The survey identifies persisting research challenges, including dataset heterogeneity, limitations in cross-domain transferability, and the accuracy-interpretability trade-off in deep models. By delineating current limitations and prospective directions, this work provides a foundational reference to guide the development of robust, generalizable, and explainable BAR systems for deployment in real-world sports intelligence applications.},
  archive      = {J_ICV},
  author       = {Marco Caruso and Lucia Cimmino and Fabio Narducci and Chiara Pero and Gianluca Ronga},
  doi          = {10.1016/j.imavis.2025.105689},
  journal      = {Image and Vision Computing},
  month        = {10},
  pages        = {105689},
  shortjournal = {Image Vis. Comput.},
  title        = {Advancements in basketball action recognition: Datasets, methods, explainability, and synthetic data applications},
  volume       = {162},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). DBNet: A depth-guided and boundary-aware network for amodal instance segmentation. <em>ICV</em>, <em>162</em>, 105688. (<a href='https://doi.org/10.1016/j.imavis.2025.105688'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Amodal instance segmentation aims to predict the amodal masks of objects, including both visible and occluded regions. Due to the high similarity between intra-class objects and the intricate shapes of objects, existing methods face challenges in accurately locating and segmenting occluded objects. To address the above issues and achieve high-quality amodal instance segmentation, we propose a depth-guided and boundary-aware network (DBNet), which simulates the process of human perception of occluded objects and leverages depth and boundary cues to efficiently segment objects in occluded scenes. DBNet consists of three key designs: the Channel Adaptive Fusion Module (CAFM), the Boundary-Aware Attention Module (BAAM) and the Coarse-to-Fine Segmentation Architecture (CFSA). Specifically, CAFM adaptively learns the feature weights of RGB and depth modalities to generate robust feature representation. BAAM aggregates global contextual information into boundary features with the objective of learning the shapes of occluded objects. These two modules are crucial for accurately distinguishing complex occlusions. Additionally, CFSA progressively refines the segmentation results, yielding more accurate amodal masks. Extensive experiments on D2SA, KINS, and COCOA-cls datasets demonstrate that DBNet consistently outperforms existing state-of-the-art methods, verifying its effectiveness in addressing occlusions.},
  archive      = {J_ICV},
  author       = {Shihui Zhang and Haonan Yang and Jiawei Zhang and Xinyu Wang},
  doi          = {10.1016/j.imavis.2025.105688},
  journal      = {Image and Vision Computing},
  month        = {10},
  pages        = {105688},
  shortjournal = {Image Vis. Comput.},
  title        = {DBNet: A depth-guided and boundary-aware network for amodal instance segmentation},
  volume       = {162},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). EGU-GS: Efficient gaussian utilization for real-time 3D gaussian splatting. <em>ICV</em>, <em>162</em>, 105687. (<a href='https://doi.org/10.1016/j.imavis.2025.105687'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, 3D Gaussian Splatting (3DGS) has garnered significant attention for its superior rendering quality and real-time performance. However, the inefficient utilization of Gaussians in 3DGS necessitates the use of millions of Gaussian primitives to adapt to the geometry and appearance of 3D scenes, leading to significant redundancy. To address this issue, we propose an efficient adaptive density control strategy that incorporates Cross-Section-Oriented splitting and Heterogeneous cloning operations. These modifications prevent the proliferation of redundant Gaussians and improve Gaussian utilization. Furthermore, we introduce opacity adaptive pruning, adaptive thresholds, and Gaussian importance weights to refine the Gaussian selection process. Our post-processing Gaussian refinement pruning further eliminates small-scale and low-opacity Gaussians. Experimental results on various challenging datasets demonstrate that our method achieves state-of-the-art rendering quality while consuming less storage space, reducing the number of Gaussians by up to 42% compared to 3DGS. The code is available at: https://github.com/zhiyu-cv/EGU .},
  archive      = {J_ICV},
  author       = {Zhiyu Zheng and Dake Zhou and Yiming Shao and Xin Yang},
  doi          = {10.1016/j.imavis.2025.105687},
  journal      = {Image and Vision Computing},
  month        = {10},
  pages        = {105687},
  shortjournal = {Image Vis. Comput.},
  title        = {EGU-GS: Efficient gaussian utilization for real-time 3D gaussian splatting},
  volume       = {162},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Pose-guided token selection for the recognition of activities of daily living. <em>ICV</em>, <em>162</em>, 105686. (<a href='https://doi.org/10.1016/j.imavis.2025.105686'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Large pre-trained video transformers are becoming the standard architecture for video processing due to their exceptional accuracy. However, their computational complexity has been a major obstacle to their practical application in problems that require the recognition of precise motion patterns in video, such as in the recognition of Activities of Daily Living (ADL). Techniques like token pruning help mitigate their computational cost, but overlook some specific aspects of this task such as the actor movement. To address this we propose an improved token selection method that integrates semantic information from the ADL recognition task with that of human motion. Our model relies on a multi-task architecture that infers human pose and activity classification from RGB videos. We show that guiding token pruning with motion information significantly improves the trade-off between higher efficiency, obtained by reducing the number of tokens, and accuracy of the classification task. We evaluate our model on three popular ADL recognition benchmarks with their respective cross-subject and cross-view setups. In our experiments, a video transformer modified with our proposed modules sets a new state-of-the-art on the ADL recognition task whilst achieving significant reductions in computational cost.},
  archive      = {J_ICV},
  author       = {Ricardo Pizarro and Roberto Valle and José M. Buenaposada and Luis M. Bergasa and Luis Baumela},
  doi          = {10.1016/j.imavis.2025.105686},
  journal      = {Image and Vision Computing},
  month        = {10},
  pages        = {105686},
  shortjournal = {Image Vis. Comput.},
  title        = {Pose-guided token selection for the recognition of activities of daily living},
  volume       = {162},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Empowering cardiovascular diagnostics with SET-MobileNet: A lightweight and accurate deep learning based classification approach. <em>ICV</em>, <em>162</em>, 105684. (<a href='https://doi.org/10.1016/j.imavis.2025.105684'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cardiovascular diseases (CVDs) remain the leading cause of mortality worldwide, necessitating early detection and accurate diagnosis for improved patient outcomes. This study introduces SET-MobileNet, a lightweight deep learning model designed for automated heart sound classification, integrating transformers to capture long-range dependencies and squeeze-and-excitation (SE) blocks to emphasize relevant acoustic features while suppressing noise artifacts. Unlike traditional methods that rely on handcrafted features, SET-MobileNet employs a multimodal feature extraction approach, incorporating log-mel spectrograms, Mel-Frequency Cepstral Coefficients (MFCCs), chroma features, and zero-crossing rates to enhance classification robustness. The model is evaluated across multiple publicly available heart sound datasets, including CirCor, HSS, GitHub, and Heartbeat Sounds, achieving a state-of-the-art accuracy of 99.95% for 2.0-second heart sound segments in the CirCor dataset. Extensive experiments demonstrate that multimodal feature representations significantly improve classification performance by capturing both time-frequency and spectral characteristics of heart sounds. SET-MobileNet is computationally efficient, with a model size of 8.61 MB and single-sample inference times under 6.5 ms, making it suitable for real-time deployment on mobile and embedded devices. Ablation studies confirm the contributions of transformers and SE blocks, showing incremental improvements in accuracy and noise suppression.},
  archive      = {J_ICV},
  author       = {Zunair Safdar and Jinfang Sheng and Muhammad Usman Saeed and Muhammad Ramzan and A. Al-Zubaidi},
  doi          = {10.1016/j.imavis.2025.105684},
  journal      = {Image and Vision Computing},
  month        = {10},
  pages        = {105684},
  shortjournal = {Image Vis. Comput.},
  title        = {Empowering cardiovascular diagnostics with SET-MobileNet: A lightweight and accurate deep learning based classification approach},
  volume       = {162},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Estimating blood pressure using video-based PPG and deep learning. <em>ICV</em>, <em>162</em>, 105683. (<a href='https://doi.org/10.1016/j.imavis.2025.105683'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper introduces a novel pipeline for estimating systolic and diastolic blood pressure using remote photoplethysmographic (rPPG) signals derived from video recordings of subjects’ faces. The pipeline consists of three main stages: rPPG signal extraction, denoising to transform the rPPG signal into a PPG-like waveform, and blood pressure estimation. This approach directly addresses the current lack of datasets that simultaneously include video, rPPG, and blood pressure data. To overcome this, the proposed pipeline leverages the extensive availability of PPG-based blood pressure estimation techniques, in combination with state-of-the-art algorithms for rPPG extraction, enabling the generation of reliable PPG-like signals from video input. To validate the pipeline, we conducted comparative analyses with state-of-the-art methods at each stage and collected a dedicated dataset through controlled laboratory experimentation. The results demonstrate that the proposed solution effectively captures blood pressure information, achieving a mean error of 9.2 ± 11.3 mmHg for systolic and 8.6 ± 9.1 mmHg for diastolic blood pressure. Moreover, the denoised rPPG signals show a strong correlation with conventional PPG signals, supporting the reliability of the transformation process. This non-invasive and contactless method offers considerable potential for long-term blood pressure monitoring, particularly in Ambient Assisted Living (AAL) systems, where unobtrusive and continuous health monitoring is essential.},
  archive      = {J_ICV},
  author       = {Gianluca Zaza and Gabriella Casalino and Sergio Caputo and Giovanna Castellano},
  doi          = {10.1016/j.imavis.2025.105683},
  journal      = {Image and Vision Computing},
  month        = {10},
  pages        = {105683},
  shortjournal = {Image Vis. Comput.},
  title        = {Estimating blood pressure using video-based PPG and deep learning},
  volume       = {162},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Diverse information aggregation with adaptive graph construction and prompts for deepfake detection. <em>ICV</em>, <em>162</em>, 105682. (<a href='https://doi.org/10.1016/j.imavis.2025.105682'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Due to the misuse of face manipulation techniques, there has been increasing attention on deepfake detection. Recently, some methods have employed ViTs to capture the inconsistency in forged faces, providing a global perspective for exploring diverse and generalized patterns to avoid overfitting. These methods typically divided an image into fixed-shape patches. However, each patch contains only a tiny fraction of facial regions, thereby inherently lacking explicit semantic and structural relations with other patches, which is insufficient to model the global context information effectively. To enhance the global context interaction, a Diverse INformation Aggregation (DINA) framework is proposed for deepfake detection, which consists of two information aggregation modules: Adaptive Graph Convolution Network (AGCN) and Multi-Scale Prompt Fusion (MSPF). Specifically, the AGCN utilizes a novel strategy to construct neighbors of each token based on spatial and feature relations. Then, a graph convolution network is applied to aggregate information from different tokens to form a token with rich semantics and local information, termed the group token. These group tokens can be used to form robust representations of global information. Moreover, the MSPF utilizes prompts to incorporate unique forgery traces from complementary information, i.e., multi-scale and frequency information, into group tokens in a fine-grained and adaptive manner, which provides extra information to further improve the robustness of group tokens. Consequently, our model can learn robust global context-aware representations, capturing more generalized forgery patterns from global information. The proposed framework outperforms the state-of-the-art competitors on several benchmarks, showing the generalization ability of our method.},
  archive      = {J_ICV},
  author       = {Zhenhua Bai and Qiangchang Wang and Lu Yang and Xinxin Zhang and Yanbo Gao and Yilong Yin},
  doi          = {10.1016/j.imavis.2025.105682},
  journal      = {Image and Vision Computing},
  month        = {10},
  pages        = {105682},
  shortjournal = {Image Vis. Comput.},
  title        = {Diverse information aggregation with adaptive graph construction and prompts for deepfake detection},
  volume       = {162},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Semantic-consistency multi-view deep subspace clustering network with frequency branches. <em>ICV</em>, <em>162</em>, 105681. (<a href='https://doi.org/10.1016/j.imavis.2025.105681'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-view subspace clustering (MvSC) can utilize high-dimensional data from different perspectives of the same target to segment them into multiple low-dimensional subspaces. However, existing methods primarily focus on the spatial information of the data, often neglecting frequency information that typically contains crucial features for data identification and characterization. Additionally, retaining sufficient consistent information across different views while removing view-specific redundant information is a significant challenge in MvSC. In this paper, we propose a semantic-consistency multi-view deep subspace clustering network to address these issues. The proposed model endows an encoder with a frequency branch to capture both spatial and frequency domain information, enriching hidden layer features. To obtain semantically consistent representations across views, we employ a feature integration module with mutual information maximization to enable the model to learn a better self-representation matrix. In addition, we introduce a multi-view information bottleneck loss to suppress unique information from individual views, thereby improving clustering performance. Our experiments demonstrate the effectiveness of the proposed model, showing superior performance compared to mainstream methods.},
  archive      = {J_ICV},
  author       = {Mengran Hou and Junmin Liu and Zengjie Song and Yongjun Wang},
  doi          = {10.1016/j.imavis.2025.105681},
  journal      = {Image and Vision Computing},
  month        = {10},
  pages        = {105681},
  shortjournal = {Image Vis. Comput.},
  title        = {Semantic-consistency multi-view deep subspace clustering network with frequency branches},
  volume       = {162},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). MMCFNet: Multi-scale and multi-modal complementary fusion network for light field salient object detection. <em>ICV</em>, <em>162</em>, 105680. (<a href='https://doi.org/10.1016/j.imavis.2025.105680'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Light field salient object detection (LFSOD) has received growing attention in recent years. Light field cameras record the direction and intensity of light in a scene, and they provide focal stacks and all-focus images with different but complementary characteristics. Previous LFSOD models lack effective feature fusion for multi-scale and multi-modal information, which leads to background interference or incomplete salient objects. In this paper, we propose a new multi-scale and multi-modal complementary fusion network (MMCFNet) for LFSOD. For the focal stacks, we design a slice interweaving enhancement module (SIEM) to emphasize the useful features among different slices and reduce inconsistency. In addition, we propose a new multi-scale and multi-modal fusion strategy, which contains high-level feature fusion module (HFFM), cross attention module (CrossA), and compact pyramid refinement (CPR) module. The HFFM fuses high-level multi-scale and multi-modal semantic information to accurately locate salient objects. The CrossA enhances low-level spatial-channel information and refines salient object edges. Finally, we use the CPR module to aggregate the multi-scale information and decode it into high-quality saliency maps. Extensive experiments on public datasets show that our method outperforms 11 state-of-the-art LFSOD methods.},
  archive      = {J_ICV},
  author       = {Xin Hu and Fen Chen and Zongju Peng and Lian Huang and Jiawei Xu},
  doi          = {10.1016/j.imavis.2025.105680},
  journal      = {Image and Vision Computing},
  month        = {10},
  pages        = {105680},
  shortjournal = {Image Vis. Comput.},
  title        = {MMCFNet: Multi-scale and multi-modal complementary fusion network for light field salient object detection},
  volume       = {162},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). FOCUS: Improving fire detection on videos by scenario adaptation. <em>ICV</em>, <em>162</em>, 105679. (<a href='https://doi.org/10.1016/j.imavis.2025.105679'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Fire detection from video is effective for most video surveillance applications. The algorithm that processes the video acquired by cameras in real time has a twofold goal: detect as many fires as possible and keep the number of false alarms low. While existing approaches obtain the first goal, they often produce many false alarms due to their inability to account for the specific characteristics of diverse application environments. This paper introduces Fire Observation and Control Using Scenarios (FOCUS), a novel configurable fire detection method designed to bridge the gap between the literature methods and the application needs by exploiting scenario-specific knowledge. FOCUS leverages scalable and configurable modules for robust fire detection, incorporating three key steps: (1) fire detection, which identifies potential fire regions using visual cues; (2) fire candidate filtering through motion analysis, to eliminate false positives by analyzing the dynamic behavior of the identified fire candidates; (3) a vision–language model, which evaluates and confirms fire alarms by correlating visual evidence with contextual knowledge. By tailoring the configuration to the scenario and integrating the advanced filtering mechanisms according to the complexity of the environment, FOCUS improves performance in all the considered application scenarios. The analysis of the results shows that the proposed approach outperforms existing methods demonstrating higher resilience on real data, which enables its usage in real-world applications.},
  archive      = {J_ICV},
  author       = {Diego Gragnaniello and Antonio Greco and Carlo Sansone and Bruno Vento},
  doi          = {10.1016/j.imavis.2025.105679},
  journal      = {Image and Vision Computing},
  month        = {10},
  pages        = {105679},
  shortjournal = {Image Vis. Comput.},
  title        = {FOCUS: Improving fire detection on videos by scenario adaptation},
  volume       = {162},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Combining spatio-temporal attention and multi-level feature fusion for video saliency prediction. <em>ICV</em>, <em>162</em>, 105678. (<a href='https://doi.org/10.1016/j.imavis.2025.105678'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, 3D convolution-based video saliency prediction models have adopted a fully convolutional encoder-decoder architecture to extract multi-level spatio-temporal features and achieved impressive performance. Deep level features encompass semantic information reflecting salient regions, shallow level features contain detailed information. However, these models have two issues: they fail to capture global information, and the equally weighted fusion mechanism they employ ignores the differences between deep and shallow features. To address these issues, we propose a novel model that combines spatio-temporal attention and multi-level feature fusion, with two main component, the global spatio-temporal correlation (GSC) structure and the attention-guided fusion (AGF) module. The GSC structure employs the Video Swin Transformer to capture global spatio-temporal correlations based on the deepest local spatio-temporal features through the multi-head attention mechanism. Rather than the equally weighted fusion mechanism, the proposed AGF module adaptively compute an attention map with only deep level features through spatio-temporal attention and channel attention branches, which guides the features to focus on salient regions and fuse. Extensive experiments over four datasets demonstrate the proposed model achieves comparable performance against state-of-the-art models and the effectiveness of each component of our model.},
  archive      = {J_ICV},
  author       = {Huiyu Luo},
  doi          = {10.1016/j.imavis.2025.105678},
  journal      = {Image and Vision Computing},
  month        = {10},
  pages        = {105678},
  shortjournal = {Image Vis. Comput.},
  title        = {Combining spatio-temporal attention and multi-level feature fusion for video saliency prediction},
  volume       = {162},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Graph hashing network for image retrieval. <em>ICV</em>, <em>162</em>, 105677. (<a href='https://doi.org/10.1016/j.imavis.2025.105677'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep supervised hashing is more popular among researchers due to its satisfactory computational efficiency and retrieval performance. Most existing models learn hash codes for data by constructing inter-sample pair-wise or triplet losses, allowing for consideration of the topological information from the label space. However, the topological relationships among samples in the feature space are not fully explored, which may result in less discriminative hash codes. To address this issue, we propose a novel graph hashing network (GHash) for image retrieval. Our GHash explores positional relationships among samples under a large receptive field through alternating updates of graph nodes and edges, generating high-quality image descriptors based on optimized positional relationships and neighborhood information. Subsequently, graph-level descriptors are mapped into highly discriminative hash codes. Additionally, we introduce an extra classification loss to enhance the accuracy of the topological relationships among samples in the graph by supervising the learning of edge features. Finally, we conduct extensive comparison and ablation experiments on three benchmark datasets, with results demonstrating that our method achieves superior retrieval performance compared to state-of-the-art deep hashing methods.},
  archive      = {J_ICV},
  author       = {Xudong Zhou and Jun Tang and Ke Wang and Nian Wang and Han Chen},
  doi          = {10.1016/j.imavis.2025.105677},
  journal      = {Image and Vision Computing},
  month        = {10},
  pages        = {105677},
  shortjournal = {Image Vis. Comput.},
  title        = {Graph hashing network for image retrieval},
  volume       = {162},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). BTMTrack: Robust RGB-T tracking via dual-template bridging and temporal-modal candidate elimination. <em>ICV</em>, <em>162</em>, 105676. (<a href='https://doi.org/10.1016/j.imavis.2025.105676'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {RGB-T tracking leverages the complementary strengths of RGB and thermal infrared (TIR) modalities to handle challenging scenarios, such as low illumination and adverse weather conditions. However, existing methods often struggle to effectively integrate temporal information and perform efficient cross-modal interactions, limiting their adaptability to dynamic targets. In this paper, we propose BTMTrack, a novel RGB-T tracking framework. At its core lies a dual-template backbone and a Temporal-Modal Candidate Elimination (TMCE) strategy. The dual-template backbone enables the effective integration of temporal information. At the same time, the TMCE strategy guides the model to focus on target-relevant tokens by evaluating temporal and modal correlations through attention correlation maps across different modalities. This not only reduces computational overhead but also mitigates the influence of irrelevant background noise. Building on this foundation, we introduce the Temporal Dual-Template Bridging (TDTB) module, which utilizes a cross-modal attention mechanism to process dynamically filtered tokens, thereby enhancing precise cross-modal fusion. This approach further strengthens the interaction between templates and the search region. Extensive experiments conducted on three benchmark datasets demonstrate the effectiveness of BTMTrack. Our method achieves state-of-the-art performance, with a 72.3% precision rate on the LasHeR test set and competitive results on the RGBT210 and RGBT234 datasets.},
  archive      = {J_ICV},
  author       = {Zhongxuan Zhang and Bi Zeng and Xinyu Ni and Yimin Du},
  doi          = {10.1016/j.imavis.2025.105676},
  journal      = {Image and Vision Computing},
  month        = {10},
  pages        = {105676},
  shortjournal = {Image Vis. Comput.},
  title        = {BTMTrack: Robust RGB-T tracking via dual-template bridging and temporal-modal candidate elimination},
  volume       = {162},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A method for skin lesion detection and localization by means of deep learning and reliable prediction explainability. <em>ICV</em>, <em>162</em>, 105675. (<a href='https://doi.org/10.1016/j.imavis.2025.105675'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Skin lesions are any abnormal growths or appearances on the skin, ranging from benign (i.e., non-cancerous) to malignant (i.e., cancerous). The identification of a skin lesion is a crucial task that is carried out in short periods of time to initiate an eventual therapeutic treatment. In this paper, we propose a method for automatic skin lesion detection, implementing Convolutional Neural Networks. Moreover, with the aim of providing a rationale behind the model prediction, we also consider explainability by adopting two different Class Activation Mapping algorithms, which highlight regions in skin images that most contribute to the network’s classification decision. We also include the indices of similarity for further quantitative analysis. Several Convolutional Neural Networks are considered, by obtaining the best results with the MobileNet model, achieving an accuracy equal to 0.935 in skin lesion detection. Moreover, in the experimental analysis, we discuss the effectiveness of Class Activation Mapping algorithms exploited for skin lesion localization.},
  archive      = {J_ICV},
  author       = {Marcello Di Giammarco and Antonella Santone and Mario Cesarelli and Fabio Martinelli and Francesco Mercaldo},
  doi          = {10.1016/j.imavis.2025.105675},
  journal      = {Image and Vision Computing},
  month        = {10},
  pages        = {105675},
  shortjournal = {Image Vis. Comput.},
  title        = {A method for skin lesion detection and localization by means of deep learning and reliable prediction explainability},
  volume       = {162},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). CoHAtNet: An integrated convolutional-transformer architecture with hybrid self-attention for end-to-end camera localization. <em>ICV</em>, <em>162</em>, 105674. (<a href='https://doi.org/10.1016/j.imavis.2025.105674'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Camera localization refers to the process of automatically determining the position and orientation of a camera within its 3D environment from the images it captures. Traditional camera localization methods often rely on Convolutional Neural Networks, which are effective at extracting local visual features but struggle to capture long-range dependencies critical for accurate localization. In contrast, Transformer-based approaches model global contextual relationships appropriately, although they often lack precision in fine-grained spatial representations. To bridge this gap, we introduce CoHAtNet, a novel Convolutional Hybrid-Attention Network that tightly integrates convolutional and self-attention mechanisms. Unlike previous hybrid models that stack convolutional and attention layers separately, CoHAtNet embeds local features extracted via Mobile Inverted Bottleneck Convolution blocks directly into the Value component of the self-attention mechanism of Transformers. This yields a hybrid self-attention block capable of dynamically capturing both local spatial detail and global semantic context within a single attention layer. Additionally, CoHAtNet enables modality-level fusion by processing RGB and depth data jointly in a unified pipeline, allowing the model to leverage complementary appearance and geometric cues throughout. Extensive evaluations have been conducted on two widely-used camera localization datasets: 7-Scenes (RGB-D) and Cambridge Landmarks (RGB). Experimental results show that CoHAtNet achieves state-of-the-art performance in both translation and orientation accuracy. These results highlight the effectiveness of our hybrid design in challenging indoor and outdoor environments. This makes CoHAtNet a strong candidate for end-to-end camera localization tasks.},
  archive      = {J_ICV},
  author       = {Hussein Hasan and Miguel Angel Garcia and Hatem Rashwan and Domenec Puig},
  doi          = {10.1016/j.imavis.2025.105674},
  journal      = {Image and Vision Computing},
  month        = {10},
  pages        = {105674},
  shortjournal = {Image Vis. Comput.},
  title        = {CoHAtNet: An integrated convolutional-transformer architecture with hybrid self-attention for end-to-end camera localization},
  volume       = {162},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Distributed collaborative machine learning in real-world application scenario: A white blood cell subtypes classification case study. <em>ICV</em>, <em>162</em>, 105673. (<a href='https://doi.org/10.1016/j.imavis.2025.105673'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {White blood cell (WBC) subtype classification is a critical step in monitoring an individual’s health. However, it remains a challenging task due to the significant morphological variability of WBCs and the domain shift introduced by differing acquisition protocols across hospitals. Numerous approaches have been proposed to mitigate domain shift, including supervised and unsupervised domain adaptation, as well as domain generalisation. These methods, however, require a suitable amount of representative target images, even if unlabelled, or a suitable amount of images from multiple sources, which may not be feasible due to privacy regulations. In this study, we explore an alternative paradigm, known as Distributed Collaborative Machine Learning (DCML), which consists of exploiting images from different sources in a privacy-preserving setup. Although DCML methods seem well suited to this application, to the best of our knowledge, they have not been used for this task or to address the above-mentioned issues. However, we argue that DCML deserves further consideration in medical images as a potential alternative solution against domain shift in a privacy-preserving setup. To substantiate our view, we consider three DCML methods: early and late fusion and federated learning approaches, each offering distinct trade-offs in terms of training constraints, computational overhead and communications costs. We then conduct an extensive, cross-dataset experimental evaluation on four benchmark datasets and provide evidence that even simple implementations of DCML methods can effectively mitigate domain shift in WBC classification tasks.},
  archive      = {J_ICV},
  author       = {Lorenzo Putzu and Simone Porcu and Andrea Loddo},
  doi          = {10.1016/j.imavis.2025.105673},
  journal      = {Image and Vision Computing},
  month        = {10},
  pages        = {105673},
  shortjournal = {Image Vis. Comput.},
  title        = {Distributed collaborative machine learning in real-world application scenario: A white blood cell subtypes classification case study},
  volume       = {162},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). MSDNet: Multi-scale decoder for few-shot semantic segmentation via transformer-guided prototyping. <em>ICV</em>, <em>162</em>, 105672. (<a href='https://doi.org/10.1016/j.imavis.2025.105672'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Few-shot Semantic Segmentation addresses the challenge of segmenting objects in query images with only a handful of annotated examples. However, many previous state-of-the-art methods either have to discard intricate local semantic features or suffer from high computational complexity. To address these challenges, we propose a new Few-shot Semantic Segmentation framework based on the Transformer architecture. Our approach introduces the spatial transformer decoder and the contextual mask generation module to improve the relational understanding between support and query images. Moreover, we introduce a multi scale decoder to refine the segmentation mask by incorporating features from different resolutions in a hierarchical manner. Additionally, our approach integrates global features from intermediate encoder stages to improve contextual understanding, while maintaining a lightweight structure to reduce complexity. This balance between performance and efficiency enables our method to achieve competitive results on benchmark datasets such as P A S C A L - 5 i and C O C O - 2 0 i in both 1-shot and 5-shot settings. Notably, our model with only 1.5 million parameters demonstrates competitive performance while overcoming limitations of existing methodologies. https://github.com/amirrezafateh/MSDNet .},
  archive      = {J_ICV},
  author       = {Amirreza Fateh and Mohammad Reza Mohammadi and Mohammad Reza Jahed-Motlagh},
  doi          = {10.1016/j.imavis.2025.105672},
  journal      = {Image and Vision Computing},
  month        = {10},
  pages        = {105672},
  shortjournal = {Image Vis. Comput.},
  title        = {MSDNet: Multi-scale decoder for few-shot semantic segmentation via transformer-guided prototyping},
  volume       = {162},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Lightweight multi-scale global attention enhancement network for image super-resolution. <em>ICV</em>, <em>162</em>, 105671. (<a href='https://doi.org/10.1016/j.imavis.2025.105671'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Transformer-based depth model has achieved impressive results in the field of image super-resolution (SR). However, these algorithms still face a series of complex problems: redundant attention operations lead to low resource utilization, and the sliding window mechanism limits the ability to capture multi-scale feature information. To address these issues, this paper proposes a lightweight multi-scale global attention enhancement network (LMGAE-Net). Specifically, to overcome the window limitations in Transformer models, we introduce a multi-scale global attack block (MGAB), which significantly enhances the model’s ability to capture long-range information by grouping input features and calculating self-attention with varying window sizes. In addition, we propose a multi-group shift fusion block (MSFB), which divides features into equal groups and shifts them in different spatial directions. While maintaining the parameter quantity equivalent to 1×1 convolution, it expands the receptive field, improves the learning and fusion effect of local features, and further enhances the network’s ability to recover image details. Extensive experiments demonstrate that LMGAE-Net outperforms state-of-the-art lightweight SR methods by a large margin.},
  archive      = {J_ICV},
  author       = {Yue Huang and Pan Wang and Yumei Zheng and Bochuan Zheng},
  doi          = {10.1016/j.imavis.2025.105671},
  journal      = {Image and Vision Computing},
  month        = {10},
  pages        = {105671},
  shortjournal = {Image Vis. Comput.},
  title        = {Lightweight multi-scale global attention enhancement network for image super-resolution},
  volume       = {162},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Pyramidal attention with progressive multi-stage iterative feature refinement for salient object segmentation. <em>ICV</em>, <em>162</em>, 105670. (<a href='https://doi.org/10.1016/j.imavis.2025.105670'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Accurate detection of salient objects in complex visual scenes remains a fundamental yet challenging task in visual intelligence, often impeded by significant scale variation, background clutter, and indistinct object boundaries. While recent approaches attempt to exploit multi-level features, they frequently encounter limitations such as semantic misalignment across feature hierarchies, spatial detail degradation, and weak cross-dataset generalization. To overcome these challenges, we propose a novel Pyramidal Attention Mechanism (PAM) with Progressive Multi-stage Iterative Feature Refinement Network (PIFRNet) designed for robust and precise Salient Object Detection (SOD). Specifically, our method begins by hierarchically aggregating features from four representative stages of a powerful backbone, ensuring rich multi-scale context and semantic diversity. To bridge semantic gaps and recover fine structures, we introduce a Progressive Bilateral Feature Refinement (PBFR) module, which enhances early-stage features through cascaded convolutions and spatial attention. Furthermore, the novel PAM, equipped with dilated convolutions, is introduced to refine high-level semantics and reinforce object completeness. The network integrates these components through a multi-stage iterative refinement process, enabling gradual enhancement of spatial precision and structural fidelity. Extensive experiments conducted on five public SOD benchmarks demonstrate that our approach achieves superior performance compared to state-of-the-art methods, both quantitatively and qualitatively. Cross-dataset evaluations further validate its strong generalization capability, making it highly applicable to real-world visual intelligence scenarios.},
  archive      = {J_ICV},
  author       = {Rahim Khan and Nada Alzaben and Yousef Ibrahim Daradkeh and Xianxun Zhu and Inam Ullah},
  doi          = {10.1016/j.imavis.2025.105670},
  journal      = {Image and Vision Computing},
  month        = {10},
  pages        = {105670},
  shortjournal = {Image Vis. Comput.},
  title        = {Pyramidal attention with progressive multi-stage iterative feature refinement for salient object segmentation},
  volume       = {162},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Modal-aware contrastive learning for hyperspectral and LiDAR classification. <em>ICV</em>, <em>162</em>, 105669. (<a href='https://doi.org/10.1016/j.imavis.2025.105669'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Contrastive learning as a self-supervised learning method has received significant attention in the hyperspectral image (HSI) and light detection and ranging (LiDAR) data classification. However, the current contrastive learning-based methods ignore the huge gap between the HSI and LiDAR data in their ability to discriminate ground objects. To fully exploit the potential of HSI in the spectral domain and LiDAR in the spatial domain, we propose a modal-aware contrastive learning (MACL) framework, which learns discriminative multimodal features in both of spatial and spectral domains. First, we design a modal-aligned sample pair construction strategy to ensure that the data structure and characteristics of constructed spectral and spatial sample pairs remain consistent. Then, the spectral and spatial branches based on contrastive learning are adopted to extract multimodal spectral and spatial features in the pre-training stage. Finally, a multimodal attentional feature fusion (MAFF) module is designed to integrate and fuse the multimodal features for the downstream classification task, whose parameters are fine-tuned with a small number of labeled data. Experimental results on three public datasets, i.e., MUUFL, Trento, and Houston2013, demonstrate that our method outperforms several state-of-the-art methods in terms of qualitative and quantitative analysis. Our source codes are available at https://github.com/zlyrs1/MACL .},
  archive      = {J_ICV},
  author       = {Liangyu Zhou and Xiaoyan Luo and Rui Xue},
  doi          = {10.1016/j.imavis.2025.105669},
  journal      = {Image and Vision Computing},
  month        = {10},
  pages        = {105669},
  shortjournal = {Image Vis. Comput.},
  title        = {Modal-aware contrastive learning for hyperspectral and LiDAR classification},
  volume       = {162},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A deep learning strategy for the 3D segmentation of colorectal tumors from ultrasound imaging. <em>ICV</em>, <em>162</em>, 105668. (<a href='https://doi.org/10.1016/j.imavis.2025.105668'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Colorectal cancer remains a leading cause of cancer-related mortality worldwide, highlighting the need for accurate and efficient diagnostic tools. While Deep Learning has shown promise in medical imaging, its application to transrectal ultrasound for colorectal tumor segmentation remains underexplored. Currently, lesion segmentation is performed manually, relying on clinician expertise and leading to significant variability across treatment centers. To overcome this limitations, we propose a novel strategy that addresses both practical challenges and technical constraints, particularly in scenarios with limited data availability, offering a robust framework for accurate 3D colorectal tumor segmentation from ultrasound imaging. We evaluate eight state-of-the-art models, including convolutional neural networks and transformer-based architectures, and introduce domain-tailored pre- and post-processing techniques such as data augmentation, patching and ensembling to enhance segmentation performance while reducing computational cost. Leading to an average improvement in term of DICE score of 0.423 absolute points (+107%), compared to baseline models, our findings demonstrate the potential of our proposal to improve the accuracy and reliability of ultrasound-based diagnostics for colorectal cancer, paving the way for clinically viable AI-driven solutions.},
  archive      = {J_ICV},
  author       = {Alessandro Sebastian Podda and Riccardo Balia and Marco Manolo Manca and Jacopo Martellucci and Livio Pompianu},
  doi          = {10.1016/j.imavis.2025.105668},
  journal      = {Image and Vision Computing},
  month        = {10},
  pages        = {105668},
  shortjournal = {Image Vis. Comput.},
  title        = {A deep learning strategy for the 3D segmentation of colorectal tumors from ultrasound imaging},
  volume       = {162},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Explainable retinal disease classification and localization through convolutional neural networks. <em>ICV</em>, <em>162</em>, 105667. (<a href='https://doi.org/10.1016/j.imavis.2025.105667'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Retinal diseases pose significant challenges to vision globally, affecting a substantial portion of the population. The reliance on expert clinicians for interpreting Optical Coherence Tomography images underscores the need for automated diagnostic process. In this paper, we propose a method aimed at automatically detecting and localizing retinal disease through deep learning convolutional neural networks starting from the analysis of optical coherence tomography imaging. In detail, we propose and design a novel deep learning model, i.e., FCNNplus, for the classification task of retinal disease, reaching 93.3% in accuracy. The focus is not only on achieving a satisfying retinal disease diagnosis but also on emphasizing the role of CAM algorithms in localizing disease-specific patterns to propose a method considering the explainability and reliability behind the prediction. FCNNplus reports precise and accurate heatmaps localization, correctly identifying the presence of the retinal disease in the images. We take into account an index of similarity aimed to enhance the qualitative aspects and provide a measure of the visual explanation coming from the heatmaps (i.e. the areas of the image under analysis that, from the model point of view are symptomatic of a certain prediction).},
  archive      = {J_ICV},
  author       = {Marcello Di Giammarco and Antonella Santone and Mario Cesarelli and Fabio Martinelli and Francesco Mercaldo},
  doi          = {10.1016/j.imavis.2025.105667},
  journal      = {Image and Vision Computing},
  month        = {10},
  pages        = {105667},
  shortjournal = {Image Vis. Comput.},
  title        = {Explainable retinal disease classification and localization through convolutional neural networks},
  volume       = {162},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Advancing heart disease diagnosis with vision-based transformer architectures applied to ECG imagery. <em>ICV</em>, <em>162</em>, 105666. (<a href='https://doi.org/10.1016/j.imavis.2025.105666'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cardiovascular disease, a critical medical condition that affects the heart and blood vessels, requires timely detection for effective clinical intervention. This includes coronary artery disease, heart failure, and myocardial infarction. Our goal is to improve the detection of heart disease through proactive interventions and personalized treatments. Early identification of at-risk individuals using advanced technologies can mitigate disease progression and reduce adverse outcomes. Using recent technological advancements, we propose a novel approach for heart disease detection using vision transformer models, namely Google-Vit, Microsoft-Beit, Deit, and Swin-Tiny. This marks the initial application of transformer models to image-based electrocardiogram (ECG) data for the detection of heart disease. The experimental results demonstrate the efficacy of vision transformers in this domain, with BEiT achieving the highest classification accuracy of 95.9% in a 5-fold cross-validation setting, further improving to 96.6% using an 80-20 holdout method. Swin-Tiny also exhibited strong performance with an accuracy of 95.2%, while Google-ViT and DeiT achieved 94.3% and 94.9%, respectively, outperforming many traditional models in ECG-based diagnostics. These findings highlight the potential of vision transformer models in enhancing diagnostic accuracy and risk stratification. The results further underscore the importance of model selection in optimizing performance, with BEiT emerging as the most promising candidate. This study contributes to the growing body of research on transformer-based medical diagnostics and paves the way for future investigations into their clinical applicability and generalizability.},
  archive      = {J_ICV},
  author       = {Zeynep Hilal Kilimci and Mustafa Yalcin and Ayhan Kucukmanisa and Amit Kumar Mishra},
  doi          = {10.1016/j.imavis.2025.105666},
  journal      = {Image and Vision Computing},
  month        = {10},
  pages        = {105666},
  shortjournal = {Image Vis. Comput.},
  title        = {Advancing heart disease diagnosis with vision-based transformer architectures applied to ECG imagery},
  volume       = {162},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). APS-NeuS: Adaptive planar and skip-sampling for 3D object surface reconstruction in high-specular scenes. <em>ICV</em>, <em>162</em>, 105665. (<a href='https://doi.org/10.1016/j.imavis.2025.105665'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {High-fidelity 3D object surface reconstruction remains challenging in real-world scenes with strong specular reflections, where multi-view consistency is disrupted by reflection artifacts. To address this, we propose APS-NeuS, an implicit neural rendering framework designed to robustly separate target objects from reflective interference. Specifically, we establish a pixel-wise auxiliary mirror plane to differentiate reflections from target objects and incorporate a Laplacian gradient to better recover their edges and fine structures. Additionally, we introduce a skip-sampling strategy to reduce the impact of reflective interference, further enhancing multi-view consistency and surface fidelity. Finally, we introduce an exclusion loss to help the model more accurately separate the target objects from the reflective parts during initialization by comparing the gradient differences. Extensive experiments on synthetic and real-world datasets show that APS-NeuS achieves superior reconstruction quality under high-specular reflection conditions, demonstrating its practical applicability to complex environments. Code is available at https://github.com/ujsjl/APS-NeuS .},
  archive      = {J_ICV},
  author       = {Wei Gao and Li Jin and Youssef Akoudad and Yang Yang},
  doi          = {10.1016/j.imavis.2025.105665},
  journal      = {Image and Vision Computing},
  month        = {10},
  pages        = {105665},
  shortjournal = {Image Vis. Comput.},
  title        = {APS-NeuS: Adaptive planar and skip-sampling for 3D object surface reconstruction in high-specular scenes},
  volume       = {162},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Deepfake detection via feature refinement and enhancement network. <em>ICV</em>, <em>162</em>, 105663. (<a href='https://doi.org/10.1016/j.imavis.2025.105663'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The rapid advancement of deepfake technology poses significant threats to the integrity and privacy of biometric systems, such as facial recognition and voice authentication. To address this issue, there is an urgent need for advanced forensic detection methods that can reliably safeguard biometric data from manipulation and unauthorized access. However, current methods mainly focus on shallow feature extraction and neglect feature refinement and enhancement, which leads to low detection accuracy and poor generalization performance. To address this problem, we propose Feature Refinement and Enhancement Network (FRENet) for deepfake detection by leveraging progressive refinement and enhanced mixed feature learning. Specifically, a Low Rank Projected Self-Attention (LPSA) module is introduced for the refinement and enhancement of features. Also, a Patch-based Focused (PatchFocus) module is proposed to highlight local texture inconsistencies in key regions. In addition, we propose a Refine Fusion (RefFus) module that integrates the refined features and associated noise information to enhance feature separability. Experimental results across five benchmark datasets demonstrate that the proposed FRENet outperforms state-of-the-art methods in terms of both accuracy and generalization. The code is available at https://github.com/weichengsong-code/FRENet .},
  archive      = {J_ICV},
  author       = {Weicheng Song and Siyou Guo and Mingliang Gao and Qilei Li and Xianxun Zhu and Imad Rida},
  doi          = {10.1016/j.imavis.2025.105663},
  journal      = {Image and Vision Computing},
  month        = {10},
  pages        = {105663},
  shortjournal = {Image Vis. Comput.},
  title        = {Deepfake detection via feature refinement and enhancement network},
  volume       = {162},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multi-scale feature fusion with task-specific data synthesis for pneumonia pathogen classification. <em>ICV</em>, <em>162</em>, 105662. (<a href='https://doi.org/10.1016/j.imavis.2025.105662'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Pneumonia pathogen diagnosis from chest X-rays (CXR) is essential for timely and effective treatment for pediatric patients. However, the radiographic manifestations of pediatric pneumonia are often less distinct than those in adults, challenging for pathogen diagnosis, even for experienced clinicians. In this work, we propose a novel framework that integrates an adaptive hierarchical fusion network (AHFF) with task-specific diffusion-based data synthesis for pediatric pneumonia pathogen classification in clinical CXR. AHFF consists of dual branches to extract global and local features, and an adaptive feature fusion module that hierarchically integrates semantic information using cross attention mechanisms. Further, we develop a classifier-guided diffusion model that uses the task-specific AHFF classifier to generate class-consistent chest X-ray images for data augmentation. Experiments on one private and two public datasets demonstrate that the proposed classification model achieves superior performance, with accuracies of 78.00%, 84.43%, and 91.73%, respectively. Diffusion-based augmentation further improves accuracy to 84.37% using the private dataset. These results highlight the potential of feature fusion and data synthesis for improving automated pathogen-specific pneumonia diagnosis in clinical settings.},
  archive      = {J_ICV},
  author       = {Yinzhe Cui and Jing Liu and Ze Teng and Shuangfeng Yang and Hongfeng Li and Pingkang Li and Jiabin Lu and Yajuan Gao and Yun Peng and Hongbin Han and Wanyi Fu},
  doi          = {10.1016/j.imavis.2025.105662},
  journal      = {Image and Vision Computing},
  month        = {10},
  pages        = {105662},
  shortjournal = {Image Vis. Comput.},
  title        = {Multi-scale feature fusion with task-specific data synthesis for pneumonia pathogen classification},
  volume       = {162},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Augmenting and mixing transformers with synthetic data for image captioning. <em>ICV</em>, <em>162</em>, 105661. (<a href='https://doi.org/10.1016/j.imavis.2025.105661'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Image captioning has attracted significant attention within the Computer Vision and Multimedia research domains, resulting in the development of effective methods for generating natural language descriptions of images. Concurrently, the rise of generative models has facilitated the production of highly realistic and high-quality images, particularly through recent advancements in latent diffusion models. In this paper, we propose to leverage the recent advances in Generative AI and create additional training data that can be effectively used to boost the performance of an image captioning model. Specifically, we combine real images with their synthetic counterparts generated by Stable Diffusion using a Mixup data augmentation technique to create novel training examples. Extensive experiments on the COCO dataset demonstrate the effectiveness of our solution in comparison to different baselines and state-of-the-art methods and validate the benefits of using synthetic data to augment the training stage of an image captioning model and improve the quality of the generated captions. Source code and trained models are publicly available at: https://github.com/aimagelab/synthcap_pp .},
  archive      = {J_ICV},
  author       = {Davide Caffagni and Marcella Cornia and Lorenzo Baraldi and Rita Cucchiara},
  doi          = {10.1016/j.imavis.2025.105661},
  journal      = {Image and Vision Computing},
  month        = {10},
  pages        = {105661},
  shortjournal = {Image Vis. Comput.},
  title        = {Augmenting and mixing transformers with synthetic data for image captioning},
  volume       = {162},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). DMNet: Image dehazing via dual-domain modulation. <em>ICV</em>, <em>162</em>, 105659. (<a href='https://doi.org/10.1016/j.imavis.2025.105659'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Feature representation of hazy images is directly related to the performance of dehazing models. However, existing approaches often struggle to jointly model spatial-frequency domain characteristics for characterization of heterogeneous haze distribution. Therefore, to address these challenges, this work presents an efficient Dual-Domain Modulation Network (DMNet) for image dehazing, which enhances the representation of uneven haze features and the global feature perception by utilizing the deformable convolution and the amplitude-phase guidance strategy. For one thing, since fixed-size convolutions are inadequate for multi-scale feature extraction and inter-channel interactions, we propose the Deformable Convolutional Operator (DCM) based on the spatial non-uniform strategy of channel interactions. Through orthogonal spatial feature aggregation mechanism, the DCM effectively aggregates spatial context information to handle non-uniform haze distribution and reconstruct fine texture details in heavily hazed regions. For another, the amplitude-centric reconstruction paradigm fails to accurately represent the nonlinear mapping between hazy and clear images in the frequency domain and neglects the importance of phase structural feature in image reconstruction. Therefore, we propose the Amplitude-Phase Guidance Module (APGM) to effectively extract global features through implementing low-pass filtering on the amplitude component and high-pass filtering on the phase component. Ultimately, by combining DCM and APGM, we propose the Dual-Domain Modulation Module (DM), which serves as the core component of DMNet to overcome the hurdles faced in achieving fusion between spatial and frequency domains. Extensive experiments demonstrate that DMNet performs favorably against the state-of-the-art (SOTA) approaches, achieving the PSNR over 41.77 dB with only 3.94M parameters.},
  archive      = {J_ICV},
  author       = {Qiqi Kou and Jiapeng Chen and Hailong Zhang and Tianshu Song and He Jiang and Deqiang Cheng and Liangliang Chen},
  doi          = {10.1016/j.imavis.2025.105659},
  journal      = {Image and Vision Computing},
  month        = {10},
  pages        = {105659},
  shortjournal = {Image Vis. Comput.},
  title        = {DMNet: Image dehazing via dual-domain modulation},
  volume       = {162},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). AI4RDD: Artificial intelligence and rare disease diagnosis: A proposal to improve the anamnesis process. <em>ICV</em>, <em>162</em>, 105658. (<a href='https://doi.org/10.1016/j.imavis.2025.105658'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Diagnosing rare and complex diseases presents significant challenges due to their inherent intricacies, limited data availability, and the need for highly skilled physicians. Traditional diagnostic processes use a decentralized approach in which patients often consult multiple specialists and visit various healthcare facilities to determine their condition. This conventional method frequently leads to delayed or inaccurate diagnoses. With over 10,000 rare diseases affecting more than 350 million people worldwide, the demand for innovative and effective diagnostic solutions is urgent and critical. Artificial intelligence (AI) advancements present promising tools to tackle these challenges. AI-driven systems, such as Clinical Decision Support Systems (CDSS) and Computer-Aided Diagnosis Systems (CAD), facilitate complex medical data processing, integrating diverse datasets, including imaging and genomics, and supporting evidence-based treatment decisions. These technologies have the potential to enable earlier and more accurate diagnoses, reduce unnecessary tests, and enhance overall healthcare efficiency. This study proposes a framework for an AI-based CAD tool that can lead to a Distributed Knowledge Model. This framework seeks to improve diagnostic precision and enhance global patient outcomes for rare diseases. This framework emphasizes ethical AI implementation for better data integration and expert collaboration.},
  archive      = {J_ICV},
  author       = {Serena Lembo and Paola Barra and Luigi Di Biasi and Thierry Bouwmans and Genoveffa Tortora},
  doi          = {10.1016/j.imavis.2025.105658},
  journal      = {Image and Vision Computing},
  month        = {10},
  pages        = {105658},
  shortjournal = {Image Vis. Comput.},
  title        = {AI4RDD: Artificial intelligence and rare disease diagnosis: A proposal to improve the anamnesis process},
  volume       = {162},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). MMFEIR: Multi-attention mutual feature enhance and instance reconstruction for category-level 6D object pose estimation. <em>ICV</em>, <em>162</em>, 105657. (<a href='https://doi.org/10.1016/j.imavis.2025.105657'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Category-level 6D object pose estimation is a fundamental problem in fields such as robotic manipulation and augmented reality. The goal of this task is to predict the rotation, translation, and size of the object. Current research typically extracts the deformation field from observed point cloud of the object for estimating 6D pose. However, they did not fully consider the interaction between the observed point cloud, prior shape, and image of the object, resulting in the loss of geometric and texture features of the object, thereby affecting the accuracy of pose estimation for objects with large intra class configuration differences. In this paper, we propose a Multi-attention Mutual Feature Enhance Module (MMFEM) to enhance the inherent linkages among different perception data of objects. MMFEM enhances the interaction between images, observed point cloud, and prior shape through multiple attention modules. This enables the network to gain a deeper understanding of the differences between distinct instances. In addition, to improve the feature expression of geometric details for objects, we propose the Instance Reconstruction Deformation Module (IRDM). IRDM reconstructed the three-dimensional instance point cloud for each object, enhancing the model’s ability to identify differences in geometric configurations of objects. Extensive experiments on the CAMERA25 and REAL275 datasets show that the proposed methods have achieved 79.0% and 91.2% on the 3D75 metric, 52.6% and 75.9% on the 5°2 cm metric, respectively, outperforming current mainstream methods.},
  archive      = {J_ICV},
  author       = {Haotian Lei and Xiangyu Liu and Yan Zhou and Guo Niu and Changan Yi and Yuexia Zhou and Xiaofeng Liang and Fuhe Liu},
  doi          = {10.1016/j.imavis.2025.105657},
  journal      = {Image and Vision Computing},
  month        = {10},
  pages        = {105657},
  shortjournal = {Image Vis. Comput.},
  title        = {MMFEIR: Multi-attention mutual feature enhance and instance reconstruction for category-level 6D object pose estimation},
  volume       = {162},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Progressive background–foreground difference enhancement for few-shot 3D point cloud semantic segmentation. <em>ICV</em>, <em>162</em>, 105656. (<a href='https://doi.org/10.1016/j.imavis.2025.105656'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Few-shot 3D point cloud semantic segmentation aims to segment query point clouds given only few annotated support point clouds. Most existing methods focus on exploring the complex relationships between support data and query data within the prototype-based framework. However, the ignored background ambiguity issue, i.e., the foregrounds of a support class are treated as backgrounds by other support classes, severely limits few-shot models’ ability to distinguish foregrounds and backgrounds, resulting in biased prototypes. In this paper, we propose a progressive background–foreground difference enhancement method to eliminate background ambiguity. Firstly, based on the fact that the background ambiguity only affects background prototypes, we develop a background–foreground difference enhancement strategy, which eliminates background ambiguity via enhancing the difference between foregrounds and backgrounds in query data. Then, we present a geometric-guided feature aggregation module, which integrates geometrical information to improve the reliability of pseudo labels. Finally, we aggregate high-confidence query features as pseudo prototypes to refine the prototypes. The iteration of these steps further improves prototype quality. Comprehensive experiments suggest that our method achieves competing performance on both S3DIS and ScanNet datasets.},
  archive      = {J_ICV},
  author       = {Tichao Wang and Fusheng Hao and Qieshi Zhang and Jun Cheng},
  doi          = {10.1016/j.imavis.2025.105656},
  journal      = {Image and Vision Computing},
  month        = {10},
  pages        = {105656},
  shortjournal = {Image Vis. Comput.},
  title        = {Progressive background–foreground difference enhancement for few-shot 3D point cloud semantic segmentation},
  volume       = {162},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). MAFUNet: Mamba with adaptive fusion UNet for medical image segmentation. <em>ICV</em>, <em>162</em>, 105655. (<a href='https://doi.org/10.1016/j.imavis.2025.105655'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In medical image segmentation tasks, accurately capturing lesion contours and understanding complex lesion information is crucial, which relies on efficient collaborative modeling of local details and global contours. However, methods based on convolutional neural networks (CNNs) and transformers are limited by local receptive fields and high computational complexity, respectively, making it difficult for existing approaches to achieve a balance between the two. Recently, state-space models represented by Mamba have gained attention due to their significant advantages in capturing long-range dependencies and computational efficiency. Based on the above advantages of Mamba, we propose M amba with A daptive F usion U Net (MAFUNet). First, we design a hierarchy-aware Mamba (HAM) module. HAM progressively transmits local and global information across different channel branches through Mamba and balances feature contributions through a dynamic gating mechanism, improving the accuracy of lesion region recognition. The multi-scale adaptive fusion (MAF) module combines HAM, convolution block, and cascaded attention mechanisms to achieve efficient fusion of lesion features at different scales, thereby enhancing the model’s robustness and precision. To address the feature alignment issue, we propose adaptive channel attention (ACA) and adaptive spatial attention (ASA) modules, where the former achieves channel enhancement through dual-scale pooling and the latter strengthens spatial representation using a dual-path convolution strategy. Extensive experiments on the BUSI, CVC-ClinicDB, and ISIC-2018 three public datasets show that MAFUNet achieves excellent performance in medical image segmentation tasks.},
  archive      = {J_ICV},
  author       = {Minchen Yang and Ziyi Yang and Nur Intan Raihana Ruhaiyem},
  doi          = {10.1016/j.imavis.2025.105655},
  journal      = {Image and Vision Computing},
  month        = {10},
  pages        = {105655},
  shortjournal = {Image Vis. Comput.},
  title        = {MAFUNet: Mamba with adaptive fusion UNet for medical image segmentation},
  volume       = {162},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Harnessing consistency for improved test-time adaptation. <em>ICV</em>, <em>162</em>, 105650. (<a href='https://doi.org/10.1016/j.imavis.2025.105650'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Test-time adaptation (TTA) is crucial for adjusting pre-trained models to new, unseen test data distributions without ground-truth labels, thereby addressing domain shifts commonly encountered in real-world scenarios. The most widely adopted self-training strategies in TTA include either pseudo-labeling or the minimization of prediction entropy. Different from these approaches, some research in natural language processing explored the use of consistency as a self-training objective. However, the performance improvements via consistency maximization have been limited. Based on this finding, we present a novel approach that employs consistency not as a primary self-training objective but as a metric for effective sample weighting and filtering. Our method, Consistency-TTA (CTTA), enhances performance and computational efficiency by implementing a sample weighting method that prioritizes samples demonstrating robustness to perturbations, and a sample filtering method that restricts backward pass to samples that are less prone to error accumulation. Our CTTA, which can be orthogonally combined with various state-of-the-art baselines, demonstrates performance improvements in extended adaptation tasks such as multi-modal TTA for 3D semantic segmentation and video domain adaptation. We evaluated CTTA on various corruption and natural domain shift datasets, consistently demonstrating meaningful performance improvements. Moreover, CTTA proved to be effective in both classification tasks and semantic segmentation benchmarks, such as CarlaTTA, highlighting its versatility across extended TTA applications.},
  archive      = {J_ICV},
  author       = {Dahuin Jung},
  doi          = {10.1016/j.imavis.2025.105650},
  journal      = {Image and Vision Computing},
  month        = {10},
  pages        = {105650},
  shortjournal = {Image Vis. Comput.},
  title        = {Harnessing consistency for improved test-time adaptation},
  volume       = {162},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). ADPNet: Attention-driven dual-path network for automated polyp segmentation in colonoscopy. <em>ICV</em>, <em>162</em>, 105648. (<a href='https://doi.org/10.1016/j.imavis.2025.105648'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Accurate automated polyp segmentation in colonoscopy images is crucial for early colorectal cancer detection and treatment, a major global health concern. Effective segmentation aids clinical decision-making and surgical planning. Leveraging advancements in deep learning, we propose an Attention-Driven Dual-Path Network (ADPNet) for precise polyp segmentation. ADPNet features a novel architecture with a specialized bridge integrating the Atrous Self-Attention Pyramid Module (ASAPM) and Dilated Convolution-Transformer Module (DCTM) between the encoder and decoder, enabling efficient feature extraction, long-range dependency capture, and enriched semantic representation. The decoder employs pixel shuffle, gated attention mechanisms, and residual blocks to enhance contextual and spatial feature refinement, ensuring precise boundary delineation and noise suppression. Comprehensive evaluations on public polyp datasets show ADPNet outperforms state-of-the-art models, demonstrating superior accuracy and robustness, particularly in challenging scenarios such as small or concealed polyps. ADPNet offers a robust solution for automated polyp segmentation, with potential to revolutionize early colorectal cancer detection and improve clinical outcomes. The code and results of this article are publicly available at https://github.com/Mkhan143/ADPNet.},
  archive      = {J_ICV},
  author       = {Mukhtiar Khan and Inam Ullah and Nadeem Khan and Sumaira Hussain and Muhammad ILyas Khattak},
  doi          = {10.1016/j.imavis.2025.105648},
  journal      = {Image and Vision Computing},
  month        = {10},
  pages        = {105648},
  shortjournal = {Image Vis. Comput.},
  title        = {ADPNet: Attention-driven dual-path network for automated polyp segmentation in colonoscopy},
  volume       = {162},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Deep hybrid manifold network with joint metric learning for image set classification. <em>ICV</em>, <em>162</em>, 105647. (<a href='https://doi.org/10.1016/j.imavis.2025.105647'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Many studies have shown that complex visual data exhibit non-linear and non-Euclidean characteristics. How to find an intrinsic and low-dimensional representation for non-linear visual data is crucial for image set classification. Due to the powerful data interpretation of deep neural networks and the intrinsic structural exploitation of manifold learning, deep Riemannian neural networks have demonstrated excellent performance on solving the non-linear and non-Euclidean data. However, on the one hand, deep Riemannian neural networks usually focus on exploring the intrinsic structure of the single manifold, while complex visual data may contain multiple potential intrinsic structures. On the other hand, the single cross-entropy is usually adopted as the sole loss function, which may lose discriminative metric information. In this paper, we propose a deep Riemannian neural network by fusing Symmetric Positive Definite (SPD) and Grassmann manifolds to explore multiple intrinsic structures in complex visual data. We innovatively employ the Jensen–Bregman LogDet Divergence and Projection metric to construct two metric learning regularization terms over SPD and Grassmann manifold networks respectively, which capture the intra-class and inter-class data distributions. Subsequently, the regularization terms corresponding to different manifolds are jointly learned in conjunction with the cross-entropy loss function to fuse multiple loss information. Extensive experiments are conducted on expression recognition, gesture recognition, and action recognition tasks. Experimental results demonstrate the superior performance of the proposed Riemannian network.},
  archive      = {J_ICV},
  author       = {Yujie Wu and Hengliang Tan and Jiao Du and Shuo Yang and Guofeng Yan},
  doi          = {10.1016/j.imavis.2025.105647},
  journal      = {Image and Vision Computing},
  month        = {10},
  pages        = {105647},
  shortjournal = {Image Vis. Comput.},
  title        = {Deep hybrid manifold network with joint metric learning for image set classification},
  volume       = {162},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Distribution-modulated binary neural network for image classification. <em>ICV</em>, <em>162</em>, 105646. (<a href='https://doi.org/10.1016/j.imavis.2025.105646'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep neural networks excel at image processing tasks, but their extensive model storage and computational overhead make deployment on edge devices challenging. Binary neural networks (BNNs) have become one of the most prevailing model compression approaches due to the advantage of memory and computation efficiency. However, there exists a large performance gap between BNNs and their full-precision counterparts due to training difficulties. When training BNNs using pseudo-gradients, both dead weights and susceptible weights hinder the optimization of BNNs. To solve these two abnormal weights, in this paper, we propose a distribution-modulated binary neural network (DM-BNN), which incorporates a new regularization for dead weights (RDW) and a novel approximation with a peak-shaped derivative (APSD) for susceptible weights. In detail, RDW can supply additional gradients to eliminate dead weights and form a compact weight distribution, while APSD reduces the number of susceptible weights by facilitating the magnitude increase of susceptible weights. The achieved state-of-the-art experimental results on CIFAR-10 and ImageNet demonstrate the effectiveness of DM-BNN. Our code will be available at https://github.com/NianKong/DM-BNN .},
  archive      = {J_ICV},
  author       = {Yingcheng Lin and Yuxiao Wang and Rui Ding and Haijun Liu and Xichuan Zhou},
  doi          = {10.1016/j.imavis.2025.105646},
  journal      = {Image and Vision Computing},
  month        = {10},
  pages        = {105646},
  shortjournal = {Image Vis. Comput.},
  title        = {Distribution-modulated binary neural network for image classification},
  volume       = {162},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). MG-KG: Unsupervised video anomaly detection based on motion guidance and knowledge graph. <em>ICV</em>, <em>162</em>, 105644. (<a href='https://doi.org/10.1016/j.imavis.2025.105644'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Unsupervised Video Anomaly Detection (VAD) is a challenging and research-valuable task that is trained with only normal samples to detect anomalous samples. However, current solutions face two key issues: (1) a lack of spatio-temporal linkage in video data, and (2) limited interpretability of VAD results. To address these, we propose a new method named Motion Guidance-Knowledge Graph (MG-KG), inspired by video saliency detection and video understanding methods. Specifically, MG-KG has two components: the Motion Guidance Network (MGNet) and the Knowledge Graph retrieval for VAD (VAD-KG). MGNet emphasizes motion in the video foreground, crucial for real-time surveillance, while VAD-KG builds a knowledge graph to store structured video information and retrieve it during testing, enhancing interpretability. This combination improves both generalization and understanding in VAD for smart surveillance systems. Additionally, since training data has only normal samples, we propose a training baseline strategy, a tabu search strategy, and a score rectification strategy to enhance MG-KG for video anomaly detection tasks, which can further exploit the potential of MG-KG and significantly improve the performance of VAD. Extensive experiments demonstrate that MG-KG achieves competitive results in VAD for intelligent video surveillance.},
  archive      = {J_ICV},
  author       = {Qiyue Sun and Yang Yang and Haoxuan Xu and Zezhou Li and Yunxia Liu and Hongjun Wang},
  doi          = {10.1016/j.imavis.2025.105644},
  journal      = {Image and Vision Computing},
  month        = {10},
  pages        = {105644},
  shortjournal = {Image Vis. Comput.},
  title        = {MG-KG: Unsupervised video anomaly detection based on motion guidance and knowledge graph},
  volume       = {162},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Invariant prompting with classifier rectification for continual learning. <em>ICV</em>, <em>162</em>, 105641. (<a href='https://doi.org/10.1016/j.imavis.2025.105641'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Continual learning aims to train a model capable of continuously learning and retaining knowledge from a sequence of tasks. Recently, prompt-based continual learning has been proposed to leverage the generalization ability of a pre-trained model with task-specific prompts for instruction. Prompt component training is a promising approach to enhancing the plasticity for prompt-based continual learning. Nevertheless, this approach changes the instructed features to be noisy for query samples from the old tasks. Additionally, the problem of scale misalignment in classifier logits between different tasks leads to misclassification. To address these issues, we propose an invariant Prompting with Classifier Rectification (iPrompt-CR) method for prompt-based continual learning. In our method, the learnable keys corresponding to each new-task component are constrained to be orthogonal to the query prototype in the old tasks for invariant prompting, which reduces feature representation noise. After prompt learning, instructed features are sampled from Gaussian-distributed prototypes for classifier rectification with unified logit scale for more accurate predictions. Extensive experimental results on four benchmark datasets demonstrate that our method outperforms the state of the arts in both class-incremental learning and more realistic general incremental learning scenarios.},
  archive      = {J_ICV},
  author       = {Chunsing Lo and Hao Zhang and Andy J. Ma},
  doi          = {10.1016/j.imavis.2025.105641},
  journal      = {Image and Vision Computing},
  month        = {10},
  pages        = {105641},
  shortjournal = {Image Vis. Comput.},
  title        = {Invariant prompting with classifier rectification for continual learning},
  volume       = {162},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Label refinement for change detection in remote sensing. <em>ICV</em>, <em>162</em>, 105639. (<a href='https://doi.org/10.1016/j.imavis.2025.105639'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Change detection in remote sensing aims to detect changes occurring in the same geographical area over time. Existing methods present two main challenges: (1) relying on single-scale features to capture multi-scale object changes, which limits their ability to effectively handle multi-scale change; and (2) misclassification issues caused by prediction uncertainty, particularly in regions near decision boundaries, leading to reduced overall detection performance. In this study, to address these limitations, we propose LRNet, a multi-scale change detection framework designed to enhance the perception of objects at varying scales and refine change region details during decoding. Abandoning the use of fixed thresholds for classification, LRNet incorporates a Label Refinement (LR) strategy that propagates information from high-confidence regions to low-confidence regions by evaluating feature-space similarity, enabling precise grouping of pixels within change regions. Extensive experiments on benchmark datasets — SYSU-CD, LEVIR-CD+, and SECOND-CD — demonstrate that LRNet outperforms state-of-the-art methods, with significant improvements of 7.9% in F1 and 12.38% in IoU on the challenging SECOND-CD dataset.},
  archive      = {J_ICV},
  author       = {Zhilong Ou and Hongxing Wang and Jiawei Tan and Jiaxin Li and Ziyi Zhao and Zhangbin Qian},
  doi          = {10.1016/j.imavis.2025.105639},
  journal      = {Image and Vision Computing},
  month        = {10},
  pages        = {105639},
  shortjournal = {Image Vis. Comput.},
  title        = {Label refinement for change detection in remote sensing},
  volume       = {162},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). BCDPose: Diffusion-based 3D human pose estimation with bone-chain prior knowledge. <em>ICV</em>, <em>162</em>, 105636. (<a href='https://doi.org/10.1016/j.imavis.2025.105636'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, diffusion-based methods have emerged as the golden standard in 3D Human Pose Estimation task, largely thanks to their exceptional generative capabilities. In the past, researchers have made concerted efforts to develop spatial and temporal denoisers utilizing transformer blocks in diffusion-based methods. However, existing Transformer-based denoisers in diffusion models often overlook implicit structural and kinematic supervision derived from prior knowledge of human biomechanics, including prior knowledge of human bone-chain structure and joint kinematics, which could otherwise enhance performance. We hold the view that joint movements are intrinsically constrained by neighboring joints within the bone-chain and by kinematic hierarchies. Then, we propose a B one- C hain enhanced D iffusion 3D pose estimation method, or BCDPose . In this method, we introduce a novel Bone-Chain prior knowledge enhanced transformer blocks within the denoiser to reconstruct contaminated 3D pose data. Additionally, we propose the Joint-DoF Hierarchical Temporal Embedding framework, which incorporates prior knowledge of joint kinematics. By integrating body hierarchy and temporal dependencies, this framework effectively captures the complexity of human motion, thereby enabling accurate and robust pose estimation. This innovation proposes a comprehensive framework for 3D human pose estimation by explicitly modeling joint kinematics, thereby overcoming the limitations of prior methods that fail to capture the intrinsic dynamics of human motion. We conduct extensive experiments on various open benchmarks to evaluate the effectiveness of BCDPose. The results convincingly demonstrate that BCDPose achieves highly competitive results compared with other state-of-the-art models. This underscores its promising potential and practical applicability in 2D–3D human pose estimation tasks. We plan to release our code publicly for further research.},
  archive      = {J_ICV},
  author       = {Xing Liu and Hao Tang},
  doi          = {10.1016/j.imavis.2025.105636},
  journal      = {Image and Vision Computing},
  month        = {10},
  pages        = {105636},
  shortjournal = {Image Vis. Comput.},
  title        = {BCDPose: Diffusion-based 3D human pose estimation with bone-chain prior knowledge},
  volume       = {162},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Athlete posture estimation and analysis based on embodied artificial intelligence. <em>ICV</em>, <em>162</em>, 105598. (<a href='https://doi.org/10.1016/j.imavis.2025.105598'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the widespread adoption of pose estimation technology in computer vision, traditional methods often struggle with insufficient accuracy and poor robustness when dealing with complex dynamic scenes and diverse backgrounds. To overcome these challenges, we propose a novel pose estimation approach that integrates a dynamic graph convolutional network (DGCN), a spatiotemporal interleaved attention mechanism (STIA), and a variable-length Transformer encoder (VLTE). The DGCN module captures the spatial dependencies of human poses and enhances the model’s ability to represent complex spatial relationships by dynamically adjusting the graph structure. The STIA module effectively captures spatiotemporal dependencies by combining both temporal and spatial information. The VLTE module improves the model’s adaptability to varying time scales by processing variable-length sequences and multi-scale information. We performed extensive experimental validation on the MPI-INF-3DHP dataset, and the results demonstrate that removing any of the modules significantly reduces the model’s performance, highlighting the importance of each component in the overall architecture. Furthermore, inference speed analysis reveals that while streamlining the model can enhance inference speed, it comes at the cost of accuracy and robustness. Our work presents a pose estimation solution that strikes a balance between computational efficiency and high accuracy, offering strong support for practical pose estimation tasks.},
  archive      = {J_ICV},
  author       = {Junxiong Zhang and Jin Peng and Kaiyun Wang},
  doi          = {10.1016/j.imavis.2025.105598},
  journal      = {Image and Vision Computing},
  month        = {10},
  pages        = {105598},
  shortjournal = {Image Vis. Comput.},
  title        = {Athlete posture estimation and analysis based on embodied artificial intelligence},
  volume       = {162},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Shape-from-template with generalised camera. <em>ICV</em>, <em>162</em>, 105579. (<a href='https://doi.org/10.1016/j.imavis.2025.105579'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article presents a new method for non-rigidly registering a 3D shape to 2D keypoints observed by a constellation of multiple cameras. Non-rigid registration of a 3D shape to observed 2D keypoints, i.e., Shape-from-Template (S f T), has been widely studied using single images, but S f T with information from multiple-cameras jointly opens new directions for extending the scope of known use-cases such as 3D shape registration in medical imaging and registration from hand-held cameras, to name a few. We represent such multi-camera setup with the generalised camera model; therefore any collection of perspective or orthographic cameras observing any deforming object can be registered. We propose multiple approaches for such S f T: the first approach where the corresponded keypoints lie on a direction vector from a known 3D point in space, the second approach where the corresponded keypoints lie on a direction vector from an unknown 3D point in space but with known orientation w.r.t some local reference frame, and a third approach where, apart from correspondences, the silhouette of the imaged object is also known. Together, these form the first set of solutions to the S f T problem with generalised cameras. The key idea behind S f T with generalised camera is the improved reconstruction accuracy from estimating deformed shape while utilising the additional information from the mutual constraints between multiple views of a deformed object. The correspondence-based approaches are solved with convex programming while the silhouette-based approach is an iterative refinement of the results from the convex solutions. We demonstrate the accuracy of our proposed methods on many synthetic and real data 1 .},
  archive      = {J_ICV},
  author       = {Agniva Sengupta and Stefan Zachow},
  doi          = {10.1016/j.imavis.2025.105579},
  journal      = {Image and Vision Computing},
  month        = {10},
  pages        = {105579},
  shortjournal = {Image Vis. Comput.},
  title        = {Shape-from-template with generalised camera},
  volume       = {162},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). BSMEF: Optimized multi-exposure image fusion using B-splines and mamba. <em>ICV</em>, <em>161</em>, 105660. (<a href='https://doi.org/10.1016/j.imavis.2025.105660'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, multi-exposure image fusion has been widely applied to process overexposed or underexposed images due to its simplicity, effectiveness, and low cost. With the development of deep learning techniques, related fusion methods have been continuously optimized. However, retaining global information from source images while preserving fine local details remains challenging, especially when fusing images with extreme exposure differences, where boundary transitions often exhibit shadows and noise. To address this, we propose a multi-exposure image fusion network model, BSMEF, based on B-Spline basis functions and Mamba. The B-Spline basis function, known for its smoothness, reduces edge artifacts and enables smooth transitions between images with varying exposure levels. In BSMEF, the feature extraction module, combining B-Spline and deformable convolutions, preserves global features while effectively extracting fine-grained local details. Additionally, we design a feature enhancement module based on Mamba blocks, leveraging its powerful global perception ability to capture contextual information. Furthermore, the fusion module integrates three feature enhancement methods: B-Spline basis functions, attention mechanisms, and Fourier transforms, addressing shadow and noise issues at fusion boundaries and enhancing the focus on important features. Experimental results demonstrate that BSMEF outperforms existing methods across multiple public datasets.},
  archive      = {J_ICV},
  author       = {Jinyong Cheng and Qinghao Cui and Guohua Lv},
  doi          = {10.1016/j.imavis.2025.105660},
  journal      = {Image and Vision Computing},
  month        = {9},
  pages        = {105660},
  shortjournal = {Image Vis. Comput.},
  title        = {BSMEF: Optimized multi-exposure image fusion using B-splines and mamba},
  volume       = {161},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Human posture recognition using random search neural architecture for accident injury severity prediction and victim identification. <em>ICV</em>, <em>161</em>, 105654. (<a href='https://doi.org/10.1016/j.imavis.2025.105654'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Numerous lives are lost due to the ignorance of the victim's conditions and the injury severity in road accidents. In developing countries like India, the challenges of emergency responders are identifying and prioritizing victims and their injuries amid chaotic environments. In this paper, a systematic approach for accident injury severity and victim identification using human posture recognition and instance segmentation is proposed. To overcome the challenge of fixed architectures limiting the adaptability to diverse accident scenarios, Random Search Neural Architecture Search (RNAS) is adapted to automatically find an optimal Convolutional Neural Network (CNN). To enhance the efficiency and accuracy of identifying victims in accident scenes, Mask RCNN, an instance segmentation technique trained over accident images, has been used. By leveraging computer vision techniques, an automated accident injury severity and victim identification system facilitating more timely decision-making for the emergency response systems is designed. The model has been experimented with and evaluated, and the introduction of random search neural architecture has determined a computationally less expensive CNN model. The model can produce an accuracy of 95% in recognizing the human posture. Further, Mask RCNN is trained, experimented with, and validated on accident images to produce 0.99 mAP in identifying victims.},
  archive      = {J_ICV},
  author       = {P. Joyce Beryl Princess and Salaja Silas and Elijah Blessing Rajsingh and Xiao-Zhi Gao},
  doi          = {10.1016/j.imavis.2025.105654},
  journal      = {Image and Vision Computing},
  month        = {9},
  pages        = {105654},
  shortjournal = {Image Vis. Comput.},
  title        = {Human posture recognition using random search neural architecture for accident injury severity prediction and victim identification},
  volume       = {161},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Real time image processing and smart healthcare using eXplainable artificial intelligence (XAI). <em>ICV</em>, <em>161</em>, 105653. (<a href='https://doi.org/10.1016/j.imavis.2025.105653'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Mental health is a term often confused by individuals depending upon one's experiences and the environment in which they have grown. While few controversies and taboo make it difficult for some people to be prepared to comprehensively examine this topic. However, people should not lose hope for they may seek solace in alternative means such as analyzing one's daily practices as people's habits can be predictive of the level of mental stress they experience. This research paper examines the notion that mental health is instrumental toward a person's total health, somatically devoid, which many people ignore in their quest to offer more attention toward the physical aspect of health. It examines myths about mental illnesses, especially the fact that many people consider sullen emotions to be ordinary and therefore stigmatizing the afflicted to loneliness. This research promotes seeking early intervention and stigma-free discussions about emotional disorders and disorders as a way to promote self-appreciation and courageously seeking help. The work examines the transformative impact of AI in modern healthcare and its application in medical imaging, mental health informatics and clinical decision-support systems. Understanding about how AI advancements like machine learning(ML), deep learning(DL), and natural language processing(NLP), and eXplainable AI(XAI) can help in improving treatment outcomes, enhance diagnostic accuracy and optimize clinical decision support systems. The paper aims at comparing the performance of ensemble machine learning techniques such as stacking, bagging, and boosting, with boosting methods like XGBoost achieving an impressive 94% accuracy. Additionally, neural networks and deep neural networks were applied, yielding better results of 93.4% and 97.50%, respectively. This integrated approach aims to enhance awareness and understanding of mental health issues while promoting proactive support and intervention measures. These models are then combined with eXplainable Artificial Intelligence (XAI) to give users personalized suggestions based on detected mental health conditions, along with a clear diagnostic report.},
  archive      = {J_ICV},
  author       = {Lakshita Aggarwal and Vikram Ranjan and Ananya Sharma},
  doi          = {10.1016/j.imavis.2025.105653},
  journal      = {Image and Vision Computing},
  month        = {9},
  pages        = {105653},
  shortjournal = {Image Vis. Comput.},
  title        = {Real time image processing and smart healthcare using eXplainable artificial intelligence (XAI)},
  volume       = {161},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). DMFASR: Dense multi-feature aggregation-based super-resolution for brain MR images. <em>ICV</em>, <em>161</em>, 105652. (<a href='https://doi.org/10.1016/j.imavis.2025.105652'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Achieving high-resolution (HR) magnetic resonance (MR) images is a significant challenge in medical imaging due to hardware limitations and long scanning times. While deep learning-based super-resolution techniques have shown promise in generating HR MR images from low-resolution (LR) MR images, they often encounter difficulties associated with ineffective feature extraction and information loss. To address these challenges, we propose a dense multi-feature aggregation-based super-resolution network (DMFASR) that utilizes a dense residual connection block (DRCB) to minimize information loss in deeper layers while effectively extracting features. Within DRCB, a multi-feature aggregation block (MFAB) is introduced to capture both local and long-range features while propagating features through the aggregation of multi-features from different layers. The extraction of both local and long-range features is achieved by effectively combining a standard convolutional layer (Conv), a dilated convolutional layer (DConv), and a deconvolutional layer (DeConv). As a result, the proposed design enables the accurate restoration of high-frequency textures and edges in brain MR images. Experimental results demonstrate that DMFASR outperforms state-of-the-art networks in both quantitative and visual assessments of HR brain MR image reconstruction.},
  archive      = {J_ICV},
  author       = {Jordan Daniel Joshua and Young Beom Kim and Jin Young Lee},
  doi          = {10.1016/j.imavis.2025.105652},
  journal      = {Image and Vision Computing},
  month        = {9},
  pages        = {105652},
  shortjournal = {Image Vis. Comput.},
  title        = {DMFASR: Dense multi-feature aggregation-based super-resolution for brain MR images},
  volume       = {161},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multi-scale pyramid convolution transformer for remote-sensing object detection. <em>ICV</em>, <em>161</em>, 105651. (<a href='https://doi.org/10.1016/j.imavis.2025.105651'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Reliable object detection in remote sensing imageries (RSIs) is an essential and challenging task in surface monitoring. However, RSIs are normally obtained from a high-altitude top-down perspective, causing intrinsic properties such as complex background, aspect ratio, color and scale variations. These properties restrict the domain transfer of sophisticated detector on nature images to RSIs, thereby deteriorating the desired detection performance. To address this issue, we propose a multi-scale pyramid convolution Transformer (MPCViT) that alleviates the limitations of ordinary visual Transformer. Specifically, we firstly employ an improved CNN to extract image features, generating initial feature pyramid. Then, bidirectional feature aggregation strategy is further used to improve feature representation capacity through feature enhancement and aggregation steps. To facilitate deep interaction of global dependencies and local details, dual-route encoding mechanism is constructed in each Transformer encoder. During inference stage, an iterative sparse keypoint sampling head is devised to enhance the detection accuracy. The competitive experimental results on NWPU VHR-10 and DIOR verify the efficacy of the proposed MPCViT.},
  archive      = {J_ICV},
  author       = {Jin Huagang and Zhou Yu},
  doi          = {10.1016/j.imavis.2025.105651},
  journal      = {Image and Vision Computing},
  month        = {9},
  pages        = {105651},
  shortjournal = {Image Vis. Comput.},
  title        = {Multi-scale pyramid convolution transformer for remote-sensing object detection},
  volume       = {161},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Optimizing multimodal personalized disease prediction accuracy using generated prompts and large language models. <em>ICV</em>, <em>161</em>, 105649. (<a href='https://doi.org/10.1016/j.imavis.2025.105649'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The rapid advancement of Large Language Models (LLMs) and Generative AI (GenAI) has unlocked new possibilities in personalized healthcare. This study proposes a novel multimodal framework that enhances disease prediction accuracy by integrating medical images and clinical reports through LLM-guided prompting. Traditional methods often treat these data sources in isolation, resulting in limited diagnostic precision. Our approach uses task-specific prompts to guide LLMs toward critical disease-relevant and patient-specific features, enabling a more contextual and comprehensive analysis. Experimental results demonstrate significant improvements in diagnostic performance, thereby advancing the scope of personalized medicine and clinical decision-making.},
  archive      = {J_ICV},
  author       = {Sini Raj Pulari and Maramreddy Umadevi and Shriram K. Vasudevan},
  doi          = {10.1016/j.imavis.2025.105649},
  journal      = {Image and Vision Computing},
  month        = {9},
  pages        = {105649},
  shortjournal = {Image Vis. Comput.},
  title        = {Optimizing multimodal personalized disease prediction accuracy using generated prompts and large language models},
  volume       = {161},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). DFF-net: Deep feature fusion network for low-light image enhancement. <em>ICV</em>, <em>161</em>, 105645. (<a href='https://doi.org/10.1016/j.imavis.2025.105645'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Low-light image enhancement methods are designed to improve brightness, recover texture details, restore color fidelity and suppress noise in images captured in low-light environments. Although many low-light image enhancement methods have been proposed, existing methods still face two limitations: (1) the inability to achieve all these objectives at the same time; and (2) heavy reliance on supervised methods that limits practical applicability in real-world scenarios. To overcome these challenges, we propose a Deep Feature Fusion Network (DFF-Net) for low-light image enhancement which builds upon Zero-DCE’s light-enhancement curve. The network is trained without requiring any paired datasets through a set of carefully designed non-reference loss functions. Furthermore, we develop a Fast Deep-level Residual Block (FDRB) to strengthen DFF-Net’s performance, which demonstrates superior performance in both feature extraction and computational efficiency. Comprehensive quantitative and qualitative experiments demonstrate that DFF-Net achieves superior performance in both subjective visual quality and downstream computer vision tasks. In low-light image enhancement experiments, DFF-Net achieves either optimal or sub-optimal metrics across all six public datasets compared to other unsupervised methods. And in low-light object detection experiments, DFF-Net achieves maximum scores in four key metrics on the ExDark dataset: P at 83.3%, F1 at 72.8%, mAP50 at 74.9%, and mAP50-95 at 48.9%. Code is available at https://github.com/WangL0ngTa0/DFF-Net .},
  archive      = {J_ICV},
  author       = {Hongchang Zhang and Longtao Wang and Qizhan Zou and Juan Zeng},
  doi          = {10.1016/j.imavis.2025.105645},
  journal      = {Image and Vision Computing},
  month        = {9},
  pages        = {105645},
  shortjournal = {Image Vis. Comput.},
  title        = {DFF-net: Deep feature fusion network for low-light image enhancement},
  volume       = {161},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). MaxSwap-enhanced knowledge consistency learning for long-tailed recognition. <em>ICV</em>, <em>161</em>, 105643. (<a href='https://doi.org/10.1016/j.imavis.2025.105643'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep learning has made significant progress in image classification. However, real-world datasets often exhibit a long-tailed distribution, where a few head classes dominate while many tail classes have very few samples. This imbalance leads to poor performance on tail classes. To address this issue, we propose MaxSwap-Enhanced Knowledge Consistency Learning which includes two core components: Knowledge Consistency Learning and MaxSwap for Confusion Suppression. Knowledge Consistency Learning leverages the outputs from different augmented views as soft labels to capture inter-class similarities and introduces a consistency constraint to enforce output consistency across different perturbations, which enables tail classes to effectively learn from head classes with similar features. To alleviate the bias towards head classes, we further propose a MaxSwap for Confusion Suppression to adaptively adjust the soft labels when the model makes incorrect predictions which mitigates overconfidence in incorrect predictions. Experimental results demonstrate that our method achieves significant improvements on long-tailed datasets such as CIFAR10-LT, CIFAR100-LT, ImageNet-LT, and Places-LT, which validates the effectiveness of our approach.},
  archive      = {J_ICV},
  author       = {Shengnan Fan and Zhilei Chai and Zhijun Fang and Yuying Pan and Hui Shen and Xiangyu Cheng and Qin Wu},
  doi          = {10.1016/j.imavis.2025.105643},
  journal      = {Image and Vision Computing},
  month        = {9},
  pages        = {105643},
  shortjournal = {Image Vis. Comput.},
  title        = {MaxSwap-enhanced knowledge consistency learning for long-tailed recognition},
  volume       = {161},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Expert-scoring guided global information interaction network for lightweight image super-resolution. <em>ICV</em>, <em>161</em>, 105642. (<a href='https://doi.org/10.1016/j.imavis.2025.105642'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Transformer architecture has exhibited significant performance superiority in the domain of image super-resolution (SR). However, most existing methods rely on local or window-based self-attention mechanisms, which limit the establishment of reliable long-range dependencies and thus hinder effective global information interaction. To address this challenge, we propose a novel expert-scoring guided global information interaction network (GIISR) tailored for lightweight image SR. Specifically, we design a token scoring expert (TSE) to measure the similarity among pixel tokens and sort them into one-dimensional sequences based on expert scores. This transformation disrupts the rigid two-dimensional spatial structure and allows similar tokens to be spatially aligned, enabling more effective feature interactions. On top of this, the global feature refinement module (GFRM) captures fine-grained long-range dependencies through depthwise 1D convolutions and multi-scale fusion. Additionally, a feature dynamic fusion module (FDFM) adaptively integrates features guided by multiple TSEs based on input content, enhancing model robustness and expressiveness. Extensive experiments on benchmark datasets demonstrate that GIISR achieves state-of-the-art performance among lightweight SR models whose parameters are under 1M, highlighting its effectiveness and efficiency. All codes are available at https://github.com/clbsb/GIISR .},
  archive      = {J_ICV},
  author       = {Huan Yang and Runtao Liu and Xiaotong Zhou and Yuhui Zheng and Ru Zhao},
  doi          = {10.1016/j.imavis.2025.105642},
  journal      = {Image and Vision Computing},
  month        = {9},
  pages        = {105642},
  shortjournal = {Image Vis. Comput.},
  title        = {Expert-scoring guided global information interaction network for lightweight image super-resolution},
  volume       = {161},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). UniFormer: Consistency regularization-based semi-supervised semantic segmentation via differential dual-branch strongly augmented perturbations. <em>ICV</em>, <em>161</em>, 105640. (<a href='https://doi.org/10.1016/j.imavis.2025.105640'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Consistency regularization is a common approach in the field of semi-supervised semantic segmentation. Many recent methods typically adopt a dual-branch structure with strongly augmented perturbations based on the DeepLabV3+ model. However, these methods suffer from the limited receptive field of DeepLabV3+ and the lack of diversity in the predictions generated by the dual branches, leading to insufficient generalization performance. To address these issues, we propose a novel consistency regularization-based semi-supervised semantic segmentation framework, which adopts dual-branch SegFormer models as the backbone to overcome the limitations of the DeepLabV3+ model, termed UniFormer. We present a Random Strong Augmentation Perturbation (RSAP) module to enhance prediction diversity between the dual branches, thereby improving the robustness and generalization performance of UniFormer. In addition, we introduce a plug-and-play self-attention module that can effectively model the global dependencies of visual features to improve segmentation accuracy. Extensive experiments demonstrate that the proposed method achieves state-of-the-art performance on most evaluation protocols across the Pascal, Cityscapes, and COCO datasets. The code and pre-trained weights are available at: https://github.com/qskun/UniFormer .},
  archive      = {J_ICV},
  author       = {Shengkun Qi and Bing Liu and Yong Zhou and Peng Liu and Chen Zhang and Siyu Chen},
  doi          = {10.1016/j.imavis.2025.105640},
  journal      = {Image and Vision Computing},
  month        = {9},
  pages        = {105640},
  shortjournal = {Image Vis. Comput.},
  title        = {UniFormer: Consistency regularization-based semi-supervised semantic segmentation via differential dual-branch strongly augmented perturbations},
  volume       = {161},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). 3CNet: Cross-modal cooperative correction network for RGB-T semantic segmentation. <em>ICV</em>, <em>161</em>, 105638. (<a href='https://doi.org/10.1016/j.imavis.2025.105638'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Utilizing RGB-Thermal (RGB-T) data, multi-modal semantic segmentation enables pixel-wise classification of images across diverse environmental conditions, including variations in lighting and occlusions. However, a significant challenge persists in reducing the discrepancies between modalities while effectively utilizing their complementary strengths. To address this issue, a novel Cross-modal Cooperative Correction Network (3CNet) is proposed for RGB-T semantic segmentation. The core of 3CNet lies in the “correction-then-fusion” strategy. The cross-modal cooperative correction module employs orthogonal and spatial attention mechanisms to rectify the feature representations, thereby ensuring consistency and reliability of features for subsequent fusion. Additionally, a unique triple-stream feature fusion structure is introduced to enhance the efficiency of multi-modal feature utilization and improve overall fusion performance. Our proposed network demonstrates state-of-the-art performance on two RGB-T datasets, highlighting the potential of multi-modal information in advancing segmentation accuracy and confirming its efficacy in practical applications. The code and results are available at https://github.com/GraceGuoo/3CNet .},
  archive      = {J_ICV},
  author       = {Yixin Guo and Zhenxue Chen and Xuewen Rong and Chengyun Liu and Lili Song and Yidi Li},
  doi          = {10.1016/j.imavis.2025.105638},
  journal      = {Image and Vision Computing},
  month        = {9},
  pages        = {105638},
  shortjournal = {Image Vis. Comput.},
  title        = {3CNet: Cross-modal cooperative correction network for RGB-T semantic segmentation},
  volume       = {161},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multiscale contextual joint feature enhancement GAN for semantic image synthesis. <em>ICV</em>, <em>161</em>, 105637. (<a href='https://doi.org/10.1016/j.imavis.2025.105637'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Semantic image synthesis aims to generate images conditioned on semantic segmentation maps. Existing methods typically employ a generative adversarial framework to combine latent variables with semantic segmentation maps. However, traditional convolutions and feature map complexity often lead to issues such as uneven color, unrealistic textures, and blurred edges in generated images. To address these issues, we propose the Multiscale Contextual Joint Feature Enhancement Generative Adversarial Network, called MSCJ-GAN. Specifically, to capture local details and enhance the global consistency of large-scale objects, a large receptive field feature enhancement module based on Fast Fourier Convolution (FFC) and Transformer is introduced. This module employs an attention mechanism in the frequency domain, enabling neurons in the early layers of the network to access contextual information from the entire image. Furthermore, to ensure clear and realistic textures for objects and their boundaries, a dual-dimensional feature enhancement module based on bias is proposed. This module fully utilizes the statistical features in the feature maps, channel differences, and the detailed expression of the bias matrix to improve the realism of the generated images. Finally, experimental results on three challenging datasets demonstrate that the proposed MSCJ-GAN outperforms state-of-the-art methods, achieving superior performance in generating large-scale objects (e.g., sky and grass) and intricate texture details (e.g., wrinkles and micro-expressions). The code will be released after this work is published: https://github.com/xinxin0312/MSCJ-GAN .},
  archive      = {J_ICV},
  author       = {Hengyou Wang and Rongxin Ma and Xiang Jiang},
  doi          = {10.1016/j.imavis.2025.105637},
  journal      = {Image and Vision Computing},
  month        = {9},
  pages        = {105637},
  shortjournal = {Image Vis. Comput.},
  title        = {Multiscale contextual joint feature enhancement GAN for semantic image synthesis},
  volume       = {161},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). FF-UNet: Feature fusion based deep learning-powered enhanced framework for accurate brain tumor segmentation in MRI images. <em>ICV</em>, <em>161</em>, 105635. (<a href='https://doi.org/10.1016/j.imavis.2025.105635'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Medical imaging technology plays a crucial role in various medical sectors, aiding doctors in diagnosing patients. With brain tumors becoming a significant health concern due to their high morbidity and mortality rates, accurate and efficient tumor segmentation is essential. Manual segmentation methods are prone to errors and time-consuming. In this study, we investigate the potential of deep learning-based brain tumor MRI image segmentation techniques. We propose an enhanced approach called FF-UNet, which leverages feature fusion and combines the power of UNet and CNN models to improve segmentation accuracy. Preprocessing techniques are employed to enhance tumor visibility, followed by the utilization of a customized layered UNet model for segmentation. To mitigate overfitting, dropout layers are introduced after each convolution block stack. Additionally, a CNN process leverages the context of brain tumor MRI images to further enhance the model's segmentation performance. Experimental results demonstrate that our proposed framework outperforms state-of-the-art models in differentiating brain tissue. Across all datasets, our method achieves above 98% accuracy, with precision and Jaccard coefficient both exceeding 90%. Evaluation metrics such as the Jaccard index, sensitivity, and specificity validate the robust performance of our approach. The FF-UNet model holds great potential as a viable diagnostic tool, enabling radiologists to accurately segment brain tumor images and improve patient care.},
  archive      = {J_ICV},
  author       = {Uzair Aslam Bhatti and Jinru Liu and Mengxing Huang and Yu Zhang},
  doi          = {10.1016/j.imavis.2025.105635},
  journal      = {Image and Vision Computing},
  month        = {9},
  pages        = {105635},
  shortjournal = {Image Vis. Comput.},
  title        = {FF-UNet: Feature fusion based deep learning-powered enhanced framework for accurate brain tumor segmentation in MRI images},
  volume       = {161},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Composed image retrieval by multimodal mixture-of-expert synergy. <em>ICV</em>, <em>161</em>, 105634. (<a href='https://doi.org/10.1016/j.imavis.2025.105634'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Composed image retrieval (CIR) is essential in security surveillance, e-commerce, and social media analysis. It provides precise information retrieval and intelligent analysis solutions for various industries. The majority of existing CIR models create a pseudo-word token from the reference image, which is subsequently incorporated into the corresponding caption for the image retrieval task. However, these pseudo-word-based prompting approaches are limited when the target image entails complex modifications to the reference image, such as object removal and attribute changes. To address the issue, we propose a Multimodal Mixture-of-Expert Synergy (MMES) model to achieve effective composed image retrieval. The MMES model initially utilizes multiple Mixture of Expert (MoE) modules through the mixture expert unit to process various types of multimodal input data. Subsequently, the outputs from these expert models are fused through the cross-modal integration module. Furthermore, the fused features generate implicit text embedding prompts, which are concatenated with the relative descriptions. Finally, retrieval is conducted using a text encoder and an image encoder. The Experiments demonstrate that the proposed method outperforms state-of-the-art CIR methods on the CIRR and Fashion-IQ datasets.},
  archive      = {J_ICV},
  author       = {Wenzhe Zhai and Mingliang Gao and Gwanggil Jeon and Qiang Zhou and David Camacho},
  doi          = {10.1016/j.imavis.2025.105634},
  journal      = {Image and Vision Computing},
  month        = {9},
  pages        = {105634},
  shortjournal = {Image Vis. Comput.},
  title        = {Composed image retrieval by multimodal mixture-of-expert synergy},
  volume       = {161},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Vision transformer enhanced with convolutional attention and graph convolution for semantic segmentation. <em>ICV</em>, <em>161</em>, 105633. (<a href='https://doi.org/10.1016/j.imavis.2025.105633'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Semantic segmentation is a dense prediction task that assigns semantic labels to every pixel in an image. Effectively modeling global contextual information is a primary challenge in this task. Recently, some methods using Vision Transformer (ViT) encoders based on self-attention mechanisms have shown significant performance improvements. However, encoding spatial information purely through self-attention mechanisms tends to provide a more holistic representation and performs inadequately in handling object details. To address this, we propose a stripe depth-wise convolutional attention (SDCA) module. This module aggregates local convolution features at multiple scales as its attention map. Utilizing attention map generated by convolution at different scales effectively compensates for the limitations of self-attention mechanisms in handling object details. Additionally, to ensure the generation of more coherent predictions, we introduce a spatial feature graph convolution (SFGC) module to explicitly model the spatial relationships between patches. We apply these two modules in parallel to the output features of the Transformer block and add their output features to the original features for subsequent layer learning. Our method achieved mIoU scores of 50.5%, 59.1% and 55.0% on the COCO-Stuff-10K, PASCAL-Context and ADE20K datasets, respectively, surpassing some of the recent state-of-the-art methods.},
  archive      = {J_ICV},
  author       = {Yongzhi Liu and Tongxin Yan},
  doi          = {10.1016/j.imavis.2025.105633},
  journal      = {Image and Vision Computing},
  month        = {9},
  pages        = {105633},
  shortjournal = {Image Vis. Comput.},
  title        = {Vision transformer enhanced with convolutional attention and graph convolution for semantic segmentation},
  volume       = {161},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). CREAM: Few-shot object counting with cross REfinement and adaptive density map. <em>ICV</em>, <em>161</em>, 105632. (<a href='https://doi.org/10.1016/j.imavis.2025.105632'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Few-shot Object Counting (FSC) aims to accurately count objects of arbitrary categories in the query images. The standard pipeline is to extract exemplar features from the feature of query image and match them to obtain the final object counts. However, query image and exemplars often contain excessive background information and biases that do not belong to a specific category, which compromises the performance of feature matching and object counting. Another problem for traditional methods is that the ground truth density map is generated with fixed-size Gaussian distribution, which is inconsistent with the objects of varying scales in actual images. To address these problems, we propose a framework, termed as Cross REfinement and Adaptive density Map (CREAM), to extract the foreground information of all exemplars, eliminate the background information and make the single exemplar feature unbiased for the category to be counted. And in the same way the exemplars are used to weed out the background information in the query image. Moreover, instead of uniformly labeling all objects with Gaussian density maps of the same scale, we design a novel algorithm to generate the ground truth density map that considers the correlation between the density distribution of a single object and its scale, which is more consistent with the realistic scenarios. Extensive experiments on large-scale datasets, such as FSC-147, CARPK and ShanghaiTech, show that our method significantly outperforms the state-of-the-art approaches, and the detailed analysis also shows the effectiveness of each designed module. Code is available at https://github.com/CBalance/CREAM .},
  archive      = {J_ICV},
  author       = {Yuanwu Xu and Minxian Li and Qiaolin Ye and Shidong Wang and Lunbo Li and Haofeng Zhang},
  doi          = {10.1016/j.imavis.2025.105632},
  journal      = {Image and Vision Computing},
  month        = {9},
  pages        = {105632},
  shortjournal = {Image Vis. Comput.},
  title        = {CREAM: Few-shot object counting with cross REfinement and adaptive density map},
  volume       = {161},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). CNN and transformer-based deep learning models for automated white blood cell detection. <em>ICV</em>, <em>161</em>, 105631. (<a href='https://doi.org/10.1016/j.imavis.2025.105631'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The classification and detection of white blood cells (WBCs) are of great significance in clinical diagnostics. With the rapid advancement of deep learning, it has become increasingly important in various domains, particularly medical imaging. Deep learning-based object detection techniques can accurately and rapidly identify and localize various WBC types in images. In this study, we first construct a subtype-stained WBC detection dataset, which contains approximately 5000 images, with an equal distribution across 1000 images per WBC subtype. Leveraging object detection techniques combined with two different networks, Transformer and Convolutional Neural Network (CNN), we implement the Single Shot Multibox Detector (SSD) and Faster Region-based CNN (R-CNN) object detection networks using ResNet-50, ConvNeXt-Tiny, and Swin-Transformer-Tiny as backbones, in addition to the You Only Look Once (YOLO) v5 network. These models are trained, validated, and tested on the constructed WBC dataset, achieving high performance while also exhibiting some differences in detection accuracy and real-time efficiency. Specifically, the mean average precision (mAP)@0.5 on the test dataset exceeds 98% for all models, with mAP@0.5:0.95 scores of 81.6%, 82.1%, 79.7%, 84.3%, 85.2%, 84.4%, and 88.4%, respectively. The detection speeds reach 112.58, 131.99, 90.85, 38.12, 41.94, 34.60, and 99.01 frames per second (FPS), respectively. These findings provide useful insights for selecting appropriate models in clinical applications, promoting more efficient and reliable diagnostics. The dataset is publicly available at https://github.com/LZF5411/dataset/tree/master .},
  archive      = {J_ICV},
  author       = {Liangzun Fu and Jin Chen and Yang Zhang and Xiwei Huang and Lingling Sun},
  doi          = {10.1016/j.imavis.2025.105631},
  journal      = {Image and Vision Computing},
  month        = {9},
  pages        = {105631},
  shortjournal = {Image Vis. Comput.},
  title        = {CNN and transformer-based deep learning models for automated white blood cell detection},
  volume       = {161},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Doctor-in-the-loop: An explainable, multi-view deep learning framework for predicting pathological response in non-small cell lung cancer. <em>ICV</em>, <em>161</em>, 105630. (<a href='https://doi.org/10.1016/j.imavis.2025.105630'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Non-small cell lung cancer (NSCLC) remains a major global health challenge, with high post-surgical recurrence rates underscoring the need for accurate pathological response predictions to guide personalized treatments. Although artificial intelligence models show promise in this domain, their clinical adoption is limited by the lack of medically grounded guidance during training, often resulting in non-explainable intrinsic predictions. To address this, we propose Doctor-in-the-Loop , a novel framework that integrates expert-driven domain knowledge with explainable artificial intelligence techniques, directing the model towards clinically relevant anatomical regions and improving both interpretability and trustworthiness. Our approach employs a gradual multi-view strategy, progressively refining the model’s focus from broad contextual features to finer, lesion-specific details. By incorporating domain insights at every stage, we enhance predictive accuracy while ensuring that the model’s decision-making process aligns more closely with clinical reasoning. Evaluated on a dataset of NSCLC patients, Doctor-in-the-Loop delivers promising predictive performance and provides transparent, justifiable outputs, representing a significant step towards clinically explainable artificial intelligence in oncology.},
  archive      = {J_ICV},
  author       = {Alice Natalina Caragliano and Filippo Ruffini and Carlo Greco and Edy Ippolito and Michele Fiore and Claudia Tacconi and Lorenzo Nibid and Giuseppe Perrone and Sara Ramella and Paolo Soda and Valerio Guarrasi},
  doi          = {10.1016/j.imavis.2025.105630},
  journal      = {Image and Vision Computing},
  month        = {9},
  pages        = {105630},
  shortjournal = {Image Vis. Comput.},
  title        = {Doctor-in-the-loop: An explainable, multi-view deep learning framework for predicting pathological response in non-small cell lung cancer},
  volume       = {161},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). BrAInVision: A hybrid explainable artificial intelligence framework for brain MRI analysis. <em>ICV</em>, <em>161</em>, 105629. (<a href='https://doi.org/10.1016/j.imavis.2025.105629'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Brain tumors pose a significant medical challenge, characterized by high incidence and mortality rates, which underscore the critical need for accurate and early diagnosis using minimally invasive techniques such as magnetic resonance imaging. In this context, Artificial Intelligence has emerged as a promising tool to enhance diagnostic precision and efficiency. However, its widespread adoption in clinical practice remains limited due to the opacity of Artificial Intelligence-driven decision-making processes. To address this challenge, we introduce BrAInVision , a hybrid and doubly explainable AI framework for brain tumor detection. The novelty of our approach is the integration of both deep learning and traditional machine learning techniques, combining deep-extracted features with hand-crafted features to create a more robust and interpretable classification system. In contrast to conventional single-explanation methods, our framework provides comprehensive explainability through a multi-level analytical approach, enhancing both interpretability and transparency. The first level employs Grad-CAM to visualize regions of interest identified by the deep feature extractor, while the second level utilizes Permutation Feature Importance and Partial Dependence Plots to understand and quantify the contribution of specific image characteristics to diagnostic decisions. The proposed framework achieved an F1-score of 97% on the four classes (Glioma/Meningioma/Pituitary/NoTumor) and an average 99% in binary classification (Glioma/NoTumor), outperforming current state-of-the-art methods. The proposed approach has been validated on both the original dataset and an independent dataset with radiologist-annotated tumor masks, demonstrating strong generalizability. Designed for seamless integration into radiologists’ workflows as a decision support system, BrAInVision ensures a high degree of explainability, thereby fostering greater trust in AI-assisted medical decision-making.},
  archive      = {J_ICV},
  author       = {Marco Gagliardi and Danilo Maurmo and Tommaso Ruga and Eugenio Vocaturo and Ester Zumpano},
  doi          = {10.1016/j.imavis.2025.105629},
  journal      = {Image and Vision Computing},
  month        = {9},
  pages        = {105629},
  shortjournal = {Image Vis. Comput.},
  title        = {BrAInVision: A hybrid explainable artificial intelligence framework for brain MRI analysis},
  volume       = {161},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). UHDNet: Unified multimodal fusion harmonization and hierarchical dependency learning for visible-infrared person re-identification. <em>ICV</em>, <em>161</em>, 105628. (<a href='https://doi.org/10.1016/j.imavis.2025.105628'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Person re-identification focuses on recognizing and matching the same pedestrian across different camera views, which has important applications in intelligent surveillance, intelligent transportation, and other fields. Due to the significant differences within and between modalities, Visible-Infrared Person Re-Identification (VI-ReID) is a highly challenging task. However, existing methods primarily map the two modalities into a shared feature space, relying solely on modality-shared features, which limits the discriminative power of feature representations and overlooks useful information in modality-specific features, such as color and contrast. Therefore, we propose a novel Unified Harmonisation and Dependency Network (UHDNet). Specifically, the Multimodal Fusion Harmonizer (MFH) dynamically models modality-specific and modality-shared features in an instance-guided manner, effectively mitigating style variations within specific modalities while preserving discriminative information. Given the critical role of relationships between different body parts in distinguishing occluded and overlapping pedestrians, we design a Hierarchical Relational Attention Module (HRAM) to capture hierarchical relationships between different parts using a differential global–local similarity matrix. Finally, to distinguish modality-specific and modality-shared feature representations, we introduce instance-level classification and alignment losses, along with modality-level alignment loss, enabling precise identity alignment and modality consistency. To verify the effectiveness of the proposed architecture, we conducted comparative and ablation studies on the public datasets SYSU-MM01, RegDB, and LLCM. Our method achieved superior performance, with mAP scores of 80.5% for all-search and 89.9% for indoor-search on SYSU-MM01, 90.1% for VIS-to-IR and 88.9% for IR-to-VIS on RegDB, and 66.7% for IR-to-VIS and 67.2% for VIS-to-IR on LLCM.},
  archive      = {J_ICV},
  author       = {Xiaobin Hong and Tarmizi Adam and Masitah Ghazali},
  doi          = {10.1016/j.imavis.2025.105628},
  journal      = {Image and Vision Computing},
  month        = {9},
  pages        = {105628},
  shortjournal = {Image Vis. Comput.},
  title        = {UHDNet: Unified multimodal fusion harmonization and hierarchical dependency learning for visible-infrared person re-identification},
  volume       = {161},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). DE-DFNet: Edge enhanced diversity feature fusion guided by differences in remote sensing imagery tiny object detection. <em>ICV</em>, <em>161</em>, 105627. (<a href='https://doi.org/10.1016/j.imavis.2025.105627'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Object detection in remote sensing images (RSIs) aims to identify targets at varying altitudes. High imaging altitudes result in diverse object scales, while complex backgrounds complicate detection. Although Feature Pyramid Networks (FPNs) and attention mechanisms have been employed to tackle these issues, FPNs often face semantic gaps that hinder accurate detection across different object sizes. Attention mechanisms primarily focus on local regions, frequently overlooking crucial edge information needed for precise object delineation. Moreover, tiny objects with similar appearances are often misclassified, as existing methods prioritize shared features over distinctive characteristics. To address these limitations, we propose DE-DFNet, a novel detection framework. We introduce the Difference Enhancer (DE), which uniquely combines distributional differences and structural similarities (SSIM) to assess feature similarities and differences. In the feature fusion stage, we design the Edge Boosting Selection Perceptron (EBSP), integrated with DE, to extract richer features while enhancing edge information, thereby improving detection accuracy for tiny objects in complex backgrounds. In the detection phase, we propose the Dynamic Selective Progressive Pyramid (DSPP) combined with DE and Multi-Layer Dilated Convolutions (MDC) to mitigate semantic gaps and enhance multi-scale feature fusion. Finally, we introduce DE-loss, which guides the model to extract diverse features from both distributional and structural perspectives, improving the classification of visually similar tiny objects. We evaluate DE-DFNet against state-of-the-art detection models from the past three years on three datasets for RSI tiny object detection: DOTA (74.2% mAP), AI-TOD (54.9% mAP) and RSOD. The source code will be released at https://github.com/futianyi3/DE-DFNet .},
  archive      = {J_ICV},
  author       = {Tianyi Fu and Hongbin Dong and Benyi Yang and Baosong Deng},
  doi          = {10.1016/j.imavis.2025.105627},
  journal      = {Image and Vision Computing},
  month        = {9},
  pages        = {105627},
  shortjournal = {Image Vis. Comput.},
  title        = {DE-DFNet: Edge enhanced diversity feature fusion guided by differences in remote sensing imagery tiny object detection},
  volume       = {161},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A survey of computational techniques for fine art painting classification. <em>ICV</em>, <em>161</em>, 105626. (<a href='https://doi.org/10.1016/j.imavis.2025.105626'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Fine art painting (FAP) classification has gained significant attention in the fields of computer vision and artificial intelligence due to its applications in art authentication, digital archiving, and cultural heritage preservation. FAP classification involves recognizing different attributes such as artist, style, and genre. It poses unique challenges due to variations in artistic techniques, brushstroke patterns, and color compositions. This paper presents a comprehensive survey of computational techniques used for FAP classification, categorizing them into three major categories: image processing-based methods, machine learning-based methods, and deep learning-based methods. The first category explores image processing based methods for FAP classification focus on handcrafted feature extraction techniques to differentiate between artistic styles, genres, and artists. The second category focuses on machine learning approaches that rely on handcrafted feature extraction techniques. Later, these extracted features are then fed into machine learning classifiers to categorize paintings based on artist, style, or genre. The final category discusses recent advances in deep learning-based methods for FAP classification. The study is particularly beneficial for art historians, museum curators, digital archivists, and cultural heritage institutions, offering insights into how computational tools can support art analysis, preservation, and access.},
  archive      = {J_ICV},
  author       = {Vishwas Rathi and Abhilasha Sharma and Aditya Venkata Nithin and Amit Kumar Singh and Brij B. Gupta},
  doi          = {10.1016/j.imavis.2025.105626},
  journal      = {Image and Vision Computing},
  month        = {9},
  pages        = {105626},
  shortjournal = {Image Vis. Comput.},
  title        = {A survey of computational techniques for fine art painting classification},
  volume       = {161},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Point-cloud-based hand gesture recognition using principal component analysis and boundary extraction. <em>ICV</em>, <em>161</em>, 105625. (<a href='https://doi.org/10.1016/j.imavis.2025.105625'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this work, we introduce a method for hand gesture recognition that utilizes a Time-of-Flight (ToF) camera and 3D point cloud networks. A dataset of hand gesture point clouds, specifically digits 0–9, is created using a ToF depth camera. These data are then subjected to a data compression algorithm, which combines point cloud principal component analysis (PCA) with point cloud boundary extraction. The effectiveness of the proposed data compression algorithm in the context of hand gesture recognition is evaluated using seven different point cloud recognition networks. The experimental results demonstrate that the algorithm not only exhibits generalizability across various point cloud classification models but also significantly reduces the size of the hand gesture point cloud data while maintaining high recognition accuracy.},
  archive      = {J_ICV},
  author       = {Yiwen Zhang and Dong An and Dongzhao Yang and Tianxu Xu and Yuxuan He and Qiang Wang and Zhongqi Pan and Yang Yue},
  doi          = {10.1016/j.imavis.2025.105625},
  journal      = {Image and Vision Computing},
  month        = {9},
  pages        = {105625},
  shortjournal = {Image Vis. Comput.},
  title        = {Point-cloud-based hand gesture recognition using principal component analysis and boundary extraction},
  volume       = {161},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Camera information-induced vision transformer for unsupervised person re-identification. <em>ICV</em>, <em>161</em>, 105624. (<a href='https://doi.org/10.1016/j.imavis.2025.105624'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Unsupervised person re-identification (ReID) is a challenging task due to the lack of true labels and the variations introduced by different camera views, which affect the model accuracy. Traditional methods often use convolutional neural networks (CNNs) to extract features, but pooling, convolution, and other operations can cause significant loss of information. Moreover, the multi-camera scenario in ReID leads to camera-induced feature biases, further complicating the task. In response to these issues, this paper proposes a camera information-induced (CII) vision transformer for unsupervised person ReID. The method incorporates camera information in two ways: by embedding it into feature extraction to reduce bias and using a camera matrix in the clustering process to minimize distance discrepancies between cameras. In addition, instance batch normalization (IBN) is employed to improve feature discrimination. Experiments on four different benchmark datasets demonstrate the superior efficiency and performance of the method, achieving better accuracy and faster processing compared to existing methods.},
  archive      = {J_ICV},
  author       = {Qing Tian and Jiashuo Shen and Zixiao Zhou and Jixin Sun and Junyu Shen and Weihua Ou},
  doi          = {10.1016/j.imavis.2025.105624},
  journal      = {Image and Vision Computing},
  month        = {9},
  pages        = {105624},
  shortjournal = {Image Vis. Comput.},
  title        = {Camera information-induced vision transformer for unsupervised person re-identification},
  volume       = {161},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Research on collaborative camouflaged object detection under dual domain entanglement. <em>ICV</em>, <em>161</em>, 105623. (<a href='https://doi.org/10.1016/j.imavis.2025.105623'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The aim of collaborative camouflaged object detection (CoCOD) is to identify camouflaged objects with the same attribute from a grouped images. However, several existing CoCOD networks demonstrate inadequate segmentation performance in complex scenes, resulting in the loss of critical texture information and structural features. To tackle the challenge, we propose a dual domain entanglement network based on frequency learning and channel shuffle (FCNet), which has three innovative points: (1) The consensus feature extraction module (CFEM) utilizes frequency domain awareness unit (FDAU) and dual domain entangled unit (DDEU) to extract and integrate co-camouflage information across both frequency and spatial domains. (2) The local semantic capture module (LSCM) identifies significant features within the image through channel shuffle and Discrete wavelet transform (DWT). (3) The attention interaction fusion module (AIFM) facilitates the interactive fusion of features both inter- and intra-images to derive the final prediction map. Comprehensive experiments conducted on the CoCOD8K dataset demonstrate that FCNet significantly surpasses 13 camouflage models and 8 state-of-the-art collaborative salient object detection (CoSOD) methods across six widely recognized evaluation metrics. The code and results of our method are available at https://github.com/ZX123445/FCNet .},
  archive      = {J_ICV},
  author       = {Yanliang Ge and Yuxi Zhong and Qiao Zhang and Junchao Ren and Min He and Hongbo Bi},
  doi          = {10.1016/j.imavis.2025.105623},
  journal      = {Image and Vision Computing},
  month        = {9},
  pages        = {105623},
  shortjournal = {Image Vis. Comput.},
  title        = {Research on collaborative camouflaged object detection under dual domain entanglement},
  volume       = {161},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). ACMC: Adaptive cross-modal multi-grained contrastive learning for continuous sign language recognition. <em>ICV</em>, <em>161</em>, 105622. (<a href='https://doi.org/10.1016/j.imavis.2025.105622'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Continuous sign language recognition helps the hearing-impaired community participate in social communication by recognizing the semantics of sign language video. However, the existing CSLR methods usually only implement cross-modal alignment at the sentence level or frame level, and do not fully consider the potential impact of redundant frames and semantically independent gloss identifiers on the recognition results. In order to improve the limitations of the above methods, we propose an adaptive cross-modal multi-grained contrastive learning (ACMC) for continuous sign language recognition, which achieve more accurate cross-modal semantic alignment through a multi-grained contrast mechanism. First, the ACMC uses the frame extractor and the temporal modeling module to obtain the fine-grained and coarse-grained features of the visual modality in turn, and extracts the fine-grained and coarse-grained features of the text modality through the CLIP text encoder. Then, the ACMC adopts coarse-grained contrast and fine-grained contrast methods to effectively align the features of visual and text modalities from global and local perspectives, and alleviate the semantic interference caused by redundant frames and semantically independent gloss identifiers through cross-grained contrast. In addition, in the video frame extraction stage, we design an adaptive learning module to strengthen the features of key regions of video frames through the calculated discrete spatial feature decision matrix, and adaptively fuse the convolution features of key frames with the trajectory information between adjacent frames, thereby reducing the computational cost. Experimental results show that the proposed ACMC model achieves very competitive recognition results on sign language datasets such as PHOENIX14, PHOENIX14-T and CSL-Daily.},
  archive      = {J_ICV},
  author       = {Xu-Hua Yang and Hong-Xiang Hu and XuanYu Lin},
  doi          = {10.1016/j.imavis.2025.105622},
  journal      = {Image and Vision Computing},
  month        = {9},
  pages        = {105622},
  shortjournal = {Image Vis. Comput.},
  title        = {ACMC: Adaptive cross-modal multi-grained contrastive learning for continuous sign language recognition},
  volume       = {161},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Similarity verification of kinship pairs using metricized emphasis. <em>ICV</em>, <em>161</em>, 105619. (<a href='https://doi.org/10.1016/j.imavis.2025.105619'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Kinship verification is the determination of the validity of biological ties or kinship between two or more individuals, giving insights about genetic trait inheritances and other applications like forensic investigations. This paper presents a deep learning approach to kinship verification that methodically evaluates the similarity between images of kin. The proposed approach, Age-Modified Metricized Filtering (AMMF), begins by augments images via a Cycle-Generative Adversarial Network setup for aging child images, which increases facial parameters and reduces age gap. It then quantifies genetic inheritance by a novel method, Metricized Weight-based Emphasis Filtering, which reconciles facial proportions between the older and younger generation, and then uses Siamese networks for feature embedding and similarity evaluation. The approach is evaluated on a merged dataset of KinFaceW-I and KinFaceW-II, and achieves state-of-the-art performance. The results are suitable for real-world applications, achieving a training accuracy, AUC, and contrastive loss of 97.4%, 0.74 and 0.11 respectively. The approach also achieves 89.41% and 87.86% training accuracy on FIW and TSKinFace datasets respectively. This will contribute toward an accurate determination of the validity of kinship ties, thus contributing to tasks like image management, genealogical research, and criminal investigations.},
  archive      = {J_ICV},
  author       = {Chhavi Maheshwari and Siddhanth Bhat and Praveen Kumar Shukla and Madhu Oruganti and Vijaypal Singh Dhaka},
  doi          = {10.1016/j.imavis.2025.105619},
  journal      = {Image and Vision Computing},
  month        = {9},
  pages        = {105619},
  shortjournal = {Image Vis. Comput.},
  title        = {Similarity verification of kinship pairs using metricized emphasis},
  volume       = {161},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Event-level multimodal feature fusion for audio–visual event localization. <em>ICV</em>, <em>161</em>, 105610. (<a href='https://doi.org/10.1016/j.imavis.2025.105610'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Audio–visual event localization, by identifying audio–visual segments most relevant to semantics from long video sequences, has become a crucial prerequisite for applications such as video content understanding and editing. Albeit the rich visual and auditory information in video data greatly enhances the accuracy of event localization models, the challenges, however, such as visual ambiguity, occlusion, small-scale targets, and sparse auditory features, hinder the acquisition of temporally continuous video segments with semantic consistency for events. To this end, we propose an event localization model with event-level multimodal feature fusion strategy to encode the event semantics consistency from video data, thereby improving the event localization accuracy. In particular, a multimodal features and distribution consistency loss is devised to train the spatial attention based architecture, along with the supervised loss, to fuse the multi-modal attended features to achieve the semantic consistency. To further mitigate the detrimental impact of outliers, i.e., the segments with non-relevant semantics, we propose to learn adaptive continuity sampling parameters to construct segment content sets with consistent semantics to the video, the experimental results demonstrate the advantages of our model against the existing event localization counterparts.},
  archive      = {J_ICV},
  author       = {Jing Zhang and Yi Yu and Yuyao Mao and Yonggong Ren},
  doi          = {10.1016/j.imavis.2025.105610},
  journal      = {Image and Vision Computing},
  month        = {9},
  pages        = {105610},
  shortjournal = {Image Vis. Comput.},
  title        = {Event-level multimodal feature fusion for audio–visual event localization},
  volume       = {161},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). VideoMamba++: Integrating state space model with dual attention for enhanced video understanding. <em>ICV</em>, <em>161</em>, 105609. (<a href='https://doi.org/10.1016/j.imavis.2025.105609'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the development of deep learning, there has been significant progress in applications related to video understanding tasks. Mamba has demonstrated promising potential in video understanding due to its linear complexity and superior long-range dependency modeling, as seen in VideoMamba. However, VideoMamba suffers from spatial relationship loss both before patching and during modeling, due to limitations in the one-dimensional data representation and processing approach of Mamba. To address these problems, this paper proposes a new video understanding model, named VideoMamba++. The model effectively captures spatial relationships before patching by introducing gated patch convolution. Additionally, it strengthens the capture of neighboring spatial relationships and mitigates the absence of channel attention by integrating a residual attention Mamba block, leveraging the synergy of dual attention mechanisms and bidirectional Mamba modules. We evaluated VideoMamba++ on two benchmark datasets, observing significant accuracy improvements over the baseline model. It achieved a Top-1 accuracy increase of 3.6% on Kinetics-400 and 3.2% on Something-Something V2, performing on par with state-of-the-art models on Kinetics-400. This novel design retains the strengths of Mamba in handling long-range dependencies while optimizing spatial relationships and channel attention, resulting in a more comprehensive video understanding.},
  archive      = {J_ICV},
  author       = {Xin Song and Wang Tian and Qiqi Zhu and Xianglong Zhang},
  doi          = {10.1016/j.imavis.2025.105609},
  journal      = {Image and Vision Computing},
  month        = {9},
  pages        = {105609},
  shortjournal = {Image Vis. Comput.},
  title        = {VideoMamba++: Integrating state space model with dual attention for enhanced video understanding},
  volume       = {161},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Visual-aware text as query for referring video object segmentation. <em>ICV</em>, <em>161</em>, 105608. (<a href='https://doi.org/10.1016/j.imavis.2025.105608'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Current referring video object segmentation (R-VOS) approaches rely on directly identifying, locating, and segmenting referenced objects from text referring expressions in videos. However, there is inherent ambiguities in text referring expressions that can significantly negatively impact model performance. To address this challenge, a novel R-VOS method taking Visual-Aware Text as Query (VATaQ) is proposed, in which the referring expression is reconstructed with the guidance of visual feature, leading text feature to be highly relevant to the current video, thereby enhancing the clarity of the expressions. Furthermore, a CLIP-side Adapter Module (CAM), which leverages semantically enriched CLIP to enhance the visual feature with more semantic information, thus helping the model achieve a more comprehensive multi-modal representation. Experimental results show that the VATaQ shows outstanding performance on four video benchmark datasets, which outperforms the baseline network by 3.4% on the largest Ref-YouTube-VOS dataset.},
  archive      = {J_ICV},
  author       = {Qi Kuang and Ying Chen},
  doi          = {10.1016/j.imavis.2025.105608},
  journal      = {Image and Vision Computing},
  month        = {9},
  pages        = {105608},
  shortjournal = {Image Vis. Comput.},
  title        = {Visual-aware text as query for referring video object segmentation},
  volume       = {161},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). SATA: Style agnostic test time adaptation for domain generalization. <em>ICV</em>, <em>161</em>, 105607. (<a href='https://doi.org/10.1016/j.imavis.2025.105607'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Test-time adaptation (TTA) techniques seek to mitigate distributional discrepancies between training and test data by adjusting the model dynamically during inference. This method is especially effective in the Domain Generalization (DG) setting, as it leverages online test data during inference for adaptation. The online model updating process of TTA can sometimes be unstable, or biased toward dominant classes presenting a significant challenge that hinders the deployment of existing TTA methods in real-world scenarios. Furthermore, existing TTA methods do not exploit specific source domain information to effectively minimize the gap between the training and test domains during test time adaptation. To address the mentioned issues, we propose a style-agnostic test time adaptation method, called SATA that utilizes source domain-specific style information, to select reliable test samples for a stable entropy minimization process. Moreover, we introduce an innovative diversity-aware consistency loss, which compels unreliable test samples to yield predictions that are both consistent and diverse. We extensively assess our approach by employing two popular DG methods, ERM and CORAL, on four well-known DG benchmark datasets: VLCS, PACS, OfficeHome, and TerraIncognita. Our approach showcases remarkable effectiveness as it consistently enhances the performance of ERM across all datasets, and surpasses current state-of-the-art techniques. The code is available at https://github.com/saeedkarimi90/SATA .},
  archive      = {J_ICV},
  author       = {Saeed Karimi and Hamdi Dibeklioğlu},
  doi          = {10.1016/j.imavis.2025.105607},
  journal      = {Image and Vision Computing},
  month        = {9},
  pages        = {105607},
  shortjournal = {Image Vis. Comput.},
  title        = {SATA: Style agnostic test time adaptation for domain generalization},
  volume       = {161},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multimodal sensitive adaptive transformer for 3D medical image segmentation. <em>ICV</em>, <em>161</em>, 105606. (<a href='https://doi.org/10.1016/j.imavis.2025.105606'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Three-dimensional medical imaging segmentation presents a significant challenge within the field, with the segmentation of multiple organs and lesions in MRI images being particularly demanding. This paper introduces an innovative approach utilizing the Multimodal Sensitive Adaptive Attention (MSAA). We refer to this new structure as the Multimodal Sensitive Adaptive Transformer Network (MSAT), which incorporates downsampling and Multimodal Sensitive Adaptive Attention into the encoding phase and integrate skip connections from different layers, outputs from Multimodal Sensitive Adaptive Attention, and upsampled feature outputs into the decoding phase. The MSAT consists of two primary components. The initial component is designed to extract a richer set of high-dimensional features through an advanced network architecture. This includes integration of different layers skip connections, outputs from the MSAA, and the results of the preceding upsampling layer. The second component features a Multimodal Sensitive Adaptive Attention block, which integrates two types of attention mechanisms: Local Sensitive Adaptive Attention (LSAA) and Spatial Sensitive Adaptive Attention (SSAA). These attention mechanisms work synergistically to blend high and low-dimensional features effectively, thereby enriching the contextual information captured by the model. Our experiments, conducted across several datasets including Synapse, BTCV, ACDC, and the BraTS 2021 dataset, demonstrate that the MSAT outperforms other existing methodologies. The MSAT shows superior segmentation capabilities for 3D multi-organ, cardiac, and brain tumor segmentation tasks.},
  archive      = {J_ICV},
  author       = {Zhibing Wang and Wenmin Wang and Nannan Li and Qi Chen and Yifan Zhang and Meng Xiao and Haomei Jia and Shenyong Zhang},
  doi          = {10.1016/j.imavis.2025.105606},
  journal      = {Image and Vision Computing},
  month        = {9},
  pages        = {105606},
  shortjournal = {Image Vis. Comput.},
  title        = {Multimodal sensitive adaptive transformer for 3D medical image segmentation},
  volume       = {161},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). SFDFNet: Leveraging spatial-frequency deep fusion for RGB-T semantic segmentation. <em>ICV</em>, <em>161</em>, 105605. (<a href='https://doi.org/10.1016/j.imavis.2025.105605'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Due to the insensitivity to lighting variations, the RGB-Thermal (RGB-T) semantic segmentation models show significant potential in processing images captured under adverse conditions, such as low light and overexposure. Current RGB-T semantic segmentation methods usually rely on complex spatial domain fusion strategies, yet they neglect the complementary frequency characteristics of RGB and thermal modalities. Through frequency analysis, we find that thermal images focus on low-frequency information, while RGB images are rich in high-frequency details. Leveraging these complementary properties, we introduce the Spatial-Frequency Deep Fusion Network (SFDFNet), which employs a dual-stream architecture to enhance RGB-T semantic segmentation. Key innovations include the Distinctive Feature Enhancement Module (DFEM) to improve feature representation in both modalities and the Spatial-Frequency Fusion Module (SFFM), which integrates spatial and frequency features to optimize cross-modal fusion. Extensive experiments on three RGB-T datasets demonstrate the superior performance of our method, both qualitatively and quantitatively, compared to state-of-the-art models.},
  archive      = {J_ICV},
  author       = {Guanhua An and Yuhe Geng and Shengyu Fang and Jichang Guo},
  doi          = {10.1016/j.imavis.2025.105605},
  journal      = {Image and Vision Computing},
  month        = {9},
  pages        = {105605},
  shortjournal = {Image Vis. Comput.},
  title        = {SFDFNet: Leveraging spatial-frequency deep fusion for RGB-T semantic segmentation},
  volume       = {161},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Design of a novel fuzzy ensemble CNN framework for ovarian cancer classification using tissue microarray images. <em>ICV</em>, <em>161</em>, 105604. (<a href='https://doi.org/10.1016/j.imavis.2025.105604'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Background: Ovarian cancer remains a significant health concern, with a high mortality rate often attributed to late diagnosis. Tissue Microarray (TMA) images offer a cost-effective diagnostic tool, but their manual analysis is time-consuming and requires expert interpretation. To address this, we aim to develop an automated deep learning solution. Purpose: This study seeks to develop a robust deep learning method for classifying ovarian cancer TMA images. Specifically, we compare the performance of different Convolutional Neural Network (CNN) architectures and propose an improved ensemble model to enhance diagnostic accuracy and streamline the clinical workflow. Methods: The training dataset comprises 12,710 TMA images sourced from various repositories. These images were meticulously labeled into five distinct categories, CC, EC, HGSC, LGSC, and MC, using original data sources and expert annotations. In the first stage, we trained five CNN models, including our proposed EOC-Net and four transfer learning models: DenseNet121, EfficientNetB0, InceptionV3, and ResNet50-v2. In the second stage, we constructed a fuzzy rank-based ensemble model utilizing the Gamma function to combine the predictions from the individual models, aiming to optimize overall accuracy. Results: In the first stage, the models achieved Training Accuracies ranging from 86.95% to 96.29% and Testing Accuracies ranging from 76.25% to 87.05%. Notably, EOC-Net, despite having significantly fewer parameters, emerged as the top-performing model. However, in the second stage, the proposed ensemble model surpassed all individual models, achieving an Accuracy of 88.73%, representing a substantial improvement of 1.68%–12.48%. Conclusion: Our study underscores the potential of Deep Learning and Ensemble Learning techniques for accurately classifying ovarian cancer TMA images. The ensemble model’s superior performance demonstrates its ability to enhance diagnostic precision, potentially reducing the workload for clinical experts and improving patient outcomes.},
  archive      = {J_ICV},
  author       = {Thien B. Nguyen-Tat and Anh T. Vu-Xuan and Vuong M. Ngo},
  doi          = {10.1016/j.imavis.2025.105604},
  journal      = {Image and Vision Computing},
  month        = {9},
  pages        = {105604},
  shortjournal = {Image Vis. Comput.},
  title        = {Design of a novel fuzzy ensemble CNN framework for ovarian cancer classification using tissue microarray images},
  volume       = {161},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Enhancing radiology report generation: A prior knowledge-aware transformer network for effective alignment and fusion of multi-modal radiological data. <em>ICV</em>, <em>161</em>, 105603. (<a href='https://doi.org/10.1016/j.imavis.2025.105603'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Medical imaging and their reports are essential in healthcare, providing crucial insights into internal structures and abnormalities for diagnosis and treatment. However, medical radiology report generation is time-consuming and further complicated by the shortage of expert radiologists. This paper presents a deep learning-based prior knowledge-aware transformer network designed to address the challenges of aligning and fusing medical images with textual data. Our method integrates medical signals of contextual biomedical entities and auxiliary medical knowledge embeddings extracted from reports with the visual features of radiology images to enhance alignment. Further, to tackle the fusion issue, we introduce Prior-Knowledge-Aware-Report-Generator, a novel module with pre-normalization layers designed to improve training stability and efficiency, a prior knowledge-aware cross-attention mechanism to focus on multi-modal unified fused prior knowledge representation of radiology images and medical signals, and a feedforward layer utilizing the SwiGLU gated activation function, enhancing receptive field coverage. This ensures the model effectively incorporates and exploits prior medical knowledge to generate high-quality reports. We evaluate our method using standard natural language generation metrics on three widely used publicly available datasets — IUXRAY, COVCTR, and PGROSS. Our approach achieves average Bleu scores of 0.383, 0.647, and 0.191 for the respective datasets, outperforming existing state-of-the-art methods further evidenced by rigorous ablation and qualitative analysis conducted that taps into the contributions of various components of our model granting relevant clinical insights. The results demonstrate that the fusion of medical signals with radiology images significantly improves report accuracy and alignment with clinical findings, providing valuable assistance to radiologists.},
  archive      = {J_ICV},
  author       = {Amaan Izhar and Norisma Idris and Nurul Japar},
  doi          = {10.1016/j.imavis.2025.105603},
  journal      = {Image and Vision Computing},
  month        = {9},
  pages        = {105603},
  shortjournal = {Image Vis. Comput.},
  title        = {Enhancing radiology report generation: A prior knowledge-aware transformer network for effective alignment and fusion of multi-modal radiological data},
  volume       = {161},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Semi-supervised cross-modality person re-identification based on pseudo label learning. <em>ICV</em>, <em>161</em>, 105602. (<a href='https://doi.org/10.1016/j.imavis.2025.105602'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Visible-infrared person re-identification (RGB-IR Re-ID) aims to find images of the same identity from different modalities. In practice, multiple person and cameras can provide abundant training samples and non-negligible modality differences makes manual labeling of all samples be impractical. How to accurately re-identify cross-modality pedestrians under the training condition of having few labeled samples and a quantity of unlabeled samples is an important research question. However, person re-identification in this scenario, which we call Semi-Supervised Cross-Modality Re-ID (SSCM Re-ID), has not been well studied. In this paper, we propose a cross-modality pseudo label learning (CPL) framework for SSCM Re-ID task. It consists of three modules: the feature mapping module, the identity alignment module and the pseudo-label generation module. The feature mapping module is designed to extract shared discriminatory features from modality-specific channels, followed by the identity alignment module that aims to align person identities jointly at the global-level and part-level aspects. Finally, the pseudo-label generation module is used to select samples with reliable pseudo labels from the unlabeled samples based on the confidence level. Moreover, we propose the dynamic center-based cross-entropy loss to constrain the distance of similar samples. Experiments on widely used cross-modality Re-ID datasets demonstrate that CPL can achieve the state-of-the-art SSCM Re-ID performance.},
  archive      = {J_ICV},
  author       = {Fei Wu and Ruixuan Zhou and Yang Gao and Yujian Feng and Qinghua Huang and Xiao-Yuan Jing},
  doi          = {10.1016/j.imavis.2025.105602},
  journal      = {Image and Vision Computing},
  month        = {9},
  pages        = {105602},
  shortjournal = {Image Vis. Comput.},
  title        = {Semi-supervised cross-modality person re-identification based on pseudo label learning},
  volume       = {161},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Deep learning advances in breast medical imaging with a focus on clinical readiness and radiologists’ perspective. <em>ICV</em>, <em>161</em>, 105601. (<a href='https://doi.org/10.1016/j.imavis.2025.105601'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Breast cancer is the leading cause of death from cancer among women globally. According to the World Health Organization (WHO), early detection and treatment can significantly reduce surgeries and improve survival rates. Since deep learning emerged in 2012, it has garnered significant research interest in breast cancer, particularly for diagnosis, treatment, prognosis, and survival prediction. This review specifically focuses on the application of deep learning to breast image analysis (MRI, mammogram, and ultrasound) with a particular emphasis on radiologist involvement in the evaluation process. Studies published between 2019 and 2024 in the Scopus database will be reviewed. We further explore radiologists’ perspectives on the clinical readiness of artificial intelligence (AI) for breast image analysis. By analyzing insights from published articles, we will discuss the challenges, limitations, and future directions for this evolving field. While the review highlights the promise of deep learning in breast image analysis, it also acknowledges critical issues that must be addressed before widespread clinical integration can be achieved.},
  archive      = {J_ICV},
  author       = {Oladosu Oyebisi Oladimeji and Abdullah Al-Zubaer Imran and Xiaoqin Wang and Saritha Unnikrishnan},
  doi          = {10.1016/j.imavis.2025.105601},
  journal      = {Image and Vision Computing},
  month        = {9},
  pages        = {105601},
  shortjournal = {Image Vis. Comput.},
  title        = {Deep learning advances in breast medical imaging with a focus on clinical readiness and radiologists’ perspective},
  volume       = {161},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Memory augmented using diffusion model for class-incremental learning. <em>ICV</em>, <em>161</em>, 105600. (<a href='https://doi.org/10.1016/j.imavis.2025.105600'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Class-incremental learning aims to learn new classes in an incremental fashion without forgetting the previously learned ones. Several research works have shown how additional data can be used by incremental models to help mitigate catastrophic forgetting. In this work, following the recent breakthrough in text-to-image generative models and their wide distribution, we propose the use of a pre-trained Diffusion Model as a source of additional data for class-incremental learning. Compared to competitive methods that rely on external, often unlabeled, datasets of real images, our approach can generate synthetic samples that belong to the same classes as the previously encountered images. This allows us to use those additional data samples not only in the distillation loss but also for replay in supervised losses such as the classification loss. Experiments on the competitive benchmarks CIFAR100, ImageNet-Subset, and ImageNet demonstrate how this new approach can be used to further improve the performance of state-of-the-art methods for class-incremental learning on large scale datasets.},
  archive      = {J_ICV},
  author       = {Quentin Jodelet and Xin Liu and Yin Jun Phua and Tsuyoshi Murata},
  doi          = {10.1016/j.imavis.2025.105600},
  journal      = {Image and Vision Computing},
  month        = {9},
  pages        = {105600},
  shortjournal = {Image Vis. Comput.},
  title        = {Memory augmented using diffusion model for class-incremental learning},
  volume       = {161},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). MambaGait: Gait recognition approach combining explicit representation and implicit state space model. <em>ICV</em>, <em>161</em>, 105597. (<a href='https://doi.org/10.1016/j.imavis.2025.105597'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Gait recognition aims to identify pedestrians based on their unique walking patterns and has gained significant attention due to its wide range of applications. Mamba, a State Space Model, has shown great potential in modeling long sequences. However, its limited ability to capture local details hinders its effectiveness in fine-grained tasks like gait recognition. Moreover, similar to convolutional neural networks and transformers, Mamba primarily relies on implicit learning, which is constrained by the sparsity of binary silhouette sequences. Inspired by explicit feature representations in scene rendering, we introduce a novel gait descriptor, the Explicit Spatial Representation Field (ESF). It represents silhouette images as directed distance fields, enhancing their sensitivity to gait motion and facilitating richer spatiotemporal feature extraction. To further improve Mamba’s ability to capture local details, we propose the Temporal Window Switch Mamba Block (TWSM), which effectively extracts local and global spatiotemporal features via bidirectional temporal window switching. By combining explicit representation and implicit Mamba modeling, MambaGait achieves state-of-the-art performance on four challenging datasets (GREW, Gait3D, CCPG, and SUSTech1K). Code: https://github.com/Haijun-Xiong/MambaGait .},
  archive      = {J_ICV},
  author       = {Haijun Xiong and Bin Feng and Bang Wang and Xinggang Wang and Wenyu Liu},
  doi          = {10.1016/j.imavis.2025.105597},
  journal      = {Image and Vision Computing},
  month        = {9},
  pages        = {105597},
  shortjournal = {Image Vis. Comput.},
  title        = {MambaGait: Gait recognition approach combining explicit representation and implicit state space model},
  volume       = {161},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Boundary-and-object collaborative learning network for camouflaged object detection. <em>ICV</em>, <em>161</em>, 105596. (<a href='https://doi.org/10.1016/j.imavis.2025.105596'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Existing camouflaged object detection (COD) approaches have achieved remarkable success in detecting and segmenting camouflaged objects that visually blend into the surroundings. However, there are still some challenging and critical issues, including inaccurate localization of target objects with varying scales, and incomplete identification of subtle details. To address these problems, we propose a novel boundary-and-object collaborative learning network (BCLNet) for camouflaged object detection, which simultaneously extracts and progressively refines the position and detail information to ensure segmentation results with uniform interiors and clear boundaries. Specifically, we design the Adaptive Feature Learning (AFL) module to generate the boundary information for identifying the details and the object information for positioning the target objects, and then optimize the two types of features in an interactive learning manner. In this way, the boundary feature and the object feature are able to learn from each other and compensate deficiencies for themselves, thus improving the semantic and detail representation. Moreover, to fully explore the complementarity between the cross-level features, we propose the Boundary-guided Selective Fusion (BSF) module to introduce the boundary cue to help the cross-level feature integration, enriching the semantic information while preserving the detail information. Extensive experimental results demonstrate that our BCLNet outperforms the state-of-the-art COD methods on four widely used datasets. The link to our code and prediction maps are available at https://github.com/ZhangQing0329/BCLNet .},
  archive      = {J_ICV},
  author       = {Chenyu Zhuang and Qing Zhang and Chenxi Zhang and Xinxin Yuan},
  doi          = {10.1016/j.imavis.2025.105596},
  journal      = {Image and Vision Computing},
  month        = {9},
  pages        = {105596},
  shortjournal = {Image Vis. Comput.},
  title        = {Boundary-and-object collaborative learning network for camouflaged object detection},
  volume       = {161},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). ADVC: Adversarial dense video captioning with unsupervised pretraining. <em>ICV</em>, <em>161</em>, 105595. (<a href='https://doi.org/10.1016/j.imavis.2025.105595'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Dense video captioning involves detecting and describing events that represent a video story in untrimmed videos using sentences. This task holds great promise for various video analytics-related applications. However, the nondeterministic nature of dense video captioning poses challenges in generating realistic events and captions. Recently, with the advent of large-scale video datasets, pretraining approaches have emerged. Nevertheless, these methods still require strict supervision and often lack accurate localization or are tightly coupled with localization and captioning. To address these challenges, this paper introduces ADVC, a novel approach for dense video captioning that combines unsupervised pre-training and adversarial adaptation. ADVC learns from readily available unlabeled videos and text corpora at scale, thereby reducing the need for strict supervision. It achieves realistic outcomes by directly learning the distribution of human-annotated events and captions through adversarial adaptation. Adversarial adaptation allows for the decoupling of localization and captioning subtasks while effectively considering their interdependence. We evaluate the performance of ADVC using multiple benchmark datasets to showcase the efficacy of our unsupervised pre-training and adversarial adaptation approach.},
  archive      = {J_ICV},
  author       = {Wangyu Choi and Jiasi Chen and Jongwon Yoon},
  doi          = {10.1016/j.imavis.2025.105595},
  journal      = {Image and Vision Computing},
  month        = {9},
  pages        = {105595},
  shortjournal = {Image Vis. Comput.},
  title        = {ADVC: Adversarial dense video captioning with unsupervised pretraining},
  volume       = {161},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Consensus exploration and detail perception for co-salient object detection in optical remote sensing images. <em>ICV</em>, <em>161</em>, 105586. (<a href='https://doi.org/10.1016/j.imavis.2025.105586'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Co-salient object detection (CoSOD) in optical remote sensing images (ORSI) aims to identify common salient objects across a set of related images. To address this, we introduce the first large-scale dataset, CoORSI, comprising 7668 high-quality images annotated with target masks, covering various macroscopic geographic scenes and man-made targets. Furthermore, we propose a novel network, Consensus Exploration and Detail Perception Network (CEDPNet), specifically designed for CoSOD in ORSI. CEDPNet incorporates a Collaborative Object Search Module (COSM) to integrate high-level features and explore collaborative objects, and a Feature Sensing Module (FSM) to enhance salient target perception through difference contrast enhancement and multi-scale detail boosting. By continuously fusing high-level semantic information with low-level detailed features, CEDPNet achieves accurate co-salient object detection. Extensive experiments demonstrate that CEDPNet significantly outperforms state-of-the-art methods on six evaluation metrics, underscoring its effectiveness for CoSOD in ORSI. The CoORSI dataset, model, and results will be publicly available at https://github.com/chen000701/CEDPNet .},
  archive      = {J_ICV},
  author       = {Yanliang Ge and Jiaxue Chen and Taichuan Liang and Yuxi Zhong and Hongbo Bi and Qiao Zhang},
  doi          = {10.1016/j.imavis.2025.105586},
  journal      = {Image and Vision Computing},
  month        = {9},
  pages        = {105586},
  shortjournal = {Image Vis. Comput.},
  title        = {Consensus exploration and detail perception for co-salient object detection in optical remote sensing images},
  volume       = {161},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Mmi-unet: Colorectal cancer CT image segmentation based on multi-modal information interaction. <em>ICV</em>, <em>161</em>, 105583. (<a href='https://doi.org/10.1016/j.imavis.2025.105583'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Colorectal cancer (CRC) segmentation from computed tomography (CT) images remains challenging, primarily due to low contrast and the irregular morphology of tumorous lesions. Existing multi-modal methods are often constrained by simplistic feature concatenation strategies, which limit the exploitation of collaborative information across modalities. Such limitations become increasingly pronounced when dealing with complex anatomical structures and highly heterogeneous lesions. To address these challenges, we propose a novel multi-modal segmentation model, referred to as multimodal interaction Unet (Mmi-Unet). Our approach employs separate ResNet encoders to extract modality-specific features, thereby preserving their independence, and leverages cross-attention mechanisms along with information entropy to capture inter-modality synergy. In addition, we introduce a dynamic fusion coefficient training module, enabling flexible adjustment of modality fusion ratios to achieve enhanced information integration. Built on a U-Net framework, Mmi-Unet further incorporates multi-scale feature fusion and collaborative optimization. Experimental results on plain and enhanced CRC imaging tasks indicate that our model surpasses existing approaches, achieving Dice coefficients and intersection-over-union (IoU) scores of up to 0.9557, 0.9559, 0.9326, and 0.9435, respectively. These findings demonstrate the superior accuracy and robustness of the proposed model for CRC segmentation.},
  archive      = {J_ICV},
  author       = {Zihao Zhao and Dinghui Wu and Qibing Zhu and Hao Wang and Yuxi Ge and Shudong Hu},
  doi          = {10.1016/j.imavis.2025.105583},
  journal      = {Image and Vision Computing},
  month        = {9},
  pages        = {105583},
  shortjournal = {Image Vis. Comput.},
  title        = {Mmi-unet: Colorectal cancer CT image segmentation based on multi-modal information interaction},
  volume       = {161},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Efficient mamba: Overcoming the visual limitations of mamba with innovative structures. <em>ICV</em>, <em>161</em>, 105569. (<a href='https://doi.org/10.1016/j.imavis.2025.105569'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Mamba models have emerged as strong competitors to Transformers due to their efficient long-sequence processing and high memory efficiency. However, their state space models (SSMs) suffer from limitations in capturing long-range dependencies, lack of channel interactions, and weak generalization in vision tasks. To address these issues, we propose Efficient Mamba (EMB), an innovative framework that enhances SSMs while integrating convolutional neural networks (CNNs) and Transformers to mitigate their inherent drawbacks. The key contributions of EMB are as follows: (1) We introduce the TransSSM module, which incorporates feature flipping and channel shuffle to enhance channel interactions and improve generalization. Additionally, we propose the Window Spatial Attention (WSA) module for precise local feature modeling and Dual Pooling Attention (DPA) to improve global feature modeling and model stability. (2) We design the MFB-SCFB composite structure, which integrates TransSSM, WSA, Inverted Residual Block(IRBs), and convolutional attention modules to facilitate effective global–local feature interaction. EMB achieves state-of-the-art (SOTA) performance across multiple vision tasks. For instance, on ImageNet classification, EMB-S/T/N achieves Top-1 accuracies of 78.9%, 76.3%, and 73.5%, with model sizes and FLOPs of 5.9M/1.5G, 2.5M/0.6G, and 1.4M/0.3G, respectively, when trained on a single NVIDIA 4090 GPU. Experimental results demonstrate that EMB provides a novel paradigm for efficient vision model design, offering valuable insights for future SSM research. Code: https://github.com/Xuwei86/EMB/tree/main .},
  archive      = {J_ICV},
  author       = {Wei Xu and Yi Wan and Dong Zhao and Long Zhang},
  doi          = {10.1016/j.imavis.2025.105569},
  journal      = {Image and Vision Computing},
  month        = {9},
  pages        = {105569},
  shortjournal = {Image Vis. Comput.},
  title        = {Efficient mamba: Overcoming the visual limitations of mamba with innovative structures},
  volume       = {161},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). BFNet: Boundary guidance signal and feature fusion network for camouflaged object detection. <em>ICV</em>, <em>160</em>, 105599. (<a href='https://doi.org/10.1016/j.imavis.2025.105599'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The purpose of Camouflaged Object Detection (COD) is to identify objects that are visually indistinguishable from their backgrounds due to high similarities in color, texture, and luminance. This task presents greater challenges compared to conventional object detection because of the intricate blending between objects and their surroundings. In this paper, a boundary guidance signal and feature fusion network (BFNet) for camouflaged object detection is proposed. The proposed method mainly consists of three key components: boundary guidance signal module (BGSM), attention-induced feature fusion module (AFFM) and gradual camouflage recognition module (GCRM). BGSM captures edge information to generate edge guidance signals. AFFM fuses cross-level features of shallow and deeper layers to obtain rich details and semantic information. Lastly, the GCRM refines the detection of camouflaged objects step by step to obtain the final prediction map. To verify the effectiveness of the proposed method, relevant experiments were conducted on the four challenging benchmark datasets. The experimental results show that BFNet significantly outperforms the other 16 existing methods under five widely used evaluation metrics.},
  archive      = {J_ICV},
  author       = {Xinglin Fu and Weixin Bian and Biao Jie and Haotong Dong and Zhiwei He},
  doi          = {10.1016/j.imavis.2025.105599},
  journal      = {Image and Vision Computing},
  month        = {7},
  pages        = {105599},
  shortjournal = {Image Vis. Comput.},
  title        = {BFNet: Boundary guidance signal and feature fusion network for camouflaged object detection},
  volume       = {160},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Learning disturbance-aware correlation filter with adaptive kaiser window for visual object tracking. <em>ICV</em>, <em>160</em>, 105585. (<a href='https://doi.org/10.1016/j.imavis.2025.105585'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Discriminative Correlation Filters (DCF) have been recognized as a classic and effective method in the field of object tracking. In order to mitigate boundary effects, prior DCF-based tracking methods have commonly employed a fixed Hanning window, limiting the adaptability to fluctuations of the response map. Therefore, we propose a disturbance-aware correlation filter with adaptive Kaiser window (DCFAK) for visual object tracking. The adaptive Kaiser window dynamically adjusts its values according to the kurtosis of the response map, effectively suppressing boundary effects. Additionally, to further improve robustness, our DCFAK introduces a disturbance peaks suppression method, which can better distinguish the target object from the objects with similar appearance in the background by attenuating the sub-peaks within the response map. We comprehensively evaluate the performance of our DCFAK on seven datasets, including OTB-2013, OTB, 2015, TC-128, DroneTB, 70, UAV123, UAVDT, and LaSOT. The results demonstrate the superior performance of our method across these datasets.},
  archive      = {J_ICV},
  author       = {Jianming Zhang and Jiangxin Dai and Wentao Chen and Ke Nai},
  doi          = {10.1016/j.imavis.2025.105585},
  journal      = {Image and Vision Computing},
  month        = {7},
  pages        = {105585},
  shortjournal = {Image Vis. Comput.},
  title        = {Learning disturbance-aware correlation filter with adaptive kaiser window for visual object tracking},
  volume       = {160},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Masked graph attention network for classification of facial micro-expression. <em>ICV</em>, <em>160</em>, 105584. (<a href='https://doi.org/10.1016/j.imavis.2025.105584'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Facial micro-expressions (MEs) are ultra-fine, quick, and short-motion muscle movements expressing a person’s true feelings. Automatic recognition of MEs with only a few samples is challenging and the extraction of subtle features becomes crucial. This paper addresses these intricacies and presents a novel dual-branch (branch1 for node locations and branch2 for optical flow patch information) masked graph attention network-based approach (MaskGAT) to classify MEs in a video. It utilizes a three-frame graph structure to extract spatio-temporal information. It learns a mask for each node to eliminate the less important node features and propagates the important node features to the neighboring nodes. A masked self-attention graph pooling layer is designed to provide the attention score to eliminate the unwanted nodes and uses only the nodes with a high attention score. An adaptive frame selection mechanism is designed that is based on a sliding window optical flow method to discard the low-intensity emotion frames. A well-designed dual-branch fusion mechanism is developed to extract informative features for the final classification of MEs. Furthermore, the paper presents a detailed mathematical analysis and visualization of the MaskGAT pipeline to demonstrate the effectiveness of node feature masking and pooling. The results are presented and compared with the state-of-the-art methods for SMIC, SAMM, CASME II, and MMEW databases. Further, cross-dataset experiments are carried out, and the results are reported.},
  archive      = {J_ICV},
  author       = {Ankith Jain Rakesh Kumar and Bir Bhanu},
  doi          = {10.1016/j.imavis.2025.105584},
  journal      = {Image and Vision Computing},
  month        = {7},
  pages        = {105584},
  shortjournal = {Image Vis. Comput.},
  title        = {Masked graph attention network for classification of facial micro-expression},
  volume       = {160},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Generalizable deepfake detection via spatial kernel selection and halo attention network. <em>ICV</em>, <em>160</em>, 105582. (<a href='https://doi.org/10.1016/j.imavis.2025.105582'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The rapid advancement of AI-Generated Content (AIGC) has enabled the unprecedented synthesis of photorealistic facial images. While these technologies offer transformative potential for creative industries, they also introduce significant risks due to the malicious manipulation of visual media. Current deepfake detection methods struggle with unseen forgeries due to their inability to consider the effects of spatial receptive fields and local representation learning. To bridge these gaps, this paper proposes a Spatial Kernel Selection and Halo Attention Network (SKSHA-Net) for deepfake detection. The proposed model incorporates two key modules, namely Spatial Kernel Selection (SKS) and Halo Attention (HA). The SKS module dynamically adjusts the spatial receptive field to capture subtle artifacts indicative of forgery. The HA module focuses on the intricate relationships between neighboring pixels for local representation learning. Comparative experiments on three public datasets demonstrate that SKSHA-Net outperforms the state-of-the-art (SOTA) methods in both intra-testing and cross-testing.},
  archive      = {J_ICV},
  author       = {Siyou Guo and Qilei Li and Mingliang Gao and Xianxun Zhu and Imad Rida},
  doi          = {10.1016/j.imavis.2025.105582},
  journal      = {Image and Vision Computing},
  month        = {7},
  pages        = {105582},
  shortjournal = {Image Vis. Comput.},
  title        = {Generalizable deepfake detection via spatial kernel selection and halo attention network},
  volume       = {160},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Automatic three-dimensional reconstruction of transparent objects with multiple optimization strategies under limited constraints. <em>ICV</em>, <em>160</em>, 105580. (<a href='https://doi.org/10.1016/j.imavis.2025.105580'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Reconstructing transparent objects with limited constraints has long been considered a highly challenging problem. Due to the complex interaction between transparent objects and light, which involves intricate refraction and reflection relationships, traditional three-dimensional (3D) reconstruction methods are less than effective for transparent objects. To address this issue, this study proposes a 3D reconstruction method specifically designed for transparent objects. Incorporating multiple optimization strategies, the method works under limited constraints to achieve the automatic reconstruction of transparent objects with only a few transparent object images in any known environment, without the need for specific data collection devices or environments. The proposed method makes use of automatic image segmentation and modifies the network interface and structure of the PointNeXt algorithm to introduce the TransNeXt network, which enhances normal features, optimizes weight attenuation, and employs a preheating cosine annealing learning rate. We use several steps to reconstruct the complete 3D shape of transparent objects. First, we initialize the transparent shape with a visual hull reconstructed with the contours obtained by the TOM-Net. Then, we construct the normal reconstruction network to estimate the normal values. Finally, we reconstruct the complete 3D shape using the TransNeXt network. Multiple experiments show that the TransNeXt network exhibits superior reconstruction performance to other networks and can effectively perform the automatic reconstruction of transparent objects even under limited constraints.},
  archive      = {J_ICV},
  author       = {Xiaopeng Sha and Xiaopeng Si and Yujie Zhu and Shuyu Wang and Yuliang Zhao},
  doi          = {10.1016/j.imavis.2025.105580},
  journal      = {Image and Vision Computing},
  month        = {7},
  pages        = {105580},
  shortjournal = {Image Vis. Comput.},
  title        = {Automatic three-dimensional reconstruction of transparent objects with multiple optimization strategies under limited constraints},
  volume       = {160},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). FaiResGAN: Fair and robust blind face restoration with biometrics preservation. <em>ICV</em>, <em>160</em>, 105575. (<a href='https://doi.org/10.1016/j.imavis.2025.105575'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Modern computer vision technologies enable systems to detect, recognize, and analyze facial features, but challenges arise when images are noisy, blurred, or low quality. Blind face restoration, which aims to recover high-quality facial images without prior knowledge of degradation, addresses this issue. In this paper, we introduce Fair Restoration GAN (FaiResGAN), a novel Generative Adversarial Network (GAN) designed to balance face restoration with the preservation of soft biometrics (identity, ethnicity, age, and gender). Our model incorporates a pseudo-random batch composition algorithm to promote fairness and mitigate bias, alongside a realistic degradation model simulating corruptions typical in surveillance images. Experimental results show that FaiResGAN outperforms state-of-the-art blind face restoration methods, both quantitatively and qualitatively. A user study involving 40 participants showed that FaiResGAN-restored images were preferred by 70% of users. Additionally, tests on VGGFace2, UTKFace, and FairFace datasets demonstrate FaiResGAN’s superior performance in preserving soft biometric attributes and ensuring fair restoration across different genders and ethnicities.},
  archive      = {J_ICV},
  author       = {George Azzopardi and Antonio Greco and Mario Vento},
  doi          = {10.1016/j.imavis.2025.105575},
  journal      = {Image and Vision Computing},
  month        = {7},
  pages        = {105575},
  shortjournal = {Image Vis. Comput.},
  title        = {FaiResGAN: Fair and robust blind face restoration with biometrics preservation},
  volume       = {160},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Deep learning-assisted 3D model for the detection and classification of knee arthritis. <em>ICV</em>, <em>160</em>, 105574. (<a href='https://doi.org/10.1016/j.imavis.2025.105574'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Osteoarthritis (OA) affects nearly 240 million people worldwide. It is a common degenerative illness that typically affects the knee joint OA causes pain, and functional disability, especially in older adults is a common disease. One of the most common and challenging medical conditions to deal with in old-aged people is the occurrence of knee osteoarthritis (KOA). Manual diagnosis involves observing X-ray images of the knee area and classifying it into different five grades. This requires the physician's expertise, suitable experience, and a lot of time, and even after that, the diagnosis can be prone to errors. Therefore, researchers in the machine learning (ML) and deep learning (DL) domains have employed the capabilities of deep neural network (DNN) models to identify and classify medical images in an automated, faster, and more accurate manner. Combining multiple imaging modalities or utilizing three-dimensional reconstructions can enhance the accuracy and completeness of 2D Images in diagnostic information. Hence to overcome the drawbacks of 2D imaging, the reconstruction of 3D models using 2D images is the main theme of our research. In this paper, we propose a deep learning-based model for the detection and classification of the early diagnosis of arthritis. It is a four-step procedure starting with data collection followed by data conversion. In this step, our proposed model deforms the target's convex hull to produce a 3D model. Herein, a series of 2D photos is utilized, along with surface rendering methods, to create a 3D model. In the third step, the feature extraction is performed followed by mesh refinement. The chamfer loss is optimized based on the rotational shape of the leg bones, and subsequently, the weight of the loss function can be allocated to the target's geometric properties. We have used a modified Gray Level Co-occurrence Matrix (GLCM) for feature extraction. In the fourth step, the image classification is performed and the suggested optimization strategy raises the model's accuracy. A comparison of results with current 3D reconstruction techniques proves that the suggested method can consistently produce a waterproof model with a greater reconstruction accuracy. The deep-seated intricacies and distinct patterns across arthritic phases are estimated through the extraction of complicated statistical variables combined with power spectral density. The high-dimensional data is divided into separate, easily observable groups using the Lion Optimization Algorithm and proposed distance metric. The F1 Score and Jaccard Metric showed an average of 0.85 and 0.23, indicating effective differentiation across clusters.},
  archive      = {J_ICV},
  author       = {D. Preethi and V. Govindaraj and S. Dhanasekar and K. Martin Sagayam and Syed Immamul Ansarullah and Farhan Amin and Isabel de la Torre D'ıez and Carlos Osorio Garc'ıa and Alina Eugenia Pascual Barrera and Fehaid Salem Alshammari},
  doi          = {10.1016/j.imavis.2025.105574},
  journal      = {Image and Vision Computing},
  month        = {7},
  pages        = {105574},
  shortjournal = {Image Vis. Comput.},
  title        = {Deep learning-assisted 3D model for the detection and classification of knee arthritis},
  volume       = {160},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Semantic scene graph generation based on an edge dual scene graph and message passing neural network. <em>ICV</em>, <em>160</em>, 105572. (<a href='https://doi.org/10.1016/j.imavis.2025.105572'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Along with generative AI, interest in scene graph generation (SGG), which comprehensively captures the relationships and interactions between objects in an image and creates a structured graph-based representation, has significantly increased in recent years. However, relying on object-centric and dichotomous relationships, existing SGG methods have a limited ability to accurately predict detailed relationships. To solve these problems, a new approach to the modeling multi-object relationships, called edge dual scene graph generation (EdgeSGG), is proposed herein. EdgeSGG is based on an edge dual scene graph and object-relation centric message passing neural network (OR-MPNN), which can capture rich contextual interactions between unconstrained objects. To facilitate the learning of edge dual scene graphs with a symmetric graph structure, the proposed OR-MPNN learns both object- and relation-centric features for more accurately predicting relation-aware contexts and allows fine-grained relational updates between objects. A comparative experiment with state-of-the-art (SoTA) methods was conducted using two public datasets for SGG operations and six metrics for three subtasks. Compared with SoTA approaches, the proposed model exhibited substantial performance improvements across all SGG subtasks. Furthermore, experiment on imbalanced class distributions revealed that incorporating the relationships between objects effectively mitigates existing long-tail problems. Our code is available at https://github.com/Chocolate-Love/EdgeSGG-pytorch .},
  archive      = {J_ICV},
  author       = {Hyeongjin Kim and Byoung Chul Ko},
  doi          = {10.1016/j.imavis.2025.105572},
  journal      = {Image and Vision Computing},
  month        = {7},
  pages        = {105572},
  shortjournal = {Image Vis. Comput.},
  title        = {Semantic scene graph generation based on an edge dual scene graph and message passing neural network},
  volume       = {160},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). SAGNet: Synergistic attention-graph network for video salient object detection. <em>ICV</em>, <em>160</em>, 105570. (<a href='https://doi.org/10.1016/j.imavis.2025.105570'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the field of video salient object detection (VSOD), accurately capturing motion information is essential. Previous approaches primarily rely on optical flow, convolutional long short term memory (ConvLSTM), or 3D convolutional neural network (CNN) to extract and utilize motion information. However, these methods capture limited motion details and increase the parameters in the network. Moreover, Transformer-based methods, while effective in high-level feature modeling, suffer from excessive computational complexity and insufficient local feature extraction, limiting their practical application in VSOD. To address these challenges, we propose a novel synergistic attention-graph network (SAGNet) that independently distills spatial–temporal cues and spatial edge features using the synergistic attention-graph module (SAGM) and the spatial edge attention module (SEM), respectively. SAGM innovatively integrates inter-frame attention with spatial–temporal graph convolution network (GCN). The inter-frame attention proposed in SAGM captures motion information between video frames while expanding the receptive field to capture long-range dependencies. Spatial–temporal GCN models video as a graph, bridge features from temporal into spatial branch, which is capable of fusing cross-modal features collaboratively. This synergy enables SAGNet to consider both global and local spatial–temporal features. SEM enhances high-level information by extracting spatial and edge features from the low-level data using the Sobel operator and spatial attention module. Experimental results on several publicly available VSOD benchmark datasets demonstrate that SAGNet outperforms existing methods in terms of detection accuracy and efficiency, confirming its superiority and practicality in VSOD.},
  archive      = {J_ICV},
  author       = {Huo Lina and Xueyuan Gao and Wei Wang and Ke Chen and Ke Wang},
  doi          = {10.1016/j.imavis.2025.105570},
  journal      = {Image and Vision Computing},
  month        = {7},
  pages        = {105570},
  shortjournal = {Image Vis. Comput.},
  title        = {SAGNet: Synergistic attention-graph network for video salient object detection},
  volume       = {160},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Calibrated gradient descent of convolutional neural networks for embodied visual recognition. <em>ICV</em>, <em>160</em>, 105568. (<a href='https://doi.org/10.1016/j.imavis.2025.105568'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Embodied visual computing seeks to learn from the real world, which requires an efficient machine learning methods. In conventional stochastic gradient descent (SGD) and its variants, the gradient estimators are expensive to be computed in many scenarios. This paper introduces a calibrated gradient descent (CGD) algorithm for efficient deep neural network optimization. A theorem is developed to prove that an unbiased estimator for network parameters can be obtained in a probabilistic way based on the Lipschitz hypothesis. We implement our CGD algorithm based on widely-used SGD and ADAM optimizers. We achieve a generic gradient calibration layer ( G C l a y e r ) which can be used to improve the performance of convolutional neural networks (C-CNNs). Our G C l a y e r only introduces extra parameters during training process, but not affect the efficiency of the inference process. Our method is generic and effective to optimize both CNNs and also quantized neural networks (C-QNNs). Extensive experimental results demonstrate that our method can achieve the state-of-the-art performance for a variety of tasks. For example, our 1-bit Faster-RCNN achieved by C-QNN obtains 20.5% mAP on COCO, leading to a new state-of-the-art performance. This work brings new insights for developing more efficient optimizers and analyzing the back-propagation algorithm.},
  archive      = {J_ICV},
  author       = {Zhiming Wang and Sheng Xu and Li’an Zhuo and Baochang Zhang and Yanjing Li and Zhenqian Wang and Guodong Guo},
  doi          = {10.1016/j.imavis.2025.105568},
  journal      = {Image and Vision Computing},
  month        = {7},
  pages        = {105568},
  shortjournal = {Image Vis. Comput.},
  title        = {Calibrated gradient descent of convolutional neural networks for embodied visual recognition},
  volume       = {160},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A hybrid approach combining images and questionnaires for early detection and severity assessment of autism spectrum disorder. <em>ICV</em>, <em>160</em>, 105547. (<a href='https://doi.org/10.1016/j.imavis.2025.105547'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this research, we propose a novel integrated system for the early diagnosis and cognitive enhancement of infants with Autism Spectrum Disorder (ASD). The system combines two core modules: the Behavioral Analytic Module and the Cognitive Skill Enhancement Module. The Behavioral Analytic Module includes a Questionnaire Analysis Sub-module, which utilizes Random Forest classifiers to analyze questionnaire data, and an Image Analysis Sub-module, which employs a fine-tuned VGG16 Convolutional Neural Network to process facial images. These sub-modules independently assess ASD likelihood and combine their outputs to generate a comprehensive diagnosis using a weighted averaging technique. The Cognitive Skill Enhancement Module integrates interactive games and web-based animations designed to improve cognitive abilities and daily living skills in toddlers with ASD. Additionally, a reward system is incorporated to reinforcement learning outcomes, adaptively calculating rewards based on the infants’ progress. The proposed system aims to provide a holistic approach to ASD diagnosis and intervention, offering an effective tool for early detection and tailored cognitive development. The system’s efficacy is demonstrated through comparative analysis, showing a 93% improvement in diagnostic accuracy and a 92% enhancement in cognitive skill development among toddlers with ASD.},
  archive      = {J_ICV},
  author       = {Rajkumar S.C. and Stefano Cirillo and Yuvasini D. and Luisa Solimando},
  doi          = {10.1016/j.imavis.2025.105547},
  journal      = {Image and Vision Computing},
  month        = {7},
  pages        = {105547},
  shortjournal = {Image Vis. Comput.},
  title        = {A hybrid approach combining images and questionnaires for early detection and severity assessment of autism spectrum disorder},
  volume       = {160},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). M3IF-NSST-MTV: Modified total variation-based multi-modal medical image fusion using laplacian energy and morphology in the NSST domain. <em>ICV</em>, <em>159</em>, 105581. (<a href='https://doi.org/10.1016/j.imavis.2025.105581'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper presents a new multi-modal medical image fusion (M3IF) technique that fuses the medical images obtained from different medical imaging modalities, such as Computed Tomography (CT), Magnetic Resonance Imaging (MRI), Single Photon Emission Computed Tomography (SPECT) or Positron Emission Tomography (PET), into a single image. This single image is enhanced and contains all the important information of the source images. This paper proposes a hybrid M3IF technique, i.e., M3IF-NSST-MTV, where input medical images are decomposed using Non-Subsampled Shearlet Transform (NSST). It decomposes the image into low frequency coefficients (LFCs), and high frequency coefficients (HFCs). The LFCs are fused using Laplacian energy, and HFCs are fused using morphology. The fused image obtained after applying inverse-NSST is directed to the modified Total Variation (TV), that refines the NSST output. This modified TV output is again fused with NSST output using Feature Similarity Index Measure (FSIM) with Correlation Coefficient (CC)-based threshold value. This modified TV refinement process is iterative process. The results of M3IF-NSST-MTV are evaluated at the pre-set number of iterations = 200. The final fusion results of M3IF-NSST-MTV are compared with some of the prevalent non-traditional methods and based on visual quality and quantitative metric-based analysis; it is found that the M3IF-NSST-MTV delivers better results than all the compared methods.},
  archive      = {J_ICV},
  author       = {Dev Kumar Chaudhary and Prabhishek Singh and Achyut Shankar and Manoj Diwakar},
  doi          = {10.1016/j.imavis.2025.105581},
  journal      = {Image and Vision Computing},
  month        = {6},
  pages        = {105581},
  shortjournal = {Image Vis. Comput.},
  title        = {M3IF-NSST-MTV: Modified total variation-based multi-modal medical image fusion using laplacian energy and morphology in the NSST domain},
  volume       = {159},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A neighbor-aware feature enhancement network for crowd counting. <em>ICV</em>, <em>159</em>, 105578. (<a href='https://doi.org/10.1016/j.imavis.2025.105578'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep neural networks have achieved significant progress in the field of crowd counting in recent years. However, many networks still face challenges in effectively representing crowd features due to the insufficient exploitation of inter-channel and inter-pixel relationships. To overcome these limitations, we propose the Neighbor-Aware Feature Enhancement Network (NAFENet), a novel architecture designed to strengthen feature representation by adequately leveraging both channel and pixel dependencies. Specifically, we introduce two modules to model channel dependencies: the Across Channel Attention Module (ACAM) and the Channel Residual Module (CRM). ACAM computes a relevance map to quantify the influence of adjacent channels on the current channel and extracts valuable information to enrich the feature representation. On the other hand, CRM learns the residual maps between adjacent channels to capture their correlations and differences, enabling the network to gain a deeper understanding of the image content. In addition, we embed a Spatial Correlation Module (SCM) in NAFENet to model long-range dependencies between pixels across neighboring rows to analyze long continuous structures more effectively. Experimental results on six challenging datasets demonstrate that the proposed method achieves impressive performance compared to state-of-the-art models. Complexity analysis further reveals that our model is more efficient, requiring less time and fewer computational resources than other approaches.},
  archive      = {J_ICV},
  author       = {Lin Wang and Jie Li and Chun Qi and Xuan Wu and Runrun Zou and Fengping Wang and Pan Wang},
  doi          = {10.1016/j.imavis.2025.105578},
  journal      = {Image and Vision Computing},
  month        = {6},
  pages        = {105578},
  shortjournal = {Image Vis. Comput.},
  title        = {A neighbor-aware feature enhancement network for crowd counting},
  volume       = {159},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). SRMA-KD: Structured relational multi-scale attention knowledge distillation for effective lightweight cardiac image segmentation. <em>ICV</em>, <em>159</em>, 105577. (<a href='https://doi.org/10.1016/j.imavis.2025.105577'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cardiac image segmentation is essential for accurately extracting structural information of the heart, aiding in precise diagnosis and personalized treatment planning. However, real-time segmentation on medical devices demands computational efficiency that often conflicts with the intensive processing and storage requirements of deep learning algorithms. These algorithms are frequently hindered by their complex models and extensive parameter sets, which limit their feasibility in clinical settings with constrained resources. Meanwhile, the performance of lightweight heart segmentation models still requires enhancement. This study introduces the SRMA-KD framework, a knowledge distillation approach for cardiac image segmentation designed to achieve high accuracy with lightweight models. The framework efficiently transfers semantic feature information and structural knowledge from a teacher model to student model, ensuring effective segmentation within clinical resource limitations. The SRMA-KD framework includes three key modules: the Global Structural Relational Block (GSRB), the Multi-scale Feature Attention Block (MFAB), and the Prediction Difference Transfer Block (PDTB). The GSRB correlates the outputs of the teacher and student networks with the ground truth, transferring structural correlations to enhance the student network's global feature learning. The MFAB enables the student network to learn multi-scale feature extraction from the teacher network, focusing on relevant semantic regions. The PDTB minimizes pixel-level differences between the segmentation images of the teacher and student networks. Our experiments demonstrate that the SRMA-KD framework significantly improves the segmentation accuracy of the student network compared to other medical imaging knowledge distillation methods, highlighting its potential as an effective solution for cardiac image segmentation in resource-limited clinical environments.},
  archive      = {J_ICV},
  author       = {Bo Chen and Youhao Huang and Yufan Liu and Dong Sui and Fei Yang},
  doi          = {10.1016/j.imavis.2025.105577},
  journal      = {Image and Vision Computing},
  month        = {6},
  pages        = {105577},
  shortjournal = {Image Vis. Comput.},
  title        = {SRMA-KD: Structured relational multi-scale attention knowledge distillation for effective lightweight cardiac image segmentation},
  volume       = {159},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). LCNet: Lightweight real-time image classification network based on efficient multipath dynamic attention mechanism and dynamic threshold convolution. <em>ICV</em>, <em>159</em>, 105576. (<a href='https://doi.org/10.1016/j.imavis.2025.105576'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Hybrid architectures that integrate convolutional neural networks (CNNs) with Transformers can comprehensively extract both local and global image features, exhibiting impressive performance in image classification. However, their large parameter sizes and high computational demands hinder deployment on low-resource devices. To address this limitation, we propose a dual-branch classification network based on a pyramid architecture, termed LCNet. First, we introduce a dynamic threshold convolution module that adaptively adjusts convolutional parameters based on the input, thereby improving the efficiency of feature extraction. Second, we design a multi-path dynamic attention mechanism that optimizes attention weights to capture salient information and enhance the significance of key features. Third, a star-shaped connection is adopted to enable efficient information fusion between the two branches in a high-dimensional implicit feature space. LCNet is evaluated on four public datasets and one wood dataset (Tiny-ImageNet, Mini-ImageNet, CIFAR100, CIFAR10, and Micro-CT) using recognition accuracy and inference efficiency as metrics. The results show that LCNet achieves a maximum accuracy of 99.50% with an inference time of only 0.0072 s per image, outperforming other state-of-the-art (SOTA) models. Extensive experiments demonstrate that LCNet is more competitive than existing neural networks and can be effectively deployed on low-performance computing devices. This broadens the applicability of image classification techniques, aligns with the trend of edge computing, reduces reliance on cloud servers, and enhances both real-time processing and data privacy.},
  archive      = {J_ICV},
  author       = {Xiaoxia Yang and Zhishuai Zheng and Huanqi Zheng and Zhedong Ge and Xiaotong Liu and Bei Zhang and Jinyang Lv},
  doi          = {10.1016/j.imavis.2025.105576},
  journal      = {Image and Vision Computing},
  month        = {6},
  pages        = {105576},
  shortjournal = {Image Vis. Comput.},
  title        = {LCNet: Lightweight real-time image classification network based on efficient multipath dynamic attention mechanism and dynamic threshold convolution},
  volume       = {159},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). FastTalker: Real-time audio-driven talking face generation with 3D gaussian. <em>ICV</em>, <em>159</em>, 105573. (<a href='https://doi.org/10.1016/j.imavis.2025.105573'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The performance of 3D talking head generation has shown significant im- provement over the past few years. Nevertheless, real-time rendering remains a challenge that needs to be overcome. To address this issue, we present the FastTalker framework, which uses 3D Gaussian Splatting (3DGS) for talking head generation. This method introduces an audio-driven Dynamic Neural Skinning (DNS) approach to facilitate flexible and high-fidelity talking head video generation. It first employs an adaptive FLAME mesh for sampling to obtain the initialized 3DGS. Then, Neural Skinning Networks (DNS) are used to account for the appearance changes of 3DGS. Finally, a pre-trained Audio Motion Net is utilized to model facial movements as the final dynamic driving facial signal. Experimental results demonstrate that FastTalker of- fers a rendering speed exceeding 100 FPS, making it the fastest audio-driven talking head generation method in terms of inference efficiency.},
  archive      = {J_ICV},
  author       = {Keliang Chen and Zongze Li and Fang Cui and Mao Ni and Shaoying Wang and Junlin Che and Feng Liu and Yonggang Qi and Fangwei Zhang and Jun Liu and Gan Guo and Rongrong Fu and Yunxia Huang},
  doi          = {10.1016/j.imavis.2025.105573},
  journal      = {Image and Vision Computing},
  month        = {6},
  pages        = {105573},
  shortjournal = {Image Vis. Comput.},
  title        = {FastTalker: Real-time audio-driven talking face generation with 3D gaussian},
  volume       = {159},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Weakly supervised camouflaged object detection based on the SAM model and mask guidance. <em>ICV</em>, <em>159</em>, 105571. (<a href='https://doi.org/10.1016/j.imavis.2025.105571'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Camouflaged object detection (COD) from a single image is a challenging task due to the high similarity between objects and their surroundings. Existing fully supervised methods require labor-intensive pixel-level annotations, making weakly supervised methods a viable compromise that balances accuracy and annotation efficiency. However, weakly supervised methods often experience performance degradation due to the use of coarse annotations. In this paper, we introduce a new weakly supervised approach for camouflaged object detection to overcome these limitations. Specifically, we propose a novel network, MGNet, which tackles edge ambiguity and missed detections by utilizing initial masks generated by our custom-designed Cascaded Mask Decoder (CMD) to guide the segmentation process and enhance edge predictions. We introduce a Context Enhancement Module (CEM) to reduce the missing detection, and a Mask-guided Feature Aggregation Module (MFAM) for effective feature aggregation. For the weak supervision challenge, we propose BoxSAM, which leverages the Segment Anything Model (SAM) with bounding-box prompts to generate pseudo-labels. By employing a redundant processing strategy, high quality pixel-level pseudo-labels are provided for training MGNet. Extensive experiments demonstrate that our method delivers competitive performance against current state-of-the-art methods.},
  archive      = {J_ICV},
  author       = {Xia Li and Xinran Liu and Lin Qi and Junyu Dong},
  doi          = {10.1016/j.imavis.2025.105571},
  journal      = {Image and Vision Computing},
  month        = {6},
  pages        = {105571},
  shortjournal = {Image Vis. Comput.},
  title        = {Weakly supervised camouflaged object detection based on the SAM model and mask guidance},
  volume       = {159},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). UncertainBEV: Uncertainty-aware BEV fusion for roadside 3D object detection. <em>ICV</em>, <em>159</em>, 105567. (<a href='https://doi.org/10.1016/j.imavis.2025.105567'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the rapid development of autonomous driving technology and intelligent transportation systems, multimodal fusion-based Bird’s-Eye-View (BEV) perception has become a key technique for environmental understanding. However, existing methods suffer from feature misalignment caused by calibration errors between different sensors, ultimately limiting the effectiveness of multimodal fusion. In this paper, we propose a robust roadside BEV perception framework, named UncertainBEV. To address feature misalignment caused by projection inaccuracies between LiDAR and camera sensors, we introduce a novel module called UncertainFuser, which models the uncertainty of both camera and LiDAR features to dynamically adjust fusion weights, thereby mitigating feature misalignment. Additionally, we optimize the sparse voxel pooling module and design a multi-head attention mechanism to enhance the quality of BEV features from both modalities. Built upon the CUDA-V2XFusion and BEVFusion frameworks, our proposed UncertainBEV achieves state-of-the-art performance on the DAIR-V2X-I dataset, with 3D mean Average Precision (mAP) improvements of 2.88%, 7.73%, and 3.68% for vehicles, pedestrians, and cyclists, respectively. Our code has been open-sourced at UncertainBEV .},
  archive      = {J_ICV},
  author       = {Jianqiang Xu and Chunying Song and Chao Shi and Huafeng Liu and Qiong Wang},
  doi          = {10.1016/j.imavis.2025.105567},
  journal      = {Image and Vision Computing},
  month        = {6},
  pages        = {105567},
  shortjournal = {Image Vis. Comput.},
  title        = {UncertainBEV: Uncertainty-aware BEV fusion for roadside 3D object detection},
  volume       = {159},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Self-distillation guided semantic knowledge feedback network for infrared–visible image fusion. <em>ICV</em>, <em>159</em>, 105566. (<a href='https://doi.org/10.1016/j.imavis.2025.105566'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Infrared–visible image fusion combines complementary information from both modalities to enhance visual quality and support downstream tasks. However, existing methods typically enhance semantic information by designing fusion functions for source images and combining them with downstream network, overlooking the optimization and guidance of the fused image itself. This neglect weakens the semantic knowledge within the fused image, limiting its alignment with task objectives and reducing accuracy in downstream tasks. To overcome these limitations, we propose the self-distillation guided Semantic Knowledge Feedback (SKFFusion) network, which extracts semantic knowledge from the fused image and feeds it back to iteratively optimize the fusion process, addressing the lack of semantic guidance. Specifically, we introduce shallow-to-deep feature fusion modules, including Shallow Texture Fusion (STF) and Deep Semantic Fusion (DSF) to integrate fine-grained details and high-level semantics. The STF uses channel and spatial attention mechanisms to aggregate detailed multi-modal information, while the DSF leverages a Mamba structure to capture long-range dependencies, enabling deeper cross-modal semantic fusion. Additionally, we design a CNN-Transformer-based Knowledge Feedback Network (KFN) to extract local detail features and capture global dependencies. A Semantic Attention Guidance (SAG) further refines the fused image’s semantic representation, aligning it with task objectives. Finally, a distillation loss provides more robust training and excellent image quality. Experimental results show that SKFFusion outperforms existing methods in visual quality and vision task performance, particularly under challenging conditions like low-light and fog. Our code is available at https://github.com/yyzzttkkjj/SKFFusion .},
  archive      = {J_ICV},
  author       = {Wei Zhou and Yingyuan Wang and Lina Zuo and Dan Ma and Yugen Yi},
  doi          = {10.1016/j.imavis.2025.105566},
  journal      = {Image and Vision Computing},
  month        = {6},
  pages        = {105566},
  shortjournal = {Image Vis. Comput.},
  title        = {Self-distillation guided semantic knowledge feedback network for infrared–visible image fusion},
  volume       = {159},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Incremental structural adaptation for camouflaged object detection. <em>ICV</em>, <em>159</em>, 105565. (<a href='https://doi.org/10.1016/j.imavis.2025.105565'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Camouflaged Object Detection (COD) is a challenging task due to the similarity between camouflaged objects and their backgrounds. Recent approaches predominantly utilize structural cues but often struggle with misinterpretations and noise, particularly for small objects. To address these issues, we propose the Structure-Adaptive Network (SANet), which incrementally supplements structural information from points to surfaces. Our method includes the Key Point Structural Information Prompting Module (KSIP) to enhance point-level structural information, Mixed-Resolution Attention (MRA) to incorporate high-resolution details, and the Structural Adaptation Patch (SAP) to selectively integrate high-resolution patches based on the shape of the camouflaged object. Experimental results on three widely used COD datasets demonstrate that SANet significantly outperforms state-of-the-art methods, achieving more accurate localization and finer edge segmentation, while minimizing background noise. Our code is available at https://github.com/vstar37/SANet/ .},
  archive      = {J_ICV},
  author       = {Qingzheng Wang and Jiazhi Xie and Ning Li and Xingqin Wang and Wenhui Liu and Zengwei Mai},
  doi          = {10.1016/j.imavis.2025.105565},
  journal      = {Image and Vision Computing},
  month        = {6},
  pages        = {105565},
  shortjournal = {Image Vis. Comput.},
  title        = {Incremental structural adaptation for camouflaged object detection},
  volume       = {159},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Corrigendum to “A novel framework for diverse video generation from a single video using frame-conditioned denoising diffusion probabilistic model and ConvNeXt-v2” [Image and vision computing 154 (2025) 105422]. <em>ICV</em>, <em>159</em>, 105556. (<a href='https://doi.org/10.1016/j.imavis.2025.105556'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_ICV},
  author       = {Ayushi Verma and Tapas Badal and Abhay Bansal},
  doi          = {10.1016/j.imavis.2025.105556},
  journal      = {Image and Vision Computing},
  month        = {6},
  pages        = {105556},
  shortjournal = {Image Vis. Comput.},
  title        = {Corrigendum to “A novel framework for diverse video generation from a single video using frame-conditioned denoising diffusion probabilistic model and ConvNeXt-v2” [Image and vision computing 154 (2025) 105422]},
  volume       = {159},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Enhancing brain tumor classification in MRI images: A deep learning-based approach for accurate diagnosis. <em>ICV</em>, <em>159</em>, 105555. (<a href='https://doi.org/10.1016/j.imavis.2025.105555'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Background Detecting brain tumors from MRI images is crucial for early intervention, accurate diagnosis, and effective treatment planning. MRI imaging offers detailed information about the location, size, and characteristics of brain tumors which enables healthcare professionals to make decisions considering treatment options such as surgery, radiation therapy, and chemotherapy. However, this process is time-consuming and demands specialized expertise to manually assess MRI images. Presently, advancements in Computer-Aided Diagnosis (CAD), machine learning, and deep learning have enabled radiologists to pinpoint brain tumors more effectively and reliably. Objective Traditional machine learning techniques used in addressing this issue necessitate manually crafted features for classification purposes. Conversely, deep learning methodologies can be formulated to circumvent the need for manual feature extraction while achieving precise classification outcomes. Accordingly, we decided to propose a deep learning based model for automatic classification of brain tumors from MRI images. Method Two different deep learning based models were designed to detect both binary (abnormal and normal) and multiclass (glioma, meningioma, and pituitary) brain tumors. Figshare, Br35H, and Harvard Medical datasets comprising 3064, 3000, and 152 MRI images were used to train the proposed models. Initially, a deep Convolutional Neural Network (CNN) including 26 layers was applied to the Figshare dataset due to its extensive MRI image count for training purposes. While the proposed ‘Deep CNN’ architecture encountered issues of overfitting, transfer learning was utilized by individually combining fine-tuned VGG16 and Xception architectures with an adaptation of the ‘Deep CNN’ model on Br35H and Harvard Medical datasets. Results Experimental results indicated that the proposed Deep CNN achieved a classification accuracy of 97.27% on the Figshare dataset. Accuracies of 97.14% and 98.57% were respectively obtained using fine-tuned VGG16 and Xception on the Br35H dataset.100% accuracy was also obtained on the Harvard Medical dataset using both fine-tuned models.},
  archive      = {J_ICV},
  author       = {Hossein Sadr and Mojdeh Nazari and Shahrokh Yousefzadeh-Chabok and Hassan Emami and Reza Rabiei and Ali Ashraf},
  doi          = {10.1016/j.imavis.2025.105555},
  journal      = {Image and Vision Computing},
  month        = {6},
  pages        = {105555},
  shortjournal = {Image Vis. Comput.},
  title        = {Enhancing brain tumor classification in MRI images: A deep learning-based approach for accurate diagnosis},
  volume       = {159},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). GCESS: A two-phase generative learning framework for estimate molecular expression to cell detection and analysis. <em>ICV</em>, <em>159</em>, 105554. (<a href='https://doi.org/10.1016/j.imavis.2025.105554'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Whole slide image (WSI) plays an important role in cancer research. Cell recognition is the foundation and key steps of WSI analysis at the cellular level, including cell segmentation, subtypes detection and molecular expression prediction at the cellular level. Current end-to-end supervised learning models rely heavily on a large amount of manually labeled data and self-supervised learning models are limited to cell binary segmentation. All of these methods lack the ability to predict the expression level of molecules in single cells. In this study, we proposed a two-phase generative adversarial learning framework, named GCESS, which can achieve end-to-end cell binary segmentation, subtypes detection and molecular expression prediction simultaneously. The framework uses generative adversarial learning to obtain better cell binary segmentation results in the first phase by integrating the cell binary segmentation results of some segmentation models and generates multiplex immunohistochemistry (mIHC) images through generative adversarial networks to predict the expression of cell molecules in the second phase. The cell semantic segmentation results can be obtained by spatially mapping the binary segmentation and molecular expression results in pixel level. The method we proposed achieves a Dice of 0.865 on cell binary segmentation, an accuracy of 0.917 on cell semantic segmentation and a Peak Signal to Noise Ratio (PSNR) of 20.929 dB on mIHC images generating, outperforming other competing methods (P-value < 0.05). The method we proposed will provide an effective tool for cellular level analysis of digital pathology images and cancer research.},
  archive      = {J_ICV},
  author       = {Tianwang Xun and Lei Su and Wenting Shang and Di Dong and Lizhi Shao},
  doi          = {10.1016/j.imavis.2025.105554},
  journal      = {Image and Vision Computing},
  month        = {6},
  pages        = {105554},
  shortjournal = {Image Vis. Comput.},
  title        = {GCESS: A two-phase generative learning framework for estimate molecular expression to cell detection and analysis},
  volume       = {159},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). RC-SODet: Reparameterized dual convolutions and compact feature enhancement for small object detector. <em>ICV</em>, <em>159</em>, 105552. (<a href='https://doi.org/10.1016/j.imavis.2025.105552'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the field of object detection, small object detection tasks have broad application prospects. However, detection models often face issues with insufficient image features for small objects and limited computational resources. To address these issues, we propose RC-SODet, a small object detector that uses reparameterization techniques combined with dual convolutions and compact feature enhancement blocks. In the detector, we design Reparameterized Dual Convolutions (RepDuConv) to replace conventional convolution and downsampling blocks. Its dual-branch advantage maintains accuracy, and the reparameterization technique built on this significantly improves inference efficiency. Compact Feature-enhanced Pyramid Network (RC-FPN) serves as the neck, using reparameterizable Cross Stage Partial with Feature Fusion Reparameterized Compact Blocks (C2fRCB) for feature enhancement. First, in the backbone network, RepDuConv replaces convolution blocks to perform downsampling on input images, thereby obtaining multi-scale features to pass to the neck. Second, the model uses RC-FPN as the feature pyramid neck to process multi-scale features from the backbone. After each front-end upsampling and fusion, dual-layer C2fRCB is applied to further refine and enhance the tensor features at different fusion scales. Finally, multi-level feature maps are fused at the back-end and passed to the detection head. Additionally, in the inference stage, both RepDuConv and C2fRCB optimize branch structures through reparameterization techniques. Experimental results show that on the small object datasets VisDrone and DroneVehicle, the highest version of RC-SODet achieves 48.1% and 82.4% mAP50, as well as 30.1% and 59.1% mAP50-95, respectively. The designed reparameterization technique increases the model inference speed by 58.1%.},
  archive      = {J_ICV},
  author       = {Ze Wu and Zhongxu Li and Huan Lei and Hong Zhao and Wenyuan Yang},
  doi          = {10.1016/j.imavis.2025.105552},
  journal      = {Image and Vision Computing},
  month        = {6},
  pages        = {105552},
  shortjournal = {Image Vis. Comput.},
  title        = {RC-SODet: Reparameterized dual convolutions and compact feature enhancement for small object detector},
  volume       = {159},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). SinWaveFusion: Learning a single image diffusion model in wavelet domain. <em>ICV</em>, <em>159</em>, 105551. (<a href='https://doi.org/10.1016/j.imavis.2025.105551'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Although recent advancements in large-scale image generation models have substantially improved visual fidelity and reliability, current diffusion models continue to encounter significant challenges in maintaining stylistic consistency with the original images. These challenges stem primarily from the intrinsic stochastic nature of the diffusion process, leading to noticeable variability and inconsistency in edited outputs. To address these challenges, this paper proposes a novel framework termed single image wavelet diffusion (SinWaveFusion) , explicitly designed to enhance the consistency and fidelity in generating images derived from a single source image while also mitigating information leakage. SinWaveFusion addresses generative artifacts by employing the multi-scale properties inherent in wavelet decomposition, which incorporates a built-in up-down scaling mechanism. This approach enables refined image manipulation while enhancing stylistic coherence. The proposed diffusion model, trained exclusively on a single source image, utilizes the hierarchical structure of wavelet subbands to effectively capture spatial and spectral information in the sampling process, minimizing reconstruction loss and ensuring high-quality, diverse outputs. Moreover, the architecture of the denoiser features a reduced receptive field, strategically preventing the model from memorizing the entire training image and thereby offering additional computational efficiency benefits. Experimental results demonstrate that SinWaveFusion achieves improved performance in both conditional and unconditional generation compared to existing generative models trained on a single image.},
  archive      = {J_ICV},
  author       = {Jisoo Kim and Jiwoo Kang and Taewan Kim and Heeseok Oh},
  doi          = {10.1016/j.imavis.2025.105551},
  journal      = {Image and Vision Computing},
  month        = {6},
  pages        = {105551},
  shortjournal = {Image Vis. Comput.},
  title        = {SinWaveFusion: Learning a single image diffusion model in wavelet domain},
  volume       = {159},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Rethinking the sample relations for few-shot classification. <em>ICV</em>, <em>159</em>, 105550. (<a href='https://doi.org/10.1016/j.imavis.2025.105550'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Feature quality is paramount for classification performance, particularly in few-shot scenarios. Contrastive learning, a widely adopted technique for enhancing feature quality, leverages sample relations to extract intrinsic features that capture semantic information and has achieved remarkable success in Few-Shot Learning (FSL). Nevertheless, current few-shot contrastive learning approaches often overlook the semantic similarity discrepancies at different granularities when employing the same modeling approach for different sample relations, which limits the potential of few-shot contrastive learning. In this paper, we introduce a straightforward yet effective contrastive learning approach, Multi-Grained Relation Contrastive Learning (MGRCL), as a pre-training feature learning model to boost few-shot learning by meticulously modeling sample relations at different granularities. MGRCL categorizes sample relations into three types: intra-sample relation of the same sample under different transformations, intra-class relation of homogeneous samples, and inter-class relation of inhomogeneous samples. In MGRCL, we design Transformation Consistency Learning (TCL) to ensure the rigorous semantic consistency of a sample under different transformations by aligning predictions of input pairs. Furthermore, to preserve discriminative information, we employ Class Contrastive Learning (CCL) to ensure that a sample is always closer to its homogeneous samples than its inhomogeneous ones, as homogeneous samples share similar semantic content while inhomogeneous samples have different semantic content. Our method is assessed across four popular FSL benchmarks, showing that such a simple pre-training feature learning method surpasses a majority of leading FSL methods. Moreover, our method can be incorporated into other FSL methods as the pre-trained model and help them obtain significant performance gains.},
  archive      = {J_ICV},
  author       = {Guowei Yin and Sheng Huang and Luwen Huangfu and Yi Zhang and Xiaohong Zhang},
  doi          = {10.1016/j.imavis.2025.105550},
  journal      = {Image and Vision Computing},
  month        = {6},
  pages        = {105550},
  shortjournal = {Image Vis. Comput.},
  title        = {Rethinking the sample relations for few-shot classification},
  volume       = {159},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). W-net: A facial feature-guided face super-resolution network. <em>ICV</em>, <em>159</em>, 105549. (<a href='https://doi.org/10.1016/j.imavis.2025.105549'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Face Super-Resolution (FSR) aims to recover high-resolution (HR) face images from low-resolution (LR) ones. Despite the progress made by convolutional neural networks in FSR, the results of existing approaches are not ideal due to their low reconstruction efficiency and insufficient utilization of prior information. Considering that faces are highly structured objects, effectively leveraging facial priors to improve FSR results is a worthwhile endeavor. This paper proposes a novel network architecture called W-Net to address this challenge. W-Net leverages a meticulously designed Parsing Block to fully exploit the resolution potential of LR image. We use this parsing map as an attention prior, effectively integrating information from both the parsing map and LR images. Simultaneously, we perform multiple fusions across different latent representation dimensions through the W-shaped network structure combined with the LPF( L R- P arsing Map F usion Module). Additionally, we utilize a facial parsing graph as a mask, assigning different weights and loss functions to key facial areas to balance the performance of our reconstructed facial images between perceptual quality and pixel accuracy. We conducted extensive comparative experiments, not only limited to conventional facial super-resolution metrics but also extending to downstream tasks such as facial recognition and facial keypoint detection. The experiments demonstrate that W-Net exhibits outstanding performance in quantitative metrics, visual quality, and downstream tasks.},
  archive      = {J_ICV},
  author       = {Hao Liu and Yang Yang and Yunxia Liu},
  doi          = {10.1016/j.imavis.2025.105549},
  journal      = {Image and Vision Computing},
  month        = {6},
  pages        = {105549},
  shortjournal = {Image Vis. Comput.},
  title        = {W-net: A facial feature-guided face super-resolution network},
  volume       = {159},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Integrating end-to-end multimodal deep learning and domain adaptation for robust facial expression recognition. <em>ICV</em>, <em>159</em>, 105548. (<a href='https://doi.org/10.1016/j.imavis.2025.105548'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper presents an advanced approach to a facial expression recognition (FER) system designed for robust performance across diverse imaging environments. The proposed method consists of four primary components: image preprocessing, feature representation and classification, cross-domain feature analysis, and domain adaptation. The process begins with facial region extraction from input images, including those captured in unconstrained imaging conditions, where variations in lighting, background, and image quality significantly impact recognition performance. The extracted facial region undergoes feature extraction using an ensemble of multimodal deep learning techniques, including end-to-end CNNs, BilinearCNN, TrilinearCNN, and pretrained CNN models, which capture both local and global facial features with high precision. The ensemble approach enriches feature representation by integrating information from multiple models, enhancing the system’s ability to generalize across different subjects and expressions. These deep features are then passed to a classifier trained to recognize facial expressions effectively in real-time scenarios. Since images captured in real-world conditions often contain noise and artifacts that can compromise accuracy, cross-domain analysis is performed to evaluate the discriminative power and robustness of the extracted deep features. FER systems typically experience performance degradation when applied to domains that differ from the original training environment. To mitigate this issue, domain adaptation techniques are incorporated, enabling the system to effectively adjust to new imaging conditions and improving recognition accuracy even in challenging real-time acquisition environments. The proposed FER system is validated using four well-established benchmark datasets: CK+, KDEF, IMFDB and AffectNet. Experimental results demonstrate that the proposed system achieves high performance within original domains and exhibits superior cross-domain recognition compared to existing state-of-the-art methods. These findings indicate that the system is highly reliable for applications requiring robust and adaptive FER capabilities across varying imaging conditions and domains.},
  archive      = {J_ICV},
  author       = {Mahmoud Hassaballah and Chiara Pero and Ranjeet Kumar Rout and Saiyed Umer},
  doi          = {10.1016/j.imavis.2025.105548},
  journal      = {Image and Vision Computing},
  month        = {6},
  pages        = {105548},
  shortjournal = {Image Vis. Comput.},
  title        = {Integrating end-to-end multimodal deep learning and domain adaptation for robust facial expression recognition},
  volume       = {159},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Underwater image restoration using joint Local–Global polarization complementary network. <em>ICV</em>, <em>159</em>, 105546. (<a href='https://doi.org/10.1016/j.imavis.2025.105546'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Underwater image always suffers from the degradation of visual quality and lack of clear details caused by light scattering effect. Since polarization imaging can effectively eliminate the backscattering light, polarization-based methods become more attractive to restore the image, which utilize the difference of polarization characteristics to boost the restoration performance. In this paper, we propose an underwater image restoration using joint Local–Global Polarization Complementary Network, named LGPCNet, to achieve a clear underwater image from multi-polarization images. In particular, we design a local polarization complement module (LCM) to adaptively fuse complementary information of local regions from images with different polarization states. By incorporating this, we can restore rich details including color and texture from other polarimetric images. Then, to balance visual effects between images with different polarization states, we propose a global appearance sharing module (GSM) to obtain the consistent brightness across different polarization images. Finally, we adaptively aggregate the restored information from each polarization states to obtain a final clear image. Experiments on an extended natural underwater polarization image dataset demonstrate that our proposed method achieves superior image restoration performance in terms of color, brightness and contrast compared with state-of-the-art image restored methods.},
  archive      = {J_ICV},
  author       = {Rui Ruan and Weidong Zhang and Zheng Liang},
  doi          = {10.1016/j.imavis.2025.105546},
  journal      = {Image and Vision Computing},
  month        = {6},
  pages        = {105546},
  shortjournal = {Image Vis. Comput.},
  title        = {Underwater image restoration using joint Local–Global polarization complementary network},
  volume       = {159},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A landmarks-assisted diffusion model with heatmap-guided denoising loss for high-fidelity and controllable facial image generation. <em>ICV</em>, <em>159</em>, 105545. (<a href='https://doi.org/10.1016/j.imavis.2025.105545'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Diffusion models have significantly advanced image generation, enabling users to create diverse and realistic images from simple prompts. However, generating high-fidelity, controllable facial images remains a challenge due to the intricate details of human faces. In this paper, we present a novel diffusion model for landmarks-assisted text to face generation which directly incorporates landmarks as guidance during the diffusion process. To address the issue of global information degradation caused by fine-tuning with local information, we introduce a heatmap-guided denoising loss that selectively focuses on feature pixels most relevant to the conditioning. This biased learning strategy ensures that the model prioritizes shape and positional information, preventing excessive deterioration of its generalization ability. Unlike existing methods relying on an extra learnable branch for conditional control, our native method eliminates the conflicts inherent in dual-branch architectures when dealing with various conditions. It also enables precise manipulation of facial features, such as shape and position. Extensive experiments on CelebA-HQ and CelebAText-HQ dataset show that our method demonstrates superior performance in generating realistic and controllable facial images, outperforming existing methods in terms of fidelity, diversity, and alignment with specified landmarks.},
  archive      = {J_ICV},
  author       = {Xing Wang and Wei Wang and Shixiang Su and Mingqi Lu and Lei Zhang and Xiaobo Lu},
  doi          = {10.1016/j.imavis.2025.105545},
  journal      = {Image and Vision Computing},
  month        = {6},
  pages        = {105545},
  shortjournal = {Image Vis. Comput.},
  title        = {A landmarks-assisted diffusion model with heatmap-guided denoising loss for high-fidelity and controllable facial image generation},
  volume       = {159},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Exemplar-free class incremental action recognition based on self-supervised learning. <em>ICV</em>, <em>159</em>, 105544. (<a href='https://doi.org/10.1016/j.imavis.2025.105544'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Class incremental action recognition faces the persistent challenge of balancing stability and plasticity, as models must learn new classes without forgetting previously acquired knowledge. Existing methods often rely on storing original samples, which significantly increases storage demands and risks overfitting to past data. To address these issues, an exemplar-free framework based on self-supervised learning and Pseudo-Feature Generation (PFG) mechanism is proposed. At each incremental step, PFG generates pseudo features for previously learned classes by using the mean and variance for each class. This framework enables effective joint training on new class data while keeping the feature extractor frozen, eliminating the need to store original data. It preserves past knowledge and dynamically adapts to new categories, striking a balance between stability and plasticity. Experiments on four extensively used datasets: UCF101, HMDB51, Kinetics, and SSV2 validate the effectiveness of the proposed framework.},
  archive      = {J_ICV},
  author       = {Chunyu Hou and Yonghong Hou and Jinyin Jiang and Gunel Abdullayeva},
  doi          = {10.1016/j.imavis.2025.105544},
  journal      = {Image and Vision Computing},
  month        = {6},
  pages        = {105544},
  shortjournal = {Image Vis. Comput.},
  title        = {Exemplar-free class incremental action recognition based on self-supervised learning},
  volume       = {159},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A2VIS: Amodal-aware approach to video instance segmentation. <em>ICV</em>, <em>159</em>, 105543. (<a href='https://doi.org/10.1016/j.imavis.2025.105543'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Handling occlusion remains a significant challenge for video instance-level tasks like Multiple Object Tracking (MOT) and Video Instance Segmentation (VIS). In this paper, we propose a novel framework, Amodal-Aware Video Instance Segmentation (A2VIS), which incorporates amodal representations to achieve a reliable and comprehensive understanding of both visible and occluded parts of objects in a video. The key intuition is that awareness of amodal segmentation through spatiotemporal dimension enables a stable stream of object information. In scenarios where objects are partially or completely hidden from view, amodal segmentation offers more consistency and less dramatic changes along the temporal axis compared to visible segmentation. Hence, both amodal and visible information from all clips can be integrated into one global instance prototype. To effectively address the challenge of video amodal segmentation, we introduce the spatiotemporal-prior Amodal Mask Head, which leverages visible information intra clips while extracting amodal characteristics inter clips. Through extensive experiments and ablation studies, we show that A2VIS excels in both MOT and VIS tasks in identifying and tracking object instances with a keen understanding of their full shape.},
  archive      = {J_ICV},
  author       = {Minh Tran and Thang Pham and Winston Bounsavy and Tri Nguyen and Ngan Le},
  doi          = {10.1016/j.imavis.2025.105543},
  journal      = {Image and Vision Computing},
  month        = {6},
  pages        = {105543},
  shortjournal = {Image Vis. Comput.},
  title        = {A2VIS: Amodal-aware approach to video instance segmentation},
  volume       = {159},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Automated dual CNN-based feature extraction with SMOTE for imbalanced diabetic retinopathy classification. <em>ICV</em>, <em>159</em>, 105537. (<a href='https://doi.org/10.1016/j.imavis.2025.105537'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The primary cause of Diabetic Retinopathy (DR) is high blood sugar due to long-term diabetes. Early and correct diagnosis of the DR is essential for timely and effective treatment. Despite high performance of recently developed models, there is still a need to overcome the problem of class imbalance issues and feature extraction to achieve accurate results. To resolve this problem, we have presented an automated model combining the customized ResNet-50 and EfficientNetB0 for detecting and classifying DR in fundus images. The proposed model addresses class imbalance using data augmentation and Synthetic Minority Oversampling Technique (SMOTE) for oversampling the training data and enhances the feature extraction process through fine-tuned ResNet50 and EfficientNetB0 models with ReLU activations and global average pooling. Combining extracted features and then passing it to four different classifiers effectively captures both local and global spatial features, thereby improving classification accuracy for diabetic retinopathy. For Experiment, The APTOS 2019 Dataset is used, and it contains of 3662 high-quality fundus images. The performance of the proposed model is assessed using several metrics, and the findings are compared with contemporary methods for diabetic retinopathy detection. The suggested methodology demonstrates substantial enhancement in diabetic retinopathy diagnosis for fundus pictures. The proposed automated model attained an accuracy of 98.5% for binary classification and 92.73% for multiclass classification.},
  archive      = {J_ICV},
  author       = {Danyal Badar Soomro and Wang ChengLiang and Mahmood Ashraf and Dina Abdulaziz AlHammadi and Shtwai Alsubai and Carlo Medaglia and Nisreen Innab and Muhammad Umer},
  doi          = {10.1016/j.imavis.2025.105537},
  journal      = {Image and Vision Computing},
  month        = {6},
  pages        = {105537},
  shortjournal = {Image Vis. Comput.},
  title        = {Automated dual CNN-based feature extraction with SMOTE for imbalanced diabetic retinopathy classification},
  volume       = {159},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Bridging efficiency and interpretability: Explainable AI for multi-classification of pulmonary diseases utilizing modified lightweight CNNs. <em>ICV</em>, <em>158</em>, 105553. (<a href='https://doi.org/10.1016/j.imavis.2025.105553'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Pulmonary diseases are notable global health challenges that contribute to increased morbidity and mortality rates. Early and accurate diagnosis is essential for effective treatment. However, traditional apprehension of chest X-ray images is tiresome and susceptible to human error, particularly in resource-constrained settings. Current progress in deep learning, particularly convolutional neural networks, has enabled the automated classification of pulmonary diseases with increased accuracy. In this study, we have proposed an explainable AI approach using modified lightweight convolution neural networks, such as MobileNetV2, EfficientNet-B0, NASNetMobile, and ResNet50V2 to achieve efficient and interpretable classification of multiple pulmonary diseases. Lightweight CNNs are designed to minimize computational complexity while maintaining robust performance, making them ideal for mobile and embedded systems with limited processing power deployment. Our models demonstrated strong performance in detecting pulmonary diseases, with EfficientNet-B0 achieving an accuracy of 94.07%, precision of 94.16%, recall of 94.07%, and F1 score of 94.04%. Furthermore, we have incorporated explainability methods (grad-CAM & t-SNE) to enhance the transparency of model predictions, providing clinicians with a trustworthy tool for diagnostic decision support. The results suggest that lightweight CNNs effectively balance accuracy, efficiency, and interpretability, making them suitable for real-time pulmonary disease detection in clinical and low-resource environments},
  archive      = {J_ICV},
  author       = {Samia Khan and Farheen Siddiqui and Mohd Abdul Ahad},
  doi          = {10.1016/j.imavis.2025.105553},
  journal      = {Image and Vision Computing},
  month        = {5},
  pages        = {105553},
  shortjournal = {Image Vis. Comput.},
  title        = {Bridging efficiency and interpretability: Explainable AI for multi-classification of pulmonary diseases utilizing modified lightweight CNNs},
  volume       = {158},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Hybrid attention transformers with fast fourier convolution for light field image super-resolution. <em>ICV</em>, <em>158</em>, 105542. (<a href='https://doi.org/10.1016/j.imavis.2025.105542'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The limited spatial resolution of light field (LF) cameras has hindered their widespread adoption, emphasizing the critical need for superresolution techniques to improve their practical use. Transformer-based methods, such as LF-DET, have shown potential in enhancing light field spatial super-resolution (LF-SR). However, LF-DET, which employs a spatial-angular separable transformer encoder with sub-sampling spatial and multiscale angular modeling for global context interaction, struggles to effectively capture global context in early layers and local details. In this work, we introduce LF-HATF, a novel network that builds on the LF-DET framework and incorporates Fast Fourier Convolution (FFC) and Hybrid Attention Transformers (HATs) to address these limitations. This integration enables LF-HATF to better capture both global and local information, significantly improving the restoration of edge details and textures, and providing a more comprehensive understanding of complex scenes. Additionally, we propose the Light Field Charbonnier loss function, designed to balance differential distributions across various LF views. This function minimizes errors both within the same perspective and across different views, further enhancing the model’s performance. Our evaluation on five public LF datasets demonstrates that LF-HATF outperforms existing methods, representing a significant advancement in LF-SR technology. This progress pushes the field forward and opens new avenues for research in light field imaging, unlocking the full potential of light field cameras.},
  archive      = {J_ICV},
  author       = {Zhicheng Ma and Yuduo Guo and Zhaoxiang Liu and Shiguo Lian and Sen Wan},
  doi          = {10.1016/j.imavis.2025.105542},
  journal      = {Image and Vision Computing},
  month        = {5},
  pages        = {105542},
  shortjournal = {Image Vis. Comput.},
  title        = {Hybrid attention transformers with fast fourier convolution for light field image super-resolution},
  volume       = {158},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Dynamic feature extraction and histopathology domain shift alignment for mitosis detection. <em>ICV</em>, <em>158</em>, 105541. (<a href='https://doi.org/10.1016/j.imavis.2025.105541'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Mitosis count is of crucial significance in cancer diagnosis; therefore, mitosis detection is a meaningful subject in medical image studies. The challenge of mitosis detection lies in the intra-class variance of mitosis and hard negatives, i.e., the sizes/ shapes of mitotic cells vary considerably and plenty of non-mitotic cells resemble mitosis, and the histopathology domain shift across datasets caused by different tissues and organs, scanners, labs, etc. In this paper, we propose a novel Domain Generalized Dynamic Mitosis Detector (DGDMD) to handle the intra-class variance and histopathology domain shift of mitosis detection with a dynamic mitosis feature extractor based on residual structured depth-wise convolution and domain shift alignment terms. The proposed dynamic mitosis feature extractor handles the intra-class variance caused by different sizes and shapes of mitotic cells as well as non-mitotic hard negatives. The proposed domain generalization schedule implemented via novel histopathology-mitosis domain shift alignments deals with the domain shift between histopathology slides in training and test datasets from different sources. We validate the domain generalization ability for mitosis detection of our algorithm on the MIDOG++ dataset and typical mitosis datasets, including the MIDOG 2021, ICPR MITOSIS 2014, AMIDA 2013, and TUPAC 16. Experimental results show that we achieve state-of-the-art (SOTA) performance on the MIDOG++ dataset for the domain generalization across tissue and organs of mitosis detection, across scanners on the MIDOG 2021 dataset, and across data sources on external datasets, demonstrating the effectiveness of our proposed method on the domain generalization of mitosis detection.},
  archive      = {J_ICV},
  author       = {Jiangxiao Han and Shikang Wang and Lianjun Wu and Wenyu Liu},
  doi          = {10.1016/j.imavis.2025.105541},
  journal      = {Image and Vision Computing},
  month        = {5},
  pages        = {105541},
  shortjournal = {Image Vis. Comput.},
  title        = {Dynamic feature extraction and histopathology domain shift alignment for mitosis detection},
  volume       = {158},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). DeepNet: Protection of deepfake images with aid of deep learning networks. <em>ICV</em>, <em>158</em>, 105540. (<a href='https://doi.org/10.1016/j.imavis.2025.105540'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the present information age, multimedia security has become a challenging task. Especially increased usage of images as multimedia data has been a key aspect in this digital transmission era. Deep fake detection of images is a real-time problem which needs to be focused. To resolve this challenge, a novel deep fake detection algorithm is proposed in this article. The presented research uses the Viola-Jones detection algorithm for efficient deep fake image detection. To protect the integrity of these images, the multiresolution domain approach is effectively utilized with redundant discrete wavelet transform (RDWT) and multiresolution singular value decomposition (MSVD). Discrete cosine transform (DCT) is applied for the extraction of frequency components. An adaptive neuro-fuzzy inference system (ANFIS)-based optimization is applied to attain the optimum weighing factor (WF). This WF exhibits a better trade-off among attributes of watermarking. Furthermore, authentication is successfully implemented with the aid of various deep learning models such as SqueezeNet, EfficientNet-B0, ResNet-50 and InceptionV3. This implementation explores the various aspects related to the ownership assertion. Analysis of comprehensive simulation results depicts the effectiveness of the proposed technique over different prevailing techniques. With the development of the proposed technique, deep fake image detection can easily be realized and safeguards the images. The average percentage improvement in the imperceptibility of the proposed technique is 52.14% and for robustness is 7.51%.},
  archive      = {J_ICV},
  author       = {Divyanshu Awasthi and Priyank Khare and Vinay Kumar Srivastava and Amit Kumar Singh and Brij B. Gupta},
  doi          = {10.1016/j.imavis.2025.105540},
  journal      = {Image and Vision Computing},
  month        = {5},
  pages        = {105540},
  shortjournal = {Image Vis. Comput.},
  title        = {DeepNet: Protection of deepfake images with aid of deep learning networks},
  volume       = {158},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). MFC-net: Amodal instance segmentation with multi-path fusion and context-awareness. <em>ICV</em>, <em>158</em>, 105539. (<a href='https://doi.org/10.1016/j.imavis.2025.105539'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Amodal instance segmentation refers to sensing the entire instance in an image, thereby segmenting the visible parts of an object and the regions that may be masked. However, existing amodal instance segmentation methods predict rough mask edges and perform poorly in segmenting objects with significant size differences. In addition, the occlusion environment greatly limits the performance of the model. To address the above problems, this work proposes an amodal instance segmentation method called MFC-Net to accurately segment objects in an image. For the rough prediction of mask edges, the model introduces the multi-path transformer structure to obtain finer object semantic features and boundary information, which improves the accuracy of edge region segmentation. For the problem of poor segmentation of object instances with significant size differences, we design an adaptive feature fusion module AFF, which dynamically captures the scale changes related to object size and fuses the multi-scale semantic feature information, so that the model obtains a receptive field adapted to the object size. To address the poor performance of segmentation in the occlusion environment, we designed the context-aware mask segmentation module CMS in the prediction module to make a preliminary prediction of the object’s amodal region. The module enhances the amodal perception of the model by modeling the long-range dependencies of the objects and capturing the contextual information of the occluded part of the object. Compared with the state-of-the-art methods, the MFC-Net proposed in this paper achieves a mAP of 73.3% on the D2SA dataset and 33.9% and 36.9% on the KINS and COCOA-cls datasets, respectively. Moreover, MFC-Net can produce complete and detailed amodal masks.},
  archive      = {J_ICV},
  author       = {Yunfei Yang and Hongwei Deng and Yichun Wu},
  doi          = {10.1016/j.imavis.2025.105539},
  journal      = {Image and Vision Computing},
  month        = {5},
  pages        = {105539},
  shortjournal = {Image Vis. Comput.},
  title        = {MFC-net: Amodal instance segmentation with multi-path fusion and context-awareness},
  volume       = {158},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Controlling vision-language model for enhancing image restoration. <em>ICV</em>, <em>158</em>, 105538. (<a href='https://doi.org/10.1016/j.imavis.2025.105538'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Restoring low-quality images to their original high-quality state remains a significant challenge due to inherent uncertainties, particularly in blind image restoration scenarios where the nature of degradation is unknown. Despite recent advances, many restoration techniques still grapple with robustness and adaptability across diverse degradation conditions. In this paper, we introduce an approach to augment the restoration model by exploiting the robust prior features of CLIP, a large-scale vision-language model, to enhance its proficiency in handling a broader spectrum of degradation tasks. We integrate the robust priors from CLIP into the pre-trained image restoration model via cross-attention mechanisms, and we design a Prior Adapter to modulate these features, thereby enhancing the model’s restoration performance. Additionally, we introduce an innovative prompt learning framework that harnesses CLIP’s multimodal alignment capabilities to fine-tune pre-trained restoration models. Furthermore, we utilize CLIP’s contrastive loss to ensure that the restored images align more closely with the prompts of clean images in CLIP’s latent space, thereby improving the quality of the restoration. Through comprehensive experiments, we demonstrate the effectiveness and robustness of our method, showcasing its superior adaptability to a wide array of degradation tasks. Our findings emphasize the potential of integrating vision-language models such as CLIP to advance the cutting-edge in image restoration.},
  archive      = {J_ICV},
  author       = {Mingwen Shao and Weihan Liu and Qiwang Li and Lingzhuang Meng and Yecong Wan},
  doi          = {10.1016/j.imavis.2025.105538},
  journal      = {Image and Vision Computing},
  month        = {5},
  pages        = {105538},
  shortjournal = {Image Vis. Comput.},
  title        = {Controlling vision-language model for enhancing image restoration},
  volume       = {158},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Wave-based cross-phase representation for weakly supervised classification. <em>ICV</em>, <em>158</em>, 105527. (<a href='https://doi.org/10.1016/j.imavis.2025.105527'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Weakly Supervised Learning (WSL) aims to improve model robustness and manage label uncertainty, but current methods struggle to handle various weak label sources, such as incomplete and noisy labels. Additionally, these methods struggle with a lack of adaptability from reliance on prior knowledge and the complexity of managing data-label dependencies. To address these problems, we propose a wave-based cross-phase network (WCPN) to enhance adaptability for incomplete and noisy labels. Specifically, we expand wave representations and design a cross-phase token mixing (CPTM) module to refine feature relationships and integrate strategies for various weak labels. The proposed CPFE algorithm in the CPTM optimizes feature relationships by using self-interference and mutual-interference to process phase information between feature tokens, thus enhancing semantic consistency and discriminative ability. Furthermore, by employing a data-driven tri-branch structure and maximizing mutual information between features and labels, WCPN effectively overcomes the inflexibility caused by reliance on prior knowledge and complex data-label dependencies. In this way, WCPN leverages wave representations to enhance feature interactions, capture data complexity and diversity, and improve feature compactness for specific categories. Experimental results demonstrate that WCPN excels across various supervision levels and consistently outperforms existing advanced methods. It effectively handles noisy and incomplete labels, showing remarkable adaptability and enhanced feature understanding.},
  archive      = {J_ICV},
  author       = {Heng Zhou and Ping Zhong},
  doi          = {10.1016/j.imavis.2025.105527},
  journal      = {Image and Vision Computing},
  month        = {5},
  pages        = {105527},
  shortjournal = {Image Vis. Comput.},
  title        = {Wave-based cross-phase representation for weakly supervised classification},
  volume       = {158},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Dual region mutual enhancement network for camouflaged object detection. <em>ICV</em>, <em>158</em>, 105526. (<a href='https://doi.org/10.1016/j.imavis.2025.105526'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Camouflaged Object Detection (COD) is a promising yet challenging task that aims to segment objects hidden in intricate surroundings. Current methods often struggle with identifying background regions that resemble camouflaged objects, posing a significant challenge. To mitigate this issue, we propose a novel Dual Region Mutual Enhancement Network (DRMENet), which separately extracts camouflaged object and background region features and these branches mutually assist each other to refine their respective region features. Specifically, in the foreground segmentation branch, we utilize the Background-assisted Foreground Region Enhancement (BFRE) subnetwork to enhance camouflaged object region features with background information. BFRE subnetwork consists of two parts: the Background-subtracted Foreground Refinement (BFR) module and the Scale-wise Feature Capturing (SFC) module, where the former obtains corresponding camouflaged object region features through cross-layer refinement with the assistance of background region features, and the latter captures scale-wise features and outputs a side output for region prediction result. Additionally, considering the noise present in low-level visual features, we introduce the Semantic-Guided Refinement (SGR) module, which progressively refines visual features based on enhanced semantic features. Experiments on challenging datasets show DRMENet’s superiority over the existing state-of-the-art methods. The source codes will be available at https://github.com/ycyinchao/DRMENet .},
  archive      = {J_ICV},
  author       = {Chao Yin and Xiaoqiang Li},
  doi          = {10.1016/j.imavis.2025.105526},
  journal      = {Image and Vision Computing},
  month        = {5},
  pages        = {105526},
  shortjournal = {Image Vis. Comput.},
  title        = {Dual region mutual enhancement network for camouflaged object detection},
  volume       = {158},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). I3Net: Intensive information interaction network for RGB-T salient object detection. <em>ICV</em>, <em>158</em>, 105525. (<a href='https://doi.org/10.1016/j.imavis.2025.105525'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-modality salient object detection (SOD) is receiving more and more attention in recent years. Infrared thermal images can provide useful information in extreme situations, such as low illumination and cluttered background. Accompany with extra information, we need a more delicate design to properly integrate multi-modal and multi-scale clues. In this paper, we propose an intensively information interaction network (I 3 Net) to perform Red-Green-Blue and Thermal (RGB-T) SOD, which optimizes the performance through modality interaction, level interaction, and scale interaction. Firstly, feature channels from different sources are dynamically selected according to the modality interaction with dynamic merging module. Then, adjacent level interaction is conducted under the guidance of coordinate channel and spatial attention with spatial feature aggregation module. Finally, we deploy pyramid attention module to obtain a more comprehensive scale interaction. Extensive experiments on four RGB-T datasets, VT821, VT1000, VT5000 and VI-RGBT3500, show that the proposed I 3 Net achieves a competitive and excellent performance against 13 state-of-the-art methods in multiple evaluation metrics, with a 1.70%, 1.41%, and 1.54% improvement in terms of weighted F-measure, mean E-measure, and S-measure.},
  archive      = {J_ICV},
  author       = {Jia Hou and Hongfa Wen and Shuai Wang and Chenggang Yan},
  doi          = {10.1016/j.imavis.2025.105525},
  journal      = {Image and Vision Computing},
  month        = {5},
  pages        = {105525},
  shortjournal = {Image Vis. Comput.},
  title        = {I3Net: Intensive information interaction network for RGB-T salient object detection},
  volume       = {158},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Towards on-device continual learning with binary neural networks in industrial scenarios. <em>ICV</em>, <em>158</em>, 105524. (<a href='https://doi.org/10.1016/j.imavis.2025.105524'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper addresses the challenges of deploying deep learning models, specifically Binary Neural Networks (BNNs), on resource-constrained embedded devices within the Internet of Things context. As deep learning continues to gain traction in IoT applications, the need for efficient models that can learn continuously from incremental data streams without requiring extensive computational resources has become more pressing. We propose a solution that integrates Continual Learning with BNNs, utilizing replay memory to prevent catastrophic forgetting. Our method focuses on quantized neural networks, introducing the quantization also for the backpropagation step, significantly reducing memory and computational requirements. Furthermore, we enhance the replay memory mechanism by storing intermediate feature maps ( i.e. latent replay) with 1-bit precision instead of raw data, enabling efficient memory usage. In addition to well-known benchmarks, we introduce the DL-Hazmat dataset, which consists of over 140k high-resolution grayscale images of 64 hazardous material symbols. Experimental results show a significant improvement in model accuracy and a substantial reduction in memory requirements, demonstrating the effectiveness of our method in enabling deep learning applications on embedded devices in real-world scenarios. Our work expands the application of Continual Learning and BNNs for efficient on-device training, offering a promising solution for IoT and other resource-constrained environments.},
  archive      = {J_ICV},
  author       = {Lorenzo Vorabbi and Angelo Carraggi and Davide Maltoni and Guido Borghi and Stefano Santi},
  doi          = {10.1016/j.imavis.2025.105524},
  journal      = {Image and Vision Computing},
  month        = {5},
  pages        = {105524},
  shortjournal = {Image Vis. Comput.},
  title        = {Towards on-device continual learning with binary neural networks in industrial scenarios},
  volume       = {158},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Counterfactual learning and saliency augmentation for weakly supervised semantic segmentation. <em>ICV</em>, <em>158</em>, 105523. (<a href='https://doi.org/10.1016/j.imavis.2025.105523'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The weakly supervised semantic segmentation based on image-level annotation has garnered widespread attention due to its excellent annotation efficiency and remarkable scalability. Numerous studies have utilized class activation maps generated by classification networks to produce pseudo-labels and train segmentation models accordingly. However, these methods exhibit certain limitations: biased localization activations, co-occurrence from the background, and semantic absence of target objects. We re-examine the aforementioned issues from a causal perspective and propose a framework for CounterFactual Learning and Saliency Augmentation (CFLSA) based on causal inference. CFLSA consists of a debiased causal chain and a positional causal chain. The debiased causal chain, through counterfactual decoupling generation module, compels the model to focus on constant target features while disregarding background features. It effectively eliminates spurious correlations between foreground objects and the background. Additionally, issues of biased activation and co-occurring pixel are alleviated. Secondly, in order to enable the model to recognize more comprehensive semantic information, we introduce a saliency augmentation mechanism in the positional causal chain to dynamically perceive foreground objects and background information. It can facilitate pixel-level feedback, leading to improved segmentation performance. With the collaboration of both chains, CFLSA achieves advanced results on the PASCAL VOC 2012 and MS COCO 2014 datasets.},
  archive      = {J_ICV},
  author       = {Xiangfu Ding and Youjia Shao and Na Tian and Li Wang and Wencang Zhao},
  doi          = {10.1016/j.imavis.2025.105523},
  journal      = {Image and Vision Computing},
  month        = {5},
  pages        = {105523},
  shortjournal = {Image Vis. Comput.},
  title        = {Counterfactual learning and saliency augmentation for weakly supervised semantic segmentation},
  volume       = {158},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Memory-MambaNav: Enhancing object-goal navigation through integration of spatial–temporal scanning with state space models. <em>ICV</em>, <em>158</em>, 105522. (<a href='https://doi.org/10.1016/j.imavis.2025.105522'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Object-goal Navigation (ObjectNav) involves locating a specified target object using a textual command combined with semantic understanding in an unknown environment. This requires the embodied agent to have advanced spatial and temporal comprehension about environment during navigation. While earlier approaches focus on spatial modeling, they either do not utilize episodic temporal memory (e.g., keeping track of explored and unexplored spaces) or are computationally prohibitive, as long-horizon memory knowledge is resource-intensive in both storage and training. To address this issue, this paper introduces the Memory-MambaNav model, which employs multiple Mamba-based layers for refined spatial–temporal modeling. Leveraging the Mamba architecture, known for its global receptive field and linear complexity, Memory-MambaNav can efficiently extract and process memory knowledge from accumulated historical observations. To enhance spatial modeling, we introduce the Memory Spatial Difference State Space Model (MSD-SSM) to address the limitations of previous CNN and Transformer-based models in terms of receptive field and computational demand. For temporal modeling, the proposed Memory Temporal Serialization SSM (MTS-SSM) leverages Mamba’s selective scanning capabilities in a cross-temporal manner, enhancing the model’s temporal understanding and interaction with bi-temporal features. We also integrate memory-aggregated egocentric obstacle-awareness embeddings (MEOE) and memory-based fine-grained rewards into our end-to-end policy training, which improve obstacle understanding and accelerate convergence by fully utilizing memory knowledge. Our experiments on the AI2-Thor dataset confirm the benefits and superior performance of proposed Memory-MambaNav, demonstrating Mamba’s potential in ObjectNav, particularly in long-horizon trajectories. All demonstration videos referenced in this paper can be viewed on the webpage ( https://sunleyuan.github.io/Memory-MambaNav ).},
  archive      = {J_ICV},
  author       = {Leyuan Sun and Yusuke Yoshiyasu},
  doi          = {10.1016/j.imavis.2025.105522},
  journal      = {Image and Vision Computing},
  month        = {5},
  pages        = {105522},
  shortjournal = {Image Vis. Comput.},
  title        = {Memory-MambaNav: Enhancing object-goal navigation through integration of spatial–temporal scanning with state space models},
  volume       = {158},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). DFDW: Distribution-aware filter and dynamic weight for open-mixed-domain test-time adaptation. <em>ICV</em>, <em>158</em>, 105521. (<a href='https://doi.org/10.1016/j.imavis.2025.105521'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Test-time adaptation (TTA) aims to adapt the pre-trained model to the unlabeled test data stream during inference. However, existing state-of-the-art TTA methods typically achieve superior performance in closed-set scenarios, and often underperform in more challenging open mixed-domain TTA scenarios. This can be attributed to ignoring two uncertainties: domain non-stationarity and semantic shifts, leading to inaccurate estimation of data distribution and unreliable model confidence. To alleviate the aforementioned issue, we propose a universal TTA method based on a Distribution-aware Filter and Dynamic Weight, called DFDW. Specifically, in order to improve the model’s discriminative ability to data distribution, our DFDW first designs a distribution-aware threshold to filter known and unknown samples from the test data, and then separates them based on contrastive learning. Furthermore, to improve the confidence and generalization of the model, we designed a dynamic weight consisting of category-reliable weight and diversity weight. Among them, category-reliable weight uses prior average predictions to enhance the guidance of high-confidence samples, and diversity weight uses negative information entropy to increase the influence of diversity samples. Based on the above approach, the model can accurately identify the distribution of semantic shift samples, and widely adapt to the diversity samples in the non-stationary domain. Extensive experiments on CIFAR and ImageNet-C benchmarks show the superiority of our DFDW.},
  archive      = {J_ICV},
  author       = {Mingwen Shao and Xun Shao and Lingzhuang Meng and Yuanyuan Liu},
  doi          = {10.1016/j.imavis.2025.105521},
  journal      = {Image and Vision Computing},
  month        = {5},
  pages        = {105521},
  shortjournal = {Image Vis. Comput.},
  title        = {DFDW: Distribution-aware filter and dynamic weight for open-mixed-domain test-time adaptation},
  volume       = {158},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Image–text feature learning for unsupervised visible–infrared person re-identification. <em>ICV</em>, <em>158</em>, 105520. (<a href='https://doi.org/10.1016/j.imavis.2025.105520'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Visible–infrared person re-identification (VI-ReID) focuses on matching infrared and visible images of the same person. To reduce labeling costs, unsupervised VI-ReID (UVI-ReID) methods typically use clustering algorithms to generate pseudo-labels and iteratively optimize the model based on these pseudo-labels. Although existing UVI-ReID methods have achieved promising performance, they often overlook the effectiveness of text semantics in inter-modality matching and modality-invariant feature learning. In this paper, we propose an image–text feature learning (ITFL) method, which not only leverages text semantics to enhance intra-modality identity-related learning but also incorporates text semantics into inter-modality matching and modality-invariant feature learning. Specifically, ITFL first performs modality-aware feature learning to generate pseudo-labels within each modality. Then, ITFL employs modality-invariant text modeling (MTM) to learn a text feature for each cluster in the visible modality, and utilizes inter-modality dual-semantics matching (IDM) to match inter-modality positive clusters. To obtain modality-invariant and identity-related image features, we not only introduce a cross-modality contrastive loss in ITFL to mitigate the impact of modality gaps, but also develop a text semantic consistency loss to further promote modality-invariant feature learning. Extensive experimental results on VI-ReID datasets demonstrate that ITFL not only outperforms existing unsupervised methods but also competes with some supervised approaches.},
  archive      = {J_ICV},
  author       = {Jifeng Guo and Zhiqi Pang},
  doi          = {10.1016/j.imavis.2025.105520},
  journal      = {Image and Vision Computing},
  month        = {5},
  pages        = {105520},
  shortjournal = {Image Vis. Comput.},
  title        = {Image–text feature learning for unsupervised visible–infrared person re-identification},
  volume       = {158},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Towards trustworthy image super-resolution via symmetrical and recursive artificial neural network. <em>ICV</em>, <em>158</em>, 105519. (<a href='https://doi.org/10.1016/j.imavis.2025.105519'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {AI-assisted living environments by widely apply the image super-resolution technique to improve the clarity of visual inputs for devices like smart cameras and medical monitors. This increased resolution enables more accurate object recognition, facial identification, and health monitoring, contributing to a safer and more efficient assisted living experience. Although rapid progress has been achieved, most current methods suffer from huge computational costs due to the complex network structures. To address this problem, we propose a symmetrical and recursive transformer network (SRTNet) for efficient image super-resolution via integrating the symmetrical CNN (S-CNN) unit and improved recursive Transformer (IRT) unit. Specifically, the S-CNN unit is equipped with a designed local feature enhancement (LFE) module and a feature distillation attention in attention (FDAA) block to realize efficient feature extraction and utilization. The IRT unit is introduced to capture long-range dependencies and contextual information to guarantee that the reconstruction image preserves high-frequency texture details. Extensive experiments demonstrate that the proposed SRTNet achieves competitive performance regarding reconstruction quality and model complexity compared with the state-of-the-art methods. In the × 2 , × 3 , and × 4 super-resolution tasks, SRTNet achieves the best performance on the BSD100, Set14, Set5, Manga109, and Urban100 datasets while maintaining low computational complexity.},
  archive      = {J_ICV},
  author       = {Mingliang Gao and Jianhao Sun and Qilei Li and Muhammad Attique Khan and Jianrun Shang and Xianxun Zhu and Gwanggil Jeon},
  doi          = {10.1016/j.imavis.2025.105519},
  journal      = {Image and Vision Computing},
  month        = {5},
  pages        = {105519},
  shortjournal = {Image Vis. Comput.},
  title        = {Towards trustworthy image super-resolution via symmetrical and recursive artificial neural network},
  volume       = {158},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Autonomous navigation and visual navigation in robot mission execution. <em>ICV</em>, <em>158</em>, 105516. (<a href='https://doi.org/10.1016/j.imavis.2025.105516'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Navigating autonomously in complex environments remains a significant challenge, as traditional methods relying on precise metric maps and conventional path planning algorithms often struggle with dynamic obstacles and demand high computational resources. To address these limitations, we propose a topological path planning approach that employs Bernstein polynomial parameterization and real-time object guidance to iteratively refine the preliminary path, ensuring smoothness and dynamic feasibility. Simulation results demonstrate that our method outperforms MSMRL, ANS, and NTS in both weighted inverse path length and navigation success rate. In real-world scenarios, it consistently achieves higher success rates and path efficiency compared to the widely used OGMADWA method. These findings confirm that our approach enables efficient and reliable navigation in dynamic environments while maintaining strong adaptability and robustness in path planning.},
  archive      = {J_ICV},
  author       = {Shulei Wang and Yan Wang and Zeyu Sun},
  doi          = {10.1016/j.imavis.2025.105516},
  journal      = {Image and Vision Computing},
  month        = {5},
  pages        = {105516},
  shortjournal = {Image Vis. Comput.},
  title        = {Autonomous navigation and visual navigation in robot mission execution},
  volume       = {158},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Dynamic semantic prototype perception for text–video retrieval. <em>ICV</em>, <em>158</em>, 105515. (<a href='https://doi.org/10.1016/j.imavis.2025.105515'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Semantic alignment between local visual regions and textual description is a promising solution for fine-grained text–video retrieval task. However, existing methods rely on the additional object detector as the explicit supervision, which is unfriendly to real application. To this end, a novel Dynamic Semantic Prototype Perception (DSP Perception) is proposed that automatically learns, constructs and infers the dynamic spatio-temporal dependencies between visual regions and text words without any explicit supervision. Specifically, DSP Perception consists of three components: the spatial semantic parsing module, the spatio-temporal semantic correlation module and the cross-modal semantic prototype alignment. The spatial semantic parsing module is leveraged to quantize visual patches to reduce the visual diversity, which helps to subsequently aggregate the similar semantic regions. The spatio-temporal semantic correlation module is introduced to learn dynamic information between adjacent frames and aggregate local features belonging to the same semantic in the video as tube. In addition, a novel global-to-local alignment strategy is proposed for the cross-modal semantic prototype alignment, which provides spatio-temporal cues for cross-modal perception of dynamic semantic prototypes. Thus, the proposed DSP Perception enables to capture local regions and their dynamic information within the video. Extensive experiments conducted on four widely-used datasets (MSR-VTT, MSVD, ActivityNet-Caption and DiDeMo) demonstrate the effectiveness of the proposed DSP Perception by comparison with several state-of-the-art methods.},
  archive      = {J_ICV},
  author       = {Henghao Zhao and Rui Yan and Zechao Li},
  doi          = {10.1016/j.imavis.2025.105515},
  journal      = {Image and Vision Computing},
  month        = {5},
  pages        = {105515},
  shortjournal = {Image Vis. Comput.},
  title        = {Dynamic semantic prototype perception for text–video retrieval},
  volume       = {158},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). MMGS: Multi-model synergistic gaussian splatting for sparse view synthesis. <em>ICV</em>, <em>158</em>, 105512. (<a href='https://doi.org/10.1016/j.imavis.2025.105512'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {3D Gaussian Splatting (3DGS) generates a field composed of 3D Gaussians to represent a scene. As the number of input training views decreases, the range of possible solutions that fit only training views expands significantly, making it challenging to identify the optimal result for 3DGS. To this end, a synergistic method is proposed during training and rendering under sparse inputs. The proposed method consists of two main components: Synergistic Transition and Synergistic Rendering. During training, we utilize multiple Gaussian fields to synergize their contributions and determine whether each Gaussian primitive has fallen into an ambiguous region. These regions impede the process for Gaussian primitives to discover alternative positions. This work extends Stochastic Gradient Langevin Dynamic updating and proposes a reformulated version of it. With this reformulation, the Gaussian primitives stuck in ambiguous regions adjust their positions, enabling them to explore an alternative solution. Furthermore, a Synergistic Rendering strategy is implemented during the rendering process. With Gaussian fields trained in the first stage, this approach synergizes the parallel branches to improve the quality of the rendered outputs. With Synergistic Transition and Synergistic Rendering, our method achieves photo-realistic novel view synthesis results under sparse inputs. Extensive experiments demonstrate that our method outperforms previous methods across diverse datasets, including LLFF, Mip-NeRF360, and Blender.},
  archive      = {J_ICV},
  author       = {Changyue Shi and Chuxiao Yang and Xinyuan Hu and Yan Yang and Jiajun Ding and Min Tan},
  doi          = {10.1016/j.imavis.2025.105512},
  journal      = {Image and Vision Computing},
  month        = {5},
  pages        = {105512},
  shortjournal = {Image Vis. Comput.},
  title        = {MMGS: Multi-model synergistic gaussian splatting for sparse view synthesis},
  volume       = {158},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Two-stream transformer tracking with messengers. <em>ICV</em>, <em>158</em>, 105510. (<a href='https://doi.org/10.1016/j.imavis.2025.105510'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, one-stream trackers gradually surpass two-stream trackers and become popular due to their higher accuracy. However, they suffer from a substantial amount of computational redundancy and an increased inference latency. This paper combines the speed advantage of two-stream trackers with the accuracy advantage of one-stream trackers, and proposes a new two-stream Transformer tracker called MesTrack. The core designs of MesTrack lie in the messenger tokens and the message integration module. The messenger tokens obtain the target-specific information during the feature extraction stage of the template branch, while the message integration module integrates the target-specific information from the template branch into the search branch. To further improve accuracy, this paper proposes an adaptive label smoothing knowledge distillation training scheme. This scheme uses the weighted sum of the teacher model’s prediction and the ground truth as supervisory information to guide the training of the student model. The weighting coefficients, which are predicted by the student model, are used to maintain the useful complementary information from the teacher model while simultaneously correcting its erroneous predictions. Evaluation on multiple popular tracking datasets show that MesTrack achieves competitive results. On the LaSOT dataset, the MesTrack-B-384 version achieves a SUC (success rate) score of 73.8%, reaching the SOTA (state of the art) performance, at an inference speed of 69.2 FPS (frames per second). When deployed with TensorRT, the speed can be further improved to 122.6 FPS.},
  archive      = {J_ICV},
  author       = {Miaobo Qiu and Wenyang Luo and Tongfei Liu and Yanqin Jiang and Jiaming Yan and Wenjuan Li and Jin Gao and Weiming Hu and Stephen Maybank},
  doi          = {10.1016/j.imavis.2025.105510},
  journal      = {Image and Vision Computing},
  month        = {5},
  pages        = {105510},
  shortjournal = {Image Vis. Comput.},
  title        = {Two-stream transformer tracking with messengers},
  volume       = {158},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A systematic review of intermediate fusion in multimodal deep learning for biomedical applications. <em>ICV</em>, <em>158</em>, 105509. (<a href='https://doi.org/10.1016/j.imavis.2025.105509'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep learning has revolutionized biomedical research by providing sophisticated methods to handle complex, high-dimensional data. Multimodal deep learning (MDL) further enhances this capability by integrating diverse data types such as imaging, textual data, and genetic information, leading to more robust and accurate predictive models. In MDL, differently from early and late fusion methods, intermediate fusion stands out for its ability to effectively combine modality-specific features during the learning process. This systematic review comprehensively analyzes and formalizes current intermediate fusion methods in biomedical applications, highlighting their effectiveness in improving predictive performance and capturing complex inter-modal relationships. We investigate the techniques employed, the challenges faced, and potential future directions for advancing intermediate fusion methods. Additionally, we introduce a novel structured notation that standardizes intermediate fusion architectures, enhancing understanding and facilitating implementation across various domains. Our findings provide actionable insights and practical guidelines intended to support researchers, healthcare professionals, and the broader deep learning community in developing more sophisticated and insightful multimodal models. Through this review, we aim to provide a foundational framework for future research and practical applications in the dynamic field of MDL.},
  archive      = {J_ICV},
  author       = {Valerio Guarrasi and Fatih Aksu and Camillo Maria Caruso and Francesco Di Feola and Aurora Rofena and Filippo Ruffini and Paolo Soda},
  doi          = {10.1016/j.imavis.2025.105509},
  journal      = {Image and Vision Computing},
  month        = {5},
  pages        = {105509},
  shortjournal = {Image Vis. Comput.},
  title        = {A systematic review of intermediate fusion in multimodal deep learning for biomedical applications},
  volume       = {158},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Part-aware distillation and aggregation network for human parsing. <em>ICV</em>, <em>158</em>, 105504. (<a href='https://doi.org/10.1016/j.imavis.2025.105504'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The current state-of-the-art human parsing models achieve remarkable success in parsing accuracy. However, the huge model size and computational cost restrict their applications on low-latency online systems or resource-limited mobile devices. In this paper, we propose a novel part-aware distillation and aggregation network for human parsing, which can be applied to any human parsing model to achieve a good trade-off between accuracy and efficiency. We design the part key-point similarity distillation and the part distribution distillation to transfer the complex teacher model’s knowledge of part structural and spatial relationships to the lightweight student model, which can help the latter to better identify small parts and semantic boundaries, and to distinguish easily confused categories. Furthermore, the online model aggregation module is introduced in the later stages of training, which can mitigate noise from both the teacher and the labels to obtain smoother and more robust results. Extensive experiments and ablation studies on the large-scale popular human parsing datasets LIP, ATR and PASCAL-Person Part fully demonstrate that our method is accurate, lightweight and general.},
  archive      = {J_ICV},
  author       = {Yuntian Lai and Yuxin Feng and Fan Zhou and Zhuo Su},
  doi          = {10.1016/j.imavis.2025.105504},
  journal      = {Image and Vision Computing},
  month        = {5},
  pages        = {105504},
  shortjournal = {Image Vis. Comput.},
  title        = {Part-aware distillation and aggregation network for human parsing},
  volume       = {158},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). MrgaNet: Multi-scale recursive gated aggregation network for tracheoscopy images. <em>ICV</em>, <em>158</em>, 105503. (<a href='https://doi.org/10.1016/j.imavis.2025.105503'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Lung cancer is a potentially fatal disease worldwide, and improving the accuracy of diagnosis plays a key role in enhancing patient outcomes. In this study, we extended computer-aided work to the task of assisting tracheoscopy in predicting lung cancer subtypes. To solve the problem of information fusion in different spatial scales and channels, we proposed MrgaNet. The network enhances classification performance by expanding interactions from low to high orders, dynamically adjusting feature weights, and incorporating a channel competition operator for efficient feature selection. Our network achieved a precision of 0.87 in the endobronchial dataset. In addition, the accuracy of 89.25% and 96.76% was achieved in the Kvasir-v2 dataset and the Kvasir-Capsule dataset, respectively. The results demonstrate that MrgaNet achieves superior performance compared to existing excellent methods.},
  archive      = {J_ICV},
  author       = {Ying Wang and Yun Tie and Dalong Zhang and Fenghui Liu and Lin Qi},
  doi          = {10.1016/j.imavis.2025.105503},
  journal      = {Image and Vision Computing},
  month        = {5},
  pages        = {105503},
  shortjournal = {Image Vis. Comput.},
  title        = {MrgaNet: Multi-scale recursive gated aggregation network for tracheoscopy images},
  volume       = {158},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Rethinking active domain adaptation: Balancing uncertainty and diversity. <em>ICV</em>, <em>158</em>, 105492. (<a href='https://doi.org/10.1016/j.imavis.2025.105492'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In applications of machine learning, usually the test data domain distributes inconsistently with the model training data, implying they are not independent and identically distributed. To address this challenge with certain annotation knowledge, the paradigm of Active Domain Adaptation (ADA) has been proposed through selectively labeling some target instances to facilitate cross-domain alignment with minimal annotation cost. However, existing ADA methods often struggle to balance uncertainty and diversity in sample selection, limiting their effectiveness. To address this, we propose a novel ADA framework: Balancing Uncertainty and Diversity (ADA-BUD), which desirably achieves ADA while balancing the data uncertainty and diversity across domains. Specifically, in ADA-BUD, the Uncertainty Range Perception (URA) module is specially designed to distinguish these most informative but uncertain target instances for annotation while appraising not only each instance itself but also their neighbors. Subsequently, the module called Representative Energy Optimization (REO) is constructed to refine diversity of the resulting annotation instances set. Last but not least, to enhance the flexibility of ADA-BUD in handling scenarios with limited data, we further build the Dynamic Sample Enhancement (DSE) module in ADA-BUD to generate class-balanced label-confident data augmentation. Experiments show ADA-BUD outperforms existing methods on challenging benchmarks, demonstrating its practical potential.},
  archive      = {J_ICV},
  author       = {Qing Tian and Yanzhi Li and Jiangsen Yu and Junyu Shen and Weihua Ou},
  doi          = {10.1016/j.imavis.2025.105492},
  journal      = {Image and Vision Computing},
  month        = {5},
  pages        = {105492},
  shortjournal = {Image Vis. Comput.},
  title        = {Rethinking active domain adaptation: Balancing uncertainty and diversity},
  volume       = {158},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Enhancing trust in large language models for streamlined decision-making in military operations. <em>ICV</em>, <em>158</em>, 105489. (<a href='https://doi.org/10.1016/j.imavis.2025.105489'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Large Language Models (LLMs) have the potential to enhance decision-making significantly in core military operational contexts that support training, readiness, and mission execution under low-risk conditions. Still, their implementation must be approached carefully, considering the associated risks. This paper examines the integration of LLMs into military decision-making, emphasizing the LLM’s ability to improve intelligence analysis, enhance situational awareness, support strategic planning, predict threats, optimize logistics, and strengthen cybersecurity. The paper also considers misinterpretation, bias, misinformation, or overreliance on AI-generated suggestions, potentially leading to errors in routine but critical decision-making processes. Our work concludes by proposing solutions and promoting the responsible implementation of LLMs to ensure their effective and ethical use in military operations. To build trust in LLMs, this paper advocates for developing cybersecurity frameworks, transparency, and ethical oversight. It further suggests using machine unlearning (MU) to selectively remove outdated or compromised data from LLM training datasets, preserving the integrity of the insights they generate. The paper underscores the imperative for integrating LLMs in low-risk military contexts, coupled with sustained research efforts to mitigate potential hazards.},
  archive      = {J_ICV},
  author       = {Emanuela Marasco and Thirimachos Bourlai},
  doi          = {10.1016/j.imavis.2025.105489},
  journal      = {Image and Vision Computing},
  month        = {5},
  pages        = {105489},
  shortjournal = {Image Vis. Comput.},
  title        = {Enhancing trust in large language models for streamlined decision-making in military operations},
  volume       = {158},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Spatio-temporal information mining and fusion feature-guided modal alignment for video-based visible-infrared person re-identification. <em>ICV</em>, <em>157</em>, 105518. (<a href='https://doi.org/10.1016/j.imavis.2025.105518'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The video-based visible-infrared person re-identification (Re-ID) aims to recognize the same person across modalities through video sequences. The core challenges of this task lie in narrowing the modal differences and deeply mining the rich spatio-temporal information contained in video to enhance model performance. However, existing research primarily focuses on addressing the modality gap, with insufficient utilization of the spatio-temporal information in video sequences. To address this, this paper proposes a novel spatio-temporal information mining and fusion feature-guided modal alignment framework for video-based visible-infrared person Re-ID. Specifically, we propose a spatio-temporal information mining method. This method employs the proposed feature correlation mechanism to enhance the discriminative features of person across different frames, while utilizing a temporal Transformer to mine person motion features. The advantage of this method lies in its ability to alleviate issues such as occlusion and frame misalignment, improving the discriminability of person features. Additionally, we introduce a fusion modality-guided modal alignment strategy, which reduces modality differences between infrared and visible video frames by aligning single-modality features with fusion features. The advantage of this strategy is that each modality not only learns its specific features but also absorbs person information from the other modality, thereby alleviating modality differences and further enhancing the discriminability of person features. Extensive comparative and ablation experiments conducted on the HITSZ-VCM and BUPTCampus datasets confirm the effectiveness and superiority of the proposed framework. The source code is available at https://github.com/lhf12278/SIMFGA .},
  archive      = {J_ICV},
  author       = {Zhigang Zuo and Huafeng Li and Yafei Zhang and Minghong Xie},
  doi          = {10.1016/j.imavis.2025.105518},
  journal      = {Image and Vision Computing},
  month        = {5},
  pages        = {105518},
  shortjournal = {Image Vis. Comput.},
  title        = {Spatio-temporal information mining and fusion feature-guided modal alignment for video-based visible-infrared person re-identification},
  volume       = {157},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Stealth sight: A multi perspective approach for camouflaged object detection. <em>ICV</em>, <em>157</em>, 105517. (<a href='https://doi.org/10.1016/j.imavis.2025.105517'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Camouflaged object detection (COD) is a challenging task due to the inherent similarity between objects and their surroundings. This paper introduces Stealth Sight , a novel framework integrating multi-view feature fusion and depth-based refinement to enhance segmentation accuracy. Our approach incorporates a pretrained multi-view CLIP encoder and a depth extraction network, facilitating robust feature representation. Additionally, we introduce a cross-attention transformer decoder and a post-training pruning mechanism to improve efficiency. Extensive evaluations on benchmark datasets demonstrate that Stealth Sight outperforms state-of-the-art methods in camouflaged object segmentation. Our method significantly enhances detection in complex environments, making it applicable to medical imaging, security, and wildlife monitoring.},
  archive      = {J_ICV},
  author       = {Domnic S. and Jayanthan K.S.},
  doi          = {10.1016/j.imavis.2025.105517},
  journal      = {Image and Vision Computing},
  month        = {5},
  pages        = {105517},
  shortjournal = {Image Vis. Comput.},
  title        = {Stealth sight: A multi perspective approach for camouflaged object detection},
  volume       = {157},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). MFKD: Multi-dimensional feature alignment for knowledge distillation. <em>ICV</em>, <em>157</em>, 105514. (<a href='https://doi.org/10.1016/j.imavis.2025.105514'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Knowledge distillation is a popular technique for compressing and transferring models in the field of deep learning. However, existing distillation methods often focus on optimizing a single dimension and overlook the importance of aligning and transforming knowledge across multiple dimensions, leading to suboptimal results. In this article, we introduce a novel approach called multi-dimensional feature alignment for knowledge distillation (MFKD) to address this limitation. The MFKD framework is built on the observation that knowledge from different dimensions can complement each other effectively. We extract knowledge from features in the spatcial, sample and channel dimensions separately. Our spatial-level part separates the foreground and background information, guiding the student to focus on crucial image regions by mimicking the teacher’s spatial and channel attention maps. Our sample-level part distills knowledge encoded in semantic correlations between sample activations by aligning the student’s activations to emulate the teacher’s clustering patterns using the Spearman correlation coefficient. Furthermore, our channel-level part encourages the student to learn standardized feature representations aligned with the teacher’s channel-wise interdependencies. Finally, we dynamically balance the loss factors of the different dimensions to optimize the overall performance of the distillation process. To validate the effectiveness of our methodology, we conduct experiments on benchmark datasets such as CIFAR-100, ImageNet and COCO. The experimental results demonstrate substantial performance improvements compared to baseline and recent state-of-the-art methods, confirming the efficacy of our MFKD framework. Furthermore, we provide a comprehensive analysis of the experimental results, offering deeper insight into the benefits and effectiveness of our approach. Through this analysis, we reinforce the significance of aligning and leveraging knowledge across multiple dimensions in knowledge distillation.},
  archive      = {J_ICV},
  author       = {Zhen Guo and Pengzhou Zhang and Peng Liang},
  doi          = {10.1016/j.imavis.2025.105514},
  journal      = {Image and Vision Computing},
  month        = {5},
  pages        = {105514},
  shortjournal = {Image Vis. Comput.},
  title        = {MFKD: Multi-dimensional feature alignment for knowledge distillation},
  volume       = {157},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Fusing grid and adaptive region features for image captioning. <em>ICV</em>, <em>157</em>, 105513. (<a href='https://doi.org/10.1016/j.imavis.2025.105513'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Image captioning aims to automatically generate grammatically correct and reasonable description sentences for given images. Improving feature optimization and processing is crucial for enhancing performance in this task. A common approach is to leverage the complementary advantages of grid features and region features. However, incorporating region features in most current methods may lead to incorrect guidance during training, along with high acquisition costs and the requirement of pre-caching. These factors impact the effectiveness and practical application of image captioning. To address these limitations, this paper proposes a method called fusing grid and adaptive region features for image captioning (FGAR). FGAR dynamically explores pseudo-region information within a given image based on the extracted grid features. Subsequently, it utilizes a combination of computational layers with varying permissions to fuse features, enabling comprehensive interaction between information from different modalities while preserving the unique characteristics of each modality. The resulting enhanced visual features provide improved support to the decoder for autoregressively generating sentences describing the content of a given image. All processes are integrated within a fully end-to-end framework, facilitating both training and inference processes while achieving satisfactory performance. Extensive experiments validate the effectiveness of the proposed FGAR method.},
  archive      = {J_ICV},
  author       = {Jiahui Wei and Zhixin Li and Canlong Zhang and Huifang Ma},
  doi          = {10.1016/j.imavis.2025.105513},
  journal      = {Image and Vision Computing},
  month        = {5},
  pages        = {105513},
  shortjournal = {Image Vis. Comput.},
  title        = {Fusing grid and adaptive region features for image captioning},
  volume       = {157},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Attention head purification: A new perspective to harness CLIP for domain generalization. <em>ICV</em>, <em>157</em>, 105511. (<a href='https://doi.org/10.1016/j.imavis.2025.105511'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Domain Generalization (DG) aims to learn a model from multiple source domains to achieve satisfactory performance on unseen target domains. Recent works introduce CLIP to DG tasks due to its superior image-text alignment and zeros-shot performance. Previous methods either utilize full fine-tuning or prompt-learning paradigms to harness CLIP for DG tasks. Those works focus on avoiding catastrophic forgetting of the original knowledge encoded in CLIP but ignore that the knowledge encoded in CLIP in nature may contain domain-specific cues that constrain its domain generalization performance. In this paper, we propose a new perspective to harness CLIP for DG, i.e., attention head purification. We observe that different attention heads may encode different properties of an image and selecting heads appropriately may yield remarkable performance improvement across domains. Based on such observations, we purify the attention heads of CLIP from two levels, including task-level purification and domain-level purification . For task-level purification, we design head-aware LoRA to make each head more adapted to the task we considered. For domain-level purification, we perform head selection via a simple gating strategy. We utilize MMD loss to encourage masked head features to be more domain-invariant to emphasize more generalizable properties/heads. During training, we jointly perform task-level purification and domain-level purification. We conduct experiments on various representative DG benchmarks. Though simple, extensive experiments demonstrate that our method performs favorably against previous state-of-the-arts.},
  archive      = {J_ICV},
  author       = {Yingfan Wang and Guoliang Kang},
  doi          = {10.1016/j.imavis.2025.105511},
  journal      = {Image and Vision Computing},
  month        = {5},
  pages        = {105511},
  shortjournal = {Image Vis. Comput.},
  title        = {Attention head purification: A new perspective to harness CLIP for domain generalization},
  volume       = {157},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). DDMCB: Open-world object detection empowered by denoising diffusion models and calibration balance. <em>ICV</em>, <em>157</em>, 105508. (<a href='https://doi.org/10.1016/j.imavis.2025.105508'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Open-world object detection (OWOD) differs from traditional object detection by being more suited to real-world, dynamic scenarios. It aims to recognize unseen objects and have the skill to learn incrementally based on newly introduced knowledge. However, the current OWOD usually relies on supervising of known objects in identifying unknown objects, using high objectness scores as critical indicators of potential unknown objects. While these methods can detect unknown objects with features similar to known objects, they also classify regions dissimilar to known objects as background, leading to label bias issues. To address this problem, we leverage the knowledge from large visual models to provide auxiliary supervision for unknown objects. Additionally, we apply the Denoising Diffusion Probabilistic Model (DDPM) in OWOD scenarios. We propose an unsupervised modeling approach based on DDPM, which significantly improves the accuracy of unknown object detection. Despite this, the classifier trained during the model training process only encounters known classes, resulting in higher confidence for known classes during inference; thus, bias issues again occur. Therefore, we propose a probability calibration technique for post-processing predictions during inference. The calibration aims to reduce the probabilities of known objects and increase the probabilities of unknown objects, thereby balancing the final probability predictions. Our experiments demonstrate that the proposed method achieves significant improvements on OWOD benchmarks, with an unknown objects detection recall rate of 54.7 U-Recall , surpassing the current state-of-the-art (SOTA) methods by 44.3% . In terms of real-time performance, Our model uses a few parameters, and pure convolutional neural networks instead of intensive attention mechanisms, achieving an inference speed of 35.04 FPS , exceeding the SOTA OWOD methods based on Faster R-CNN and Deformable DETR by 2.79 and 10.95 FPS , respectively.},
  archive      = {J_ICV},
  author       = {Yangyang Huang and Xing Xi and Ronghua Luo},
  doi          = {10.1016/j.imavis.2025.105508},
  journal      = {Image and Vision Computing},
  month        = {5},
  pages        = {105508},
  shortjournal = {Image Vis. Comput.},
  title        = {DDMCB: Open-world object detection empowered by denoising diffusion models and calibration balance},
  volume       = {157},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Self-supervised monocular depth learning from unknown cameras: Leveraging the power of raw data. <em>ICV</em>, <em>157</em>, 105505. (<a href='https://doi.org/10.1016/j.imavis.2025.105505'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Self-supervised monocular depth estimation from wild videos with unknown camera intrinsics is a practical and challenging task in computer vision. Most of the existing methods in literature employed a camera decoder and a pose decoder to estimate camera intrinsics and poses respectively, however, their performances would be degraded significantly in many complex scenarios with severe noise and large camera rotations. To address this problem, we propose a novel self-supervised monocular depth estimation method, which could be trained from wild videos with a joint optimization strategy for simultaneously estimating camera intrinsics and poses. In the proposed method, a depth encoder is employed to learn scene depth features, and then by taking these features as inputs, a Neighborhood Influence Module (NIM) is designed for predicting each pixel’s depth by fusing the depths of its neighboring pixels, which could explicitly enforce the depth accuracy. In addition, a knowledge distillation mechanism is introduced to learn a lightweight depth encoder from a large-scale depth encoder, for achieving a balance between computational speed and accuracy. Experimental results on four public datasets demonstrate that the proposed method outperforms some state-of-the-art methods in most cases. Moreover, once the proposed method is trained with a mixed set of different datasets, its performance would be further boosted in comparison to the proposed method trained with each involved single dataset. Codes are available at: https://github.com/ZhuYongChaoUSST/IntrLessMonoDepth .},
  archive      = {J_ICV},
  author       = {Xiaofei Qin and Yongchao Zhu and Lin Wang and Xuedian Zhang and Changxiang He and Qiulei Dong},
  doi          = {10.1016/j.imavis.2025.105505},
  journal      = {Image and Vision Computing},
  month        = {5},
  pages        = {105505},
  shortjournal = {Image Vis. Comput.},
  title        = {Self-supervised monocular depth learning from unknown cameras: Leveraging the power of raw data},
  volume       = {157},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Advanced deep learning and large language models: Comprehensive insights for cancer detection. <em>ICV</em>, <em>157</em>, 105495. (<a href='https://doi.org/10.1016/j.imavis.2025.105495'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, the rapid advancement of machine learning (ML), particularly deep learning (DL), has revolutionized various fields, with healthcare being one of the most notable beneficiaries. DL has demonstrated exceptional capabilities in addressing complex medical challenges, including the early detection and diagnosis of cancer. Its superior performance, surpassing both traditional ML methods and human accuracy, has made it a critical tool in identifying and diagnosing diseases such as cancer. Despite the availability of numerous reviews on DL applications in healthcare, a comprehensive and detailed understanding of DL’s role in cancer detection remains lacking. Most existing studies focus on specific aspects of DL, leaving significant gaps in the broader knowledge base. This paper aims to bridge these gaps by offering a thorough review of advanced DL techniques, namely transfer learning (TL), reinforcement learning (RL), federated learning (FL), Transformers, and large language models (LLMs). These cutting-edge approaches are pushing the boundaries of cancer detection by enhancing model accuracy, addressing data scarcity, and enabling decentralized learning across institutions while maintaining data privacy. TL enables the adaptation of pre-trained models to new cancer datasets, significantly improving performance with limited labeled data. RL is emerging as a promising method for optimizing diagnostic pathways and treatment strategies, while FL ensures collaborative model development without sharing sensitive patient data. Furthermore, Transformers and LLMs, traditionally utilized in natural language processing (NLP), are now being applied to medical data for enhanced interpretability and context-based predictions. In addition, this review explores the efficiency of the aforementioned techniques in cancer diagnosis, it addresses key challenges such as data imbalance, and proposes potential solutions. It aims to be a valuable resource for researchers and practitioners, offering insights into current trends and guiding future research in the application of advanced DL techniques for cancer detection.},
  archive      = {J_ICV},
  author       = {Yassine Habchi and Hamza Kheddar and Yassine Himeur and Adel Belouchrani and Erchin Serpedin and Fouad Khelifi and Muhammad E.H. Chowdhury},
  doi          = {10.1016/j.imavis.2025.105495},
  journal      = {Image and Vision Computing},
  month        = {5},
  pages        = {105495},
  shortjournal = {Image Vis. Comput.},
  title        = {Advanced deep learning and large language models: Comprehensive insights for cancer detection},
  volume       = {157},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Rif-diff: Improving image fusion based on diffusion model via residual prediction. <em>ICV</em>, <em>157</em>, 105494. (<a href='https://doi.org/10.1016/j.imavis.2025.105494'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper proposes an image fusion framework Rif-Diff, which adopts several strategies and approaches to improve current fusion methods based on diffusion model. Rif-Diff employs residual images as the generation target of the diffusion model to optimize the model’s convergence process and enhance the fusion performance. For fusion tasks lacking ground truth, image fusion prior is utilized to facilitate the production of residual images. Simultaneously, to overcome the limitations of the model’s learning capacity imposed by training with image fusion prior, Rif-Diff introduces the idea of image restoration to enable the initial fused images to incorporate more expected information. Additionally, a dual-step decision module is designed to address the blurriness issue of fused images in existing multi-focus image fusion methods that do not rely on decision maps. Extensive experiments demonstrate the effectiveness of Rif-Diff across multiple fusion tasks including multi-focus image fusion, multi-exposure image fusion, and infrared-visible image fusion. The code is available at: https://github.com/peixuanWu/Rif-Diff .},
  archive      = {J_ICV},
  author       = {Peixuan Wu and Shen Yang and Jin Wu and Qian Li},
  doi          = {10.1016/j.imavis.2025.105494},
  journal      = {Image and Vision Computing},
  month        = {5},
  pages        = {105494},
  shortjournal = {Image Vis. Comput.},
  title        = {Rif-diff: Improving image fusion based on diffusion model via residual prediction},
  volume       = {157},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Strengthening incomplete multi-view clustering: An attention contrastive learning method. <em>ICV</em>, <em>157</em>, 105493. (<a href='https://doi.org/10.1016/j.imavis.2025.105493'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Incomplete multi-view clustering presents greater challenges than traditional multi-view clustering. In recent years, significant progress has been made in this field, multi-view clustering relies on the consistency and integrity of views to ensure the accurate transmission of data information. However, during the process of data collection and transmission, data loss is inevitable, leading to partial view loss and increasing the difficulty of joint learning on incomplete multi-view data. To address this issue, we propose a multi-view contrastive learning framework based on the attention mechanism. Previous contrastive learning mainly focused on the relationships between isolated sample pairs, which limited the robustness of the method. Our method selects positive samples from both global and local perspectives by utilizing the nearest neighbor graph to maximize the correlation between local features and latent features of each view. Additionally, we use a cross-view encoder network with self-attention structure to fuse the low dimensional representations of each view into a joint representation, and guide the learning of the joint representation through a high confidence structure. Furthermore, we introduce graph constraint learning to explore potential neighbor relationships among instances to facilitate data reconstruction. The experimental results on six multi-view datasets demonstrate that our method exhibits significant effectiveness and superiority compared to existing methods.},
  archive      = {J_ICV},
  author       = {Shudong Hou and Lanlan Guo and Xu Wei},
  doi          = {10.1016/j.imavis.2025.105493},
  journal      = {Image and Vision Computing},
  month        = {5},
  pages        = {105493},
  shortjournal = {Image Vis. Comput.},
  title        = {Strengthening incomplete multi-view clustering: An attention contrastive learning method},
  volume       = {157},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Early progression detection from MCI to AD using multi-view MRI for enhanced assisted living. <em>ICV</em>, <em>157</em>, 105491. (<a href='https://doi.org/10.1016/j.imavis.2025.105491'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Alzheimer's disease (AD) is a progressive neurodegenerative disorder. Early detection is crucial for timely intervention and treatment to improve assisted living. Although magnetic resonance imaging (MRI) is a widely used neuroimaging modality for the diagnosis of AD, most studies focus on a single MRI plane, missing comprehensive spatial information. In this study, we proposed a novel approach that leverages multiple MRI planes (axial, coronal, and sagittal) from 3D MRI volumes to predict progression from stable mild cognitive impairment (sMCI) to progressive MCI (pMCI) and AD. We employed a list of convolutional neural networks, including EfficientNet-B7, ConvNext, and DenseNet-121, to extract deep features from each MRI plane, followed by a feature enhancement step through an attention module. The optimized feature set was then passed through a Bayesian-optimized pool of classification heads (i.e., multilayer perceptron (MLP), long short-term memory (LSTM), and multi-head attention (MHA)) to obtain the most effective model for each MRI plane. The optimal model for each MRI plane was then integrated into homogeneous and heterogeneous ensembles to further enhance the performance of the model. Using the ADNI dataset, the proposed model achieved 91% accuracy, 87% sensitivity, 88% specificity, and 92% AUC. To enhance the interpretability of the model, we used the Grad-CAM explainability technique to generate attention maps for each MRI plane, which identified critical brain regions affected by disease progression. These attention maps revealed consistent patterns of tissue damage across the MRI scans. The results demonstrate the effectiveness of combining multiplane MRI data with ensemble learning and attention mechanisms to improve the early detection and tracking of AD progression in patients with MCI, offering a more comprehensive diagnostic tool and enhanced clinical decision-making. The datasets, results, and code used to conduct the comprehensive analysis are made available to the research community through the following link: https://github.com/nasir3843/Early_Progression_detection_MCI-to_AD},
  archive      = {J_ICV},
  author       = {Nasir Rahim and Naveed Ahmad and Waseem Ullah and Jatin Bedi and Younhyun Jung},
  doi          = {10.1016/j.imavis.2025.105491},
  journal      = {Image and Vision Computing},
  month        = {5},
  pages        = {105491},
  shortjournal = {Image Vis. Comput.},
  title        = {Early progression detection from MCI to AD using multi-view MRI for enhanced assisted living},
  volume       = {157},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multi-modal few-shot image recognition with enhanced semantic and visual integration. <em>ICV</em>, <em>157</em>, 105490. (<a href='https://doi.org/10.1016/j.imavis.2025.105490'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Few-Shot Learning (FSL) enables models to recognize new classes with only a few examples by leveraging knowledge from known classes. Although some methods incorporate class names as prior knowledge, effectively integrating visual and semantic information remains challenging. Additionally, conventional similarity measurement techniques often result in information loss, obscure distinctions between samples, and fail to capture intra-sample diversity. To address these issues, this paper presents a Multi-modal Few-shot Image Recognition (MFSIR) approach. We first introduce the Multi-Scale Interaction Module (MSIM), which facilitates multi-scale interactions between semantic and visual features, significantly enhancing the representation of visual features. We also propose the Hybrid Similarity Measurement Module (HSMM), which integrates information from multiple dimensions to evaluate the similarity between samples by dynamically adjusting the weights of various similarity measurement methods, thereby improving the accuracy and robustness of similarity assessments. Experimental results demonstrate that our approach significantly outperforms existing methods on four FSL benchmarks, with marked improvements in FSL accuracy under 1-shot and 5-shot scenarios.},
  archive      = {J_ICV},
  author       = {Chunru Dong and Lizhen Wang and Feng Zhang and Qiang Hua},
  doi          = {10.1016/j.imavis.2025.105490},
  journal      = {Image and Vision Computing},
  month        = {5},
  pages        = {105490},
  shortjournal = {Image Vis. Comput.},
  title        = {Multi-modal few-shot image recognition with enhanced semantic and visual integration},
  volume       = {157},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Object tracking based on temporal and spatial context information. <em>ICV</em>, <em>157</em>, 105488. (<a href='https://doi.org/10.1016/j.imavis.2025.105488'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Currently, numerous advanced trackers improve stability by optimizing the target visual appearance models or by improving interactions between templates and search areas. Despite these advancements, appearance-based trackers still primarily depend on the visual information of targets without adequately integrating spatio-temporal context information, thus limiting their effectiveness in handling similar objects around the target. To address this challenge, a novel object tracking method, TSCTrack, which leverages spatio-temporal context information, has been introduced. TSCTrack overcomes the shortcomings of traditional center-cropping preprocessing techniques by introducing Global Spatial Position Embedding, effectively preserving spatial information and capturing motion data of targets. Additionally, TSCTrack incorporates a Spatial Relationship Aggregation module and a Temporal Relationship Aggregation module—the former captures static spatial context information per frame, while the latter integrates dynamic temporal context information. This sophisticated integration allows the Dynamic Tracking Prediction module to generate precise target coordinates effectively, greatly reducing the impact of target deformations and scale changes on tracking performance. Demonstrated across multiple public tracking datasets including LaSOT, TrackingNet, UAV123, GOT-10k, and OTB, TSCTrack showcases superior performance and validates its exceptional tracking capabilities in diverse scenarios.},
  archive      = {J_ICV},
  author       = {Yan Chen and Tao Lin and Jixiang Du and Hongbo Zhang},
  doi          = {10.1016/j.imavis.2025.105488},
  journal      = {Image and Vision Computing},
  month        = {5},
  pages        = {105488},
  shortjournal = {Image Vis. Comput.},
  title        = {Object tracking based on temporal and spatial context information},
  volume       = {157},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). An edge-aware high-resolution framework for camouflaged object detection. <em>ICV</em>, <em>157</em>, 105487. (<a href='https://doi.org/10.1016/j.imavis.2025.105487'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Camouflaged objects are often seamlessly assimilated into their surroundings and exhibit indistinct boundaries. The complex environmental conditions and the high intrinsic similarity between camouflaged targets and their backgrounds present significant challenges in accurately locating and fully segmenting these objects. Although existing methods have achieved remarkable performance across various real-world scenarios, they still struggle with challenging cases such as small targets, thin structures, and blurred boundaries. To address these issues, we propose a novel edge-aware high-resolution network. Specifically, we design a High-Resolution Feature Enhancement Module to exploit multi-scale features while preserving local details. Furthermore, we introduce an Edge Prediction Module to generate high-quality edge prediction maps. Subsequently, we develop an Attention-Guided Fusion Module to effectively leverage the edge prediction maps. With these key modules, the proposed model achieves real-time performance at 58 FPS and surpasses 21 state-of-the-art algorithms across six standard evaluation metrics. Source code will be publicly available at https://github.com/clelouch/EHNet .},
  archive      = {J_ICV},
  author       = {Jingyuan Ma and Tianyou Chen and Jin Xiao and Xiaoguang Hu and Yingxun Wang},
  doi          = {10.1016/j.imavis.2025.105487},
  journal      = {Image and Vision Computing},
  month        = {5},
  pages        = {105487},
  shortjournal = {Image Vis. Comput.},
  title        = {An edge-aware high-resolution framework for camouflaged object detection},
  volume       = {157},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Adaptive scale matching for remote sensing object detection based on aerial images. <em>ICV</em>, <em>157</em>, 105482. (<a href='https://doi.org/10.1016/j.imavis.2025.105482'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Remote sensing object detection based on aerial images presents challenges due to their complex backgrounds, and the utilization of specific a contextual information can enhance detection accuracy. Inadequate long-range background information may lead to erroneous detection of small remotely sensed objects, with variations in background complexity observed across different object types. In this paper, we propose a new YOLO -based real-time object detector. The detector aims to S cale- M atch the proportions of various objects in remote sensing images using the model named YOLO-SM . Specifically, this paper proposes a straightforward yet highly efficient building block that dynamically adjusts the necessary receptive field for each object, minimizing the loss of feature information caused by consecutive convolutions. Additionally, a supplementary bottom-up pathway is incorporated to improve the representation of smaller objects. Empirical evaluations conducted on DOTA-v1.0, DOTA-v1.5, DIOR-R, and HRSC2016 datasets confirm the efficacy of the proposed methodology. On DOTA-v1.0, compared to RTMDet-R-L, YOLO-SM-S achieved competitive accuracy while significantly reducing parameters by 74.8% and FLOPs by 78.5%. Compared to LSKNet on HRSC2016, YOLO-SM-Tiny dramatically reduces 76% of parameters and 90% of FLOPs and improves FPS by about three times while maintaining stable accuracy.},
  archive      = {J_ICV},
  author       = {Lu Han and Nan Li and Zeyuan Zhong and Dong Niu and Bingbing Gao},
  doi          = {10.1016/j.imavis.2025.105482},
  journal      = {Image and Vision Computing},
  month        = {5},
  pages        = {105482},
  shortjournal = {Image Vis. Comput.},
  title        = {Adaptive scale matching for remote sensing object detection based on aerial images},
  volume       = {157},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Video wire inpainting via hierarchical feature mixture. <em>ICV</em>, <em>157</em>, 105460. (<a href='https://doi.org/10.1016/j.imavis.2025.105460'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Video wire inpainting aims at automatically eliminating visible wires from film footage, significantly streamlining post-production workflows. Previous models address redundancy in wire removal by eliminating redundant blocks to enhance focus on crucial wire details for more accurate reconstruction. However, once redundancy is removed, the disorganized non-redundant blocks disrupt temporal and spatial coherence, making seamless inpainting challenging. The absence of multi-scale feature fusion further limits the model’s ability to handle different wire scales and blend inpainted regions with complex backgrounds. To address these challenges, we propose a Hierarchical Feature Mixture Network (HFM-Net) that integrates two novel modules: a Hierarchical Transformer Module (HTM) and a Spatio-temporal Feature Mixture Module (SFM). Specifically, the HTM employs redundancy-aware attention modules and lightweight transformers to reorganize and fuse key high- and low-dimensional patches. The lightweight transformers are sufficient due to the reduced number of non-redundant blocks processing. By aggregating similar features, these transformers guide the alignment of non-redundant blocks and achieve effective spatio-temporal synchronization. Building on this, the SFM incorporates gated convolutions and GRU to enhance spatial and temporal integration further. Gated convolutions fuse low- and high-dimensional features, while the GRU captures temporal dependencies, enabling seamless inpainting of dynamic wire patterns. Additionally, we introduce a lightweight 3D separable convolution discriminator to improve video quality during the inpainting process while reducing computational costs. Experimental results demonstrate that HFM-Net achieves state-of-the-art performance on the video wire removal task.},
  archive      = {J_ICV},
  author       = {Zhong Ji and Yimu Su and Yan Zhang and Shuangming Yang and Yanwei Pang},
  doi          = {10.1016/j.imavis.2025.105460},
  journal      = {Image and Vision Computing},
  month        = {5},
  pages        = {105460},
  shortjournal = {Image Vis. Comput.},
  title        = {Video wire inpainting via hierarchical feature mixture},
  volume       = {157},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). MUNet: A lightweight mamba-based under-display camera restoration network. <em>ICV</em>, <em>156</em>, 105486. (<a href='https://doi.org/10.1016/j.imavis.2025.105486'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Under-Display Camera (UDC) restoration aims to recover the underlying clean images from the degraded images captured by UDC. Although promising results have been achieved, most existing UDC restoration methods still suffer from two vital obstacles in practice: (i) existing UDC restoration models are parameter-intensive, and (ii) most of them struggle to capture long-range dependencies within high-resolution images. To overcome above drawbacks, we study a challenging problem in UDC restoration, namely, how to design a lightweight UDC restoration model that could capture long-range image dependencies. To this end, we propose a novel lightweight Mamba-based UDC restoration network (MUNet) consisting of two modules, named Separate Multi-scale Mamba (SMM) and Separate Convolutional Feature Extractor (SCFE). Specifically, SMM exploits our proposed alternate scanning strategy to efficiently capture long-range dependencies across multi-scale image features. SCFE preserves local dependencies through convolutions with various receptive fields. Thanks to SMM and SCFE, MUNet achieves state-of-the-art lightweight UDC restoration performance with significantly fewer parameters, making it well-suited for deployment on mobile devices. Our codes will be available after acceptance.},
  archive      = {J_ICV},
  author       = {Wenxin Wang and Boyun Li and Wanli Liu and Xi Peng and Yuanbiao Gou},
  doi          = {10.1016/j.imavis.2025.105486},
  journal      = {Image and Vision Computing},
  month        = {4},
  pages        = {105486},
  shortjournal = {Image Vis. Comput.},
  title        = {MUNet: A lightweight mamba-based under-display camera restoration network},
  volume       = {156},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Dense small target detection algorithm for UAV aerial imagery. <em>ICV</em>, <em>156</em>, 105485. (<a href='https://doi.org/10.1016/j.imavis.2025.105485'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Unmanned aerial vehicle (UAV) aerial images make dense small target detection challenging due to the complex background, small object size in the wide field of view, low resolution, and dense target distribution. Many aerial target detection networks and attention-based methods have been proposed to enhance the capability of dense small target detection, but there are still problems, such as insufficient effective information extraction, missed detection, and false detection of small targets in dense areas. Therefore, this paper proposes a novel dense small target detection algorithm (DSTDA) for UAV aerial images suitable for various high-altitude complex environments. The core component of the proposed DSTDA consists of the multi-axis attention units, the adaptive feature transformation mechanism, and the target-guided sample allocation strategy. Firstly, by introducing the multi-axis attention units into DSTDA, the limitation of DSTDA on global information perception can be addressed. Thus, the detailed features and spatial relationships of small targets at long distances can be sufficiently extracted by our proposed algorithm. Secondly, an adaptive feature transformation mechanism is designed to flexibly adjust the feature map according to the characteristics of the target distribution, which enables the DSTDA to focus more on densely populated target areas. Lastly, a goal-oriented sample allocation strategy is presented, combining coarse screening based on positional information and fine screening guided by target prediction information. By employing this dynamic sample allocation from coarse to fine, the detection performance of small and dense targets in complex backgrounds is further improved. These above innovative improvements empower the DSTDA with enhanced global perception and target-focusing capabilities, effectively addressing the challenges of detecting dense small targets in complex aerial scenes. Experimental validation was conducted on three publicly available datasets: VisDrone, SIMD, and CARPK. The results showed that the proposed DSTDA outperforms other state-of-the-art algorithms in terms of comprehensive performance. The algorithm significantly improves the issues of false alarms and missed detection in drone-based target detection, showcasing remarkable accuracy and real-time performance. It proves to be proficient in the task of detecting dense small targets in drone scenarios.},
  archive      = {J_ICV},
  author       = {Sheng Lu and Yangming Guo and Jiang Long and Zun Liu and Zhuqing Wang and Ying Li},
  doi          = {10.1016/j.imavis.2025.105485},
  journal      = {Image and Vision Computing},
  month        = {4},
  pages        = {105485},
  shortjournal = {Image Vis. Comput.},
  title        = {Dense small target detection algorithm for UAV aerial imagery},
  volume       = {156},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Real-time localization and navigation method for autonomous vehicles based on multi-modal data fusion by integrating memory transformer and DDQN. <em>ICV</em>, <em>156</em>, 105484. (<a href='https://doi.org/10.1016/j.imavis.2025.105484'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the field of autonomous driving, real-time localization and navigation are the core technologies that ensure vehicle safety and precise operation. With advancements in sensor technology and computing power, multi-modal data fusion has become a key method for enhancing the environmental perception capabilities of autonomous vehicles. This study aims to explore a novel visual-language navigation technology to achieve precise navigation of autonomous cars in complex environments. By integrating information from radar, sonar, 5G networks, Wi-Fi, Bluetooth, and a 360-degree visual information collection device mounted on the vehicle's roof, the model fully exploits rich multi-source data. The model uses the Memory Transformer for efficient data encoding and a data fusion strategy with a self-attention network, ensuring a balance between feature integrity and algorithm real-time performance. Furthermore, the encoded data is input into a DDQN vehicle navigation algorithm based on an automatically growing environmental target knowledge graph and large-scale scene maps, enabling continuous learning and optimization in real-world environments. Comparative experiments show that the proposed model outperforms existing SOTA models, particularly in terms of macro-spatial reference from large-scale scene maps, background knowledge support from the automatically growing knowledge graph, and the experience-optimized navigation strategies of the DDQN algorithm. In the comparative experiments with the SOTA models, the proposed model achieved scores of 3.99, 0.65, 0.67, 0.65, 0.63, and 0.63 on the six metrics NE, SR, OSR, SPL, CLS, and DTW, respectively. All of these results significantly enhance the intelligent positioning and navigation capabilities of autonomous driving vehicles.},
  archive      = {J_ICV},
  author       = {Li Zha and Chen Gong and Kunfeng Lv},
  doi          = {10.1016/j.imavis.2025.105484},
  journal      = {Image and Vision Computing},
  month        = {4},
  pages        = {105484},
  shortjournal = {Image Vis. Comput.},
  title        = {Real-time localization and navigation method for autonomous vehicles based on multi-modal data fusion by integrating memory transformer and DDQN},
  volume       = {156},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A spatial-frequency domain multi-branch decoder method for real-time semantic segmentation. <em>ICV</em>, <em>156</em>, 105483. (<a href='https://doi.org/10.1016/j.imavis.2025.105483'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Semantic segmentation is crucial for the functionality of autonomous driving systems. However, most of the existing real-time semantic segmentation models focus on encoder design and underutilize spatial and frequency domain information in the decoder, limiting the segmentation accuracy of the model. To solve this problem, this paper proposes a multi-branch decoder network combining spatial domain and frequency domain to meet the real-time and accuracy requirements of the semantic segmentation task of road scenes for autonomous driving systems. Firstly, the network introduces a novel multi-scale dilated fusion block that gradually enlarges the receptive field through three consecutive dilated convolutions, and integrates features from different levels using skip connections. At the same time, a strategy of gradually reducing the number of channels is adopted to effectively remove redundant features. Secondly, we design three branches for the decoder. The global branch utilizes a lightweight Transformer architecture to extract global features and employs horizontal and vertical convolutions to achieve interaction among global features. The multi-scale branch combines dilated convolution and adaptive pooling to perform multi-scale feature extraction through fusion and post-processing. The wavelet transform feature converter maps spatial domain features into low-frequency and high-frequency components, which are then fused with global and multi-scale features to enhance the model representation. Finally, we conduct experiments on multiple datasets. The experimental results show that the proposed method best balances segmentation accuracy and inference speed.},
  archive      = {J_ICV},
  author       = {Liwei Deng and Boda Wu and Songyu Chen and Dongxue Li and Yanze Fang},
  doi          = {10.1016/j.imavis.2025.105483},
  journal      = {Image and Vision Computing},
  month        = {4},
  pages        = {105483},
  shortjournal = {Image Vis. Comput.},
  title        = {A spatial-frequency domain multi-branch decoder method for real-time semantic segmentation},
  volume       = {156},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). CMS-net: Edge-aware multimodal MRI feature fusion for brain tumor segmentation. <em>ICV</em>, <em>156</em>, 105481. (<a href='https://doi.org/10.1016/j.imavis.2025.105481'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the growing application of artificial intelligence in medical image processing, multimodal MRI brain tumor segmentation has become crucial for clinical diagnosis and treatment. Accurate segmentation relies heavily on the effective utilization of multimodal information. However, most existing methods primarily focus on global and local deep semantic features, often overlooking critical aspects such as edge information and cross-channel correlations. To address these limitations while retaining the strengths of existing methods, we propose a novel brain tumor segmentation approach: an edge-aware feature fusion model based on a dual-encoder architecture. CMS-Net is a novel brain tumor segmentation model that integrates edge-aware fusion, cross-channel interaction, and spatial state feature extraction to fully leverage multimodal information for improved segmentation accuracy. The architecture comprises two main components: an encoder and a decoder. The encoder utilizes both convolutional downsampling and Smart Swin Transformer downsampling, with the latter employing Shifted Spatial Multi-Head Self-Attention (SSW-MSA) to capture global features and enhance long-range dependencies. The decoder reconstructs the image via the CMS-Block, which consists of three key modules: the Multi-Scale Deep Convolutional Cross-Channel Attention module (MDTA), the Spatial State Module (SSM), and the Boundary-Aware Feature Fusion module (SWA). CMS-Net's dual-encoder architecture allows for deep extraction of both local and global features, enhancing segmentation performance. MDTA generates attention maps through cross-channel covariance, while SSM models spatial context to improve the understanding of complex structures. The SWA module, combining SSW-MSA with pooling, subtraction, and convolution, facilitates feature fusion and edge extraction. Dice and Focal loss functions were introduced to optimize cross-channel and spatial feature extraction. Experimental results on the BraTS2018, BraTS2019, and BraTS2020 datasets demonstrate that CMS-Net effectively integrates spatial state, cross-channel, and boundary information, significantly improving multimodal brain tumor segmentation accuracy.},
  archive      = {J_ICV},
  author       = {Chunjie Lv and Biyuan Li and Xiuwei Wang and Pengfei Cai and Bo Yang and Xuefeng Jia and Jun Yan},
  doi          = {10.1016/j.imavis.2025.105481},
  journal      = {Image and Vision Computing},
  month        = {4},
  pages        = {105481},
  shortjournal = {Image Vis. Comput.},
  title        = {CMS-net: Edge-aware multimodal MRI feature fusion for brain tumor segmentation},
  volume       = {156},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Spatial cascaded clustering and weighted memory for unsupervised person re-identification. <em>ICV</em>, <em>156</em>, 105478. (<a href='https://doi.org/10.1016/j.imavis.2025.105478'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent advancements in unsupervised person re-identification (re-ID) methods have demonstrated high performance by leveraging fine-grained local context, often referred to as part-based methods. However, many existing part-based methods rely on horizontal division to obtain local contexts, leading to misalignment issues caused by various human poses. Moreover, misalignment of semantic information within part features hampers the effectiveness of metric learning, thereby limiting the potential of part-based methods. These challenges result in under-utilization of part features in existing approaches. To address these issues, we introduce the Spatial Cascaded Clustering and Weighted Memory (SCWM) method. SCWM aims to parse and align more accurate local contexts for different human body parts while allowing the memory module to balance hard example mining and noise suppression. Specifically, we first analyze the issues of foreground omissions and spatial confusions in previous methods. We then propose foreground and space corrections to enhance the completeness and reasonableness of human parsing results. Next, we introduce a weighted memory and utilize two weighting strategies. These strategies address hard sample mining for global features and enhance noise resistance for part features, enabling better utilization of both global and part features. Extensive experiments conducted on Market-1501, DukeMTMC-reID and MSMT17 datasets validate the effectiveness of the proposed method over numerous state-of-the-art methods.},
  archive      = {J_ICV},
  author       = {Jiahao Hong and Jialong Zuo and Chuchu Han and Ruochen Zheng and Ming Tian and Changxin Gao and Nong Sang},
  doi          = {10.1016/j.imavis.2025.105478},
  journal      = {Image and Vision Computing},
  month        = {4},
  pages        = {105478},
  shortjournal = {Image Vis. Comput.},
  title        = {Spatial cascaded clustering and weighted memory for unsupervised person re-identification},
  volume       = {156},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multi-information guided camouflaged object detection. <em>ICV</em>, <em>156</em>, 105470. (<a href='https://doi.org/10.1016/j.imavis.2025.105470'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Camouflaged Object Detection (COD) aims to identify the objects hidden in the background environment. Though more and more COD methods have been proposed in recent years, existing methods still perform poorly for detecting small objects, obscured objects, boundary-rich objects, and multi-objects, mainly because they fail to effectively utilize context information, texture information, and boundary information simultaneously. Therefore, in this paper, we propose a Multi-information Guided Camouflaged Object Detection Network (MIGNet) to fully utilize multi-information containing context information, texture information, and boundary information to boost the performance of camouflaged object detection. Specifically, firstly, we design the texture and boundary label and the Texture and Boundary Enhanced Module (TBEM) to obtain differentiated texture information and boundary information. Next, the Neighbor Context Information Exploration Module (NCIEM) is designed to obtain rich multi-scale context information. Then, the Parallel Group Bootstrap Module (PGBM) is designed to maximize the effective aggregation of context information, texture information and boundary information. Finally, Information Enhanced Decoder (IED) is designed to effectively enhance the interaction of neighboring layer features and suppress the background noise for good detection results. Extensive quantitative and qualitative experiments are conducted on four widely used datasets. The experimental results indicate that our proposed MIGNet with good performance of camouflaged object detection outperforms the other 22 COD models.},
  archive      = {J_ICV},
  author       = {Caijuan Shi and Lin Zhao and Rui Wang and Kun Zhang and Fanyue Kong and Changyu Duan},
  doi          = {10.1016/j.imavis.2025.105470},
  journal      = {Image and Vision Computing},
  month        = {4},
  pages        = {105470},
  shortjournal = {Image Vis. Comput.},
  title        = {Multi-information guided camouflaged object detection},
  volume       = {156},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). SFFEF-YOLO: Small object detection network based on fine-grained feature extraction and fusion for unmanned aerial images. <em>ICV</em>, <em>156</em>, 105469. (<a href='https://doi.org/10.1016/j.imavis.2025.105469'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Unmanned aerial vehicles (UAVs) images object detection has emerged as a research hotspot, yet remains a significant challenge due to variable target scales and the high proportion of small objects caused by UAVs’ diverse altitudes and angles. To address these issues, we propose a novel Small Object Detection Network Based on Fine-Grained Feature Extraction and Fusion(SFFEF-YOLO). First, we introduce a tiny prediction head to replace the large prediction head, enhancing the detection accuracy for tiny objects while reducing model complexity. Second, we design a Fine-Grained Information Extraction Module (FIEM) to replace standard convolutions. This module improves feature extraction and reduces information loss during downsampling by utilizing multi-branch operations and SPD-Conv. Third, we develop a Multi-Scale Feature Fusion Module (MFFM), which adds an additional skip connection branch based on the bidirectional feature pyramid network (BiFPN) to preserve fine-grained information and improve multi-scale feature fusion. We evaluated SFFEF-YOLO on the VisDrone2019-DET and UAVDT datasets. Compared to YOLOv8, experimental results demonstrate that SFFEF-YOLO achieves a 9.9% mAP0.5 improvement on the VisDrone2019-DET dataset and a 3.6% mAP0.5 improvement on the UAVDT dataset.},
  archive      = {J_ICV},
  author       = {Chenxi Bai and Kexin Zhang and Haozhe Jin and Peng Qian and Rui Zhai and Ke Lu},
  doi          = {10.1016/j.imavis.2025.105469},
  journal      = {Image and Vision Computing},
  month        = {4},
  pages        = {105469},
  shortjournal = {Image Vis. Comput.},
  title        = {SFFEF-YOLO: Small object detection network based on fine-grained feature extraction and fusion for unmanned aerial images},
  volume       = {156},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Joint transformer and mamba fusion for multispectral object detection. <em>ICV</em>, <em>156</em>, 105468. (<a href='https://doi.org/10.1016/j.imavis.2025.105468'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multispectral object detection is generally considered better than single-modality-based object detection, due to the complementary properties of multispectral image pairs. However, how to integrate features from images of different modalities for object detection is still an open problem. In this paper, we propose a new multispectral object detection framework based on the Transformer and Mamba architectures, called the joint Transformer and Mamba detection (JTMDet). Specifically, we divide the feature fusion process into two stages, the intra-scale fusion stage and the inter-scale fusion stage, to comprehensively utilize the multi-modal features at different scales. To this end, we designed the so-called cross-modal fusion (CMF) and cross-level fusion (CLF) modules, both of which contain JTMBlock modules. A JTMBlock module interweaves the Transformer and Mamba layers to robustly capture the useful information in multispectral image pairs while maintaining high inference speed. Extensive experiments on three publicly available datasets conclusively show that the proposed JTMDet framework achieves state-of-the-art multispectral object detection performance, and is competitive with current leading methods. Code and pre-trained models are publicly available at https://github.com/LiC2023/JTMDet .},
  archive      = {J_ICV},
  author       = {Chao Li and Xiaoming Peng},
  doi          = {10.1016/j.imavis.2025.105468},
  journal      = {Image and Vision Computing},
  month        = {4},
  pages        = {105468},
  shortjournal = {Image Vis. Comput.},
  title        = {Joint transformer and mamba fusion for multispectral object detection},
  volume       = {156},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). AF2CN: Towards effective demoiréing from multi-resolution images. <em>ICV</em>, <em>156</em>, 105467. (<a href='https://doi.org/10.1016/j.imavis.2025.105467'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, CNN-based methods have gained significant attention for addressing the demoiré task due to their powerful feature extraction capabilities. However, these methods are generally trained on datasets with fixed resolutions, limiting their applicability to diverse real-world scenarios. To address this limitation, we introduce a more generalized task: effective demoiréing across multiple resolutions. To facilitate this task, we constructed MTADM, the first multi-resolution moiré dataset, designed to capture diverse real-world scenarios. Leveraging this dataset, we conducted extensive studies and introduced the Adaptive Fractional Calculus and Adjacency Fusion Convolution Network (AF2CN). Specifically, we employ fractional derivatives to develop an adaptive frequency enhancement module, which refines spatial distribution and texture details in moiré patterns. Additionally, we design a spatial attention gate to enhance deep feature interaction. Extensive experiments demonstrate that AF2CN effectively handles multi-resolution moiré patterns. It significantly outperforms previous state-of-the-art methods on fixed-resolution benchmarks while requiring fewer parameters and achieving lower computational costs.},
  archive      = {J_ICV},
  author       = {Shitan Asu and Yujin Dai and Shijie Li and Zheng Li},
  doi          = {10.1016/j.imavis.2025.105467},
  journal      = {Image and Vision Computing},
  month        = {4},
  pages        = {105467},
  shortjournal = {Image Vis. Comput.},
  title        = {AF2CN: Towards effective demoiréing from multi-resolution images},
  volume       = {156},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Innovative underwater image enhancement algorithm: Combined application of adaptive white balance color compensation and pyramid image fusion to submarine algal microscopy. <em>ICV</em>, <em>156</em>, 105466. (<a href='https://doi.org/10.1016/j.imavis.2025.105466'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Real-time collected microscopic images of harmful algal blooms (HABs) in coastal areas often suffer from significant color deviations and loss of fine cellular details. To address these issues, this paper proposes an innovative method for enhancing underwater marine algal microscopic images based on Adaptive White Balance Color Compensation (AWBCC) and Image Pyramid Fusion (IPF). Firstly, an effective Algorithm Adaptive Cyclic Channel Compensation (ACCC) is proposed based on the gray world assumption to enhance the color of underwater images. Then, the Maximum Color Channel Attention Guidance (MCCAG) method is employed to reduce color disturbance caused by ignoring light absorption. This paper introduces an Empirical Contrast Enhancement (ECH) module based on multi-scale IPF tailored for underwater microscopic images of algae, which is used for global contrast enhancement, texture detail enhancement, and noise control. Secondly, this paper proposes a network based on a diffusion probability model for edge detection in HABs, which simultaneously considers both high-order and low-order features extracted from images. This approach enriches the semantic information of the feature maps and enhances edge detection accuracy. This edge detection method achieves an ODS of 0.623 and an OIS of 0.683. Experimental evaluations demonstrate that our underwater algae microscopic image enhancement method amplifies local texture features while preserving the original image structure. This significantly improves the accuracy of edge detection and key point matching. Compared to several state-of-the-art underwater image enhancement methods, our approach achieves the highest values in contrast, average gradient, entropy, and Enhancement Measure Estimation (EME), and also delivers competitive results in terms of image noise control. .},
  archive      = {J_ICV},
  author       = {Yi-Ning Fan and Geng-Kun Wu and Jia-Zheng Han and Bei-Ping Zhang and Jie Xu},
  doi          = {10.1016/j.imavis.2025.105466},
  journal      = {Image and Vision Computing},
  month        = {4},
  pages        = {105466},
  shortjournal = {Image Vis. Comput.},
  title        = {Innovative underwater image enhancement algorithm: Combined application of adaptive white balance color compensation and pyramid image fusion to submarine algal microscopy},
  volume       = {156},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Feature field fusion for few-shot novel view synthesis. <em>ICV</em>, <em>156</em>, 105465. (<a href='https://doi.org/10.1016/j.imavis.2025.105465'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Reconstructing neural radiance fields from limited or sparse views has given very promising potential for this field of research. Previous methods usually constrain the reconstruction process with additional priors, e.g. semantic-based or patch-based regularization. Nevertheless, such regularization is given to the synthesis of unseen views, which may not effectively assist the field of learning, in particular when the training views are sparse. Instead, we propose a feature Field Fusion (FFusion) NeRF in this paper that can learn structure and more details from features extracted from pre-trained neural networks for the sparse training views, and use as extra guide for the training of the RGB field. With such extra feature guides, FFusion predicts more accurate color and density when synthesizing novel views. Experimental results have shown that FFusion can effectively improve the quality of the synthesized novel views with only limited or sparse inputs.},
  archive      = {J_ICV},
  author       = {Junting Li and Yanghong Zhou and Jintu Fan and Dahua Shou and Sa Xu and P.Y. Mok},
  doi          = {10.1016/j.imavis.2025.105465},
  journal      = {Image and Vision Computing},
  month        = {4},
  pages        = {105465},
  shortjournal = {Image Vis. Comput.},
  title        = {Feature field fusion for few-shot novel view synthesis},
  volume       = {156},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Gait recognition via view-aware part-wise attention and multi-scale dilated temporal extractor. <em>ICV</em>, <em>156</em>, 105464. (<a href='https://doi.org/10.1016/j.imavis.2025.105464'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Gait recognition based on silhouette sequences has made significant strides in recent years through the extraction of body shape and motion features. However, challenges remain in achieving accurate gait recognition under covariate changes, such as variations in view and clothing. To tackle these issues, this paper introduces a novel methodology incorporating a View-aware Part-wise Attention (VPA) mechanism and a Multi-scale Dilated Temporal Extractor (MDTE) to enhance gait recognition. Distinct from existing techniques, VPA mechanism acknowledges the differential sensitivity of various body parts to view changes, applying targeted attention weights at the feature level to improve the efficacy of view-aware constraints in areas of higher saliency or distinctiveness. Concurrently, MDTE employs dilated convolutions across multiple scales to capture the temporal dynamics of gait at diverse levels, thereby refining the motion representation. Comprehensive experiments on the CASIA-B, OU-MVLP, and Gait3D datasets validate the superior performance of our approach. Remarkably, our method achieves a 91.0% accuracy rate under clothing-change conditions on the CASIA-B dataset using solely silhouette information, surpassing the current state-of-the-art (SOTA) techniques. These results underscore the effectiveness and adaptability of our proposed strategy in overcoming the complexities of gait recognition amidst covariate changes.},
  archive      = {J_ICV},
  author       = {Xu Song and Yang Wang and Yan Huang and Caifeng Shan},
  doi          = {10.1016/j.imavis.2025.105464},
  journal      = {Image and Vision Computing},
  month        = {4},
  pages        = {105464},
  shortjournal = {Image Vis. Comput.},
  title        = {Gait recognition via view-aware part-wise attention and multi-scale dilated temporal extractor},
  volume       = {156},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Deep learning for brain tumor segmentation in multimodal MRI images: A review of methods and advances. <em>ICV</em>, <em>156</em>, 105463. (<a href='https://doi.org/10.1016/j.imavis.2025.105463'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Background and Objectives: Image segmentation is crucial in applications like image understanding, feature extraction, and analysis. The rapid development of deep learning techniques in recent years has significantly enhanced the field of medical image processing, with the process of segmenting tumor from MRI images of the brain emerging as a particularly active area of interest within the medical science community. Existing reviews predominantly focus on traditional CNNs and Transformer models but lack systematic analysis and experimental validation on the application of the emerging Mamba architecture in multimodal brain tumor segmentation, the handling of missing modalities, the potential of multimodal fusion strategies, and the heterogeneity of datasets. Methods: This paper provides a comprehensive literature review of recent deep learning-based methods for multimodal brain tumor segmentation using multimodal MRI images, including performance and quantitative analysis of state-of-the-art approaches. It focuses on the handling of multimodal fusion, adaptation techniques, and missing modality, while also delving into the performance, advantages, and disadvantages of deep learning models such as U-Net, Transformer, hybrid deep learning, and Mamba-based methods in segmentation tasks. Results: Through the entire review process, It is found that most researchers preferred to use the Transformer-based U-Net model and mamba-based U-Net, especially the fusion model combination of U-Net and mamba, for image segmentation.},
  archive      = {J_ICV},
  author       = {Bin Jiang and Maoyu Liao and Yun Zhao and Gen Li and Siyu Cheng and Xiangkai Wang and Qingling Xia},
  doi          = {10.1016/j.imavis.2025.105463},
  journal      = {Image and Vision Computing},
  month        = {4},
  pages        = {105463},
  shortjournal = {Image Vis. Comput.},
  title        = {Deep learning for brain tumor segmentation in multimodal MRI images: A review of methods and advances},
  volume       = {156},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multispectral images reconstruction using median filtering based spectral correlation. <em>ICV</em>, <em>156</em>, 105462. (<a href='https://doi.org/10.1016/j.imavis.2025.105462'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multispectral images are widely utilized in various computer vision applications because they capture more information than traditional color images. Multispectral imaging systems utilize a multispectral filter array (MFA), an extension of the color filter array found in standard RGB cameras. This approach provides an efficient, cost-effective, and practical method for capturing multispectral images. The primary challenge with multispectral imaging systems using an MFA is the significant undersampling of spectral bands in the mosaicked image. This occurs because a multispectral mosaic image contains a greater number of spectral bands compared to an RGB mosaicked image, leading to reduced sampling density per band. Now, multispectral demosaicing algorithm is required to generate the complete multispectral image from the mosaicked image. The effectiveness of demosaicing algorithms relies heavily on the efficient utilization of spatial and spectral correlations inherent in mosaicked images. In the proposed method, a binary tree-based MFA pattern is employed to capture the mosaicked image. Rather than directly leveraging spectral correlations between bands, median filtering is applied to the spectral differences to mitigate the impact of noise on these correlations. Experimental results demonstrate that the proposed method achieves an improvement of 1.03 dB and 0.92 dB on average from 5-band to 10-band multispectral images from the widely used TokyoTech and CAVE datasets, respectively.},
  archive      = {J_ICV},
  author       = {Vishwas Rathi and Abhilasha Sharma and Amit Kumar Singh},
  doi          = {10.1016/j.imavis.2025.105462},
  journal      = {Image and Vision Computing},
  month        = {4},
  pages        = {105462},
  shortjournal = {Image Vis. Comput.},
  title        = {Multispectral images reconstruction using median filtering based spectral correlation},
  volume       = {156},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). NPVForensics: Learning VA correlations in non-critical phoneme–viseme regions for deepfake detection. <em>ICV</em>, <em>156</em>, 105461. (<a href='https://doi.org/10.1016/j.imavis.2025.105461'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Advanced deepfake technology enables the manipulation of visual and audio signals within videos, leading to visual–audio (VA) inconsistencies. Current multimodal detectors primarily rely on VA contrastive learning to identify such inconsistencies, particularly in critical phoneme–viseme (PV) regions. However, state-of-the-art deepfake techniques have aligned critical PV pairs, thereby reducing the inconsistency traces on which existing methods rely. Due to technical constraints, forgers cannot fully synchronize VA in non-critical phoneme–viseme (NPV) regions. Consequently, we exploit inconsistencies in NPV regions as a general cue for deepfake detection. We propose NPVForensics, a two-stage VA correlation learning framework specifically designed to detect VA inconsistencies in NPV regions of deepfake videos. Firstly, to better extract VA unimodal features, we utilize the Swin Transformer to capture long-term global dependencies. Additionally, the Local Feature Aggregation (LFA) module aggregates local features from spatial and channel dimensions, thus preserving more comprehensive and subtle information. Secondly, the VA Correlation Learning (VACL) module enhances intra-modal augmentation and inter-modal information interaction, exploring intrinsic correlations between the two modalities. Moreover, Representation Alignment is introduced for real videos to narrow the modal gap and effectively extract VA correlations. Finally, our model is pre-trained on real videos using a self-supervised strategy and fine-tuned for the deepfake detection task. We conducted extensive experiments on six widely used deepfake datasets: FaceForensics++, FakeAVCeleb, Celeb-DF-v2, DFDC, FaceShifter, and DeeperForensics-1.0. Our method achieves state-of-the-art performance in cross-manipulation generalization and robustness. Notably, our approach demonstrates superior performance on VA-coordinated datasets such as A2V, T2V-L, and T2V-S. It indicates that VA inconsistencies in NPV regions serve as a general cue for deepfake detection.},
  archive      = {J_ICV},
  author       = {Yu Chen and Yang Yu and Rongrong Ni and Haoliang Li and Wei Wang and Yao Zhao},
  doi          = {10.1016/j.imavis.2025.105461},
  journal      = {Image and Vision Computing},
  month        = {4},
  pages        = {105461},
  shortjournal = {Image Vis. Comput.},
  title        = {NPVForensics: Learning VA correlations in non-critical phoneme–viseme regions for deepfake detection},
  volume       = {156},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). FRoundation: Are foundation models ready for face recognition?. <em>ICV</em>, <em>156</em>, 105453. (<a href='https://doi.org/10.1016/j.imavis.2025.105453'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Foundation models are predominantly trained in an unsupervised or self-supervised manner on highly diverse and large-scale datasets, making them broadly applicable to various downstream tasks. In this work, we investigate for the first time whether such models are suitable for the specific domain of face recognition (FR). We further propose and demonstrate the adaptation of these models for FR across different levels of data availability, including synthetic data. Extensive experiments are conducted on multiple foundation models and datasets of varying scales for training and fine-tuning, with evaluation on a wide range of benchmarks. Our results indicate that, despite their versatility, pre-trained foundation models tend to underperform in FR in comparison with similar architectures trained specifically for this task. However, fine-tuning foundation models yields promising results, often surpassing models trained from scratch, particularly when training data is limited. For example, after fine-tuning only on 1K identities, DINOv2 ViT-S achieved average verification accuracy on LFW, CALFW, CPLFW, CFP-FP, and AgeDB30 benchmarks of 87.10%, compared to 64.70% achieved by the same model and without fine-tuning. While training the same model architecture, ViT-S, from scratch on 1k identities reached 69.96%. With access to larger-scale FR training datasets, these performances reach 96.03% and 95.59% for the DINOv2 and CLIP ViT-L models, respectively. In comparison to the ViT-based architectures trained from scratch for FR, fine-tuned same architectures of foundation models achieve similar performance while requiring lower training computational costs and not relying on the assumption of extensive data availability. We further demonstrated the use of synthetic face data, showing improved performances over both pre-trained foundation and ViT models. Additionally, we examine demographic biases, noting slightly higher biases in certain settings when using foundation models compared to models trained from scratch. We release our code and pre-trained models’ weights at github.com/TaharChettaoui/FRoundation .},
  archive      = {J_ICV},
  author       = {Tahar Chettaoui and Naser Damer and Fadi Boutros},
  doi          = {10.1016/j.imavis.2025.105453},
  journal      = {Image and Vision Computing},
  month        = {4},
  pages        = {105453},
  shortjournal = {Image Vis. Comput.},
  title        = {FRoundation: Are foundation models ready for face recognition?},
  volume       = {156},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). PolarDETR: Polar parametrization for vision-based surround-view 3D detection. <em>ICV</em>, <em>156</em>, 105438. (<a href='https://doi.org/10.1016/j.imavis.2025.105438'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {3D detection based on surround-view camera system is a critical and promising technique in autopilot. In this work, we exploit the view symmetry of surround-view camera system as inductive bias to improve optimization and boost performance. We parameterize object’s position by polar coordinate and decompose velocity along radial and tangential direction. And the perception range, label assignment and loss function are correspondingly reformulated in polar coordinate system. This new Polar Parametrization scheme establishes explicit associations between image patterns and prediction targets. Based on it, we propose a surround-view 3D detection method, termed PolarDETR. PolarDETR achieves competitive performance on nuScenes dataset. Thorough ablation studies are provided to validate the effectiveness.},
  archive      = {J_ICV},
  author       = {Shaoyu Chen and Xinggang Wang and Tianheng Cheng and Qian Zhang and Chang Huang and Wenyu Liu},
  doi          = {10.1016/j.imavis.2025.105438},
  journal      = {Image and Vision Computing},
  month        = {4},
  pages        = {105438},
  shortjournal = {Image Vis. Comput.},
  title        = {PolarDETR: Polar parametrization for vision-based surround-view 3D detection},
  volume       = {156},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). DAN: Distortion-aware network for fisheye image rectification using graph reasoning. <em>ICV</em>, <em>156</em>, 105423. (<a href='https://doi.org/10.1016/j.imavis.2025.105423'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Despite the wide-field view of fisheye images, their application is still hindered by the presentation of distortions. Existing learning-based methods still suffer from artifacts and loss of details, especially at the image edges. To address this, we introduce the Distortion-aware Network (DAN), a novel deep network architecture for fisheye image rectification that leverages graph reasoning. Specifically, we employ the superior relational understanding capability of graph technology to associate distortion patterns in different regions, generating an accurate and globally consistent unwarping flow. Meanwhile, during the image reconstruction process, we utilize deformable convolution to construct same-resolution feature blocks and employ skip connections to supplement the detailed information. Additionally, we introduce a weight decay-based multi-scale loss function, enabling the model to focus more on accuracy at high-resolution layers while enhancing the model’s generalization ability. To address the lack of quantitative evaluation standards for real fisheye images, we propose a new metric called the “Line Preservation Metric.” Through qualitative and quantitative experiments on PLACE365, COCO2017 and real fisheye images, the proposed method proves to outperform existing methods in terms of performance and generalization.},
  archive      = {J_ICV},
  author       = {Yongjia Yan and Hongzhe Liu and Cheng Zhang and Cheng Xu and Bingxin Xu and Weiguo Pan and Songyin Dai and Yiqing Song},
  doi          = {10.1016/j.imavis.2025.105423},
  journal      = {Image and Vision Computing},
  month        = {4},
  pages        = {105423},
  shortjournal = {Image Vis. Comput.},
  title        = {DAN: Distortion-aware network for fisheye image rectification using graph reasoning},
  volume       = {156},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). CMASR: Lightweight image super-resolution with cluster and match attention. <em>ICV</em>, <em>155</em>, 105457. (<a href='https://doi.org/10.1016/j.imavis.2025.105457'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Transformer has recently achieved impressive success in image super-resolution due to its ability to model long-range dependencies with multi-head self-attention (MHSA). However, most existing MHSAs focus only on the dependencies among individual tokens, and ignore the ones among token clusters containing several tokens, resulting in the inability of Transformer to adequately explore global features. On the other hand, Transformer neglects local features, which inevitably hinders accurate detail reconstruction. To address the above issues, we propose a lightweight image super-resolution method with cluster and match attention (CMASR). Specifically, a token Clustering block is designed to divide input tokens into token clusters of different sizes with depthwise separable convolution. Subsequently, we propose an efficient axial matching self-attention (AMSA) mechanism, which introduces an axial matrix to extract local features, including axial similarities and symmetries. Further, by combining AMSA and Window Self-Attention, we construct a Hybrid Self-Attention block to capture the dependencies among token clusters of different sizes to sufficiently extract axial local features and global features. Extensive experiments demonstrate that the proposed CMASR outperforms state-of-the-art methods with fewer computational cost (i.e., the number of parameters and FLOPs).},
  archive      = {J_ICV},
  author       = {Detian Huang and Mingxin Lin and Hang Liu and Huanqiang Zeng},
  doi          = {10.1016/j.imavis.2025.105457},
  journal      = {Image and Vision Computing},
  month        = {3},
  pages        = {105457},
  shortjournal = {Image Vis. Comput.},
  title        = {CMASR: Lightweight image super-resolution with cluster and match attention},
  volume       = {155},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). ESDA: Zero-shot semantic segmentation based on an embedding semantic space distribution adjustment strategy. <em>ICV</em>, <em>155</em>, 105456. (<a href='https://doi.org/10.1016/j.imavis.2025.105456'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, the CLIP model, which is pre-trained on large-scale vision-language data, has promoted the development of zero-shot recognition tasks. Some researchers apply CLIP to zero-shot semantic segmentation, but they often struggle to achieve satisfactory results. This is because this dense prediction task requires not only a precise understanding of semantics, but also a precise perception of different regions within one image. However, CLIP is trained on image-level vision-language data, resulting in ineffective perception of pixel-level regions. In this paper, we propose a new zero-shot semantic segmentation (ZS3) method based on an embedding semantic space distribution adjustment strategy (ESDA), which enables CLIP to accurately perceive both semantics and regions. This method inserts additional trainable blocks into the CLIP image encoder, enabling it to effectively perceive regions without losing semantic understanding. Besides, we design spatial distribution losses to guide the update of parameters of the trainable blocks, thereby further enhancing the regional characteristics of pixel-level image embeddings. In addition, previous methods only obtain semantic support through a text [CLS] token, which is far from sufficient for the dense prediction task. Therefore, we design a vision-language embedding interactor, which can obtain richer semantic support through the interaction between the entire text embedding and image embedding. It can also further enhance the semantic support and strengthen the image embedding. Plenty of experiments on PASCAL- 5 i and COCO- 2 0 i prove the effectiveness of our method. Our method achieves new state-of-the-art for zero-shot semantic segmentation and exceeds many few-shot semantic segmentation methods. Codes are available at https://github.com/Jiaguang-NEU/ESDA .},
  archive      = {J_ICV},
  author       = {Jiaguang Li and Ying Wei and Wei Zhang and Chuyuan Wang},
  doi          = {10.1016/j.imavis.2025.105456},
  journal      = {Image and Vision Computing},
  month        = {3},
  pages        = {105456},
  shortjournal = {Image Vis. Comput.},
  title        = {ESDA: Zero-shot semantic segmentation based on an embedding semantic space distribution adjustment strategy},
  volume       = {155},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). FGS-NeRF: A fast glossy surface reconstruction method based on voxel and reflection directions. <em>ICV</em>, <em>155</em>, 105455. (<a href='https://doi.org/10.1016/j.imavis.2025.105455'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Neural surface reconstruction technology has great potential for recovering 3D surfaces from multiview images. However, surface gloss can severely affect the reconstruction quality. Although existing methods address the issue of glossy surface reconstruction, achieving rapid reconstruction remains a challenge. While DVGO can achieve rapid scene geometry search, it tends to create numerous holes in glossy surfaces during the search process. To address this, we design a geometry search method based on SDF and reflection directions, employing a method called progressive voxel-MLP scaling to achieve accurate and efficient geometry searches for glossy scenes. To mitigate object edge artifacts caused by reflection directions, we use a simple loss function called sigmoid RGB loss, which helps reduce artifacts around objects during the early stages of training and promotes efficient surface convergence. In this work, we introduce the FGS-NeRF model, which uses a coarse-to-fine training method combined with reflection directions to achieve rapid reconstruction of glossy object surfaces based on voxel grids. The training time on a single RTX 4080 GPU is 20 min. Evaluations on the Shiny Blender and Smart Car datasets confirm that our model significantly improves the speed when compared with existing glossy object reconstruction methods while achieving accurate object surfaces. Code: https://github.com/yosugahhh/FGS-nerf .},
  archive      = {J_ICV},
  author       = {Han Hong and Qing Ye and Keyun Xiong and Qing Tao and Yiqian Wan},
  doi          = {10.1016/j.imavis.2025.105455},
  journal      = {Image and Vision Computing},
  month        = {3},
  pages        = {105455},
  shortjournal = {Image Vis. Comput.},
  title        = {FGS-NeRF: A fast glossy surface reconstruction method based on voxel and reflection directions},
  volume       = {155},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). AHA-track: Aggregating hierarchical awareness features for single. <em>ICV</em>, <em>155</em>, 105454. (<a href='https://doi.org/10.1016/j.imavis.2025.105454'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Single Object Tracking (SOT) plays a crucial role in various real-world applications but still faces significant challenges, including scale variations and background distractions. While Vision Transformers (ViTs) have demonstrated improvements in tracking performance, they are often hindered by high computational costs. To address these issues, this paper propose a lightweight single object tracking model by aggregating hierarchical awareness features (AHA-Track). The template information is aggregated by aggregate token awareness module, and the key points of template are highlighted to reduce background interference. In addition, the hierarchical deep feature aggregation module has a more comprehensive understanding of object at different resolutions. It ultimately helps to improve the accuracy and robustness of challenging tracking scenes. AHA-Track enhances both tracking accuracy and speed, while maintaining computational efficiency. Extensive experimental evaluations across several benchmark datasets demonstrate that AHA-Track outperforms existing state-of-the-art methods in terms of both tracking accuracy and efficiency. The codes and pretrained models are available at https://github.com/YangMinbobo/AHATrack .},
  archive      = {J_ICV},
  author       = {Min Yang and Zhiqing Guo and Liejun Wang},
  doi          = {10.1016/j.imavis.2025.105454},
  journal      = {Image and Vision Computing},
  month        = {3},
  pages        = {105454},
  shortjournal = {Image Vis. Comput.},
  title        = {AHA-track: Aggregating hierarchical awareness features for single},
  volume       = {155},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Two-modal multiscale feature cross fusion for hyperspectral unmixing. <em>ICV</em>, <em>155</em>, 105445. (<a href='https://doi.org/10.1016/j.imavis.2025.105445'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Hyperspectral images (HSI) possess rich spectral characteristics but suffer from low spatial resolution, which has led many methods to focus on extracting more spatial information from HSI. However, the spatial information that can be extracted from a single HSI is limited, making it difficult to distinguish objects with similar materials. To address this issue, we propose a multimodal unmixing network called MSFF-Net. This network enhances unmixing performance by integrating the spatial information from light detection and ranging (LiDAR) data into the unmixing process. To ensure a more comprehensive fusion of features from the two modalities, we introduce a multi-scale cross-fusion method, providing a new approach to multimodal data fusion. Additionally, the network employs attention mechanisms to enhance channel-wise and spatial features, boosting the model's representational capacity. Our proposed model effectively consolidates multimodal information, significantly improving its unmixing capability, especially in complex environments, leading to more accurate unmixing results and facilitating further analysis of HSI. We evaluate our method using two real-world datasets. Experimental results demonstrate that our proposed approach outperforms other state-of-the-art methods in terms of both stability and effectiveness.},
  archive      = {J_ICV},
  author       = {Senlong Qin and Yuqi Hao and Minghui Chu and Xiaodong Yu},
  doi          = {10.1016/j.imavis.2025.105445},
  journal      = {Image and Vision Computing},
  month        = {3},
  pages        = {105445},
  shortjournal = {Image Vis. Comput.},
  title        = {Two-modal multiscale feature cross fusion for hyperspectral unmixing},
  volume       = {155},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Enhanced residual network for burst image super-resolution using simple base frame guidance. <em>ICV</em>, <em>155</em>, 105444. (<a href='https://doi.org/10.1016/j.imavis.2025.105444'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Burst or multi-frame image super-resolution (MFSR) has emerged as a critical area in computer vision, aimed at reconstructing high-resolution images from low-resolution bursts. Unlike single-image super-resolution (SISR), which has been extensively studied, MFSR leverages information from multiple shifted frames in order to mitigate the ill-posed nature of SISR. The rapid advancement in the capabilities of handheld devices, including enhanced processing power and faster image capture rates also add a layer of relevance in this field. In our previous work, we proposed a simple yet effective deep learning method tailored for RAW images, called Simple Base Frame Burst (SBFBurst). This method, based on residual convolutional architecture, demonstrated significant performance improvements by incorporating base frame guidance mechanisms such as skip frame connections and concatenation of the base frame alongside the network. Despite the promising outcomes obtained, given the outlined context and the limited investigation compared to SISR, it is evident that further extensions and experiments are required to propel the field of MFSR forward. In this paper, we extend our recent work on SBFBurst by conducting a comprehensive analysis of the method from various perspectives. Our primary contribution lies in adapting and testing the architecture to handle both RAW Bayer pattern images and RGB images, allowing the evaluation using the novel RealBSR-RGB dataset. Our experiments revealed that SBFBurst still consistently outperforms existing state-of-the-art approaches both quantitatively and qualitatively, even after the introduction of a new method, FBANet, for comparison. We also extended our experiments to assess the impact of architecture parameters, model generalization, and its capacity to leverage complementary information. These exploratory extensions may open new avenues for advance in this field. Our code and models are publicly available at https://github.com/AndersonCotrim/SBFBurst .},
  archive      = {J_ICV},
  author       = {Anderson Nogueira Cotrim and Gerson Barbosa and Cid Adinam Nogueira Santos and Helio Pedrini},
  doi          = {10.1016/j.imavis.2025.105444},
  journal      = {Image and Vision Computing},
  month        = {3},
  pages        = {105444},
  shortjournal = {Image Vis. Comput.},
  title        = {Enhanced residual network for burst image super-resolution using simple base frame guidance},
  volume       = {155},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Proactive robot task sequencing through real-time hand motion prediction in human–robot collaboration. <em>ICV</em>, <em>155</em>, 105443. (<a href='https://doi.org/10.1016/j.imavis.2025.105443'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Human–robot collaboration (HRC) is essential for improving productivity and safety across various industries. While reactive motion re-planning strategies are useful, there is a growing demand for proactive methods that predict human intentions to enable more efficient collaboration. This study addresses this need by introducing a framework that combines deep learning-based human hand trajectory forecasting with heuristic optimization for robotic task sequencing. The deep learning model advances real-time hand position forecasting using a multi-task learning loss to account for both hand positions and contact delay regression, achieving state-of-the-art performance on the Ego4D Future Hand Prediction benchmark. By integrating hand trajectory predictions into task planning, the framework offers a cohesive solution for HRC. To optimize task sequencing, the framework incorporates a Dynamic Variable Neighborhood Search (DynamicVNS) heuristic algorithm, which allows robots to pre-plan task sequences and avoid potential collisions with human hand positions. DynamicVNS provides significant computational advantages over the generalized VNS method. The framework was validated on a UR10e robot performing a visual inspection task in a HRC scenario, where the robot effectively anticipated and responded to human hand movements in a shared workspace. Experimental results highlight the system’s effectiveness and potential to enhance HRC in industrial settings by combining predictive accuracy and task planning efficiency.},
  archive      = {J_ICV},
  author       = {Shyngyskhan Abilkassov and Michael Gentner and Almas Shintemirov and Eckehard Steinbach and Mirela Popa},
  doi          = {10.1016/j.imavis.2025.105443},
  journal      = {Image and Vision Computing},
  month        = {3},
  pages        = {105443},
  shortjournal = {Image Vis. Comput.},
  title        = {Proactive robot task sequencing through real-time hand motion prediction in human–robot collaboration},
  volume       = {155},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Vehicle re-identification with large separable kernel attention and hybrid channel attention. <em>ICV</em>, <em>155</em>, 105442. (<a href='https://doi.org/10.1016/j.imavis.2025.105442'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the rapid development of intelligent transportation systems and the popularity of smart city infrastructure, Vehicle Re-ID technology has become an important research field. The vehicle Re-ID task faces an important challenge, which is the high similarity between different vehicles. Existing methods use additional detection or segmentation models to extract differentiated local features. However, these methods either rely on additional annotations or greatly increase the computational cost. Using attention mechanism to capture global and local features is crucial to solve the challenge of high similarity between classes in vehicle Re-ID tasks. In this paper, we propose LSKA-ReID with large separable kernel attention and hybrid channel attention. Specifically, the large separable kernel attention (LSKA) utilizes the advantages of self-attention and also benefits from the advantages of convolution, which can extract the global and local features of the vehicle more comprehensively. We also compare the performance of LSKA and large kernel attention (LKA) on the vehicle ReID task. We also introduce hybrid channel attention (HCA), which combines channel attention with spatial information, so that the model can better focus on channels and feature regions, and ignore background and other disturbing information. Extensive experiments on three popular datasets VeRi-776, VehicleID and VERI-Wild demonstrate the effectiveness of LSKA-ReID. In particular, on VeRi-776 dataset, mAP reaches 86.78% and Rank-1 reaches 98.09%.},
  archive      = {J_ICV},
  author       = {Xuezhi Xiang and Zhushan Ma and Xiaoheng Li and Lei Zhang and Xiantong Zhen},
  doi          = {10.1016/j.imavis.2025.105442},
  journal      = {Image and Vision Computing},
  month        = {3},
  pages        = {105442},
  shortjournal = {Image Vis. Comput.},
  title        = {Vehicle re-identification with large separable kernel attention and hybrid channel attention},
  volume       = {155},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Resource-aware strategies for real-time multi-person pose estimation. <em>ICV</em>, <em>155</em>, 105441. (<a href='https://doi.org/10.1016/j.imavis.2025.105441'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {When using deep learning applications for human posture estimation (HPE), especially on devices with limited resources, accuracy and efficiency must be balanced. Common deep-learning architectures have a propensity to use a large amount of processing power while yielding low accuracy. This work proposes the implementation of Efficient YoloPose, a new architecture based on You Only Look Once version 8 (YOLOv8)-Pose, in an attempt to address these issues. Advanced lightweight methods like Depthwise Convolution, Ghost Convolution, and the C3Ghost module are used by Efficient YoloPose to replace traditional convolution and C2f (a quicker implementation of the Cross Stage Partial Bottleneck). This approach greatly decreases the inference, parameter count, and computing complexity. To improve posture estimation even further, Efficient YoloPose integrates the Squeeze Excitation (SE) attention method into the network. The main focus of this process during posture estimation is the significant areas of an image. Experimental results show that the suggested model performs better than the current models on the COCO and OCHuman datasets. The proposed model lowers the inference time from 1.1 milliseconds (ms) to 0.9 ms, the computational complexity from 9.2 Giga Floating-point operations (GFlops) to 4.8 GFlops and the parameter count from 3.3 million to 1.3 million when compared to YOLOv8-Pose. In addition, this model maintains an average precision (AP) score of 78.8 on the COCO dataset. The source code for Efficient YoloPose has been made publicly available at [ https://github.com/malareeqi/Efficient-YoloPose ].},
  archive      = {J_ICV},
  author       = {Mohammed A. Esmail and Jinlei Wang and Yihao Wang and Li Sun and Guoliang Zhu and Guohe Zhang},
  doi          = {10.1016/j.imavis.2025.105441},
  journal      = {Image and Vision Computing},
  month        = {3},
  pages        = {105441},
  shortjournal = {Image Vis. Comput.},
  title        = {Resource-aware strategies for real-time multi-person pose estimation},
  volume       = {155},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). MAFMv3: An automated multi-scale attention-based feature fusion MobileNetv3 for spine lesion classification. <em>ICV</em>, <em>155</em>, 105440. (<a href='https://doi.org/10.1016/j.imavis.2025.105440'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Spine lesion classification is a crucial task in medical imaging that plays a significant role in the early diagnosis and treatment of spinal conditions. In this paper, we propose an MAFMv3 (Multi-Scale Attention Feature Fusion MobileNetv3) model for automated spine lesion classification, which builds upon MobileNetv3, incorporating Attention and Atrous Spatial Pyramid Pooling (ASPP) modules to enhance focus on lesion regions and capture multi-scale features. This novel architecture uses raw, normalized, and histogram-equalized images to generate a comprehensive 3D feature map, significantly improving classification performance. Preprocessing steps include Histogram Equalization, and data augmentation techniques are applied to expand the dataset and enhance model generalization. The proposed model is evaluated on the VinDr-SpineXR publicly available dataset. The MAFMv3 model achieves state-of-the-art results with an accuracy of 96.81%, precision of 98.38%, recall of 97.95%, F1-score of 98.15%, and AUC of 99.98%, demonstrating its potential for clinical applications in medical imaging. Future work will focus on further optimizations and validating the model in real-world clinical environments to enhance its diagnostic impact.},
  archive      = {J_ICV},
  author       = {Aqsa Dastgir and Wang Bin and Muhammad Usman Saeed and Jinfang Sheng and Salman Saleem},
  doi          = {10.1016/j.imavis.2025.105440},
  journal      = {Image and Vision Computing},
  month        = {3},
  pages        = {105440},
  shortjournal = {Image Vis. Comput.},
  title        = {MAFMv3: An automated multi-scale attention-based feature fusion MobileNetv3 for spine lesion classification},
  volume       = {155},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). DiffusionLoc: A diffusion model-based framework for crowd localization. <em>ICV</em>, <em>155</em>, 105439. (<a href='https://doi.org/10.1016/j.imavis.2025.105439'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The accurate location of individuals in dense crowds remains a challenging problem and is of significant importance for crowd analysis. Traditional methods, such as box-based and map-based approaches, often fail to achieve ideal accuracy in high-density scenarios. Point-based localization methods have recently shown promising results but generally rely on heuristic priors to address localization tasks. This reliance on priors can lead to unstable performance across diverse scenarios, especially in crowds with significant density variations, where the methods struggle to generalize effectively. In this work, we introduce a framework called DiffusionLoc built upon the diffusion models, which directly generates target points from random noise, simplifying the pipeline of point-based methods. Moreover, we design a feature interpolation method, called Differential Attention-based Implicit Feature Interpolation (DF-IFI), which effectively mitigates the instability of noisy points while extracting their features. Extensive experiments show that DiffusionLoc demonstrates superior competitive performance, and adapts flexibly to different scenarios by dynamically modifying the number of noisy points and iteration steps.},
  archive      = {J_ICV},
  author       = {Qi Zhang and Yuan Li and Yiran Liu and Yanzhao Zhou and Jianbin Jiao},
  doi          = {10.1016/j.imavis.2025.105439},
  journal      = {Image and Vision Computing},
  month        = {3},
  pages        = {105439},
  shortjournal = {Image Vis. Comput.},
  title        = {DiffusionLoc: A diffusion model-based framework for crowd localization},
  volume       = {155},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Markerless multi-view 3D human pose estimation: A survey. <em>ICV</em>, <em>155</em>, 105437. (<a href='https://doi.org/10.1016/j.imavis.2025.105437'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {3D human pose estimation aims to reconstruct the human skeleton of all the individuals in a scene by detecting several body joints. The creation of accurate and efficient methods is required for several real-world applications including animation, human–robot interaction, surveillance systems or sports, among many others. However, several obstacles such as occlusions, random camera perspectives, or the scarcity of 3D labelled data, have been hampering the models’ performance and limiting their deployment in real-world scenarios. The higher availability of cameras has led researchers to explore multi-view solutions due to the advantage of being able to exploit different perspectives to reconstruct the pose. Most existing reviews focus mainly on monocular 3D human pose estimation and a comprehensive survey only on multi-view approaches to determine the 3D pose has been missing since 2012. Thus, the goal of this survey is to fill that gap and present an overview of the methodologies related to 3D pose estimation in multi-view settings, understand what were the strategies found to address the various challenges and also, identify their limitations. According to the reviewed articles, it was possible to find that most methods are fully-supervised approaches based on geometric constraints. Nonetheless, most of the methods suffer from 2D pose mismatches, to which the incorporation of temporal consistency and depth information have been suggested to reduce the impact of this limitation, besides working directly with 3D features can completely surpass this problem but at the expense of higher computational complexity. Models with lower supervision levels were identified to overcome some of the issues related to 3D pose, particularly the scarcity of labelled datasets. Therefore, no method is yet capable of solving all the challenges associated with the reconstruction of the 3D pose. Due to the existing trade-off between complexity and performance, the best method depends on the application scenario. Therefore, further research is still required to develop an approach capable of quickly inferring a highly accurate 3D pose with bearable computation cost. To this goal, techniques such as active learning, methods that learn with a low level of supervision, the incorporation of temporal consistency, view selection, estimation of depth information and multi-modal approaches might be interesting strategies to keep in mind when developing a new methodology to solve this task.},
  archive      = {J_ICV},
  author       = {Ana Filipa Rodrigues Nogueira and Hélder P. Oliveira and Luís F. Teixeira},
  doi          = {10.1016/j.imavis.2025.105437},
  journal      = {Image and Vision Computing},
  month        = {3},
  pages        = {105437},
  shortjournal = {Image Vis. Comput.},
  title        = {Markerless multi-view 3D human pose estimation: A survey},
  volume       = {155},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A small object detection model for drone images based on multi-attention fusion network. <em>ICV</em>, <em>155</em>, 105436. (<a href='https://doi.org/10.1016/j.imavis.2025.105436'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Object detection in aerial images is crucial for various applications, including precision agriculture, urban planning, disaster management, and military surveillance, as it enables the automated identification and localization of ground objects from high-altitude images. However, this field encounters several significant challenges: (1) The uneven distribution of objects; (2) High-resolution aerial images contain numerous small objects and complex backgrounds; (3) Significant variation in object sizes. To address these challenges, this paper proposes a new detection network architecture based on the fusion of multiple attention mechanisms named MAFDet. MAFDet comprises three main components: the multi-attention focusing sub-network, the multi-scale Swin transformer backbone, and the detection head. The multi-attention focusing sub-network generates attention maps to identify regions with dense small objects for precise detection. The multi-scale Swin transformer embeds the efficient multi-scale attention module into the Swin transformer block to extract better multi-layer features and mitigate background interference, thereby significantly enhancing the model’s feature extraction capability. Finally, the detector processes regions with dense small objects and global images separately, subsequently fusing the detection results to produce the final output. Experimental results demonstrate that MAFDet outperforms existing methods on widely used aerial image datasets, VisDrone and UAVDT, achieving improvements in small object detection average precision ( A P s ) of 1.21% and 1.98%, respectively.},
  archive      = {J_ICV},
  author       = {Jie Hu and Ting Pang and Bo Peng and Yongguo Shi and Tianrui Li},
  doi          = {10.1016/j.imavis.2025.105436},
  journal      = {Image and Vision Computing},
  month        = {3},
  pages        = {105436},
  shortjournal = {Image Vis. Comput.},
  title        = {A small object detection model for drone images based on multi-attention fusion network},
  volume       = {155},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Semantic consistency learning for unsupervised multi-modal person re-identification. <em>ICV</em>, <em>155</em>, 105434. (<a href='https://doi.org/10.1016/j.imavis.2025.105434'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Unsupervised multi-modal person re-identification poses significant challenges due to the substantial modality gap and the absence of annotations. Although previous efforts have aimed to bridge this gap by establishing modality correspondences, their focus has been confined to the feature and image level correspondences, neglecting full utilization of semantic information. To tackle these issues, we propose a Semantic Consistency Learning Network (SCLNet) for unsupervised multi-modal person re-identification. SCLNet first predicts pseudo-labels using a hierarchical clustering algorithm, which capitalizes on common semantics to perform mutual refinement across modalities and establishes cross-modality label correspondences based on semantic analysis. Besides, we also design a cross-modality loss that utilizes contrastive learning to acquire modality-invariant features, effectively reducing the inter-modality gap and enhancing the robustness of the model. Furthermore, we construct a new multi-modality dataset named Subway-TM. This dataset not only encompasses visible and infrared modalities but also includes a depth modality, captured by three cameras across 266 identities, comprising 10,645 RGB images, 10,529 infrared images, and 10,529 depth images. To the best of our knowledge, this is the first person re-identification dataset with three modalities. We conduct extensive experiments, utilizing the widely employed person re-identification datasets SYSU-MM01 and RegDB, along with our newly proposed multi-modal Subway-TM dataset. The experimental results show that our proposed method is promising compared to the current state-of-the-art methods.},
  archive      = {J_ICV},
  author       = {Yuxin Zhang and Zhu Teng and Baopeng Zhang},
  doi          = {10.1016/j.imavis.2025.105434},
  journal      = {Image and Vision Computing},
  month        = {3},
  pages        = {105434},
  shortjournal = {Image Vis. Comput.},
  title        = {Semantic consistency learning for unsupervised multi-modal person re-identification},
  volume       = {155},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Accurate LiDAR–camera calibration using feature edges. <em>ICV</em>, <em>155</em>, 105394. (<a href='https://doi.org/10.1016/j.imavis.2024.105394'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Accurate extrinsic calibration is essential for minimizing alignment errors between LiDAR and cameras, ensuring precise sensor data registration, and enhancing the robustness and accuracy of Simultaneous Localization and Mapping (SLAM) systems. Although previous calibration techniques employing plane and point features, as well as neural networks, have shown promise, they are not devoid of limitations. Particularly, prominent point features such as chessboard corners may lack precise counterparts in the sparse point clouds generated by LiDAR. To address these challenges, we introduce a novel LiDAR–camera extrinsic calibration method that leverages edge registration with known correspondences. This approach significantly reduces calibration discrepancies associated with imprecise correspondences and systemic noise, providing a systematic and rigorous framework to improve the precision of extrinsic calibration between LiDAR and camera systems. Additionally, the flexibility of our method allows for the use of common everyday objects, such as boxes, books, or sheets of paper, for calibration purposes, simplifying the procedure and enhancing its practical applicability.},
  archive      = {J_ICV},
  author       = {YunFeng Hua and QinYu Liu and TengFei Jiang and Jian Zhang and WeiWei Xu and Yan Tian},
  doi          = {10.1016/j.imavis.2024.105394},
  journal      = {Image and Vision Computing},
  month        = {3},
  pages        = {105394},
  shortjournal = {Image Vis. Comput.},
  title        = {Accurate LiDAR–camera calibration using feature edges},
  volume       = {155},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Spatial–temporal-channel collaborative feature learning with transformers for infrared small target detection. <em>ICV</em>, <em>154</em>, 105435. (<a href='https://doi.org/10.1016/j.imavis.2025.105435'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Infrared small target detection holds significant importance for real-world applications, particularly in military applications. However, it encounters several notable challenges, such as limited target information. Due to the localized characteristic of Convolutional Neural Networks (CNNs), most methods based on CNNs are inefficient in extracting and preserving global information, potentially leading to the loss of detailed information. In this work, we propose a transformer-based method named Spatial-Temporal-Channel collaborative feature learning network (STC). Recognizing the difficulty in detecting small targets solely based on spatial information, we incorporate temporal and channel information into our approach. Unlike the Vision Transformer used in other vision tasks, our STC comprises three distinct transformer encoders that extract spatial, temporal and channel information respectively, to obtain more accurate representations. Subsequently, a transformer decoder is employed to fuse the three attention features in a way that akin to human vision system. Additionally, we propose a new Semantic-Aware positional encoding method for video clips that incorporate temporal information into positional encoding and is scale-invariant. Through the multiple experiments and comparisons with current methods, we demonstrate the effectiveness of STC in addressing the challenges of infrared small target detection. Our source codes are available at https://github.com/UESTC-nnLab/STC .},
  archive      = {J_ICV},
  author       = {Sicheng Zhu and Luping Ji and Shengjia Chen and Weiwei Duan},
  doi          = {10.1016/j.imavis.2025.105435},
  journal      = {Image and Vision Computing},
  month        = {2},
  pages        = {105435},
  shortjournal = {Image Vis. Comput.},
  title        = {Spatial–temporal-channel collaborative feature learning with transformers for infrared small target detection},
  volume       = {154},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). EMA-GS: Improving sparse point cloud rendering with EMA gradient and anchor upsampling. <em>ICV</em>, <em>154</em>, 105433. (<a href='https://doi.org/10.1016/j.imavis.2025.105433'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The 3D Gaussian Splatting (3D-GS) technique combines 3D Gaussian primitives with differentiable rasterization for real-time high-quality novel view synthesis. However, in sparse regions of the initial point cloud, this often results in blurring and needle-like artifacts owing to the inadequacies of the existing densification criterion. To address this, an innovative approach that utilizes the Exponential Moving Average (EMA) of homodirectional positional gradients as the densification criterion is introduced. Additionally, in the early stages of training, anchors are upsampled near representative locations to infill details into the sparse initial point clouds. Testing on challenging datasets such as Mip-NeRF 360, Tanks and Temples, and DeepBlending, the results demonstrate that the proposed method achieves fine detail recovery without redundant Gaussians, exhibiting superior handling of complex scenes with high-quality reconstruction and without requiring excessive storage. The code will be available upon the acceptance of the article.},
  archive      = {J_ICV},
  author       = {Ding Yuan and Sizhe Zhang and Hong Zhang and Yangyan Deng and Yifan Yang},
  doi          = {10.1016/j.imavis.2025.105433},
  journal      = {Image and Vision Computing},
  month        = {2},
  pages        = {105433},
  shortjournal = {Image Vis. Comput.},
  title        = {EMA-GS: Improving sparse point cloud rendering with EMA gradient and anchor upsampling},
  volume       = {154},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Advancing brain tumor segmentation and grading through integration of FusionNet and IBCO-based ALCResNet. <em>ICV</em>, <em>154</em>, 105432. (<a href='https://doi.org/10.1016/j.imavis.2025.105432'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Brain tumors represent a significant global health challenge, characterized by uncontrolled cerebral cell growth. The variability in size, shape, and anatomical positioning complicates computational classification, which is crucial for effective treatment planning. Accurate detection is essential, as even small diagnostic inaccuracies can significantly increase the mortality risk. Tumor grade stratification is also critical for automated diagnosis; however, current deep learning models often fall short in achieving the desired effectiveness. In this study, we propose an advanced approach that leverages cutting-edge deep learning techniques to improve early detection and tumor severity grading, facilitating automated diagnosis. Clinical bioinformatics datasets are used to source representative brain tumor images, which undergo pre-processing and data augmentation via a Generative Adversarial Network (GAN). The images are then classified using the Adaptive Layer Cascaded ResNet (ALCResNet) model, optimized with the Improved Border Collie Optimization (IBCO) algorithm for enhanced diagnostic accuracy. The integration of FusionNet for precise segmentation and the IBCO-enhanced ALCResNet for optimized feature extraction and classification forms a novel framework. This unique combination ensures not only accurate segmentation but also enhanced precision in grading tumor severity, addressing key limitations of existing methodologies. For segmentation, the FusionNet deep learning model is employed to identify abnormal regions, which are subsequently classified as Meningioma, Glioma, or Pituitary tumors using ALCResNet. Experimental results demonstrate significant improvements in tumor identification and severity grading, with the proposed method achieving superior precision (99.79%) and accuracy (99.33%) compared to existing classifiers and heuristic approaches.},
  archive      = {J_ICV},
  author       = {Abbas Rehman and Gu Naijie and Asma Aldrees and Muhammad Umer and Abeer Hakeem and Shtwai Alsubai and Lucia Cascone},
  doi          = {10.1016/j.imavis.2025.105432},
  journal      = {Image and Vision Computing},
  month        = {2},
  pages        = {105432},
  shortjournal = {Image Vis. Comput.},
  title        = {Advancing brain tumor segmentation and grading through integration of FusionNet and IBCO-based ALCResNet},
  volume       = {154},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Partitioned token fusion and pruning strategy for transformer tracking. <em>ICV</em>, <em>154</em>, 105431. (<a href='https://doi.org/10.1016/j.imavis.2025.105431'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Transformer-based tracking algorithms have shown outstanding performance in the field of object tracking due to their powerful global information capture capability. However, the redundant background information in the search region results in interference and high computational complexity in searching for the tracked object. To address this problem, we design a partitioned token fusion and pruning strategy for one-stream transformer trackers. The strategy can achieve a better balance between information retention and interference reduction, and it can improve tracking robustness while accelerating inference. Specifically, we partition search tokens into high-correlation, medium-correlation, and low-correlation based on their relevance to the object template. The feature information in the medium-correlation part is fused into the high-correlation part. Low-correlation tokens are directly discarded. Through the differentiated partitioned token fusion and pruning strategy, we not only reduce the number of tokens in the input network, thus reducing the high computational cost of the transformer, but also improve the robustness of tracking by retaining the useful information of the medium-relevant features while reducing the weight of the accompanying background noise information. The proposed strategy has been comprehensively evaluated experimentally in several challenging public benchmarks, and the results show that our approach achieves excellent overall performance compared with current state-of-the-art tracking methods.},
  archive      = {J_ICV},
  author       = {Chi Zhang and Yun Gao and Tao Meng and Tao Wang},
  doi          = {10.1016/j.imavis.2025.105431},
  journal      = {Image and Vision Computing},
  month        = {2},
  pages        = {105431},
  shortjournal = {Image Vis. Comput.},
  title        = {Partitioned token fusion and pruning strategy for transformer tracking},
  volume       = {154},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Semantic-aware for point cloud domain adaptation with self-distillation learning. <em>ICV</em>, <em>154</em>, 105430. (<a href='https://doi.org/10.1016/j.imavis.2025.105430'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Unsupervised domain adaptation aims to apply knowledge gained from a label-rich domain, i.e., the source domain, to a label-scare domain, i.e., the target domain. However, direct alignment between the source and the target domains is challenging due to significant distribution differences. This paper introduces a novel unsupervised domain adaptation method for 3D point clouds. Specifically, to better learn the pattern of the target domain, we propose a self-distillation framework that effectively learns feature representations in a large-scale unlabeled target domain while enhancing resilience to noise and variations. Moreover, we propose Asymmetric Transferable Semantic Augmentation (AsymTSA) to bridge the gaps between theory and practical issues by extending the multivariate normal distribution assumption to multivariate skew-normal distribution, and progressively learning the semantic information in the target domain. Comprehensive experiments conducted on two benchmarks, PointDA-10, and GraspNetPC-10, and the results demonstrate the effectiveness and superiority of our method.},
  archive      = {J_ICV},
  author       = {Jiming Yang and Feipeng Da and Ru Hong},
  doi          = {10.1016/j.imavis.2025.105430},
  journal      = {Image and Vision Computing},
  month        = {2},
  pages        = {105430},
  shortjournal = {Image Vis. Comput.},
  title        = {Semantic-aware for point cloud domain adaptation with self-distillation learning},
  volume       = {154},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Transparency and privacy measures of biometric patterns for data processing with synthetic data using explainable artificial intelligence. <em>ICV</em>, <em>154</em>, 105429. (<a href='https://doi.org/10.1016/j.imavis.2025.105429'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper the need of biometric authentication with synthetic data is analyzed for increasing the security of data in each transmission systems. Since more biometric patterns are represented the complexity of recognition changes where low security features are enabled in transmission process. Hence the process of increasing security is carried out with image biometric patterns where synthetic data is created with explainable artificial intelligence technique thereby appropriate decisions are made. Further sample data is generated at each case thereby all changing representations are minimized with increase in original image set values. Moreover the data flows at each identified biometric patterns are increased where partial decisive strategies are followed in proposed approach. Further more complete interpretabilities that are present in captured images or biometric patterns are reduced thus generated data is maximized to all end users. To verify the outcome of proposed approach four scenarios with comparative performance metrics are simulated where from the comparative analysis it is found that the proposed approach is less robust and complex at a rate of 4% and 6% respectively.},
  archive      = {J_ICV},
  author       = {Achyut Shankar and Hariprasath Manoharan and Adil O. Khadidos and Alaa O. Khadidos and Shitharth Selvarajan and S.B. Goyal},
  doi          = {10.1016/j.imavis.2025.105429},
  journal      = {Image and Vision Computing},
  month        = {2},
  pages        = {105429},
  shortjournal = {Image Vis. Comput.},
  title        = {Transparency and privacy measures of biometric patterns for data processing with synthetic data using explainable artificial intelligence},
  volume       = {154},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Efficient and robust multi-camera 3D object detection in bird-eye-view. <em>ICV</em>, <em>154</em>, 105428. (<a href='https://doi.org/10.1016/j.imavis.2025.105428'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Bird's-eye view (BEV) representations are increasingly used in autonomous driving perception due to their comprehensive, unobstructed vehicle surroundings. Compared to transformer or depth based methods, ray transformation based methods are more suitable for vehicle deployment and more efficient. However, these methods typically depend on accurate extrinsic camera parameters, making them vulnerable to performance degradation when calibration errors or installation changes occur. In this work, we follow ray transformation based methods and propose an extrinsic parameters free approach, which reduces reliance on accurate offline camera extrinsic calibration by using a neural network to predict extrinsic parameters online and can effectively improve the robustness of the model. In addition, we propose a multi-level and multi-scale image encoder to better encode image features and adopt a more intensive temporal fusion strategy. Our framework further mainly contains four important designs: (1) a multi-level and multi-scale image encoder, which can leverage multi-scale information on the inter-layer and the intra-layer for better performance, (2) ray-transformation with extrinsic parameters free approach, which can transfers image features to BEV space and lighten the impact of extrinsic disturbance on m-odel's detection performance, (3) an intensive temporal fusion strategy using motion information from five historical frames. (4) a high-performance BEV encoder that efficiently reduces the spatial dimensions of a voxel-based feature map and fuse the multi-scale and the multi-frame BEV features. Experiments on nuScenes show that our best model (R101@900 × 1600) realized competitive 41.7% mAP and 53.8% NDS on the validation set, which outperforming several state-of-the-art visual BEV models in 3D object detection.},
  archive      = {J_ICV},
  author       = {Yuanlong Wang and Hengtao Jiang and Guanying Chen and Tong Zhang and Jiaqing Zhou and Zezheng Qing and Chunyan Wang and Wanzhong Zhao},
  doi          = {10.1016/j.imavis.2025.105428},
  journal      = {Image and Vision Computing},
  month        = {2},
  pages        = {105428},
  shortjournal = {Image Vis. Comput.},
  title        = {Efficient and robust multi-camera 3D object detection in bird-eye-view},
  volume       = {154},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). An ontological approach to investigate the impact of deep convolutional neural networks in anomaly detection of left ventricular hypertrophy using echocardiography images. <em>ICV</em>, <em>154</em>, 105427. (<a href='https://doi.org/10.1016/j.imavis.2025.105427'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Left Ventricular Hypertrophy (LVH) is a critical predictor of cardiovascular disease, making it essential to incorporate it as a fundamental parameter in both diagnostic screening and clinical management. Addressing the need for efficient, accurate, and scalable medical image analysis, we introduce a state-of-the-art preprocessing pipeline coupled with a novel Deep Convolutional Neural Network (DCNN) architecture. This paper details our choice of the HMC-QU dataset, selected for its robustness and its proven efficacy in enhancing model generalization. We also describe innovative preprocessing techniques aimed at improving the quality of input data, thereby boosting the model's feature extraction capabilities. Our multi-disciplinary approach includes deploying a DCNN for automated LVH diagnosis using echocardiography A4C and A2C images. We evaluated the model using architectures based on VGG16, ResNet50, and InceptionV3, where our proposed DCNN exhibited enhanced performance. In our study, 93 out of 162 A4C recordings and 68 out of 130 A2C recordings confirmed the presence of LVH. The novel DCNN model achieved an impressive 99.8% accuracy on the training set and 98.0% on the test set. Comparatively, ResNet50 and InceptionV3 models showed lower accuracy and higher loss values both in training and testing phases. Our results underscore the potential of our DCNN architecture in enhancing the precision of MRI echocardiograms in diagnosing LVH, thereby providing critical support in the screening and treatment of cardiovascular conditions. The high accuracy and minimal losses observed with the novel DCNN model indicate its utility in clinical settings, making it a valuable tool for improving patient outcomes in cardiovascular care.},
  archive      = {J_ICV},
  author       = {Umar Islam and Hathal Salamah Alwageed and Saleh Alyahyan and Manal Alghieth and Hanif Ullah and Naveed Khan},
  doi          = {10.1016/j.imavis.2025.105427},
  journal      = {Image and Vision Computing},
  month        = {2},
  pages        = {105427},
  shortjournal = {Image Vis. Comput.},
  title        = {An ontological approach to investigate the impact of deep convolutional neural networks in anomaly detection of left ventricular hypertrophy using echocardiography images},
  volume       = {154},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Skeleton action recognition via group sparsity constrained variant graph auto-encoder. <em>ICV</em>, <em>154</em>, 105426. (<a href='https://doi.org/10.1016/j.imavis.2025.105426'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Human skeleton action recognition has garnered significant attention from researchers due to its promising performance in real-world applications. Recently, graph neural networks (GNNs) have been applied to this field, with graph convolution networks (GCNs) being commonly utilized to modulate the spatial configuration and temporal dynamics of joints. However, the GCN-based paradigm for skeleton action recognition fails to recognize and disentangle the heterogeneous factors of action representation. Consequently, the learned action features are susceptible to irrelevant factors, hindering further performance enhancement. To address this issue and learn a disentangled action representation, we propose a novel skeleton action recognition method, termed β -bVGAE. The proposed method leverages group sparsity constrained Variant graph auto-encoder, rather than graph convolutional networks, to learn the discriminative features of the skeleton sequence. Extensive experiments conducted on benchmark action recognition datasets demonstrate that our proposed method outperforms existing GCN-based skeleton action recognition methods, highlighting the significant potential of the variant auto-encoder architecture in the field of skeleton action recognition.},
  archive      = {J_ICV},
  author       = {Hongjuan Pei and Jiaying Chen and Shihao Gao and Taisong Jin and Ke Lu},
  doi          = {10.1016/j.imavis.2025.105426},
  journal      = {Image and Vision Computing},
  month        = {2},
  pages        = {105426},
  shortjournal = {Image Vis. Comput.},
  title        = {Skeleton action recognition via group sparsity constrained variant graph auto-encoder},
  volume       = {154},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). EHGFormer: An efficient hypergraph-injected transformer for 3D human pose estimation. <em>ICV</em>, <em>154</em>, 105425. (<a href='https://doi.org/10.1016/j.imavis.2025.105425'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, Transformer-based approaches have demonstrated remarkable success in 3D human pose estimation. However, these methods usually overlook crucial structural information inherent in human skeletal connections. In this paper, we propose a novel hypergraph-injected Transformer-based architecture(EHGFormer). The spatial feature extractor in our model decomposes joint relationships into first-order (joint-to-joint) and potential higher-order (joint-to-hyperedge) connections, and the attention mechanism of the spatial Transformer block, which integrates these relationships, forms the hypergraph-injected spatial attention. In addition, to address the trade-off between inference efficiency and estimation accuracy introduced by the hypergraph-injected spatial attention module, we design a multi-start grouped downsampling and restoration strategy. With this strategy, consistency in the sequence’s input and output order is maintained, while the temporal receptive field is expanded without requiring additional parameters. Furthermore, we propose a hierarchical feature distillation scheme, which applies different distillation strategies for tokens from various positions of the teacher network. This allows the narrower student network to selectively learn from the teacher network, yet improving its accuracy compared to existing feature distillation methods. Extensive experiments show that the proposed method achieves state-of-the-art performance on two benchmark datasets: Human3.6M and MPI-INF-3DHP. Code and models will be available at: https://github.com/Brian417-cup/EHGFormer .},
  archive      = {J_ICV},
  author       = {Siyuan Zheng and Weiqun Cao},
  doi          = {10.1016/j.imavis.2025.105425},
  journal      = {Image and Vision Computing},
  month        = {2},
  pages        = {105425},
  shortjournal = {Image Vis. Comput.},
  title        = {EHGFormer: An efficient hypergraph-injected transformer for 3D human pose estimation},
  volume       = {154},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Synthesizing multilevel abstraction ear sketches for enhanced biometric recognition. <em>ICV</em>, <em>154</em>, 105424. (<a href='https://doi.org/10.1016/j.imavis.2025.105424'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Sketch understanding poses unique challenges for general-purpose vision algorithms due to the sparse and semantically ambiguous nature of sketches. This paper introduces a novel approach to biometric recognition that leverages sketch-based representations of ears, a largely unexplored but promising area in biometric research. Specifically, we address the “ sketch-2-image ” matching problem by synthesizing ear sketches at multiple abstraction levels, achieved through a triplet-loss function adapted to integrate these levels. The abstraction level is determined by the number of strokes used, with fewer strokes reflecting higher abstraction. Our methodology combines sketch representations across abstraction levels to improve robustness and generalizability in matching. Extensive evaluations were conducted on four ear datasets (AMI, AWE, IITDII, and BIPLab) using various pre-trained neural network backbones, showing consistently superior performance over state-of-the-art methods. These results highlight the potential of ear sketch-based recognition, with cross-dataset tests confirming its adaptability to real-world conditions and suggesting applicability beyond ear biometrics.},
  archive      = {J_ICV},
  author       = {David Freire-Obregón and Joao Neves and Žiga Emeršič and Blaž Meden and Modesto Castrillón-Santana and Hugo Proença},
  doi          = {10.1016/j.imavis.2025.105424},
  journal      = {Image and Vision Computing},
  month        = {2},
  pages        = {105424},
  shortjournal = {Image Vis. Comput.},
  title        = {Synthesizing multilevel abstraction ear sketches for enhanced biometric recognition},
  volume       = {154},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A novel framework for diverse video generation from a single video using frame-conditioned denoising diffusion probabilistic model and ConvNeXt-v2. <em>ICV</em>, <em>154</em>, 105422. (<a href='https://doi.org/10.1016/j.imavis.2025.105422'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Denoising Diffusion Probabilistic Model (DDPM) has significantly advanced video generation and synthesis. DDPM relies on extensive datasets for its training process. The study presents a novel method for generating videos from a single video through a frame-conditioned Denoising Diffusion Probabilistic Model (DDPM). Additionally, incorporating the ConvNeXt-V2 model significantly boosts the framework’s feature extraction, improving video generation performance. Addressing the data scarcity challenge in video generation, the proposed model framework exploits a single video’s intrinsic complexities and temporal dynamics to generate diverse and realistic sequences. The model’s ability to generalize motion is demonstrated through thorough quantitative assessments, wherein it is trained on segments of the original video and evaluated on previously unseen frames. Integrating Global Response Normalization and Sigmoid-Weighted Linear Unit (SiLU) activation functions within the DDPM framework has enhanced generated video quality. Comparatively, the proposed model markedly outperforms the Sinfusion model across crucial image quality metrics, achieving a lower Freschet Video Distance (FVD) score of 106.52, lower Learned Perceptual Image Patch Similarity (LPIPS) score of 0.085, higher Structural Similarity Index Measure (SSIM) score of 0.089, higher Nearest-Neighbor-Field (NNF) based diversity (NNFDIV) score of 0.44. Furthermore, the model achieves a Peak Signal to Noise Ratio score of 23.95, demonstrating its strength in preserving image integrity despite noise. The integration of Global Response Normalization and SiLU significantly enhances content synthesis, while ConvNeXt-V2 boosts feature extraction, amplifying the model’s efficacy.},
  archive      = {J_ICV},
  author       = {Ayushi Verma and Tapas Badal and Abhay Bansal},
  doi          = {10.1016/j.imavis.2025.105422},
  journal      = {Image and Vision Computing},
  month        = {2},
  pages        = {105422},
  shortjournal = {Image Vis. Comput.},
  title        = {A novel framework for diverse video generation from a single video using frame-conditioned denoising diffusion probabilistic model and ConvNeXt-v2},
  volume       = {154},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Correlation embedding semantic-enhanced hashing for multimedia retrieval. <em>ICV</em>, <em>154</em>, 105421. (<a href='https://doi.org/10.1016/j.imavis.2025.105421'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Due to its feature extraction and information processing advantages, deep hashing has achieved significant success in multimedia retrieval. Currently, mainstream unsupervised multimedia hashing methods do not incorporate associative relationship information as part of the original features in generating hash codes, and their similarity measurements do not consider the transitivity of similarity. To address these challenges, we propose the Correlation Embedding Semantic-Enhanced Hashing (CESEH) for multimedia retrieval, which primarily consists of a semantic-enhanced similarity construction module and a correlation embedding hashing module. First, the semantic-enhanced similarity construction module generates a semantically enriched similarity matrix by thoroughly exploring similarity adjacency relationships and deep semantic associations within the original data. Next, the correlation embedding hashing module integrates semantic-enhanced similarity information with intra-modal semantic information, achieves precise correlation embedding and preserves semantic information integrity. Extensive experiments on three widely-used datasets demonstrate that the CESEH method outperforms state-of-the-art unsupervised hashing methods in both retrieval accuracy and robustness. The code is available at https://github.com/YunfeiChenMY/CESEH .},
  archive      = {J_ICV},
  author       = {Yunfei Chen and Yitian Long and Zhan Yang and Jun Long},
  doi          = {10.1016/j.imavis.2025.105421},
  journal      = {Image and Vision Computing},
  month        = {2},
  pages        = {105421},
  shortjournal = {Image Vis. Comput.},
  title        = {Correlation embedding semantic-enhanced hashing for multimedia retrieval},
  volume       = {154},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Privacy-preserving explainable AI enable federated learning-based denoising fingerprint recognition model. <em>ICV</em>, <em>154</em>, 105420. (<a href='https://doi.org/10.1016/j.imavis.2025.105420'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Most existing fingerprint recognition methods are based on machine learning and often overlook the privacy and heterogeneity of data when training on large datasets, leading to user information leakage and decreased recognition accuracy. To collaboratively optimize model accuracy under privacy protection, a novel fingerprint recognition algorithm based on artificial intelligence enable federated learning-based Fingerprint Recognition, (AI-Fed-FR) is proposed. First, federated learning is used to iteratively aggregate parameters from various clients, thereby improving the performance of the global model. Second, Explainable AI is applied for denoising low-quality fingerprint images to enhance fingerprint texture structure. Third, to address the fairness issue caused by client heterogeneity, a client scheduling strategy based on reservoir sampling is proposed. Finally, simulation experiments are conducted on three real-world datasets to analyze the effectiveness of AI-Fed-FR. Experimental results show that AI-Fed-FR improves accuracy by 5.32% compared to local learning and by 8.56% compared to the federated averaging algorithm, achieving accuracy close to centralized learning. This study is the first to demonstrate the feasibility of combining federated learning with fingerprint recognition, enhancing the security and scalability of fingerprint recognition algorithms and providing a reference for the application of federated learning in biometric technologies.},
  archive      = {J_ICV},
  author       = {Haewon Byeon and Mohammed E. Seno and Divya Nimma and Janjhyam Venkata Naga Ramesh and Abdelhamid Zaidi and Azzah AlGhamdi and Ismail Keshta and Mukesh Soni and Mohammad Shabaz},
  doi          = {10.1016/j.imavis.2025.105420},
  journal      = {Image and Vision Computing},
  month        = {2},
  pages        = {105420},
  shortjournal = {Image Vis. Comput.},
  title        = {Privacy-preserving explainable AI enable federated learning-based denoising fingerprint recognition model},
  volume       = {154},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Grad-CAM based explanations for multiocular disease detection using xception net. <em>ICV</em>, <em>154</em>, 105419. (<a href='https://doi.org/10.1016/j.imavis.2025.105419'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Age-related macular degeneration (AMD), cataract, diabetic retinopathy (DR) and glaucoma are the four most common ocular conditions that lead to vision loss. Early detection in asymptomatic stages is necessary to alleviate vision loss. Manual diagnosis is costly, tedious, laborious and burdensome; assistive tools such as computer aided diagnosis (CAD) systems can help to alleviate these issues. Existing CAD systems for ocular diseases primarily address a single disease condition, employing disease-specific algorithms that rely on anatomical and morphological characteristics for localization of regions of interest (ROIs). The dependence on exhaustive image processing algorithms for pre-processing, ROI detection and feature extraction often results in overly complex systems prone to errors that affect classifier performance. Conglomerating many such individual diagnostic frameworks, each targeting a single disease, is not a practical solution for detecting multiple ocular diseases, especially in mass screening. Alternatively, a single generic CAD framework modeled as a multiclass problem serves to be useful in such high throughput scenarios, significantly reducing cost, time and manpower. Nevertheless, ambiguities in the overlapping features of multiple classes representing different diseases should be effectively addressed. This paper proposes a segmentation-independent approach based on deep learning (DL) to realize a single framework for the detection of different ocular conditions. The proposed work alleviates the need for pixel-level operations and segmentation techniques specific to different ocular diseases, offering a solution that has an upper hand compared to conventional systems in terms of complexity and accuracy. Further, explainability is incorporated as a value-addition that assures trust and confidence in the model. The system involves automatic feature extraction from full fundus images using Xception, a pre-trained deep model. Xception utilizes depthwise separable convolutions to capture subtle patterns in fundus images, effectively addressing the similarities between clinical indicators, such as drusen in AMD and exudates in DR, which often lead to misdiagnosis. A random over-sampling technique is performed to address class imbalance by equalizing the number of training samples across the classes. These features are fed to extreme gradient boosting (XGB) for classification. This study further aims to unveil the “black box” paradigm of model classification, by leveraging gradient-weighted class activation mapping (Grad-CAM) technique to highlight relevant ROIs. The combination of Xception based feature extraction and XGB classification results in 99.31% accuracy, 99.5% sensitivity, 99.8% specificity, 99.4% F1-score and 99.4% precision. The proposed system can be a promising tool aiding conventional manual screening in primary health care centres and mass screening scenarios for efficiently diagnosing multiple ocular diseases, enhancing personalized and remote eye care, particularly in resource-limited settings. By combining objective performance metrics such as accuracy, sensitivity, and specificity with subjective Grad-CAM visualizations, the system offers a comprehensive evaluation framework, ensuring transparency and building trust in ocular healthcare, making it well-suited for clinical adoption.},
  archive      = {J_ICV},
  author       = {M. Raveenthini and R. Lavanya and Raul Benitez},
  doi          = {10.1016/j.imavis.2025.105419},
  journal      = {Image and Vision Computing},
  month        = {2},
  pages        = {105419},
  shortjournal = {Image Vis. Comput.},
  title        = {Grad-CAM based explanations for multiocular disease detection using xception net},
  volume       = {154},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). FSBI: Deepfake detection with frequency enhanced self-blended images. <em>ICV</em>, <em>154</em>, 105418. (<a href='https://doi.org/10.1016/j.imavis.2025.105418'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Advances in deepfake research have led to the creation of almost perfect image manipulations that are undetectable to the human eye and some deepfake detection tools. Recently, several techniques have been proposed to differentiate deepfakes from real images and videos. This study introduces a frequency enhanced self-blended images (FSBI) approach for deepfake detection. This proposed approach utilizes discrete wavelet transforms (DWT) to extract discriminative features from self-blended images (SBI). The features are then used to train a convolutional network architecture model. SBIs blend the image with itself by introducing several forgery artifacts in a copy of the image before blending it. This prevents the classifier from overfitting specific artifacts by learning more generic representations. These blended images are then fed into the frequency feature extractor to detect artifacts that could not be detected easily in the time domain. The proposed approach was evaluated on FF++ and Celeb-DF datasets, and the obtained results outperformed state-of-the-art techniques using the cross-dataset evaluation protocol, achieving an AUC of 95.49% on Celeb-DF dataset. It also achieved competitive performance in the within-dataset evaluation setup. These results highlight the robustness and effectiveness of our method in addressing the challenging generalization problem inherent in deepfake detection. The code is available at https://github.com/gufranSabri/FSBI .},
  archive      = {J_ICV},
  author       = {Ahmed Abul Hasanaath and Hamzah Luqman and Raed Katib and Saeed Anwar},
  doi          = {10.1016/j.imavis.2025.105418},
  journal      = {Image and Vision Computing},
  month        = {2},
  pages        = {105418},
  shortjournal = {Image Vis. Comput.},
  title        = {FSBI: Deepfake detection with frequency enhanced self-blended images},
  volume       = {154},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). AI-based intelligent hybrid framework (BO-DenseXGB) for multi- classification of brain tumor using MRI. <em>ICV</em>, <em>154</em>, 105417. (<a href='https://doi.org/10.1016/j.imavis.2025.105417'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A brain tumor is one of the most deadly tumors in the world and can affect both adults and children. According to its shape, severity, or region affected, it comes in different types or grades. The precise treatment strategy necessitates the early detection and classification of the correct type and grade of the tumor. Magnetic Resonance imaging (MRI) is the most extensively used medical imaging technique for examining tumors. The manual examination in clinical practices is constrained by the huge amount of data generated by MRI, which makes tumor classification challenging and time-consuming. Hence, automated methods are the need of the hour for precise and timely diagnosis. This paper proposes Artificial Intelligence (AI) based automated framework to classify tumors into meningioma, glioma, and pituitary classes. The proposed framework exploits the hierarchical feature learning capabilities of the Convolutional Neural Network (CNN) in combination with an optimized boosting classifier. Hyper-parameters of the boosting classifier are tuned with Bayesian Optimization. An overall accuracy of 99.02% is obtained during the experimental evaluation of the proposed model using the benchmark Figshare dataset, which comprises 3064 MRI images. The experimental outcomes confirm that the proposed deep learning strategy outperforms the existing approaches in a convincing manner.},
  archive      = {J_ICV},
  author       = {Chandni and Monika Sachdeva and Alok Kumar Singh Kushwaha},
  doi          = {10.1016/j.imavis.2025.105417},
  journal      = {Image and Vision Computing},
  month        = {2},
  pages        = {105417},
  shortjournal = {Image Vis. Comput.},
  title        = {AI-based intelligent hybrid framework (BO-DenseXGB) for multi- classification of brain tumor using MRI},
  volume       = {154},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). AGSAM-net: UAV route planning and visual guidance model for bridge surface defect detection. <em>ICV</em>, <em>154</em>, 105416. (<a href='https://doi.org/10.1016/j.imavis.2025.105416'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Crack width is a critical indicator of bridge structural health. This paper proposes a UAV-based method for detecting bridge surface defects and quantifying crack width, aiming to improve efficiency and accuracy. The system integrates a UAV with a visual navigation system to capture high-resolution images (7322 × 5102 pixels) and GPS data, followed by image resolution computation and plane correction. For crack detection and segmentation, we introduce AGSAM-Net, a multi-class semantic segmentation network enhanced with attention gating to accurately identify and segment cracks at the pixel level. The system processes 8064 × 6048 pixel images in 2.4 s, with a detection time of 0.5 s per 540 × 540 pixel crack bounding box. By incorporating distance data, the system achieves over 90% accuracy in crack width quantification across multiple datasets. The study also explores potential collaboration with robotic arms, offering new insights into automated bridge maintenance.},
  archive      = {J_ICV},
  author       = {Rongji Li and Ziqian Wang},
  doi          = {10.1016/j.imavis.2025.105416},
  journal      = {Image and Vision Computing},
  month        = {2},
  pages        = {105416},
  shortjournal = {Image Vis. Comput.},
  title        = {AGSAM-net: UAV route planning and visual guidance model for bridge surface defect detection},
  volume       = {154},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Image re-identification: Where self-supervision meets vision-language learning. <em>ICV</em>, <em>154</em>, 105415. (<a href='https://doi.org/10.1016/j.imavis.2025.105415'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, large-scale vision-language pre-trained models like CLIP have shown impressive performance in image re-identification (ReID). In this work, we explore whether self-supervision can aid in the use of CLIP for image ReID tasks. Specifically, we propose SVLL-ReID, the first attempt to integrate self-supervision and pre-trained CLIP via two training stages to facilitate the image ReID. We observe that: (1) incorporating language self-supervision in the first training stage can make the learnable text prompts more identity-specific, and (2) incorporating vision self-supervision in the second training stage can make the image features learned by the image encoder more discriminative. These observations imply that: (1) the text prompt learning in the first stage can benefit from the language self-supervision, and (2) the image feature learning in the second stage can benefit from the vision self-supervision. These benefits jointly facilitate the performance gain of the proposed SVLL-ReID. By conducting experiments on six image ReID benchmark datasets without any concrete text labels, we find that the proposed SVLL-ReID achieves the overall best performances compared with state-of-the-arts. Codes will be publicly available at https://github.com/BinWangGzhu/SVLL-ReID .},
  archive      = {J_ICV},
  author       = {Bin Wang and Yuying Liang and Lei Cai and Huakun Huang and Huanqiang Zeng},
  doi          = {10.1016/j.imavis.2025.105415},
  journal      = {Image and Vision Computing},
  month        = {2},
  pages        = {105415},
  shortjournal = {Image Vis. Comput.},
  title        = {Image re-identification: Where self-supervision meets vision-language learning},
  volume       = {154},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Co-salient object detection with consensus mining and consistency cross-layer interactive decoding. <em>ICV</em>, <em>154</em>, 105414. (<a href='https://doi.org/10.1016/j.imavis.2025.105414'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The main goal of co-salient object detection (CoSOD) is to extract a group of notable objects that appear together in the image. The existing methods face two major challenges: the first is that in some complex scenes or in the case of interference by other salient objects, the mining of consensus cues for co-salient objects is inadequate; the second is that other methods input consensus cues from top to bottom into the decoder, which ignores the compactness of the consensus and lacks cross-layer interaction. To solve the above problems, we propose a consensus mining and consistency cross-layer interactive decoding network, called CCNet, which consists of two key components, namely, a consensus cue mining module (CCM) and a consistency cross-layer interactive decoder (CCID). Specifically, the purpose of CCM is to fully mine the cross-consensus clues among the co-salient objects in the image group, so as to achieve the group consistency modeling of the group of images. Furthermore, CCID accepts features of different levels as input and receives semantic information of group consensus from CCM, which is used to guide features of other levels to learn higher-level feature representations and cross-layer interaction of group semantic consensus clues, thereby maintaining the consistency of group consensus cues and enabling accurate co-saliency map prediction. We evaluated the proposed CCNet using four widely accepted metrics across three challenging CoSOD datasets and the experimental results demonstrate that our proposed approach outperforms other existing state-of-the-art CoSOD methods, particularly on the CoSal2015 and CoSOD3k datasets. The results of our method are available at https://github.com/jinghuaipan/CCNet .},
  archive      = {J_ICV},
  author       = {Yanliang Ge and Jinghuai Pan and Junchao Ren and Min He and Hongbo Bi and Qiao Zhang},
  doi          = {10.1016/j.imavis.2025.105414},
  journal      = {Image and Vision Computing},
  month        = {2},
  pages        = {105414},
  shortjournal = {Image Vis. Comput.},
  title        = {Co-salient object detection with consensus mining and consistency cross-layer interactive decoding},
  volume       = {154},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Hierarchical spatiotemporal feature interaction network for video saliency prediction. <em>ICV</em>, <em>154</em>, 105413. (<a href='https://doi.org/10.1016/j.imavis.2025.105413'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Transformer can build effective long-range dependency relationships and has been effectively utilized for video saliency prediction. However, fewer works have been devoted to the design of Transformer-based models for video saliency prediction. Furthermore, the existing Transformer-based models do not sufficiently explore multi-level Transformer features. To address this limitation, we present a novel Hierarchical Spatiotemporal Feature Interaction Network ( i.e. , HSFI-Net), which involves three crucial steps, namely multi-scale feature integration, hierarchical feature enhancement, and semantic-guided saliency prediction. Firstly, the multi-level Transformer-based spatiotemporal features are merged step by step using the multi-scale feature integration (MFI) units. Particularly, each MFI unit successively splits and cross-concatenation of features, promoting the interaction of different-level features. Furthermore, it endows features with multi-scale temporal receptive fields via different time-size kernel-based 3D convolutions. Secondly, the temporal-extended feature enhancement (TFE) unit and channel-correlated feature enhancement (CFE) unit are deployed to conduct hierarchical feature enhancement. Here, the TFE unit and the CFE unit learn rich contextual information from the temporal and channel dimensions respectively, providing powerful representations for visual attention regions in videos. Lastly, we design the semantic-guided saliency prediction (SSP) module to consolidate multi-level spatiotemporal features into the final saliency map, where the semantic information serves as a filter for purifying the fused spatiotemporal feature. We conduct extensive experiments on four challenging video saliency datasets, including DHF1K, Hollywood-2, UCF, and DIEM. The experimental results clearly demonstrate that our saliency model outperforms state-of-the-art methods. The code is available at https://github.com/JYJPush/HSFI-Net .},
  archive      = {J_ICV},
  author       = {Yingjie Jin and Xiaofei Zhou and Zhenjie Zhang and Hao Fang and Ran Shi and Xiaobin Xu},
  doi          = {10.1016/j.imavis.2025.105413},
  journal      = {Image and Vision Computing},
  month        = {2},
  pages        = {105413},
  shortjournal = {Image Vis. Comput.},
  title        = {Hierarchical spatiotemporal feature interaction network for video saliency prediction},
  volume       = {154},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). 1D kernel distillation network for efficient image super-resolution. <em>ICV</em>, <em>154</em>, 105411. (<a href='https://doi.org/10.1016/j.imavis.2024.105411'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, there have been significant strides in single-image super-resolution, especially with the integration of transformers. However, the escalating computational demands of large models pose challenges for deployment on edge devices. Therefore, in pursuit of Efficient Image Super-Resolution (EISR), achieving a better balance between task computational complexity and image fidelity becomes imperative. In this paper, we introduce the 1D kernel distillation network (OKDN). Within this network, we have devised a lightweight 1D Large Kernel (OLK) block, incorporating a more lightweight yet highly effective attention mechanism. This block significantly expands the effective receptive field, enhancing performance while mitigating computational costs. Additionally, we develop a Channel Shift Enhanced Distillation (CSED) block to improve distillation efficiency, allocating more computational resources towards increasing network depth. We utilize methods involving partial channel shifting and global feature supervision (GFS) to further augment the effective receptive field. Furthermore, we introduce learnable Gaussian perturbation convolution (LGPConv) to enhance the model’s feature extraction and performance capabilities while upholding low computational complexity. Experimental results demonstrate that our proposed approach achieves superior results with significantly lower computational complexity compared to state-of-the-art models. The code is available at https://github.com/satvio/OKDN .},
  archive      = {J_ICV},
  author       = {Yusong Li and Longwei Xu and Weibin Yang and Dehua Geng and Mingyuan Xu and Zhiqi Dong and Pengwei Wang},
  doi          = {10.1016/j.imavis.2024.105411},
  journal      = {Image and Vision Computing},
  month        = {2},
  pages        = {105411},
  shortjournal = {Image Vis. Comput.},
  title        = {1D kernel distillation network for efficient image super-resolution},
  volume       = {154},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). GANSD: A generative adversarial network based on saliency detection for infrared and visible image fusion. <em>ICV</em>, <em>154</em>, 105410. (<a href='https://doi.org/10.1016/j.imavis.2024.105410'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Image fusion technology, which integrates infrared images providing valuable contrast information with visible light images rich in texture details, represents an effective and rational approach for object detection and tracking. Previous methods have often neglected crucial information due to a lack of saliency detection and have failed to fully utilize complementary information by separately processing different features from the two original images. To address these limitations and enhance fusion techniques, we propose a generative adversarial network with saliency detection (GANSD) for image fusion through an adversarial process. This approach simplifies the design of fusion rules and improves the quality of fused images. By incorporating saliency detection, GANSD effectively preserves both foreground and background information from the input images. The architecture also integrates complementary information to prevent data loss from the input images. Simultaneously, an attention mechanism within the generator emphasizes the importance of different feature channels. Extensive experiments on two public datasets, TNO and Roadscene, demonstrate that GANSD provides both qualitative and quantitative advantages over nine state-of-the-art (SOTA) methods.},
  archive      = {J_ICV},
  author       = {Yinghua Fu and Zhaofeng Liu and Jiansheng Peng and Rohit Gupta and Dawei Zhang},
  doi          = {10.1016/j.imavis.2024.105410},
  journal      = {Image and Vision Computing},
  month        = {2},
  pages        = {105410},
  shortjournal = {Image Vis. Comput.},
  title        = {GANSD: A generative adversarial network based on saliency detection for infrared and visible image fusion},
  volume       = {154},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Self-ensembling for 3D point cloud domain adaptation. <em>ICV</em>, <em>154</em>, 105409. (<a href='https://doi.org/10.1016/j.imavis.2024.105409'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently 3D point cloud learning has been a hot topic in computer vision and autonomous driving. Due to the fact that it is difficult to manually annotate a qualitative large-scale 3D point cloud dataset, unsupervised domain adaptation (UDA) is popular in 3D point cloud learning which aims to transfer the learned knowledge from the labeled source domain to the unlabeled target domain. Existing methods mainly resort to a deformation reconstruction in the target domain, leveraging the deformable invariance process for generalization and domain adaptation. In this paper, we propose a conceptually new yet simple method, termed as self-ensembling network (SEN) for domain generalization and adaptation. In SEN, we propose a soft classification loss on the source domain and a consistency loss on the target domain to stabilize the feature representations and to capture better invariance in the UDA task. In addition, we extend the pointmixup module on the target domain to increase the diversity of point clouds which further boosts cross domain generalization. Extensive experiments on several 3D point cloud UDA benchmarks show that our SEN outperforms the state-of-the-art methods on both classification and segmentation tasks.},
  archive      = {J_ICV},
  author       = {Qing Li and Xiaojiang Peng and Chuan Yan and Pan Gao and Qi Hao},
  doi          = {10.1016/j.imavis.2024.105409},
  journal      = {Image and Vision Computing},
  month        = {2},
  pages        = {105409},
  shortjournal = {Image Vis. Comput.},
  title        = {Self-ensembling for 3D point cloud domain adaptation},
  volume       = {154},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Lightweight and efficient feature fusion real-time semantic segmentation network. <em>ICV</em>, <em>154</em>, 105408. (<a href='https://doi.org/10.1016/j.imavis.2024.105408'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The increasing demand for real-time performance in semantic segmentation for the field of autonomous driving has prompted a significant focus on the trade-off between speed and accuracy. Recently, many real-time semantic segmentation networks have opted for lightweight classification networks as their backbone. However, their lack of specificity for real-time semantic segmentation tasks compromises their ability to extract advanced semantic information effectively. This paper introduces the LAFFNet, a lightweight and efficient feature-fusion real-time semantic segmentation network. We devised a novel lightweight feature extraction block (LEB) to construct the encoder part, employing a combination of deep convolution and dilated convolution to extract local and global semantic features with minimal parameters, thereby enhancing feature map characterization. Additionally, we propose a coarse feature extractor block (CFEB) to recover lost local details during encoding and improve connectivity between encoding and decoding parts. In the decoding phase, we introduce the bilateral feature fusion block (BFFB), leveraging features from different inference stages to enhance the model’s ability to capture multi-scale features and conduct efficient feature fusion operations. Without pre-training, LAFFNet achieves a processing speed of 63.7 FPS on high-resolution (1024 × 2048) images from the Cityscapes dataset, with an mIoU of 77.06%. On the Camvid dataset, the model performs equally well, reaching 107.4 FPS with an mIoU of 68.29%. Notably, the model contains only 0.96 million parameters, demonstrating its exceptional efficiency in lightweight network design. These results demonstrate that LAFFNet achieves an optimal balance between accuracy and speed, providing an effective and precise solution for real-time semantic segmentation tasks.},
  archive      = {J_ICV},
  author       = {Jie Zhong and Aiguo Chen and Yizhang Jiang and Chengcheng Sun and Yuheng Peng},
  doi          = {10.1016/j.imavis.2024.105408},
  journal      = {Image and Vision Computing},
  month        = {2},
  pages        = {105408},
  shortjournal = {Image Vis. Comput.},
  title        = {Lightweight and efficient feature fusion real-time semantic segmentation network},
  volume       = {154},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Cross-set data augmentation for semi-supervised medical image segmentation. <em>ICV</em>, <em>154</em>, 105407. (<a href='https://doi.org/10.1016/j.imavis.2024.105407'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Medical image semantic segmentation is a fundamental yet challenging research task. However, training a fully supervised model for this task requires a substantial amount of pixel-level annotated data, which poses a significant challenge for annotators due to the necessity of specialized medical expert knowledge. To mitigate the labeling burden, a semi-supervised medical image segmentation model that leverages both a small quantity of labeled data and a substantial amount of unlabeled data has attracted prominent attention. However, the performance of current methods is constrained by the distribution mismatch problem between limited labeled and unlabeled datasets. To address this issue, we propose a cross-set data augmentation strategy aimed at minimizing the feature divergence between labeled and unlabeled data. Our approach involves mixing labeled and unlabeled data, as well as integrating ground truth with pseudo-labels to produce augmented samples. By employing three distinct cross-set data augmentation strategies, we enhance the diversity of the training dataset and fully exploit the perturbation space. Our experimental results on COVID-19 CT data, spinal cord gray matter MRI data and prostate T2-weighted MRI data substantiate the efficacy of our proposed approach. The code has been released at: CDA .},
  archive      = {J_ICV},
  author       = {Qianhao Wu and Xixi Jiang and Dong Zhang and Yifei Feng and Jinhui Tang},
  doi          = {10.1016/j.imavis.2024.105407},
  journal      = {Image and Vision Computing},
  month        = {2},
  pages        = {105407},
  shortjournal = {Image Vis. Comput.},
  title        = {Cross-set data augmentation for semi-supervised medical image segmentation},
  volume       = {154},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Edge guided and fourier attention-based dual interaction network for scene text erasing. <em>ICV</em>, <em>154</em>, 105406. (<a href='https://doi.org/10.1016/j.imavis.2024.105406'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Scene text erasing (STE) aims to remove the text regions and inpaint those regions with reasonable content in the image. It involves a potential task, i.e., scene text segmentation, in implicate or explicate ways. Most previous methods used cascaded or parallel pipelines to segment text in one branch and erase text in another branch. However, they have not fully explored the information between the two subtasks, i.e., using an interactive method to enhance each other. In this paper, we introduce a novel one-stage STE model called Dual Interaction Network (DINet), which encourages interaction between scene text segmentation and scene text erasing in an end-to-end manner. DINet adopts a shared encoder and two parallel decoders for text segmentation and erasing respectively. Specifically, the two decoders interact via an Interaction Enhancement Module (IEM) in each layer, aggregating the residual information from each other. To facilitate effective and efficient mutual enhancement between the dual tasks, we propose a novel Fourier Transform-based Attention Module (FTAM). In addition, we incorporate an Edge-Guided Module (EGM) into the text segmentation branch to better erase the text boundary regions and generate natural-looking images. Extensive experiments demonstrate that the DINet achieves state-of-the-art performances on several benchmarks. Furthermore, the ablation studies indicate the effectiveness and efficiency of our proposed modules in DINet.},
  archive      = {J_ICV},
  author       = {Ran Gong and Anna Zhu and Kun Liu},
  doi          = {10.1016/j.imavis.2024.105406},
  journal      = {Image and Vision Computing},
  month        = {2},
  pages        = {105406},
  shortjournal = {Image Vis. Comput.},
  title        = {Edge guided and fourier attention-based dual interaction network for scene text erasing},
  volume       = {154},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Adversarially enhanced learning (AEL): Robust lightweight deep learning approach for radiology image classification against adversarial attacks. <em>ICV</em>, <em>154</em>, 105405. (<a href='https://doi.org/10.1016/j.imavis.2024.105405'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep learning models perform well in medical image classification, particularly in radiology. However, their vulnerability to adversarial attacks raises concerns about robustness and reliability in clinical applications. To address these concerns, a novel approach for radiology image classification, referred to as Adversarially Enhanced Learning (AEL) has been proposed. This approach introduces a novel deep learning model ConvDepth-InceptNet designed to enhance the robustness and accuracy in radiology image classification through three key phases. In Phase 1, adversarial images are generated to deceive the classifier using the proposed model initially trained for classification. Phase 2 entails re-training the model with a mix of clean and adversarial images, improving its robustness by functioning as a discriminator for both types of images. Phase 3 refines adversarial images with Total Variation Minimization (TVM) denoising before classification by re-trained model. Pre-attack analysis with VGG16, ResNet-50, and XceptionNet achieved 98% accuracy with just 10,946 parameters. Post-attack analysis subjected to attacks such as Fast Gradient Sign Method, Basic Iterative Method, and Projected Gradient Descent, yields an average adversarial accuracy of 94.8%, with standard deviation of 1.6%, and an attack success rate of 3.3%. Comparative analysis with ResNet50, VGG16, and InceptionV3 indicates minimal performance drops. Furthermore, post-defense analysis shows that the adversarial images refined with TVM denoising are evaluated with re-trained model, achieving an outstanding ac- curacy of 98.83%. The combination of denoising techniques (Phase 3) and robust re-training (Phase 2) enhances robustness by providing a layered defense mechanism. The analysis validates the robustness of this approach.},
  archive      = {J_ICV},
  author       = {Anshu Singh and Maheshwari Prasad Singh and Amit Kumar Singh},
  doi          = {10.1016/j.imavis.2024.105405},
  journal      = {Image and Vision Computing},
  month        = {2},
  pages        = {105405},
  shortjournal = {Image Vis. Comput.},
  title        = {Adversarially enhanced learning (AEL): Robust lightweight deep learning approach for radiology image classification against adversarial attacks},
  volume       = {154},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Domain adaptive object detection via synthetically generated intermediate domain and progressive feature alignment. <em>ICV</em>, <em>154</em>, 105404. (<a href='https://doi.org/10.1016/j.imavis.2024.105404'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The domain adaptive object detection problem is to accurately identify objects within varying target domains. The complexity arises from the discrepancies in weather conditions or diverse scenarios across different domains, which would significantly hinder the object detection model to generalize the learned knowledge from the source domain to the target domains. Currently, the teacher-student model with feature alignment is widely used to address this problem. However, most researchers only use the data from the source and target domains. To make the best use of the available data, we propose to generate the intermediate domain images by using a generative model and incorporate these images into the teacher-student model. The intermediate domain inherits the labels from the source domain and has a similar distribution to that of the target domain. To balance the influences of data from different domains on the model, we introduce the Progressive Feature Alignment (PFA) module. This strategy refines the feature alignment process. We align the source domain with the target domain by using a larger weight factor. For the intermediate domain, we use a lower weight factor for alignment with the target domain. The proposed method could significantly improve the performance of domain adaptive object detection as indicated in our experimental results: We achieve 47.9% mAP on Foggy Cityscape (from Cityscape), 63.2% AP on Cityscape (from Sim10k), and 50.6% AP on Cityscape (from KITTI).},
  archive      = {J_ICV},
  author       = {Ding Gao and Qian Wang and Jian Yang and Junlong Wu},
  doi          = {10.1016/j.imavis.2024.105404},
  journal      = {Image and Vision Computing},
  month        = {2},
  pages        = {105404},
  shortjournal = {Image Vis. Comput.},
  title        = {Domain adaptive object detection via synthetically generated intermediate domain and progressive feature alignment},
  volume       = {154},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). SAMNet: Adapting segment anything model for accurate light field salient object detection. <em>ICV</em>, <em>154</em>, 105403. (<a href='https://doi.org/10.1016/j.imavis.2024.105403'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Light field salient object detection (LF SOD) is an important task that aims to segment visually salient objects from the surroundings. However, existing methods still struggle to achieve accurate detection, especially in complex scenes. Recently, segment anything model (SAM) excels in various vision tasks with its strong object segmentation ability and generalization capability, which is suitable for solving the LF SOD challenge. In this paper, we aim to adapt the SAM for accurate LF SOD. Specifically, we propose a network named SAMNet with two adaptation designs. Firstly, to enhance the perception of salient objects, we design a task-oriented multi-scale convolution adapter (MSCA) integrated into SAM’s image encoder. Parameters in the image encoder except MSCA are frozen to balance detection accuracy and computational requirements. Furthermore, to effectively utilize the rich scene information of LF data, we design a data-oriented cross-modal fusion module (CMFM) to fuse SAM features of different modalities. Comprehensive experiments on four benchmark datasets demonstrate the effectiveness of SAMNet over current state-of-the-art methods. In particular, SAMNet achieves the highest F-measures of 0.945, 0.819, 0.868, and 0.898, respectively. To the best of our knowledge, this is the first work that adapts a vision foundation model to LF SOD.},
  archive      = {J_ICV},
  author       = {Xingzheng Wang and Jianbin Wu and Shaoyong Wu and Jiahui Li},
  doi          = {10.1016/j.imavis.2024.105403},
  journal      = {Image and Vision Computing},
  month        = {2},
  pages        = {105403},
  shortjournal = {Image Vis. Comput.},
  title        = {SAMNet: Adapting segment anything model for accurate light field salient object detection},
  volume       = {154},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). TPSFusion: A transformer-based pyramid screening fusion network for 6D pose estimation. <em>ICV</em>, <em>154</em>, 105402. (<a href='https://doi.org/10.1016/j.imavis.2024.105402'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {RGB-D based 6D pose estimation is a key technology for autonomous driving and robotics applications. Recently, methods based on dense correspondence have achieved huge progress. However, it still suffers from heavy computational burden and insufficient combination of two modalities. In this paper, we propose a novel 6D pose estimation algorithm (TPSFusion) which is based on Transformer and multi-level pyramid fusion features. We first introduce a Multi-modal Features Fusion module, which is composed of the Multi-modal Attention Fusion block (MAF) and Multi-level Screening-feature Fusion block (MSF) to enable high-quality cross-modality information interaction. Subsequently, we introduce a new weight estimation branch to calculate the contribution of different keypoints. Finally, our method has competitive results on YCB-Video, LineMOD, and Occlusion LineMOD datasets.},
  archive      = {J_ICV},
  author       = {Jiaqi Zhu and Bin Li and Xinhua Zhao},
  doi          = {10.1016/j.imavis.2024.105402},
  journal      = {Image and Vision Computing},
  month        = {2},
  pages        = {105402},
  shortjournal = {Image Vis. Comput.},
  title        = {TPSFusion: A transformer-based pyramid screening fusion network for 6D pose estimation},
  volume       = {154},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). An active transfer learning framework for image classification based on maximum differentiation classifier. <em>ICV</em>, <em>154</em>, 105401. (<a href='https://doi.org/10.1016/j.imavis.2024.105401'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep learning has been extensively adopted across various domains, yielding satisfactory outcomes. However, it heavily relies on extensive labeled datasets, collecting data labels is expensive and time-consuming. We propose a novel framework called Active Transfer Learning (ATL) to address this issue. The ATL framework consists of Active Learning (AL) and Transfer Learning (TL). AL queries the unlabeled samples with high inconsistency by Maximum Differentiation Classifier (MDC). The MDC pulls the discrepancy between the labeled data and their augmentations to select and annotate the informative samples. Additionally, we also explore the potential of incorporating TL techniques. The TL comprises pre-training and fine-tuning. The former learns knowledge from the origin-augmentation domain to pre-train the model, while the latter leverages the acquired knowledge for the downstream tasks. The results indicate that the combination of TL and AL exhibits complementary effects, while the proposed ATL framework outperforms state-of-the-art methods in terms of accuracy, precision, recall, and F1-score.},
  archive      = {J_ICV},
  author       = {Peng Zan and Yuerong Wang and Haohao Hu and Wanjun Zhong and Tianyu Han and Jingwei Yue},
  doi          = {10.1016/j.imavis.2024.105401},
  journal      = {Image and Vision Computing},
  month        = {2},
  pages        = {105401},
  shortjournal = {Image Vis. Comput.},
  title        = {An active transfer learning framework for image classification based on maximum differentiation classifier},
  volume       = {154},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Robust auxiliary modality is beneficial for video-based cloth-changing person re-identification. <em>ICV</em>, <em>154</em>, 105400. (<a href='https://doi.org/10.1016/j.imavis.2024.105400'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The core of video-based cloth-changing person re-identification is the extraction of cloth-irrelevant features, such as body shape, face, and gait. Most current methods rely on auxiliary modalities to help the model focus on these features. Although these modalities can resist the interference of clothing appearance, they are not robust against cloth-changing, which affects model recognition. The joint point information of pedestrians was considered to better resist the impact of cloth-changing; however, it contained limited pedestrian discrimination information. In contrast, the silhouettes had rich pedestrian discrimination information and could resist interference from clothing appearance but were vulnerable to cloth-changing. Therefore, we combined these two modalities to construct a more robust modality that minimized the impact of clothing on the model. We designed different usage methods for the temporal and spatial aspects based on the characteristics of the fusion modality to enhance the model for extracting cloth-irrelevant features. Specifically, at the spatial level, we developed a guiding method retaining fine-grained, cloth-irrelevant features while using fused features to reduce the focus on cloth-relevant features in the original image. At the temporal level, we designed a fusion method that combined action features from the silhouette and joint point sequences, resulting in more robust action features for cloth-changing pedestrians. Experiments on two video-based cloth-changing datasets, CCPG-D and CCVID, indicated that our proposed model outperformed existing state-of-the-art methods. Additionally, tests on the gait dataset CASIA-B demonstrated that our model achieved optimal average precision.},
  archive      = {J_ICV},
  author       = {Youming Chen and Ting Tuo and Lijun Guo and Rong Zhang and Yirui Wang and Shangce Gao},
  doi          = {10.1016/j.imavis.2024.105400},
  journal      = {Image and Vision Computing},
  month        = {2},
  pages        = {105400},
  shortjournal = {Image Vis. Comput.},
  title        = {Robust auxiliary modality is beneficial for video-based cloth-changing person re-identification},
  volume       = {154},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). CPFusion: A multi-focus image fusion method based on closed-loop regularization. <em>ICV</em>, <em>154</em>, 105399. (<a href='https://doi.org/10.1016/j.imavis.2024.105399'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The purpose of Multi-Focus Image Fusion (MFIF) is to extract the clear portions from multiple blurry images with complementary features to obtain a fully focused image, which is considered a prerequisite for other advanced visual tasks. With the development of deep learning technologies, significant breakthroughs have been achieved in multi-focus image fusion. However, most existing methods still face challenges related to detail information loss and misjudgment in boundary regions. In this paper, we propose a method called CPFusion for MFIF. On one hand, to fully preserve all detail information from the source images, we utilize an Invertible Neural Network (INN) for feature information transfer. The strong feature retention capability of INN allows for better preservation of the complementary features of the source images. On the other hand, to enhance the network’s performance in image fusion, we design a closed-loop structure to guide the fusion process. Specifically, during the training process, the forward operation of the network is used to learn the mapping from source images to fused images and decision maps, while the backward operation simulates the degradation of the focused image back to the source images. The backward operation serves as an additional constraint to guide the performance of the network’s forward operation. To achieve more natural fusion results, our network simultaneously generates an initial fused image and a decision map, utilizing the decision map to retain the details of the source images, while the initial fused image is employed to improve the visual effects of the decision map fusion method in boundary regions. Extensive experimental results demonstrate that the proposed method achieves excellent results in both subjective visual quality and objective metric assessments.},
  archive      = {J_ICV},
  author       = {Hao Zhai and Peng Chen and Nannan Luo and Qinyu Li and Ping Yu},
  doi          = {10.1016/j.imavis.2024.105399},
  journal      = {Image and Vision Computing},
  month        = {2},
  pages        = {105399},
  shortjournal = {Image Vis. Comput.},
  title        = {CPFusion: A multi-focus image fusion method based on closed-loop regularization},
  volume       = {154},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Learning to estimate 3D interactive two-hand poses with attention perception. <em>ICV</em>, <em>154</em>, 105398. (<a href='https://doi.org/10.1016/j.imavis.2024.105398'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {3D hand pose estimation has attracted increasing research interest due to its broad real-world applications. While encouraging performance has been achieved in single-hand cases, 3D hand-pose estimation of two interactive hands from RGB images still faces two challenging problems: severe intra-hand and inter-hand occlusion and ill-posed projection from 2D hand images to 3D hand joints. To address this, in this paper, we propose a Decoupled Dual-branch Attention Network (DDANet) for 3D interactive two-hand pose estimation. First, we extract multiscale shallow feature maps via a ResNet backbone. Then, we simultaneously learn the context-aware 2D visual and 3D depth features of two interactive hands via two separate attention branches to extensively exploit the two-hand occluded semantic information from RGB images. After that, we define learnable feature vectors to perceive the 3D spatial positions of two-hand joints by interacting them with both 2D visual and 3D depth feature maps. In this way, ill-posed hand-joint positions can be characterized in 3D spaces. Furthermore, we refine the 3D hand-joint spatial positions by capturing the underlying hand-joint connections via GCN learning for 3D two-hand pose estimation. Experimental results on five public datasets show that the proposed DDANet outperforms most state-of-the-art methods with promising generalization.},
  archive      = {J_ICV},
  author       = {Wai Keung Wong and Hao Liang and Hongkun Sun and Weijun Sun and Haoliang Yuan and Shuping Zhao and Lunke Fei},
  doi          = {10.1016/j.imavis.2024.105398},
  journal      = {Image and Vision Computing},
  month        = {2},
  pages        = {105398},
  shortjournal = {Image Vis. Comput.},
  title        = {Learning to estimate 3D interactive two-hand poses with attention perception},
  volume       = {154},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Rotating-YOLO: A novel YOLO model for remote sensing rotating object detection. <em>ICV</em>, <em>154</em>, 105397. (<a href='https://doi.org/10.1016/j.imavis.2024.105397'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Satellite remote sensing images are characterized by large rotation angles and dense targets, which result in less than satisfactory detection accuracy for existing remote sensing target detectors. To tackle these challenges, this paper introduces an object detection algorithm called Rotating-YOLO, which ensures the detection accuracy of remote sensing targets while also reducing the number of model parameters. Initially, an efficient multi-branch feature fusion (EMFF) is designed to filter out redundant feature information, thereby enhancing the model’s efficiency in feature extraction and fusion. Subsequently, to address the issue of sample imbalance in remote sensing images, this paper introduces angular parameters and adopts rotated bounding boxes to decrease the interference of background noise on the detection task. Additionally, the rotated bounding boxes are transformed into Gaussian distributions, and a new loss function named GaussianLoss is designed to calculate the loss between Gaussian distributions, assisting the model in better learning the size and orientation features of targets, thus improving detection accuracy. Finally, the efficient multi-scale attention (EMA) mechanism is embedded in the model’s neck in a residual form, and low-level feature extraction layers and corresponding detection heads are added to the backbone network to enhance the detection accuracy of small targets. Experimental results demonstrate that compared to the baseline model YOLOv8, the Rotating-YOLO model has reduced the number of parameters by 33.25% and increased the mean average precision (mAP) by 1.4%.},
  archive      = {J_ICV},
  author       = {Zhiguo Liu and Yuqi Chen and Yuan Gao},
  doi          = {10.1016/j.imavis.2024.105397},
  journal      = {Image and Vision Computing},
  month        = {2},
  pages        = {105397},
  shortjournal = {Image Vis. Comput.},
  title        = {Rotating-YOLO: A novel YOLO model for remote sensing rotating object detection},
  volume       = {154},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). MD-mamba: Feature extractor on 3D representation with multi-view depth. <em>ICV</em>, <em>154</em>, 105396. (<a href='https://doi.org/10.1016/j.imavis.2024.105396'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {3D sensors provide rich depth information and are widely used across various fields, making 3D vision a hot topic of research. Point cloud data, as a crucial type of 3D data, offers precise three-dimensional coordinate information and is extensively utilized in numerous domains, especially in robotics. However, the unordered and unstructured nature of point cloud data poses a significant challenge for feature extraction. Traditional methods have relied on designing complex local feature extractors to achieve feature extraction, but these approaches have reached a performance bottleneck. To address these challenges, this paper introduces MD-Mamba, a novel network that enhances point cloud feature extraction by integrating multi-view depth maps. Our approach leverages multi-modal learning, treating the multi-view depth maps as an additional global feature modality. By fusing these with locally extracted point cloud features, we achieve richer and more distinctive representations. We utilize an innovative feature extraction strategy, performing real projections of point clouds and treating multi-view projections as video streams. This method captures dynamic features across viewpoints using a specially designed Mamba network. Additionally, the incorporation of the Siamese Cluster module optimizes feature spacing, improving class differentiation. Extensive evaluations on ModelNet40, ShapeNetPart, and ScanObjectNN datasets validate the effectiveness of MD-Mamba, setting a new benchmark for multi-modal feature extraction in point cloud analysis.},
  archive      = {J_ICV},
  author       = {Qihui Li and Zongtan Li and Lianfang Tian and Qiliang Du and Guoyu Lu},
  doi          = {10.1016/j.imavis.2024.105396},
  journal      = {Image and Vision Computing},
  month        = {2},
  pages        = {105396},
  shortjournal = {Image Vis. Comput.},
  title        = {MD-mamba: Feature extractor on 3D representation with multi-view depth},
  volume       = {154},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). PAdapter: Adapter combined with prompt for image and video classification. <em>ICV</em>, <em>154</em>, 105395. (<a href='https://doi.org/10.1016/j.imavis.2024.105395'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In computer vision, parameter-efficient transfer learning has become an extensively used technology. Adapter is one of the commonly used basic modules, and its simplicity and efficiency have been widely proven. In the case of freezing the network backbone, only fine-tuning additional adapters can often achieve similar or even better results with lower computational costs compared to fully fine-tuning. However, the bottleneck structure of Adapter leads to a non-negligible information loss, thereby limiting the performance of Adapter. To alleviate this problem, this work proposes a plug-and-play lightweight module called PAdapter, which is a Prompt-combined Adapter that can achieve parameter-efficient transfer learning on image classification and video action recognition tasks. PAdapter is improved based on Adapter, and Prompt is introduced at the bottleneck to supplement the information that may be lost. Specifically, in the bottleneck structure of Adapter, we concatenate a learnable Prompt with bottleneck features at dimension D to supplement information and even enhance the visual expression ability of bottleneck features. Many experiments on image classification and video action recognition show that PAdapter achieves or exceeds the accuracy of full fine-tuning models with less than 2% extra parameters updated. For example, on the SSv2 and HMDB-51 datasets, PAdapter improves the accuracy by 5.49% and 16.68% respectively compared to full fine-tuning. And in almost all experiments, our PAdapter achieved higher accuracy than Adapter with similar number of tunable parameters. Code is available at https://github.com/owlholy/PAdapter .},
  archive      = {J_ICV},
  author       = {Youwei Li and Junyong Ye and Xubin Wen and Guangyi Xu and Jingjing Wang and Xinyuan Liu},
  doi          = {10.1016/j.imavis.2024.105395},
  journal      = {Image and Vision Computing},
  month        = {2},
  pages        = {105395},
  shortjournal = {Image Vis. Comput.},
  title        = {PAdapter: Adapter combined with prompt for image and video classification},
  volume       = {154},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Class-discriminative domain generalization for semantic segmentation. <em>ICV</em>, <em>154</em>, 105393. (<a href='https://doi.org/10.1016/j.imavis.2024.105393'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Existing domain generalization semantic segmentation methods aim to improve the generalization ability by learning domain-invariant information for generalizing well on unseen domains. However, these methods ignore the class discriminability of models, which may lead to a class confusion problem. In this paper, a class-discriminative domain generalization (CDDG) approach is proposed to simultaneously alleviate the distribution shift and class confusion for semantic segmentation. Specifically, a dual prototypical contrastive learning module is proposed. Since the high-frequency component is consistent across different domains, a class-text-guided high-frequency prototypical contrastive learning is proposed. It uses text embeddings as prior knowledge for guiding the learning of high-frequency prototypical representation from high-frequency components to mine domain-invariant information and further improve the generalization ability. However, the domain-specific information may also contain label-related information which refers to the discrimination of a specific class. Thus, only learning the domain-invariant information may limit the class discriminability of models. To address this issue, a low-frequency prototypical contrastive learning is proposed to learn the class-discriminative representation from low-frequency components since it is more domain-specific across different domains. Finally, the class-discriminative representation and high-frequency prototypical representation are fused to simultaneously improve the generalization ability and class discriminability of the model. Extensive experiments demonstrate that the proposed approach outperforms current methods on single- and multi-source domain generalization benchmarks.},
  archive      = {J_ICV},
  author       = {Muxin Liao and Shishun Tian and Yuhang Zhang and Guoguang Hua and Rong You and Wenbin Zou and Xia Li},
  doi          = {10.1016/j.imavis.2024.105393},
  journal      = {Image and Vision Computing},
  month        = {2},
  pages        = {105393},
  shortjournal = {Image Vis. Comput.},
  title        = {Class-discriminative domain generalization for semantic segmentation},
  volume       = {154},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Understanding document images by introducing explicit semantic information and short-range information interaction. <em>ICV</em>, <em>154</em>, 105392. (<a href='https://doi.org/10.1016/j.imavis.2024.105392'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Methods on the document visual question answering (DocVQA) task have achieved great success by using pre-trained multimodal models. However, two issues are limiting their performances from further improvement. On the one hand, previous methods didn't use explicit semantic information for answer prediction. On the other hand, these methods predict answers only based on global information interaction results and generate low-quality answers. To address the above issues, in this paper, we propose to utilize document semantic segmentation to introduce explicit semantic information of documents into the DocVQA task and design a star-shaped topology structure to enable the interaction of different tokens in short-range contexts. This way, we can obtain token representations with richer multimodal and contextual information for the DocVQA task. With these two strategies, our method can achieve 0.8430 ANLS (Average Normalized Levenshtein Similarity) on the test set of the DocVQA dataset, demonstrating the effectiveness of our method.},
  archive      = {J_ICV},
  author       = {Yufeng Cheng and Dongxue Wang and Shuang Bai and Jingkai Ma and Chen Liang and Kailong Liu and Tao Deng},
  doi          = {10.1016/j.imavis.2024.105392},
  journal      = {Image and Vision Computing},
  month        = {2},
  pages        = {105392},
  shortjournal = {Image Vis. Comput.},
  title        = {Understanding document images by introducing explicit semantic information and short-range information interaction},
  volume       = {154},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Enhancing weakly supervised semantic segmentation with efficient and robust neighbor-attentive superpixel aggregation. <em>ICV</em>, <em>154</em>, 105391. (<a href='https://doi.org/10.1016/j.imavis.2024.105391'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Image-level Weakly-Supervised Semantic Segmentation (WSSS) has become prominent as a technique that utilizes readily available image-level supervisory information. However, traditional methods that rely on pseudo-segmentation labels derived from Class Activation Maps (CAMs) are limited in terms of segmentation accuracy, primarily due to the incomplete nature of CAMs. Despite recent advancements in improving the comprehensiveness of CAM-derived pseudo-labels, challenges persist in handling ambiguity at object boundaries, and these methods also tend to be computationally intensive. To address these challenges, we propose a novel framework called Neighbor-Attentive Superpixel Aggregation (NASA). Inspired by the effectiveness of superpixel segmentation in homogenizing images through color and texture analysis, NASA enables the transformation from superpixel-wise to pixel-wise pseudo-labels. This approach significantly reduces semantic uncertainty at object boundaries and alleviates the computational overhead associated with direct pixel-wise label generation from CAMs. Besides, we introduce a superpixel augmentation strategy to enhance the model’s discrimination capabilities across different superpixels. Empirical studies demonstrate the superiority of NASA over existing WSSS methodologies. On the PASCAL VOC 2012 and MS COCO 2014 datasets, NASA achieves impressive mIoU scores of 73.5% and 46.4%, respectively.},
  archive      = {J_ICV},
  author       = {Chen Wang and Huifang Ma and Di Zhang and Xiaolong Li and Zhixin Li},
  doi          = {10.1016/j.imavis.2024.105391},
  journal      = {Image and Vision Computing},
  month        = {2},
  pages        = {105391},
  shortjournal = {Image Vis. Comput.},
  title        = {Enhancing weakly supervised semantic segmentation with efficient and robust neighbor-attentive superpixel aggregation},
  volume       = {154},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Enriching visual feature representations for vision–language tasks using spectral transforms. <em>ICV</em>, <em>154</em>, 105390. (<a href='https://doi.org/10.1016/j.imavis.2024.105390'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper presents a novel approach to enrich visual feature representations for vision–language tasks, such as image classification and captioning, by incorporating spectral transforms. Although spectral transforms have been widely utilized in signal processing, their application in deep learning has been relatively under-explored. We conducted extensive experiments on various transforms, including the Discrete Fourier Transform (DFT), Discrete Cosine Transform, Discrete Hartley Transform, and Hadamard Transform. Our findings highlight the effectiveness of the DFT, mainly when using the magnitude of complex outputs, in enriching visual features. The proposed method, validated on the MS COCO and Kylberg datasets, demonstrates superior performance compared to previous models, with a 4.8% improvement in CIDEr scores for image captioning tasks. Additionally, our approach enhances caption diversity by up to 3.1% and improves generation speed by up to 2% in Transformer models. These results underscore the potential of spectral feature enrichment in advancing vision–language tasks.},
  archive      = {J_ICV},
  author       = {Oscar Ondeng and Heywood Ouma and Peter Akuon},
  doi          = {10.1016/j.imavis.2024.105390},
  journal      = {Image and Vision Computing},
  month        = {2},
  pages        = {105390},
  shortjournal = {Image Vis. Comput.},
  title        = {Enriching visual feature representations for vision–language tasks using spectral transforms},
  volume       = {154},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Exploring underwater image quality: A review of current methodologies and emerging trends. <em>ICV</em>, <em>154</em>, 105389. (<a href='https://doi.org/10.1016/j.imavis.2024.105389'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The complex underwater environment often leads to issues such as light scattering, color distortion, structural blurring, and noise interference in underwater images, hindering accurate scene representation. Numerous algorithms have been devised for underwater image recovery and enhancement, yet their outcomes exhibit significant variability. Thus, evaluating the quality of underwater images effectively is crucial for assessing these algorithms. This paper provides an overview of research on Underwater Image Quality Assessment (UIQA) by examining its methodologies, challenges, and future trends. Initially, the imaging principle of underwater images is introduced to summarize the primary factors affecting their quality. Subsequently, publicly available underwater image databases and UIQA methods are systematically classified and analyzed. Furthermore, extensive experimental comparisons are conducted to evaluate the performance of published quality assessment algorithms and discuss the relationship between perceived quality and utility in underwater images. Lastly, future trends in UIQA research are anticipated.},
  archive      = {J_ICV},
  author       = {Xiaoyi Xu and Hui Cai and Mingjie Wang and Weiling Chen and Rongxin Zhang and Tiesong Zhao},
  doi          = {10.1016/j.imavis.2024.105389},
  journal      = {Image and Vision Computing},
  month        = {2},
  pages        = {105389},
  shortjournal = {Image Vis. Comput.},
  title        = {Exploring underwater image quality: A review of current methodologies and emerging trends},
  volume       = {154},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Spatial–temporal sequential network for anomaly detection based on long short-term magnitude representation. <em>ICV</em>, <em>154</em>, 105388. (<a href='https://doi.org/10.1016/j.imavis.2024.105388'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Notable advancements have been made in the field of video anomaly detection in recent years. The majority of existing methods approach the problem as a weakly-supervised classification problem based on multi-instance learning. However, the identification of key clips in this context is less precise due to a lack of effective connection between the spatial and temporal information in the video clips. The proposed solution to this issue is the Spatial-Temporal Sequential Network (STSN), which employs the Long Short-Term Magnitude Representation (LST-MR). The processing of spatial and temporal information is conducted in a sequential manner within a spatial–temporal sequential structure, with the objective of enhancing temporal localization performance through the utilization of spatial information. Furthermore, the long short-term magnitude representation is employed in spatial and temporal graphs to enhance the identification of key clips from both global and local perspectives. The combination of classification loss and distance loss is employed with magnitude guidance to reduce the omission of anomalous behaviors. The results on three widely used datasets: UCF-Crime, ShanghaiTech, and XD-Violence, demonstrate that the proposed method performs favorably when compared to existing methods.},
  archive      = {J_ICV},
  author       = {Zhongyue Wang and Ying Chen},
  doi          = {10.1016/j.imavis.2024.105388},
  journal      = {Image and Vision Computing},
  month        = {2},
  pages        = {105388},
  shortjournal = {Image Vis. Comput.},
  title        = {Spatial–temporal sequential network for anomaly detection based on long short-term magnitude representation},
  volume       = {154},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). HSIRMamba: An effective feature learning for hyperspectral image classification using residual mamba. <em>ICV</em>, <em>154</em>, 105387. (<a href='https://doi.org/10.1016/j.imavis.2024.105387'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep learning models have recently demonstrated outstanding results in classifying hyperspectral images (HSI). The Transformer model is among the various deep learning models that have received increasing interest due to its superior ability to simulate the long-term dependence of spatial-spectral information in HSI. Due to its self-attention mechanism, the Transformer exhibits quadratic computational complexity, which makes it heavier than other models and limits its application in the processing of HSI. Fortunately, the newly developed state space model Mamba exhibits excellent computing effectiveness and achieves Transformer-like modeling capabilities. Therefore, we propose a novel enhanced Mamba-based model called HSIRMamba that integrates residual operations into the Mamba architecture by combining the power of Mamba and the residual network to extract the spectral properties of HSI more effectively. It also includes a concurrent dedicated block for spatial analysis using a convolutional neural network. HSIRMamba extracts more accurate features with low computational power, making it more powerful than transformer-based models. HSIRMamba was tested on three majorly used HSI Datasets-Indian Pines, Pavia University, and Houston 2013. The experimental results demonstrate that the proposed method achieves competitive results compared to state-of-the-art methods.},
  archive      = {J_ICV},
  author       = {Rajat Kumar Arya and Siddhant Jain and Pratik Chattopadhyay and Rajeev Srivastava},
  doi          = {10.1016/j.imavis.2024.105387},
  journal      = {Image and Vision Computing},
  month        = {2},
  pages        = {105387},
  shortjournal = {Image Vis. Comput.},
  title        = {HSIRMamba: An effective feature learning for hyperspectral image classification using residual mamba},
  volume       = {154},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multiangle feature fusion network for style transfer. <em>ICV</em>, <em>154</em>, 105386. (<a href='https://doi.org/10.1016/j.imavis.2024.105386'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, arbitrary style transfer has gained a lot of attention from researchers. Although existing methods achieve good results, the generated images are usually biased towards styles, resulting in images with artifacts and repetitive patterns. To address the above problems, we propose a multi-angle feature fusion network for style transfer (MAFST). MAFST consists of a Multi-Angle Feature Fusion module (MAFF), a Multi-Scale Style Capture module (MSSC), multi-angle loss, and a content temporal consistency loss. MAFF can process the captured features from channel level and pixel level, and feature fusion is performed both locally and globally. MSSC processes the shallow style features and optimize generated images. To guide the model to focus on local features, we introduce a multi-angle loss. The content temporal consistency loss extends image style transfer to video style transfer. Extensive experiments have demonstrated that our proposed MAFST can effectively avoid images with artifacts and repetitive patterns. MAFST achieves advanced performance.},
  archive      = {J_ICV},
  author       = {Zhenshan Hu and Bin Ge and Chenxing Xia},
  doi          = {10.1016/j.imavis.2024.105386},
  journal      = {Image and Vision Computing},
  month        = {2},
  pages        = {105386},
  shortjournal = {Image Vis. Comput.},
  title        = {Multiangle feature fusion network for style transfer},
  volume       = {154},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Enhancing 3D object detection in autonomous vehicles based on synthetic virtual environment analysis. <em>ICV</em>, <em>154</em>, 105385. (<a href='https://doi.org/10.1016/j.imavis.2024.105385'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Autonomous Vehicles (AVs) rely on real-time processing of natural images and videos for scene understanding and safety assurance through proactive object detection. Traditional methods have primarily focused on 2D object detection, limiting their spatial understanding. This study introduces a novel approach by leveraging 3D object detection in conjunction with augmented reality (AR) ecosystems for enhanced real-time scene analysis. Our approach pioneers the integration of a synthetic dataset, designed to simulate various environmental, lighting, and spatiotemporal conditions, to train and evaluate an AI model capable of deducing 3D bounding boxes. This dataset, with its diverse weather conditions and varying camera settings, allows us to explore detection performance in highly challenging scenarios. The proposed method also significantly improves processing times while maintaining accuracy, offering competitive results in conditions previously considered difficult for object recognition. The combination of 3D detection within the AR framework and the use of synthetic data to tackle environmental complexity marks a notable contribution to the field of AV scene analysis.},
  archive      = {J_ICV},
  author       = {Vladislav Li and Ilias Siniosoglou and Thomai Karamitsou and Anastasios Lytos and Ioannis D. Moscholios and Sotirios K. Goudos and Jyoti S. Banerjee and Panagiotis Sarigiannidis and Vasileios Argyriou},
  doi          = {10.1016/j.imavis.2024.105385},
  journal      = {Image and Vision Computing},
  month        = {2},
  pages        = {105385},
  shortjournal = {Image Vis. Comput.},
  title        = {Enhancing 3D object detection in autonomous vehicles based on synthetic virtual environment analysis},
  volume       = {154},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). APOVIS: Automated pixel-level open-vocabulary instance segmentation through integration of pre-trained vision-language models and foundational segmentation models. <em>ICV</em>, <em>154</em>, 105384. (<a href='https://doi.org/10.1016/j.imavis.2024.105384'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, substantial advancements have been achieved in vision-language integration and image segmentation, particularly through the use of pre-trained models like BERT and Vision Transformer (ViT). Within the domain of open-vocabulary instance segmentation (OVIS), accurately identifying an instance's positional information is critical, as it directly influences the precision of subsequent segmentation tasks. However, many existing methods rely on supplementary networks to generate pseudo-labels, such as multiple anchor frames containing object positional information. While these pseudo-labels aid visual language models in recognizing the absolute position of objects, they often compromise the overall efficiency and performance of the OVIS pipeline. In this study, we introduce a novel Automated Pixel-level OVIS (APOVIS) framework aimed at enhancing OVIS. Our approach automatically generates pixel-level annotations by leveraging the matching capabilities of pre-trained vision-language models for image-text pairs alongside a foundational segmentation model that accepts multiple prompts (e.g., points or anchor boxes) to guide the segmentation process. Specifically, our method first utilizes a pre-trained vision-language model to match instances within image-text pairs to identify relative positions. Next, we employ activation maps to visualize the instances, enabling us to extract instance location information and generate pseudo-label prompts that direct the segmentation process. These pseudo-labels then guide the segmentation model to execute pixel-level segmentation, enhancing both the accuracy and generalizability of object segmentation across images. Extensive experimental results demonstrate that our model significantly outperforms current state-of-the-art models in object detection accuracy and pixel-level instance segmentation on the COCO dataset. Additionally, the generalizability of our approach is validated through image-text pair data inference tasks on the Open Images, Pascal VOC 2012, Pascal Context, and ADE20K datasets. The code will be available at https://github.com/ijetma/APOVIS .},
  archive      = {J_ICV},
  author       = {Qiujie Ma and Shuqi Yang and Lijuan Zhang and Qing Lan and Dongdong Yang and Honghan Chen and Ying Tan},
  doi          = {10.1016/j.imavis.2024.105384},
  journal      = {Image and Vision Computing},
  month        = {2},
  pages        = {105384},
  shortjournal = {Image Vis. Comput.},
  title        = {APOVIS: Automated pixel-level open-vocabulary instance segmentation through integration of pre-trained vision-language models and foundational segmentation models},
  volume       = {154},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). CFENet: Context-aware feature enhancement network for efficient few-shot object counting. <em>ICV</em>, <em>154</em>, 105383. (<a href='https://doi.org/10.1016/j.imavis.2024.105383'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Few-shot object counting (FSOC) is designed to estimate the number of objects in any category given a query image and several bounding boxes. Existing methods usually ignore shape information when extracting the appearance of exemplars from query images, resulting in reduced object localization accuracy and count estimates. Meanwhile, these methods also utilize a fixed inner product or convolution for similarity matching, which may introduce background interference and limit the matching of objects with significant intra-class differences. To address the above challenges, we propose a Context-aware Feature Enhancement Network (CFENet) for FSOC. Specifically, our network comprises three main modules: Hierarchical Perception Joint Enhancement Module (HPJEM), Learnable Similarity Matcher (LSM), and Feature Fusion Module (FFM). Firstly, HPJEM performs feature enhancement on the scale transformations of query images and the shapes of exemplars, improving the network’s ability to recognize dense objects. Secondly, LSM utilizes learnable dilated convolutions and linear layers to expand the similarity metric of a fixed inner product, obtaining similarity maps. Then convolution with a given kernel is performed on the similarity maps to get the weighted features. Finally, FFM further fuses weighted features with multi-scale features obtained by HPJEM. We conduct extensive experiments on the specialized few-shot dataset FSC-147 and the subsets Val-COCO and Test-COCO of the COCO dataset. Experimental results validate the effectiveness of our method and show competitive performance. To further verify the generalization of CFENet, we also conduct experiments on the car dataset CARPK.},
  archive      = {J_ICV},
  author       = {Shihui Zhang and Gangzheng Zhai and Kun Chen and Houlin Wang and Shaojie Han},
  doi          = {10.1016/j.imavis.2024.105383},
  journal      = {Image and Vision Computing},
  month        = {2},
  pages        = {105383},
  shortjournal = {Image Vis. Comput.},
  title        = {CFENet: Context-aware feature enhancement network for efficient few-shot object counting},
  volume       = {154},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). EDCAANet: A lightweight COD network based on edge detection and coordinate attention assistance. <em>ICV</em>, <em>154</em>, 105382. (<a href='https://doi.org/10.1016/j.imavis.2024.105382'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In order to obtain the higher efficiency and the more accuracy in camouflaged object detection (COD), a lightweight COD network based on edge detection and coordinate attention assistance (EDCAANet) is presented in this paper. Firstly, an Integrated Edge and Global Context Information Module (IEGC) is proposed, which uses edge detection as an auxiliary means to collaborate with the atrous spatial convolution pooling pyramid (ASPP) for obtaining global context information to achieve the preliminary positioning of the camouflaged object. Then, the Receptive Field Module based on Coordinate Attention (RFMC) is put forward, in which the Coordinate Attention (CA) mechanism is employed as another aid means to expand receptive ffeld features and then achieve global comprehensive of the image. In the final stage of feature fusion, the proposed lightweight Adjacent and Global Context Focusing module (AGCF) is employed to aggregate the multi-scale semantic features output by RFMC at adjacent levels and the global context features output by IEGC. These aggregated features are mainly refined by the proposed Multi Scale Convolutional Aggregation (MSDA) blocks in the module, allowing features to interact and combine at various scales to ultimately produce prediction results. The experiments include performance comparison experiment, testing in complex background, generalization experiment, as well as ablation experiment and complexity analysis. Four public datasets are adopted for experiments, four recognized COD metrics are employed for performance evaluation, 3 backbone networks and 18 methods are used for comparison. The experimental results show that the proposed method can obtain both the more excellent detection performance and the higher efficiency.},
  archive      = {J_ICV},
  author       = {Qing Pan and Xiayuan Feng and Nili Tian},
  doi          = {10.1016/j.imavis.2024.105382},
  journal      = {Image and Vision Computing},
  month        = {2},
  pages        = {105382},
  shortjournal = {Image Vis. Comput.},
  title        = {EDCAANet: A lightweight COD network based on edge detection and coordinate attention assistance},
  volume       = {154},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). FPDIoU loss: A loss function for efficient bounding box regression of rotated object detection. <em>ICV</em>, <em>154</em>, 105381. (<a href='https://doi.org/10.1016/j.imavis.2024.105381'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Bounding box regression is one of the important steps of object detection. However, rotation detectors often involve a more complicated loss based on SkewIoU which is unfriendly to gradient-based training. Most of the existing loss functions for rotated object detection calculate the difference between two bounding boxes only focus on the deviation of area or each points distance (e.g., L S m o o t h − L 1 , L R o t a t e d I o U and L P I o U ). The calculation process of some loss functions is extremely complex (e.g. L K F I o U ). In order to improve the efficiency and accuracy of bounding box regression for rotated object detection, we proposed a novel metric for arbitrary shapes comparison based on minimum points distance, which takes most of the factors from existing loss functions for rotated object detection into account, i.e., the overlap or nonoverlapping area, the central points distance and the rotation angle. We also proposed a loss function called L F P D I o U based on four points distance for accurate bounding box regression focusing on faster and high quality anchor boxes. In the experiments, F P D I o U loss has been applied to state-of-the-art rotated object detection (e.g., RTMDET, H2RBox) models training with three popular benchmarks of rotated object detection including DOTA, DIOR, HRSC2016 and two benchmarks of arbitrary orientation scene text detection including ICDAR 2017 RRC-MLT and ICDAR 2019 RRC-MLT, which achieves better performance than existing loss functions. The code is available at https://github.com/JacksonMa618/FPDIoU},
  archive      = {J_ICV},
  author       = {Siliang Ma and Yong Xu},
  doi          = {10.1016/j.imavis.2024.105381},
  journal      = {Image and Vision Computing},
  month        = {2},
  pages        = {105381},
  shortjournal = {Image Vis. Comput.},
  title        = {FPDIoU loss: A loss function for efficient bounding box regression of rotated object detection},
  volume       = {154},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A fast and lightweight train image fault detection model based on convolutional neural networks. <em>ICV</em>, <em>154</em>, 105380. (<a href='https://doi.org/10.1016/j.imavis.2024.105380'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Trains play a vital role in the life of residents. Fault detection of trains is essential to ensuring their safe operation. Aiming at the problems of many parameters, slow detection speed, and low detection accuracy of the current train image fault detection model, a fast and lightweight train image fault detection model using convolutional neural network (FL-TINet) is proposed in this study. First, the joint depthwise separable convolution and divided-channel convolution strategy are applied to the feature extraction network in FL-TINet to reduce the number of parameters and computation amount in the backbone network, thereby increasing the detection speed. Second, a mixed attention mechanism is designed to make FL-TINet focus on key features. Finally, an improved discrete K-means clustering algorithm is designed to set the anchor boxes so that the anchor box can cover the object better, thereby improving the detection accuracy. Experimental results on PASCAL 2012 and train datasets show that FL-TINet can detect faults at 119 frames per second. Compared with the state-of-the-art CenterNet, RetinaNet, SSD, Faster R-CNN, MobileNet, YOLOv3, YOLOv4, YOLOv7-Tiny, YOLOv8_n and YOLOX-Tiny models, FL-TINet’s detection speed is increased by 96.37% on average, and it has higher detection accuracy and fewer parameters. The robustness test shows that FL-TINet can resist noise and illumination changes well.},
  archive      = {J_ICV},
  author       = {Longxin Zhang and Wenliang Zeng and Peng Zhou and Xiaojun Deng and Jiayu Wu and Hong Wen},
  doi          = {10.1016/j.imavis.2024.105380},
  journal      = {Image and Vision Computing},
  month        = {2},
  pages        = {105380},
  shortjournal = {Image Vis. Comput.},
  title        = {A fast and lightweight train image fault detection model based on convolutional neural networks},
  volume       = {154},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Enhancing few-shot object detection through pseudo-label mining. <em>ICV</em>, <em>154</em>, 105379. (<a href='https://doi.org/10.1016/j.imavis.2024.105379'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Few-shot object detection involves adapting an existing detector to a set of unseen categories with few annotated examples. This data limitation makes these methods to underperform those trained on large labeled datasets. In many scenarios, there is a high amount of unlabeled data that is never exploited. Thus, we propose to e xPAND the initial novel set by mining pseudo-labels. From a raw set of detections, xPAND obtains reliable pseudo-labels suitable for training any detector. To this end, we propose two new modules: Class and Box confirmation. Class Confirmation aims to remove misclassified pseudo-labels by comparing candidates with expected class prototypes. Box Confirmation estimates IoU to discard inadequately framed objects. Experimental results demonstrate that xPAND enhances the performance of multiple detectors up to +5.9 nAP and +16.4 nAP50 points for MS-COCO and PASCAL VOC, respectively, establishing a new state of the art. Code: https://github.com/PAGF188/xPAND .},
  archive      = {J_ICV},
  author       = {Pablo Garcia-Fernandez and Daniel Cores and Manuel Mucientes},
  doi          = {10.1016/j.imavis.2024.105379},
  journal      = {Image and Vision Computing},
  month        = {2},
  pages        = {105379},
  shortjournal = {Image Vis. Comput.},
  title        = {Enhancing few-shot object detection through pseudo-label mining},
  volume       = {154},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Dual multi scale networks for medical image segmentation using contrastive learning. <em>ICV</em>, <em>154</em>, 105371. (<a href='https://doi.org/10.1016/j.imavis.2024.105371'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {DMSNet, a novel model for medical image segmentation is proposed in this research work. DMSNet employs a dual multi-scale architecture, combining the computational efficiency of EfficientNet B5 with the contextual understanding of the Pyramid Vision Transformer (PVT). Integration of a multi-scale module in both encoders enhances the model's capacity to capture intricate details across various resolutions, enabling precise delineation of complex foreground boundaries. Notably, DMSNet incorporates contrastive learning with a novel pixel-wise contrastive loss function during training, contributing to heightened segmentation accuracy and improved generalization capabilities. The model's performance is demonstrated through experimental evaluation on the four diverse datasets including Brain tumor segmentation (BraTS 2020), Diabetic Foot ulcer segmentation (DFU), Polyps (KVASIR-SEG) and Breast cancer segmentation (BCSS). We have employed recently introduced metrics to evaluate and compare our model with other state-of-the-art architectures. By advancing segmentation accuracy through innovative architectural design, multi-scale modules, and contrastive learning techniques, DMSNet represents a significant stride in the field, with potential implications for improved patient care and outcomes.},
  archive      = {J_ICV},
  author       = {Akshat Dhamale and Ratnavel Rajalakshmi and Ananthakrishnan Balasundaram},
  doi          = {10.1016/j.imavis.2024.105371},
  journal      = {Image and Vision Computing},
  month        = {2},
  pages        = {105371},
  shortjournal = {Image Vis. Comput.},
  title        = {Dual multi scale networks for medical image segmentation using contrastive learning},
  volume       = {154},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Unmasking deepfakes: Eye blink pattern analysis using a hybrid LSTM and MLP-CNN model. <em>ICV</em>, <em>154</em>, 105370. (<a href='https://doi.org/10.1016/j.imavis.2024.105370'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent progress in the field of computer vision incorporates robust tools for creating convincing deepfakes. Hence, the propagation of fake media may have detrimental effects on social communities, potentially tarnishing the reputation of individuals or groups. Furthermore, this phenomenon may manipulate public sentiments and skew opinions about the affected entities. Recent research determines Convolution Neural Networks (CNNs) as a viable solution for detecting deepfakes within the networks. However, existing techniques struggle to accurately capture the differences between frames in the collected media streams. To alleviate these limitations, our work proposes a new Deepfake detection approach using a hybrid model using the Multi-layer Perceptron Convolution Neural Network (MLP-CNN) model and LSTM (Long Short Term Memory). Our model has utilized Contrast Limited Adaptive Histogram Equalization (CLAHE) (Musa et al., 2018) approach to enhance the contrast of the image and later on applying Viola Jones Algorithm (VJA) (Paul et al., 2018) to the preprocessed image for detecting the face. The extracted features such as Improved eye blinking pattern detection (IEBPD), active shape model (ASM), face attributes, and eye attributes features along with the age and gender of the corresponding image are fed to the hybrid deepfake detection model that involves two classifiers MLP-CNN and LSTM model. The proposed model is trained with these features to detect the deepfake images proficiently. The experimentation demonstrates that our proposed hybrid model has been evaluated on two datasets, i.e. World Leader Dataset (WLDR) and the DeepfakeTIMIT Dataset. From the experimental results, it is affirmed that our proposed hybrid model outperforms existing approaches such as DeepVision, DNN (Deep Neutral Network), CNN (Convolution Neural Network), RNN (Recurrent Neural network), DeepMaxout, DBN (Deep Belief Networks), and Bi-GRU (Bi-Directional Gated Recurrent Unit).},
  archive      = {J_ICV},
  author       = {Ruchika Sharma and Rudresh Dwivedi},
  doi          = {10.1016/j.imavis.2024.105370},
  journal      = {Image and Vision Computing},
  month        = {2},
  pages        = {105370},
  shortjournal = {Image Vis. Comput.},
  title        = {Unmasking deepfakes: Eye blink pattern analysis using a hybrid LSTM and MLP-CNN model},
  volume       = {154},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A temporally-aware noise-informed invertible network for progressive video denoising. <em>ICV</em>, <em>154</em>, 105369. (<a href='https://doi.org/10.1016/j.imavis.2024.105369'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Video denoising is a critical task in computer vision, aiming to enhance video quality by removing noise from consecutive video frames. Despite significant progress, existing video denoising methods still suffer from challenges in maintaining temporal consistency and adapting to different noise levels. To address these issues, a temporally-aware and noise-informed invertible network is proposed by following divide-and-conquer principle for progressive video denoising. Specifically, a recurrent attention-based reversible network is designed to distinctly extract temporal information from consecutive frames, thus tackling the learning problem of temporal consistency. Simultaneously, a noise-informed two-way dense block is developed by using estimated noise as conditional guidance to adapt to different noise levels. The noise-informed guidance can then be used to guide the learning of dense block for efficient video denoising. Under the framework of invertible network, the designed two parts can be further integrated to achieve invertible learning to enable progressive video denoising. Experiments and comparative studies demonstrate that our method can achieve good denoising accuracy and fast inference speed in both synthetic scenes and real-world applications.},
  archive      = {J_ICV},
  author       = {Yan Huang and Huixin Luo and Yong Xu and Xian-Bing Meng},
  doi          = {10.1016/j.imavis.2024.105369},
  journal      = {Image and Vision Computing},
  month        = {2},
  pages        = {105369},
  shortjournal = {Image Vis. Comput.},
  title        = {A temporally-aware noise-informed invertible network for progressive video denoising},
  volume       = {154},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Scene flow estimation from point cloud based on grouped relative self-attention. <em>ICV</em>, <em>154</em>, 105368. (<a href='https://doi.org/10.1016/j.imavis.2024.105368'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {3D scene flow estimation is a fundamental task in computer vision, which aims to estimate the 3D motions of point clouds. The point cloud is disordered, and the point density in the local area of the same object is non-uniform. The features extracted by previous methods are not discriminative enough to obtain accurate scene flow. Besides, scene flow may be misestimated when two adjacent frames of point clouds have large movements. From our observation, the quality of point cloud feature extraction and the correlations of two-frame point clouds directly affect the accuracy of scene flow estimation. Therefore, we propose an improved self-attention structure named Grouped Relative Self-Attention (GRSA) that simultaneously utilizes the grouping operation and offset subtraction operation with normalization refinement to process point clouds. Specifically, we embed the Grouped Relative Self-Attention (GRSA) into feature extraction and each stage of flow refinement to gain lightweight but efficient self-attention respectively, which can extract discriminative point features and enhance the point correlations to be more adaptable to long-distance movements. Furthermore, we use a comprehensive loss function to avoid outliers and obtain robust results. We evaluate our method on the FlyingThings3D and KITTI datasets and achieve superior performance. In particular, our method outperforms all other methods on the FlyingThings3D dataset, where Outliers achieves a 16.9% improvement. On the KITTI dataset, Outliers also achieves a 6.7% improvement.},
  archive      = {J_ICV},
  author       = {Xuezhi Xiang and Xiankun Zhou and Yingxin Wei and Xi Wang and Yulong Qiao},
  doi          = {10.1016/j.imavis.2024.105368},
  journal      = {Image and Vision Computing},
  month        = {2},
  pages        = {105368},
  shortjournal = {Image Vis. Comput.},
  title        = {Scene flow estimation from point cloud based on grouped relative self-attention},
  volume       = {154},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Information gap based knowledge distillation for occluded facial expression recognition. <em>ICV</em>, <em>154</em>, 105365. (<a href='https://doi.org/10.1016/j.imavis.2024.105365'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Facial Expression Recognition (FER) with occlusion presents a challenging task in computer vision because facial occlusions result in poor visual data features. Recently, the region attention technique has been introduced to address this problem by researchers, which make the model perceive occluded regions of the face and prioritize the most discriminative non-occluded regions. However, in real-world scenarios, facial images are influenced by various factors, including hair, masks and sunglasses, making it difficult to extract high-quality features from these occluded facial images. This inevitably limits the effectiveness of attention mechanisms. In this paper, we observe a correlation in facial emotion features from the same image, both with and without occlusion. This correlation contributes to addressing the issue of facial occlusions. To this end, we propose a Information Gap based Knowledge Distillation (IGKD) to explore the latent relationship. Specifically, our approach involves feeding non-occluded and masked images into separate teacher and student networks. Due to the incomplete emotion information in the masked images, there exists an information gap between the teacher and student networks. During training, we aim to minimize this gap to enable the student network to learn this relationship. To enhance the teacher’s guidance, we introduce a joint learning strategy where the teacher conducts knowledge distillation on the student during the training of the teacher. Additionally, we introduce two novel constraints, called knowledge learn and knowledge feedback loss, to supervise and optimize both the teacher and student networks. The reported experimental results show that IGKD outperforms other algorithms on four benchmark datasets. Specifically, our IGKD achieves 87.57% on Occlusion-RAF-DB, 87.33% on Occlusion-FERPlus, 64.86% on Occlusion-AffectNet, and 73.25% on FED-RO, clearly demonstrating its effectiveness and robustness. Source code is released at: .},
  archive      = {J_ICV},
  author       = {Yan Zhang and Zenghui Li and Duo Shen and Ke Wang and Jia Li and Chenxing Xia},
  doi          = {10.1016/j.imavis.2024.105365},
  journal      = {Image and Vision Computing},
  month        = {2},
  pages        = {105365},
  shortjournal = {Image Vis. Comput.},
  title        = {Information gap based knowledge distillation for occluded facial expression recognition},
  volume       = {154},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). CLBSR: A deep curriculum learning-based blind image super resolution network using geometrical prior. <em>ICV</em>, <em>154</em>, 105364. (<a href='https://doi.org/10.1016/j.imavis.2024.105364'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Blind image super resolution (SR) is a challenging computer vision task, which involves enhancing the quality of the low-resolution (LR) images obtained by various degradation operations. Deep neural networks have provided state-of-the-art performances for the task of image SR in a blind fashion. It has been shown in the literature that by decoupling the task of blind image SR into the blurring kernel estimation and high-quality image reconstruction, superior performance can be obtained. In this paper, we first propose a novel optimization problem that, by using the geometrical information as prior, is able to estimate the blurring kernels in an accurate manner. We then propose a novel blind image SR network that employs the blurring kernel thus estimated in its network architecture and learning algorithm in order to generate high-quality images. In this regard, we utilize the curriculum learning strategy, wherein the training process of the SR network is initially facilitated by using the ground truth (GT) blurring kernel and then continued with the estimated blurring kernel obtained from our optimization problem. The results of various experiments show the effectiveness of the proposed blind image SR scheme in comparison to state-of-the-art methods on various degradation operations and benchmark datasets.},
  archive      = {J_ICV},
  author       = {Alireza Esmaeilzehi and Amir Mohammad Babaei and Farshid Nooshi and Hossein Zaredar and M. Omair Ahmad},
  doi          = {10.1016/j.imavis.2024.105364},
  journal      = {Image and Vision Computing},
  month        = {2},
  pages        = {105364},
  shortjournal = {Image Vis. Comput.},
  title        = {CLBSR: A deep curriculum learning-based blind image super resolution network using geometrical prior},
  volume       = {154},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). AwareTrack: Object awareness for visual tracking via templates interaction. <em>ICV</em>, <em>154</em>, 105363. (<a href='https://doi.org/10.1016/j.imavis.2024.105363'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Current popular trackers, whether based on the Siamese network or Transformer, have focused their main work on relation modeling between the template and the search area, and on the design of the tracking head, neglecting the fundamental element of tracking, the template. Templates are often mixed with too much background information, which can interfere with the extraction of template features. To address the above issue, a template object-aware tracker (AwareTrack) is proposed. Through the information interaction between multiple templates, the attention of the templates can be truly focused on the object itself, and the background interference can be suppressed. To ensure that the foreground objects of the templates have the same appearance to the greatest extent, the concept of awareness templates is proposed, which consists of two close frames. In addition, an awareness templates sampling method based on similarity discrimination via Siamese network is also proposed, which adaptively determines the interval between two awareness templates, ensure the maximization of background differences in the awareness templates. Meanwhile, online updates to the awareness templates ensure that our tracker has access to the most recent features of the foreground object. Our AwareTrack achieves state-of-the-art performance on multiple benchmarks, particularly on the one-shot tracking benchmark GOT-10k, achieving the AO of 78.1%, which is a 4.4% improvement over OSTrack-384.},
  archive      = {J_ICV},
  author       = {Hong Zhang and Jianbo Song and Hanyang Liu and Yang Han and Yifan Yang and Huimin Ma},
  doi          = {10.1016/j.imavis.2024.105363},
  journal      = {Image and Vision Computing},
  month        = {2},
  pages        = {105363},
  shortjournal = {Image Vis. Comput.},
  title        = {AwareTrack: Object awareness for visual tracking via templates interaction},
  volume       = {154},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Pixel integration from fine to coarse for lightweight image super-resolution. <em>ICV</em>, <em>154</em>, 105362. (<a href='https://doi.org/10.1016/j.imavis.2024.105362'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, Transformer-based methods have made significant progress on image super-resolution. They encode long-range dependencies between image patches through self-attention mechanism. However, when extracting all tokens from the entire feature map, the computational cost is expensive. In this paper, we propose a novel lightweight image super-resolution approach, pixel integration network(PIN). Specifically, our method employs fine pixel integration and coarse pixel integration from local and global receptive field. In particular, coarse pixel integration is implemented by a retractable attention, consisting of dense and sparse self-attention. In order to focus on enriching features with contextual information, spatial-gate mechanism and depth-wise convolution are introduced to multi-layer perception. Besides, a spatial frequency fusion block is adopted to obtain more comprehensive, detailed, and stable information at the end of deep feature extraction. Extensive experiments demonstrate that PIN achieves the state-of-the-art performance with small parameters on lightweight super-resolution.},
  archive      = {J_ICV},
  author       = {Yuxiang Wu and Xiaoyan Wang and Xiaoyan Liu and Yuzhao Gao and Yan Dou},
  doi          = {10.1016/j.imavis.2024.105362},
  journal      = {Image and Vision Computing},
  month        = {2},
  pages        = {105362},
  shortjournal = {Image Vis. Comput.},
  title        = {Pixel integration from fine to coarse for lightweight image super-resolution},
  volume       = {154},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). GPLM: Enhancing underwater images with global pyramid linear modulation. <em>ICV</em>, <em>154</em>, 105361. (<a href='https://doi.org/10.1016/j.imavis.2024.105361'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Underwater imagery often suffers from challenges such as color distortion, low contrast, blurring, and noise due to the absorption and scattering of light in water. These degradations complicate visual interpretation and hinder subsequent image processing. Existing methods struggle to effectively address the complex, spatially varying degradations without prior environmental knowledge or may produce unnatural enhancements. To overcome these limitations, we propose a novel method called Global Pyramid Linear Modulation that integrates physical degradation modeling with deep learning for underwater image enhancement. Our approach extends Feature-wise Linear Modulation to a four-dimensional structure, enabling fine-grained, spatially adaptive modulation of feature maps. Our method captures multi-scale contextual information by incorporating a feature pyramid architecture with self-attention and feature fusion mechanisms, effectively modeling complex degradations. We validate our method by integrating it into the MixDehazeNet model and conducting experiments on benchmark datasets. Our approach significantly improves the Peak Signal-to-Noise Ratio, increasing from 28.6 dB to 30.6 dB on the EUVP-515-test dataset. Compared to recent state-of-the-art methods, our method consistently outperforms them by over 3 dB in PSNR on datasets with ground truth. It improves the Underwater Image Quality Measure by more than one on datasets without ground truth. Furthermore, we demonstrate the practical applicability of our method on a real-world underwater dataset, achieving substantial improvements in image quality metrics and visually compelling results. These experiments confirm that our method effectively addresses the limitations of existing techniques by adaptively modeling complex underwater degradations, highlighting its potential for underwater image enhancement tasks.},
  archive      = {J_ICV},
  author       = {Jinxin Shao and Haosu Zhang and Jianming Miao},
  doi          = {10.1016/j.imavis.2024.105361},
  journal      = {Image and Vision Computing},
  month        = {2},
  pages        = {105361},
  shortjournal = {Image Vis. Comput.},
  title        = {GPLM: Enhancing underwater images with global pyramid linear modulation},
  volume       = {154},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). HPD-depth: High performance decoding network for self-supervised monocular depth estimation. <em>ICV</em>, <em>154</em>, 105360. (<a href='https://doi.org/10.1016/j.imavis.2024.105360'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Self-supervised monocular depth estimation methods have shown promising results by leveraging geometric relationships among image sequences for network supervision. However, existing methods often face challenges such as blurry depth edges, high computational overhead, and information redundancy. This paper analyzes and investigates technologies related to deep feature encoding, decoding, and regression, and proposes a novel depth estimation network termed HPD-Depth, optimized by three strategies: utilizing the Residual Channel Attention Transition (RCAT) module to bridge the semantic gap between encoding and decoding features while highlighting important features; adopting the Sub-pixel Refinement Upsampling (SPRU) module to obtain high-resolution feature maps with detailed features; and introducing the Adaptive Hybrid Convolutional Attention (AHCA) module to address issues of local depth confusion and depth boundary blurriness. HPD-Depth excels at extracting clear scene structures and capturing detailed local information while maintaining an effective balance between accuracy and parameter count. Comprehensive experiments demonstrate that HPD-Depth performs comparably to state-of-the-art algorithms on the KITTI benchmarks and exhibits significant potential when trained with high-resolution data. Compared with the baseline model, the average relative error and squared relative error are reduced by 6.09% and 12.62% in low-resolution experiments, respectively, and by 11.3% and 18.5% in high-resolution experiments, respectively. Moreover, HPD-Depth demonstrates excellent generalization performance on the Make3D dataset.},
  archive      = {J_ICV},
  author       = {Liehao Wu and Laihua Wang and Guanghui Wei and Yang Yu},
  doi          = {10.1016/j.imavis.2024.105360},
  journal      = {Image and Vision Computing},
  month        = {2},
  pages        = {105360},
  shortjournal = {Image Vis. Comput.},
  title        = {HPD-depth: High performance decoding network for self-supervised monocular depth estimation},
  volume       = {154},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). DALSCLIP: Domain aggregation via learning stronger domain-invariant features for CLIP. <em>ICV</em>, <em>154</em>, 105359. (<a href='https://doi.org/10.1016/j.imavis.2024.105359'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {When the test data follows a different distribution from the training data, neural networks experience domain shift. We can address this issue with domain generalization (DG), which aims to develop models that can perform well on unknown domains. In this paper, we propose a simple yet effective framework called DALSCLIP to achieve high-performance generalization of CLIP, Contrastive LanguageImage Pre-training, in DG. Specifically, we optimize CLIP in two aspects: images and prompts. For images, we propose a method to remove domain-specific features from input images and learn better domain-invariant features. We first train specific classifiers for each domain to learn their corresponding domain-specific information and then learn a mapping to remove domain-specific information. For prompts, we design a lightweight optimizer(Attention-based MLP) to automatically optimize the prompts and incorporate domain-specific information into the input, helping the prompts better adapt to the domain. Meanwhile, we freeze the network parameters during training to maximize the retention of pre-training model information. We extensively evaluate our model on three public datasets. Qualitative and quantitative experiments demonstrate that our framework outperforms other baselines significantly.},
  archive      = {J_ICV},
  author       = {Yuewen Zhang and Jiuhang Wang and Hongying Tang and Ronghua Qin},
  doi          = {10.1016/j.imavis.2024.105359},
  journal      = {Image and Vision Computing},
  month        = {2},
  pages        = {105359},
  shortjournal = {Image Vis. Comput.},
  title        = {DALSCLIP: Domain aggregation via learning stronger domain-invariant features for CLIP},
  volume       = {154},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). EPFDNet: Camouflaged object detection with edge perception in frequency domain. <em>ICV</em>, <em>154</em>, 105358. (<a href='https://doi.org/10.1016/j.imavis.2024.105358'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Camouflaged object detection (COD) is a relatively new field of computer vision research. The challenge of this task lies in accurately segmenting camouflaged objects from backgrounds that are similar in appearance. In fact, the generation of reliable edges is an effective mean of distinguishing between the foreground and background of the image, which is beneficial for assisting in determining the location of camouflaged objects. Inspired by this, we design an Edge Encoder that decomposes features into different frequency bands adopting learnable wavelets and focuses on high-frequency components with sufficient edge details to extract accurate edges. Subsequently, the Feature Aggregation Module is proposed to integrate contextual features, which generates rough edge details by sensing the difference between two branch features and use this information to further refine our edge features. Furthermore, the Stage Enhancement Module is developed to enhance the features through reverse attention guidance and dilate convolution, which mines the detailed structural information of the camouflaged objects area by eliminating foreground. The superiority of our proposed method (EPFDNet) over the existing 17 state-of-the-art methods is demonstrated through extensive experiments on three widely used COD benchmark datasets. The code has been released at https://github.com/LitterMa-820/EPFDNet .},
  archive      = {J_ICV},
  author       = {Xian Fang and Jiatong Chen and Yaming Wang and Mingfeng Jiang and Jianhua Ma and Xin Wang},
  doi          = {10.1016/j.imavis.2024.105358},
  journal      = {Image and Vision Computing},
  month        = {2},
  pages        = {105358},
  shortjournal = {Image Vis. Comput.},
  title        = {EPFDNet: Camouflaged object detection with edge perception in frequency domain},
  volume       = {154},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Increase the sensitivity of moderate examples for semantic image segmentation. <em>ICV</em>, <em>154</em>, 105357. (<a href='https://doi.org/10.1016/j.imavis.2024.105357'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Dominant paradigms in modern semantic segmentation resort to the scheme of pixel-wise classification and do supervised training with the standard cross-entropy loss (CE). Although CE is intuitively straightforward and suitable for this task, it only cares about the predicted score of the target category and ignores the probability distribution information. We further notice that fitting hard examples, even if their number is small, results in model over-fitting in the test stage, as accumulated CE losses overwhelm the model during training. Besides, a large number of easy examples may also dazzle the model training. Based on this observation, this work presents a novel loss function we call Sensitive Loss (SL), which utilizes the overall predicted probability distribution information to down-weight the contribution of extremely hard examples (outliers) and easy examples (inliers) during training and rapidly focuses model learning on moderate examples. In this manner, SL encourages the model to learn potential feature generalities rather than diving into the details and noise implied by outliers to the extent. Thus, it is capable of alleviating over-fitting and improving generalization capacity. We also propose a dynamic Learning Rate Scaling (LRS) strategy to alleviate the decreasing gradient and improve the performance of SL. Extensive experiments evidence that our Sensitive Loss is superior to existing handcrafted loss functions and on par with searched losses, which generalize well to a wide range of datasets and algorithms. Specifically, training with the proposed SL brings a notable 1.7% mIoU improvement for the Mask2Former framework on Cityscapes dataset off the shelf.},
  archive      = {J_ICV},
  author       = {Quan Tang and Fagui Liu and Dengke Zhang and Jun Jiang and Xuhao Tang and C.L. Philip Chen},
  doi          = {10.1016/j.imavis.2024.105357},
  journal      = {Image and Vision Computing},
  month        = {2},
  pages        = {105357},
  shortjournal = {Image Vis. Comput.},
  title        = {Increase the sensitivity of moderate examples for semantic image segmentation},
  volume       = {154},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). De-noising mask transformer for referring image segmentation. <em>ICV</em>, <em>154</em>, 105356. (<a href='https://doi.org/10.1016/j.imavis.2024.105356'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Referring Image Segmentation (RIS) is a challenging computer vision task that involves identifying and segmenting specific objects in an image based on a natural language description. Unlike conventional segmentation methodologies, RIS needs to bridge the gap between visual and linguistic modalities to exert the semantic information provided by natural language. Most existing RIS approaches are confronted with the common issue that the intermediate predicted target region also participates in the later feature generation and parameter updating. Then the wrong prediction, especially occurs in the early training stage, will bring the gradient misleading and ultimately affect the training stability. To tackle this issue, we propose de-noising mask (DNM) transformer to fuse the cross-modal integration, a novel framework to replace the cross-attention by DNM-attention in traditional transformer. Furthermore, two kinds of DNM-attention, named mask-DNM and cluster-DNM, are proposed, where noisy ground truth information is adopted to guide the attention mechanism to produce accurate object queries, i.e. , de-nosing query. Thus, DNM-attention leverages noisy ground truth information to guide the attention mechanism to produce additional de-nosing queries, which effectively avoids the gradient misleading. Experimental results show that the DNM transformer improves the performance of RIS and outperforms most existing RIS approaches on three benchmarks.},
  archive      = {J_ICV},
  author       = {Yehui Wang and Fang Lei and Baoyan Wang and Qiang Zhang and Xiantong Zhen and Lei Zhang},
  doi          = {10.1016/j.imavis.2024.105356},
  journal      = {Image and Vision Computing},
  month        = {2},
  pages        = {105356},
  shortjournal = {Image Vis. Comput.},
  title        = {De-noising mask transformer for referring image segmentation},
  volume       = {154},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Generative AI in the context of assistive technologies: Trends, limitations and future directions. <em>ICV</em>, <em>154</em>, 105347. (<a href='https://doi.org/10.1016/j.imavis.2024.105347'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the tremendous successes of Large Language Models (LLMs) like ChatGPT for text generation and Dall-E for high-quality image generation, generative Artificial Intelligence (AI) models have shown a hype in our society. Generative AI seamlessly delved into different aspects of society ranging from economy, education, legislation, computer science, finance, and even healthcare. This article provides a comprehensive survey on the increased and promising use of generative AI in assistive technologies benefiting different parties, ranging from the assistive system developers, medical practitioners, care workforce, to the people who need the care and the comfort. Ethical concerns, biases, lack of transparency, insufficient explainability, and limited trustworthiness are major challenges when using generative AI in assistive technologies, particularly in systems that impact people directly. Key future research directions to address these issues include creating standardized rules, establishing commonly accepted evaluation metrics and benchmarks for explainability and reasoning processes, and making further advancements in understanding and reducing bias and its potential harms. Beyond showing the current trends of applying generative AI in the scope of assistive technologies in four identified key domains, which include care sectors, medical sectors, helping people in need, and co-working, the survey also discusses the current limitations and provides promising future research directions to foster better integration of generative AI in assistive technologies.},
  archive      = {J_ICV},
  author       = {Biying Fu and Abdenour Hadid and Naser Damer},
  doi          = {10.1016/j.imavis.2024.105347},
  journal      = {Image and Vision Computing},
  month        = {2},
  pages        = {105347},
  shortjournal = {Image Vis. Comput.},
  title        = {Generative AI in the context of assistive technologies: Trends, limitations and future directions},
  volume       = {154},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Feature extraction and fusion algorithm for infrared visible light images based on residual and generative adversarial network. <em>ICV</em>, <em>154</em>, 105346. (<a href='https://doi.org/10.1016/j.imavis.2024.105346'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the application and popularization of depth cameras, image fusion techniques based on infrared and visible light are increasingly used in various fields. Object detection and robot navigation impose more stringent requirements on the texture details and image quality of fused images. Existing residual network, attention mechanisms, and generative adversarial network are ineffective in dealing with the image fusion problem because of insufficient detail feature extraction and non-conformity to the human visual perception system during the fusion of infrared and visible light images. Our newly developed RGFusion network relies on a two-channel attentional mechanism, a residual network, and a generative adversarial network that introduces two new components: a high-precision image feature extractor and an efficient multi-stage training strategy. The network is preprocessed by a high-dimensional mapping and the complex feature extractor is processed through a sophisticated two-stage image fusion process to obtain feature structures with multiple features, resulting in high-quality fused images rich in detailed features. Extensive experiments on public datasets validate this fusion approach, and RGFusion is at the forefront of SD metrics for EN and SF, reaching 7.366, 13.322, and 49.281 on the TNO dataset and 7.276, 19.171, and 53.777 on the RoadScene dataset, respectively.},
  archive      = {J_ICV},
  author       = {Naigong Yu and YiFan Fu and QiuSheng Xie and QiMing Cheng and Mohammad Mehedi Hasan},
  doi          = {10.1016/j.imavis.2024.105346},
  journal      = {Image and Vision Computing},
  month        = {2},
  pages        = {105346},
  shortjournal = {Image Vis. Comput.},
  title        = {Feature extraction and fusion algorithm for infrared visible light images based on residual and generative adversarial network},
  volume       = {154},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). 3D human avatar reconstruction with neural fields: A recent survey. <em>ICV</em>, <em>154</em>, 105341. (<a href='https://doi.org/10.1016/j.imavis.2024.105341'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {3D human avatar reconstruction aims to reconstruct the 3D geometric shape and appearance of the human body from various data inputs, such as images, videos, and depth information, acting as a key component in human-oriented 3D vision in the metaverse. With the progress in neural fields for 3D reconstruction in recent years, significant advancements have been made in this research area for shape accuracy and appearance quality. Meanwhile, substantial efforts on dynamic avatars with the representation of neural fields have exhibited their effect. Although significant improvements have been achieved, challenges still exist in in-the-wild and complex environments, detailed shape recovery, and interactivity in real-world applications. In this survey, we present a comprehensive overview of 3D human avatar reconstruction methods using advanced neural fields. We start by introducing the background of 3D human avatar reconstruction and the mainstream paradigms with neural fields. Subsequently, representative research studies are classified based on their representation and avatar partswith detailed discussion. Moreover, we summarize the commonly used available datasets, evaluation metrics, and results in the research area. In the end, we discuss the open problems and highlight the promising future directions, hoping to inspire novel ideas and promote further research in this area.},
  archive      = {J_ICV},
  author       = {Meiying Gu and Jiahe Li and Yuchen Wu and Haonan Luo and Jin Zheng and Xiao Bai},
  doi          = {10.1016/j.imavis.2024.105341},
  journal      = {Image and Vision Computing},
  month        = {2},
  pages        = {105341},
  shortjournal = {Image Vis. Comput.},
  title        = {3D human avatar reconstruction with neural fields: A recent survey},
  volume       = {154},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). IRPE: Instance-level reconstruction-based 6D pose estimator. <em>ICV</em>, <em>154</em>, 105340. (<a href='https://doi.org/10.1016/j.imavis.2024.105340'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The estimation of an object’s 6D pose is a fundamental task in modern commercial and industrial applications. Vision-based pose estimation has gained popularity due to its cost-effectiveness and ease of setup in the field. However, this type of estimation tends to be less robust compared to other methods due to its sensitivity to the operating environment. For instance, in robot manipulation applications, heavy occlusion and clutter are common, posing significant challenges. For safety and robustness in industrial environments, depth information is often leveraged instead of relying solely on RGB images. Nevertheless, even with depth information, 6D pose estimation in such scenarios still remains challenging. In this paper, we introduce a novel 6D pose estimation method that promotes the network’s learning of high-level object features through self-supervised learning and instance reconstruction. The feature representation of the reconstructed instance is subsequently utilized in direct 6D pose regression via a multi-task learning scheme. As a result, the proposed method can differentiate and retrieve each object instance from a scene that is heavily occluded and cluttered, thereby surpassing conventional pose estimators in such scenarios. Additionally, due to the standardized prediction of reconstructed image, our estimator exhibits robustness performance against variations in lighting conditions and color drift. This is a significant improvement over traditional methods that depend on pixel-level sparse or dense features. We demonstrate that our method achieves state-of-the-art performance (e.g., 85.4% on LM-O) on the most commonly used benchmarks with respect to the ADD(-S) metric. Lastly, we present a CLIP dataset that emulates intense occlusion scenarios of industrial environment and conduct a real-world experiment for manipulation applications to verify the effectiveness and robustness of our proposed method.},
  archive      = {J_ICV},
  author       = {Le Jin and Guoshun Zhou and Zherong Liu and Yuanchao Yu and Teng Zhang and Minghui Yang and Jun Zhou},
  doi          = {10.1016/j.imavis.2024.105340},
  journal      = {Image and Vision Computing},
  month        = {2},
  pages        = {105340},
  shortjournal = {Image Vis. Comput.},
  title        = {IRPE: Instance-level reconstruction-based 6D pose estimator},
  volume       = {154},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). ProtoMed: Prototypical networks with auxiliary regularization for few-shot medical image classification. <em>ICV</em>, <em>154</em>, 105337. (<a href='https://doi.org/10.1016/j.imavis.2024.105337'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Although deep learning has shown impressive results in computer vision, the scarcity of annotated medical images poses a significant challenge for its effective integration into Computer-Aided Diagnosis (CAD) systems. Few-Shot Learning (FSL) opens promising perspectives for image recognition in low-data scenarios. However, applying FSL for medical image diagnosis presents significant challenges, particularly in learning disease-specific and clinically relevant features from a limited number of images. In the medical domain, training samples from different classes often exhibit visual similarities. Consequently, certain medical conditions may present striking resemblances, resulting in minimal inter-class variation. In this paper, we propose a prototypical network-based approach for few-shot medical image classification for low-prevalence diseases detection. Our method leverages meta-learning to use prior knowledge gained from common diseases, enabling generalization to new cases with limited data. However, the episodic training inherent in meta-learning tends to disproportionately emphasize the connections between elements in the support set and those in the query set, which can compromise the understanding of complex relationships within medical image data during the training phase. To address this, we propose an auxiliary network as a regularizer in the meta-training phase, designed to enhance the similarity of image representations from the same class while enforcing dissimilarity between representations from different classes in both the query and support sets. The proposed method has been evaluated using three medical diagnosis problems with different imaging modalities and different levels of visual imaging details and patterns. The obtained model is lightweight and efficient, demonstrating superior performance in both efficiency and accuracy compared to state-of-the-art. These findings highlight the potential of our approach to improve performance in practical applications, balancing resource limitations with the need for high diagnostic accuracy.},
  archive      = {J_ICV},
  author       = {Achraf Ouahab and Olfa Ben Ahmed},
  doi          = {10.1016/j.imavis.2024.105337},
  journal      = {Image and Vision Computing},
  month        = {2},
  pages        = {105337},
  shortjournal = {Image Vis. Comput.},
  title        = {ProtoMed: Prototypical networks with auxiliary regularization for few-shot medical image classification},
  volume       = {154},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Unified volumetric avatar: Enabling flexible editing and rendering of neural human representations. <em>ICV</em>, <em>153</em>, 105345. (<a href='https://doi.org/10.1016/j.imavis.2024.105345'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Neural Radiance Field (NeRF) has emerged as a leading method for reconstructing 3D human avatars with exceptional rendering capabilities, particularly for novel view and pose synthesis. However, current approaches for editing these avatars are limited, typically allowing only global geometry adjustments or texture modifications via neural texture maps. This paper introduces Unified Volumetric Avatar, a novel framework enabling independent and simultaneous global and local editing of both geometry and texture of 3D human avatars and user-friendly manipulation. The proposed approach seamlessly integrates implicit neural fields with an explicit polygonal mesh, leveraging distinct geometry and appearance latent codes attached to the body mesh for precise local edits. These trackable latent codes permeate through the 3D space via barycentric interpolation, mitigating spatial ambiguity with the aid of a local signed height indicator. Furthermore, our method enhances surface illumination representation across different poses by incorporating a pose-dependent shading factor instead of relying on view-dependent radiance color. Experimental results on multiple human avatars demonstrate its efficacy in achieving competitive results for novel view synthesis and novel pose rendering, showcasing its potential for versatile human representation. The source code will be made publicly available.},
  archive      = {J_ICV},
  author       = {Jinlong Fan and Xudong Lv and Xuepu Zeng and Zhengyi Bao and Zhiwei He and Mingyu Gao},
  doi          = {10.1016/j.imavis.2024.105345},
  journal      = {Image and Vision Computing},
  month        = {1},
  pages        = {105345},
  shortjournal = {Image Vis. Comput.},
  title        = {Unified volumetric avatar: Enabling flexible editing and rendering of neural human representations},
  volume       = {153},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Infrared and visible image fusion using quantum computing induced edge preserving filter. <em>ICV</em>, <em>153</em>, 105344. (<a href='https://doi.org/10.1016/j.imavis.2024.105344'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Information fusion by utilization of visible and thermal images provides a more comprehensive scene understanding in the resulting image rather than individual source images. It applies to wide areas of applications such as navigation, surveillance, remote sensing, and military where significant information is obtained from diverse modalities making it quite challenging. The challenges involved in integrating the various sources of data are due to the diverse modalities of imaging sensors along with the complementary information. So, there is a need for precise information integration in terms of infrared (IR) and visible image fusion while retaining useful information from both sources. Therefore, in this article, a unique image fusion methodology is presented that focuses on enhancing the prominent details of both images, preserving the textural information with reduced noise from either of the sources. In this regard, we put forward a quantum computing-induced IR and visible image fusion technique which preserves the required information with highlighted details from the source images efficiently. Initially, the proposed edge detail preserving strategy is capable of retaining the salient details accurately from the source images. Further, the proposed quantum computing-induced weight map generation mechanism preserves the complementary details with fewer redundant details which produces quantum details. Again the prominent features of the source images are retained using highly rich information. Finally, the quantum and the prominent details are utilized to produce the fused image for the corresponding source image pair. Both subjective and objective analyses are utilized to validate the effectiveness of the proposed algorithm. The efficacy of the developed model is validated by comparing the outcomes attained by it against twenty-six existing fusion algorithms. From various experiments, it is observed that the developed framework achieved higher accuracy in terms of visual demonstration as well as quantitative assessments compared to different deep-learning and non-deep learning-based state-of-the-art (SOTA) techniques.},
  archive      = {J_ICV},
  author       = {Priyadarsan Parida and Manoj Kumar Panda and Deepak Kumar Rout and Saroj Kumar Panda},
  doi          = {10.1016/j.imavis.2024.105344},
  journal      = {Image and Vision Computing},
  month        = {1},
  pages        = {105344},
  shortjournal = {Image Vis. Comput.},
  title        = {Infrared and visible image fusion using quantum computing induced edge preserving filter},
  volume       = {153},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Mobile-friendly and multi-feature aggregation via transformer for human pose estimation. <em>ICV</em>, <em>153</em>, 105343. (<a href='https://doi.org/10.1016/j.imavis.2024.105343'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Human pose estimation is pivotal for human-centric visual tasks, yet deploying such models on mobile devices remains challenging due to high parameter counts and computational demands. In this paper, we study Mobile-Friendly and Multi-Feature Aggregation architectural designs for human pose estimation and propose a novel model called MobileMultiPose. Specifically, a lightweight aggregation method, incorporating multi-scale and multi-feature, mitigates redundant shallow semantic extraction and local deep semantic constraints. To efficiently aggregate diverse local and global features, a lightweight transformer module, constructed from a self-attention mechanism with linear complexity, is designed, achieving deep fusion of shallow and deep semantics. Furthermore, a multi-scale loss supervision method is incorporated into the training process to enhance model performance, facilitating the effective fusion of edge information across various scales. Extensive experiments show that the smallest variant of MobileMultiPose outperforms lightweight models (MobileNetv2, ShuffleNetv2, and Small HRNet) by 0.7, 5.4, and 10.1 points, respectively, on the COCO validation set, with fewer parameters and FLOPs. In particular, the largest MobileMultiPose variant achieves an impressive AP score of 72.4 on the COCO test-dev set, notably, its parameters and FLOPs are only 16% and 18% of HRNet-W32, and 7% and 9% of DARK, respectively. We aim to offer novel insights into designing lightweight and efficient feature extraction networks, supporting mobile-friendly model deployment.},
  archive      = {J_ICV},
  author       = {Biao Li and Shoufeng Tang and Wenyi Li},
  doi          = {10.1016/j.imavis.2024.105343},
  journal      = {Image and Vision Computing},
  month        = {1},
  pages        = {105343},
  shortjournal = {Image Vis. Comput.},
  title        = {Mobile-friendly and multi-feature aggregation via transformer for human pose estimation},
  volume       = {153},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). IFE-net: Integrated feature enhancement network for image manipulation localization. <em>ICV</em>, <em>153</em>, 105342. (<a href='https://doi.org/10.1016/j.imavis.2024.105342'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Image tampering techniques can lead to distorted or misleading information, which in turn poses a threat in many areas, including social, legal and commercial. Numerous image tampering detection algorithms lose important low-level detail information when extracting deep features, reducing the accuracy and robustness of detection. In order to solve the problems of current methods, this paper proposes a new network called IFE-Net to detect three types of tampered images, namely copy-move, heterologous splicing and removal. Firstly, this paper constructs the noise stream using the attention mechanism CBAM to extract and optimize the noise features. The high-level features are extracted by the backbone network of RGB stream, and the FEASPP module is built for capturing and enhancing the features at different scales. In addition, in this paper, the initial features of RGB stream are additionally supervised so as to limit the detection area and reduce the false alarm. Finally, the final prediction results are obtained by fusing the noise features with the RGB features through the Dual Attention Mechanism (DAM) module. Extensive experimental results on multiple standard datasets show that IFE-Net can accurately locate the tampering region and effectively reduce false alarms, demonstrating superior performance.},
  archive      = {J_ICV},
  author       = {Lichao Su and Chenwei Dai and Hao Yu and Yun Chen},
  doi          = {10.1016/j.imavis.2024.105342},
  journal      = {Image and Vision Computing},
  month        = {1},
  pages        = {105342},
  shortjournal = {Image Vis. Comput.},
  title        = {IFE-net: Integrated feature enhancement network for image manipulation localization},
  volume       = {153},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A lightweight depth completion network with spatial efficient fusion. <em>ICV</em>, <em>153</em>, 105335. (<a href='https://doi.org/10.1016/j.imavis.2024.105335'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Depth completion is a low-level task rebuilding the dense depth from a sparse set of measurements from LiDAR sensors and corresponding RGB images. Current state-of-the-art depth completion methods used complicated network designs with much computational cost increase, which is incompatible with the realistic-scenario limited computational environment. In this paper, we explore a lightweight and efficient depth completion model named Light-SEF. Light-SEF is a two-stage framework that introduces local fusion and global fusion modules to extract and fuse local and global information in the sparse LiDAR data and RGB images. We also propose a unit convolutional structure named spatial efficient block (SEB), which has a lightweight design and extracts spatial features efficiently. As the unit block of the whole network, SEB is much more cost-efficient compared to the baseline design. Experimental results on the KITTI benchmark demonstrate that our Light-SEF achieves significant declines in computational cost (about 53% parameters, 50% FLOPs & MACs, and 36% running time) while showing competitive results compared to state-of-the-art methods.},
  archive      = {J_ICV},
  author       = {Zhichao Fu and Anran Wu and Zisong Zhuang and Xingjiao Wu and Jun He},
  doi          = {10.1016/j.imavis.2024.105335},
  journal      = {Image and Vision Computing},
  month        = {1},
  pages        = {105335},
  shortjournal = {Image Vis. Comput.},
  title        = {A lightweight depth completion network with spatial efficient fusion},
  volume       = {153},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Detection of fractional difference in inter vertebral disk MRI images for recognition of low back pain. <em>ICV</em>, <em>153</em>, 105333. (<a href='https://doi.org/10.1016/j.imavis.2024.105333'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Low Back Pain (LBP) diagnosis through MR images of IVDs is a challenging task due to complex spinal anatomy and varying image quality. These factors make it difficult to analyse and segment IVD images accurately. Further, simple metrics are ineffective in interpreting nuanced features from IVD images for accurate diagnoses. Overcoming these challenges is crucial to improving the precision and reliability of IVD-based LBP diagnosis. Also, the existing systems have a very high false negative rate pushes the system towards less use. This research study proposes a new framework for the detection of LBP symptoms using the Otsu Segmented Structural and Gray-Level Co-occurrence Matrix (GLCM) feature-based ML-model (OSSG-ML model) that eliminates manual intervention for low back pain detection. The proposed framework uses Otsu segmentation’s dynamic thresholding to differentiate spinal and backdrop pixel clusters. The segmented image is then used by the feature extraction using GLCM and Wavelet-Fourier module to extract two types of features. The first feature type analyzes the structural variation between normal and low back pain symptom patients. The second feature type detects LBP using statistical measures in image analysis and texture recognition of the MRI IVD segmented image. Various machine learning models are built for LBP detection, utilizing both features separately. First, the model employs structural and geometric differences, while the second model analyzes statistical measurements. On evaluating the model’s performance, it accurately detects low back pain with a 98 to 100% accuracy rate and a very low false negative rate.},
  archive      = {J_ICV},
  author       = {Manvendra Singh and Md. Sarfaraj Alam Ansari and Mahesh Chandra Govil},
  doi          = {10.1016/j.imavis.2024.105333},
  journal      = {Image and Vision Computing},
  month        = {1},
  pages        = {105333},
  shortjournal = {Image Vis. Comput.},
  title        = {Detection of fractional difference in inter vertebral disk MRI images for recognition of low back pain},
  volume       = {153},
  year         = {2025},
}
</textarea>
</details></li>
</ul>

</body>
</html>
