<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>NEUCOM</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="neucom">NEUCOM - 202</h2>
<ul>
<li><details>
<summary>
(2026). Boosting graph neural network via dynamic structure refinement for multi-omics cancer classification. <em>NEUCOM</em>, <em>662</em>, 132006. (<a href='https://doi.org/10.1016/j.neucom.2025.132006'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cancer ranks among the foremost causes of mortality, and its complex pathological features present formidable obstacles to precision therapies. Integrating multi-omics data, which captures molecular information across diverse scales, is essential for a holistic understanding of tumor biology and for informing personalized treatment strategies. Graph neural networks (GNNs) have emerged as a powerful tool for mining the intricate relationships inherent in multi-omics datasets and for achieving accurate biomedical classification. Nevertheless, their performance is critically dependent on graph quality, and prevailing methods often construct semantic graphs directly from raw omics features, introducing spurious connections that degrade model accuracy. To overcome this limitation, we propose RGancer, a GNN framework with graph refinement that dynamically adjusts the graph structure to optimize message passing. In RGancer, each edge is augmented with two key attributes: (i) a Euclidean distance computed from learned node representations, reflecting the semantic relevance between node pairs; and (ii) an energy score derived from an energy-based model, which flags edges likely to be corrupted by noise. The product of these attributes yields an edge interference score, a fine-grained metric that guides the reconstruction of each node’s receptive field, thereby attenuating the influence of long-range neighbors with negative semantics during message propagation. Furthermore, we assemble a global graph linking multiple omics from the same patient to facilitate cross-modality feature interaction. Experiments across real-world cancer cohorts demonstrate that RGancer consistently outperforms advanced models in cancer subtype/grade classification.},
  archive      = {J_NEUCOM},
  author       = {Sheng Huang and Sifan Chen and Sujia Huang and Wenzhe Liu and Zhiyu Chen},
  doi          = {10.1016/j.neucom.2025.132006},
  journal      = {Neurocomputing},
  month        = {1},
  pages        = {132006},
  shortjournal = {Neurocomputing},
  title        = {Boosting graph neural network via dynamic structure refinement for multi-omics cancer classification},
  volume       = {662},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). YOLO-RD: Road defect detection with context-aware attention and balanced loss. <em>NEUCOM</em>, <em>662</em>, 132000. (<a href='https://doi.org/10.1016/j.neucom.2025.132000'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Road defects compromise transportation safety and infrastructure durability, making timely and accurate detection crucial for effective maintenance and risk mitigation. In the context of automated road monitoring, existing algorithms face significant challenges in detailed feature extraction, semantic ambiguity, and severe class imbalance, particularly in complex road environments. To overcome these limitations, we propose YOLO-RD—a lightweight yet high-precision framework for road defect detection. First, we introduce the Convolution Spatial-to-Depth Attention Fusion (CSAF) module, which dynamically integrates detailed and semantic information to enhance feature extraction and capture subtle defect characteristics. Second, the Local-Global Enhanced Context Attention (LGECA) mechanism is designed to combine global and local attention, effectively resolving semantic ambiguities and enriching contextual representation. Third, we present the Smoothed Root-Weighted Binary Cross Entropy (SR-WBCE) loss function to mitigate class imbalance by re-weighting rare and common defect categories. Experimental evaluations on the RM-RDD dataset demonstrate that YOLO-RD, compared to the baseline YOLOv8s, reduces the number of parameters, computational complexity, and model size by 39.8 %, 12.1 %, and 39.9 %, respectively. At the same time, it achieves improvements in F1-score, mAP@0.5, and mAP@0.5:0.95 by 1.1 %, 1.9 %, and 1.7 %, reaching final values of 65.8 %, 66.7 %, and 41.2 %, respectively. Moreover, comprehensive comparative analyses against a series of state-of-the-art YOLO algorithms across the RM-RDD, RDD2022, and N-RDD2024 datasets further demonstrate YOLO-RD’s significant advancements, confirming its superior performance and potential for robust, real-world road defect detection.},
  archive      = {J_NEUCOM},
  author       = {Peng Wang and Longqi Cheng and Jiamei Liu and Haibin Wang and Decheng Wu and Gang Ma and Wanjing Ma},
  doi          = {10.1016/j.neucom.2025.132000},
  journal      = {Neurocomputing},
  month        = {1},
  pages        = {132000},
  shortjournal = {Neurocomputing},
  title        = {YOLO-RD: Road defect detection with context-aware attention and balanced loss},
  volume       = {662},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). TOMPEI-CMMD: An adaptive radiological feature analysis using unitary U-net for high-precision mammography segmentation. <em>NEUCOM</em>, <em>662</em>, 131982. (<a href='https://doi.org/10.1016/j.neucom.2025.131982'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Accurate and reliable segmentation of lesions in mammograms is an essential task for early diagnosis of breast cancer and treatment planning. The traditional deep learning models, particularly U-Net and its transformer-based variants, have become the standard, they typically rely on static(fixed), spatially invariant operators. These fixed operators may not be optimal for capturing the vast morphological diversity of lesions. To address this limitation, inspired by the characterization of unitary matrix integrals through linear differential equations, we introduce a novel segmentation architecture, the Robust Unitary Flow U-Net (UF-U-Net) with ROI-Conditioned features, designed to overcome it. The core innovation is a data-dependent processing block that generates a unique, geometry-preserving feature transformation for each specific lesion it encounters. This is achieved by conditioning the parameters of a low-rank unitary flow on the features extracted from a region-of-interest (ROI) mask, resulting in the ROI-conditioned low-rank unitary flow (RC-LR-UF). We conducted a rigorous comparative analysis against four strong baselines: U-Net, U-Net+ +, Swin-Unet, and TransUNet, using an identical, high-performance training and evaluation pipeline. On a held-out internal test set from the CMMD dataset, our proposed model achieved the first benchmark result for this dataset, with a Dice score of 0.8385 and an IoU of 0.7219, significantly outperforming all baselines. To demonstrate its generalization capabilities, the model was then tested on a completely unseen external dataset, CBIS-DDSM, achieving a higher Dice score of 0.8572 and an IoU of 0.7501. These results demonstrate the significant potential of incorporating concepts from random matrix theory into adaptive, data-dependent operators that set new benchmarks in medical image segmentation.},
  archive      = {J_NEUCOM},
  author       = {V. Kalpana and S. Thilagamani},
  doi          = {10.1016/j.neucom.2025.131982},
  journal      = {Neurocomputing},
  month        = {1},
  pages        = {131982},
  shortjournal = {Neurocomputing},
  title        = {TOMPEI-CMMD: An adaptive radiological feature analysis using unitary U-net for high-precision mammography segmentation},
  volume       = {662},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Spectral decomposition and adaptation for non-stationary time series anomaly detection. <em>NEUCOM</em>, <em>662</em>, 131978. (<a href='https://doi.org/10.1016/j.neucom.2025.131978'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Unsupervised anomaly detection in time series data is crucial for identifying unusual patterns across various fields. However, existing methods often struggle when dealing with non-stationary time series, constraining their practical application. In this paper, we delve into the challenges surrounding non-stationary time series and put forward a novel framework along with a test-time adaptation strategy. When it comes to the framework, non-stationary time series pose difficulties for modeling due to their blend of time-invariant statistics and evolving temporal dependencies. To address this issue, we explicitly break down the input into variant and invariant components through spectral analysis, with the aim of separately modeling these aspects. Besides, the absence of anomalies during training leads to significant distribution discrepancies between training and testing phases, which is ignored by most existing methods. To deal with this, we propose a flexible test-time adaptation strategy to further amplify the normal-abnormal distinguishability. Our proposed S pectral D ecomposition and A daptation method (SDA) outperforms existing detection frameworks in terms of effectiveness and efficiency. Specifically, compared to state-of-the-art models, SDA achieves superior performance while reducing training time by 66.2 % and memory usage by 98.8 % .},
  archive      = {J_NEUCOM},
  author       = {Huanyu Zhang and Yi-Fan Zhang and Jian Liang and Zhang Zhang and Liang Wang},
  doi          = {10.1016/j.neucom.2025.131978},
  journal      = {Neurocomputing},
  month        = {1},
  pages        = {131978},
  shortjournal = {Neurocomputing},
  title        = {Spectral decomposition and adaptation for non-stationary time series anomaly detection},
  volume       = {662},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). How secure is forgetting? linking machine unlearning to machine learning attacks. <em>NEUCOM</em>, <em>662</em>, 131971. (<a href='https://doi.org/10.1016/j.neucom.2025.131971'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As Machine Learning (ML) continues to evolve, so does the sophistication of security threats targeting data privacy and model integrity. In response, Machine Unlearning (MU) has emerged as a promising paradigm that enables the selective removal of data influence from trained models. By supporting compliance with privacy regulations (such as the GDPR’s right to be forgotten) and facilitating model refinement, MU holds significant practical and legal value. Additionally, the effective deployment of MU introduces new security concerns. In real-world settings, malicious actors may exploit vulnerabilities in MU mechanisms, such as incomplete or inaccurate data removal, to infer deleted information, reintroduce adversarial behavior, or manipulate model updates. These risks highlight the urgency of understanding how classical ML threats relate to the design and operation of MU systems. However, despite its growing relevance, this intersection remains underexplored. In this article, we present a structured analysis of four major attack classes in ML (Backdoor Attacks, Membership Inference Attacks, Adversarial Attacks, and Inversion Attacks) and examine their implications for MU across multiple dimensions: (i) as direct threats targeting MU mechanisms, (ii) as challenges that MU can potentially mitigate, (iii) as evaluation metrics to measure the effectiveness and performance of MU techniques, and (iv) as verification factors to validate the success and completeness of the unlearning process. We note that not all attacks exhibit all these perspectives simultaneously; their relevance varies depending on the attack characteristics and MU scenario. We also propose a novel classification that reflects how these attacks are typically employed in this context. Finally, we identify open challenges, including ethical considerations, and highlight promising directions for future research to advance secure and privacy-preserving Machine Unlearning.},
  archive      = {J_NEUCOM},
  author       = {Muhammed Shafi K.P. and Serena Nicolazzo and Antonino Nocera and Vinod P.},
  doi          = {10.1016/j.neucom.2025.131971},
  journal      = {Neurocomputing},
  month        = {1},
  pages        = {131971},
  shortjournal = {Neurocomputing},
  title        = {How secure is forgetting? linking machine unlearning to machine learning attacks},
  volume       = {662},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). PDAViT: Pyramid dual-attention vision transformer. <em>NEUCOM</em>, <em>662</em>, 131966. (<a href='https://doi.org/10.1016/j.neucom.2025.131966'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, multi-scale visual transformers have demonstrated impressive results. Spatial Reduction (SR) in multi-scale transformers is a commonly employed technique to reduce the resolution of key-value pairs for efficient memory management, encompassing 2D convolutional reduction, shifted window mechanism, and pooling reduction. This paper presents a novel approach that combines point-wise convolution, unfold operation, and self-attention to achieve effective spatial reduction. This approach, called spatial reduction dual-attention (SRDA), focuses on self-checking the inner similarity between key and value tokens before applying the second self-attention. Compared to the traditional 2D convolutional design, SRDA reduces computational complexity by a factor of k 2 (kernel size). Based on SRDA, we propose a new Pyramid Dual-Attention Vision Transformer named PDAViT. Extensive experiments demonstrate that our models achieve state-of-the-art classification performance. Specifically, PDAViT-B and PDAViT-L achieve 84.1 % and 84.6 % top-1 accuracy on the IN-1 k dataset, using only 36.5 M and 49.8 M parameters, respectively. Moreover, PDAViT-L ↑ 384 achieves 87.5 % top-1 accuracy with only 50 M parameters and 32 G FLOPs after fine-tuning on IN-21 k. In downstream tasks, the PDAViT-B outperforms the MViTv2-S by 0.8 AP b and 0.6 AP m with Cascade Mask-RCNN 3 × on COCO and surpasses the Focal-T model by 2.4 and 2.3 mIoU/MS mIoU with UperNet on ADE20k. These results demonstrate that PDAViT is a robust backbone for various challenging computer vision tasks.},
  archive      = {J_NEUCOM},
  author       = {Xin Zhou and Zeyu Jiang and Zhaohui Ren and Yongchao Zhang and Tianzhuang Yu and Wenyao Ji and Zheng Liu},
  doi          = {10.1016/j.neucom.2025.131966},
  journal      = {Neurocomputing},
  month        = {1},
  pages        = {131966},
  shortjournal = {Neurocomputing},
  title        = {PDAViT: Pyramid dual-attention vision transformer},
  volume       = {662},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Local geometry-enhanced anchor learning for multi-view clustering. <em>NEUCOM</em>, <em>662</em>, 131953. (<a href='https://doi.org/10.1016/j.neucom.2025.131953'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Anchor-based multi-view clustering has emerged as a promising paradigm for analyzing large-scale datasets, offering significant improvements in computational and space efficiency compared to traditional multi-view clustering methods. However, existing anchor learning strategies are often difficult to adapt to the heterogeneity among different views, which leads to inadequate expression of anchors and affects the clustering performance. To address this issue, we propose a novel Local Geometry-Enhanced Anchor Learning for Multi-View Clustering (LGEAC) model. Specifically, the coarse-grained anchor learning mechanism maps the view anchors to the consensus space through a transformation matrix to avoid the forced sharing of single anchors, thus enhancing the effectiveness and expressiveness of the framework. In addition, we introduce Laplace Regularization to effectively model fine-grained geometric structures, enabling the capture of local geometric patterns within the samples. Furthermore, a novel regularization strategy is proposed to pursue a more discriminative representation. By integrating these innovations, LGEAC ensures a seamless unification of anchor generation and representation learning, improving the discrimination and quality of the learned representation. Extensive experimental results demonstrate the superior performance and effectiveness of the proposed method on multiple evaluation metrics.},
  archive      = {J_NEUCOM},
  author       = {Zisen Kong and Zhiqiang Fu and Dongxia Chang and Yiming Wang and Pengyuan Li and Yao Zhao},
  doi          = {10.1016/j.neucom.2025.131953},
  journal      = {Neurocomputing},
  month        = {1},
  pages        = {131953},
  shortjournal = {Neurocomputing},
  title        = {Local geometry-enhanced anchor learning for multi-view clustering},
  volume       = {662},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). High-performance multi-agent path finding in high-obstacle-density and large-size maps. <em>NEUCOM</em>, <em>662</em>, 131943. (<a href='https://doi.org/10.1016/j.neucom.2025.131943'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Reinforcement Learning (RL) is an attractive solution to the Multi-Agent Path Finding problem due to its scalability compared to search-based approaches. However, existing RL-based methods often suffer from learning instability and poor performance on complex maps that require significant coordination for collision-free path planning. This is primarily because they overlook non-stationarity and rely on individual reward functions conditioned on the goal distance. To address this limitation, we propose the Proximal Value Decomposition Network (PVDN). PVDN enhances the individual reward through potential-based reward shaping to ensure consistent policy performance regardless of goal distance. It trains the agent and its immediate neighbors by maximizing the team reward, namely, the sum of the individual rewards, to alleviate the exponential growth of action-observation space and memory demands. To eliminate the non-stationarity, PVDN employs the centralized training with decentralized execution paradigm, where the joint Q function is decomposed into individual Q functions. Benefiting from this paradigm, PVDN can also achieve credit assignment and ensure policy consistency between centralized policy and individual policies. Experimental results on a 160 × 160 random map with 30 % obstacles and 1024 agents show that PVDN outperforms the existing RL-based planners by a large margin and can fully solve the task when goal selection is restricted such that at least 3 out of the 4 cardinally adjacent cells are obstacle-free.},
  archive      = {J_NEUCOM},
  author       = {Shiguang Sun and Chang Tang and Shitao Chen and Zeyang Liu and Xingyu Chen and Zhiqiang Tian and Xuguang Lan},
  doi          = {10.1016/j.neucom.2025.131943},
  journal      = {Neurocomputing},
  month        = {1},
  pages        = {131943},
  shortjournal = {Neurocomputing},
  title        = {High-performance multi-agent path finding in high-obstacle-density and large-size maps},
  volume       = {662},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). 3D-CDNeT: Cross-domain learning with enhanced speed and robustness for point cloud recognition. <em>NEUCOM</em>, <em>662</em>, 131939. (<a href='https://doi.org/10.1016/j.neucom.2025.131939'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Despite progress in 3D object recognition using deep learning (DL), challenges such as domain shift, occlusion, and viewpoint variations hinder robust performance. Additionally, the high computational cost and lack of labeled data limit real-time deployment in applications such as autonomous driving and robotic manipulation. To address these challenges, we propose 3D-CDNeT, a novel cross-domain deep learning network designed for unsupervised learning, enabling efficient and robust point cloud recognition. At the core of our model is a lightweight graph-infused attention encoder (GIAE) that enables effective feature interaction between the source and target domains. It not only improves recognition accuracy but also reduces inference time, which is essential for real-time applications. To enhance robustness and adaptability, we introduce a feature invariance learning module (FILM) using contrastive loss for learning invariant features. In addition, we adopt a Generative Decoder (GD) based on a Variational Auto-Encoder (VAE) to model diverse latent spaces and reconstruct meaningful 3D structures from the point cloud. This reconstruction process acts as a self-supervised generative objective that complements the discriminative recognition task, guiding the encoder to learn structure-preserving and domain-invariant features that improve recognition under occlusion and cross-domain conditions. Our proposed model unifies generative and discriminative tasks by using self-attention on the object covariance matrix to facilitate efficient information exchange, enabling the extraction of both local and global features. We further develop a self-supervised pretraining strategy that learns both global and local object invariances through GIAE and GD, respectively. A new loss function, combining contrastive loss and Chamfer distance, is proposed to strengthen cross-domain feature alignment. Experimental results on three benchmark datasets demonstrate that 3D-CDNeT outperforms existing state-of-the-art (SOTA) methods in recognition accuracy and inference speed, offering a practical solution for real-time 3D perception tasks. It achieves accuracies of 90.6 % on ModelNet40, 95.2 % on ModelNet10, and 76.4 % on the ScanObjectNN dataset in linear evaluation tasks, all while reducing runtime by 45 % without compromising performance. Detailed qualitative comparisons and ablation studies are provided to validate the effectiveness of each component and demonstrate the superior performance of our proposed method.},
  archive      = {J_NEUCOM},
  author       = {Abu Bakor Hayat Arnob and A.A.M. Muzahid and Hua Han and Yujin Zhang and Ferdous Sohel},
  doi          = {10.1016/j.neucom.2025.131939},
  journal      = {Neurocomputing},
  month        = {1},
  pages        = {131939},
  shortjournal = {Neurocomputing},
  title        = {3D-CDNeT: Cross-domain learning with enhanced speed and robustness for point cloud recognition},
  volume       = {662},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). OFMAD-TC: A tropical cyclone detection method with optical flow and morphology awareness. <em>NEUCOM</em>, <em>662</em>, 131936. (<a href='https://doi.org/10.1016/j.neucom.2025.131936'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Tropical cyclone (TC) detection is a vital component of coastal disaster prevention systems, with TC object identification and TC center localization being the two key technical challenges. While conventional methodologies have long served as the foundation for TC analysis, deep learning approaches have gradually emerged and evolved as complementary solutions, offering new perspectives on addressing these challenges. However, most existing approaches emphasize static features, while underutilizing the temporal dynamics embedded in multi-frame observations. To address this limitation, we propose OFMAD-TC, a novel framework that leverages optical flow and morphological information to capture the spatiotemporal characteristics of TCs. Specifically, our method employs a deep learning-based optical flow algorithm to extract motion features from consecutive TC image sequences. In addition, we introduce a query mechanism designed to capture TC morphological patterns, which improves the extraction of shape-related features and enhances detection performance. Extensive experiments on publicly available datasets demonstrate the effectiveness of our method, with improvements in detection accuracy.},
  archive      = {J_NEUCOM},
  author       = {Xiaoxian Tian and Lu Yang and Chongke Bi and Ce Yu},
  doi          = {10.1016/j.neucom.2025.131936},
  journal      = {Neurocomputing},
  month        = {1},
  pages        = {131936},
  shortjournal = {Neurocomputing},
  title        = {OFMAD-TC: A tropical cyclone detection method with optical flow and morphology awareness},
  volume       = {662},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). IPIRNNs: Integration-based physics informed randomized neural networks for solving stiff ODEs. <em>NEUCOM</em>, <em>662</em>, 131911. (<a href='https://doi.org/10.1016/j.neucom.2025.131911'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A novel framework of the integration-based physics-informed randomized neural networks (IPIRNNs) is put forward in this study that presents a framework for accurate and computationally efficient stiff ordinary differential equations (ODEs) solution. By transforming the stiff ODEs into integral equations, IPIRNNs reduce the numerical unstable behavior that is caused by the differential equations characterized by disparate time scales eliminating the necessity to make very tiny time steps, which is a problem inherent in typical differential equations. The approach combines randomized neural networks, such as extreme learning machines (ELMs), in which we initialize the hidden-layer parameters randomly and compute the output weights analytically through the least square method, eliminating gradient-based optimization and speeding up the learning process. The framework is validated by thorough numerical experiments that include linear/nonlinear stiff ODEs, Lotka-Volterra systems, and chemical kinetics models. The results indicate that IPIRNNs deliver higher accuracy (up to a 10 −10 error reduction) and stability compared to the traditional Physics-Informed Randomized Neural Networks (PIRNNs) and ODE15s solver in MATLAB, especially when applied to high-stiffness conditions. The most important benefits involve the prevention of large condition numbers of least square system, adaptive scalability with increasing network size (from seconds to some minutes for large-scale problems). Due to these results, IPIRNNs have become a robust method for stiff systems, with great developments, such as in solid mechanics, fluid mechanics, and multi-scale phenomena.},
  archive      = {J_NEUCOM},
  author       = {Hang Zhou and Zhuhong Wang and Geping Qi and Yisheng Wang},
  doi          = {10.1016/j.neucom.2025.131911},
  journal      = {Neurocomputing},
  month        = {1},
  pages        = {131911},
  shortjournal = {Neurocomputing},
  title        = {IPIRNNs: Integration-based physics informed randomized neural networks for solving stiff ODEs},
  volume       = {662},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). HyCTAS: Multi-objective hybrid convolution-transformer architecture search for real-time image segmentation. <em>NEUCOM</em>, <em>662</em>, 131909. (<a href='https://doi.org/10.1016/j.neucom.2025.131909'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Real-time image segmentation demands architectures that preserve fine spatial detail while capturing global context under tight latency and memory budgets. Image segmentation is one of the most fundamental problems in computer vision and has drawn significant attention due to its vast applications in image understanding and autonomous driving. However, designing effective and efficient segmentation neural architectures is a labor-intensive process that may require numerous trials by human experts. In this paper, we address the challenge of integrating multi-head self-attention into high-resolution representation CNNs efficiently by leveraging architecture search. Manually replacing convolution layers with multi-head self-attention is non-trivial due to the costly overhead in memory required to maintain high resolution. By contrast, we develop a multi-target multi-branch supernet method, which not only fully utilizes the advantages of high-resolution features but also identifies the proper location for placing the multi-head self-attention module. Our search algorithm is optimized towards multiple objectives (e.g., latency and mIoU) and is capable of finding architectures on the approximate Pareto front with an arbitrary number of branches in a single search. We further present a series of models via the Hybrid Convolutional-Transformer Architecture Search (HyCTAS) method that searches for the best hybrid combination of lightweight convolution layers and memory-efficient self-attention layers between branches from different resolutions and fuses them at high resolution for both efficiency and effectiveness. On Cityscapes, ADE20K, and COCO, HyCTAS discovers competitive real-time models without ImageNet pretraining, delivering strong accuracy and latency trade-offs. Code and models are available at https://github.com/MarvinYu1995/HyCTAS .},
  archive      = {J_NEUCOM},
  author       = {Hongyuan Yu and Cheng Wan and Xiyang Dai and Mengchen Liu and Dongdong Chen and Bin Xiao and Yan Huang and Yuan Lu and Liang Wang},
  doi          = {10.1016/j.neucom.2025.131909},
  journal      = {Neurocomputing},
  month        = {1},
  pages        = {131909},
  shortjournal = {Neurocomputing},
  title        = {HyCTAS: Multi-objective hybrid convolution-transformer architecture search for real-time image segmentation},
  volume       = {662},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Influence maximization in complex networks using community-structure-based technique with multi-level information fusion. <em>NEUCOM</em>, <em>662</em>, 131875. (<a href='https://doi.org/10.1016/j.neucom.2025.131875'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Exploring the problem of influence maximization (IM) in complex networks is of great significance in practical applications. Current community-structure-based methods suffer from such limitations as overlooking global network information, ignoring node characteristic and diversity when distinguishing communities, and failing to detect small-scale communities when using modularity-optimization-based community detection algorithms. For that reason, this study originally proposes a novel community-structure-based influence maximization technique called Importance Differentiated Community-based Multi-level Information Fusion (CMIF) model by integrating semi-local, local, and global information. The framework of the proposed technique involves four main components: a newly-developed Influential Identity Label Diffusion (IILD) algorithm which can achieve high-quality community detection with low computational cost, a hybrid information filtering mechanism designed for differentiating community importance by considering node characteristics and diversity, a novel ranking scheme called Multi-dimensional Hierarchical Ranking (MHR) for refining node ordering within communities, and an Overlap-Reduced Deferred Ordering (ORDO) strategy used to reduce influence overlap and mitigate the rich-club phenomenon. Extensive experiments validate the effectiveness of our proposals. The IILD algorithm achieves competitive performance compared with 20 state-of-the-art community detection algorithms on 14 real-world networks, demonstrating greater stability and efficiency across diverse network scales. Meanwhile, the CMIF model outperforms 19 advanced IM techniques on eight real-world networks, achieving superior diffusion performance and robustness.},
  archive      = {J_NEUCOM},
  author       = {Yi Liu and Xiaoan Tang and Qiang Zhang and Witold Pedrycz},
  doi          = {10.1016/j.neucom.2025.131875},
  journal      = {Neurocomputing},
  month        = {1},
  pages        = {131875},
  shortjournal = {Neurocomputing},
  title        = {Influence maximization in complex networks using community-structure-based technique with multi-level information fusion},
  volume       = {662},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Reducing hallucination in MLLMs for action recognition with adaptive masking and dual prompting. <em>NEUCOM</em>, <em>662</em>, 131861. (<a href='https://doi.org/10.1016/j.neucom.2025.131861'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Existing multimodal large language models show promise in action recognition, especially in complex scenes and similar actions. However, they are vulnerable to irrelevant visual background elements and biases stemming from learned language priors, leading to hallucination—inaccurate predictions caused by incorrect associations. To mitigate these challenges, we propose a dual-strategy collaborative framework. First, we design an adaptive dynamic masking strategy that computes frame-level weights using self-attention, with a dynamically determined masking rate via normal distribution sampling, to filter out irrelevant visual content. Second, we combine instruction-based and reverse prompting strategies. Instruction-based prompts clearly define task objectives, guiding the model to focus on essential actions. Reverse prompts question the model’s reasoning process, encouraging continuous evaluation and preventing reliance on outdated language patterns. Extensive experiments demonstrate that our approach significantly mitigates hallucinations arising from both visual distractions and language prior biases, improving the accuracy in action recognition.},
  archive      = {J_NEUCOM},
  author       = {Minglin Hong and Bo Sun and Jun He and Yinghui Zhang and Li Yuan},
  doi          = {10.1016/j.neucom.2025.131861},
  journal      = {Neurocomputing},
  month        = {1},
  pages        = {131861},
  shortjournal = {Neurocomputing},
  title        = {Reducing hallucination in MLLMs for action recognition with adaptive masking and dual prompting},
  volume       = {662},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). TabSynergy: Leveraging large language models for high-quality feature generation and augmentation in small tabular datasets. <em>NEUCOM</em>, <em>662</em>, 131859. (<a href='https://doi.org/10.1016/j.neucom.2025.131859'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The challenge of limited training data remains a significant obstacle to developing effective machine learning models, creating strong demand for specialized data augmentation methods tailored to small-scale tabular data. Existing augmentation techniques often struggle due to poor data quality, incomplete distribution capture, and unstable training. To address these issues, we propose TabSynergy, a novel approach that leverages prior knowledge from large language models to iteratively generate high-quality tabular features, identifies informative context subsets via a multi-subset sampling strategy, and applies a K-Nearest Neighbors-based graph attention network to capture both feature-level and structural information. Experiments demonstrate that TabSynergy achieves state-of-the-art classification accuracy across various datasets, preserving feature diversity and exhibiting stable, reliable performance, making it particularly suited for real-world applications, including an educational project where it has already been implemented.},
  archive      = {J_NEUCOM},
  author       = {Sen Shen and Zhixuan Xiao and Dongkun Han},
  doi          = {10.1016/j.neucom.2025.131859},
  journal      = {Neurocomputing},
  month        = {1},
  pages        = {131859},
  shortjournal = {Neurocomputing},
  title        = {TabSynergy: Leveraging large language models for high-quality feature generation and augmentation in small tabular datasets},
  volume       = {662},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). LHNet: Lightweight hybrid network with multi-scale sliding window attention for real-time semantic segmentation. <em>NEUCOM</em>, <em>662</em>, 131857. (<a href='https://doi.org/10.1016/j.neucom.2025.131857'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Convolutional Neural Networks (CNNs) and Transformers have made significant progress in the field of semantic segmentation. Numerous works have explored combining CNN with Transformer-based architectures to effectively model both local and global semantic contexts. However, under constrained computational resources, there remains room for improvement. To address the challenges of lightweight semantic segmentation, we propose a novel model called LHNet, which combines the spatial detail extraction capability of CNN with the global dependency modeling strength of Transformers. Specifically, LHNet adopts a hybrid parallel architecture and consists of several key components, including a Spatial-Channel Adaptive Feature Module (SCAFM), a detail branch, a semantic branch, an information fusion module, and an efficient segmentation head. The proposed SCAFM enhances feature representation through multi-scale feature modulation and dynamic channel interaction, effectively overcoming the locality limitation of conventional convolution. The detail branch, built upon CNN, adopts a Compress-Enhance-Restore structure to preserve spatial detail information. The semantic branch, based on a Transformer architecture, utilizes a Multi-Scale Sliding Window Attention (MSSWA) mechanism to reduce computational overhead while strengthening long-range dependency modeling. Comprehensive experiments on commonly used datasets show that LHNet achieves competitive accuracy with only 2.95 M parameters. Specifically, LHNet reaches 76.6 % mIoU with 33.15 GFLOPs on the Cityscapes dataset and 39.78 % mIoU with 3.64 GFLOPs on the Pascal Context dataset.},
  archive      = {J_NEUCOM},
  author       = {Zhehan Liu and Bin Liu and Zhiyu Tao and Yangming Zhou and Chuzhao Li},
  doi          = {10.1016/j.neucom.2025.131857},
  journal      = {Neurocomputing},
  month        = {1},
  pages        = {131857},
  shortjournal = {Neurocomputing},
  title        = {LHNet: Lightweight hybrid network with multi-scale sliding window attention for real-time semantic segmentation},
  volume       = {662},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Selective black-box adversarial examples: Protecting a specific model while inducing misperceptions in others. <em>NEUCOM</em>, <em>662</em>, 131848. (<a href='https://doi.org/10.1016/j.neucom.2025.131848'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper addresses the susceptibility of deep neural networks to adversarial examples, which are perturbations causing misinterpretations by the model. Black-box transfer attacks, where adversarial examples from one model induce misclassifications in other unknown models, are a significant concern. The proposed method introduces selective black-box adversarial examples, correctly recognized by one model while causing misinterpretations in undisclosed models. The “styless” method preserves the original content while introducing style-related noise for effective black-box attacks. We utilized the PyTorch machine learning library as our experimental environment, with the ImageNet dataset serving as our dataset. Experimental results indicate that, with an epsilon value of 0.03 on the ImageNet dataset, our proposed adversarial examples achieve an average accuracy of 94 % for the protected model, an average accuracy of 11 % for the stylized model, and an average accuracy of 15 % for the other model.},
  archive      = {J_NEUCOM},
  author       = {Hyun Kwon and Jang-Woon Baek},
  doi          = {10.1016/j.neucom.2025.131848},
  journal      = {Neurocomputing},
  month        = {1},
  pages        = {131848},
  shortjournal = {Neurocomputing},
  title        = {Selective black-box adversarial examples: Protecting a specific model while inducing misperceptions in others},
  volume       = {662},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). MK-SGN: A spiking graph convolutional network with multimodal fusion and knowledge distillation for skeleton-based action recognition. <em>NEUCOM</em>, <em>662</em>, 131796. (<a href='https://doi.org/10.1016/j.neucom.2025.131796'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, multimodal Graph Convolutional Networks (GCNs) have achieved remarkable performance in skeleton-based action recognition. The reliance on high-energy-consuming continuous floating-point operations inherent in GCN-based methods poses significant challenge for deployment in energy-constrained, battery-powered edge devices. To address the limitation, MK-SGN, a Spiking Graph Convolutional Network with Multimodal Fusion and Knowledge Distillation, is proposed to leverage the energy efficiency of Spiking Neural Networks (SNNs) for skeleton-based action recognition for the first time. By integrating the energy-saving properties of SNNs with the graph representation capabilities of GCNs, MK-SGN achieves significant reduction in energy consumption while maintaining competitive recognition accuracy. Firstly, we formulate a Spiking Multimodal Fusion (SMF) module to fuse the multimodal skeleton spike-form features effectively. Secondly, we propose a Self-Attention Spiking Graph Convolution (SA-SGC) module and a Spiking Temporal Convolution (STC) module to capture spatial relationships and temporal dynamics of spike-form features, respectively. Finally, we propose an integrated knowledge distillation strategy to transfer information from the multimodal GCN to the SGN, incorporating inner-layer feature and soft-label distillation to enhance the SGN’s performance. MK-SGN exhibits substantial advantages, surpassing state-of-the-art GCN frameworks in energy efficiency and outperforming state-of-the-art SNN frameworks in recognition accuracy. The proposed method remarkably reduces energy consumption, with 98 % less consumed than conventional GCN-based approaches. In general, this research establishes a strong baseline for developing high-performance, energy-efficient SNN-based models for skeleton-based action recognition.},
  archive      = {J_NEUCOM},
  author       = {Naichuan Zheng and Hailun Xia and Zeyu Liang and Yuchen Du},
  doi          = {10.1016/j.neucom.2025.131796},
  journal      = {Neurocomputing},
  month        = {1},
  pages        = {131796},
  shortjournal = {Neurocomputing},
  title        = {MK-SGN: A spiking graph convolutional network with multimodal fusion and knowledge distillation for skeleton-based action recognition},
  volume       = {662},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Sarcasm detection via incongruity-guided gated contrast-attention and contextualized-understanding network. <em>NEUCOM</em>, <em>662</em>, 131735. (<a href='https://doi.org/10.1016/j.neucom.2025.131735'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Sarcasm detection remains a major challenge in natural language processing due to its reliance on identifying semantic incongruity between literal expressions and contextual implications. Existing approaches fail to adequately model semantic conflicts between pivotal lexical units and underutilize contextual information. To address these gaps, we propose the Gated Contrast-Attention Contextualized-Understanding (GCACU) network, a dual-path architecture that integrates contrastive semantic learning with deep contextual comprehension via gated mechanisms. The contrast-attention pathway quantifies conflicts between key lexical pairs, while the contextualized-understanding pathway leverages ELMo-enhanced bidirectional GRU networks for contextual understanding. A hierarchical gating system dynamically fuses these representations to capture both lexical-level incongruity and discourse-level context. Experimental results show that GCACU achieves an F1-score of 77.0 % on Twitter SemEval-2018, outperforming the best baseline by 0.7 %; on the IAC-V1 and IAC-V2 datasets, it reaches 67.64 % and 82.66 % in F1-score respectively, with improvements of 1.24 % and 0.56 % over the baselines. Ablation studies further validate the gating system as a critical contributor to performance gains, underscoring the effectiveness of explicit incongruity modeling for sarcasm analysis.},
  archive      = {J_NEUCOM},
  author       = {Yufeng Diao and Kai Xie and Shiqi Li and Hao Zhang and Xiaoyu Liu and Xueqian Su and Xiaochao Fan},
  doi          = {10.1016/j.neucom.2025.131735},
  journal      = {Neurocomputing},
  month        = {1},
  pages        = {131735},
  shortjournal = {Neurocomputing},
  title        = {Sarcasm detection via incongruity-guided gated contrast-attention and contextualized-understanding network},
  volume       = {662},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). GMS: Graph-guided multi-modal spatio-temporal fusion model for trajectory prediction in autonomous driving. <em>NEUCOM</em>, <em>661</em>, 132010. (<a href='https://doi.org/10.1016/j.neucom.2025.132010'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Accurate trajectory prediction in autonomous driving remains a challenging task due to the complexity of multi-agent interactions, the diversity of behavioral intent, and the long-range temporal dependencies inherent in dynamic traffic environments. In this paper, we propose GMS, a Graph-Guided Multi-Modal Spatio-Temporal Fusion model that jointly captures local interaction structures and global maneuver patterns for robust future motion forecasting. GMS integrates a graph-aware attention mechanism that dynamically captures both local interactions and temporally structured dependencies among agents, as well as a unified spatio-temporal fusion module that jointly encodes motion dynamics, spatial topology, and behavioral intent to generate coherent and context-aware trajectory predictions. Experiments on two widely used benchmarks, NGSIM and HighD, demonstrate that GMS achieves great performance under both short- and long-term prediction horizons. The proposed framework offers a generalizable and interpretable solution for trajectory prediction in real-world driving scenarios.},
  archive      = {J_NEUCOM},
  author       = {Xin Chen and Geye Gu and Xingyi Ning},
  doi          = {10.1016/j.neucom.2025.132010},
  journal      = {Neurocomputing},
  month        = {1},
  pages        = {132010},
  shortjournal = {Neurocomputing},
  title        = {GMS: Graph-guided multi-modal spatio-temporal fusion model for trajectory prediction in autonomous driving},
  volume       = {661},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Low-redundancy motor imagery EEG decoding based on dynamic attention and feature reconstruction. <em>NEUCOM</em>, <em>661</em>, 132004. (<a href='https://doi.org/10.1016/j.neucom.2025.132004'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Electroencephalography (EEG) motor imagery (MI) decoding is a critical component in the development of motor imagery brain-computer interfaces (MI-BCI). The decoding performance reflects the ability to accurately identify and classify brain activity from EEG signals, which is essential during motor imagery tasks. A major challenge lies in accurately recognizing human intent from EEG-based MI (MI-EEG) brain signal that possess low signal-to-noise ratios and instability. The effectiveness of MI classification directly depends on decoding performance, impacting the practicality and reliability of BCI systems. To address this challenge, we investigate a dynamic combinable attention-based spatial and channel reconstruction convolutional network (DCA-SCRCNet) that leverages redundant features in weak MI-EEG signals to optimize spatial distribution features and inter-channel interactions relationships. This optimization is achieved through weight-based feature reconstruction in the spatial and channel reconstruction convolution (SCRC) module. Importantly, dynamic combinable attention (DCA) enables DCA-SCRCNet to adapt to differences in the non-stationary MI-EEG signals individual characteristics by weighting input features and integrating local and global information. The processed features are then fed into the temporal convolutional network (TCN) for further extraction of long-term dependencies and finer timing characteristics. On the BCI Competition IV-2a dataset (BCI-2a), subject-independent and subject-dependent models yielded accuracies of 70.7 % and 90.5 %, respectively, demonstrating its efficient spatio-temporal feature extraction capability while reducing computational redundancy. On the PhysioNet dataset, the proposed algorithm achieved multi-subject validation accuracies of 92.7 % and 76.2 % for two and four-class classification tasks, respectively, surpassing current mainstream methods.},
  archive      = {J_NEUCOM},
  author       = {Jie Pan and Zhijie Dong and Pengjun Zhao and Jiayin Chen and Xiaoyu Zou and Jiajing Song and Liwen Zhang},
  doi          = {10.1016/j.neucom.2025.132004},
  journal      = {Neurocomputing},
  month        = {1},
  pages        = {132004},
  shortjournal = {Neurocomputing},
  title        = {Low-redundancy motor imagery EEG decoding based on dynamic attention and feature reconstruction},
  volume       = {661},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Defense against unauthorized distillation in image restoration via feature space perturbation. <em>NEUCOM</em>, <em>661</em>, 131984. (<a href='https://doi.org/10.1016/j.neucom.2025.131984'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Knowledge distillation (KD) attacks pose a significant threat to deep model intellectual property by enabling adversaries to train student networks using a teacher model’s outputs. While recent defenses in image classification have successfully disrupted KD by perturbing output probabilities, extending these methods to image restoration is difficult. Unlike classification, restoration is a generative task with continuous, high-dimensional outputs that depend on spatial coherence and fine details. Minor perturbations are often insufficient, as students can still learn the underlying mapping. To address this, we propose Adaptive Singular Value Perturbation (ASVP), a runtime defense tailored for image restoration models. ASVP operates on internal feature maps of the teacher using singular value decomposition (SVD). It amplifies the top-k singular values to inject structured, high-frequency perturbations, disrupting the alignment needed for distillation. This hinders student learning while preserving the teacher’s output quality. We evaluate ASVP across five image restoration tasks: super-resolution, low-light enhancement, underwater enhancement, dehazing, and deraining. Experiments show ASVP reduces student PSNR by up to 4 dB and SSIM by 60–75 %, with negligible impact on the teacher’s performance. Compared to prior methods, ASVP offers a stronger and more consistent defense.Our approach provides a practical solution to protect open-source restoration models from unauthorized knowledge distillation.},
  archive      = {J_NEUCOM},
  author       = {Han Hu and Zhuoran Zheng and Chen Lyu},
  doi          = {10.1016/j.neucom.2025.131984},
  journal      = {Neurocomputing},
  month        = {1},
  pages        = {131984},
  shortjournal = {Neurocomputing},
  title        = {Defense against unauthorized distillation in image restoration via feature space perturbation},
  volume       = {661},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Contrastive siamese network for detecting AI-generated text across domains and models. <em>NEUCOM</em>, <em>661</em>, 131983. (<a href='https://doi.org/10.1016/j.neucom.2025.131983'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The rapid proliferation of large language models (LLMs) has raised growing concerns about distinguishing between human-written and AI-generated text. This work addresses the task of detecting AI-generated content by evaluating the latent similarity between a given input text and an alternative response generated for the same prompt, either known or inferred. Accordingly, CLAID (Contrastive Learning for AI Detection) is proposed as a Siamese Neural Network architecture utilizing BERT encoders and contrastive loss to capture semantic similarity between text pairs. Unlike prior approaches that rely on explicit classification or domain-specific features, our method focuses on modeling pairwise similarity, enabling a flexible and model-agnostic detection framework. To evaluate the generalization capabilities of the system, a comprehensive multi-domain and multi-model benchmark comprising three diverse datasets (i.e., HC3, DAIGT, and OUTFOX), encompassing a wide range of text genres, prompt structures, and generative models, has been constructed. Experimental results show that the proposed model achieves near-perfect classification accuracy across both single-domain and mixed-domain scenarios, demonstrating strong robustness to domain shifts, prompt variability, and authorship ambiguity. The model also exhibits strong data efficiency, attaining high performance with minimal supervision.},
  archive      = {J_NEUCOM},
  author       = {Maria Di Gisi and Giuseppe Fenza and Mariacristina Gallo and Vincenzo Loia},
  doi          = {10.1016/j.neucom.2025.131983},
  journal      = {Neurocomputing},
  month        = {1},
  pages        = {131983},
  shortjournal = {Neurocomputing},
  title        = {Contrastive siamese network for detecting AI-generated text across domains and models},
  volume       = {661},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Position regularization-based temporal-spectral imputation for source-free time-series domain adaptation. <em>NEUCOM</em>, <em>661</em>, 131981. (<a href='https://doi.org/10.1016/j.neucom.2025.131981'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Source-free domain adaptation (SFDA) has been widely explored with the progress of deep learning models, which leverages the knowledge from the well-labeled source domain to classify the unlabeled target domain and protects the data privacy of the source domain at the same time. Existing SFDA-based time-series classification models utilize the cross-domain MAsking and imPUting (MAPU) process to achieve domain-invariant characteristics for classification. However, these models neglected the absolute positions of key patterns and the spectral-temporal dependencies during imputing, which caused performance bottlenecks. To address these challenges, we proposed a novel Position Regularization-based Temporal-Spectral Imputation (PRTSI) model for time-series SFDA. Specifically, we extended the MAPU process by adding an absolute position regularization and a spectral-temporal imputer. These novel modules will enhance the positions of key patterns and obtain bidirectional temporal-spectral dependencies while imputing the masked time-series. We conducted comprehensive experiments on our PRTSI model among three time-series benchmarked datasets, and its SFDA performance surpasses several state-of-the-art models. Moreover, we also conducted ablation experiments of the two extra modules for MAPU process, as well as parameters sensitivity experiments. Our model therefore provides a novel SFDA choice for time-series classification tasks while preserving privacy of data.},
  archive      = {J_NEUCOM},
  author       = {Haojie Lin and Guangju Yang and Tian-jian Luo},
  doi          = {10.1016/j.neucom.2025.131981},
  journal      = {Neurocomputing},
  month        = {1},
  pages        = {131981},
  shortjournal = {Neurocomputing},
  title        = {Position regularization-based temporal-spectral imputation for source-free time-series domain adaptation},
  volume       = {661},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). DDCT-UniSegNet: A unified framework for deepfake temporal segmentation. <em>NEUCOM</em>, <em>661</em>, 131979. (<a href='https://doi.org/10.1016/j.neucom.2025.131979'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deepfake videos present a critical challenge to the integrity of digital media, and many detection models fail to generalize to unseen deepfake content. In this paper, we propose DDCT-UniSegNet, a unified framework that integrates Denoising Diffusion Probabilistic Models (DDPM) and Temporal Convolutional Networks (TCN) to effectively identify and segment fake portions within videos. Here, pretrained DDPM improves spatial discrepancies between real and fake frames, while lightweight CNN classifiers such as ResNeXt50, MesoNet, and Xception efficiently classify frames. Subsequently, TCN segments the manipulated portions throughout the video. Our approach significantly reduces computational overhead compared to state-of-the-art transformer-based models, achieving 33 % faster performance in detecting deepfake sequences. Experimental results on the FaceForensics++ dataset demonstrate that DDCT-UniSegNet outperforms existing CNN models in speed, achieving similar AUC and IoU scores for temporal deepfake segmentation. Additionally, DDCT-UniSegNet with Xception achieves the highest accuracy exceeding 80 % while maintaining a lower inference time (0.0200s), outperforming Vision Transformers (ViT) in both speed and accuracy. We also benchmark our model against Spatio-Temporal Graph Neural Network (SSTGNN) approaches, where DDCT-UniSegNet shows competitive or superior segmentation performance with significantly reduced computational complexity. This combination makes DDCT-UniSegNet particularly suitable for real-time deepfake detection and segmentation in resource-constrained scenarios.},
  archive      = {J_NEUCOM},
  author       = {Shovon Raul and Tamal Pal},
  doi          = {10.1016/j.neucom.2025.131979},
  journal      = {Neurocomputing},
  month        = {1},
  pages        = {131979},
  shortjournal = {Neurocomputing},
  title        = {DDCT-UniSegNet: A unified framework for deepfake temporal segmentation},
  volume       = {661},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Training dynamics of GANs through the lens of persistent homology. <em>NEUCOM</em>, <em>661</em>, 131976. (<a href='https://doi.org/10.1016/j.neucom.2025.131976'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Generative Adversarial Networks (GANs) aim to produce realistic samples by mapping a low-dimensional latent space with known distribution to a high-dimensional data space, exploiting an adversarial training mechanism. However, without an effective characterisation of the generative process, such models face significant challenges in training and architecture selection. In this work, we propose a Topological Data Analysis based approach, using persistent homology, which can provide such a characterisation, where topological information of a data manifold is summarised by its Persistent Diagram and the evolution of its topological features is tracked throughout training. Our approach is applied across multiple GAN architectures using two benchmark datasets, where we demonstrate that conventional metrics such as Fréchet Inception Distance and intrinsic dimension estimates cannot adequately capture the quality of generated samples. Instead, our results confirm that a topological description of the generative process within GANs successfully captures training convergence and mode collapse. Finally, the layer-wise topological analysis determines the role each layer plays in the generative process, and may provide future guidance for refinement of architectures. Code available at https://github.com/bcorrad/genfold25.git .},
  archive      = {J_NEUCOM},
  author       = {Barbara Toniella Corradini and Ben Cullen and Caterina Gallegati and Sara Marziali and Giuseppe Alessio D’Inverno and Monica Bianchini and Franco Scarselli},
  doi          = {10.1016/j.neucom.2025.131976},
  journal      = {Neurocomputing},
  month        = {1},
  pages        = {131976},
  shortjournal = {Neurocomputing},
  title        = {Training dynamics of GANs through the lens of persistent homology},
  volume       = {661},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Disentanglement with factor quantized variational autoencoders. <em>NEUCOM</em>, <em>661</em>, 131968. (<a href='https://doi.org/10.1016/j.neucom.2025.131968'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Disentangled representation learning aims to represent the underlying generative factors of a dataset in a latent representation independently of one another. In our work, we propose a discrete variational autoencoder (VAE) based model where the ground truth information about the generative factors not provided to the model. We demonstrate the advantages of learning discrete representations over learning continuous representations in facilitating disentanglement. Furthermore, we propose incorporating an inductive bias into the model to further enhance disentanglement. Precisely, we propose scalar quantization of the latent variables in a latent representation with scalar values from a global codebook, and we add a total correlation term to the optimization as an inductive bias. Our method called FactorQVAE combines optimization based disentanglement approaches with discrete representation learning, and it outperforms previous disentanglement methods in terms of two disentanglement metrics (DCI and InfoMEC) while improving the reconstruction performance. Our code can be found at https://github.com/ituvisionlab/FactorQVAE .},
  archive      = {J_NEUCOM},
  author       = {Gulcin Baykal and Melih Kandemir and Gozde Unal},
  doi          = {10.1016/j.neucom.2025.131968},
  journal      = {Neurocomputing},
  month        = {1},
  pages        = {131968},
  shortjournal = {Neurocomputing},
  title        = {Disentanglement with factor quantized variational autoencoders},
  volume       = {661},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Time series anomaly detection methods for IoT in smart grids: A systematic literature review. <em>NEUCOM</em>, <em>661</em>, 131967. (<a href='https://doi.org/10.1016/j.neucom.2025.131967'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Smart Grid Systems (SGS) generate substantial volumes of Multivariate Time Series (MTS) data, necessitating advanced Time Series Anomaly Detection (TSAD) methodologies for effective analysis. These techniques enable anomalies to be rapidly localized and comprehensively investigated, given that minor discrepancies may trigger cascading power disruptions. This systematic literature review synthesizes 150 relevant publications (2019–2025), highlighting critical challenges and advancements in TSAD research. Comprehensive examination of TSAD is conducted through multiple analytical dimensions including series characteristics, statistical methods, machine learning techniques, and task-oriented versus model-oriented approaches. Primary challenges identified include the requirement for real-time detection capabilities to process high-dimensional dynamic data streams, alongside the need for automated classification systems addressing distributed renewable integration complexities. Existing datasets relevant to SGS and general Internet of Things (IoT) applications are systematically cataloged, establishing a methodological foundation for subsequent research.},
  archive      = {J_NEUCOM},
  author       = {Tao Deng and Haibin Li and Chengxin Ni and Ningjiang Chen},
  doi          = {10.1016/j.neucom.2025.131967},
  journal      = {Neurocomputing},
  month        = {1},
  pages        = {131967},
  shortjournal = {Neurocomputing},
  title        = {Time series anomaly detection methods for IoT in smart grids: A systematic literature review},
  volume       = {661},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Arbitrary style transfer via learning the separation and fusion of content and style. <em>NEUCOM</em>, <em>661</em>, 131965. (<a href='https://doi.org/10.1016/j.neucom.2025.131965'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Arbitrary style transfer offers a fascinating approach to artistic creation. During the research and development process, researchers primarily focus on the content structure and style representation of the image. Although the existing models have designed a variety of modules to statistically align the features of the content image and the features of the style image, the self-style information of the content image and the self-content information of the style image have not been effectively stripped away, which affects the quality of the stylized image. To address this issue, in this paper, we design a separation module to separate the content image features and style image features into content parts and style parts, respectively, and use pure content image–content parts and style image–style parts for stylization. In addition, we design a fusion module to fuse the above two. Its characteristic is that it learns fusion through the network instead of feature statistical alignment, which helps to capture more diverse fusion features. Sufficient experiments have demonstrated that the stylization effect can be improved by separation module. Compared with the mainstream fusion module, the designed fusion module performs better by learning.},
  archive      = {J_NEUCOM},
  author       = {Xiaoming Yu and Jie Tian and Zhenhua Hu},
  doi          = {10.1016/j.neucom.2025.131965},
  journal      = {Neurocomputing},
  month        = {1},
  pages        = {131965},
  shortjournal = {Neurocomputing},
  title        = {Arbitrary style transfer via learning the separation and fusion of content and style},
  volume       = {661},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Design of memory network model based on DNA strand displacement and its application in prediction. <em>NEUCOM</em>, <em>661</em>, 131964. (<a href='https://doi.org/10.1016/j.neucom.2025.131964'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, DNA strand displacement (DSD) technology has attracted considerable attention for its exceptional capabilities in parallel computing, programmability, memory capacity, and nonlinear operations. The core of DSD technology lies in the molecular principle of base complementary pairing, which drives DNA strand structural reorganization through toehold binding, branch migration and single-strand displacement. In this paper, in order to endow biological neural networks with memory properties, a design of memory network model based on DSD is proposed. First, this paper constructs six core modules based on DSD reactions, including encircling prey, bubble net attacks, search for prey, weighted sum, activation function, and the output module. Second, this paper employs the WOA for hyperparameter tuning, stores the optimized parameters within DNA strands, and constructs a memory neural network. Finally, the lithium-ion battery capacity is predicted using the DSD-based memory network model, thus achieving a cross-domain integration of biomolecular networks and performance prediction. Experimental results on lithium-ion battery capacity prediction show that the model achieves an M A E of 0.029 , an R M S E of 0.0341 and an R 2 of 0.943 . Compared to the classical memory network model ( M A E = 0.041 , R M S E = 0.0514 , R 2 = 0.912 ), the proposed model demonstrates an improvement in both accuracy and stability.},
  archive      = {J_NEUCOM},
  author       = {Junwei Sun and Qi’an Sun and Yanfeng Wang and Zicheng Wang},
  doi          = {10.1016/j.neucom.2025.131964},
  journal      = {Neurocomputing},
  month        = {1},
  pages        = {131964},
  shortjournal = {Neurocomputing},
  title        = {Design of memory network model based on DNA strand displacement and its application in prediction},
  volume       = {661},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). FRConv-based lightweight feature reconstruction network for photovoltaic cell defect detection. <em>NEUCOM</em>, <em>661</em>, 131963. (<a href='https://doi.org/10.1016/j.neucom.2025.131963'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Convolutional neural networks (CNNs) have been widely employed for photovoltaic (PV) cell defect detection in electroluminescence (EL) images, yet they still suffer from high hardware dependency and the persistent challenge of balancing detection accuracy with inference speed. To overcome these challenges, this paper introduces FRNet, a lightweight network specifically designed for detecting PV cell defects in EL images. The core contribution is the Feature Reconstruction Convolution (FRConv) module, which effectively addresses two inherent drawbacks of conventional convolutions: rigid geometric structures that limit feature adaptability and inefficient utilization of channel information. The proposed FRConv module enhances feature extraction using a dual mechanism. It combines dynamically deformable convolution with channel importance weighting to adaptively adjust sampling positions. Additionally, gated cross-reconstruction operations are employed to fuse odd- and even-channel features, thereby improving the information flow. The network architecture uses depthwise separable convolutions with feature-bridged connections. This forms an efficient hierarchical structure capable of extracting defect features at multiple scales. To mitigate class imbalance in PV cell defect datasets, we introduce an adaptive focal loss that integrates learnable class weights with dynamic scaling factors, promoting balanced optimization across classes. Extensive experiments demonstrate that FRNet achieves a detection accuracy of 98.58 % on public EL datasets, while maintaining a lightweight design with only 1.30 M parameters and 0.78 G FLOPs, thereby significantly outperforming existing state-of-the-art methods. On an industrial-scale dataset, the model achieves 93.84 % and 89.71 % accuracy for 2-class and 14-class classification tasks, respectively. Moreover, FRNet delivers real-time inference at 29 FPS on resource-constrained hardware, highlighting its strong generalization capability and practical applicability in real-world scenarios.},
  archive      = {J_NEUCOM},
  author       = {Yirong Zhang and Kai Ding and Zhongguan Liu and Felix T.S. Chan},
  doi          = {10.1016/j.neucom.2025.131963},
  journal      = {Neurocomputing},
  month        = {1},
  pages        = {131963},
  shortjournal = {Neurocomputing},
  title        = {FRConv-based lightweight feature reconstruction network for photovoltaic cell defect detection},
  volume       = {661},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Collaborative multi-view fuzzy clustering based on gaussian mixture model. <em>NEUCOM</em>, <em>661</em>, 131961. (<a href='https://doi.org/10.1016/j.neucom.2025.131961'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Current multi-view fuzzy clustering methods are mostly designed based on the strength of prototype-based clustering. In comparison to the prototype-based clustering, Gaussian mixture model (GMM)-based clustering offers greater flexibility in modeling data distributions, making it more applicable to diverse cluster shapes. In this paper, an innovative GMM-based collaborative multi-view fuzzy clustering algorithm is proposed, where a collaborative learning mechanism is designed to facilitate the fusion of multiple views. The valuable knowledge in each view can be learned by other views to guide and improve their own data clustering, and global consistency between views can be guaranteed by adding consensus constraints. Furthermore, to identify inter-view divergence, a self-adaptive learning strategy is established to dynamically adjust the learning rate based on the clustering status of each view. Finally, the maximum entropy regularization is employed to assign optimal weights to each view, emphasizing the importance of the outstanding views. Extensive experiments on real-world multi-view datasets validate the effectiveness of the proposed method in comparison with other traditional multi-view fuzzy clustering algorithms.},
  archive      = {J_NEUCOM},
  author       = {Xingchen Li and Shiyuan Han and Jin Zhou and C.L.Philip Chen and Tong Zhang and Yuehui Chen and Lin Wang and Tao Du and Xiaoyu Zheng},
  doi          = {10.1016/j.neucom.2025.131961},
  journal      = {Neurocomputing},
  month        = {1},
  pages        = {131961},
  shortjournal = {Neurocomputing},
  title        = {Collaborative multi-view fuzzy clustering based on gaussian mixture model},
  volume       = {661},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Auto-weighted projective one-step multi-view clustering. <em>NEUCOM</em>, <em>661</em>, 131960. (<a href='https://doi.org/10.1016/j.neucom.2025.131960'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-view clustering (MVC) aims to enhance clustering performance by leveraging complementary information from diverse feature representations (views) of the same data. Despite progress, existing methods often suffer from inefficient optimization frameworks and inconsistent feature alignment across views, which degrade clustering accuracy and scalability. To address these challenges, we propose Auto-weighted Projective One-step Multi-view Clustering (AWPOMC), a novel framework that unifies feature matrix decomposition and clustering into a joint optimization process. AWPOMC introduces a learnable projection matrix with an innovative regularization mechanism to enforce robust cross-view feature alignment, thereby improving cluster consistency while preserving view-specific characteristics. Meanwhile, AWPOMC introduces an adaptive weight coefficient to balance the contributions of different feature subspaces after projection. AWPOMC achieves efficient one-step learning, significantly reducing computational overhead. Extensive experiments on diverse benchmark datasets demonstrate that AWPOMC outperforms state-of-the-art methods in clustering accuracy, computational efficiency, and convergence stability, especially on the HandWritten dataset, where the accuracy rate is improved by 7 % points compared to the previous best method. Our work provides a principled and scalable solution for MVC, particularly suited to high-dimensional, heterogeneous multi-view data.},
  archive      = {J_NEUCOM},
  author       = {Xin Mou and Weikai Li and Bing Li and Jin Hu},
  doi          = {10.1016/j.neucom.2025.131960},
  journal      = {Neurocomputing},
  month        = {1},
  pages        = {131960},
  shortjournal = {Neurocomputing},
  title        = {Auto-weighted projective one-step multi-view clustering},
  volume       = {661},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Local entropy-adversarial oversampling for imbalanced datasets. <em>NEUCOM</em>, <em>661</em>, 131959. (<a href='https://doi.org/10.1016/j.neucom.2025.131959'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Imbalanced classification problems are prevalent across numerous machine learning tasks. While existing sampling methods can balance class distributions, their effectiveness is limited in scenarios with severe class overlap. Some approaches rely on rigid geometric sampling regions or remove majority class instances in overlapping areas, which often leads to poor boundary adaptability or information loss. To address these challenges, a novel oversampling method, termed Local Entropy-Adversarial Oversampling (LEAO), is introduced. This approach leverages local information entropy to quantify class uncertainty at the instance level, and constructs an entropy-based adversarial model characterized by majority-class repulsion and minority-class attraction. This model adaptively guides the generation of synthetic instances toward regions with clear class boundaries and high discriminability, thereby eliminating the reliance on predefined geometric sampling regions. Furthermore, an entropy-weighted sampling strategy is introduced to enhance the identification and coverage of complex minority sub-concept regions. Extensive experiments on 60 imbalanced datasets demonstrate that LEAO achieves significant improvements in AUC, G-mean, and F-measure across four mainstream classifiers (KNN, NB, SVM and RF), outperforming 12 mainstream sampling methods. Notably, LEAO exhibits superior robustness and generalization performance, especially in datasets with highly overlapping and complex class structures.},
  archive      = {J_NEUCOM},
  author       = {Liangliang Tao},
  doi          = {10.1016/j.neucom.2025.131959},
  journal      = {Neurocomputing},
  month        = {1},
  pages        = {131959},
  shortjournal = {Neurocomputing},
  title        = {Local entropy-adversarial oversampling for imbalanced datasets},
  volume       = {661},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). MFHNN-DDI: Molecular fragment-based hypergraph neural networks for drug–drug interaction prediction. <em>NEUCOM</em>, <em>661</em>, 131958. (<a href='https://doi.org/10.1016/j.neucom.2025.131958'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Accurate prediction of drug–drug interactions (DDIs) is essential for advancing combination therapy and drug discovery. However, existing methods face two key limitations: inadequate modeling of molecular substructure relationships and insufficient integration of multi-source drug information. To address these shortcomings, we propose a novel model named M olecular F ragment-Based H ypergraph N eural N etworks for D rug- D rug I nteraction Prediction (MFHNN-DDI), which leverages hypergraph neural networks to model high-order molecular interactions and employs multi-encoder fusion to integrate heterogeneous drug information. First, MFHNN-DDI constructs molecular fragment hypergraphs to model high-order interactions between drugs and their substructures, which enable a richer representation of molecular relationships than traditional pairwise methods. Second, we design a multi-encoder architecture that synergistically combines: (1) hypergraph encoder for fragment-level associations, (2) graph convolution for network-level DDI patterns, and (3) 1D convolution with residual connections for sequence-level features. Third, our fusion mechanism integrates these complementary representations to capture both local molecular details and global interaction patterns. Experimental results on two real-world datasets demonstrate that MFHNN-DDI outperforms twelve state-of-the-art baselines. Notably, MFHNN-DDI exhibits remarkable performance on the DDInter dataset, with ACC and F1-score both exceeding 0.9615 , showing considerable gains of 2.00 % and 2.08 % respectively over the second-best methods.},
  archive      = {J_NEUCOM},
  author       = {Liwei Zheng and Bin Dai and Fan Yang and Jie Xia and Kaibiao Lin and Zhongqi Cai},
  doi          = {10.1016/j.neucom.2025.131958},
  journal      = {Neurocomputing},
  month        = {1},
  pages        = {131958},
  shortjournal = {Neurocomputing},
  title        = {MFHNN-DDI: Molecular fragment-based hypergraph neural networks for drug–drug interaction prediction},
  volume       = {661},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). QRT-DETR: Post-training quantization for real-time detection transformer. <em>NEUCOM</em>, <em>661</em>, 131957. (<a href='https://doi.org/10.1016/j.neucom.2025.131957'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As a state-of-the-art real-time object detection model, Real-Time Detection Transformer (RT-DETR) excels in complex vision tasks but faces deployment challenges on resource-constrained platforms due to its high computational and memory demands. To address this deployment bottleneck, we propose QRT-DETR, a systematic post-training quantization (PTQ) framework tailored for RT-DETR. Our solution constructs a holistic pipeline that integrates dynamic quantization, progressive reconstruction, and architectural optimization to tackle the severe performance collapse in low-bit quantization. Within this framework, we introduce an EMA-MSE quantizer that combines EMA for dynamic distribution tracking with MSE as a quality metric, enabling adaptive and precise calibration of quantization ranges. Furthermore, a two-stage reconstruction-aware quantization strategy is developed to iteratively optimize quantization parameters. Experimental evaluations demonstrate that QRT-DETR enables viable real-time detection under low-bit quantization, achieving a breakthrough 22.6 % AP at 4-bit where prior methods fail completely (0 % AP ). The related code is available at https://github.com/ashleyhuo/QRT-DETR .},
  archive      = {J_NEUCOM},
  author       = {Ying Huo and Tianle Wu and Yuxuan Shen and Xiaomeng Li and Zhuo Tao and Dawei Yang},
  doi          = {10.1016/j.neucom.2025.131957},
  journal      = {Neurocomputing},
  month        = {1},
  pages        = {131957},
  shortjournal = {Neurocomputing},
  title        = {QRT-DETR: Post-training quantization for real-time detection transformer},
  volume       = {661},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Heterophily-aware fair recommendation using graph convolutional networks. <em>NEUCOM</em>, <em>661</em>, 131956. (<a href='https://doi.org/10.1016/j.neucom.2025.131956'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, graph neural networks (GNNs) have become a popular tool to improve the accuracy and performance of recommender systems. Modern recommender systems are not only designed to serve end users, but also to benefit other participants, such as items and item providers. These participants may have different or conflicting goals and interests, which raises the need for fairness and popularity bias considerations. GNN-based recommendation methods also face the challenges of unfairness and popularity bias, and their normalization and aggregation processes suffer from these challenges. In this paper, we propose a fair GNN-based recommender system, called HetroFair, to improve item-side fairness. HetroFair uses two separate components to generate fairness-aware embeddings: i) Fairness-aware attention, which incorporates the dot product in the normalization process of GNNs to decrease the effect of nodes’ degrees. ii) Heterophily feature weighting, to assign distinct weights to different features during the aggregation process. To evaluate the effectiveness of HetroFair, we conduct extensive experiments over six real-world datasets. Our experimental results reveal that HetroFair not only alleviates unfairness and popularity bias on the item side but also achieves superior accuracy on the user side. Our implementation is publicly available at https://github.com/NematGH/HetroFair .},
  archive      = {J_NEUCOM},
  author       = {Nemat Gholinejad and Mostafa Haghir Chehreghani},
  doi          = {10.1016/j.neucom.2025.131956},
  journal      = {Neurocomputing},
  month        = {1},
  pages        = {131956},
  shortjournal = {Neurocomputing},
  title        = {Heterophily-aware fair recommendation using graph convolutional networks},
  volume       = {661},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Local augmentation and debiasing for graph contrastive learning. <em>NEUCOM</em>, <em>661</em>, 131954. (<a href='https://doi.org/10.1016/j.neucom.2025.131954'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Contrastive Learning (CL) has demonstrated remarkable results in graph self-supervised tasks, with augmentation strategies and negative samples serving as two pivotal factors. The augmentation process aims to generate new samples that are semantically similar yet subtly distinct. Given that the semantic information embedded in graph data may not be as intuitive as it is in computer vision, devising suitable graph augmentation strategies has garnered significant attention from researchers. Furthermore, existing node-level CL methods commonly regard all nodes, excluding the target node itself, as negative samples. This approach may lead to sampling bias, where inaccurate negative samples (false negatives) are utilized for contrastive learning, ultimately compromising the quality of the learned representations. Drawing inspiration from the design of Graph Neural Networks (GNNs), which aggregate local information, we propose a novel Local Augmentation and Debiased (LAD) framework for graph contrastive learning. Specifically, we craft local samples for each node and employ statistical methods to estimate the distribution of these local samples. Subsequently, we sample from each distribution to generate the new views. Additionally, to mitigate sampling bias, we assign weighted penalties to nodes used in generating local samples, considering them as false negatives for their respective nodes. Theoretically, debiasing objective function introduces a stricter lower bound of the mutual information between node features and node embeddings in augmented views. The extensive node classification experiments conducted on real-world datasets demonstrate the efficacy of the proposed method.},
  archive      = {J_NEUCOM},
  author       = {Shihao Gao and Lingling Li and Zhongwei Xiong and Taisong Jin},
  doi          = {10.1016/j.neucom.2025.131954},
  journal      = {Neurocomputing},
  month        = {1},
  pages        = {131954},
  shortjournal = {Neurocomputing},
  title        = {Local augmentation and debiasing for graph contrastive learning},
  volume       = {661},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Multiscale spatial-spectral attention network for arbitrary-scale hyperspectral image super-resolution. <em>NEUCOM</em>, <em>661</em>, 131950. (<a href='https://doi.org/10.1016/j.neucom.2025.131950'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Super-resolution is vital for enhancing hyperspectral images (HSIs), as it directly mitigates the inherent trade-off between their spatial and spectral resolution. Existing methods often train a specific model for each scale factor and struggle in real-world scenarios requiring arbitrary-scale super-resolution (ASSR). To overcome these limitations, we propose a Multi-scale Spatial-Spectral Attention Network named MSANet for HSI arbitrary-scale super-resolution. While HSIs contain rich spectral information, 2D/3D convolutions fail to capture global dependencies, which often results in spectral distortion, and Transformers tend to incur high computational costs. MSANet addresses this via a dual-branch architecture for joint feature extraction and fusion. The spectral branch incorporates a Multi-scale Spectral Attention (MSSA) module that uses triple-dilated convolutions to expand the receptive field and model cross-scale spectral relationships. The spatial-spectral branch adopts Non-Local Spatial-Spectral Attention (NLS 2 A), which adaptively integrates non-local dependencies between spatial and spectral features while preserving inter-band continuity during spatial detail enhancement. To overcome the discontinuous reconstructions typical of coordinate-based latent codes upsampling, we introduce a Global-Local Query Reconstruction (GLQR) that maps non-local spectral features into a continuous spatial feature space, enhancing latent code representations and reducing signal discontinuity. Instead of relying on spatial coordinates, HR tokens derived from an implicit spectral token are matched with the refined latent features through a query mechanism, achieving ASSR with reinforced spatial-spectral consistency. Extensive experiments demonstrate the superior performance of our MSANet on benchmark datasets compared with state-of-the-art models, especially in preserving spatial details and spectral continuity.},
  archive      = {J_NEUCOM},
  author       = {Shiyan Wang and Jiawei Zhao and Jiajia Tang},
  doi          = {10.1016/j.neucom.2025.131950},
  journal      = {Neurocomputing},
  month        = {1},
  pages        = {131950},
  shortjournal = {Neurocomputing},
  title        = {Multiscale spatial-spectral attention network for arbitrary-scale hyperspectral image super-resolution},
  volume       = {661},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). SoFoNO: Arbitrary-scale image super-resolution via sobolev fourier neural operator. <em>NEUCOM</em>, <em>661</em>, 131944. (<a href='https://doi.org/10.1016/j.neucom.2025.131944'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Accurately reconstructing fine textures and sharp edges remains a significant challenge in Single Image Super-Resolution (SISR) tasks, often resulting in overly smooth and less realistic images. To alleviate this issue we propose a novel SISR framework named Sobolev Fourier Neural Operator (SoFoNO). Central to our approach is a specialized architecture featuring a Sobolev Branch, which effectively captures detailed structures in the frequency domain via a learnable Sobolev exponent. Importantly, the learned Sobolev exponent is directly employed as derivative order parameters within the Sobolev loss function, enabling more precise and visually coherent reconstructions. Unlike conventional pixel-level loss functions, the Sobolev loss explicitly incorporates frequency-domain penalties, significantly enhancing the reconstruction quality of detailed image structures. Extensive experiments conducted on multiple datasets under both in-scale and out-scale scenarios demonstrate that our SoFoNO provides robust and effective performance in arbitrary-scale super-resolution, consistently outperforming representative existing methods across various tested scale factors without relying on attention mechanisms.},
  archive      = {J_NEUCOM},
  author       = {Jong Kwon Oh and Hwijae Son and Hyung Ju Hwang and Jihyong Oh},
  doi          = {10.1016/j.neucom.2025.131944},
  journal      = {Neurocomputing},
  month        = {1},
  pages        = {131944},
  shortjournal = {Neurocomputing},
  title        = {SoFoNO: Arbitrary-scale image super-resolution via sobolev fourier neural operator},
  volume       = {661},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Ethics of trustworthy AI in healthcare: Challenges, principles, and practical pathways. <em>NEUCOM</em>, <em>661</em>, 131942. (<a href='https://doi.org/10.1016/j.neucom.2025.131942'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Artificial Intelligence (AI) is transforming healthcare by enhancing diagnostics, personalizing treatment planning, and streamlining patient care. Yet, its adoption is hindered by persistent ethical challenges, including algorithmic bias, lack of transparency, privacy risks, and unclear accountability. Existing international frameworks articulate high-level principles but seldom provide operational guidance for clinical deployment. We bridge this gap by synthesizing trust dimensions for healthcare, with measurable metrics for fairness, explainability, privacy, accountability, and robustness, and proposing the Healthcare AI Trustworthiness Index (HAITI), a composite, context-aware readiness score with explicit normalization, weighting, and uncertainty reporting. We outline a development–deployment–governance blueprint and present two case studies (diagnostic bias mitigation; privacy-preserving federated learning). Together, these contributions translate ethical principles into measurable practices that can foster trust, improve equity, and accelerate responsible AI integration in clinical settings.},
  archive      = {J_NEUCOM},
  author       = {Pegah Ahadian and Wei Xu and Dongfang Liu and Qiang Guan},
  doi          = {10.1016/j.neucom.2025.131942},
  journal      = {Neurocomputing},
  month        = {1},
  pages        = {131942},
  shortjournal = {Neurocomputing},
  title        = {Ethics of trustworthy AI in healthcare: Challenges, principles, and practical pathways},
  volume       = {661},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Data-driven optimal control for aero-engines with fuel saturation constraints in transient state. <em>NEUCOM</em>, <em>661</em>, 131941. (<a href='https://doi.org/10.1016/j.neucom.2025.131941'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Transient control of aircraft engines is a topic of considerable academic interest and presents significant challenges within the field of control engineering. This paper proposes a data-driven optimal control algorithm for regulating aircraft engines during transient states while explicitly addressing input saturation constraints. By using a hyperbolic tangent transformation, the problem is reformulated as an unconstrained optimization problem. In order to solve the Hamilton-Jacobi-Bellman (HJB) equation to obtain the optimal control law, the policy iteration scheme is employed and two neural networks—an evaluation and a policy generation neural network—are used to approximate the unknown dynamic components. Then a data-driven framework is adopted to collect data without requiring explicit knowledge of the system dynamics, and the least squares method is applied to update weights of the two neural networks. Simulation results verify that the proposed method achieves enhanced transient performance for aircraft engines while ensuring system stability.},
  archive      = {J_NEUCOM},
  author       = {Xiumei Han and Shuoshuo Liu and Zhen Liu and Ying Zhao and Xudong Zhao},
  doi          = {10.1016/j.neucom.2025.131941},
  journal      = {Neurocomputing},
  month        = {1},
  pages        = {131941},
  shortjournal = {Neurocomputing},
  title        = {Data-driven optimal control for aero-engines with fuel saturation constraints in transient state},
  volume       = {661},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Skew-normal distributions for modeling asymmetric moving tendencies in pedestrian trajectories. <em>NEUCOM</em>, <em>661</em>, 131934. (<a href='https://doi.org/10.1016/j.neucom.2025.131934'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Predicting multiple possible trajectories for pedestrians is crucial for autonomous driving and social navigation systems. Although normal distributions are often used to model trajectory uncertainty due to their explainability and tractability, they struggle with the asymmetric movement patterns commonly seen in pedestrian trajectories. Our empirical evidence and theoretical analysis confirm that normal distributions lack the expressiveness needed for complex trajectories. To address this, we introduce a plug-and-play skew-normal (SN) module that uses asymmetric SN distributions or their mixtures. Theoretically, this module offers a clear geometric interpretation of pedestrian movement, greater expressiveness for multi-step predictions, and reduced prediction uncertainty, all while maintaining computational complexity similar to normal distribution-based approaches. Experiments on real-world pedestrian trajectory prediction datasets demonstrate that the SN module effectively decreases prediction error and produces results with lower uncertainty and higher diversity. The code is available at https://github.com/hilbert9221/SkewNormalTrajectory .},
  archive      = {J_NEUCOM},
  author       = {Siyuan Chen and Yatie Xiao and Yangtao Wang and Yanzhao Xie and Tong Zhu and Rui Duan and Jinbiao Chen},
  doi          = {10.1016/j.neucom.2025.131934},
  journal      = {Neurocomputing},
  month        = {1},
  pages        = {131934},
  shortjournal = {Neurocomputing},
  title        = {Skew-normal distributions for modeling asymmetric moving tendencies in pedestrian trajectories},
  volume       = {661},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). DPSHE: A privacy-preserving federated learning scheme based on homomorphic encryption and differential privacy for medical image. <em>NEUCOM</em>, <em>661</em>, 131931. (<a href='https://doi.org/10.1016/j.neucom.2025.131931'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The rapid advancement of data-driven AI technologies has facilitated the integration of big data applications across various industries, particularly in healthcare, where data has experienced unprecedented growth. While healthcare data possesses substantial value, it also contains sensitive information, leading the industry to exercise caution regarding data sharing due to the potentially serious repercussions of privacy breaches, which hinder the collaborative training of AI. To address this challenge, federated learning has emerged as a promising solution that enables distributed data sharing while ensuring privacy. However, federated learning is susceptible to various threats such as poisoning attacks, inference attacks, and collusion attacks from different entities, all of which may disrupt training process or compromise private information. In response to these threats, this paper proposes a novel defense mechanism called DPSHE, which enhances the robustness of federated learning by employing differential privacy and threshold homomorphic encryption. DPSHE utilizes a hybrid policy to detect and distinguish malicious clients attempting poisoning attacks. At the same time, threshold homomorphic encryption is used to encrypt model parameters and achieve directional aggregation. Furthermore, a threshold for the number of collaborative decryption clients is used to ensure the security of the model under collusive attacks. Experimental results demonstrate that DPSHE effectively mitigates the impact of poisoning attacks launched by malicious clients, safeguards model privacy, and provides robust protection against collusion threats among participants. Simultaneously, it ensures the convergence speed of the training process while enhancing model accuracy, achieving improvements of up to 82.62 %.},
  archive      = {J_NEUCOM},
  author       = {Xueyan Liu and Hao Sun and Nan Sun and Bolong Jia and Dehao Xiao},
  doi          = {10.1016/j.neucom.2025.131931},
  journal      = {Neurocomputing},
  month        = {1},
  pages        = {131931},
  shortjournal = {Neurocomputing},
  title        = {DPSHE: A privacy-preserving federated learning scheme based on homomorphic encryption and differential privacy for medical image},
  volume       = {661},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Pro-CLIP: Residual learning and object-agnostic prompts for few-shot anomaly detection. <em>NEUCOM</em>, <em>661</em>, 131929. (<a href='https://doi.org/10.1016/j.neucom.2025.131929'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Few-shot anomaly detection is crucial in domains where training data is scarce or unavailable due to data privacy concerns and high annotation costs. Despite recent advances in large pre-trained models, particularly vision–language models like CLIP, existing methods still face significant challenges. First, they heavily rely on handcrafted text prompts specifically designed for certain defect types, limiting their adaptability across different domains and anomaly categories. Second, they struggle to generalize across domains with substantial variations in foreground objects, abnormal regions, and background features, leading to suboptimal performance in complex real-world scenarios. To address these challenges, we propose Pro-CLIP, a novel framework for few-shot anomaly detection that eliminates the need for manual prompt engineering through object-agnostic text prompt learning. This enables the model to capture general normality and abnormality concepts, ensuring adaptability across diverse applications such as industrial defect inspection and medical imaging without domain-specific customization. Additionally, we introduce a residual learning strategy, which utilizes few-shot normal samples to refine anomaly detection based on image residuals, enhancing robustness against diverse anomaly types and complex backgrounds. Extensive experiments on multiple real-world datasets demonstrate that Pro-CLIP outperforms existing methods, providing a scalable and effective solution for generalized few-shot anomaly detection across different domains and modalities.},
  archive      = {J_NEUCOM},
  author       = {Yuqing Zhao and Min Meng and Jigang Wu},
  doi          = {10.1016/j.neucom.2025.131929},
  journal      = {Neurocomputing},
  month        = {1},
  pages        = {131929},
  shortjournal = {Neurocomputing},
  title        = {Pro-CLIP: Residual learning and object-agnostic prompts for few-shot anomaly detection},
  volume       = {661},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Improving monte carlo dropout uncertainty estimation with stable output layers. <em>NEUCOM</em>, <em>661</em>, 131927. (<a href='https://doi.org/10.1016/j.neucom.2025.131927'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Uncertainty estimation in neural networks is important for reliable predictions. Various statistical methodologies, each with their own characteristics, are utilized to estimate uncertainty. Bootstrap provides robust uncertainty estimation, but its high computational cost is required to be repeated. Monte Carlo dropout (MC dropout) approximates Bayesian inference without additional training, but it can induce excessive uncertainty by applying dropout at every layer. This study proposes MC dropout simulation with a Stable Output Layer (SOL) to address these issues. Our method, SOL MC dropout, requires the same amount of time as a standard MC dropout but produces improved uncertainty estimation. It provides bootstrap-like robust prediction distribution with a much lower computational cost. Experiments on benchmark datasets show that SOL MC dropout provides enhanced uncertainty estimation while maintaining the prediction performance of standard MC dropout. These results suggest that SOL MC dropout can be an efficient and practical approach for uncertainty estimation.},
  archive      = {J_NEUCOM},
  author       = {Suhan Son and Junhee Seok},
  doi          = {10.1016/j.neucom.2025.131927},
  journal      = {Neurocomputing},
  month        = {1},
  pages        = {131927},
  shortjournal = {Neurocomputing},
  title        = {Improving monte carlo dropout uncertainty estimation with stable output layers},
  volume       = {661},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Magic3DSketch: Create colorful 3D models from sketch-based 3D modeling guided by text and language-image pre-training. <em>NEUCOM</em>, <em>661</em>, 131925. (<a href='https://doi.org/10.1016/j.neucom.2025.131925'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The requirement for 3D content is growing as AR/VR applications emerge. However, 3D modeling is challenging for novice users due to its labor-intensive and skill-intensive nature. Our proposed Magic3DSketch adopts a novel technique to encode sketches to predict 3D meshes, guided by text descriptions, and leverages external prior knowledge obtained through text and language image pre-training. The integration of language image pre-trained neural networks complements the sparsity and ambiguity of single-view sketch input. Magic3DSketch achieves state-of-the-art performance on both synthetic and real datasets. It generates more detailed structures guided by text input and realistic shapes with the help of text input. According to our user studies, users are also more satisfied with the models obtained through Magic3DSketch. Compared with existing text-to-3D methods, our method provides higher controllability. Additionally, we are also the first, to our knowledge, to add color based on text descriptions to the sketch-derived shapes. By combining sketches and text guidance with the help of language-image pretrained models, our Magic3DSketch allows novice users to create custom 3D models with minimal effort and maximum creative freedom, with the potential to revolutionize future 3D modeling pipelines.},
  archive      = {J_NEUCOM},
  author       = {Ying Zang and Yidong Han and Chaotao Ding and Jianqi Zhang and Tianrun Chen},
  doi          = {10.1016/j.neucom.2025.131925},
  journal      = {Neurocomputing},
  month        = {1},
  pages        = {131925},
  shortjournal = {Neurocomputing},
  title        = {Magic3DSketch: Create colorful 3D models from sketch-based 3D modeling guided by text and language-image pre-training},
  volume       = {661},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Li-MFNet: A lightweight multi-scale input mamba fusion network for medical image segmentation. <em>NEUCOM</em>, <em>661</em>, 131916. (<a href='https://doi.org/10.1016/j.neucom.2025.131916'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Medical image segmentation plays a crucial role in clinical diagnosis and treatment planning, but existing methods often face challenges in balancing computational efficiency and segmentation accuracy. In this paper, we propose novel lightweight medical image segmentation network designed to address this issue, named lightweight multi-scale input Mamba Fusion Network (Li-MFNet). Specifically, two novel components are introduced: (1) dual attention parallel Mamba (DA-PMamba) utilizing parallel Mamba architecture and two novel attention blocks that highlight the key information and capture global features with few parameters; (2) dual enhancement residual fusion (DERF) which is designed to effectively integrate low-level features, high-level features and predict mask features without redundant information and computational consumption. Extensive experiments on the ISIC2017, CVC-ColonDB, and LIDC-IDRI datasets demonstrate that the proposed method achieves superior performance compared with ten state-of-the-art methods across six evaluation metrics.},
  archive      = {J_NEUCOM},
  author       = {Youyang Tao and Hongmin Deng and Yanling Liu and Yibing Yao},
  doi          = {10.1016/j.neucom.2025.131916},
  journal      = {Neurocomputing},
  month        = {1},
  pages        = {131916},
  shortjournal = {Neurocomputing},
  title        = {Li-MFNet: A lightweight multi-scale input mamba fusion network for medical image segmentation},
  volume       = {661},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). WKAN-UNet: Wavelet and kolmogorov-arnold network augmented u-net for ISAR image denoising. <em>NEUCOM</em>, <em>661</em>, 131912. (<a href='https://doi.org/10.1016/j.neucom.2025.131912'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In low signal-to-noise ratio (SNR) environments, inverse synthetic aperture radar (ISAR) images often exhibit degraded resolution, blurred edges, and loss of fine details, which significantly limit their practical utility. To address these issues, this paper proposes WKAN-UNet, a novel hybrid deep learning framework that uses wavelet transforms to decouple noise and signal components in the frequency domain, achieving multi-scale spectral separation and robust denoising. In the spatial domain, B-spline-based Kolmogorov-Arnold Network (KAN) modules adaptively model nonlinear features, effectively capturing both local and global scattering patterns while preserving critical edge information. By integrating a U-Net-based multi-scale encoder–decoder, the framework progressively fuses hierarchical features from textures to structures, enhancing representational capability for complex targets. The model supports end-to-end training and deployment without requiring hand-crafted priors or post-processing steps. Extensive experiments on self-constructed datasets AID and SID show that the proposed method outperforms state-of-the-art approaches such as CSwin-UNet and Mamba-UNet in terms of normalized root mean square error (NRMSE), peak signal-to-noise ratio (PSNR), structural similarity index (SSIM), edge preservation ratio (EPR) and image contrast (IC), particularly under low SNR, where it preserves target contours and fine structures despite severe noise. Ablation studies validate the effectiveness of individual components and their synergistic contributions, demonstrating strong noise robustness and cross-dataset generalization. This work presents a reliable and efficient solution for high-fidelity ISAR imaging in complex noisy environments. The core code and dataset are available at https://github.com/XDXM147/WKAN-UNet .},
  archive      = {J_NEUCOM},
  author       = {Xuemei Ren and Xiaoyong Li and Chunye Liu and Lei Liu and Xueru Bai and Feng Zhou},
  doi          = {10.1016/j.neucom.2025.131912},
  journal      = {Neurocomputing},
  month        = {1},
  pages        = {131912},
  shortjournal = {Neurocomputing},
  title        = {WKAN-UNet: Wavelet and kolmogorov-arnold network augmented u-net for ISAR image denoising},
  volume       = {661},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). SRNet: Self-supervised structure regularization for stereo matching. <em>NEUCOM</em>, <em>661</em>, 131907. (<a href='https://doi.org/10.1016/j.neucom.2025.131907'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Depth estimation from stereo or multi-view images is of substantial interest due to a wide range of applications. Recently, deep learning based approaches have been shown to be promising for stereo matching. However, existing stereo matching approaches are mostly data-driven, which often converge to local minima biased toward the training data. In this paper, we propose a simple but effective regularization framework to improve the training of the stereo matching networks. More specifically, we propose using low-level structure detection such as edge detection and keypoint detection as constraints for the regularization of the stereo matching network via multi-task learning. By introducing the low-level structure detection as an auxiliary task, we are able to improve the model training of stereo matching. In addition, a disparity aggregation module is also proposed to consider the association between the stereo matching and low-level structures. We apply the proposed structure regularization on four different CNN-based stereo matching algorithms. The experimental results on four public datasets, including Scene Flow, KITTI 2012, KITTI 2015 and Middlebury, verify our assumptions and show the effectiveness and generality of the proposed framework.},
  archive      = {J_NEUCOM},
  author       = {Jun Cheng and Zaiwang Gu and Weide Liu and Jiayuan Fan and Zhengguo Li and Chuan-Sheng Foo},
  doi          = {10.1016/j.neucom.2025.131907},
  journal      = {Neurocomputing},
  month        = {1},
  pages        = {131907},
  shortjournal = {Neurocomputing},
  title        = {SRNet: Self-supervised structure regularization for stereo matching},
  volume       = {661},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Low-rank quaternion tensor completion by truncated nuclear norm regularization for color video recovery. <em>NEUCOM</em>, <em>661</em>, 131905. (<a href='https://doi.org/10.1016/j.neucom.2025.131905'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Low-rank quaternion tensor completion has attracted significant attention for recovering incomplete visual data with missing elements. While the singular value decomposition (SVD) of quaternion tensors effectively characterizes the tensor low-rankness, existing approaches relying on transform-based tensor products substantially increase the computational complexity in the completion procedure. To address this limitation, we directly employ the t-product to examine the tensor completion. The proposed model establishes a truncated nuclear norm through quaternion tensor SVD to effectively capture the low-rank structural features. The fast quaternion tensor singular value decomposition (FQT-SVD) based on t-product and the alternating direction method of multipliers (ADMM) are used to solve the model. Numerical experiments on color video recovery demonstrate our method’s superior performance and efficiency compared to state-of-the-art approaches.},
  archive      = {J_NEUCOM},
  author       = {Han Jiao and Qilong Liu and Kun Liang and Liyuan An},
  doi          = {10.1016/j.neucom.2025.131905},
  journal      = {Neurocomputing},
  month        = {1},
  pages        = {131905},
  shortjournal = {Neurocomputing},
  title        = {Low-rank quaternion tensor completion by truncated nuclear norm regularization for color video recovery},
  volume       = {661},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). DeinfoAttack: A heuristic graph adversarial attack algorithm leveraging graph topological information entropy. <em>NEUCOM</em>, <em>661</em>, 131904. (<a href='https://doi.org/10.1016/j.neucom.2025.131904'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Graph Neural Networks (GNNs) have been widely studied for their ability to capture node interdependencies and integrate node attributes, improving node classification accuracy. Recent research has shown that GNNs designed for heterophilic graphs can learn more information than traditional GNNs, bringing new challenges for developing attack and defense strategies across varying heterophily levels. However, existing adversarial attack algorithms have predominantly concentrated on homophilic graphs, often overlooking the heterophilic scenario where their effectiveness is compromised. We tackle this challenge and propose a novel universal heuristic attack algorithm - DeinfoAttack, which stands for De creasing info rmativeness while Attack ing . For a deeper understanding, we theoretically analyze the more critical factor beyond homophily and heterophily: Graph Topological Information Entropy derived from Neighborhood Similarity Matrix S ( A ) , to quantify the informativeness provided by the graph topology during GNN predictions. DeinfoAttack aims to maximize Graph Topological Information Entropy within a limited budget by preferentially targeting nodes that provide more graph topological information. Extensive experiments demonstrate that DeinfoAttack is efficient and effective, with the potential to reduce the GNNs’ accuracy by up to 30 %. For example, the accuracy decreased from 74.2 % to 44.2 % after attacking Cora, and the accuracy decreased from 52.63 % to 22.59 % after attacking Chameleon. Furthermore, the overall performance is superior to existing attack algorithms in terms of attack effect, attack cost, and generality.},
  archive      = {J_NEUCOM},
  author       = {Yajing Wang and Huawei Cao and Shuhan Song},
  doi          = {10.1016/j.neucom.2025.131904},
  journal      = {Neurocomputing},
  month        = {1},
  pages        = {131904},
  shortjournal = {Neurocomputing},
  title        = {DeinfoAttack: A heuristic graph adversarial attack algorithm leveraging graph topological information entropy},
  volume       = {661},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Efficient early exit single object tracking via general distribution. <em>NEUCOM</em>, <em>661</em>, 131888. (<a href='https://doi.org/10.1016/j.neucom.2025.131888'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Current mainstream object tracking algorithms require search region to undergo feature extraction and relation modeling through the entire backbone network. For simple scenarios with high object-background distinguishability and minimal object appearance changes, this fixed inference path design paradigm, which does not consider tracking difficulty levels, leads to redundant computations. This not only reduces inference speed but also wastes significant computational resources. To address this limitation, this work proposes an early exit object tracking algorithm based on General Distribution (GD). Leveraging the property that GD can reflect current tracking localization quality, we use the information entropy of GD as the early exit criterion, enabling differentiated processing of simple and challenging tracking scenarios. We introduce Bypass Branch Modules to enhance the prediction capability of shallow prediction heads and design a two-stage training method that maintains the performance of the final prediction head while further improving the prediction capability of shallow heads through self-distillation. Experimental results on multiple benchmark datasets demonstrate that our proposed algorithm not only achieves accuracy metrics comparable to current state-of-the-art methods but also significantly improves inference speed. Through quantitative analysis on the LaSOT dataset test set, predictions obtained from early exits at shallow heads exhibit good prediction accuracy, which not only demonstrates that our method can effectively distinguish between detection and challenging scenes, but also validates the rationality of applying early inference termination for simple scenes.},
  archive      = {J_NEUCOM},
  author       = {Yachun Feng and Ding Yuan and Jianbo Song and Hanyang Liu and Yifan Yang and Tianxiao Zhang},
  doi          = {10.1016/j.neucom.2025.131888},
  journal      = {Neurocomputing},
  month        = {1},
  pages        = {131888},
  shortjournal = {Neurocomputing},
  title        = {Efficient early exit single object tracking via general distribution},
  volume       = {661},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Automatic data-free pruning via channel similarity reconstruction. <em>NEUCOM</em>, <em>661</em>, 131885. (<a href='https://doi.org/10.1016/j.neucom.2025.131885'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Structured pruning methods are developed to bridge the gap between the massive scale of neural networks and the limited hardware resources. Most current structured pruning methods rely on training datasets to fine-tune the compressed model, resulting in high computational burdens and being inapplicable for scenarios with stringent requirements on privacy and security. As an alternative, some data-free methods have been proposed, however, these methods often require handcrafted parameter tuning and can only achieve inflexible reconstruction. In this paper, we propose the Automatic Data-Free Pruning (AutoDFP) method that achieves automatic pruning and reconstruction without fine-tuning. Our approach is based on the assumption that the loss of information can be partially compensated by retaining focused information from similar channels. Specifically, we formulate data-free pruning as an optimization problem, which can be effectively addressed through reinforcement learning. AutoDFP assesses the similarity of channels for each layer and provides this information to the reinforcement learning agent, guiding the pruning and reconstruction process of the network. We evaluate AutoDFP with multiple networks on multiple datasets, achieving impressive compression results.},
  archive      = {J_NEUCOM},
  author       = {Siqi Li and Jun Chen and Jingyang Xiang and Chengrui Zhu and Jiandang Yang and Xiaobin Wei and Yunliang Jiang and Yong Liu},
  doi          = {10.1016/j.neucom.2025.131885},
  journal      = {Neurocomputing},
  month        = {1},
  pages        = {131885},
  shortjournal = {Neurocomputing},
  title        = {Automatic data-free pruning via channel similarity reconstruction},
  volume       = {661},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). AI-based collimation optimization for X-ray imaging using depth cameras. <em>NEUCOM</em>, <em>661</em>, 131881. (<a href='https://doi.org/10.1016/j.neucom.2025.131881'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Collimation during radiography, which is the process of defining the area to be radiated, is a crucial factor for the protection of the patient and for the diagnostic quality of a radiograph. Moreover, incorrect collimation is one of the main causes of a retake and the associated costs. In this paper we propose a novel collimation optimization approach using depth cameras and deep neural networks trained end-to-end. We have acquired two new datasets for this purpose. The first, obtained in a clinical environment, consists of depth images of the lower leg and abdomen and the second, captured in real clinical practice, consists of depth images and corresponding radiographs of thorax examinations. For all depth images, the ideal collimation was labeled by experts either on the depth image or directly on the radiograph. Using this dataset to learn to predict the optimal collimation, we show that it is possible to learn different shapes of collimations and to achieve results that are on par with those obtained by radiographers. Such an AI assistant trained with optimal collimation could reduce the radiation exposure to which the patient is exposed, improve the workflow in radiography, and finally increase the diagnostic quality of radiographs.},
  archive      = {J_NEUCOM},
  author       = {Dominik Mairhöfer and Manuel Laufer and Lennart Berkel and Malte Sieren and Arpad Bischof and Erhardt Barth and Jörg Barkhausen and Thomas Martinetz},
  doi          = {10.1016/j.neucom.2025.131881},
  journal      = {Neurocomputing},
  month        = {1},
  pages        = {131881},
  shortjournal = {Neurocomputing},
  title        = {AI-based collimation optimization for X-ray imaging using depth cameras},
  volume       = {661},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Bridging physical constraints and deep generative models via physics-aware normalizing flows. <em>NEUCOM</em>, <em>661</em>, 131880. (<a href='https://doi.org/10.1016/j.neucom.2025.131880'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Generative artificial intelligence suffers from critical limitations including hallucination and violation of real-world constraints. To address this, we present a physics-aware generative modeling framework combining adversarial learning with physical forward models and Normalizing Flows. Systematic evaluation across simple to complex electric circuit models and four Normalizing Flow architectures demonstrates broad applicability. Training stabilization is achieved through discriminator ensembles, instance noise scheduling, and penalty terms. Downstream supervised learning tasks validate the framework’s ability to synthesize labeled training data from unlabeled impedance measurements, achieving competitive performance across several circuit elements compared to original datasets.},
  archive      = {J_NEUCOM},
  author       = {Benjamin Schindler and Melle Mendikowski and Thomas Schmid},
  doi          = {10.1016/j.neucom.2025.131880},
  journal      = {Neurocomputing},
  month        = {1},
  pages        = {131880},
  shortjournal = {Neurocomputing},
  title        = {Bridging physical constraints and deep generative models via physics-aware normalizing flows},
  volume       = {661},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Graph online change point detection based on fréchet statistics. <em>NEUCOM</em>, <em>661</em>, 131874. (<a href='https://doi.org/10.1016/j.neucom.2025.131874'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper introduces a method for online change point detection in dynamic graphs using Fréchet statistics of graph Laplacians to accurately quantify structural changes. We develop a CUSUM-style test statistic with derived distributions for both null and alternative hypotheses. Extensive numerical experiments and two real-world applications, including one in container liner shipping networks during 2021 and 2022, demonstrate that our algorithm achieves superior detection performance and reduces false alarms compared to baseline methods. Our method offers an effective tool for real-time monitoring and timely detection of changes in dynamic networks.},
  archive      = {J_NEUCOM},
  author       = {Rui Luo and Jie Bao and Hing Cheung So and Suqun Cao and Mengqiao Xu},
  doi          = {10.1016/j.neucom.2025.131874},
  journal      = {Neurocomputing},
  month        = {1},
  pages        = {131874},
  shortjournal = {Neurocomputing},
  title        = {Graph online change point detection based on fréchet statistics},
  volume       = {661},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). RD2: Reconstructing the residual sequence via under decomposing and dendritic learning for generalized time series predictions. <em>NEUCOM</em>, <em>661</em>, 131867. (<a href='https://doi.org/10.1016/j.neucom.2025.131867'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Time series predictions are crucial topics in the field of machine learning, yet existing methods often have notable shortcomings, including poor prediction accuracy, long computation times, and inefficient use of computational resources. In this study, we propose a novel framework, R econstructing the R esidual sequence via under D ecomposing and D endritic learning (RD2), which incorporates more suitable preprocessing techniques, machine learning models, and training algorithms to efficiently handle time series data. The distinctive aspect of this model lies in the use of variational mode decomposition (VMD) as the preprocessing method. Specifically, the VMD extracts a predetermined number of modes and an under-decomposed residual sequence from the original sequence, which are then processed individually by the phase space reconstruction (PSR) method and dendritic learning (DL) module to generate predictions. After that, the predictions are summed to produce the final result. The proposed RD2 is tested on six real-world time series datasets covering a range of domains, including meteorology, finance, electricity, and transportation. Experimental results demonstrate that RD2 significantly outperforms existing models across various evaluation metrics. The success of RD2 can be attributed to the effective selection of methods, and these methods can improve both accuracy and efficiency of predictions. Consequently, the RD2 model not only holds strong practical potential for applications in the aforementioned domains but also offers valuable insights for researchers tackling complex time series prediction tasks.},
  archive      = {J_NEUCOM},
  author       = {Zhenqian Zhang and Houtian He and Zhenyu Lei and Zihang Zhang and Shangce Gao},
  doi          = {10.1016/j.neucom.2025.131867},
  journal      = {Neurocomputing},
  month        = {1},
  pages        = {131867},
  shortjournal = {Neurocomputing},
  title        = {RD2: Reconstructing the residual sequence via under decomposing and dendritic learning for generalized time series predictions},
  volume       = {661},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). EUAD: End-to-end unsupervised anomaly detection based on few normal data. <em>NEUCOM</em>, <em>661</em>, 131864. (<a href='https://doi.org/10.1016/j.neucom.2025.131864'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, unsupervised anomaly detection has attracted significant attention in the field, as it relies solely on unlabeled samples for training while achieving good performance. Most unsupervised models either adopt anomaly simulation to generate labeled samples or use memory banks to store normal features for comparison. However, existing simulation strategies mainly focus on object-type anomalies, with insufficient attention to fine-grained texture-type ones. Meanwhile, memory bank approaches whether by storing all data or using random sampling lead to excessive memory overhead and lack representativeness in the sampled data. To address these issues, we propose an end-to-end unsupervised anomaly detection framework (EUAD). Specifically, EUAD simulates a wider variety of realistic texture defects by expanding the texture anomaly library to improve the accuracy of texture anomaly detection. Then, EUAD obtains a representative feature set through feature clustering to improve the effectiveness and reliability of sample storage. Finally, we propose a new feature fusion module that integrates multi-scale input features and memory bank features, which includes the Convolutional Block Attention Module (CBAM) and Pixel-based Multi-scale Fusion Module (PMFM). Leveraging this module, EUAD fully utilizes limited normal features to suppress irrelevant information and highlight anomalies, ultimately improving detection sensitivity to anomalies of different sizes. Extensive experimental results on public datasets show that the proposed EUAD in this paper outperforms other anomaly detection models.},
  archive      = {J_NEUCOM},
  author       = {Jiayin Zhou and Waikeung Wong and Kaihang Jiang and Pengjie Tan},
  doi          = {10.1016/j.neucom.2025.131864},
  journal      = {Neurocomputing},
  month        = {1},
  pages        = {131864},
  shortjournal = {Neurocomputing},
  title        = {EUAD: End-to-end unsupervised anomaly detection based on few normal data},
  volume       = {661},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Simultaneous feature and label propagation for multi-view graph convolutional network. <em>NEUCOM</em>, <em>661</em>, 131856. (<a href='https://doi.org/10.1016/j.neucom.2025.131856'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-view learning aims to exploit beneficial patterns from heterogeneous data sources, and it has captured growing attention in recent years. Among the variety of multi-view approaches, graph-based methods, especially graph convolutional networks, have shown considerable potential in mining complex semantic information from multi-view data. Despite their effectiveness, existing methods remain vulnerable to feature-space noise: message passing amplifies such noise, propagating invalid information. This feature-induced bias corrupts semantic information within each view and distorts cross-view interactions, thereby limiting the consistency and discriminative power of multi-view representation learning. In this paper, we propose a framework named simultaneous propagation feature and label multi-view graph convolutional network. Specifically, it comprises two primary components: a feature propagation module that employs the message-passing mechanism of graph convolutional networks supplemented by an attention mechanism to adjust the influence of each view dynamically, and a label propagation module that initializes and iteratively updates label distributions based on the graph structure of each view. Experimental results demonstrate the superiority of the proposed method over existing baseline methods, emphasizing its effectiveness in multi-view semi-supervised classification tasks.},
  archive      = {J_NEUCOM},
  author       = {Yiqing Shi and Jie Lian and Jiayuan Wang and Zhiling Cai and Shiping Wang},
  doi          = {10.1016/j.neucom.2025.131856},
  journal      = {Neurocomputing},
  month        = {1},
  pages        = {131856},
  shortjournal = {Neurocomputing},
  title        = {Simultaneous feature and label propagation for multi-view graph convolutional network},
  volume       = {661},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Dynamic event-triggered distributed fuzzy h∞ optimal formation control for input-constrained underactuated AUVs with unknown dynamics. <em>NEUCOM</em>, <em>661</em>, 131855. (<a href='https://doi.org/10.1016/j.neucom.2025.131855'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cooperative operations of underactuated autonomous underwater vehicles suffer from the issues of limited energy, computation and communication resources. To address these issues, a leader–follower-based dynamic event-triggered distributed fuzzy H ∞ optimal formation control scheme is proposed. First, a filtered formation error equation is designed to transform a second-order system into an affine form error dynamic system. Subsequently, an asymmetric input-constrained H ∞ control problem is converted to a zero-sum game problem with a non-quadratic cost function, thus handling disturbances and input constraints. Next, by reconstructing system dynamics and establishing a Bellman equation, an event-triggered approximate optimal formation control law and an approximate worst-case disturbance law with a single critic structure and without dynamics are designed via a dynamic event triggering mechanism and a generalized fuzzy hyperbolic model. Finally, an improved fuzzy parameter update law is developed to remove requirements for initial stabilization control. Compared with the existing studies, the proposed scheme avoids knowledge and approximation of dynamics, and uses only error feedback data to achieve overall optimal control while reducing computation and communication resources of the controller. Theoretical analyses show that the closed-loop system signals are uniformly ultimately bounded and free of the Zeno phenomenon. Simulation validates the effectiveness and advantages of the scheme.},
  archive      = {J_NEUCOM},
  author       = {Huibin Gong and Li-Ying Hao and Yuan-Xin Li},
  doi          = {10.1016/j.neucom.2025.131855},
  journal      = {Neurocomputing},
  month        = {1},
  pages        = {131855},
  shortjournal = {Neurocomputing},
  title        = {Dynamic event-triggered distributed fuzzy h∞ optimal formation control for input-constrained underactuated AUVs with unknown dynamics},
  volume       = {661},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). A blur-guided multi-attention network based on left-right consistency for gradual defocus deblurring in binocular images. <em>NEUCOM</em>, <em>661</em>, 131853. (<a href='https://doi.org/10.1016/j.neucom.2025.131853'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Froth apparent visual information plays a crucial role in flotation process monitoring. Using binocular camera to reconstruct the 3D surface structure of the froth layer provides richer stereoscopic information, enhancing the ability to perceive flotation working conditions. However, variations in froth layer height introduce gradual defocus blur into binocular froth images, which in turn affects the quality of visual information acquisition. To address this challenge, a blur-guided multi-attention network is proposed to improve denoising performance by precisely localizing and quantifying blur, and by incorporating a left-right consistency framework. Specifically, a blur-aware weighting module is designed to detect blurred regions and estimate local blur severity. To enhance the model's ability to learn from different blurred regions, a blur-guided multi-attention module is designed, enabling the model to focus on severely blurred areas, thereby improving feature learning and optimizing feature representation across different regions. A left-right consistency framework is proposed, which leverages the disparity information between the left and right views to further enhance the detail reconstruction of blurred regions. Experimental results on both the Holopix50k dataset and flotation froth images dataset demonstrate that the proposed method outperforms baseline models by approximately 5 % in performance, and industrial application experiments verify the model's effectiveness and feasibility for real-world deployment. Meanwhile, analysis of representative bad cases reveals limitations when dealing with mixed defocus and motion blur, suggesting an important direction for future improvements.},
  archive      = {J_NEUCOM},
  author       = {Zekai Chen and Zhaohui Tang and Yuze Zhong and Hu Zhang and Zhien Dai and Yongfang Xie},
  doi          = {10.1016/j.neucom.2025.131853},
  journal      = {Neurocomputing},
  month        = {1},
  pages        = {131853},
  shortjournal = {Neurocomputing},
  title        = {A blur-guided multi-attention network based on left-right consistency for gradual defocus deblurring in binocular images},
  volume       = {661},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Deep multi-view subspace adversarial clustering network via NMF feature enhancement. <em>NEUCOM</em>, <em>661</em>, 131852. (<a href='https://doi.org/10.1016/j.neucom.2025.131852'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-view subspace clustering has demonstrated superior performance over the single-view clustering by leveraging the complementary and consistent information across multiple views. However, existing clustering methods primarily focus on learning linear feature representations and deriving consensus latent representations, which overlook the nonlinear structures inherent in the data, and thus lead to the representation degradation and suboptimal clustering performance. To address these challenges, we propose a deep multi-view subspace adversarial clustering network via non-negative matrix factorization feature enhancement, termed NFEMAC. Specifically, the original data is first projected into a high-dimensional space by leveraging the separability, then the fully connected layer and the encoder network are employed to preserve the local manifold structure of the data. Furthermore, we introduce a novel Cross-view Feature Enhancement Fusion (CFEF) module, which utilizes the Non-negative Matrix Factorization (NMF) to extract the distributional structure of the data from the local manifold information, thereby enhancing the latent representations of individual views. An attention mechanism is subsequently applied to obtain a consistent latent representation. Finally, a Generative Adversarial Network (GAN) is incorporated to further enhance the robustness of the shared latent representation. Extensive experiments are implemented on seven publicly available datasets to evaluate the performance of the proposed method. Experimental results demonstrate the superiority of NFEMAC compared with state-of-the-art methods.},
  archive      = {J_NEUCOM},
  author       = {Chong Li and Dongxu Cheng and Yan Yang and Pengyun Jin},
  doi          = {10.1016/j.neucom.2025.131852},
  journal      = {Neurocomputing},
  month        = {1},
  pages        = {131852},
  shortjournal = {Neurocomputing},
  title        = {Deep multi-view subspace adversarial clustering network via NMF feature enhancement},
  volume       = {661},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). A brain-inspired neurodynamic model for efficient sequence memory. <em>NEUCOM</em>, <em>661</em>, 131849. (<a href='https://doi.org/10.1016/j.neucom.2025.131849'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Sequence memory, such as memorizing digit strings and music, is a fundamental cognitive function involving multiple brain regions. It allows for both goal-based retrieval and context-based retrieval after memory formation, which are essential for memory integration and adaptability in improved multitasking performance. However, existing brain-inspired neurodynamic models face significant challenges, including the implementation of bidirectional retrieval functions and the allocation of neurons according to task-specific requirements. Inspired by physiological clues, we hypothesize that sequence memory should involve an information processing architecture corresponding to perception, memory, and semantic memory (PMS) functions. Therefore, we constructed a neurodynamic model conforming to the PMS architecture. Our findings reveal that this network not only addresses the challenges of bidirectional retrieval and task-adaptive neuron allocation, but also spontaneously utilizes inter-layer information coupling to achieve memory retrieval while enabling efficient memorization through reuse of existing spatial patterns with similar sequences during learning. Based on this network framework, we investigated the impact of connection degradation in different brain regions on both goal-based and contextual retrieval. Moreover, in the PMS model, we discovered that the three-layer network architecture and inter-layer coupling play crucial roles in sequence memory. Overall, the brain-inspired PMS model provides theoretical possibilities for simulating and explaining phenomena in sequence memory. It also demonstrates potential for achieving complex cognitive functions through implementation of deeper structural hierarchies, thereby inspiring next-generation sequence memory algorithms based on neurodynamic neural networks.},
  archive      = {J_NEUCOM},
  author       = {Runchen Lai and Youjun Li and Fan Yang and Ying Li and Nan Yao and Chunwang Su and Si-Ping Zhang and Yuanyuan Mi and Celso Grebogi and Zi-Gang Huang},
  doi          = {10.1016/j.neucom.2025.131849},
  journal      = {Neurocomputing},
  month        = {1},
  pages        = {131849},
  shortjournal = {Neurocomputing},
  title        = {A brain-inspired neurodynamic model for efficient sequence memory},
  volume       = {661},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Vulnerabilities of label and model protection in vertical federated learning based on simulation models. <em>NEUCOM</em>, <em>661</em>, 131839. (<a href='https://doi.org/10.1016/j.neucom.2025.131839'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Vertical Federated Learning (VFL) is gaining increasing attention in the context of Artificial Intelligence. Compared to Horizontal Federated Learning (HFL), VFL presents more challenges for existing attacks due to differences in data partitioning, model structure, and the rules of aggregation. While some recent attacks on VFL have been proposed, they tend to rely on strong assumptions, lack flexibility, and target only specific objectives. To overcome these limitations, we introduce a novel high-precision label inference and fine-grained manipulation attack for VFL. In this approach, even without knowing the top model, we generate malicious bottom model outputs by manipulating the top model, which influences the global model’s output during predictions. The feasibility of this attack stems from the leakage of private information via intermediate gradients during the training phase. Leveraging the acquired labels, the adversary simulates a top model to perturb the global model’s output during the prediction phase. The effectiveness of the proposed attack is thoroughly validated via extensive experiments on four representative datasets. Compared to the state-of-the-art methods, the proposed approach achieves a 19.62 % improvement on the CIFAR-10 dataset. Moreover, it attains 99.93 % accuracy on the CINIC-10 dataset. Additionally, we discuss existing defenses that are expected to build a solid security foundation for the practical application of VFL in the future. These findings highlight the importance of developing more effective and efficient defense strategies against such attacks. Our codebase is publicly accessible at https://github.com/Persistsq/VFLLIMAcode .},
  archive      = {J_NEUCOM},
  author       = {Xiaojie Zhao and Zhenhan Ke and Hang Yu and Yi Li and Hongfan Chen and Hui Xiang and Zeyu Li and Sen Liu and Jinqiao Shi},
  doi          = {10.1016/j.neucom.2025.131839},
  journal      = {Neurocomputing},
  month        = {1},
  pages        = {131839},
  shortjournal = {Neurocomputing},
  title        = {Vulnerabilities of label and model protection in vertical federated learning based on simulation models},
  volume       = {661},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Normalization-consistent data curation for generalizable deepfake detection. <em>NEUCOM</em>, <em>661</em>, 131838. (<a href='https://doi.org/10.1016/j.neucom.2025.131838'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deepfake has recently garnered considerable attention due to its potential threat. Recent detectors often struggle to generalize due to sensitivity to dataset-specific biases. We identify a key factor: their performance varies significantly with different normalization parameters, indicating reliance on preprocessing artifacts rather than authentic manipulation traces. To address this, we propose Normalization-Consistent Data Curation (NormCura), which selects training samples based on their prediction stability across normalization variations. NormCura first evaluates sample consistency under multiple normalization conditions, then trains only on stable samples. This filters out normalization-sensitive artifacts while retaining robust forensic patterns. Extensive cross-dataset evaluations on nine deepfake datasets demonstrate that this approach significantly improves generalization performance, including emerging diffusion-based synthetic faces, confirming that normalization consistency is an effective proxy for learning generalizable deepfake detection features.},
  archive      = {J_NEUCOM},
  author       = {Shijie Hou and Xinghao Jiang and Ke Xu and Qiang Xu and Laijin Meng and Tanfeng Sun},
  doi          = {10.1016/j.neucom.2025.131838},
  journal      = {Neurocomputing},
  month        = {1},
  pages        = {131838},
  shortjournal = {Neurocomputing},
  title        = {Normalization-consistent data curation for generalizable deepfake detection},
  volume       = {661},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). SVDTI: Stacked variational autoencoder with SMILES-based drug representations for identifying drug-target interaction. <em>NEUCOM</em>, <em>661</em>, 131837. (<a href='https://doi.org/10.1016/j.neucom.2025.131837'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The rapid identification of novel drug–target interactions (DTIs) remains a critical challenge in drug development, as traditional experimental methods are both resource-intensive and time-consuming. Motivated by the need to accelerate drug discovery and reduce experimental costs, computational strategies have emerged as powerful alternatives, leveraging advanced algorithms and data-driven approaches to predict potential DTIs efficiently. In this paper, we introduce a novel method, which employs a stacked variational autoencoder (SVAE) to efficiently predict drug–target interactions, with the goal of enhancing the understanding and identification of these crucial relationships in drug discovery. This model leverages protein sequences and drug chemical properties as input features. It employs a stacked variational autoencoder (SVAE) with Long Short-Term Memory (LSTM) networks to map high-dimensional data into compact, informative low-dimensional vectors. The LSTM architecture captures temporal dependencies in protein sequences, thereby enhancing the model's ability to encode complex patterns. Next, the feature representation is fed into a neural collaborative filtering (NCF) model. This model combines the linear characteristics of matrix factorization with the nonlinear representation power of a multi-layer perceptron (MLP) to generate the final prediction, thereby improving the accuracy of DTI prediction. As a result, in comparison to existing state-of-the-art methods for DTIs prediction, our model demonstrates remarkable improvements in predictive performance. These findings highlight the capability of the proposed model to effectively integrate diverse sources of information for predicting DTIs, addressing critical challenges in drug discovery and offering a robust and efficient framework that contributes valuable perspectives to the field.},
  archive      = {J_NEUCOM},
  author       = {Jihwan Ha},
  doi          = {10.1016/j.neucom.2025.131837},
  journal      = {Neurocomputing},
  month        = {1},
  pages        = {131837},
  shortjournal = {Neurocomputing},
  title        = {SVDTI: Stacked variational autoencoder with SMILES-based drug representations for identifying drug-target interaction},
  volume       = {661},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). DSM-STWave: Enhancing traffic flow prediction for both offline and online scenarios. <em>NEUCOM</em>, <em>661</em>, 131836. (<a href='https://doi.org/10.1016/j.neucom.2025.131836'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Accurate traffic flow prediction is crucial for intelligent transportation systems. Although wave-decomposition methods effectively capture temporal dynamics, existing models such as STWave are constrained by single spatial feature extraction and static architectures . We propose DSM-STWave, a Dual Spatial and Memory-enhanced framework that overcomes these limitations with two core innovations: (1) a dual spatial attention mechanism that integrates sparse global attention O ( k N ) and local k -nearest neighbor attention O ( N log ⁡ N ) to efficiently model both long- and short-range dependencies; (2) a lightweight adaptive memory module that dynamically refines prediction strategies during inference, enabling resilience to distribution shifts in both online and offline settings. Extensive experiments on four real-world datasets highlight the practical advantages of DSM-STWave: it reduces MAE by 5.7 % on PeMSD4 ( 19.28 → 18.18 ), accelerates training by 67.8 % ( 142.1 s → 45.8 s ), and speeds up inference by 28.6 % compared with STWave, delivering both efficiency and reliability for real-time traffic prediction under dynamic conditions.},
  archive      = {J_NEUCOM},
  author       = {Yinglan Liang and Sanyang Liu and Yiguang Bai and Yudong Gong and Tianqing Zhu},
  doi          = {10.1016/j.neucom.2025.131836},
  journal      = {Neurocomputing},
  month        = {1},
  pages        = {131836},
  shortjournal = {Neurocomputing},
  title        = {DSM-STWave: Enhancing traffic flow prediction for both offline and online scenarios},
  volume       = {661},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Backdoor defense framework with sparse training and detection in federated learning. <em>NEUCOM</em>, <em>661</em>, 131831. (<a href='https://doi.org/10.1016/j.neucom.2025.131831'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Backdoor attacks occur throughout the cycle of Federated Learning (FL), where an attacker injects backdoor parameters into the global model, causing the global model to behave in a specific way when it encounters backdoor samples. Existing defense methods based on statistical differences and pruning strategies often fail to detect high-dimensional malicious models or prevent collusion among malicious clients. In this paper, we propose a Backdoor Defense Framework with Sparse Training and Detection (BDF-STD) in FL. First, we propose a sparsification strategy based on the Erdös-Rényi-Kernel (ERK), which prevents collusion among malicious parameters, and its update schedule is optimized. Then, a malicious model detection method is designed based on Minkowski distance, which identifies malicious models through dynamic weighting and anomaly score evaluation. In addition, a novel pruning method is proposed based on the cumulative value of the gradient change for local model pruning and recovery to further enhance the defense effect. Finally, we perform extensive experimental evaluations of BDF-STD and show that BDF-STD outperforms the state-of-the-art methods on the four datasets, reducing the success rate of backdoor attacks to less than 3 % and to 0.3 % in the best case, while maintaining the accuracy of the global model.},
  archive      = {J_NEUCOM},
  author       = {Yingna Li and Yongde Wang and Haoheng Yuan and Pengfei Zhang and Wei Huang and Zhiquan Liu},
  doi          = {10.1016/j.neucom.2025.131831},
  journal      = {Neurocomputing},
  month        = {1},
  pages        = {131831},
  shortjournal = {Neurocomputing},
  title        = {Backdoor defense framework with sparse training and detection in federated learning},
  volume       = {661},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Event-triggered quantization control for uncertain networked control systems under DoS attacks: A 1-bit encoding scheme. <em>NEUCOM</em>, <em>661</em>, 131818. (<a href='https://doi.org/10.1016/j.neucom.2025.131818'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper investigates how to stabilize the uncertain networked control systems under denial-of-service (DoS) attacks using a 1-bit encoding scheme. This scheme quantizes the input signal of the quantizer and encodes it into a 1-bit binary value, effectively reducing the number of transmitted bits while potentially introducing significant quantization error. Additionally, uncertain system parameters and DoS attacks exacerbate quantization errors, potentially causing discrepancies between the quantized signals at the encoder and decoder, thereby compromising system stability. To address these issues, a 1-bit adaptive quantizer is proposed that operates without prior knowledge of the system parameters and dynamically adjusts the quantization signal based on the sign of the quantization error. In conjunction with this quantizer, a switching event-triggered mechanism (ETM) is designed to ensure consistency between the quantized signals at the encoder and decoder while reducing communication resource consumption. Moreover, the parameters of the switching ETM and the 1-bit adaptive quantizer are co-designed to guarantee system stability. Finally, confirmatory and comparative simulations validate the effectiveness of the theoretical results and highlight the advantages of the proposed strategy.},
  archive      = {J_NEUCOM},
  author       = {Wen-Hui Wang and Yan-Wu Wang and Xiao-Kang Liu and Wu Yang and Wu-Hua Chen},
  doi          = {10.1016/j.neucom.2025.131818},
  journal      = {Neurocomputing},
  month        = {1},
  pages        = {131818},
  shortjournal = {Neurocomputing},
  title        = {Event-triggered quantization control for uncertain networked control systems under DoS attacks: A 1-bit encoding scheme},
  volume       = {661},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). MHFFNet: Multi-category hybrid feature fusion network for few-shot anomaly detection. <em>NEUCOM</em>, <em>661</em>, 131812. (<a href='https://doi.org/10.1016/j.neucom.2025.131812'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Anomaly Detection (AD) plays a crucial role in industrial procedure. Due to the difficulty of obtaining samples in actual industrial environments, Few-Shot Anomaly Detection (FSAD) technology has recently received widespread attention. However, in real-world production environments, it is often essential to handle unknown samples from multiple categories, and samples from various categories exhibit distinct defect patterns and visual differences, which subsequently influence the detection and localization of anomalous regions in samples from unknown categories. To improve the model’s detection performance for unknown categories, a novel FASD model named Multi-category Hybrid Feature Fusion Network (MHFFNet) has been proposed. The MHFFNet randomly selects sample pairs from multiple categories and conducts feature extraction and fusion, employing an attention mechanism to enhance feature representation. Following feature encoding, it implements strategies to maximize the feature similarity between sample pairs to facilitate model training. When testing new category samples, one can determine whether the test samples are anomalous based on the normal sample feature distribution without the need to retrain or fine-tune the model. The experimental results demonstrate that on the MVTec-AD and MPDD datasets, the proposed MHFFNet outperforms existing methods by approximately 1 %–3 % in anomaly detection and localization tasks.},
  archive      = {J_NEUCOM},
  author       = {Ao Liu and Shu Zhao and Xingsheng Lu and Zede Zhu and Dingjun Qu and Shenghui Zhao},
  doi          = {10.1016/j.neucom.2025.131812},
  journal      = {Neurocomputing},
  month        = {1},
  pages        = {131812},
  shortjournal = {Neurocomputing},
  title        = {MHFFNet: Multi-category hybrid feature fusion network for few-shot anomaly detection},
  volume       = {661},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Deep learning-based group activity recognition in videos: A survey. <em>NEUCOM</em>, <em>661</em>, 131150. (<a href='https://doi.org/10.1016/j.neucom.2025.131150'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Group activity recognition (GAR), which aims to identify the activity performed by a group of people in a given video, is one of the representative tasks for video understanding. With the advancement of deep neural networks, deep learning-based methods have emerged as a dominant alternative to handcrafted feature engineering, which can automatically mine the feature representations from the input data. In this survey, we present a comprehensive review with in-depth analysis of deep learning-based group activity recognition from 2016 to 2024. Specifically, we first briefly outline the definition and several major challenges of group activity recognition. Then, a detailed taxonomy is introduced in terms of different supervision types, network types, modeling mechanism types, and input types, which can better classify the existing state-of-the-art methods from different perspectives. To the best of our knowledge, we are the first to develop such a taxonomy for GAR. For a better understanding of the pros and cons of each type, the detailed discussions under each type are also presented with further categorization. Moreover, we also provide the datasets, evaluation metrics, and performance comparisons for group activity recognition. In the end, we conclude the survey by suggesting future potential research directions in this rapidly growing GAR field to facilitate new research ideas.},
  archive      = {J_NEUCOM},
  author       = {Xiaolin Zhu and Dongli Wang and Yan Zhou and Zixin Zhang and Jianxun Li and Rui Su and Yongcan Weng and Tao Zhu},
  doi          = {10.1016/j.neucom.2025.131150},
  journal      = {Neurocomputing},
  month        = {1},
  pages        = {131150},
  shortjournal = {Neurocomputing},
  title        = {Deep learning-based group activity recognition in videos: A survey},
  volume       = {661},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). ML-HAN: Multi-level heterogeneous graph attention network for representation learning with semantic diversity via feature-node-semantic attention. <em>NEUCOM</em>, <em>660</em>, 131962. (<a href='https://doi.org/10.1016/j.neucom.2025.131962'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Graph neural networks (GNNs) have been widely recognized for their prominent capability in modeling complex systems. Nevertheless, developing GNNs to handle heterogeneous graphs with diverse node and link types remains a non-trivial challenge. The intrinsic diversity of nodes and edges induces semantic diversity, which further exacerbates the difficulty of representation learning in heterogeneous graphs. Thus, there is an urgent need to advance GNNs tailored for heterogeneous graph scenarios. Attention mechanisms, which have exhibited remarkable performance across multiple domains, have recently been introduced into graph representation learning to alleviate this issue. In this study, we propose a novel multi-level heterogeneous graph attention network (ML-HAN) for effective information extraction from heterogeneous networks. Specifically, ML-HAN incorporates three hierarchical attention modules: the feature-level attention module quantifies the significance of neighbors at the feature level; the node-level attention module assigns differentiated weights to distinct node features; and the semantic-level attention module identifies the optimal combination of node embeddings derived from various meta-paths. Leveraging this hierarchical framework, ML-HAN fully exploits information from three critical dimensions of heterogeneous graphs—neighbors, features, and meta-paths—thereby enabling effective extraction of rich semantic information. Additionally, we conduct three ablation experiments to validate the independent contribution of each module, and extensive experiments on three real-world datasets empirically verify the superior effectiveness of our proposed model.},
  archive      = {J_NEUCOM},
  author       = {Zhiyao La and Jinhui Shen and Yuchen Song and Shuchuan Tian and Weijun Gong},
  doi          = {10.1016/j.neucom.2025.131962},
  journal      = {Neurocomputing},
  month        = {1},
  pages        = {131962},
  shortjournal = {Neurocomputing},
  title        = {ML-HAN: Multi-level heterogeneous graph attention network for representation learning with semantic diversity via feature-node-semantic attention},
  volume       = {660},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). DCTNet: A dual-branch CNN-transformer network for SAR-optical image classification. <em>NEUCOM</em>, <em>660</em>, 131949. (<a href='https://doi.org/10.1016/j.neucom.2025.131949'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Land cover classification in remote sensing imagery remains a challenging task. It requires modeling both global scene context and fine-scale details, while dealing with complex landscapes, varied spatial resolutions, and heterogeneous target types. Many existing methods struggle to harmonize modality gaps across multi-source datasets. They also face challenges in capturing textural patterns across different scales and in modeling long-range spatial dependencies. To address these limitations, we propose a dual-branch CNN–transformer architecture, termed DCTNet. The network integrates local details with global context to produce expressive and highly discriminative representations. In our design, a three-level convolutional pipeline extracts low-level features from SAR data. Meanwhile, a hierarchical spectral–spatial network processes optical imagery. To preserve detailed structures and capture extended dependencies, we incorporate a transformer-based component. This ensures a balance between fine-grained local patterns and high-level global semantics. Comprehensive evaluations on Augsburg, Berlin, and FUSAR-Map datasets demonstrate the robustness and performance advantages of the proposed method.},
  archive      = {J_NEUCOM},
  author       = {Zhiqiang Zhao and Qi Han and Xiaofan Wang and Yuan Qiu and Xinhong Hei},
  doi          = {10.1016/j.neucom.2025.131949},
  journal      = {Neurocomputing},
  month        = {1},
  pages        = {131949},
  shortjournal = {Neurocomputing},
  title        = {DCTNet: A dual-branch CNN-transformer network for SAR-optical image classification},
  volume       = {660},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). VGen-adapter: A vision generalization adapter for stable diffusion 3. <em>NEUCOM</em>, <em>660</em>, 131948. (<a href='https://doi.org/10.1016/j.neucom.2025.131948'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent years have witnessed rapid advancements in large-scale text-to-image generation models, leading to substantial improvements in generated image quality and the continuous enrichment of the associated ecosystem. Concurrently, various control methods tailored for image generation have emerged, enabling diverse control tasks based on input images. The rise of Transformer architectures has led to the progressive replacement of traditional U-Net-based generative models with Diffusion Transformer (DiT) frameworks. However, most existing control methods face significant challenges in directly adapting to DiT architectures, commonly exhibiting limitations such as inadequate control precision, inefficient training processes, excessive parameter scales, and heavy reliance on large training datasets. To address these issues, we introduce a lightweight image control adapter method tailored for DiT-based Stable Diffusion 3 frameworks, termed VGen-adapter ( V ision Ge neralization adapter ). The proposed method incorporates an image feature extractor and optimizes the feature fusion strategy, significantly enhancing control precision. Regarding training strategy, VGen-adapter employs a parameter-efficient approach by completely freezing all parameters of the original model while implementing a lightweight design for the adapter module. Through the integration of noise perturbation for data augmentation, our approach enables high-quality image generation using limited text-image datasets. Furthermore, with only 95 M parameters (approximately 5 % of the SD3-Medium model), this architecture achieves substantial computational efficiency. Comparative experiments on the COCO dataset demonstrate that the VGen-adapter achieves excellent control performance while maintaining high efficiency and low parameter count, verifying its feasibility and effectiveness under the DiT architecture.},
  archive      = {J_NEUCOM},
  author       = {Zhenglong Xiang and Chenghao Zhou and Yu Xue},
  doi          = {10.1016/j.neucom.2025.131948},
  journal      = {Neurocomputing},
  month        = {1},
  pages        = {131948},
  shortjournal = {Neurocomputing},
  title        = {VGen-adapter: A vision generalization adapter for stable diffusion 3},
  volume       = {660},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Dynamic-aware self-attention with embedded CWT for imbalanced ECG classification in cardiac disease detection. <em>NEUCOM</em>, <em>660</em>, 131945. (<a href='https://doi.org/10.1016/j.neucom.2025.131945'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Accurate detection of arrhythmia and congestive heart failure (CHF) from electrocardiogram (ECG) signals is crucial for timely intervention, but remains challenging due to class imbalance, noise, and the need to capture both short-term and long-term temporal patterns. This study introduces a dynamic-aware self-attention network that incorporates a differentiable continuous wavelet transform (CWT) layer and an adaptive class-weighting loss. The model combines gated recurrent units (GRUs) for sequential modeling, self-attention to highlight diagnostically relevant segments, and embedded CWT for optimized time–frequency learning. Standardized nonlinear dynamical features, such as the Lyapunov exponent and sample entropy, are combined with deep features to enhance disease state discrimination. On a composite ECG dataset of 162 recordings, evaluated with stratified splits and 5-fold cross-validation, the model achieved an average accuracy of 98.17 % and 100 % sensitivity for arrhythmia, with no misclassification between normal and diseased cases. Confidence intervals confirmed the model's robustness, though the perfect sensitivity should be interpreted with caution due to the dataset size. Comparative analysis indicates that the proposed framework matches or exceeds recent state-of-the-art methods, and inference times of approximately 50 ms per 10-second ECG segment support near real-time use. These findings demonstrate the framework's potential to enable imbalance-resilient, interpretable, and end-to-end ECG analysis, supporting earlier and more reliable diagnosis of cardiovascular disease.},
  archive      = {J_NEUCOM},
  author       = {Fuad E. Alsaadi and Njud S. Alharbi},
  doi          = {10.1016/j.neucom.2025.131945},
  journal      = {Neurocomputing},
  month        = {1},
  pages        = {131945},
  shortjournal = {Neurocomputing},
  title        = {Dynamic-aware self-attention with embedded CWT for imbalanced ECG classification in cardiac disease detection},
  volume       = {660},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Diff-mamba: A diffusion-mamba framework for hyperspectral image classification. <em>NEUCOM</em>, <em>660</em>, 131930. (<a href='https://doi.org/10.1016/j.neucom.2025.131930'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The accuracy of modeling complicated spectral-spatial distribution is crucial for hyperspectral image (HSI) classification. However, most existing methods primarily focus on enhancing global and local feature representations, while neglecting the complex spectral-spatial distribution relationships in high-dimensional data. To address this issue, we propose a novel framework combining diffusion models and Mamba called Diff-Mamba for characterizing the real spectral-spatial distribution relationships from a generative perspective. Specifically, the framework comprises a spectral-spatial diffusion feature generation module based on diffusion models, a Mamba-based global feature extraction module, and a center pixel-driven semantic token generator. Firstly, the spectral-spatial diffusion feature generation module is developed to capture spectral-spatial distribution information of objects in HSI. This module consists of forward and reverse diffusion processes. Within the reverse diffusion process, we propose a Transformer-based spectral-spatial diffusion network specifically designed for HSI classification tasks, effectively capturing the complex distributional relationships inherent in HSI. In this process, iteratively denoising and explicitly constructing the data generation process are adopted to adaptively construct cross-correlation between samples and effectively generate diffusion features reflecting spectral-spatial distribution information. Secondly, the diffusion features are fed into the Mamba-based global feature extraction module to model the global correlation of spectral-spatial features with linear complexity. Finally, a center pixel-driven semantic token generator is designed to dynamically focus on the feature representation of the central pixel for patch-wise HSI classification. Experimental results on four public HSI datasets demonstrate that the proposed Diff-Mamba is competitive with state-of-the-art methods. The code will be made publicly available at https://github.com/shike801/Diff-Mamba .},
  archive      = {J_NEUCOM},
  author       = {Shuaibing Shi and Min Li and Yongqi Yin and Yujie He and Aitao Yang},
  doi          = {10.1016/j.neucom.2025.131930},
  journal      = {Neurocomputing},
  month        = {1},
  pages        = {131930},
  shortjournal = {Neurocomputing},
  title        = {Diff-mamba: A diffusion-mamba framework for hyperspectral image classification},
  volume       = {660},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). On synchronization of discontinuous competitive fuzzy neural networks with time-varying delays via non-chattering quantization. <em>NEUCOM</em>, <em>660</em>, 131920. (<a href='https://doi.org/10.1016/j.neucom.2025.131920'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper is dedicated to studying the global asymptotic synchronization of discontinuous competitive fuzzy neural networks (DCFNNs) with time-varying delays. To address the technical challenges arising from excessive communication burden and time-varying delays, an innovative non-chattering quantized controller is designed. By comprehensively applying Lyapunov functionals, differential inclusion theory, and inequality techniques, novel and practical synchronization criteria are derived. Furthermore, an adaptive non-chattering quantized controller adjustment law with less conservative control gains is constructed. Finally, the effectiveness of the aforementioned theoretical analysis is fully verified through simulation examples.},
  archive      = {J_NEUCOM},
  author       = {Yinjie Qian and Yuanhua Qiao},
  doi          = {10.1016/j.neucom.2025.131920},
  journal      = {Neurocomputing},
  month        = {1},
  pages        = {131920},
  shortjournal = {Neurocomputing},
  title        = {On synchronization of discontinuous competitive fuzzy neural networks with time-varying delays via non-chattering quantization},
  volume       = {660},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). MGC-net: Semi-supervised domain generalization in medical image segmentation via multi-granularity consistency. <em>NEUCOM</em>, <em>660</em>, 131919. (<a href='https://doi.org/10.1016/j.neucom.2025.131919'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Medical image segmentation tasks face two major challenges: the scarcity of annotated data and poor generalization on unseen domains. While semi-supervised learning and domain generalization techniques have been explored separately, existing methods often focus on only one challenge, limiting their real-world applicability. Furthermore, existing methods lack sufficient consideration of both image and feature levels. Ignoring image granularity consistency tends to generate images with pronounced style fluctuations and semantic shift. Overlooking feature granularity invariance may lead to yielding different feature representation for structures of same category but different styles. Jointly imposing both constraints allows us to effectively extract domain-invariant features. Specifically, MGC-Net integrates an SSDG segmentation branch and a self-supervised reconstruction branch to design consistency rules from the image granularity and feature granularity. The Fourier transform is utilized to achieve image style enhancement, and a reconstruction network is designed to implement the consistency constraint of the image granularity. For feature granularity consistency, a Feature Stabilization and Orthogonality (FSO) module and a Domain Consistency and Class Separation (DCCS) module are developed. FSO ensures that features learned by the segmentation network remain invariant to these style changes. DCCS aims to achieve inter-domain consistency and inter-class separability across data from different domains. Experimental results on the M&Ms and SCGM datasets demonstrate that the proposed method achieves competitive results compared with existing methods.},
  archive      = {J_NEUCOM},
  author       = {Yanfeng Li and Xiaochen Ma and Jia Sun and Houjin Chen and Bijuan Ren and Minjun Wang},
  doi          = {10.1016/j.neucom.2025.131919},
  journal      = {Neurocomputing},
  month        = {1},
  pages        = {131919},
  shortjournal = {Neurocomputing},
  title        = {MGC-net: Semi-supervised domain generalization in medical image segmentation via multi-granularity consistency},
  volume       = {660},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Multi-scale regulation of reservoir topology in echo state networks. <em>NEUCOM</em>, <em>660</em>, 131918. (<a href='https://doi.org/10.1016/j.neucom.2025.131918'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Echo State Networks (ESNs) are widely used in many fields. The core of the original ESNs is the reservoir, whose topology is random and unchanged during training. However, the random topology usually performs poorly on complex tasks. It is always an open topic to regulate the topology of the reservoir according to the given task. Inspired by the synaptic plasticity of the brain network in biology, this paper proposes a multi-scale update approach to optimize the topology of ESNs, named multi-scale echo state network (MS-ESN). Initially, based on the structural and functional connections in the brain network, we divide the reservoir connection weight matrix into the Hadamard product of a functional connectivity matrix and structural connectivity matrix. Subsequently, we propose a method to optimize the functional matrix and structural matrix across different temporal scales. Finally, experiments are conducted on five benchmark datasets. The experimental results show that the proposed MS-ESN significantly improves the performance of ESNs in time series prediction tasks.},
  archive      = {J_NEUCOM},
  author       = {Xinyu Shen and Fanjun Li and Jiayue Feng and Wenjie Li and Jingyi Chen},
  doi          = {10.1016/j.neucom.2025.131918},
  journal      = {Neurocomputing},
  month        = {1},
  pages        = {131918},
  shortjournal = {Neurocomputing},
  title        = {Multi-scale regulation of reservoir topology in echo state networks},
  volume       = {660},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Adaptive memory refinement and perception enhancement for exo-to-ego video generation. <em>NEUCOM</em>, <em>660</em>, 131917. (<a href='https://doi.org/10.1016/j.neucom.2025.131917'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The task of synthesizing cross-view videos from an exocentric (third-person) to an egocentric (first-person) perspective, referred to as the E2VG problem, remains highly challenging. This is due to the significant viewpoint differences and limited spatial overlap between the two perspectives. Current approaches often fail to capture the temporal dynamics essential for target-view synthesis, and insufficiently leverage source-view perceptual features. In this paper, we present a video-based framework, Adaptive Memory Refinement and Perception Enhancement (ARPE), to address the problem. To capture long-horizon dependencies beyond redundant short-term dynamics, we propose a Distant Temporal Dependencies (DTD) module that extracts egocentric-relevant semantics from temporally distant exocentric frames. By leveraging a sliding window, DTD aligns long-range temporal patterns across views and refines exocentric features through an egocentric-memory guidance. To enhance the focus of the model on informative content, we propose a Saliency-guided Relevance Weighting (SRW) module that adaptively highlights semantically relevant frames and spatial regions. Specifically, SRW assigns inter-frame attention to distant frames based on their relevance to the target-view reconstruction, and further applies intra-frame weighting to emphasize salient areas within each selected frame. These weights are guided by the similarity between the temporal dynamics of the two views, ensuring spatial-temporal consistency. Recognizing the need for semantic consistency across views, we propose the DINOv2 Perception Enhancement (DPE) module. It leverages DINOv2 features to capture view-invariant object-scene cues, thereby improving cross-view feature coherence. Our extensive experimental analysis demonstrates that our approach outperforms existing state-of-the-art methods, excelling in both quantitative metrics and qualitative assessments.},
  archive      = {J_NEUCOM},
  author       = {Jianhui Li and Weipeng Hu and Xingyue Wang and Jiun Tian Hoe and Ping Hu and Xudong Jiang and Yap-Peng Tan},
  doi          = {10.1016/j.neucom.2025.131917},
  journal      = {Neurocomputing},
  month        = {1},
  pages        = {131917},
  shortjournal = {Neurocomputing},
  title        = {Adaptive memory refinement and perception enhancement for exo-to-ego video generation},
  volume       = {660},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). When less is more: Sample-aware pruning of uninformative features improves neural-network regression. <em>NEUCOM</em>, <em>660</em>, 131913. (<a href='https://doi.org/10.1016/j.neucom.2025.131913'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Modern regression datasets often contain a mix of weakly informative predictors and purely noisy features. We establish, both theoretically and empirically, that discarding variables whose population covariance with the target is zero does not increase expected generalisation risk, up to a vanishing O ( log ⁡ n / n ) term under weight decay, and typically enhances performance in small-sample regimes. Risk bounds show that pruning reduces finite-sample estimation error O ( log ⁡ n / n ) without affecting approximation error, while gradients associated with noise dimensions vanish asymptotically. Controlled synthetic benchmarks spanning 27 different configurations confirm these predictions: at n = 10 2 samples with 18/20 informative features (correlation strength of 0.5), pruning reduces MSE by 44 % and increases R 2 from 0.426 to 0.678; as n grows to 10 4 , the benefit tapers, reflecting the growing influence of implicit regularisation. Attribution analysis via SHAP corroborates the oracle-level identification of relevant features. These conclusions are further validated on real-world data using the Boston Housing Dataset, where pruning just two statistically less informative features yields consistent gains in both full-sample and small-sample regimes, despite the presence of negatively correlated predictors—supporting our theoretical claim that correlation strength, not sign, determines informativeness. The safety guarantee is conditional on informativeness defined by nonzero population covariance with the target; variables that are informative only through purely non-monotonic or symmetric effects fall outside this scope. Our results caution against the automatic retention of high- p -value variables, provide sample-size-aware guidelines for feature filtering, and extend to any shrinkage-based learner, including ridge regression, kernel methods, and Gaussian processes. Code and data are publicly released to ensure full reproducibility.},
  archive      = {J_NEUCOM},
  author       = {Nicholas Christakis and Dimitris Drikakis},
  doi          = {10.1016/j.neucom.2025.131913},
  journal      = {Neurocomputing},
  month        = {1},
  pages        = {131913},
  shortjournal = {Neurocomputing},
  title        = {When less is more: Sample-aware pruning of uninformative features improves neural-network regression},
  volume       = {660},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). High-order structural attribution distillation for link prediction. <em>NEUCOM</em>, <em>660</em>, 131910. (<a href='https://doi.org/10.1016/j.neucom.2025.131910'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, Graph Neural Networks (GNNs) have achieved great success in processing graph-structured data and have shown remarkable improvements in link prediction. Despite these advances, neighborhood fetching incurred by graph dependency leads to expensive computational and time costs. Fortunately, GNN-to-MLP distillation bridges this gap by transferring graph dependency from a heavy GNN teacher into a more efficient MLP student. However, most of the existing works are devoted to learning one-hop graph structure information, which primarily focuses on local neighborhoods and fails to fully exploit higher-order relationships, making the graph-independent MLPs even more challenging to uncover complex connections and implicit patterns within the graph. Therefore, this work proposes a H igh-order S tructural A ttribution D istillation framework ( HSAD ) for link prediction task to fully leverage high-order relationships to facilitate GNN-to-MLP distillation. It primarily consists of two key processes: Structural Attribution Learning (SAL) and Structural Attribution Transfer (SAT). SAL leverages high-order adjacency matrices as input and estimates a structural similarity between nodes. By incorporating a structure extraction network (SEN) for nonlinear mapping, it learns complex structural attributions that contain local topology, enabling the model to capture implicit patterns within the graph. Subsequently, SAT employs multiple distillation strategies to incorporate the structural attribution into the student’s graph-independent node features, enhancing the student’s perception of the graph structure. Extensive experiments are conducted on seven benchmark datasets. The results demonstrate that our HSAD framework greatly improves the performance of MLP and achieves state-of-the-art results in link prediction.},
  archive      = {J_NEUCOM},
  author       = {Junyang Feng and Shunzhi Yang and Changdong Wang and Xiaowen Ma and Yibo Meng and Yunwen Chen and Zhenhua Huang},
  doi          = {10.1016/j.neucom.2025.131910},
  journal      = {Neurocomputing},
  month        = {1},
  pages        = {131910},
  shortjournal = {Neurocomputing},
  title        = {High-order structural attribution distillation for link prediction},
  volume       = {660},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Multi-scale hypergraph representation learning with channel attention for next POI recommendation. <em>NEUCOM</em>, <em>660</em>, 131908. (<a href='https://doi.org/10.1016/j.neucom.2025.131908'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the rapid development of location-based applications and mobile internet technologies, improving the performance of next Point-of-Interest (PoI) recommendation has become crucial for enhancing user experience and advancing the intelligence of applications. Addressing the limitations of existing POI recommendation methods in modeling spatio-temporal coupling features, multi-view feature fusion, and high-order relational capture, this paper proposes a novel Channel Attention-enhanced Multi-Scale Hypergraph Learning for PoI recommendation CAMSHG. CAMSHG first constructs a multi-semantic hypergraph and introduces a channel attention mechanism to dynamically model the importance of different semantic channels, thereby enhancing high-order feature representations. Second, a view-consistent alignment mechanism based on contrastive learning is employed to improve the consistency of multi-view representations. Third, a multi-scale convolutional feature fusion module is designed to fully integrate multi-view information at local and global granularities, further boosting recommendation performance. Extensive experiments and evaluations on three publicly available datasets demonstrate that CAMSHG consistently outperforms various advanced baseline methods across multiple metrics. Ablation studies and hyperparameter sensitivity analyses further validate the effectiveness of each designed module and the robustness of the overall framework. The results confirm that CAMSHG offers a unified and efficient solution for next POI recommendation in complex application scenarios, showing strong potential for practical deployment and further development. To facilitate future research, we release the code at https://github.com/Z70rain/CAMSHG .},
  archive      = {J_NEUCOM},
  author       = {Qilin Zhou and Jinkang Ye and Wei Zhou and Junhao Wen},
  doi          = {10.1016/j.neucom.2025.131908},
  journal      = {Neurocomputing},
  month        = {1},
  pages        = {131908},
  shortjournal = {Neurocomputing},
  title        = {Multi-scale hypergraph representation learning with channel attention for next POI recommendation},
  volume       = {660},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). A spiking convolutional neural network for glioma brain tumor segmentation using a spike-timing-dependent plasticity method. <em>NEUCOM</em>, <em>660</em>, 131903. (<a href='https://doi.org/10.1016/j.neucom.2025.131903'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Brain tumor segmentation efficiently can enhance patient prognosis. However, because of variations in the shape, size, and texture of gliomas from patient to patient, brain tumor segmentation is a challenging task. In this paper, brain tumors are segmented with a high rate of accuracy using a Spiking Convolutional Neural Network (SCNN). The SCNN framework utilizes a spike-time neural encoding technique that is followed by some convolutional and pooling layers. Our technique uses four feature extraction routes for exploring essential features from four MRI modalities including Flair, T1, T1C, and T2. The first layer of our model in each route applies the Difference of Gaussians (DoG) kernels for detecting contrasts in each MRI image. The input image is converted into an asynchronous spike train at the first layer that encodes the visual information in the temporal order of the spikes. Using neurons in a convolutional layer, these input spikes are mixed and emit a spike as soon as the intensity change exceeds a distinct threshold. All convolutional layers are equipped with spike-timing-dependent plasticity (STDP) for learning concealed patterns. The essential role of the first step in Spiking-based Neural Networks (SNNs) is to encode the input data into discrete spikes in the temporal domain. Verification of our model with the BraTS 2018 dataset shows the improvement of the performance parameter values compared to the state-of-the-art models. The mean of dice indexes of the SCNN framework on the BraTS 2018 dataset for the Core, Whole, and Enhanced areas are 0.92, 0.89, and 0.92, respectively.},
  archive      = {J_NEUCOM},
  author       = {Abbas Bagherian Kasgari and Soroush Sadeghi and Payam Zarbakhsh and Saeid Jafarzadeh Ghoushchi and Ramin Ranjbarzadeh},
  doi          = {10.1016/j.neucom.2025.131903},
  journal      = {Neurocomputing},
  month        = {1},
  pages        = {131903},
  shortjournal = {Neurocomputing},
  title        = {A spiking convolutional neural network for glioma brain tumor segmentation using a spike-timing-dependent plasticity method},
  volume       = {660},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). ULMGNN: Fragmented layer grouping in GUI designs through graph learning based on multimodal information. <em>NEUCOM</em>, <em>660</em>, 131894. (<a href='https://doi.org/10.1016/j.neucom.2025.131894'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Automating the generation of front-end code from UI designs can significantly accelerate the development and iteration of applications. UI components in these designs are often composed of fragmented layers, resulting in the generation of redundant view elements in the code. This not only compromises the maintainability but also the usability of the code. Existing research primarily focuses on leveraging multimodal information from UI designs to identify fragmented layers, often overlooking the structural knowledge of the UI. This paper presents ULMGNN, a multimodal graph learning-based approach that models the relationships between UI layers and incorporates multimodal information from design prototypes to effectively address the grouping of fragmented layers. Drawing inspiration from object detection tasks, we propose a novel method that classifies UI layers and concurrently regresses their corresponding component bounding boxes, enabling an end-to-end solution for fragmented layer grouping. Experimental results on two real-world datasets demonstrate that our model outperforms the current state-of-the-art baselines. User studies further validate the effectiveness of our approach in practical software engineering applications. Our end-to-end method for grouping fragmented layers offers a solution for enhancing UI-related software engineering tasks.},
  archive      = {J_NEUCOM},
  author       = {Yunnong Chen and Shuhong Xiao and Jiazhi Li and Tingting Zhou and Lingyun Sun and Liuqing Chen},
  doi          = {10.1016/j.neucom.2025.131894},
  journal      = {Neurocomputing},
  month        = {1},
  pages        = {131894},
  shortjournal = {Neurocomputing},
  title        = {ULMGNN: Fragmented layer grouping in GUI designs through graph learning based on multimodal information},
  volume       = {660},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Endogenous event-related analysis reveals dynamic brain network reorganization abnormalities in depression. <em>NEUCOM</em>, <em>660</em>, 131893. (<a href='https://doi.org/10.1016/j.neucom.2025.131893'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Current research on depression using electroencephalogram (EEG) microstates primarily focuses on static features of microstate sequences, such as duration and occurrence rate, which overlook dynamic transition processes reflecting rapid brain network reorganization. Additionally, traditional EEG acquisition paradigms relying on precise external events require patients to sustain attention to task events, resulting in excessive cognitive load and limiting their applicability in clinical diagnosis and intervention for depression. To address these dual challenges of insufficient capture of dynamic EEG features and limited clinical applicability, this study proposes an endogenous event-related analysis algorithm for the first time. This algorithm uses spontaneously occurring microstate transition events as endogenous markers, enabling dynamic EEG event-related analysis without requiring external event triggers. The method was validated on two independent public datasets (total sample size: 282). Results demonstrate that EEG microstate features exhibit consistency across datasets in both temporal and spatial dimensions, and the superimposed temporal signals associated with microstate transition events show significant cross-dataset correlations. Furthermore, analysis of depressive disorders indicates that patients with depression exhibit significant abnormal event-related synchronization or desynchronization in key brain regions (such as the occipital lobe, parietal lobe, temporal pole, supplementary motor area, limbic lobe) before and after specific microstate transition events compared to healthy subjects. Overall, the proposed endogenous event analysis algorithm and its revelation of transition-specific neural activity abnormalities in depression provide a methodological basis for developing closed-loop neurofeedback interventions targeting specific brain network state transitions, and establishes a new theoretical pathway for studying intrinsic brain dynamics.},
  archive      = {J_NEUCOM},
  author       = {Kunbo Cui and Yue Du and Lixin Zhang and Zhongqing Wu and Hua Jiang and Fuze Tian and Mingqi Zhao and Qinglin Zhao and Bin Hu},
  doi          = {10.1016/j.neucom.2025.131893},
  journal      = {Neurocomputing},
  month        = {1},
  pages        = {131893},
  shortjournal = {Neurocomputing},
  title        = {Endogenous event-related analysis reveals dynamic brain network reorganization abnormalities in depression},
  volume       = {660},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). A bio-inspired hardware implementation of an analog spike-based hippocampus memory model. <em>NEUCOM</em>, <em>660</em>, 131892. (<a href='https://doi.org/10.1016/j.neucom.2025.131892'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The need for processing at the edge of the increasing amount of data that is being produced by multitudes of sensors has led to the demand for more power-efficient computational systems, by exploring alternative computing paradigms and technologies. Neuromorphic engineering is a promising approach that can address this need by developing electronic systems that faithfully emulate the computational properties of animal brains. In particular, the hippocampus stands out as one of the most relevant brain regions for implementing auto associative memories capable of learning large amounts of information quickly and recalling it efficiently. In this work, we present a computational spike-based memory model inspired by the hippocampus that takes advantage of the features of analog electronic circuits: energy efficiency, compactness, and real-time operation. This model can learn memories, recall them from a partial fragment and forget. It has been implemented as a Spiking Neural Networks directly on a mixed-signal neuromorphic chip. We describe the details of the hardware implementation and demonstrate its operation via a series of benchmark experiments, showing how this research prototype paves the way for the development of future robust and low-power mixed-signal neuromorphic processing systems.},
  archive      = {J_NEUCOM},
  author       = {Daniel Casanueva-Morato and Alvaro Ayuso-Martinez and Giacomo Indiveri and Juan P. Dominguez-Morales and Gabriel Jimenez-Moreno},
  doi          = {10.1016/j.neucom.2025.131892},
  journal      = {Neurocomputing},
  month        = {1},
  pages        = {131892},
  shortjournal = {Neurocomputing},
  title        = {A bio-inspired hardware implementation of an analog spike-based hippocampus memory model},
  volume       = {660},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). HG-former: Utilizing multi-scale transformer links prediction based on hypergraph structure. <em>NEUCOM</em>, <em>660</em>, 131891. (<a href='https://doi.org/10.1016/j.neucom.2025.131891'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Continuous-time dynamic graphs contain rich node structure and temporal information, which can significantly improve the accuracy of link prediction. However, we argue that existing node feature construction methods in dynamic graph link prediction fail to capture the rich spatiotemporal structural information of nodes. To address this issue, we propose a novel dynamic graph link prediction model, namely HG-Former. This model incorporates the concept of H yper G raphs in dynamic graphs, and our hypergraph construction, which is different from that in discrete dynamic graphs, can better capture the high-order temporal dependencies between nodes. We also introduce a new multi-scale trans former , which dynamically adjusts according to interaction frequencies to enable the fusion learning of sequences at different scales. This method provides multi-granularity concatenated information for subsequent node connection prediction, thereby further enhancing the predictive capability of the model. Extensive experiments on seven real-world continuous-time dynamic graph datasets demonstrate that HG-Former significantly outperforms baseline models in both inductive and transductive settings. In addition, experiments show that the hypergraph structures extracted from models such as TGAT, CAWN, TCL, and GraphMixer can effectively improve the link prediction accuracy of these baseline models.},
  archive      = {J_NEUCOM},
  author       = {Ruxin Xue and Jinggui Huang and Zaitang Huang},
  doi          = {10.1016/j.neucom.2025.131891},
  journal      = {Neurocomputing},
  month        = {1},
  pages        = {131891},
  shortjournal = {Neurocomputing},
  title        = {HG-former: Utilizing multi-scale transformer links prediction based on hypergraph structure},
  volume       = {660},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). An initialization-free distributed prescribed-time optimization algorithm based on multiagent systems for solving economic dispatch problem. <em>NEUCOM</em>, <em>660</em>, 131890. (<a href='https://doi.org/10.1016/j.neucom.2025.131890'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, the economic dispatch problem (EDP) is investigated, aiming at scheduling a cluster of generators to fulfill the supply–demand balance and capability limitations at the minimized cost, where the generators in a system communicate over undirected and connected communication networks. For solving the EDP, a distributed initialization-free prescribed-time optimization approach is designed. On account of Lyapunov stability analysis and convex optimization theory, the algorithm proposed herein is proved scrupulously to converge to the optimal solutions of EDP within a prescribed time. Distinct from both the distributed finite-time optimization approaches (where the settling time heavily depends on incipient conditions) and the distributed fixed-time optimization approaches (where the settling-time cannot be arbitrarily pre-specified), the designed method herein has prescribed-time convergence and optimality. Moreover, an upper bound of the settling time function of the dynamic system is independent of controller gains and incipient conditions, implying that the convergence time of the presented algorithm can be preset under any allowable range physically. Lastly, three examples are put forward to validate the efficiency of theoretical results.},
  archive      = {J_NEUCOM},
  author       = {Yanling Zheng and Jie Zhong and Yaguan Qian and Qingshan Liu},
  doi          = {10.1016/j.neucom.2025.131890},
  journal      = {Neurocomputing},
  month        = {1},
  pages        = {131890},
  shortjournal = {Neurocomputing},
  title        = {An initialization-free distributed prescribed-time optimization algorithm based on multiagent systems for solving economic dispatch problem},
  volume       = {660},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Adversarially robust fourier-aware multimodal medical image fusion for LSCI. <em>NEUCOM</em>, <em>660</em>, 131889. (<a href='https://doi.org/10.1016/j.neucom.2025.131889'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Selecting representative frames from Laser Speckle Contrast Imaging (LSCI) sequences is crucial for accurate blood flow analysis, yet it remains highly subjective due to image instability caused by noise, motion artifacts, and illumination inconsistency. While CT/MRI and PET/MRI fusion offer complementary insights, existing methods tend to prioritize intensity information and neglect cross-modality frequency discrepancies. Due to their low intensity and high-frequency edges, blood flow regions are often diluted in fused images, despite being clinically critical. This motivates our design of an adversarially robust Fourier-aware fusion framework for enhancing blood flow saliency in multimodal medical images, supporting reliable LSCI frame selection. Specifically, a Defensive Refinement Module (DRM) introduces adversarial perturbations to improve the model’s robustness against LSCI-specific degradations. With the robust fusion, we further adopt a Fourier Enhancement Module (FEM) that selectively amplifies flow-relevant details by coupling spatial structure awareness with frequency-based refinement. A Fourier Consistency Loss guides the model to jointly constrain amplitude and phase consistency to preserve modality-specific spectral features. Expert blind evaluations demonstrate that our method notably improves the representativeness and selection efficiency of LSCI frames in practical biomedical workflows. The code is available at https://github.com/JZD151/ARFFusion .},
  archive      = {J_NEUCOM},
  author       = {Liang Zhou and Zhidong Jiao and Yuchun He and Xingyue Zhu and Weipeng Sun and Xingyuan Li and Yang Zou},
  doi          = {10.1016/j.neucom.2025.131889},
  journal      = {Neurocomputing},
  month        = {1},
  pages        = {131889},
  shortjournal = {Neurocomputing},
  title        = {Adversarially robust fourier-aware multimodal medical image fusion for LSCI},
  volume       = {660},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Discrepancy-aware contrastive learning with mixture of experts for cross-modal image-text semantic alignment. <em>NEUCOM</em>, <em>660</em>, 131887. (<a href='https://doi.org/10.1016/j.neucom.2025.131887'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the rapid growth of multimodal data, cross-modal image-text retrieval has become essential for enabling machines to align and understand heterogeneous visual and textual information. Bridging the semantic gap between these modalities is critical for tasks such as image captioning, visual question answering, and retrieval. However, current approaches often suffer from incompatible feature representations, inefficient fusion mechanisms, and poor adaptability to diverse input patterns. This paper proposes DA-MoE, a Discrepancy-Aware Mixture of Experts framework designed for accurate cross-modal semantic alignment. DA-MoE adopts a dual-stream mixture-of-experts architecture, integrating powerful pretrained vision encoders and language models through learnable projection layers. SigLIP encoders are introduced as benchmark experts to guide others and mitigate semantic bias in cross-modal transfer learning. To adaptively fuse multimodal features, DA-MoE employs a cosine similarity-based dynamic routing strategy with temperature scaling and Top-k expert selection. Additionally, a difference-aware contrastive learning strategy is proposed, which adjusts training emphasis based on sample difficulty to improve discrimination of hard examples. Extensive experiments on the Flickr30K dataset demonstrate that DA-MoE achieves Recall@10 of 99.4 % (I→T) and 95.8 % (T→I), surpassing state-of-the-art baselines by over 2 %. Analysis confirms that dynamic expert selection significantly outperforms static fusion, especially in complex and diverse image-text scenarios. This work introduces a new direction for handling modality heterogeneity in cross-modal retrieval.},
  archive      = {J_NEUCOM},
  author       = {Xin Li and Xin He and Gaigai Tang and Kaiyuan Qi and Guangfeng Su and Huiyun Zhang},
  doi          = {10.1016/j.neucom.2025.131887},
  journal      = {Neurocomputing},
  month        = {1},
  pages        = {131887},
  shortjournal = {Neurocomputing},
  title        = {Discrepancy-aware contrastive learning with mixture of experts for cross-modal image-text semantic alignment},
  volume       = {660},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Worst-case convergence analysis of relatively inexact gradient descent on smooth convex functions. <em>NEUCOM</em>, <em>660</em>, 131883. (<a href='https://doi.org/10.1016/j.neucom.2025.131883'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider the classical gradient descent algorithm with constant stepsizes, where some error is introduced in the computation of each gradient. More specifically, we assume some relative bound on the inexactness in the sense that the norm of the difference between the true gradient and its approximate value is bounded by a certain fraction of the gradient norm. This paper presents a worst-case convergence analysis of this so-called relatively inexact gradient descent on smooth convex functions, using the Performance Estimation Problem (PEP) framework. We first derive the exact worst-case behavior of the method after one step. Then we study the case of several steps and provide computable upper and lower bounds using the PEP framework. Finally, we discuss the optimal choice of constant stepsize according to the obtained worst-case convergence rates.},
  archive      = {J_NEUCOM},
  author       = {Pierre Vernimmen and François Glineur},
  doi          = {10.1016/j.neucom.2025.131883},
  journal      = {Neurocomputing},
  month        = {1},
  pages        = {131883},
  shortjournal = {Neurocomputing},
  title        = {Worst-case convergence analysis of relatively inexact gradient descent on smooth convex functions},
  volume       = {660},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Improving on early exaggeration in t-SNE: Early hierarchization better preserves global structure. <em>NEUCOM</em>, <em>660</em>, 131882. (<a href='https://doi.org/10.1016/j.neucom.2025.131882'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In dimensionality reduction, t -SNE is a local method of neighbor embedding that requires to be carefully initialized in order to preserve the global structure of data to a good extent. In standard t -SNE, the low-dimensional embedding is initialized either randomly or with PCA. Next, gradient descent runs for two successive phases to refine the embedding coordinates iteratively. In the first phase, named early exaggeration , the attractive forces between points are artificially strengthened to prevent the repulsive forces to scatter fragments of the still poorly organized embedding, before the second phase takes over with the genuine gradient, until final convergence. A novel initialization of t -SNE is proposed in this extended work. It proceeds by hierarchizing the data points into a space-partitioning binary tree that yields faithful subsamples of data with 4 , 8 , 16 , …, 2 ⌊ log 2 ⁡ N ⌋ , N points; t -SNE runs on these growing subsamples, each obtained embedding initializing the next run. Between two runs, the prototypical point in each tree branch is split into its two children and the embedding is rescaled to account for the increased population. Extended experimental results with 5 repetitions show quantitatively the effectiveness of the method on a variety of artificial and real data sets, while running times get only multiplied by a small constant factor, leaving the computational complexity unchanged. This confirms that early hierarchization can advantageously replace initialization and early exaggeration, making t -SNE a more homogeneous method with fewer meta-parameters. The proposed method is compatible with any method of neighbor embedding ( t -SNE, UMAP, etc.) with quadratic, log-linear, or even linear iterations, provided early exaggeration can be disabled and initial coordinates of the embedded data points can be specified.},
  archive      = {J_NEUCOM},
  author       = {John A. Lee and Edouard Couplet and Pierre H. Lambert and Pierre Merveille and Ludovic Journaux and Dounia Mulders and Cyril de Bodt and Michel Verleysen},
  doi          = {10.1016/j.neucom.2025.131882},
  journal      = {Neurocomputing},
  month        = {1},
  pages        = {131882},
  shortjournal = {Neurocomputing},
  title        = {Improving on early exaggeration in t-SNE: Early hierarchization better preserves global structure},
  volume       = {660},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). A survey of deep learning based knowledge tracing from cognitive processing perspective. <em>NEUCOM</em>, <em>660</em>, 131879. (<a href='https://doi.org/10.1016/j.neucom.2025.131879'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Knowledge tracing (KT) has emerged as a prominent research area in intelligent education. Its primary aim is to assess students’ knowledge mastery based on their historical interactions, ultimately predicting future performance. With the rapid advancement of deep neural networks, deep learning technologies have become integral to this domain, giving rise to Deep Learning-based Knowledge Tracing (DLKT), which is adopted by a majority of scholars. However, existing DLKT surveys mainly focus on technical advancements, ignoring educational principles. This absence of a comprehensive review of DLKT methods from a cognitive processing perspective may lead to deviations from the memory principles inherent in learning process. Therefore, this paper endeavors to provide an extensive overview of recent representative DLKT methods from a cognitive processing perspective. In pursuit of a more comprehensive understanding of the KT task, we extract three key functions and propose a unified model adaptable to most DLKT methods. Additionally, we present a general DLKT framework comprising four essential modules: educational features, input representation, architectures for learning process modeling, and output representation. Furthermore, we propose a novel classification that incorporates both technical and cognitive processing perspectives, followed by an extensive analysis of all representative DLKT methods. Finally, we review commonly employed datasets, discuss experimental results, and explore the application of visualization, interpretability, and personalized interventions.},
  archive      = {J_NEUCOM},
  author       = {Huali Yang and Junjie Hu and Shengze Hu and Zhuoran Xu and Xinjia Ou and Jing Geng and Linxia Tang and Tao Huang},
  doi          = {10.1016/j.neucom.2025.131879},
  journal      = {Neurocomputing},
  month        = {1},
  pages        = {131879},
  shortjournal = {Neurocomputing},
  title        = {A survey of deep learning based knowledge tracing from cognitive processing perspective},
  volume       = {660},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). A concept-cognitive learning approach from the perspective of causal reasoning. <em>NEUCOM</em>, <em>660</em>, 131876. (<a href='https://doi.org/10.1016/j.neucom.2025.131876'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Concept-cognitive learning (CCL) aims to simulate the formation and evolution of human conceptual understanding. However, existing CCL models often ignore causal relationships and assign equal importance to all attributes, limiting their ability to capture critical information. To address this, we propose a novel method named causal-POFSA, which integrates causal reasoning and attribute weighting into the CCL framework. Specifically: (1) We introduce a causal reasoning mechanism to identify the causal relationships between attributes and decision classes; (2) We construct a causal concept space using a partial order structure and causal power computation; (3) We generate a weighted pseudo-concept space via similarity-driven clustering and perform concept prediction based on a weighted distance measure. Experiments on nine public datasets demonstrate that the proposed method effectively captures causal relationships and constructs meaningful concept representations, and the results further indicate that causal-POFSA consistently achieves higher accuracy and balanced performance across multiple evaluation metrics, thereby confirming both its robustness and interpretability.},
  archive      = {J_NEUCOM},
  author       = {Ningqing Xie and Enliang Yan and Xinyu Yao and Qiliang Chen and Tianyong Hao},
  doi          = {10.1016/j.neucom.2025.131876},
  journal      = {Neurocomputing},
  month        = {1},
  pages        = {131876},
  shortjournal = {Neurocomputing},
  title        = {A concept-cognitive learning approach from the perspective of causal reasoning},
  volume       = {660},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). From discrete to continuous: A spatiotemporal evolution-aware adversarial diffusion framework for retinal disease progression prediction. <em>NEUCOM</em>, <em>660</em>, 131873. (<a href='https://doi.org/10.1016/j.neucom.2025.131873'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Diabetic macular edema (DME) is one of the leading causes of vision loss in patients with diabetes. Although anti-vascular endothelial growth factor (anti-VEGF)-injection is the most commonly used treatment, individual responses to treatment vary significantly. Therefore, accurately predicting treatment outcomes is crucial for developing personalized therapeutic strategies. Compared to classification or regression approaches, image prediction provides a more intuitive depiction of retinal structural changes. However, most existing methods fail to model the spatiotemporal continuity inherent in follow-up data and rely on discrete, single-time-point inputs, limiting both predictive performance and clinical interpretability. To address these limitations, we propose a novel two-stage prediction framework that integrates spatiotemporal evolution modeling with adversarial conditional diffusion generation to forecast post-treatment optical coherence tomography (OCT) images with DME. In the first stage, we innovatively adopt an implicit neural representation (INR) to learn a continuous mapping from spatiotemporal coordinates to voxel intensities, enabling the modeling of continuous retinal evolution trajectories and preliminary image prediction. In the second stage, to mitigate the oversmoothed texture issue commonly observed in INR outputs, we introduce an adversarial conditional diffusion model to enhance image detail. Specifically, we design a dual-stream alternating guidance mechanism that integrates INR predictions with features from the original follow-up images to jointly guide the generation process. Additionally, we propose a dual-supervision adversarial module that combines noise-prediction loss with a discriminative image-level constraint, thereby producing anatomically plausible and visually realistic predictions. Extensive experiments on the DME and neovascular age-related macular degeneration (nAMD) prediction tasks demonstrate that our method achieves continuous, high-fidelity OCT prediction, offering a promising tool to support individualized treatment planning for retinal disease. Our code is accessible at: https://github.com/bemyself96/STE-ACD .},
  archive      = {J_NEUCOM},
  author       = {Xiaohui Li and Yuhan Zhang and Sijie Niu and Songtao Yuan and Qiang Chen},
  doi          = {10.1016/j.neucom.2025.131873},
  journal      = {Neurocomputing},
  month        = {1},
  pages        = {131873},
  shortjournal = {Neurocomputing},
  title        = {From discrete to continuous: A spatiotemporal evolution-aware adversarial diffusion framework for retinal disease progression prediction},
  volume       = {660},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Preference modeling with multi-graph graph attention network. <em>NEUCOM</em>, <em>660</em>, 131872. (<a href='https://doi.org/10.1016/j.neucom.2025.131872'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Accurately predicting user preferences requires leveraging complex relational data from multiple sources (e.g., user-item interactions, social networks, item similarities). While Graph Neural Networks (GNNs) excel at learning from graph-structured data, existing models often fail to fully integrate and differentiate the influence of these diverse network graphs, limiting prediction accuracy. To address this, we propose a Multi-Graph Graph Attention Network (MG-GAT) specifically for preference learning. MG-GAT’s key contribution lies in its ability to effectively integrate information from multiple network graphs with a two-level attention mechanism that enables nuanced and adaptive integration of multiple graph information. At the first level, intra-network attention identifies the most informative neighbors within each graph. Then the inter-network attention dynamically weighs the contribution of each graph view to the final representation. Unlike prior multi-view GNNs, which assume shared nodes or treat all relations equally, MG-GAT preserves the source clarity of each graph and avoids mixing different views indiscriminately. This hierarchical design allows MG-GAT to preserve structural diversity and contextually prioritize relevant information, leading to more expressive embeddings of users and items. Furthermore, the inherent ability to leverage network-based relationships also provides an auxiliary benefit in mitigating cold-start scenarios for new users or items. Extensive experiments on three real-world datasets demonstrate that MG-GAT substantially outperforms state-of-the-art baselines, validating the effectiveness of multi-graph integration and two-level attention modeling.},
  archive      = {J_NEUCOM},
  author       = {Yipeng Zhuang and Chenlu Wang and Philip L.H. Yu},
  doi          = {10.1016/j.neucom.2025.131872},
  journal      = {Neurocomputing},
  month        = {1},
  pages        = {131872},
  shortjournal = {Neurocomputing},
  title        = {Preference modeling with multi-graph graph attention network},
  volume       = {660},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). DPBPRMF: A rigorous differential privacy scheme with bayesian personalized ranking for implicit recommendation. <em>NEUCOM</em>, <em>660</em>, 131871. (<a href='https://doi.org/10.1016/j.neucom.2025.131871'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The large-scale and sensitive data collection and processing in recommender systems (RSs) cause privacy issues to take a prior stage. Approaches have employed differential privacy (DP) in matrix factorization (MF) models, which play a core role in privacy-protected RSs. However, these existing methods are not well-suited for implicit feedback data. Moreover, quantitatively analyzing the sensitivity of DP under the implicit context is challenging, and the difficulty in determining the upper bound may lead to waste of the privacy budget. Thus, we propose DPBPRMF, a rigorous and implicit differential privacy MF recommendation algorithm. It leverages the Bayesian personalized ranking (BPR) and sampling techniques to enhance the efficiency of implicit data utilization. To our knowledge, the privacy loss calculation is the first meticulous analysis of the BPR-MF framework. We further present a gradient perturbation strategy to achieve ϵ -DP, meeting users’ high privacy demands. Finally, theoretical arguments and experimental results demonstrate that the proposed method provides strong privacy protection while maintaining recommendation quality.},
  archive      = {J_NEUCOM},
  author       = {Yongdong Wang and Zhiqiang Zhang and Chenhong Luo and Jiangzhou Deng and Jianmei Ye and Yong Wang and Kobiljon Kh. Khushvakhtsoda},
  doi          = {10.1016/j.neucom.2025.131871},
  journal      = {Neurocomputing},
  month        = {1},
  pages        = {131871},
  shortjournal = {Neurocomputing},
  title        = {DPBPRMF: A rigorous differential privacy scheme with bayesian personalized ranking for implicit recommendation},
  volume       = {660},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Efficient network compression via gradient-score aware pruning. <em>NEUCOM</em>, <em>660</em>, 131870. (<a href='https://doi.org/10.1016/j.neucom.2025.131870'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Convolutional neural networks (CNNs) have demonstrated significant achievements in the field of computer vision, yet their high computational demands restrict practical applications. Current pruning methods seek to mitigate this issue, which however often rely on heuristic manual approaches, encountering challenges in maintaining both significant model compression and accuracy. To address the above issues, a fast neural architecture search pruning (FNP) technique is proposed in this paper. Firstly, an importance matrix (IM) based preprocessing stage efficiently removes redundant structures by considering both weight importance and computational complexity, providing a compact baseline for subsequent pruning. Secondly, we adapt fast genetic algorithms (FGA) to identify optimally pruned model configurations. Furthermore, to accelerate the search process, we utilize a zero-shot learning approach to estimate model performance with the score of the frame (SoF), which is a gradient-based score. Compared with state-of-the-art (SOTA) pruning techniques, FNP demonstrates superior performance in terms of search duration and compression ratio. On the CIFAR-10 dataset, our method removes 95.24 % of the parameters in VGG-16 while achieving a 0.72 % accuracy improvement compared with the baseline. On the ImageNet dataset, we prune 68.98 % of the parameters in ResNet-50 and obtain a 1.2 % accuracy improvement compared with state-of-the-art (SOTA) approaches, while reducing the search time by 98.94 %. The code is available at https://github.com/aqiu1222/FNP.git},
  archive      = {J_NEUCOM},
  author       = {Qiuying Li and Zhixiang Chen and Yu Li and Zhiyuan Jiang and Shan Cao},
  doi          = {10.1016/j.neucom.2025.131870},
  journal      = {Neurocomputing},
  month        = {1},
  pages        = {131870},
  shortjournal = {Neurocomputing},
  title        = {Efficient network compression via gradient-score aware pruning},
  volume       = {660},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Invertible koopman network with external inputs: A novel data-driven modeling method for non-autonomous nonlinear systems with discrete spectrum. <em>NEUCOM</em>, <em>660</em>, 131869. (<a href='https://doi.org/10.1016/j.neucom.2025.131869'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Koopman operator theory is a popular candidate for data-driven modeling because it provides a global linearization representation of the nonlinear dynamical systems. This work develops a novel data-driven modeling method for non-autonomous systems with discrete spectra, denoted as the Invertible Koopman Network with external inputs. The Invertible Neural Network is utilized to obtain the parameterized observable function, which does not introduce additional assumptions about the observable function’s form and can explicitly provide the corresponding inverse without relying on the reconstruction loss. Additionally, the input lifting function is also realized by another Invertible Neural Network rather than assuming it to be an identity. The original system’s states and external inputs are both encoded, enabling the proposed method to better model the nonlinear state-dependent term in general nonlinear systems. Two learnable linear matrices are utilized to approximate the Koopman operator, resulting in linear dynamics in the embedding space. Based on this, linearization control schemes for the nonlinear dynamical systems are further developed. Various numerical examples demonstrate the effectiveness and superiority of our proposed approach. Finally, we examine and verify the proposed method’s data-driven modeling performance in a real-world nonlinear isolation system through a dynamic experiment.},
  archive      = {J_NEUCOM},
  author       = {Yuhong Jin and Lei Hou and Haiming Yi and Shun Zhong and Jun Li and Shijun Wang and Hao Chen},
  doi          = {10.1016/j.neucom.2025.131869},
  journal      = {Neurocomputing},
  month        = {1},
  pages        = {131869},
  shortjournal = {Neurocomputing},
  title        = {Invertible koopman network with external inputs: A novel data-driven modeling method for non-autonomous nonlinear systems with discrete spectrum},
  volume       = {660},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Synchronization mechanisms in coupled neural networks with time-varying topologies: From edge-level rewiring to network-wide collective behaviors. <em>NEUCOM</em>, <em>660</em>, 131868. (<a href='https://doi.org/10.1016/j.neucom.2025.131868'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This study investigates the synchronization issue for coupled neural networks with time-varying communication topologies, with a particular focus on establishing the cross-scale relationship between microscopic-scale edge rewiring dynamics and macroscopic network synchronization behaviors. Firstly, some characteristic parameters are introduced to quantify the dynamic rewiring behaviors of each edge within the network. Leveraging these parameters and employing the techniques such as Jensen’s inequality and Barbalat’s lemma, some synchronization conditions related to the dynamic rewiring characteristics of edges are established. Based on these conditions, the impact of edge rewiring dynamics on network synchronization is analyzed. Our analysis reveals that the synchronization becomes more likely when: (1) the time required for the switching network topologies to achieve joint connectivity is shorter, and (2) the duration each edge exists within a joint connectivity period is longer. Notably, the derived synchronization conditions impose no requirement for a positive lower bound on the communication topologies’ dwell time. In contrast to the classical average dwell-time approach, these conditions impose fewer constraints on topology switching frequency and exhibit wider applicability. Furthermore, these synchronization conditions impose weak connectivity requirements on the switching topologies, necessitating only joint connectivity. Finally, some numerical experiments confirm the validity of the theoretical results.},
  archive      = {J_NEUCOM},
  author       = {Xudong Hai and Qingyun Wang and Ting Kang and Boqiang Cao},
  doi          = {10.1016/j.neucom.2025.131868},
  journal      = {Neurocomputing},
  month        = {1},
  pages        = {131868},
  shortjournal = {Neurocomputing},
  title        = {Synchronization mechanisms in coupled neural networks with time-varying topologies: From edge-level rewiring to network-wide collective behaviors},
  volume       = {660},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). GRIFFIN: A gated relaxing interaction-aware fuzzy framework with integrated negation-aware neurocomputing to learn incomplete and non-separable rules for classification. <em>NEUCOM</em>, <em>660</em>, 131866. (<a href='https://doi.org/10.1016/j.neucom.2025.131866'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper proposes a novel gated neuro-fuzzy architecture that can gate only relevant input variables to form non-separable and incomplete fuzzy rules, considering nonlinear interactions among them, while applying relaxation to the effect of redundant features in defining rules (GRIFFIN). GRIFFIN learns to select important input variables to extract the feature space required for defining each fuzzy rule through an adaptive gating mechanism. Next, it learns to map the selected input subspace to a new feature embedding for each fuzzy rule to consider the interactions among the selected input variables related to the covered region of the fuzzy rule. Then, it decides whether the positive or negative literal for each linguistic value should be considered. Afterward, it learns to cancel the effect of irrelevant linguistic terms in the embedding space to form incomplete fuzzy rules. Consequently, the fuzzy rules are formed by taking into account the interactions among input variables while they have various numbers of positive or negative predicates in their antecedent parts. Thus, the formed fuzzy rules would be incomplete, non-separable, and in the conjunctive normal form, which makes the inference more general. The consequent parts utilize the selected features that are extracted for each fuzzy rule. This leads to providing skip connections that improve learning through error backpropagation. A novel unsupervised data-driven interaction-aware initialization method called Fuzzy Explanation Maximization (FExMax) is proposed based on maximizing the rules’ coverage. Afterward, the rules’ parameters are fine-tuned by applying error backpropagation. The performance of GRIFFIN is studied in real-world classification applications which confirm its capability to achieve the highest performance with the lowest number of rules. GRIFFIN outperforms previous SOTA with 7.9 % improvement in terms of Accuracy.},
  archive      = {J_NEUCOM},
  author       = {Armin Salimi-Badr and Aras Valizadeh and Mohammad Mahdi Parchamijalal},
  doi          = {10.1016/j.neucom.2025.131866},
  journal      = {Neurocomputing},
  month        = {1},
  pages        = {131866},
  shortjournal = {Neurocomputing},
  title        = {GRIFFIN: A gated relaxing interaction-aware fuzzy framework with integrated negation-aware neurocomputing to learn incomplete and non-separable rules for classification},
  volume       = {660},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). YCSC-UNet: A Y-shaped composite spatial channel network based on U-net for breast lesion ultrasound image segmentation. <em>NEUCOM</em>, <em>660</em>, 131865. (<a href='https://doi.org/10.1016/j.neucom.2025.131865'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Since the first global assessment of cancer burden in the 1980s, breast cancer has remained the most prevalent cancer among women. The accurate segmentation of breast tumors on ultrasound images is of significant importance for early detection of breast cancer. However, current approaches still face challenges such as contextual information loss and inadequate feature capture capabilities. To address these issues, this work proposes a Y-shaped composite spatial channel network based on U-Net (YCSC-UNet). We have four contributions: 1) Y-shaped structural branches that continuously replenish contextual information; 2) Efficient Encoding Component (EEC) to deliver effective information with minimal parameter overhead; 3) Dual Long-Range Channel Attention (DLRCA), a composite channel attention mechanism for refined feature fusion; 4) Long-Range Spatial Attention (LRSA), integrating spatial attention to comprehensively extract long-range dependencies. Extensive experiments on two public datasets demonstrate that YCSC-UNet achieves superior performance on metrics including Dice and Jaccard coefficients, outperforming multiple state-of-the-art methods. The code is available at: https://github.com/wwiinndd/YCSC-UNet .},
  archive      = {J_NEUCOM},
  author       = {Wenjun Ma and Qian Jiang and Qianqian Wang and Dongjian Yu and Yilong Huang and Bo He and Xin Jin},
  doi          = {10.1016/j.neucom.2025.131865},
  journal      = {Neurocomputing},
  month        = {1},
  pages        = {131865},
  shortjournal = {Neurocomputing},
  title        = {YCSC-UNet: A Y-shaped composite spatial channel network based on U-net for breast lesion ultrasound image segmentation},
  volume       = {660},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). LUTFormer: Lookup table transformer for image enhancement. <em>NEUCOM</em>, <em>660</em>, 131863. (<a href='https://doi.org/10.1016/j.neucom.2025.131863'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Existing image enhancement methods based on 3D lookup tables (LUTs) often yield suboptimal results by oversimplifying image context into a single global feature and disrupting the inherent geometric structure of a LUT during regression. To address these issues, we propose LUTFormer, a novel framework that reframes LUT prediction as a query-based refinement task. LUTFormer preserves geometric integrity by initializing LUT grid points as structured query tokens, which are then progressively refined by a transformer decoder. This decoder leverages a novel progressive cross-attention mechanism to inject multi-level image context, yielding a context-aware LUT transformation. Extensive experiments on multiple benchmark datasets confirm the effectiveness and efficiency of the proposed LUTFormer. The source code is available at https://github.com/Jinwon-Ko/LUTFormer .},
  archive      = {J_NEUCOM},
  author       = {Jinwon Ko and Keunsoo Ko and Hanul Kim and Chang-Su Kim},
  doi          = {10.1016/j.neucom.2025.131863},
  journal      = {Neurocomputing},
  month        = {1},
  pages        = {131863},
  shortjournal = {Neurocomputing},
  title        = {LUTFormer: Lookup table transformer for image enhancement},
  volume       = {660},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Privacy-preserving 3D human skeleton reconstruction from ankle-level 2D LiDAR using deep learning. <em>NEUCOM</em>, <em>660</em>, 131862. (<a href='https://doi.org/10.1016/j.neucom.2025.131862'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Human skeleton reconstruction enables smart monitoring and biomechanical analysis, yet RGB/RGB-D systems face privacy concerns in public spaces, illumination sensitivity, and occlusion. To overcome these limitations, we propose a privacy-preserving approach for 3D skeleton reconstruction using ankle-level 2D LiDAR-based Motion History Images (MHIs). Our method integrates a direction-guided multi-head deep learning model, leveraging ResNet18 as the backbone to extract features from the MHIs and classify walking directions (front, left, right, back), thereby improving pose estimation across diverse orientations. Experiments comparing ResNet18, ResNet50, and DenseNet121 demonstrate significant improvements in accuracy, with DenseNet121 showing the best performance in terms of joint position error, while ResNet18 offers the best trade-off between accuracy and inference speed for real-time deployment. An ablation study reveals the importance of direction classification, with the removal of this feature leading to increased errors, especially in joints such as the spine base. We further validate anatomical consistency by estimating subject-specific height from predicted skeletons, observing consistent accuracy across individuals. These results position our approach as a robust and privacy-conscious alternative to conventional methods for 3D skeleton reconstruction.},
  archive      = {J_NEUCOM},
  author       = {Md Mohibullah and Yusuke Suda and Yuhei Hironaka and Takuma Miyawaki and Ryota Suzuki and Mahmudul Hasan and Yoshinori Kobayashi},
  doi          = {10.1016/j.neucom.2025.131862},
  journal      = {Neurocomputing},
  month        = {1},
  pages        = {131862},
  shortjournal = {Neurocomputing},
  title        = {Privacy-preserving 3D human skeleton reconstruction from ankle-level 2D LiDAR using deep learning},
  volume       = {660},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Average controllability of N-duplication corona product networks. <em>NEUCOM</em>, <em>660</em>, 131860. (<a href='https://doi.org/10.1016/j.neucom.2025.131860'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The average controllability can quantify the difficulty of controlling a network based on controllability Gramian metrics in terms of energy triggered by the leaders. This work addresses the average controllability of a class of composite networks generated by factor networks via N -duplication corona product, considering the simple undirected factor networks with Laplacian dynamics within a leader-follower framework, and further provides a technique to characterize the average controllability of N -duplication corona product networks ( N -CPNs) by the spectral properties of factors, which greatly reduces the computational complexity and analytical difficulty. In addition, it is found that the ‘size’ of average controllability (i.e., trace of Gramian matrix) is dependent on N , the assignment of leaders, the number of agents and the spectral properties of factors.},
  archive      = {J_NEUCOM},
  author       = {Qiang Zhang and Junjie Huang and Bo Liu and Housheng Su and Alatancang Chen},
  doi          = {10.1016/j.neucom.2025.131860},
  journal      = {Neurocomputing},
  month        = {1},
  pages        = {131860},
  shortjournal = {Neurocomputing},
  title        = {Average controllability of N-duplication corona product networks},
  volume       = {660},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). DBTSP-net: A temporal-spatial parallel network with optuna optimization for subject-specific motor imagery EEG decoding and visualization. <em>NEUCOM</em>, <em>660</em>, 131858. (<a href='https://doi.org/10.1016/j.neucom.2025.131858'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The accuracy and stability of decoding EEG-based motor imagery (MI-EEG) is critical for achieving effective human-machine interaction and promoting motor function recovery in patients with severe motor dysfunction. In this paper, we propose a novel dual-branch temporal–spatial parallel hybrid classification network named DBTSP-Net. In addition, we introduce an adaptive weighted feature fusion method to decode MI-EEG signals on the basis of the Optuna optimization algorithm. Nine subjects were recruited to participate in the MI-EEG decoding experiment. We evaluated the classification performance of both conventional and state-of-the-art MI-EEG models using the public BCI Competition IV 2a and 2b datasets. The experimental results demonstrated that the classification performance of DBTSP-Net surpassed that of the other baseline methods, attaining average classification accuracies of 79.61 % ± 14.43 and 86.21 % ± 12.17, respectively, with corresponding kappa values of 0.7856 and 0.7189, respectively. We further conducted ablation experiments to verify the rationality of the design of each module. Additionally, EEG topological maps and t-distributed stochastic neighbor embedding (t-SNE) were utilized for feature visualization. The decoding accuracy of MI-EEG signals was increased, and a solid theoretical foundation for the future practical application of MI-BCI systems in motion control and neural rehabilitation training was obtained. The code has been released at https://github.com/xinchenPhD/DBTSPNet .},
  archive      = {J_NEUCOM},
  author       = {Xin Chen and Longjie Yu and Hongze Lin and Mingyu Du and Wei Wei and Xinyu Wu and Guanjun Bao and Shibo Cai},
  doi          = {10.1016/j.neucom.2025.131858},
  journal      = {Neurocomputing},
  month        = {1},
  pages        = {131858},
  shortjournal = {Neurocomputing},
  title        = {DBTSP-net: A temporal-spatial parallel network with optuna optimization for subject-specific motor imagery EEG decoding and visualization},
  volume       = {660},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Adaptive RBF neural network-based boundary consensus control for nonlinear reaction–diffusion agents with non-collocated measurements. <em>NEUCOM</em>, <em>660</em>, 131854. (<a href='https://doi.org/10.1016/j.neucom.2025.131854'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article proposes a constrained boundary controller to achieve practical consensus for nonlinear reaction–diffusion agents in the presence of unknown uncertainties and external disturbances. The proposed non-collocated observer, in contrast to the conventional assumption based on the Lipschitz condition, incorporates an adaptive radial basis function (RBF) neural network law to accurately estimate the nonlinear dynamics in the spatiotemporal domain. By enhancing the Lyapunov direct approach, a linear matrix inequality (LMI)-based sufficient condition is derived to ensure the convergence of consensus errors, even in the presence of unknown terms. Furthermore, an invariant set is presented by constructing the relationship between the boundary input and Lyapunov function to address the problem of actuator saturation. The comparison results ultimately demonstrate the effectiveness and merits of the proposed methodology.},
  archive      = {J_NEUCOM},
  author       = {Jiaqi Zhong and Yan Feng and Huai-Ning Wu},
  doi          = {10.1016/j.neucom.2025.131854},
  journal      = {Neurocomputing},
  month        = {1},
  pages        = {131854},
  shortjournal = {Neurocomputing},
  title        = {Adaptive RBF neural network-based boundary consensus control for nonlinear reaction–diffusion agents with non-collocated measurements},
  volume       = {660},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Dual-branch normalizing flow for anomaly detection and localization from images. <em>NEUCOM</em>, <em>660</em>, 131850. (<a href='https://doi.org/10.1016/j.neucom.2025.131850'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In visual anomaly detection, unexpected or abnormal patterns are identified from given image samples. Existing methods for visual anomaly detection are mainly divided into two types: semantic inter-class (i.e., image-level) anomaly detection and texture intra-class anomaly detection. The normalizing flow-based method can effectively map normal training data to a Gaussian distribution due to its excellent distribution expression ability, thus interpretably identifying lower likelihood abnormal data from normal data. However, with normalizing flow based methods, it is challenging to achieve high precision detection and localization of anomalies due to the unpredictable scale of the anomalies within texture anomalies. The large scale span of potential anomalous textures makes it difficult to balance the learning of distributions from both global and local features in existing normalizing flow methods. To address this limitation, we propose a dual-branch architecture to model the density mapping of global and local features, respectively. Our proposed model can achieve coarse-grained and fine-grained image anomaly detection and localization, by modeling both the global features and local texture attributes of the input images with a dual branch normalizing flow. Furthermore, we design a Dynamic Spatial Attention Module (DSAM) in each branch of the flow module to enhance the model’s ability to capture anomaly features. Extensive experiments on two public datasets have demonstrated that our model is effective in detecting various real-world sample defects, especially in unsupervised visual anomaly detection tasks, achieving substantially promising results. The codes are available at https://github.com/SYLan2019/DNFAD .},
  archive      = {J_NEUCOM},
  author       = {Yao Li and Wei Ma and Shuai He and Shiyong Lan and Wenwu Wang and Yixin Qiao and Guonan Deng},
  doi          = {10.1016/j.neucom.2025.131850},
  journal      = {Neurocomputing},
  month        = {1},
  pages        = {131850},
  shortjournal = {Neurocomputing},
  title        = {Dual-branch normalizing flow for anomaly detection and localization from images},
  volume       = {660},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). FA-seg: A fast and accurate diffusion-based method for open-vocabulary segmentation. <em>NEUCOM</em>, <em>660</em>, 131844. (<a href='https://doi.org/10.1016/j.neucom.2025.131844'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Open-vocabulary semantic segmentation (OVSS) aims to segment objects from arbitrary text categories without requiring densely annotated datasets. Although contrastive learning based models enable zero-shot segmentation, they often lose fine spatial precision at the pixel level, due to global representation bias. In contrast, diffusion-based models naturally encode fine-grained spatial features via attention mechanisms that capture both global context and local details. However, they often face challenges in balancing computational costs and the quality of the segmentation mask. In this work, we present FA-Seg, a F ast and A ccurate training-free framework for open-vocabulary segmentation based on diffusion models. FA-Seg performs segmentation using only a (1+1)-step from a pretrained diffusion model. Moreover, instead of running multiple times for different classes, FA-Seg performs segmentation for all classes at once. To further enhance the segmentation quality, FA-Seg introduces three key components: (i) a dual-prompt mechanism for discriminative, class-aware attention extraction, (ii) a Hierarchical Attention Refinement Method (HARD) that enhances semantic precision via multi-resolution attention fusion, and (iii) a Test-Time Flipping (TTF) scheme designed to improve spatial consistency. Extensive experiments show that FA-Seg achieves state-of-the-art training-free performance, achieving 43.8 % average mIoU across PASCAL VOC, PASCAL Context, and COCO Object benchmarks while maintaining superior inference efficiency. Our results demonstrate that FA-Seg provides a strong foundation for extendability, bridging the gap between segmentation quality and inference efficiency. Code is available at https://github.com/chequanghuy/FA-Seg .},
  archive      = {J_NEUCOM},
  author       = {Huy Che and Vinh-Tiep Nguyen},
  doi          = {10.1016/j.neucom.2025.131844},
  journal      = {Neurocomputing},
  month        = {1},
  pages        = {131844},
  shortjournal = {Neurocomputing},
  title        = {FA-seg: A fast and accurate diffusion-based method for open-vocabulary segmentation},
  volume       = {660},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). StyleBoost: Controlling style-content fusion with SVD for text-driven. <em>NEUCOM</em>, <em>660</em>, 131843. (<a href='https://doi.org/10.1016/j.neucom.2025.131843'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, text-driven style transfer techniques have made significant progress in the text-to-image (T2I) generation domain, demonstrating strong potential in fine-grained style control. However, existing methods still face several challenges, including overfitting to reference image styles, difficulty in effectively disentangling style from semantic content, and insufficient alignment between generated images and text prompts. These issues limit the diversity and controllability of generation results. To address these challenges, this paper proposes two strategies to enhance style representation and text alignment: First, based on Singular Value Decomposition (SVD), we analyze the content and style components within reference image embeddings and design a style extraction method that regulates the nuclear norm of the approximated reference image embeddings. This approach effectively suppresses structural and content information in the reference image, enabling accurate extraction and preservation of style features, thereby mitigating content leakage. Second, we introduce an improved cross-attention alignment module that strengthens the fusion and interaction between text and approximated reference image embeddings, boosting the generated images’ responsiveness to textual semantics and accuracy in inheriting style. A series of comprehensive experiments show that the proposed approach notably enhances both style consistency and alignment with textual prompts, all without the need for extra training or fine-tuning. It also exhibits strong generalizability and scalability and is compatible with other control modules such as ControlNet. Our approach offers an effective solution for achieving high-quality, controllable, text-guided stylized image generation. Code is available: https://github.com/math-ddup/StyleBoost .},
  archive      = {J_NEUCOM},
  author       = {Shujie Zhang and Jingyue Wang and Meiqing Wang and Yuanxiang Fang and Huimin Liu},
  doi          = {10.1016/j.neucom.2025.131843},
  journal      = {Neurocomputing},
  month        = {1},
  pages        = {131843},
  shortjournal = {Neurocomputing},
  title        = {StyleBoost: Controlling style-content fusion with SVD for text-driven},
  volume       = {660},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). A multimodal knowledge synergistic pre-training framework with masked vision-language modeling for remote sensing. <em>NEUCOM</em>, <em>660</em>, 131841. (<a href='https://doi.org/10.1016/j.neucom.2025.131841'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep learning has been widely applied to Remote Sensing Image (RSI) analysis, such as scene classification and road segmentation, with ImageNet-based pre-trained models commonly used as backbone networks. However, the significant domain shift between RSIs and natural images can result in sub-optimal performance. To address this, Self-supervised Learning (SSL) has been explored to improve performance, but the reliance on a single image modality limits its potential performance gains. Meanwhile, large vision-language foundation models have demonstrated remarkable performance, but the training typically requires large-scale datasets and poses challenges in terms of reproducibility and network customization without substantial computational resources. This raises a fundamental question: Can we achieve knowledge-enhanced pre-training without relying on large-scale datasets? To this end, we propose a novel Multimodal Knowledge Synergistic Pre-training (MKSP) framework, which encapsulates image-language multimodal knowledge into pre-trained vision models. Our MKSP framework is built upon both Masked Image Modeling (MIM) and Masked Language Modeling (MLM), while the complementarity between the image and language domains is generated by two newly proposed loss functions: Global Contrastive Loss (GCL) and Object-aware Contrastive Loss (OCL). GCL models global information communications between image and language embeddings, while OCL injects fine-grained object-level knowledge into the pre-trained model. Driven by multimodal synergistic learning, the pre-trained model demonstrates strong transferability, even with a relatively small pre-training dataset. Specifically, with MKSP pre-training, we conduct performance evaluations on challenging RSI tasks: image classification, semantic segmentation, and object detection. Extensive experimental results showcase that MKSP can learn robust feature representations despite using minimal labeled samples.},
  archive      = {J_NEUCOM},
  author       = {Meng Lou and Yunliang Qi and Shiqiang Du and Chunbo Xu and Zhen Yang and Yulin Shen},
  doi          = {10.1016/j.neucom.2025.131841},
  journal      = {Neurocomputing},
  month        = {1},
  pages        = {131841},
  shortjournal = {Neurocomputing},
  title        = {A multimodal knowledge synergistic pre-training framework with masked vision-language modeling for remote sensing},
  volume       = {660},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Hierarchical encoder-decoder for image captioning. <em>NEUCOM</em>, <em>660</em>, 131833. (<a href='https://doi.org/10.1016/j.neucom.2025.131833'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Image captioning aims to encode the visual information from images and integrate it into text decoding to generate an accurate natural language descriptions. Existing methods often neglect the hierarchical structure of visual information, fail to model global relationships between visual elements, and have not yet explored the synergistic effect of multi-level semantics, resulting in inaccurate captions. To address these issues, we propose a Hierarchical Encoder-decoder for Image Captioning (HierCap) to guide text generation with hierarchical visual information at three levels: global (encompassing positional relationships), regional (highlighting principal objects), and grid (capturing local details). Specifically, the hierarchical encoder employs three dedicated sub-encoders to build complementary visual representations at each level. For the decoder, a hierarchical fusion module with four variants is provided to explore the cross-modal synergistic fusion between hierarchical visual features and textual features. Extensive experiments demonstrate that HierCap achieves state-of-the-art performance on four datasets: COCO, NoCaps, Flickr8k, and Flickr30k. The results validate the effectiveness of hierarchical visual encoding and cross-modal hierarchical fusion in generating accurate and semantically rich descriptions. The source code is available at https://github.com/Panlizhi/HierCap .},
  archive      = {J_NEUCOM},
  author       = {Lizhi Pan and Chengtian Song and Xiaozheng Gan and Keyu Xu and Mengqian Deng},
  doi          = {10.1016/j.neucom.2025.131833},
  journal      = {Neurocomputing},
  month        = {1},
  pages        = {131833},
  shortjournal = {Neurocomputing},
  title        = {Hierarchical encoder-decoder for image captioning},
  volume       = {660},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Equivariant feature extraction: Enhancing 3D point cloud analysis with robust rotational equivariance. <em>NEUCOM</em>, <em>660</em>, 131832. (<a href='https://doi.org/10.1016/j.neucom.2025.131832'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent advancements in 3D point cloud representation techniques have significantly facilitated research in downstream tasks such as classification and segmentation. Despite the impressive capabilities of existing methods, they often struggle with rotated data due to a lack of rotation equivariance. This paper introduces the Equivariant Feature Extractor (EFE), a feature extraction method based on equivariant representations and spherical harmonics. EFE encodes 3D position data using equivariant group representations and extracts high-quality equivariant features through a series of equivariant operators. This method can be seamlessly integrated into classic neural networks, ensuring rotation equivariance while introducing only a small number of learnable parameters. Experimental results demonstrate that integrating EFE into mainstream point cloud models achieves outstanding performance, particularly on rotated test sets. This study highlights EFE’s ability to extract equivariant features and its direct applicability to classic models, contributing significantly to improving downstream task performance and advancing point cloud feature extraction techniques.},
  archive      = {J_NEUCOM},
  author       = {Qianwei Tang and Baile Xu and Jian Zhao and Furao Shen},
  doi          = {10.1016/j.neucom.2025.131832},
  journal      = {Neurocomputing},
  month        = {1},
  pages        = {131832},
  shortjournal = {Neurocomputing},
  title        = {Equivariant feature extraction: Enhancing 3D point cloud analysis with robust rotational equivariance},
  volume       = {660},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Boosting confidence in class-incremental learning via discriminative local feature enhancement. <em>NEUCOM</em>, <em>660</em>, 131829. (<a href='https://doi.org/10.1016/j.neucom.2025.131829'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Class-incremental learning aims to enable models to continuously learn new classes without accessing samples from previously learned classes, which is a challenging scenario as it requires learning new class knowledge while preventing the forgetting of knowledge related to old classes. An effective class-incremental learning approach involves isolating task-specific parameters for different tasks to mitigate interference-induced forgetting. However, this method still faces challenges, such as the model’s difficulty in accurately focusing on discriminative features of each class and its susceptibility to interference from similar background information across different classes. To address these issues, this study proposes a novel strategy for selecting discriminative local features in images. By leveraging the background information of images, the strategy identifies and extracts key local features critical for model discrimination, directing the model to prioritize these important features during training to enhance classification confidence. Additionally, the learning of visual representations is guided by semantic textual features from a pre-trained vision–language model, further improving the model’s class-incremental learning performance. Extensive empirical evaluations under various settings on multiple image classification datasets demonstrate that our method outperforms existing approaches by at least 1 % in accuracy.},
  archive      = {J_NEUCOM},
  author       = {Weizhuo Zhang and Ping Hu and Qiu Li and Jiantao Tan and Zhijun Tan and Ruixuan Wang},
  doi          = {10.1016/j.neucom.2025.131829},
  journal      = {Neurocomputing},
  month        = {1},
  pages        = {131829},
  shortjournal = {Neurocomputing},
  title        = {Boosting confidence in class-incremental learning via discriminative local feature enhancement},
  volume       = {660},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Anchor transfer learning-based multi-view subspace clustering. <em>NEUCOM</em>, <em>660</em>, 131827. (<a href='https://doi.org/10.1016/j.neucom.2025.131827'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Anchor-based multi-view subspace clustering algorithms have attracted significant attention due to their superior computational efficiency. However, existing methods face limitations when dealing with redundant noise in the original data, discrepancies in data distributions across different views, and heterogeneity between view domains. These issues significantly compromise the quality of the anchor points. To address these challenges, this paper introduces a novel algorithm called Anchor Transfer Learning-based Multi-view Subspace Clustering (ATMVC). ATMVC utilizes a projection matrix to map the original data into a low-rank subspace. This mapping helps extract informative features while minimizing the influence of redundancy and noise. A dual-anchor strategy is adopted to efficiently capture consensus and complementary information across views, ensuring information integrity. Furthermore, inspired by transfer learning, the algorithm guides both types of anchor points to learn from each other within a unified framework. This design effectively reduces the negative impact caused by inter-view domain discrepancies. Therefore, ATMVC can overcome the limitations of existing methods by enhancing anchor quality and leveraging inter-view knowledge transfer. Comparative experiments with state-of-the-art methods demonstrate that the proposed approach offers comprehensive advantages in both performance and efficiency.},
  archive      = {J_NEUCOM},
  author       = {Lin Bai and Jingxuan Liu and Qian Xue and Mengchen Sun and Xiaoying Pan},
  doi          = {10.1016/j.neucom.2025.131827},
  journal      = {Neurocomputing},
  month        = {1},
  pages        = {131827},
  shortjournal = {Neurocomputing},
  title        = {Anchor transfer learning-based multi-view subspace clustering},
  volume       = {660},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). SR-TCUR: Scalable and robust tubal CUR decomposition for large-scale multidimensional tensors. <em>NEUCOM</em>, <em>660</em>, 131826. (<a href='https://doi.org/10.1016/j.neucom.2025.131826'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Tensor factorization techniques are valuable tools in the processing and analysis of large-scale data, such as images and videos, especially the CUR factorization methods, as they offer several advantages over other decompositions, such as interpretability and computational efficiency. Moreover, tensor decompositions based on the t-product concept have attracted much attention due to their success in various applications. This paper proposes a scalable and efficient tubal CUR decomposition method based on t-product, called SR-TCUR, that samples the significant lateral and horizontal slices based on the deterministic sampling method. SR-TCUR only requires observing and loading a few data samples into memory to construct the low-rank tensor approximation, making it scalable for large-scale data tensors. Various experiments on synthetic and real-world datasets with different dimensions and characteristics are conducted to evaluate the performance of the SR-TCUR method. The experimental results demonstrate that SR-TCUR outperforms the state-of-the-art methods of deterministic tensor CUR (TCUR) based on the t-product in computational complexity. It achieves an approximation accuracy close to the standard tensor CUR decomposition based on the tensor discrete empirical interpolation selection method, but with significant computational efficiency. Moreover, we utilize SR-TCUR to solve the tensor completion problem and recover the missing data entries.},
  archive      = {J_NEUCOM},
  author       = {Muhammad A.A. Abdelgawad and Ray C.C. Cheung and Hong Yan},
  doi          = {10.1016/j.neucom.2025.131826},
  journal      = {Neurocomputing},
  month        = {1},
  pages        = {131826},
  shortjournal = {Neurocomputing},
  title        = {SR-TCUR: Scalable and robust tubal CUR decomposition for large-scale multidimensional tensors},
  volume       = {660},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Enhancing unsupervised unified anomaly detection via semi-supervised learning with multi-source uncertainty mining. <em>NEUCOM</em>, <em>660</em>, 131824. (<a href='https://doi.org/10.1016/j.neucom.2025.131824'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Semi-Supervised Learning (SSL) strategies are widely adopted in industrial applications to improve the performance of anomaly detection models. However, most existing SSL methods focus on modeling a limited range of known anomalies, requiring prior definition and labeling. Consequently, SSL has limited applicability when encountering unseen anomalies. In contrast, unsupervised anomaly detection models do not rely on prior anomaly knowledge and effectively handle unknown anomalies. In this study, we propose a semi-supervised approach to improve unsupervised anomaly detection models by gradually enhancing anomaly detection performance using a small set of unlabeled samples. Our approach comprises two plug-and-play modules: the Multi-source Uncertainty Mining Network (MUMNet) and the Anomaly Segmentation Network (ASNet). These modules aim to mitigate bias in the results of the reconstruction-based unsupervised anomaly detection model (base model). During training, MUMNet integrates multiple cross-attention modules and an uncertainty-weighted loss to iteratively identify commonalities and discrepancies in pseudo-labels generated by diverse unsupervised models. This enables MUMNet to adaptively select reliable labels, which then supervise ASNet in learning a unified decision boundary. During testing, only the synergy between ASNet and the base model is necessary to help identify unbiased differences associated with anomalies. We conducted extensive experiments on the MVTec AD dataset, demonstrating that integrating MUM-UAD led to performance enhancements of up to 16.0 % compared to the base model.},
  archive      = {J_NEUCOM},
  author       = {Borui Kang and Yuzhong Zhong and Lin Deng and Maoning Wang and Jianwei Zhang},
  doi          = {10.1016/j.neucom.2025.131824},
  journal      = {Neurocomputing},
  month        = {1},
  pages        = {131824},
  shortjournal = {Neurocomputing},
  title        = {Enhancing unsupervised unified anomaly detection via semi-supervised learning with multi-source uncertainty mining},
  volume       = {660},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). FSCFNet: Lightweight neural networks via multi-dimensional importance-aware optimization. <em>NEUCOM</em>, <em>660</em>, 131823. (<a href='https://doi.org/10.1016/j.neucom.2025.131823'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Network lightweighting has become an effective technique for compressing CNNs by eliminating redundant structures. However, several challenges remain unresolved. For instance, some convolution optimization methods decompose convolutions into multiple segments, which reduce FLOPs but simultaneously increase memory access overhead. Similarly, ensuring that quantization and pruning techniques achieve substantial improvements in computational efficiency without compromising accuracy remains a pressing challenge. Most importantly, many methods neglect hardware adaptation, resulting in no significant performance improvement. To address these limitations, we propose a framework that integrates multi-dimensional evaluation with hardware-aware optimization. 1) We introduce a multi-dimensionally important convolution module. By selectively processing only the most informative features, this module substantially reduces both Floating Point Operations per Second (FLOPs) and memory access, while maintaining accuracy. 2) We propose a mixed-precision quantization module based on multi-dimensional importance and hardware awareness. This module assigns different precision levels (high, medium, or low) to features according to their importance and how well the hardware adapts to various computational precisions. This design markedly reduces the parameter count with only marginal accuracy degradation. 3) We introduce a channel pruning module based on channel contribution assessment. Through structured pruning of channels with minimal contribution to prediction accuracy, this module further improves computational efficiency with negligible accuracy loss. We validate the proposed framework on both GPU- and CPU-based platforms. Extensive experiments demonstrate that our method improves processing speed by 2.6 times in terms of FPS and reduces the total parameter count by 71 %, all without compromising model accuracy.},
  archive      = {J_NEUCOM},
  author       = {Mengyang Nie and Jinqiu Sun and Hongsong Guoyang and Axi Niu and Yaoqi Hu and Qingsen Yan and Yu Zhu and Yanning Zhang},
  doi          = {10.1016/j.neucom.2025.131823},
  journal      = {Neurocomputing},
  month        = {1},
  pages        = {131823},
  shortjournal = {Neurocomputing},
  title        = {FSCFNet: Lightweight neural networks via multi-dimensional importance-aware optimization},
  volume       = {660},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Deep learning for analyzing chaotic dynamics in biological time series: Insights from frog heart signals. <em>NEUCOM</em>, <em>660</em>, 131820. (<a href='https://doi.org/10.1016/j.neucom.2025.131820'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The study of experimental data is a relevant task in several physical, chemical and biological applications. In particular, the analysis of chaotic dynamics in cardiac systems is crucial as it can be related to certain pathological arrhythmias. When working with short and noisy experimental time series, some standard techniques for chaos detection cannot provide reliable results because of such data characteristics. Moreover, when small datasets are available, some Deep Learning techniques cannot be applied directly (that is, using part of the data to train the network, and using the trained network to analyze the remaining dataset). To overcome all these limitations, we propose an automatic algorithm that combines Deep Learning and some selection strategies based on a mathematical model of the same nature of the experimental data. To demonstrate its performance, we test it with experimental data obtained from ex-vivo frog heart experiments, achieving highly accurate results.},
  archive      = {J_NEUCOM},
  author       = {Carmen Mayora-Cebollero and Flavio H. Fenton and Molly Halprin and Conner Herndon and Mikael J. Toye and Roberto Barrio},
  doi          = {10.1016/j.neucom.2025.131820},
  journal      = {Neurocomputing},
  month        = {1},
  pages        = {131820},
  shortjournal = {Neurocomputing},
  title        = {Deep learning for analyzing chaotic dynamics in biological time series: Insights from frog heart signals},
  volume       = {660},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Enhancing reverse distillation model for multi-class unsupervised anomaly detection via latent space regularization. <em>NEUCOM</em>, <em>660</em>, 131819. (<a href='https://doi.org/10.1016/j.neucom.2025.131819'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the field of multi-class anomaly detection (AD), widely used reconstruction-based methods suffer from the over-generalization problem, where both normal and abnormal samples are well reconstructed, leading to the failure of AD relying on reconstruction error. In response, we propose a unified reverse distillation (RD) model for multi-class AD, named UniRD4AD. Specifically, we introduce a plug-and-play cluster loss for reconstruction-based methods, leveraging latent space regularization to obtain a compact normal representation. This enhances the separability between normal and anomalous samples, thereby mitigating over-generalization. Furthermore, we identify that existing RD models neglect the reconstruction of low-level features, which can result in false positives. To address this issue, we propose a decoder with long skip connections to enhance anomaly localization. Extensive experiments on three real-world AD datasets, including MVTec-AD, VisA, and Real-IAD, demonstrate that UniRD4AD outperforms state-of-the-art AD methods while maintaining competitive inference speed. The code is available at https://github.com/Liaaaar/UniRD4AD .},
  archive      = {J_NEUCOM},
  author       = {Kaiwen Fu and Fei Qi and Xiaotian Wang and Kun Liu},
  doi          = {10.1016/j.neucom.2025.131819},
  journal      = {Neurocomputing},
  month        = {1},
  pages        = {131819},
  shortjournal = {Neurocomputing},
  title        = {Enhancing reverse distillation model for multi-class unsupervised anomaly detection via latent space regularization},
  volume       = {660},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Multimodal large language models in medical research and clinical practice: Development, applications, challenges and future. <em>NEUCOM</em>, <em>660</em>, 131817. (<a href='https://doi.org/10.1016/j.neucom.2025.131817'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the highly intertwined field of healthcare, cross-modal medical data (including diagnostic imaging profiles, physiological time series signals, clinical voice archives, electronic medical records, surgical video streams, and genomics data) play a key role in clinical diagnostic and treatment decisions and basic medical research. Despite the breakthroughs in medical knowledge management and information processing made by generalized Large Language Models (LLMs), the existing systems are mainly limited to the processing capability of text modality, and this unimodal limitation seriously restricts the practical application value of intelligent medical systems. Multimodal Large Language Models (MLLMs) can realize multidimensional patient feature modeling through the deep semantic fusion of multimodal medical data, which can significantly improve the efficiency of disease identification and medical resource scheduling, and support the formulation of precise individual treatment strategies. In this paper, we systematically describe the development, applications, chanllenges and future of MLLMs in clinical practice and medical research, and provide a theoretical framework and practical guidelines for building a new generation of intelligent healthcare infrastructure. However, data silos, modality fusion strategies, computational resource requirements, and ethical compliance are still the main bottlenecks. Through this review, it is hoped that it can help more medical researchers to understand the progress of MLLMs technology, and help more medical researchers and MLLMs developers to work closely with each other through interdisciplinary cooperation, to create more products that are beneficial to the development of human health and put them into practical applications.},
  archive      = {J_NEUCOM},
  author       = {Peng Jun Xu and Shuang Xiang Kan and Jing Jin and Zhou Jing Zhang and Ya Xin Gu and Bo Zhang and You Lang Zhou},
  doi          = {10.1016/j.neucom.2025.131817},
  journal      = {Neurocomputing},
  month        = {1},
  pages        = {131817},
  shortjournal = {Neurocomputing},
  title        = {Multimodal large language models in medical research and clinical practice: Development, applications, challenges and future},
  volume       = {660},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). CMaP-SAM: Contraction mapping prior for SAM-driven few-shot segmentation. <em>NEUCOM</em>, <em>660</em>, 131816. (<a href='https://doi.org/10.1016/j.neucom.2025.131816'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Few-shot segmentation (FSS) aims to segment new classes using few annotated images. While recent FSS methods have shown considerable improvements by leveraging Segment Anything Model (SAM), they face two critical limitations: insufficient utilization of structural correlations in query images, and significant information loss when converting continuous position priors to discrete point prompts. To address these challenges, we propose CMaP-SAM, a novel framework that introduces contraction mapping theory to optimize position priors for SAM-driven few-shot segmentation. CMaP-SAM consists of three key components: (1) a contraction mapping module that formulates position prior optimization as a Banach contraction mapping with convergence guarantees. This module iteratively refines position priors through pixel-wise structural similarity, generating a converged prior that preserves both semantic guidance from reference images and structural correlations in query images; (2) an adaptive distribution alignment module bridging continuous priors with SAM’s binary mask prompt encoder; and (3) a foreground-background decoupled refinement architecture producing accurate final segmentation masks. Extensive experiments demonstrate CMaP-SAM’s effectiveness, achieving state-of-the-art performance with 71.1 mIoU on PASCAL- 5 i and 56.1 on COCO- 20 i datasets. Code is available at https://github.com/Chenfan0206/CMaP-SAM .},
  archive      = {J_NEUCOM},
  author       = {Shuai Chen and Fanman Meng and Liming Lei and Haoran Wei and Chenhao Wu and Qingbo Wu and Linfeng Xu and Hongliang Li},
  doi          = {10.1016/j.neucom.2025.131816},
  journal      = {Neurocomputing},
  month        = {1},
  pages        = {131816},
  shortjournal = {Neurocomputing},
  title        = {CMaP-SAM: Contraction mapping prior for SAM-driven few-shot segmentation},
  volume       = {660},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Middle modality interactive feature attention learning for visible-infrared person re-identification. <em>NEUCOM</em>, <em>660</em>, 131815. (<a href='https://doi.org/10.1016/j.neucom.2025.131815'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The scarcity of paired training samples and significant cross-modality disparities poses substantial challenges for visible-infrared person re-identification (VI-ReID). Existing methods primarily focus on generating middle-modality images to alleviate the data scarcity and bridge the modality gap. However, despite progress, these approaches face limitations: the auxiliary role of middle modalities remains constrained, network structures tend to be simplistic, and the design of effective loss functions is challenging. These limitations hinder models from fully leveraging the information within the middle modality. To address these issues, this paper introduces an Interactive Feature Attention Learning Network (IFALNet). The framework first employs a Middle Modality Generator (MMG) to synthesize middle-modality images, which are then processed by the feature extraction network. Within this network, a Modal Interactive Attention (MIA) module captures discriminative information regarding modality-shared features, while a Multi-scale Feature Aggregation (MFA) block focuses on modality-specific discriminative information, collectively enhancing feature representation. Furthermore, a Triple Centers Aggregation (TCA) loss guides the feature learning across visible, infrared, and middle modalities. This loss function effectively reduces the distance between modality centers, thereby eliminating modality gaps. Extensive experiments conducted on the SYSU-MM01, RegDB, and LLCM datasets demonstrate the superiority of the proposed method. Notably, compared to the recent MMN approach of the same type, IFALNet achieves significant improvements of 6.6 % in Rank-1 accuracy and 6.2 % in mAP on the challenging SYSU-MM01 dataset. Code is available at https://github.com/ZHY-tech11/IFALNet .},
  archive      = {J_NEUCOM},
  author       = {Haoyi Zhao and Shanmin Yang and Xiaojie Li and Jing Peng and Xi Wu},
  doi          = {10.1016/j.neucom.2025.131815},
  journal      = {Neurocomputing},
  month        = {1},
  pages        = {131815},
  shortjournal = {Neurocomputing},
  title        = {Middle modality interactive feature attention learning for visible-infrared person re-identification},
  volume       = {660},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Unveiling distinguishing properties of adversarial examples through neural tangent kernels. <em>NEUCOM</em>, <em>660</em>, 131813. (<a href='https://doi.org/10.1016/j.neucom.2025.131813'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep neural networks (DNNs), in addition to their great power, exhibit significant weaknesses, with one of the most notable being their susceptibility to adversarial attacks by carefully crafted, minimally perturbed, adversarial examples. Despite numerous efforts toward creating reliable detectors and defense mechanisms, the underlying factors that make such attacks possible have not yet been fully understood. In this paper, we further explore the use of local intrinsic dimensionality (LID), and, for the first time, hubness, as indicators for detection of adversarial examples. Furthermore, we involve neural tangent kernels (NTKs) in this task, showing that NTK-induced distance measures offer more information on how to detect adversarial examples, through both LID and hubness, than distances computed on raw input data or one network layer. Furthermore, we combine LID and hubness as features into a well-performing classifier that exhibits better detection accuracy than either of the two features used on their own. Finally, we combine the LID and hubness features with state-of-the-art adversarial attack detectors, demonstrating their general usefulness for the task. Our findings offer insight into the properties and nature of adversarial examples of various types, paving the way for the construction of better detectors and, possibly, the construction of models that are more robust to such attacks.},
  archive      = {J_NEUCOM},
  author       = {Alexandros Nanopoulos and Miloš Radovanović},
  doi          = {10.1016/j.neucom.2025.131813},
  journal      = {Neurocomputing},
  month        = {1},
  pages        = {131813},
  shortjournal = {Neurocomputing},
  title        = {Unveiling distinguishing properties of adversarial examples through neural tangent kernels},
  volume       = {660},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Exploiting structure-semantic consistency for photorealistic SLAM with 3D gaussian splatting. <em>NEUCOM</em>, <em>660</em>, 131809. (<a href='https://doi.org/10.1016/j.neucom.2025.131809'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Semantic SLAM systems aim to jointly estimate camera poses, reconstruct 3D geometry, and build meaningful semantic representations of the environment. To meet these goals with high fidelity and real-time performance, 3D Gaussian Splatting (3DGS) has recently emerged as a promising scene representation, offering dense, photorealistic reconstructions and efficient rendering that make it well-suited for semantic SLAM. However, extending 3DGS to semantic SLAM requires addressing two key problems: how to initialize Gaussians in a structure-aware manner to improve reconstruction efficiency, and how to robustly integrate and align semantic information with geometry and appearance for unified multi-modal optimization. To address these issues, we propose a superpixel-guided dense semantic SLAM framework based on 3D Gaussian Splatting. Given an input RGB- d -Semantic stream, we design a semantic-consistent superpixel extraction module. It generates superpixels with geometric convexity and semantic consistency, providing reliable structural units for Gaussian initialization. A superpixel-guided strategy is employed to generate structure-aware Gaussian primitives, while semantic information is simultaneously embedded to form a unified multi-modal representation of geometry, appearance, and semantics. To improve scalability in large-scale environments, our system decomposes the scene into independently optimized submaps, within which frame-to-model tracking and local refinement are performed. By introducing a soft-mask alignment constraint, we enable progressive fusion and promote spatial consistency during cross-modal reconstruction. Extensive experiments on public datasets demonstrate that our method achieves robust camera tracking, photorealistic scene reconstruction, and accurate semantic mapping.},
  archive      = {J_NEUCOM},
  author       = {Meiyi Yang and Qianang Zhou and Hai Liu and You-fu Li and Junlin Xiong},
  doi          = {10.1016/j.neucom.2025.131809},
  journal      = {Neurocomputing},
  month        = {1},
  pages        = {131809},
  shortjournal = {Neurocomputing},
  title        = {Exploiting structure-semantic consistency for photorealistic SLAM with 3D gaussian splatting},
  volume       = {660},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Cluster synchronization of fractional-order reaction–diffusion neural networks under modular directed topologies. <em>NEUCOM</em>, <em>660</em>, 131803. (<a href='https://doi.org/10.1016/j.neucom.2025.131803'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper investigates the cluster synchronization behavior in a class of fractional-order reaction–diffusion neural networks (FORDNNs) under modular directed topologies. The considered networks allow general signed and asymmetric inter-cluster couplings, which are commonly observed in realistic neural systems but are rarely addressed in existing studies. To handle this generality, we introduce the cluster-input equivalence (CIE) condition, which enables synchronization analysis without requiring symmetry, balanced interactions, or zero-row-sum assumptions. By constructing a tailored Lyapunov function and employing fractional-order inequality techniques, we derive sufficient conditions that guarantee intra-cluster synchronization. Numerical simulations are conducted on modular FORDNNs networks to validate the theoretical results and demonstrate the effectiveness of the proposed framework.},
  archive      = {J_NEUCOM},
  author       = {Rixu Hao and Yongqing Yang and Huixian Weng and Boling Zhou},
  doi          = {10.1016/j.neucom.2025.131803},
  journal      = {Neurocomputing},
  month        = {1},
  pages        = {131803},
  shortjournal = {Neurocomputing},
  title        = {Cluster synchronization of fractional-order reaction–diffusion neural networks under modular directed topologies},
  volume       = {660},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). A comprehensive survey of adversarial defense techniques in the visual domain. <em>NEUCOM</em>, <em>660</em>, 131799. (<a href='https://doi.org/10.1016/j.neucom.2025.131799'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, deep neural networks (DNNs) have achieved widespread success in computer vision tasks, including face recognition, autonomous driving, and medical diagnosis. However, their vulnerability to adversarial attacks has also raised serious concerns regarding system security and reliability. This paper provides a comprehensive overview of recent advances in adversarial defense methods in computer vision. It explores defense strategies for Large Vision-Language Models (LVLMs) while covering traditional adversarial defense methods. The article first explains the basic concepts of adversarial samples, their generation principles, and their actual threats in white-box, black-box and physical worlds. It then systematically examines the multi-dimensional defense strategies based on model architecture design, dynamic adversarial training, and input-output space purification. Meanwhile, for the security challenges of LVLM in the era of significant models, two types of strategies, input-output space defense and dynamic training defense, are discussed. Finally, this paper summarizes the key issues facing adversarial defense and provides an outlook on future research directions.},
  archive      = {J_NEUCOM},
  author       = {Yibin Dong and Jun Lei and Sheng Long and Shuohao Li and Jun Zhang},
  doi          = {10.1016/j.neucom.2025.131799},
  journal      = {Neurocomputing},
  month        = {1},
  pages        = {131799},
  shortjournal = {Neurocomputing},
  title        = {A comprehensive survey of adversarial defense techniques in the visual domain},
  volume       = {660},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Spiking neural network analysis of MT-MST pathways in biological motion processing. <em>NEUCOM</em>, <em>660</em>, 131795. (<a href='https://doi.org/10.1016/j.neucom.2025.131795'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Understanding the neural mechanisms underlying biological motion perception remains a significant challenge in neuroscience. To further explore this mechanism, we construct the BioMotionNet model using real bio-neural data from the MT to MST regions in macaques. To characterize neuron activity within particular time windows, we propose the window learning strategy, which employs windowed learning to extract crucial information related to specific events or stimuli. By analyzing the connectivity structure of the BioMotionNet model, we identify regular projection patterns from MT to MST, reflected in the varying response characteristics of MT neurons based on their projection strength to different MST neuron populations. Our data/codes are available at https://github.com/BrainCogLab/MT_MST .},
  archive      = {J_NEUCOM},
  author       = {Yun Zhang and Ying Liu and Tingting Feng and Tao Zhang and Hong Qu and Yi Zhang},
  doi          = {10.1016/j.neucom.2025.131795},
  journal      = {Neurocomputing},
  month        = {1},
  pages        = {131795},
  shortjournal = {Neurocomputing},
  title        = {Spiking neural network analysis of MT-MST pathways in biological motion processing},
  volume       = {660},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Integrating spatio-temporal modeling of RGB video with multi-stream skeleton representations for advanced human action recognition. <em>NEUCOM</em>, <em>660</em>, 131791. (<a href='https://doi.org/10.1016/j.neucom.2025.131791'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Unimodal human action recognition techniques have undergone significant development and are widely used. Currently, RGB video and skeleton data are the most common and effective modalities, but they still have some obvious shortcomings. For example, RGB video is easily affected by redundant background information, and the skeleton lacks interaction with the scene. Multimodal learning can achieve compensation of information from both modalities. However, most existing multimodal methods are limited in building spatio-temporal features in the video and do not explore the higher-order features of the skeleton. Therefore, this paper proposes a novel method that integrates RGB spatio-temporal features and multi-stream skeleton representation. Specifically, for the RGB modality, we use the skeleton coordinates to crop the redundant background. Meanwhile, we design a fine-grained spatio-temporal modeling module to enhance the spatio-temporal features. For the skeleton modality, we introduce three representations of joint, bone, and motion to delve deeper into the skeleton features. Finally, we design a score-weighting strategy to fuse the inference results of the two modalities. The experimental results on the benchmarks NTU RGB+D, PKU-MMD, and a real-world dataset, AAD, demonstrate the effectiveness and advancement of the proposed method.},
  archive      = {J_NEUCOM},
  author       = {Yukai Zhao and Jingwei Wang and Tianyi Yin and Jiexuan Cai and Min Liu and Yunlong Ma},
  doi          = {10.1016/j.neucom.2025.131791},
  journal      = {Neurocomputing},
  month        = {1},
  pages        = {131791},
  shortjournal = {Neurocomputing},
  title        = {Integrating spatio-temporal modeling of RGB video with multi-stream skeleton representations for advanced human action recognition},
  volume       = {660},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). HGSim: High-fidelity and generalizable simulation frame-work for autonomous driving scenes. <em>NEUCOM</em>, <em>660</em>, 131784. (<a href='https://doi.org/10.1016/j.neucom.2025.131784'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Simulating autonomous driving scenarios presents significant challenges, particularly in terms of fidelity. Although game engine-based simulators demonstrate potential, they still exhibit substantial discrepancies from the real world in both visual realism and physical consistency. To address this issue, this research proposes HGSim, a high-fidelity simulation framework for autonomous driving scenario generation and generalization. First, a four-dimensional Gaussian decoupling network is developed and supervised with semantic segmentation masks to accurately separate dynamic vehicles from static backgrounds, thereby generating artifact-free and high-fidelity static scenes. Second, an extended mask-based static vehicle decoupling method is introduced to effectively isolate static vehicles from their surrounding environments. Diffusion models are then employed to reconstruct background details, enabling realistic novel view synthesis. In addition, HGSim models vehicles with fine-grained geometric structures and assigns independent feature control parameters to each vehicle, allowing flexible simulation across diverse driving scenarios. Experimental results demonstrate that HGSim substantially outperforms existing baseline methods in terms of scene realism, representing a significant advancement in the generalization capability of autonomous driving simulations.},
  archive      = {J_NEUCOM},
  author       = {Yue Tian and Wenbo Chu and Wei Zhou and Xiaolin Tang and Keqiang Li},
  doi          = {10.1016/j.neucom.2025.131784},
  journal      = {Neurocomputing},
  month        = {1},
  pages        = {131784},
  shortjournal = {Neurocomputing},
  title        = {HGSim: High-fidelity and generalizable simulation frame-work for autonomous driving scenes},
  volume       = {660},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Text enhanced curriculum supervised contrastive learning for food image recognition. <em>NEUCOM</em>, <em>660</em>, 131781. (<a href='https://doi.org/10.1016/j.neucom.2025.131781'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Food image recognition faces significant challenges due to high intra-class variations, subtle inter-class distinctions, and biased hierarchical taxonomies. Traditional methods fail to effectively capture fine-grained culinary semantics, while prevailing contrastive learning frameworks exhibit limited adaptability to the dynamic evolution of feature representations during progressive training. To address these limitations, we propose Text Enhanced Curriculum Supervised Contrastive Learning (TEC-SCL), a novel multimodal learning framework that synergizes Vision-Language Model (VLM)-generated semantic descriptions with visual features through cross-modal attention fusion. In addition, we introduce a curriculum-based scheduler to dynamically optimize contrastive pairs by prioritizing hard negatives. Extensive experiments conducted on ETH Food-101, ISIA Food-500, and UEC-Food 256 datasets demonstrate that our method achieves state-of-the-art performance, obtaining the highest Top-1 accuracy for fine-grained retrieval. The framework bridges the gap between generic vision models and domain-specific food image recognition, offering significant potential for intelligent food systems. Our code and data are released at: https://github.com/FourierAI/TEC_SCL},
  archive      = {J_NEUCOM},
  author       = {Feng Jiang and Zhipeng Ye and Lili Zhou and Jiaqi Huang},
  doi          = {10.1016/j.neucom.2025.131781},
  journal      = {Neurocomputing},
  month        = {1},
  pages        = {131781},
  shortjournal = {Neurocomputing},
  title        = {Text enhanced curriculum supervised contrastive learning for food image recognition},
  volume       = {660},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). KE-STCN: An adaptive multi-scale traffic flow prediction method based on knowledge graph. <em>NEUCOM</em>, <em>660</em>, 131771. (<a href='https://doi.org/10.1016/j.neucom.2025.131771'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Traffic flow prediction constitutes a fundamental task in intelligent traffic management systems. However, existing approaches primarily extract explicit spatio-temporal features while overlooking latent traffic flow patterns and external influencing factors, thereby constraining the performance potential of prediction models. To address this limitation, we propose a Knowledge-Enhanced Spatio-Temporal Convolutional Network (KE-STCN) that captures hidden patterns and leverages external knowledge to enhance traffic flow prediction. The proposed framework consists of two key components: a knowledge enhancement module and a spatiotemporal convolutional network. Specifically, the knowledge enhancement module constructs a comprehensive city-level traffic knowledge graph to incorporate heterogeneous external data. It employs a dynamic fusion mechanism to update traffic states in real time. The spatio-temporal network decouples temporal and spatial modeling, extracting long- and short-term temporal dependencies from global and local perspectives, respectively, while employing both dynamic and static adjacency matrices to reveal latent spatial semantic correlations. Extensive experiments were conducted on real-world datasets from Shenzhen, Manhattan, and Los Angeles. Experimental results demonstrate that KE-STCN achieves prediction accuracies of 88.9 % and 74.3 % on the Manhattan and Shenzhen datasets, respectively, when external knowledge is incorporated, substantially outperforming state-of-the-art baselines. On the Los Angeles dataset, the model achieves a 60-minute prediction Mean Absolute Error (MAE) of 3.43, highlighting its strong generalization capability. Ablation studies further validate the essential role of external knowledge and the effectiveness of individual components, offering a robust benchmark for advancing traffic flow prediction accuracy.},
  archive      = {J_NEUCOM},
  author       = {Jianrong Cao and Xing Sheng and Bingxin Yang and Zongtao Duan},
  doi          = {10.1016/j.neucom.2025.131771},
  journal      = {Neurocomputing},
  month        = {1},
  pages        = {131771},
  shortjournal = {Neurocomputing},
  title        = {KE-STCN: An adaptive multi-scale traffic flow prediction method based on knowledge graph},
  volume       = {660},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Map wisely for efficient transfer learning across heterogeneous data sources. <em>NEUCOM</em>, <em>660</em>, 131762. (<a href='https://doi.org/10.1016/j.neucom.2025.131762'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Transfer learning is an effective mechanism to learn on a small dataset if parameters can be transferred after being learned on a larger dataset, so that one can achieve decent performance despite the data scarcity in the small dataset. This mechanism has been used in developing and using foundation models, which have outperformed other classical methods. However, this paradigm of learning does not apply to small tabular data scenarios mostly dealing with classical learning methods though there are plenty of scenarios in the real world demanding such techniques. We observe an inherent challenge in adopting transfer learning for tabular datasets. Such datasets are recorded by different agencies in various locations and over a time period. There is hardly any universal norm in ordering the variables. However, parameters are very sensitive to the ordering of the variables. Forcefully transferring the parameters in such cases despite a mismatch in variables is not a very sensible approach. Our objective is to address this challenge and make it possible to transfer learn in a situation where the number of variables and the order of variables change across datasets (referred to as heterogeneous). Our approach is based on a simple statistics-based methodology. We compute proximity among variables across datasets using samples corresponding to the variables, and then align the variables based on proximity. We explore two categories of methods depending on whether the number of samples is same or different. In each category, we explore a plethora of techniques and conduct extensive studies. We validate our approach with several statistical methods, and on various types of datasets including real-world datasets. Our approach is not only more scientific than forcibly transferring the parameters but also provides empirical benefits to transfer learning with greater efficiency.},
  archive      = {J_NEUCOM},
  author       = {Snigdhatanu Acharya and Mrinal Das},
  doi          = {10.1016/j.neucom.2025.131762},
  journal      = {Neurocomputing},
  month        = {1},
  pages        = {131762},
  shortjournal = {Neurocomputing},
  title        = {Map wisely for efficient transfer learning across heterogeneous data sources},
  volume       = {660},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). No-reference image quality assessment via bidirectional feature fusion and regional distortion extraction. <em>NEUCOM</em>, <em>660</em>, 131761. (<a href='https://doi.org/10.1016/j.neucom.2025.131761'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Image quality assessment is a core technology in image processing, primarily used to evaluate the quality and distortion level of images by analyzing their characteristics. It serves as an important metric for other visual tasks. While significant progress has been made in evaluating synthetically distorted images, assessing authentically distorted images remains a challenge. Unlike synthetic distortions, which typically have uniform distribution, authentic distortions are more complex, with unevenly distributed distortion regions and varying distortion types and levels. Moreover, authentic distortions are influenced by both high-level semantic features and low-level visual features. To address these challenges, we propose a no-reference image quality assessment algorithm based on bidirectional feature fusion and regional distortion extraction. Human-perceived visual quality is influenced by both low-level visual features and high-level semantic features, a relationship that is particularly evident in authentically distorted images. To capture this, the proposed method employs a bidirectional feature fusion structure, which integrates both high-level and low-level visual information through top-down and bottom-up pathways, respectively, thereby combining multi-layered features. Synthetic distortions are usually globally distributed, while authentic distortions typically manifest regionally, which complicates their evaluation. To address this, the proposed method uses window attention to extract local distortion features and overlapping cross-window attention to strengthen the interconnections between local distortions, yielding regionally distributed distortion features. Finally, the paper trains a model using contrastive learning to extract distortion features for various distortion types and levels. A mixture-of-experts cross-attention module is introduced to fuse distortion and image features. Experimental results on two synthetic distortion datasets and three authentic distortion datasets demonstrate that the proposed method achieves competitive performance.},
  archive      = {J_NEUCOM},
  author       = {Xiong Nie and Lihua Tian and Chen Li},
  doi          = {10.1016/j.neucom.2025.131761},
  journal      = {Neurocomputing},
  month        = {1},
  pages        = {131761},
  shortjournal = {Neurocomputing},
  title        = {No-reference image quality assessment via bidirectional feature fusion and regional distortion extraction},
  volume       = {660},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Task-aware dynamic routing network for cross-domain few-shot learning. <em>NEUCOM</em>, <em>660</em>, 131752. (<a href='https://doi.org/10.1016/j.neucom.2025.131752'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cross-domain few-shot learning (CD-FSL) is a highly challenging task. It aims to learn classifiers for unseen classes and domains using limited labeled samples. Existing methods adopt learnable task-specific parameters, but the fixed-parameter paradigm faces two intertwined issues. First, significant distribution gaps between source and target domains make fixed parameters unable to flexibly adapt to target tasks: excessive parameters cause overfitting to sparse target samples, while insufficient ones fail to capture patterns required for domain transfer. Second, sparse labels hinder feature learning, leaving models struggling to extract discriminative features that are robust across domains. This problem is more severe when handling complex visual elements.Inspired by the strong correlation between task-specific parameter configurations and target task characteristics, we propose the Task-Aware dynamiC Routing Network (TACO) for CD-FSL. During training, TACO uses reinforcement learning to dynamically adjust its architecture based on discrepancies between source and target domains. This optimizes task-specific configurations to mitigate overfitting and under-adaptation. Additionally, it integrates self-attention and knowledge distillation to enhance feature extraction, enabling better representation of complex visual elements. Experiments on 8 CD-FSL datasets demonstrate that TACO outperforms existing methods in average accuracy and generalization ability. Compared with state-of-the-art results on individual datasets, our approach achieves a performance improvement of up to 3.03 %.},
  archive      = {J_NEUCOM},
  author       = {Yanan Li and Haoyang Ye and Huabing Zhou and Tao Lu and Hao Lu},
  doi          = {10.1016/j.neucom.2025.131752},
  journal      = {Neurocomputing},
  month        = {1},
  pages        = {131752},
  shortjournal = {Neurocomputing},
  title        = {Task-aware dynamic routing network for cross-domain few-shot learning},
  volume       = {660},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). AutoODL: Automated on-device learning via dynamic unfreezing and semi-supervised labeling for edge AI. <em>NEUCOM</em>, <em>660</em>, 131720. (<a href='https://doi.org/10.1016/j.neucom.2025.131720'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose AutoODL, a fully automated, semi-supervised on-device learning framework for embedded neural network fine-tuning. AutoTag generates robust pseudo-labels via class-specific dynamic thresholds with consistency regularization; Dynamic Unfreezing progressively activates layers based on loss stagnation; Batch Data Streaming uses gradient accumulation for stable large-batch-like updates under memory constraints. On CIFAR-10/100 with ResNet, SENet, and ViT, AutoODL improves accuracy over TinyOL/TinyTL/SensOL by up to 15.96 %/17.45 %/12.27 % while adding only 10–18 MB peak memory. Deployed on Raspberry Pi 4, we report step latency and average power using a measurement protocol. By seamlessly integrating these mechanisms, AutoODL achieves label-free, real-time fine-tuning on edge devices with minimal memory overhead.},
  archive      = {J_NEUCOM},
  author       = {Jun Wang and Yanwen Luo and Yanan Liu and Yifan Wang and Xinyu Yang and Huixin Zhong and Gaoyu Dai},
  doi          = {10.1016/j.neucom.2025.131720},
  journal      = {Neurocomputing},
  month        = {1},
  pages        = {131720},
  shortjournal = {Neurocomputing},
  title        = {AutoODL: Automated on-device learning via dynamic unfreezing and semi-supervised labeling for edge AI},
  volume       = {660},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Fine-grained food image classification with multi-modal collaboration. <em>NEUCOM</em>, <em>660</em>, 131714. (<a href='https://doi.org/10.1016/j.neucom.2025.131714'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Fine-grained image classification focuses on distinguishing subclasses. In food image classification, food’s shape, color, and texture are easily influenced by cooking methods, presentation, and lighting. This makes differences among the same kind of food more subtle and harder to capture. To address this, we propose a multimodal framework fusing image and text modalities. The text modality, with distinct statistical properties, data attributes, and relational structures compared to images, enables precise characterization of fine-grained food features, providing complementary semantic information for classification. Specifically, cross-modal knowledge distillation is employed to transfer textual semantics into the image feature space, while contrastive learning is leveraged to discriminate between homogeneous and heterogeneous features across modalities and facilitate feature complementarity. Experiments on benchmark fine-grained food datasets VegFru, FoodX-251, and CNFood show that the proposed method achieves classification accuracies of 97.22 %, 83.25 %, and 84.96 %, respectively, outperforming state-of-the-art (SOTA) methods and validating the effectiveness of the multimodal cross-modal fusion strategy in fine-grained food classification tasks.},
  archive      = {J_NEUCOM},
  author       = {Li Cheng and Linyi Lan and Zhongjie Xiao and Jianzhang Chen and Jiaxiong Lu and Huanrong Wang and Yi-Ping Phoebe Chen},
  doi          = {10.1016/j.neucom.2025.131714},
  journal      = {Neurocomputing},
  month        = {1},
  pages        = {131714},
  shortjournal = {Neurocomputing},
  title        = {Fine-grained food image classification with multi-modal collaboration},
  volume       = {660},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). FNCD-CS: A neurocomputing-optimized hybrid neural network for cross-domain code search. <em>NEUCOM</em>, <em>659</em>, 131847. (<a href='https://doi.org/10.1016/j.neucom.2025.131847'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Code search is pivotal for enhancing software development efficiency, yet existing deep learning models face critical limitations in cross-domain scenarios from a neural network perspective: single-modality feature extraction lacks dedicated neural modules to model code-specific syntactic dependencies, and information loss arises from gradient degradation during deep neural training—two inherent challenges of neural networks when processing structured code data. To address these issues, we propose FNCD-CS, a neurocomputing-optimized neural network architecture with a layered, synergistic design: (1) fastText initializes lexical embeddings by simulating morphological recognition in biological neural systems. It uses character n-gram splitting to resolve the out-of-vocabulary limitation of fixed-vocabulary embedding models; (2) a 2-head self-attention module constructs global syntactic dependency graphs, overcoming the local feature bias of traditional neural networks by simultaneously capturing token-level and structure-level relationships; (3) residual connections establish "short-circuit paths" for feature transmission, alleviating gradient vanishing-induced information loss and preserving 89.7 % of syntactic features in deep layers; (4) an LSTM-based domain adapter imitates synaptic plasticity via gated weight adjustment, aligning cross-domain syntactic feature spaces without retraining. Experimental results on Java (source domain) and three target domain datasets demonstrate that FNCD-CS outperforms four baseline models. It achieves 7.61 %-15.26 % higher H@1-H@3 accuracy over all baselines and a 21.90 % MRR improvement compared to the state-of-the-art cross-domain model AdaCS. This performance gain stems from the synergistic optimization of its neural modules: self-attention enhances syntactic feature discrimination, residual connections preserve feature integrity, and the LSTM adapter boosts cross-domain transferability. Notably, FNCD-CS maintains real-time inference efficiency at 50 queries per second—17.5 % faster than AdaCS—owing to optimized neural computation complexity with 234k parameters, 34.3 % fewer than AdaCS. Source code can be extracted from: https://share.weiyun.com/ngCifCi .},
  archive      = {J_NEUCOM},
  author       = {Mengge Fang and Li-e Wang and Haize Hu},
  doi          = {10.1016/j.neucom.2025.131847},
  journal      = {Neurocomputing},
  month        = {1},
  pages        = {131847},
  shortjournal = {Neurocomputing},
  title        = {FNCD-CS: A neurocomputing-optimized hybrid neural network for cross-domain code search},
  volume       = {659},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Multi-organ medical image segmentation via adaptive disentangled domain generalization collaborative learning. <em>NEUCOM</em>, <em>659</em>, 131846. (<a href='https://doi.org/10.1016/j.neucom.2025.131846'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Semi-supervised 3D medical image segmentation holds great promise for clinical practice but remains challenged by domain shifts caused by imaging protocol variations and by the difficulty of accurately segmenting small organs with sparse annotations. To address these issues, we propose a novel Adaptive Disentangled Domain Generalization Collaborative Learning (AD-DGCL) framework. It integrates two core components: a Semi-Supervised Representation Disentanglement (SSRD) module that separates domain-specific style and anatomical content via dual encoders and cross-domain contrastive learning, and a Style-induced Consistency Training (SCT) module that enhances robustness through synthetic style perturbations and consistency regularization. In addition, an adaptive region-specific loss function is designed to prioritize small organs by dynamically adjusting weights according to pixel frequency. Extensive experiments demonstrate that AD-DGCL substantially improves segmentation accuracy, particularly for small organs, while effectively mitigating domain gaps, thus showing strong potential for robust and generalizable clinical deployment.},
  archive      = {J_NEUCOM},
  author       = {Min Dong and Yishuang Liu and Ating Yang and Ye Zhang and Rongchang Zhao and Ling Liu},
  doi          = {10.1016/j.neucom.2025.131846},
  journal      = {Neurocomputing},
  month        = {1},
  pages        = {131846},
  shortjournal = {Neurocomputing},
  title        = {Multi-organ medical image segmentation via adaptive disentangled domain generalization collaborative learning},
  volume       = {659},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). WP-CrackNet: A collaborative adversarial learning framework for end-to-end weakly-supervised road crack detection. <em>NEUCOM</em>, <em>659</em>, 131845. (<a href='https://doi.org/10.1016/j.neucom.2025.131845'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Road crack detection is essential for intelligent infrastructure maintenance in smart cities. To reduce reliance on costly pixel-level annotations, we propose WP-CrackNet, an end-to-end weakly-supervised method that trains with only image-level labels for pixel-wise crack detection. WP-CrackNet integrates three components: a classifier generating class activation maps (CAMs), a reconstructor measuring feature inferability, and a detector producing pixel-wise road crack detection results. During training, the classifier and reconstructor alternate in adversarial learning to encourage crack CAMs to cover complete crack regions, while the detector learns from pseudo labels derived from post-processed crack CAMs. This mutual feedback among the three components improves learning stability and detection accuracy. To further boost detection performance, we design a path-aware attention module (PAAM) that fuses high-level semantics from the classifier with low-level structural cues from the reconstructor by modeling spatial and channel-wise dependencies. Additionally, a center-enhanced CAM consistency module (CECCM) is proposed to refine crack CAMs using center Gaussian weighting and consistency constraints, enabling better pseudo-label generation. We create three image-level datasets and extensive experiments show that WP-CrackNet achieves comparable results to supervised methods and outperforms existing weakly-supervised methods, significantly advancing scalable road inspection. The source code package and datasets are available at https://mias.group/WP-CrackNet/ .},
  archive      = {J_NEUCOM},
  author       = {Nachuan Ma and Zhengfei Song and Qiang Hu and Xiaoyu Tang and Chengxi Zhang and Rui Fan and Lihua Xie},
  doi          = {10.1016/j.neucom.2025.131845},
  journal      = {Neurocomputing},
  month        = {1},
  pages        = {131845},
  shortjournal = {Neurocomputing},
  title        = {WP-CrackNet: A collaborative adversarial learning framework for end-to-end weakly-supervised road crack detection},
  volume       = {659},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Focusing on what humans see: Robustness enhancement through adversarial supervised contrastive learning. <em>NEUCOM</em>, <em>659</em>, 131842. (<a href='https://doi.org/10.1016/j.neucom.2025.131842'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep Neural Networks (DNNs) have various vulnerabilities such as unpredictable behavior on adversarial examples. Adversarial training (AT) encourages models to learn robust, human-perceptible features rather than non-robust features present in the data distribution, which, although imperceptible to humans, are utilized for classification and lead to adversarial examples. However, while traditional AT methods achieve robustness against adversarial attacks, they suffer from various performance degradations. We hypothesize that this phenomenon is related to cross-entropy loss, and can be mitigated by using contrastive loss, which learns common features across samples through batch-wise comparisons. In response, we propose Robust Supervised Contrastive Learning (RSupCon), which extends supervised contrastive learning to the adversarial domain with two strategies: combining various augmentations and separating the anchor and contrast sets. The combined augmentations encourage the model to focus on learning robust features, and separating the contrast set reduces the learning of non-robust features. With these two strategies, RSupCon effectively helps the model discriminate robust features within images transformed in various ways and adversarial examples. Experiments on benchmark datasets demonstrate that RSupCon offers adversarial robustness comparable to traditional AT methods while mitigating performance degradation. Visual evidence further confirms the ability of our method to learn representations centered on robust features. Furthermore, several experimental results and analyses offer novel insights into non-robust features and adversarial training.},
  archive      = {J_NEUCOM},
  author       = {Keon Kim and Yongsuk Choi},
  doi          = {10.1016/j.neucom.2025.131842},
  journal      = {Neurocomputing},
  month        = {1},
  pages        = {131842},
  shortjournal = {Neurocomputing},
  title        = {Focusing on what humans see: Robustness enhancement through adversarial supervised contrastive learning},
  volume       = {659},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Collaborative filtering enhanced subgraph embedding for link direction and sign prediction. <em>NEUCOM</em>, <em>659</em>, 131840. (<a href='https://doi.org/10.1016/j.neucom.2025.131840'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The enclosing subgraph centered around a link is undoubtedly a strong indicator of the label of the link in various graph types. However, effectively representing and interpreting such link-centric subgraphs for predicting link direction and sign remains a significant challenge. In this paper, we propose a framework called C ollaborative F iltering Enhanced S ubgraph E mbedding (CFSE) for link direction and link sign prediction in (signed) directed networks. Specifically, inspired by collaborative filtering, CFSE first identifies a fundamental path within the subgraph as a key indicator of link direction and sign. Then, using a matrix representation of this path, CFSE generates subgraph embeddings to train a neural network classifier for link prediction tasks. Compared to graph learning methods based on pairwise transitivity and triad motifs, it is demonstrated that CFSE can achieve superior prediction performance across both signed and unsigned directed networks. The advantage of our approach lies in the fact that it only takes usage of one-hop neighborhood information, yet can achieve better link prediction performance than those node-centric subgraph or graph embedding approaches. The code is available at https://github.com/fang98/CFSE .},
  archive      = {J_NEUCOM},
  author       = {Zhihong Fang and Shaolin Tan and Zhe Li and Qiu Fang and Yao Chen},
  doi          = {10.1016/j.neucom.2025.131840},
  journal      = {Neurocomputing},
  month        = {1},
  pages        = {131840},
  shortjournal = {Neurocomputing},
  title        = {Collaborative filtering enhanced subgraph embedding for link direction and sign prediction},
  volume       = {659},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). A deep neural network model with physics-guided term for automatic identification of atmospheric fronts. <em>NEUCOM</em>, <em>659</em>, 131835. (<a href='https://doi.org/10.1016/j.neucom.2025.131835'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Intense weather phenomena often occur near atmospheric fronts, making accurate front identification crucial for weather analysis and forecasting. However, current deep learning methods for atmospheric front identification directly input multiple meteorological elements into networks for training and recognition. This black-box nature makes it difficult to trust that such approaches can yield highly interpretable results consistent with physical laws. These methods heavily rely on data quality and training processes, exhibiting poor generalization capability. Additionally, conflicts in front characteristics among multiple meteorological elements hinder network training and identification. To address these issues, we integrate empirical, theoretical, and computational science paradigms to propose a physics-guided intelligent atmospheric front identification model (PG-AMUnet). This model employs physics-guided terms to extract frontal physical information as prior knowledge, accelerating network training and improving identification accuracy. In subsequent network stages, an encoder-decoder architecture with attention mechanisms extracts frontal features from physical information and determines front categories at grid points. The encoder incorporates convolutional block attention modules to simultaneously consider channel and spatial information, thereby enhancing the quality of front-related features represented by physical information. The decoder utilizes scale-adaptive attention to dynamically adjust fusion weights of abstract feature maps at different scales, effectively capturing weak frontal features embedded in physical information while filtering out strong non-frontal features. Multiple experimental results demonstrate that incorporating physics-guided terms and attention mechanisms into deep learning networks for atmospheric front identification significantly promotes network training, improves recognition precision, and enhances model robustness and generalization capability. Therefore, the proposed PG-AMUnet model can be widely utilized in practical weather analysis applications.},
  archive      = {J_NEUCOM},
  author       = {Xinya Ding and Qian Li and Liang Zhang and Tianying Wang and Xiaoping Zhao and Yudi Liu and Yunpeng Zhang},
  doi          = {10.1016/j.neucom.2025.131835},
  journal      = {Neurocomputing},
  month        = {1},
  pages        = {131835},
  shortjournal = {Neurocomputing},
  title        = {A deep neural network model with physics-guided term for automatic identification of atmospheric fronts},
  volume       = {659},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Speeding up omnidirectional video quality assessment with reinforcement learning based key-frame selection. <em>NEUCOM</em>, <em>659</em>, 131834. (<a href='https://doi.org/10.1016/j.neucom.2025.131834'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Omnidirectional video quality assessment (OVQA) models typically require to extract spatiotemporal features from high-resolution video, leading to high computational complexity. To mitigate this, some existing methods employ uniform frame sampling to reduce the number of frames processed, but the sampled frames often fail to capture critical spatiotemporal degradations. In this work, we propose a key-frame selection network that adopts reinforcement learning to select distortion-aware key-frames according to inter-frame salient changes. Based on the key-frame selection network, we develop a novel OVQA framework. The resulting model samples key-frames at the original resolution for spatial feature extraction and reuses the temporal features from the key-frame selection network as inter-frame temporal representations to reduce the computational complexity of the model. Then, the computed spatio-temporal features are aggregated for quality prediction. We evaluate the performance of the proposed quality model on three publicly available OVQA databases. The experimental results show that the proposed model achieves superior performance relative to state-of-the-art quality models, especially in inference speed.},
  archive      = {J_NEUCOM},
  author       = {Ruikang Yu and Zongyao Hu},
  doi          = {10.1016/j.neucom.2025.131834},
  journal      = {Neurocomputing},
  month        = {1},
  pages        = {131834},
  shortjournal = {Neurocomputing},
  title        = {Speeding up omnidirectional video quality assessment with reinforcement learning based key-frame selection},
  volume       = {659},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Enhancing quasi-nonlinear long-term cognitive networks with temporal attention for pattern classification. <em>NEUCOM</em>, <em>659</em>, 131830. (<a href='https://doi.org/10.1016/j.neucom.2025.131830'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Long-term Cognitive Networks (LTCNs) are knowledge-based recurrent neural networks that hold significant promise in machine learning settings, particularly in structured pattern classification. These neural systems enable hybrid intelligence by allowing domain experts to encode knowledge into the network through a non-trainable weight matrix. However, LTCN-based classifiers face two key limitations that restrict their approximation capabilities. (i) Both neural concepts and weights must map to specific components of the physical system. (ii) Temporal states are not fully exploited when deriving class labels. This paper presents an enhanced LTCN-based classifier that addresses these limitations. First, we introduce a tunable quasi-nonlinear reasoning rule. Each neural concept has independent learnable unbounded parameters that evolve over iterations. Second, we propose a temporal attention mechanism that projects hidden states to a new state space and assigns different attention weights to each iteration based on its relevance. We provide theoretical evidence that this temporal attention mechanism outperforms residual-like learnable connections. Finally, we formalize a gradient-based learning algorithm to fine-tune both the quasi-nonlinear parameters and the projection matrices used by the temporal attention mechanism. Numerical simulations on real-world datasets confirm that our classifier achieves superior classification accuracy compared to tree ensembles, neurosymbolic methods, transformer-based methods, and kernel machines.},
  archive      = {J_NEUCOM},
  author       = {Gonzalo Nápoles and Yamisleydi Salgueiro},
  doi          = {10.1016/j.neucom.2025.131830},
  journal      = {Neurocomputing},
  month        = {1},
  pages        = {131830},
  shortjournal = {Neurocomputing},
  title        = {Enhancing quasi-nonlinear long-term cognitive networks with temporal attention for pattern classification},
  volume       = {659},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). LLM guided counterfactual reasoning for zero-shot knowledge based visual question answering. <em>NEUCOM</em>, <em>659</em>, 131828. (<a href='https://doi.org/10.1016/j.neucom.2025.131828'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Knowledge-Based Visual Question Answering (KB-VQA) requires not only understanding image content but also reasoning with external knowledge. Recent advancements in large language models (LLMs) have enabled zero-shot inference for KB-VQA. However, existing paradigms still suffer from three major limitations: a knowledge disconnect among the image, the caption model, and the LLM, a lack of out-of-distribution task awareness, and difficulty in capturing fine-grained visual details. To address these challenges, we propose LLM guided Counterfactual Samples Synthesizing and Training (LCSST), a unified framework that enhances caption models through counterfactual reasoning. Specifically, we design three synergistic modules: Counterfactual Caption Generation (CounCG) introduces alternative factual perspectives via vision-language models and LLMs. Counterfactual Image Synthesis (CounIS) generates counterfactual images, guided by counterfactual captions, using advanced detection, segmentation, and image synthesis techniques. CounCG and CounIS generate diverse and informative textual and visual counterfactual samples to mitigate knowledge disconnect. Additionally, we introduce a Diversity-Aware Contrastive Loss (DACL) to improve generalization and encourage fine-grained discrimination by emphasizing semantically similar and diverse negatives. Extensive experiments on OK-VQA and A-OKVQA demonstrate that LCSST substantially outperforms existing zero-shot KB-VQA methods.},
  archive      = {J_NEUCOM},
  author       = {Zhuhan Zhang and Min Jiang and Jun Kong and Jiayi Li},
  doi          = {10.1016/j.neucom.2025.131828},
  journal      = {Neurocomputing},
  month        = {1},
  pages        = {131828},
  shortjournal = {Neurocomputing},
  title        = {LLM guided counterfactual reasoning for zero-shot knowledge based visual question answering},
  volume       = {659},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Automated brain extraction on diffusion-weighted images using pseudo and cross semi-supervised method. <em>NEUCOM</em>, <em>659</em>, 131825. (<a href='https://doi.org/10.1016/j.neucom.2025.131825'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Brain extraction in diffusion-weighted images (DWI) is a primary and crucial step in lesion analysis for stroke patients, where imperfect brain region segmentation can lead to a decrease in the accuracy of downstream tasks. However, the development of DWI-specific brain extraction tools (BET) has been significantly hindered by the scarcity of publicly available DWI datasets for supervised deep learning, in stark contrast to the abundance of T1-weighted (T1w) images. In response, we proposed SSL-BET, a semi-supervised brain extraction method designed to make effective use of large amounts of unlabeled clinical DWI data, thereby reducing the reliance on costly and time-consuming manual annotations. With limited labeled training data, our method enables the construction of deep learning models for high-accuracy brain extraction from DWI scans of stroke patients. SSL-BET is designed upon the principles of consistency regularization and entropy minimization, allowing the generation of high-quality supervision from unlabeled clinical data to enhance model performance in semi-supervised settings. Our method attained a Dice Similarity Coefficient (DSC) of 97.32 %±0.59 % across 50 test samples, necessitating merely 3 labeled training samples. The experiments revealed that our approach significantly surpassed the commonly employed BET methods, such as FSL-BET, SPM-BET, and HD-BET ( p < 0.05). Compared to other semi-supervised methods, it was shown that with a single labeled data, ours achieved enhanced results (DSC: 96.46 %±0.88 %). To summarize, our method helped to alleviate the need for annotation of DWI brain extraction, thereby holding promise to facilitate the automated analysis of DWI scans for stroke patients.},
  archive      = {J_NEUCOM},
  author       = {Yibing Chen and Benqi Zhao and Jinyang Li and Zhilin Wang and Yingchun Fan and Kaiyue Su and Zhuozhao Zheng and Zhensen Chen},
  doi          = {10.1016/j.neucom.2025.131825},
  journal      = {Neurocomputing},
  month        = {1},
  pages        = {131825},
  shortjournal = {Neurocomputing},
  title        = {Automated brain extraction on diffusion-weighted images using pseudo and cross semi-supervised method},
  volume       = {659},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). A hybrid SGC-transformer network for EEG emotion recognition with historical data integration. <em>NEUCOM</em>, <em>659</em>, 131822. (<a href='https://doi.org/10.1016/j.neucom.2025.131822'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, great progress has been made in emotion recognition based on electroencephalogram (EEG). However, the complex relationships between EEG channels have not been fully explored, and existing methods rarely leverage the potential of EEG data volume, both of which impact performance of emotion recognition models. For capturing inter-channel relationships, we consider combining Simple Graph Convolutional Network (SGC) and Transformer to exploit relationships from channels’ topology and features, proposing an SGC-Transformer network for emotion recognition. Specifically, the network consists of multiple SGC-Transformer (SGCT) blocks, each incorporating one SGC layer and one Transformer architecture. The SGCT block first captures topological structure of channels through the SGC, and then utilizes the features of topological nodes to learn the long-distance dependencies between channels with Transformer, thereby enabling the complementary integration of topology-based and feature-based complex relationships. Furthermore, to maximize the utilization of EEG data, we develop an EEG-data integration strategy that incorporates historically collected data into the current training set, surprisingly finding that it significantly improves model prediction accuracy. Extensive experiments on the SEED, SEED-IV and HBUED, demonstrate that integrating both topology-based and feature-based information is more effective in exploring inter-channel relationships, compared with feature-based methods alone. Additionally, the EEG-data integration strategy provides an important reference for future studies involving increased data volumes. The relevant code can be found at https://github.com/braverSheep/SGCTNet .},
  archive      = {J_NEUCOM},
  author       = {Yong Yang and Kaibo Shi and Nan Zhou and Wenhao Wang and Shiping Wen and Ming Zhu and Yuanlun Xie},
  doi          = {10.1016/j.neucom.2025.131822},
  journal      = {Neurocomputing},
  month        = {1},
  pages        = {131822},
  shortjournal = {Neurocomputing},
  title        = {A hybrid SGC-transformer network for EEG emotion recognition with historical data integration},
  volume       = {659},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Reinforcement learning based deep fuzzy hierarchical clustering to generate personalized non-fungible token artwork. <em>NEUCOM</em>, <em>659</em>, 131821. (<a href='https://doi.org/10.1016/j.neucom.2025.131821'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the realm of information technology and software development, digital assets increasingly manifest as Non-Fungible Tokens (NFTs) on blockchain platforms, embodying intrinsic material value that enhances user satisfaction. Despite their potential, the creation of NFTs remains costly, time-consuming, and labor-intensive. To address these challenges, we introduce a digital asset generation engine specifically designed for producing NFT artwork. Utilizing real-world datasets curated by digital art experts, our engine synthesizes individual image layers into cohesive, high-quality artistic outputs. Leveraging artificial intelligence, we employ a novel Deep Fuzzy Hierarchical Clustering approach, which integrates autoencoder neural networks, fuzzy clustering, and hierarchical clustering methods. This integrated approach enables precise classification of image layers, achieving an impressive accuracy rate of 95 %. Here, we demonstrate the potential of AI-enhanced solutions in the digital art and NFT space. Our engine not only reduces costs and labor intensity in digital art production but also allows users to personalize their NFT collections by selecting desired layers and specifying rarity, arrangement order, and metadata details. This study underscores the significance of intersectional research between artificial intelligence and fine arts, opening avenues for future advancements in computational art analysis and creative AI applications.},
  archive      = {J_NEUCOM},
  author       = {Arman Daliri and Nora Mahdavi and Mahdieh Zabihimayvan and Aynaz Norouzi Baranghar and Nima Zaeimzadeh and Javad mohammadzadeh},
  doi          = {10.1016/j.neucom.2025.131821},
  journal      = {Neurocomputing},
  month        = {1},
  pages        = {131821},
  shortjournal = {Neurocomputing},
  title        = {Reinforcement learning based deep fuzzy hierarchical clustering to generate personalized non-fungible token artwork},
  volume       = {659},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Design of energy-efficient LIF neuron using CMOS compatible gate-all-around floating nanosheet FET for bio-inspired spiking neural networks. <em>NEUCOM</em>, <em>659</em>, 131814. (<a href='https://doi.org/10.1016/j.neucom.2025.131814'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The fundamental component of an artificial spiking neural network (SNN) is an electronic device designed to emulate a biological neuron effectively. However, a key concern is the high energy consumption and large area associated with these artificial neurons, rendering them highly inefficient. This article presents a CMOS-compatible Gate-All-Around Floating Nanosheet Field-Effect Transistor (GAA FNSFET)-based Leaky Integrate-and-Fire (LIF) neuron with gate length of 100 nm. This design achieves an exceptionally low energy consumption of 4.88 fJ, thereby establishing a new benchmark in the field. Utilizing well calibrated 3D TCAD simulation and the SRH recombination model, along with the Unibo2 model to represent impact ionization phenomena, the proposed GAA FNSFET LIF neuron effectively captures integration and recombination aspects within it. Moreover, it operates with a supply voltage of merely 1 V, significantly lower than conventional bulk FinFET and PD-SOI MOSFET-based LIF neurons. Also, it demonstrates spiking frequency of ∼19.3 MHz, approximately ∼10 5 times higher than that of a biological neuron, controlled by the input voltage of the device, allowing the neuron adapt dynamically. The FNSFET technology offers superior electrostatic control, minimizing parasitic capacitances and facilitating rapid switching capabilities, thereby optimizing both power and performance simultaneously. Furthermore, the effective area of the FNSFET amounts to 0.033 µm 2 making it an appealing choice for low-power, area efficient, large-scale SNN applications.},
  archive      = {J_NEUCOM},
  author       = {Yashodhan Bhatawdekar and Syed Mohammad Riyaz and Lakshmi Amrutha Yechuri and Sresta Valasa and Venkata Ramakrishna Kotha and Sunitha Bhukya and Shubham Tayal and Narendar Vadthiya},
  doi          = {10.1016/j.neucom.2025.131814},
  journal      = {Neurocomputing},
  month        = {1},
  pages        = {131814},
  shortjournal = {Neurocomputing},
  title        = {Design of energy-efficient LIF neuron using CMOS compatible gate-all-around floating nanosheet FET for bio-inspired spiking neural networks},
  volume       = {659},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). DermClinical: Clinical-oriented dataset and evaluation for computer-aided dermatological diagnosis. <em>NEUCOM</em>, <em>659</em>, 131811. (<a href='https://doi.org/10.1016/j.neucom.2025.131811'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Dermatosis is one of the most common diseases and requires a heavy workload for diagnosis due to its high incidence. Despite the development of deep-learning-based computer-aided diagnosis reported in the literature, application in real clinical practice remains a challenge. We argue that the key reason is the misalignment between the evaluation of benchmark studies and expectations of workflows in practice. In this paper, we systematically address the problem from perspectives of dataset, metric, as well as evaluation schemes, collectively denoted as an evaluation framework DermClinical. Specifically, we build a so-far largest dataset, with the introduction of the “unknown" category that measures the ability of models to deal with cases from unseen categories. On this basis, we propose a novel metric which, as opposed to common classification based metrics, directly measures the labor saved by a computational model for doctors. Finally, we design a limited-data evaluation scheme to simulate situations when unseen categories are encountered in real practice. Extensive experiments are conducted to evaluate and compare current computational methods. The dataset, metric and code will be released to facilitate the research.},
  archive      = {J_NEUCOM},
  author       = {Zihao Liu and Zhiqiang Hu and Ruiqin Xiong and Shaoting Zhang and Tingting Jiang},
  doi          = {10.1016/j.neucom.2025.131811},
  journal      = {Neurocomputing},
  month        = {1},
  pages        = {131811},
  shortjournal = {Neurocomputing},
  title        = {DermClinical: Clinical-oriented dataset and evaluation for computer-aided dermatological diagnosis},
  volume       = {659},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). SolNet and machine learning models for effective dust detection in photovoltaic systems. <em>NEUCOM</em>, <em>659</em>, 131810. (<a href='https://doi.org/10.1016/j.neucom.2025.131810'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As photovoltaic (PV) systems continue to gain popularity as a cost-effective energy solution, maintaining their efficiency has become increasingly important. One of the main challenges these systems face is dust buildup on PV panels, which can significantly lower their performance. In response to this issue, this paper proposes a machine learning-based automated dust detection system designed to optimize the cleaning process for PV modules. The study evaluates various models, including Decision Tree (DT), Support Vector Machine (SVM), Random Forest (RF), and K-Nearest Neighbor (KNN), and compares to the most state-of-the-art like SolNet + , to identify the most effective model for dust detection. Several techniques, such as oversampling, scaling, feature selection, and hyperparameter tuning, were applied to improve model performance. A new optimizer, the loin optimizer, was employed for hyperparameter tuning. In addition, these models were compared to 28 deep-learning models. Among the models tested, SolNet + emerged as the top performer, consistently achieving the highest accuracy, recall, and F1 scores for dust detection. When compared to two previous studies, SolNet + stands out, particularly in Scenario 2, where it reached an impressive 88 % accuracy—just behind EfficientNetB7 (89.8 %) and EfficientNetB4 (88.9 %). While Random Forest showed strong accuracy, it faced challenges with imbalanced data. SVM performed moderately, and KNN and Decision Tree ranked lower, especially in terms of precision and recall in imbalanced data conditions. These findings highlight the significant potential of machine learning to enhance PV maintenance by effectively detecting dust accumulation.},
  archive      = {J_NEUCOM},
  author       = {Hmeda Najemddin Musbah and Ali Othmam Albaji and Meftah Elsaraiti and Araf Taher Almasri},
  doi          = {10.1016/j.neucom.2025.131810},
  journal      = {Neurocomputing},
  month        = {1},
  pages        = {131810},
  shortjournal = {Neurocomputing},
  title        = {SolNet and machine learning models for effective dust detection in photovoltaic systems},
  volume       = {659},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Bipartite synchronization for multi-level networks with antagonistic interactions. <em>NEUCOM</em>, <em>659</em>, 131808. (<a href='https://doi.org/10.1016/j.neucom.2025.131808'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we investigate the bipartite synchronization (Bi-Syn) problem of multi-level networks which have antagonistic interactions. Firstly, we propose a novel multiweighted multi-level network model with antagonistic interactions, which generalizes existing single-layer and multi-layer network models (with or without antagonistic interactions) as special cases. Then, the multi-level network, which is described by multiweighted and signed digraphs, can be transformed into multi-level network with a single weight by rearranging variables’ order technique. Moreover, different from previous studies which restricted that the networks should be structurally balanced, the topology structure of our newly proposed multiweighted multi-level network can be structurally unbalanced and not strongly connected. What’s more, by constructing Lyapunov functions based on the Kronecker product of left eigenvectors of the multi-level coupling matrices, sufficient conditions for exponential Bi-Syn of multi-level networks are derived. Obtained criteria can reduce the conservatism of existing results, as validated by simulations.},
  archive      = {J_NEUCOM},
  author       = {Leijing Xie and Xiwei Liu},
  doi          = {10.1016/j.neucom.2025.131808},
  journal      = {Neurocomputing},
  month        = {1},
  pages        = {131808},
  shortjournal = {Neurocomputing},
  title        = {Bipartite synchronization for multi-level networks with antagonistic interactions},
  volume       = {659},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Finite-time asynchronous state estimation for two-time-scale complex networks with sojourn probabilities and event-based AF relay protocols. <em>NEUCOM</em>, <em>659</em>, 131807. (<a href='https://doi.org/10.1016/j.neucom.2025.131807'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we study the finite-time H ∞ asynchronous state estimation problem for a two-time-scale complex network with sojourn probabilities under event-based amplify-and-forward (AF) relay protocols, where the mode sojourn probabilities are partially known. An asynchronous and general state estimator is proposed, which includes four different types of state estimators. A new dynamic event-triggered mechanism (DETM) is embedded in each AF relay to reduce the energy loss of the relay and save network resources in the relay-to-estimator channel. A DETM of the same form is embedded in each sensor to save network resources in the sensor-to-relay channel. Each proposed DETM includes a multiplicative adjusting variable and an additive internal dynamic variable (IDV), allowing the overall system to save network resources while maintaining estimation performance. The aim of this study is to design an asynchronous state estimator such that the resulting error dynamics are stochastically finite-time bounded with H ∞ performance. The conditions for the state estimator design are derived by constructing a Lyapunov function associated with the singular perturbation parameter, the system mode and two sets of IDVs. A numerical example validates the effectiveness of the proposed method and the superiority of the designed DETMs.},
  archive      = {J_NEUCOM},
  author       = {Jinrong Fan and Niewen Xu and Xiongbo Wan and Leimin Wang},
  doi          = {10.1016/j.neucom.2025.131807},
  journal      = {Neurocomputing},
  month        = {1},
  pages        = {131807},
  shortjournal = {Neurocomputing},
  title        = {Finite-time asynchronous state estimation for two-time-scale complex networks with sojourn probabilities and event-based AF relay protocols},
  volume       = {659},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). FEMTL-DR: A feature-enhanced multi-task learning model for flexible drug recommendation. <em>NEUCOM</em>, <em>659</em>, 131806. (<a href='https://doi.org/10.1016/j.neucom.2025.131806'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multiple disease-based syndrome reasonings and their corresponding drug recommendations are crucial for personalized diagnosis and treatment in Traditional Chinese Medicine (TCM). However, it remains a challenging task to effectively extract and integrate various entities and multi-dimensional relationships for syndrome-based drug recommendations. This study investigates FEMTL-DR, a novel feature-enhanced multi-task learning model for flexible drug recommendation. Based on various entity characteristics, we propose a hybrid multi-entity encoding strategy to realize diverse disease, syndrome, herb, and herb property encodings. Then, the pairwise similarity between different entity encodings is calculated to reconstruct the adjacency matrix of the heterogeneous graph. A state-space transformer-based strategy is presented to enhance node features by capturing long-range dependencies and extracting local and global information. Finally, a multi-task learning framework combined with a graph neural network and transformer module is constructed to learn the relationships between enhanced node features for syndrome classification and drug recommendation. Taking Reflux Esophagitis (RE) as an instance, a series of experiments have been conducted to verify the performance of the proposed FEMTL-DR, including comparison experiments, ablation verification, and parameter sensitivity tests. The experimental results indicate that the proposed model outperforms baselines on representative evaluation metrics. Specifically, the overall evaluation (integrating syndrome classification and drug recommendation) achieves improvements of at least 2.48 % in Average Precision (AP), 2.93 % in Precision, 2.77 % in Recall, and 3.08 % in F1-score. This study provides a novel solution for personalized diagnosis and treatment for RE in TCM and can be considered a paradigm for other diseases.},
  archive      = {J_NEUCOM},
  author       = {Junyang Leng and Yin Zhang and Fang Hu and Meng Zhang and Pin-Han Ho},
  doi          = {10.1016/j.neucom.2025.131806},
  journal      = {Neurocomputing},
  month        = {1},
  pages        = {131806},
  shortjournal = {Neurocomputing},
  title        = {FEMTL-DR: A feature-enhanced multi-task learning model for flexible drug recommendation},
  volume       = {659},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Feature distribution learning based on variance transfer and center shift for long-tailed classification. <em>NEUCOM</em>, <em>659</em>, 131805. (<a href='https://doi.org/10.1016/j.neucom.2025.131805'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In real-world scenarios, data distributions often exhibit a long-tailed nature: a few categories (head classes) contain the majority of samples, while the remaining categories (tail classes) suffer from severe data scarcity. Due to the limited number of samples, the observed distributions of tail classes often fail to capture their true underlying distributions, resulting in substantially degraded generalization performance. We observe a strong negative correlation between the distances of feature centers across categories and the similarity of their covariance matrices in the feature space. Motivated by this finding, we propose estimating the true distributional shapes of tail classes by leveraging the covariance structures of head classes. Moreover, our experiments reveal that the observed feature centers of tail classes are frequently biased away from their true centers. Interestingly, this bias direction tends to align closely—with high cosine similarity—with the direction pointing toward the feature center of the nearest head class. Building on this observation, we predict and apply such bias shifts to better approximate the true feature distributions, thereby refining the decision boundaries. This approach leads to a substantial improvement in the generalization performance of tail classes.},
  archive      = {J_NEUCOM},
  author       = {Chenxi Hong and Qiang Zhao and Min He and Tao Tan and Chenggang Yan},
  doi          = {10.1016/j.neucom.2025.131805},
  journal      = {Neurocomputing},
  month        = {1},
  pages        = {131805},
  shortjournal = {Neurocomputing},
  title        = {Feature distribution learning based on variance transfer and center shift for long-tailed classification},
  volume       = {659},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Data driven modeling and stability analysis for stochastic interconnected systems with uncertainties. <em>NEUCOM</em>, <em>659</em>, 131804. (<a href='https://doi.org/10.1016/j.neucom.2025.131804'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper proposes a deep neural network (DNN)-augmented inverse optimal adaptive control for stabilizing stochastic interconnected systems, focusing on challenges posed by unmodeled dynamics and uncertainties. A structured small-gain framework is developed to address the mutual dependencies among interconnected subsystems and to ensure the input-to-state practical stability in probability. To approximate unmodeled nonlinearities, a DNN-based identifier is integrated into each subsystem, allowing real-time estimation of uncertain dynamics while preserving the analytical tractability of the controller. Through this idea, we develop an adaptive controller that guarantees uniform boundedness of the interconnected system and ensures state convergence within a small neighborhood of the origin while optimizing overall system performance. Finally, an automobile suspension system is presented to demonstrate how the proposed approach effectively achieves closed-loop stability for stochastic interconnected systems while incorporating inverse optimal control design.},
  archive      = {J_NEUCOM},
  author       = {Yu Shao and Xinyu Zhang and Runzi Liao and Shihua Li and Rongjie Liu},
  doi          = {10.1016/j.neucom.2025.131804},
  journal      = {Neurocomputing},
  month        = {1},
  pages        = {131804},
  shortjournal = {Neurocomputing},
  title        = {Data driven modeling and stability analysis for stochastic interconnected systems with uncertainties},
  volume       = {659},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Quasi-bipartite synchronization of a new stochastic impulsive reaction-diffusion network by self-triggered control approach. <em>NEUCOM</em>, <em>659</em>, 131801. (<a href='https://doi.org/10.1016/j.neucom.2025.131801'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This note aims to develop a quasi-bipartite synchronization control strategy for stochastic reaction-diffusion networks via self-triggered control technology. Based on the information of past triggered moments of nodes, a new self-triggered impulsive controller is designed, and the Zeno behavior is avoided. The impulse control rate is proposed, and a self-triggered mechanism to determine the timing of impulses is provided, taking into account the Zeno behavior at the triggering moment, addressing practical challenges in fields such as mechanical engineering and aerial vehicles. On this basis, parameter mismatch and quasi-synchronization conditions of the network are fully considered, which can be applied to multi-node nonuniform synchronization systems. Furthermore, sufficient conditions and detailed proof process for quasi-bipartite synchronization of stochastic reaction-diffusion networks are provided. Finally, the correctness of the theoretical results of quasi-bipartite synchronization is verified by means of two numerical examples.},
  archive      = {J_NEUCOM},
  author       = {Minghui Song and Yonggui Kao and Wei Xie and Chuntao Shao and Xinsong Yang},
  doi          = {10.1016/j.neucom.2025.131801},
  journal      = {Neurocomputing},
  month        = {1},
  pages        = {131801},
  shortjournal = {Neurocomputing},
  title        = {Quasi-bipartite synchronization of a new stochastic impulsive reaction-diffusion network by self-triggered control approach},
  volume       = {659},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). RA2Net: Rotated alignment and aggregation network for oriented object detection in aerial images. <em>NEUCOM</em>, <em>659</em>, 131798. (<a href='https://doi.org/10.1016/j.neucom.2025.131798'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Oriented object detection in aerial images presents unique challenges, including feature misalignment caused by rotation-agnostic convolutional backbones, severe scale and aspect ratio imbalances among densely packed objects, and instability in loss optimization due to angular discontinuity. Traditional horizontal detectors fail to capture orientation variance, while existing rotated detection methods often struggle with aligning rotated sampling points or applying uniform penalties regardless of object geometry. In this paper, we propose RA 2 Net (Rotated Alignment and Aggregation Network), a novel framework that dynamically aligns features and adaptively optimizes loss for robust oriented object detection. First, the Rotated Point Alignment (RPA) module addresses feature misalignment by predicting oriented bounding boxes (OBBs) and refining convolutional sampling points via rotated cross convolution, enabling rotation-equivariant feature extraction. Second, the Rotated Feature Aggregation (RFA) module integrates dual-branch attention to fuse multi-scale local and global features, selectively enhancing informative regions while suppressing background noise. Third, we introduce an Adaptive Geometric-aware Loss (AGL) that combines thermodynamic energy modeling for small objects with polar coordinate regression for elongated objects, thereby stabilizing training and improving localization accuracy. Extensive experiments on aerial object detection benchmarks demonstrate that RA 2 Net not only achieves high precision but also maintains robustness across diverse object scales and orientations.},
  archive      = {J_NEUCOM},
  author       = {Min Dang and Qijie Xu and Gang Liu and Hao Li and Xu Wang},
  doi          = {10.1016/j.neucom.2025.131798},
  journal      = {Neurocomputing},
  month        = {1},
  pages        = {131798},
  shortjournal = {Neurocomputing},
  title        = {RA2Net: Rotated alignment and aggregation network for oriented object detection in aerial images},
  volume       = {659},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Evidence ratio classifier: A one-pass model for fast incremental learning. <em>NEUCOM</em>, <em>659</em>, 131797. (<a href='https://doi.org/10.1016/j.neucom.2025.131797'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We introduce the Evidence Ratio Classifier (ERC), a one-pass learning rule in which every weight is a closed-form ratio of empirical co-occurrence counts. Training therefore reduces to a single linear scan of the data, and inference to a few table look-ups followed by a winner-takes-all comparison; no back-propagation, learning-rate tuning, or other iterative optimisation is required. ERC is benchmarked on four standard tabular datasets— Adult Income , Mushroom , Iris , and Credit-Card Fraud . On Adult ( 32 561 training instances) it reaches 85.1 % accuracy versus 85.9 % for a variational Bayesian neural network and 82.1 % for Gaussian Naive Bayes, while requiring about 1700 × fewer arithmetic operations end-to-end and about 11 × fewer at inference. Across the other tasks ERC matches or exceeds Naive Bayes and stays within one percentage point of the variational baseline, with a 4–8 × lower arithmetic cost on smaller datasets. Because ERC stores, for every input pattern R , the empirical conditional probability P ( class ∣ R ) , its lookup table is self-explanatory: each entry quantifies the data support for the rule R → class . This intrinsic transparency is attractive wherever auditability is required, e.g., credit scoring and anti-money-laundering (Basel III; EU AI Act, Art. 13), medical decision support (FDA SaMD guidance), insurance underwriting, or judicial risk assessment. Model updates consist solely of incrementing integer counters; no gradients or floating-point arithmetic are involved. Such integer-only updates execute on processors that lack a hardware FPU or must remain fully deterministic, as found in ultra-low-power micro-controllers, hard real-time UAV and robotic controllers, or certified safety-critical systems (DO-178C avionics, CENELEC EN 50,128 railway, IEC 62,304 medical implants). ERC thus combines accuracy, explainability, and hardware efficiency in a single, making it deployable on resource–constrained hardware.},
  archive      = {J_NEUCOM},
  author       = {F.L.Olivier Manette},
  doi          = {10.1016/j.neucom.2025.131797},
  journal      = {Neurocomputing},
  month        = {1},
  pages        = {131797},
  shortjournal = {Neurocomputing},
  title        = {Evidence ratio classifier: A one-pass model for fast incremental learning},
  volume       = {659},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Designing a hybrid optimization methodology for delineating boundary of ultrasound prostate cancer with an explainable mathematical model. <em>NEUCOM</em>, <em>659</em>, 131794. (<a href='https://doi.org/10.1016/j.neucom.2025.131794'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Accurately segmenting prostate cancer (PCa) is crucial for enhancing male survival rates, yet it poses challenges due to low-intensity contrast around the PCa outline caused by intestinal gas interference, the existence of shadow artifact, the impact of the heterogeneity among different patients, and human anatomical diversities. Our study proposes a novel ultrasound (US)-guided hybrid optimization algorithm, containing two subnetworks: 1) the first subnetwork uses a deep parallel network structure to complete the initial segmentation stage automatically; 2) the second subnetwork is used to fine-tune the initial outcome, where the initial PCa outlines are optimized via an intelligent hunting polygon tracking method linked to a modified quantum-inspired evolutionary neural network. After the neural network’s training, an explainable mathematical mapping formula based on the optimal parameters of an evolutionary neural network is adopted to produce smooth PCa outlines. Experimental outcomes prove the superiority of our proposed methodology over other recent medical image segmentation approaches, achieving an average Dice score (DS), Jaccard index (JI), and accuracy (ACC) of 83.3 + 2.5 %, 82.6 + 3.2 %, and 83.2 + 2.7 %, respectively.},
  archive      = {J_NEUCOM},
  author       = {Tao Peng and Dehui Xiang and Zhongyi Zhang and Binbin Jiang and Baoqing Nie and Fei Shi and Derun Li and Caishan Wang and Weifang Zhu and Jing Cai and Enting Gao and Xinjian Chen},
  doi          = {10.1016/j.neucom.2025.131794},
  journal      = {Neurocomputing},
  month        = {1},
  pages        = {131794},
  shortjournal = {Neurocomputing},
  title        = {Designing a hybrid optimization methodology for delineating boundary of ultrasound prostate cancer with an explainable mathematical model},
  volume       = {659},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). OV-KFA: Open-vocabulary object detection via key feature alignment. <em>NEUCOM</em>, <em>659</em>, 131790. (<a href='https://doi.org/10.1016/j.neucom.2025.131790'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Open-vocabulary object detection (OVD) leverages vision-language models (VLMs) to extend detection capabilities beyond predefined categories, enabling flexible recognition in real-world open scenarios. However, existing OVD frameworks suffer from significant performance degradation when directly transferring VLM knowledge to detection tasks. This fundamental challenge arises because VLMs are trained to match whole images with textual descriptions, whereas object detection demands precise alignment between cropped image regions and corresponding text spans. Furthermore, during training on base classes, the model may learn to align irrelevant features, leading to overfitting issues that hinder generalization to novel categories. To address this fundamental alignment challenge, we propose Open-Vocabulary Object Detection via Key Feature Alignment (OV-KFA), which introduces two key innovations: (1) Bottleneck Adapter (BA), a lightweight plug-and-play module that distills key features and enhances cross-modal alignment, and (2) Transferable Prompt (TP), a novel training paradigm that learns generalizable prompt representations without architectural modifications. Our approach seamlessly improves open vocabulary detectors’ generalization to novel classes while preserving computational efficiency during inference. Extensive evaluations on the challenging OV-COCO and OV-LVIS benchmarks demonstrate substantial performance gains, validating the effectiveness of our approach. Code will be publicly available.},
  archive      = {J_NEUCOM},
  author       = {Yunqing Jiang and Sunyuan Qiang and Wuchen Li and Huijia Zhao and Yanyan Liang},
  doi          = {10.1016/j.neucom.2025.131790},
  journal      = {Neurocomputing},
  month        = {1},
  pages        = {131790},
  shortjournal = {Neurocomputing},
  title        = {OV-KFA: Open-vocabulary object detection via key feature alignment},
  volume       = {659},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Efficient guided diffusion toward adverse weather image restoration. <em>NEUCOM</em>, <em>659</em>, 131788. (<a href='https://doi.org/10.1016/j.neucom.2025.131788'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Image restoration under adverse weather conditions aims to recover clean scenes from degraded inputs. While existing CNN-based methods often yield unrealistic results as they prioritize minimizing pixel-level distortion, which has poor correlation with human perception. Although diffusion-based models achieve commendable perceptual quality, they encounter challenges in balancing quality and efficiency. To address these issues, we propose a weather-general guided diffusion model, namely WGDiff, for efficient and high-quality image restoration under adverse weather conditions. Specifically, a dual-branch transformer-based architecture is constructed with Direct Restoration Network (DRRN) and Diffusive Restoration Network (DFRN), where DRRN employs physics-aware weather-degradation model with explicit constraints for high fidelity restoration, while DFRN leverages conditional diffusion to iteratively generated results with high perceptual quality. Additionally, an error-reduction guided diffusive restoration method is proposed, which utilizes DRRN outputs as guidance and iteratively refines intermediate diffusion results of DFRN for high-quality restoration. Furthermore, an efficient shifted-window patch sampling strategy is developed to handle the trade-off between quality and efficiency for size-agnostic image restoration, significantly boosting the sampling speed and simultaneously alleviating blocking artifacts. Extensive experimental results on synthetic and real-world datasets demonstrate that our approach achieves state-of-the-art (SOTA) performance in generating more realistic restoration results and striking a superior balance between quality and efficiency. Notably, our method outperforms SOTA diffusion-based methods with a remarkable reduction of 44.8 %–96.5 % in parameters and a significant speed enhancement of 7.9–23.7 times. The source code will be released in https://github.com/TianhangTang/WGDiff .},
  archive      = {J_NEUCOM},
  author       = {Tang Tianhang and Chen Jie and Lv Shun and Lei Ling and Liu Yiguang},
  doi          = {10.1016/j.neucom.2025.131788},
  journal      = {Neurocomputing},
  month        = {1},
  pages        = {131788},
  shortjournal = {Neurocomputing},
  title        = {Efficient guided diffusion toward adverse weather image restoration},
  volume       = {659},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Feature representation learning for image denoising. <em>NEUCOM</em>, <em>659</em>, 131787. (<a href='https://doi.org/10.1016/j.neucom.2025.131787'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Image denoising plays a vital role in enhancing image quality for various downstream vision tasks by learning robust feature representations that distinguish clean signals from noise. Recent advances in deep learning have enabled data-driven feature extraction but frequently face challenges such as spatial-channel feature redundancy, suboptimal fusion of multi-level features, and over-smoothing due to pixel-wise loss functions. To address these interconnected issues, this paper proposes the Perceptual Feature Learning Network (PFLN), a lightweight architecture explicitly designed for efficient, discriminative feature learning. PFLN introduces a Redundancy Filtering Block (RFB) to suppress redundant information, a Selective Attention Fusion Block (SAFB) to adaptively integrate complementary features, and multi-level feature constraints combining pixel, perceptual, and spatial losses for holistic optimization. Experiments demonstrate that PFLN effectively learns compact, context-aware representations that improve denoising fidelity and texture preservation while maintaining computational efficiency, providing a balanced solution for real-world image denoising tasks.},
  archive      = {J_NEUCOM},
  author       = {Yuxuan Hu and Shichao Zhang},
  doi          = {10.1016/j.neucom.2025.131787},
  journal      = {Neurocomputing},
  month        = {1},
  pages        = {131787},
  shortjournal = {Neurocomputing},
  title        = {Feature representation learning for image denoising},
  volume       = {659},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). DSRJR: A monitoring substitution framework via dual-stream reconstruction and joint representation for fault diagnosis. <em>NEUCOM</em>, <em>659</em>, 131785. (<a href='https://doi.org/10.1016/j.neucom.2025.131785'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Rotary machinery functions in harsh environments and is susceptible to failure, so dependable fault diagnosis methods are essential to ensure equipment safety. Most existing methods assume that the sensor types of inputs are consistent in the training and testing datasets. However, in practice, sensor failure or the lack of corresponding sensors installed on the equipment can cause the model to fail due to incomplete input information, which ultimately leads to monitoring interruption. Therefore, developing an effective diagnostic knowledge transfer mechanism is essential. To address fault monitoring interruptions caused by the absence of the single sensor, this study proposes a monitoring substitution framework via dual-stream reconstruction and joint representation for fault diagnosis. Dual-stream joint alignment framework (DSJA) designs unique feature extraction models according to the characteristics of each signal, and introduces locality-sensitive latent diffusion space (LSLDS) for feature reconstruction. Building on this foundation, joint representation of feature consistency and domain invariance is developed to effectively map and align feature spaces across different modalities. The monitoring-substitution of the two signals is validated using gradient-based class activation map (Grad-CAM) visualization, and the diagnostic and monitoring-substitution performance of the proposed method is assessed across four scenarios. Compared with existing methods, the proposed approach demonstrates superior fault diagnosis and monitoring-substitution capabilities.},
  archive      = {J_NEUCOM},
  author       = {Qin Han and Li Jin and Nan Li and Hui Shi and Xiaoyin Nie and Gang Xie and Haifeng Yang},
  doi          = {10.1016/j.neucom.2025.131785},
  journal      = {Neurocomputing},
  month        = {1},
  pages        = {131785},
  shortjournal = {Neurocomputing},
  title        = {DSRJR: A monitoring substitution framework via dual-stream reconstruction and joint representation for fault diagnosis},
  volume       = {659},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Variation-aware proxy learning for semantic segmentation. <em>NEUCOM</em>, <em>659</em>, 131783. (<a href='https://doi.org/10.1016/j.neucom.2025.131783'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In semantic segmentation, accurately modeling intra-class variation is essential for capturing fine-grained details and resolving ambiguity near class boundaries. While existing proxy-based embedding methods represent each class with a single prototype, they struggle to reflect diverse intra-class structures, especially in complex scenes. In this paper, we propose a novel representation learning framework called Variation-Aware Proxy Learning, which introduces a representative proxy to encode shared class semantics and multiple variation vectors to capture fine-grained intra-class variations. These components are integrated through a factorized similarity score, enabling more expressive and discriminative embedding structures. To further enhance learning in ambiguous regions, we introduce focal modulation and design a new Compositional Similarity Loss composed of attraction and repulsion terms that adaptively amplify the contribution of hard examples. Our method is model-agnostic and requires no additional inference-time cost. Extensive experiments across multiple segmentation benchmarks—Cityscapes, COCO-Stuff10k, iSAID and ADE20K—and diverse backbones including CNNs and Transformers, demonstrate consistent improvements in mIoU and boundary precision, particularly in challenging regions with high intra-class variability.},
  archive      = {J_NEUCOM},
  author       = {Haejun Bae and Byung Cheol Song},
  doi          = {10.1016/j.neucom.2025.131783},
  journal      = {Neurocomputing},
  month        = {1},
  pages        = {131783},
  shortjournal = {Neurocomputing},
  title        = {Variation-aware proxy learning for semantic segmentation},
  volume       = {659},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Low coupling and high interaction dual-branch contrastive pseudo supervision for semi-supervised medical image segmentation. <em>NEUCOM</em>, <em>659</em>, 131779. (<a href='https://doi.org/10.1016/j.neucom.2025.131779'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Fully supervised medical image segmentation methods have demonstrated remarkable success in clinical applications. However, their performance heavily relies on large volumes of high-quality annotated data. The annotation of medical images is both labor-intensive and prohibitively expensive, significantly limiting the scalability and practical applicability of fully supervised approaches. To overcome this limitation, semi-supervised learning (SSL) techniques utilize abundant unlabeled data together with a limited quantity of labeled data, thereby reducing dependence on annotations. However, current SSL-based segmentation methods face major challenges, including noisy pseudo-labels and insufficient feature space supervision, which ultimately degrade segmentation accuracy. To address these challenges, we propose a novel semi-supervised segmentation method, Low Coupling and High Interaction Dual-Branch Contrastive Pseudo Supervision (DBCPS). We design a low-coupling, high-interaction dual-branch architecture combined with a cross-consistency pseudo supervision strategy to mitigate pseudo-label noise and improve their reliability. To enhance feature representation, we incorporate a multi-scale attention aggregation module that integrates spatial and channel attention to effectively fuse contextual features across scales. Deep supervision is also applied at multiple depths to facilitate hierarchical learning. In addition, we propose a novel voxel-level boundary perception contrastive loss that improves inter-class separability, enhances intra-class compactness, and refines boundary delineation. Experiments on four public medical image datasets show that the proposed method outperforms state-of-the-art semi-supervised segmentation approaches, demonstrating its effectiveness. Future work may explore its adaptation to multi-modal data or integration with active learning strategies. The source code is available at https://github.com/UncleTom09/DBCPS.git .},
  archive      = {J_NEUCOM},
  author       = {Changlong Yu and Yunfeng Zhang and Rui Zhang and Fangxun Bao and Huijian Han},
  doi          = {10.1016/j.neucom.2025.131779},
  journal      = {Neurocomputing},
  month        = {1},
  pages        = {131779},
  shortjournal = {Neurocomputing},
  title        = {Low coupling and high interaction dual-branch contrastive pseudo supervision for semi-supervised medical image segmentation},
  volume       = {659},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). YOLO-FOD: Lightweight object detection based on multibranch and multiscale feature fusion for adverse weather. <em>NEUCOM</em>, <em>659</em>, 131778. (<a href='https://doi.org/10.1016/j.neucom.2025.131778'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Object detection plays a critical role in enhancing traffic safety and operational efficiency. However, existing methods often degrade in performance when confronted with low-quality images captured under adverse weather conditions (e.g., fog, snow, and rain), due to reduced visibility and feature ambiguity. To address these challenges, this paper proposes a lightweight real-time detection model, You Only Look Once for Foggy Object Detection (YOLO-FOD), specifically designed for road traffic applications in harsh environments. Architecturally, we introduce the Online Scaling Diverse Branch Block with Efficient Layer Aggregation Network (OSDBBELAN) module, which enhances feature extraction while minimizing computational cost. To mitigate information loss during downsampling in complex scenes, we design the Space-to-Depth with Depthwise Separable Convolution (SPD-DSC) structure. Furthermore, a novel Global Information Sharing Module with Depthwise Separable Convolution (GISM-DSC) is developed for the neck to strengthen multiscale feature fusion. In addition, we propose the F-EIoU loss function to improve bounding box regression and accelerate convergence under adverse weather conditions. Extensive experiments demonstrate that YOLO-FOD maintains its lightweight design while outperforming state-of-the-art lightweight detectors. Compared with the baseline model, YOLO-FOD achieves a 40.5 % reduction in GFLOPs and a 4.1 % increase in mAP 50 on the RTTS dataset. The proposed model also delivers notable accuracy gains across multiple challenging environmental datasets, providing an effective and efficient solution for real-time object detection in traffic management systems under severe weather conditions.},
  archive      = {J_NEUCOM},
  author       = {Yan Liu and Tongyang Yuan and Aifeng Ren and Youyu Kuo and Xiangrui Xiong},
  doi          = {10.1016/j.neucom.2025.131778},
  journal      = {Neurocomputing},
  month        = {1},
  pages        = {131778},
  shortjournal = {Neurocomputing},
  title        = {YOLO-FOD: Lightweight object detection based on multibranch and multiscale feature fusion for adverse weather},
  volume       = {659},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). DiM: Improving multivariate time series forecasting with DI embedding and multi-head graph learning mechanism. <em>NEUCOM</em>, <em>659</em>, 131777. (<a href='https://doi.org/10.1016/j.neucom.2025.131777'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Accurate multivariate time series (MTS) forecasting is crucial for applications in traffic planning, energy management, financial investment, and healthcare. The challenge of forecasting MTS lies in managing the complex temporal dynamics and inter-channel relationships. However, despite the progress achieved by previous studies, they still fall short of adeptly addressing these complexities, leaving substantial scope for further refinement. To fill this gap, this paper proposes DiM, which seamlessly integrates the difference-inverted (DI) embedding strategy and the multi-head graph learning mechanism within the Metaformer framework. Specifically, the DI embedding employs a straightforward differencing operation to compensate for the limitations of previous inverted embedding methods in capturing temporal dynamics, thereby enhancing the model’s ability to discern temporal patterns without significantly increasing computational complexity. The multi-head graph learning mechanism dynamically adjusts multiple graph structures to better represent the evolving relationships between channels, surpassing the constraints of static graph structures typically used in GNNs. The effectiveness of the DiM has been validated across eleven public datasets, where it achieved the best results in 53 out of 55 mean absolute error metrics compared to existing state-of-the-art models, establishing a new benchmark in MTS forecasting. Code is available at https://github.com/Yipengmo/DiM .},
  archive      = {J_NEUCOM},
  author       = {Yipeng Mo and Haoxin Wang and Zuhua Yao and Chengteng Yang and Bixiong Li and Yiping Jiang and Songhai Fan and Site Mo},
  doi          = {10.1016/j.neucom.2025.131777},
  journal      = {Neurocomputing},
  month        = {1},
  pages        = {131777},
  shortjournal = {Neurocomputing},
  title        = {DiM: Improving multivariate time series forecasting with DI embedding and multi-head graph learning mechanism},
  volume       = {659},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). A lightweight dual path kolmogorov-arnold convolution network for medical optical image segmentation. <em>NEUCOM</em>, <em>659</em>, 131776. (<a href='https://doi.org/10.1016/j.neucom.2025.131776'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Medical optical image segmentation (MOIS) significantly aids physicians in diagnosing and monitoring diseases by providing valuable information about the necessary anatomical regions. However, MOIS frequently incurs high computational costs because of its complex morphology, rich edge information, and high resolution. To this end, we propose a lightweight dual-path architecture U-Net called KAU-Net. The newly proposed network achieves reuse and re-exploration of historical information through information interaction between image feature paths and historical information paths, enabling deep layers to learn comprehensive features. To satisfy the lightweight requirements of MOIS, Kolmogorov-Arnold convolution (KAConv) is used to replace traditional convolution, thereby obtaining more information while greatly reducing computational costs. Based on KAConv, we further propose a dual-path information fusion (DPIF) module to improve segmentation accuracy. This module establishes information fusion mechanisms across paths, network layers, and stages, enabling KAU-Net to fully utilize and re-explore historical information. To further enhance the information extracted by the dual-path encoder, a multi-scale information fusion (MSIF) module is designed, effectively connecting the encoder and decoder. Compared to the state-of-the-art approach, KAU-Net obtains IoU metrics of 85.32 %, 85.02 %, 82.87 %, and 63.15 % on the four tasks and achieves Dice scores of 93.84 %, 91.25 %, 90.12 %, and 85.10 %, respectively. Moreover, only 15.15 M parameters, 1.72 G FLOPs and 504 M memory are required. The proposed method has the advantages of being lightweight, versatile, and highly accurate in segmentation. Our study provides a reference solution for the lightweighting of MOIS models.},
  archive      = {J_NEUCOM},
  author       = {Jun Yuan and Lijun Zhou and Meirong He and Changyu Luo and Junran Zhang},
  doi          = {10.1016/j.neucom.2025.131776},
  journal      = {Neurocomputing},
  month        = {1},
  pages        = {131776},
  shortjournal = {Neurocomputing},
  title        = {A lightweight dual path kolmogorov-arnold convolution network for medical optical image segmentation},
  volume       = {659},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Centipede optimization algorithm: Best-of-L local search with contractive consensus for constrained engineering optimization. <em>NEUCOM</em>, <em>659</em>, 131774. (<a href='https://doi.org/10.1016/j.neucom.2025.131774'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present the Centipede Optimization Algorithm (CeOA), an operator-scheduled metaheuristic that composes a per-agent best-of-L Gaussian local search with an explicit contractive consensus under box clipping for constrained engineering optimization. CeOA pairs two generic operators in a distinct per-iteration schedule: (i) a multi-proposal, best-of-L Gaussian local search per agent; and (ii) a contractive consensus step that averages each agent with the current elite under bound clipping. A brief first-order rationale (Appendix A) explains why best-of-L proposals raise expected improvement and why the consensus map is non-expansive, helping stabilize feasibility with death-penalty constraint handling. We evaluate CeOA on six standard design benchmarks (spring, welded beam, pressure vessel, cantilever beam, three-bar truss, tubular column) and on 10 representative CEC-2017 functions spanning unimodal, multimodal, hybrid, and composition families, under equal evaluation budgets. We also include four conventional evolutionary baselines GA, DE, PSO and ES. Statistical significance is assessed via paired Wilcoxon tests with Holm correction. Compared with established methods (GWO, SSA) and recent heuristics (SMA, GBO), CeOA attains higher solution quality and faster convergence, with robustness confirmed by statistical tests. The operator-level design multi-proposal search followed by a contractive consensus balances exploitation with steady progress under constraints and requires few hyperparameters. These capabilities position CeOA as a practical tool for constrained design. Future work will extend CeOA to multi-objective and dynamic settings and evaluate its scalability on higher-dimensional benchmarks, or explore its hybridization with other computational intelligence methods.},
  archive      = {J_NEUCOM},
  author       = {Mohammad Mohammadi and Yousef Bazargan Lari and Kimia Bazargan Lari},
  doi          = {10.1016/j.neucom.2025.131774},
  journal      = {Neurocomputing},
  month        = {1},
  pages        = {131774},
  shortjournal = {Neurocomputing},
  title        = {Centipede optimization algorithm: Best-of-L local search with contractive consensus for constrained engineering optimization},
  volume       = {659},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). MSER: Multi-scale event representation model for enhanced spatio-temporal feature extraction. <em>NEUCOM</em>, <em>659</em>, 131773. (<a href='https://doi.org/10.1016/j.neucom.2025.131773'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Event cameras, designed to mimic biological perception principles, are a new type of visual sensor offering advantages such as low latency, low power consumption, and high dynamic range. However, the non-uniform, discontinuous nature of their spatially sparse yet temporally dense data presents significant challenges for extracting meaningful spatio-temporal features. Representing such novel data paradigms to provide high-quality inputs for neural network models remains a formidable task. Consequently, a key consideration is how to extract spatio-temporal features from event streams effectively. Currently, most work on event data typically performs feature extraction at either a single temporal or spatial scale, which often struggles to fully utilize the complex spatio-temporal relationships, leading to inaccurate capture of dynamic changes. Inspired by the brain’s multi-scale processing mechanisms, we propose the Multi-scale Event Representation (MSER) model to enable a comprehensive understanding of spatio-temporal information. Among these, Temporal Multi-scale Encoding (TMSE) leverages a logarithmic perception algorithm with an adaptive decay factor to capture temporal features across multiple scales, effectively establishing temporal correlations. Then Spatial Multi-scale Fusion strategy (SMSF) is designed to address spatial variations by employing a feature pyramid structure with a bottom-up strategy to extract and integrate multi-scale spatial features. We validate the proposed method on a series of publicly available neuromorphic datasets, including N-CARS, N-MNIST, N-Caltech101, CIFAR10-DVS, and DvsGesture. Experimental results demonstrate that the features we extract exhibit excellent expressiveness and generalization, achieving state-of-the-art performance.},
  archive      = {J_NEUCOM},
  author       = {Wanying Xu and Ping He and Xin Zhou and Rong Xiao and Chenwei Tang and Jiancheng Lv and Huajin Tang},
  doi          = {10.1016/j.neucom.2025.131773},
  journal      = {Neurocomputing},
  month        = {1},
  pages        = {131773},
  shortjournal = {Neurocomputing},
  title        = {MSER: Multi-scale event representation model for enhanced spatio-temporal feature extraction},
  volume       = {659},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Importance estimation of hyperparameters in reinforcement learning. <em>NEUCOM</em>, <em>659</em>, 131770. (<a href='https://doi.org/10.1016/j.neucom.2025.131770'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Hyperparameters play a critical role in enabling reinforcement learning (RL) agents to achieve high performance, yet their optimization remains computationally demanding. To address this challenge, we evaluate methodologies from the field of sensitivity analysis (SA) for assessing hyperparameter importance, thereby enabling more informed resource allocation through (I) hyperparameter prioritization, (II) hyperparameter fixation, and (III) hyperparameter mapping across the value space. Following a theoretical analysis of RL-specific characteristics and available methodologies from the field of SA, we identify functional ANOVA (fANOVA) as the most promising candidate. Our empirical investigation evaluates the validity, reliability, sample efficiency, and usability of fANOVA within the RL context. The results demonstrate its overall effectiveness in achieving goals I–III, while also highlighting limitations related to the sample efficiency and especially regarding the usage of data generated during hyperparameter optimization.},
  archive      = {J_NEUCOM},
  author       = {Dominic Weller and Maximilian Moll},
  doi          = {10.1016/j.neucom.2025.131770},
  journal      = {Neurocomputing},
  month        = {1},
  pages        = {131770},
  shortjournal = {Neurocomputing},
  title        = {Importance estimation of hyperparameters in reinforcement learning},
  volume       = {659},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). An in-depth look at approximation via deep and narrow neural networks. <em>NEUCOM</em>, <em>659</em>, 131769. (<a href='https://doi.org/10.1016/j.neucom.2025.131769'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In 2017, Hanin and Sellke showed that the class of arbitrarily deep, real-valued, feed-forward and ReLU-activated networks of width w forms a dense subset of the space of continuous functions on R n , with respect to the topology of uniform convergence on compact sets, if and only if w > n holds. To show the necessity, a concrete counterexample function f : R n → R was used. In this note we actually approximate this very f by neural networks in the two cases w = n and w = n + 1 around the aforementioned threshold. We study how the approximation quality behaves if we vary the depth and what effects (spoiler alert: dying neurons) cause that behavior.},
  archive      = {J_NEUCOM},
  author       = {Joris Dommel and Sven A. Wegner},
  doi          = {10.1016/j.neucom.2025.131769},
  journal      = {Neurocomputing},
  month        = {1},
  pages        = {131769},
  shortjournal = {Neurocomputing},
  title        = {An in-depth look at approximation via deep and narrow neural networks},
  volume       = {659},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Projection-based incomplete multi-view consensus bipartite graph representation learning. <em>NEUCOM</em>, <em>659</em>, 131766. (<a href='https://doi.org/10.1016/j.neucom.2025.131766'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Incomplete Multi-View Clustering (IMVC) has garnered significant attention in recent years due to its ability to effectively handle incomplete multi-view data. Although graph-based IMVC methods are widely adopted, the reliability of the obtained affinity graphs is often compromised by the presence of missing instances. Moreover, imputation-based methods are frequently adversely affected by redundant features or noise, which can distort data relationships and structures, ultimately leading to inaccurate clustering results and limiting their practical applicability. To address these challenges, we propose a novel method termed Projection-based Incomplete Multi-view Consensus Bipartite Graph Representation Learning (PIMV_CBG). Specifically, we integrate projection learning with consensus bipartite graph construction into a unified framework, where they mutually enhance each other to reduce the impact of noise and redundancy while strengthening cross-view interactions, leading to high-quality bipartite graph representations. Furthermore, imputation is performed in the clean subspace, regularized by graph constraints derived from the consensus structure, which improves imputation accuracy and fully exploits latent information. Extensive experiments on benchmark datasets have demonstrated the effectiveness of PIMV_CBG in addressing IMVC tasks, achieving state-of-the-art clustering performance in most cases. The source code is publicly available at https://github.com/superkeranbing/PIMV_CBG/tree/main .},
  archive      = {J_NEUCOM},
  author       = {Qiuyu Ji and Hui Huang and Liang Mao and Nan Zhang},
  doi          = {10.1016/j.neucom.2025.131766},
  journal      = {Neurocomputing},
  month        = {1},
  pages        = {131766},
  shortjournal = {Neurocomputing},
  title        = {Projection-based incomplete multi-view consensus bipartite graph representation learning},
  volume       = {659},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). A sparse dynamic graph transformer for traffic flow prediction. <em>NEUCOM</em>, <em>659</em>, 131765. (<a href='https://doi.org/10.1016/j.neucom.2025.131765'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Traffic flow prediction is a crucial component of Intelligent Transportation Systems (ITS), aimed at optimizing traffic management and enhancing urban mobility. The main challenges in traffic flow prediction are accurately modeling both long-term stability and short-term dynamic spatial-temporal dependencies in traffic data. Graph Neural Networks (GNNs) and attention mechanisms have demonstrated significant potential in addressing these challenges. However, most GNN-based models capture spatial correlations in a static manner, limiting their ability to represent the dynamic characteristics of urban traffic patterns. Moreover, the self-attention mechanism, which considers the entire data range, may cause oversmoothing. To this end, we propose a Sparse Dynamic Graph Transformer (SDGFormer) model for more accurate traffic flow prediction. By incorporating long-term historical data, the model better captures periodic patterns in traffic flow. A sparse dynamic adaptive graph convolution module is appropriately designed, which adopts an adaptive adjacency matrix to capture the long-term stable traffic structures and a dynamic adjacency matrix to model the short-term dynamic spatial dependencies, respectively. This module applies top- k selection operators with varying sparsity to retain the most critical nodes for more effective feature aggregation. Additionally, we propose a frequency-decoupled temporal attention module that employs Fourier transforms to decompose complex temporal components of traffic flow series, integrating low and high-frequency information to enhance sensitivity to short-term events and mitigate oversmoothing. Experiments on seven public datasets demonstrate that SDGFormer achieves state-of-the-art performance, especially the Mean Absolute Percentage Error (MAPE) value of the PeMS07 dataset experiment is improved by 9.7 %. The code is available at https://github.com/userTGQ/SDGFormer .},
  archive      = {J_NEUCOM},
  author       = {Guoqing Teng and Han Wu and Ao He and Yu Chen and Meng Zhao},
  doi          = {10.1016/j.neucom.2025.131765},
  journal      = {Neurocomputing},
  month        = {1},
  pages        = {131765},
  shortjournal = {Neurocomputing},
  title        = {A sparse dynamic graph transformer for traffic flow prediction},
  volume       = {659},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). EHIN: Early-aware hierarchical interaction network for weakly-supervised referring image segmentation. <em>NEUCOM</em>, <em>659</em>, 131764. (<a href='https://doi.org/10.1016/j.neucom.2025.131764'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Referring image segmentation (RIS) aims to segment target regions in images based on natural language descriptions. Although weakly-supervised RIS frameworks have been proposed to reduce reliance on costly manual annotations, their performance remains limited due to both the low quality of pseudo-labels and the inherent difficulty in achieving effective interaction between visual and textual features. In this paper, we propose a novel weakly-supervised framework named Early-aware Hierarchical Interaction Network (EHIN). The proposed network includes two key components, which are designed to enhance pseudo-label generation and improve the interaction between visual and textual features for RIS, respectively. First, EHIN incorporates an Early-aware Contrastive Learning Module (ECLM) that enhances feature discrimination by leveraging contrastive learning to distinguish target features from background noise. By integrating the module early into the processing pipeline, ECLM operates on raw image features directly, preserving richer visual details while reducing reliance on labeled data and thus improving the reliability of pseudo-labels. Second, EHIN integrates a Hierarchical Interaction Prompt Module (HIPM) to facilitate comprehensive interaction between visual and textual features and enhance subsequent feature fusion. Extensive experimental results on four benchmark datasets demonstrate that the proposed EHIN outperforms the state-of-the-art RIS. Code is available at https://github.com/CDUT-DBGroup/MFP-TRIS .},
  archive      = {J_NEUCOM},
  author       = {Hongjun Li and Nan Wang and Anqing Chen and Jiang Liu and Wanli Ma and Weide Liu and Yakun Ju and Paul L. Rosin and Hantao Liu and Wei Zhou},
  doi          = {10.1016/j.neucom.2025.131764},
  journal      = {Neurocomputing},
  month        = {1},
  pages        = {131764},
  shortjournal = {Neurocomputing},
  title        = {EHIN: Early-aware hierarchical interaction network for weakly-supervised referring image segmentation},
  volume       = {659},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). A semantic driven adaptive framework for few-shot knowledge graph completion. <em>NEUCOM</em>, <em>659</em>, 131763. (<a href='https://doi.org/10.1016/j.neucom.2025.131763'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Few-shot Knowledge Graph Completion (FKGC) aims to infer missing triples under unseen relations with minimal reference data. Existing methods predominantly rely on structural patterns while overlooking semantic context, leading to suboptimal performance on sparse or ambiguous entities. In this paper, we propose Semantic Driven Adaptive Framework (SeDA), a novel FKGC model that introduces dynamic semantic-structural synergy. SeDA first extracts semantic information by dynamically selecting diverse neighborhood triples and generating entity descriptions via large language models. The entity descriptions are encoded with BERT and fused with local structural features through a graph attention network (GAT), which performs adaptive aggregation by weighting neighboring entities and relations based on task-specific attention scores. SeDA introduces a semantic-driven negative sampling strategy that categorizes relations into exclusive and inclusive types, thereby enhancing discriminative learning through the dynamic interplay between relation semantics and structural patterns. The framework effectively bridges semantic gaps, offering a scalable solution for FKGC tasks in sparse real-world knowledge graphs. Experimental results on the frequently used benchmark datasets NELL-One and FB15k237-One demonstrate that SeDA significantly outperforms existing methods.},
  archive      = {J_NEUCOM},
  author       = {ChengJia OuYang and Tinghua Zhang and Weihao Yu and Jin Huang},
  doi          = {10.1016/j.neucom.2025.131763},
  journal      = {Neurocomputing},
  month        = {1},
  pages        = {131763},
  shortjournal = {Neurocomputing},
  title        = {A semantic driven adaptive framework for few-shot knowledge graph completion},
  volume       = {659},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). New intent discovery with syntactic structure masking pretraining and density-aware contrastive learning. <em>NEUCOM</em>, <em>659</em>, 131760. (<a href='https://doi.org/10.1016/j.neucom.2025.131760'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {New intent discovery remains a critical challenge in natural language understanding. Existing methods fail to adequately incorporate the syntactic characteristics of intent data, leading to insufficient capture of deep semantic features and imbalanced sentence embedding distributions. In these distributions, embeddings of similar intents tend to cluster closely, thereby reducing discovery accuracy. This paper proposes SSM-IntentBERT (Syntax Structure Masking strategy Intent Bidirectional Encoder Representations from Transformers) to address these limitations. SSM-IntentBERT is a novel intent discovery model that leverages syntax-aware masking pretraining. Capitalizing on the interrogative-dominated nature of intent data, we design a syntax structure masking strategy. This strategy selectively masks specific words during pretraining to learn sentence representations that encode both semantic and syntactic features. Additionally, we introduce density-aware contrastive learning to maximize the similarity between positive sample pairs and the differences between negative pairs, thereby adjusting the distribution of sentence embeddings to prevent the clustering of semantically similar intents and improve discovery accuracy. Experimental results on four datasets, including Banking (financial transactions), Stackoverflow (community Q&A), Mcid (COVID-19 healthcare), and HWU64 (21 domains), demonstrate that SSM-IntentBERT outperforms baseline models (DCN, SCCL, USNID, MTP-CLNN) with average improvements of 2.43 % in Adjusted Rand Index (ARI), 2.10 % in Accuracy (ACC), and 1.61 % in Normalized Mutual Information (NMI). The code for this research work is available at: https://github.com/lllforeverlll/SSM-IntentBERT .},
  archive      = {J_NEUCOM},
  author       = {Di Wu and Liming Feng and Xiaoyu Wang},
  doi          = {10.1016/j.neucom.2025.131760},
  journal      = {Neurocomputing},
  month        = {1},
  pages        = {131760},
  shortjournal = {Neurocomputing},
  title        = {New intent discovery with syntactic structure masking pretraining and density-aware contrastive learning},
  volume       = {659},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). SynapseHD: A unified training framework for bridging spiking neural networks and hyperdimensional computing. <em>NEUCOM</em>, <em>659</em>, 131757. (<a href='https://doi.org/10.1016/j.neucom.2025.131757'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Spiking Neural Networks (SNNs) and Hyperdimensional Computing (HDC) are two prominent brain-inspired computing paradigms with complementary characteristics. SNNs effectively simulate biological neural mechanisms to achieve powerful feature extraction but typically rely on complex, gradient-based training algorithms. Conversely, HDC models the brain at a more abstract, functional level, enabling gradient-free training at the cost of less sophisticated feature extraction capabilities. The potential for a synergistic integration of these two approaches remains largely unexplored. To bridge this gap, we introduce SynapseHD, a novel deep learning framework that harmonizes the strengths of SNNs and HDC. In our architecture, an SNN module first extracts low-level features from the spatio-temporal correlations within data. These features are then mapped into a high-dimensional space by an HDC module to establish memory representations for classification. A core contribution of our work is a novel unbalanced co-training strategy that orchestrates the learning paradigm, temporal scheduling, loss transmission, and weight updating to ensure efficient and stable joint training. Experimental results demonstrate the superiority of our proposed framework. First, on the CIFAR-10 dataset, SynapseHD reduces training epochs by 30 % and improves accuracy by a remarkable 40 % compared to state-of-the-art HDC works. Furthermore, when applied to the more challenging task of few-shot class-incremental learning, SynapseHD surpasses existing methods by delivering higher accuracy with significantly fewer parameters and lower computational complexity, while maintaining a gradient-free process.},
  archive      = {J_NEUCOM},
  author       = {Lingfeng Zhou and Huiyao Wang and Bohan Wang and Jinghai Wang and Zhiyi Yu and Shanlin Xiao},
  doi          = {10.1016/j.neucom.2025.131757},
  journal      = {Neurocomputing},
  month        = {1},
  pages        = {131757},
  shortjournal = {Neurocomputing},
  title        = {SynapseHD: A unified training framework for bridging spiking neural networks and hyperdimensional computing},
  volume       = {659},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). ConDNS: A novel conditional diffusion-based negative sampling method for knowledge graph embedding. <em>NEUCOM</em>, <em>659</em>, 131751. (<a href='https://doi.org/10.1016/j.neucom.2025.131751'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Knowledge Graph Embedding (KGE) maps entities and relations into continuous vector spaces to facilitate link prediction tasks. Given the inherent inability of knowledge graphs to directly supply high-quality negative samples with multi-level difficulty, existing methods typically rely on post-sampling assessment strategies, which lack controllable generation of difficulty-calibrated negatives tailored to diverse KGE training requirements. To address these challenges, we propose ConDNS , a novel conditional diffusion-based negative sampling method for knowledge graph embedding. By adjusting the diffusion timestep, our model achieves dynamic difficulty modulation of synthetic negatives through global entity-relation information utilization. This enables generation of semantically valid samples that synergistically integrate with conventional samples, thereby overcoming single-strategy sampling bottlenecks and establishing a multiscale difficulty configuration. Experiments demonstrate that ConDNS achieves state-of-the-art performance across multiple benchmarks with minimal synthetic samples while functioning as a plug-and-play module compatible with mainstream KGE architectures. Source code is available at: https://github.com/zrj-wang/ConDNS .},
  archive      = {J_NEUCOM},
  author       = {Zhaorongjie Wang and Nan Li and Kai Chen and Aiping Li and Liqun Gao},
  doi          = {10.1016/j.neucom.2025.131751},
  journal      = {Neurocomputing},
  month        = {1},
  pages        = {131751},
  shortjournal = {Neurocomputing},
  title        = {ConDNS: A novel conditional diffusion-based negative sampling method for knowledge graph embedding},
  volume       = {659},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Sparse-view CT image reconstruction using conditional embedding fusion diffusion model. <em>NEUCOM</em>, <em>659</em>, 131748. (<a href='https://doi.org/10.1016/j.neucom.2025.131748'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The main goal of sparse-view CT reconstruction is to reconstruct high-quality images from fewer projection data, thereby reducing the radiation dose. However, the reduction in the amount of projection data also makes the reconstruction process ill-posed, leading to an increase in artifacts in the reconstructed image. Recently, diffusion models have shown strong generative capabilities, providing new ideas for sparse-view CT reconstruction. But due to the stochastic nature of the diffusion process, how to guide the generation process to achieve high-quality reconstruction remains a major challenge. To address this issue, we propose a Conditional Embedding Fusion Diffusion Model (CEF-DM) to improve reconstruction quality. Specifically, we design a FourierNet to generate an initial reconstruction, which serves as a condition to guide the CEF-DM in generating the remaining detail residuals. CEF-DM employs a conditional attention embedding module (CAEM) to comprehensively incorporate conditional input and time-step information throughout the generation process. The initial reconstruction is then summed with the residual details to obtain the final reconstruction. The experimental results show that our method outperforms existing methods in terms of reconstruction accuracy and image quality, providing an efficient and robust solution for sparse view CT reconstruction with potential clinical application value.},
  archive      = {J_NEUCOM},
  author       = {Chenchun Zhou and Yubao Sun and Jing Liang and Jia Liu and Qingshan Liu},
  doi          = {10.1016/j.neucom.2025.131748},
  journal      = {Neurocomputing},
  month        = {1},
  pages        = {131748},
  shortjournal = {Neurocomputing},
  title        = {Sparse-view CT image reconstruction using conditional embedding fusion diffusion model},
  volume       = {659},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Towards trustworthy and interpretable prediction of school bullying: A NAS-driven framework with shapley value explanation. <em>NEUCOM</em>, <em>659</em>, 131744. (<a href='https://doi.org/10.1016/j.neucom.2025.131744'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {School bullying represents a critical societal challenge with lasting psychological and academic consequences for affected students. Despite recent advances in machine learning for predicting bullying behaviors, conventional models struggle to capture the complex, non-linear relationships among contributing factors, especially under the imbalanced data distributions typical of real-world bullying cases. Furthermore, the inherent opacity of Deep Neural Networks (DNNs) restricts their application in educational contexts where interpretability and actionable insights are essential. In this paper, we propose a novel automated framework that integrates Neural Architecture Search (NAS) with Shapley value-based explanation methods to jointly address performance and interpretability challenges. Our framework automatically identifies optimal DNN architectures tailored for bullying prediction, incorporating mechanisms to handle class imbalance without extensive manual tuning. To address model transparency, we employ a Shapley value analysis pipeline that systematically attributes predictions to key risk factors, offering educators and policymakers principled and quantitative insights. Extensive experiments on publicly available datasets demonstrate that our method significantly outperforms state-of-the-art baselines, achieving notable improvements in Accuracy (+2.58 %), F1-Score (+34.52 %), and AUC (+6.47 %). Importantly, the feature importance rankings from our Shapley analysis closely align with established sociological and educational theories on bullying, affirming the model’s interpretability and practical relevance. Cross-dataset validation further verifies the framework’s generalizability to broader youth behavioral risk prediction tasks. Our code is submitted at https://github.com/clsyc/Bullyingshapley .},
  archive      = {J_NEUCOM},
  author       = {Wei Qu and Cong Chen and Wei Lu and Haodong Chen and Tao Li},
  doi          = {10.1016/j.neucom.2025.131744},
  journal      = {Neurocomputing},
  month        = {1},
  pages        = {131744},
  shortjournal = {Neurocomputing},
  title        = {Towards trustworthy and interpretable prediction of school bullying: A NAS-driven framework with shapley value explanation},
  volume       = {659},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). SVP: Stratified vertical priors for LiDAR-based 3D object detection. <em>NEUCOM</em>, <em>659</em>, 131737. (<a href='https://doi.org/10.1016/j.neucom.2025.131737'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Three-dimensional (3D) object detection is a fundamental perceptual task in autonomous driving, attracting significant attention from both academia and industry. Recent pillar-based 3D object detection algorithms have made notable progress in reducing model inference time. However, the detection performance remains unsatisfactory due to the loss of vertical dimension information. In this paper, we propose a stratified vertical priors framework to improve the pillar-based 3D object detection algorithms. Specifically, we first extract the stratified vertical information and horizontal information of the object from the point cloud. Subsequently, we design a parallel feature extraction architecture to extract the vertical features and horizontal features, respectively. A shared information fusion module is designed to extract and fuse the key features of the vertical and horizontal feature maps. Finally, we fuse vertical features into horizontal features at multiple scales to enhance the representation of horizontal features. Extensive experiments on KITTI and nuScenes datasets demonstrate that our proposed algorithm significantly improves the performance of pillar-based 3D object detection algorithms. Remarkably, it improves PointPillars by 3.45 % and 3.83 % mAP on the KITTI validation and test datasets, respectively. Code is available at https://github.com/1064783536/SVP .},
  archive      = {J_NEUCOM},
  author       = {Lei Ao and Wenkang Wan and Nan Ouyang and Jianzhao Li and Qingqing Li and Maoguo Gong and Kai Sheng},
  doi          = {10.1016/j.neucom.2025.131737},
  journal      = {Neurocomputing},
  month        = {1},
  pages        = {131737},
  shortjournal = {Neurocomputing},
  title        = {SVP: Stratified vertical priors for LiDAR-based 3D object detection},
  volume       = {659},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Facial sketch synthesis with multi-level guided latent diffusion model. <em>NEUCOM</em>, <em>659</em>, 131734. (<a href='https://doi.org/10.1016/j.neucom.2025.131734'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a facial sketch synthesis (FSS) model to generate multi-style facial sketches from photos. Recent FSS methods often rely on Generative Adversarial Networks (GANs) due to their excellent performance in generating realistic images. However, many of these methods are limited to single-style sketches, and some methods have weaknesses in abstract expression of content according to the sketch style. The great difference between photos and sketches, along with limited training samples, makes it challenging to ensure high fidelity to both content and style. Diffusion models (DMs) not only generate high-quality images comparable to those produced by GANs but also offer superior mode coverage. Inspired by this, we propose a Multi-Level Guided Latent Diffusion Model (MLGLDM). We first train a photo autoencoder and a sketch autoencoder to encode photos into the latent space and decode sketches from the latent space, respectively. A diffusion model is then applied in the learned latent space. To improve the fidelity of synthesized sketches, we introduce a Multi-Level Guidance (MLG) mechanism, which enhances the guiding effect of photos by incorporating a Conditional Information Enhancement (CIE) module into the U-Net within the latent diffusion model. Additionally, we propose a region-adaptive loss function to synthesize more precise and detailed sketches by focusing on the more challenging regions. Furthermore, we adopt a three-stage training strategy to address the challenge of limited training samples. Extensive experiments on the FS2K dataset demonstrate that our method achieves comparable or better performance beyond the state-of-the-art methods both qualitatively and quantitatively.},
  archive      = {J_NEUCOM},
  author       = {Dan Lu and Zhenxue Chen and Chengyun Liu and Yuchen Hu and Q.M.Jonathan Wu},
  doi          = {10.1016/j.neucom.2025.131734},
  journal      = {Neurocomputing},
  month        = {1},
  pages        = {131734},
  shortjournal = {Neurocomputing},
  title        = {Facial sketch synthesis with multi-level guided latent diffusion model},
  volume       = {659},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Convex space learning for tabular synthetic data generation. <em>NEUCOM</em>, <em>659</em>, 131722. (<a href='https://doi.org/10.1016/j.neucom.2025.131722'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {While synthetic data for images and text has seen significant advancements, the structured nature of tabular data presents unique challenges in generating high-utility synthetic datasets while preserving privacy. In imbalanced learning, the Convex Space Learning (CSL) approach has been widely adopted, but it has not been explored for synthetic tabular data generation in general. Moreover, deriving convex spaces (or convex hulls) for individual rows typically relies on a single distance metric; however, applying a single metric directly across data with diverse feature types limits generation quality. To address these two research gaps, we propose NextConvGeN, an extension of the ConvGeN framework that generalizes CSL for entire tabular data generation. NextConvGeN employs a generator-discriminator architecture that uses deep cooperative learning, refining synthetic data generation within local convex data neighborhoods. These neighborhoods are accessed by a nonlinear dimension reduction technique with a feature-type specific similarity metric to handle diverse feature types. We then compare several state-of-the-art synthetic tabular data generation models to assess their performances qualitatively and quantitatively in the context of privacy-utility balance. Our results show that NextConvGeN prioritizes utility preservation while incorporating privacy measures, making it a promising tool for generating high-utility synthetic data for analysis. This work advances synthetic tabular data generation by expanding convex space learning beyond imbalanced classification, strengthening the theoretical foundation of synthetic data sampling, and providing a structured evaluation of utility-driven tabular data generation, especially in the biomedical domain.},
  archive      = {J_NEUCOM},
  author       = {Manjunath Mahendra and Chaithra Umesh and Kristian Schultz and Olaf Wolkenhauer and Saptarshi Bej},
  doi          = {10.1016/j.neucom.2025.131722},
  journal      = {Neurocomputing},
  month        = {1},
  pages        = {131722},
  shortjournal = {Neurocomputing},
  title        = {Convex space learning for tabular synthetic data generation},
  volume       = {659},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). PM-adapter: MoE based dynamic denoising fine-tuning for thermal infrared object detection. <em>NEUCOM</em>, <em>659</em>, 131718. (<a href='https://doi.org/10.1016/j.neucom.2025.131718'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Thermal infrared (TIR) technology has become an indispensable component in the community of image acquisition and processing. While TIR images contain rich thermal information, they lack texture details, rendering conventional object detectors based on visible imagery less effective under low-light conditions. Additionally, noise introduced by sensor limitations and environmental factors further degrades detection performance, increasing false positives and false negatives. To address these challenges, we propose a parameter-efficient dynamic denoising fine-tuning module named PM-Adapter ( P erona- M alik diffusion-based Adapter ). Specifically, we leverage the Perona-Malik diffusion equation in the form of discrete wavelet transform to denoise TIR features, and the edge enhancement unit to enhance edge details. More crucially, to adaptively handle the complex noise characteristics across TIR images, we introduce the Mixture of Experts mechanism to dynamically allocate the most appropriate denoising expert, further improving detection performance. Comprehensive experiments on the FLIR, LLVIP and M 3 FD datasets demonstrate that the proposed PM-Adapter consistently outperforms existing methods, validating its effectiveness of the proposed approach.},
  archive      = {J_NEUCOM},
  author       = {Haijun Liu and Jun Zhang and Hang Yu and Boya Wei and Jing Nie and Suju Li and Xichuan Zhou},
  doi          = {10.1016/j.neucom.2025.131718},
  journal      = {Neurocomputing},
  month        = {1},
  pages        = {131718},
  shortjournal = {Neurocomputing},
  title        = {PM-adapter: MoE based dynamic denoising fine-tuning for thermal infrared object detection},
  volume       = {659},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Enhanced 3D tumor synthesis and segmentation framework using multiscale diffusion and hybrid SAM-swin models across diverse anatomical datasets. <em>NEUCOM</em>, <em>659</em>, 131713. (<a href='https://doi.org/10.1016/j.neucom.2025.131713'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep learning-based tumor segmentation often focuses on optimizing network architectures in 3D medical images while overlooking the potential of anatomically realistic synthetic tumor data to enhance performance. This work proposes a novel framework integrating a multiscale attention diffusion model to synthesize high-fidelity tumors and a hybrid segmentation approach using the 3D SAM-Adapter and Swin Transformer. The diffusion model generates synthetic tumors that replicate real pathological features with accurate boundaries and textures across multiple resolutions. Our framework, evaluated on diverse datasets—including public brain and liver tumor datasets, a private pelvic dataset, and others—achieves state-of-the-art segmentation performance, with Dice Similarity Coefficients of 98.61 %, 88.60 %, and 91.93 % on BraTS, DLDS, and the pelvic dataset, respectively. Our method captures global anatomical context and fine-grained local details by leveraging prompt-driven segmentation and hierarchical refinement. This reduces burdens of manual annotations, improves accuracy in segmentation, and demonstrates good generalization performance across a diverse set of anatomical sites and imaging protocols.},
  archive      = {J_NEUCOM},
  author       = {Jincao Yao and Mudassar Ali and Wenjie Zheng and Jiaqi Hu and Jing Wang and Xingze Zou and Haoji Hu and Qiong Luo and Weizeng Zheng and Neng Jin and Dong Xu},
  doi          = {10.1016/j.neucom.2025.131713},
  journal      = {Neurocomputing},
  month        = {1},
  pages        = {131713},
  shortjournal = {Neurocomputing},
  title        = {Enhanced 3D tumor synthesis and segmentation framework using multiscale diffusion and hybrid SAM-swin models across diverse anatomical datasets},
  volume       = {659},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Enhancing neural combinatorial optimization by progressive training paradigm. <em>NEUCOM</em>, <em>659</em>, 131707. (<a href='https://doi.org/10.1016/j.neucom.2025.131707'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Neural Combinatorial Optimization (NCO) methods have garnered considerable attention, due to their effectiveness in automatic algorithm design for solving combinatorial optimization problems. Current constructive NCO methods predominantly employ a one-stage training paradigm using either reinforcement learning (RL) or supervised learning (SL). The one-stage training inevitably entails the computation-intensive labeling (i.e., solving optimal solutions) in SL or less-informative sparse rewards in RL. In this work, we propose a progressive training paradigm that pre-trains a neural network on small-scale instances using SL and then fine-tunes it using RL. In the former stage, the optimal solutions as labels effectively guide the neural network training, thereby bypassing the sparse reward issue. In the latter, the neural network is trained using RL to solve large-scale problems, avoiding the labels of optimal solutions that are hard to obtain. Moreover, we propose a decomposition-based approach that enables RL training with larger problem scales, alleviating the issue of insufficient memory induced by the heavy neural network. The proposed paradigm advances existing NCO models to obtain near-optimal solutions for the Traveling Salesman Problem (TSP) and Capacitated Vehicle Routing Problem (CVRP) with up to 10,000 nodes. Additionally, it enhances the generalization performance across instances of different sizes and distributions, as well as real-world TSPLib and CVRPLib instances.},
  archive      = {J_NEUCOM},
  author       = {Zhi Cao and Yaoxin Wu and Yaqing Hou and Hongwei Ge},
  doi          = {10.1016/j.neucom.2025.131707},
  journal      = {Neurocomputing},
  month        = {1},
  pages        = {131707},
  shortjournal = {Neurocomputing},
  title        = {Enhancing neural combinatorial optimization by progressive training paradigm},
  volume       = {659},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). FGVoxel3D: Fine-grained multi-resolution voxel network for 3D object detection. <em>NEUCOM</em>, <em>659</em>, 131702. (<a href='https://doi.org/10.1016/j.neucom.2025.131702'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Three-dimensional object detection from LiDAR point clouds is an indispensable component of perception systems for autonomous vehicles. Recent advances in voxel-based 3D object detection methods demonstrate rapid progress due to their efficient point cloud processing capabilities. However, existing research primarily focuses on single-voxel approaches with limited exploration of multi-voxel methods. To our knowledge, no published work investigates multi-voxel 3D object detection approaches based on voxels of arbitrary resolutions. This paper proposes FGVoxel3D, a novel Fine-Grained Voxel Network for 3D object detection. It specifically addresses multi-voxel detection at arbitrary resolutions. The network incorporates an Arbitrary Resolution Voxel (ARV) extraction module for fine-grained multi-resolution voxel extraction. Subsequently, a Sparse-to-Dense Multi-Voxel feature extraction network (SDMV) learns robust bird’s-eye view (BEV) features from these voxels. Furthermore, a feature map scale alignment strategy precisely harmonizes feature map dimensions. These core components collectively enable FGVoxel3D to capture and utilize richer scene details for enhanced detection. Extensive experiments on the authoritative KITTI and nuScenes datasets validate the effectiveness of our method. For instance, on the KITTI testing set, FGVoxel3D improves the Car mAP of the SECOND baseline by 3.52%, and on the nuScenes validation set, it surpasses the CenterPoint baseline by 2.0% in mAP and 1.8% in NDS. It also demonstrates robust gains in detecting partially occluded objects.},
  archive      = {J_NEUCOM},
  author       = {Lei Ao and Xingzheng Wang and Wenkang Wan and Nan Ouyang and Qingqing Li and Kai Sheng},
  doi          = {10.1016/j.neucom.2025.131702},
  journal      = {Neurocomputing},
  month        = {1},
  pages        = {131702},
  shortjournal = {Neurocomputing},
  title        = {FGVoxel3D: Fine-grained multi-resolution voxel network for 3D object detection},
  volume       = {659},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). A responsive approach to multivariate time-series anomaly detection with K-distance based calibrated reconstruction. <em>NEUCOM</em>, <em>659</em>, 131689. (<a href='https://doi.org/10.1016/j.neucom.2025.131689'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Anomaly detection is of great importance in the Industrial Internet of Things (IIoT) as it enables intelligent process control, analysis, and management, etc. There is a growing demand for responsive anomaly detection models that exhibit high sensitivity and precision. Various models have been proposed to address these challenges, with reconstruction-based models currently dominating the field. These models concentrate on learning complex representations of time-series; however, representation learning can be negatively affected by anomaly contamination and may experience varying degrees of detection delay. In this paper, we introduce a responsive approach for multivariate time-series anomaly detection. We design a new distance function that emphasizes the importance of the current timestamp in calculating anomaly scores to achieve more responsive detection. Additionally, we employ a calibration method that allows the model to focus exclusively on reconstructing normal patterns and an integrated prediction mechanism to enhance detection precision. Comprehensive experiments conducted on five real-world datasets indicate that our approach surpasses the state-of-the-art methods in multivariate time-series anomaly detection, shortens detection delays, and provides support for responsive anomaly reporting.},
  archive      = {J_NEUCOM},
  author       = {Jin Fan and YanHao Bi and Jin’an Yao and Liangkang Huang and HuiFeng Wu and Jia Wu},
  doi          = {10.1016/j.neucom.2025.131689},
  journal      = {Neurocomputing},
  month        = {1},
  pages        = {131689},
  shortjournal = {Neurocomputing},
  title        = {A responsive approach to multivariate time-series anomaly detection with K-distance based calibrated reconstruction},
  volume       = {659},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Quantum annealing-based feature selection. <em>NEUCOM</em>, <em>659</em>, 131673. (<a href='https://doi.org/10.1016/j.neucom.2025.131673'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Feature selection is crucial for enhancing the accuracy and efficiency of machine learning models. This work investigates the possibility of the usefulness of a quantum annealer to tackle the challenge of selecting features based on maximizing mutual information (MI) and conditional mutual information (CMI). Calculating the optimal feature set for maximum MI and CMI remains computationally intractable for large datasets on classical computers, even with approximation methods. This study employs a Mutual Information Quadratic Unconstrained Binary Optimization (MIQUBO) formulation, enabling its solution on a quantum annealer. Importantly, the study demonstrates the relevance of this approach in identifying the best feature combinations that maximize the MI and CMI. To showcase its real-world applicability, we apply MIQUBO to forecasting the price of used excavators. Our results demonstrate that using the MIQUBO approach leads to an improvement in the prediction of machine learning models for datasets, with a smaller MI concentration on a subset of all features.},
  archive      = {J_NEUCOM},
  author       = {Daniel Pranjić and Bharadwaj Chowdary Mummaneni and Christian Tutschku},
  doi          = {10.1016/j.neucom.2025.131673},
  journal      = {Neurocomputing},
  month        = {1},
  pages        = {131673},
  shortjournal = {Neurocomputing},
  title        = {Quantum annealing-based feature selection},
  volume       = {659},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Finite-time anti-synchronization of neural networks with general discrete time-varying delays. <em>NEUCOM</em>, <em>659</em>, 131666. (<a href='https://doi.org/10.1016/j.neucom.2025.131666'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper investigates the finite-time (F-T) anti-synchronization problem of neural networks (NNs) with discrete time-varying delays. Firstly, a F-T anti-synchronization theorem is proposed for a class of non-linear dynamical systems. It is important to note that the considered non-linear dynamical systems encompass various NN models, and the obtained result is applicable to a broader range of time delays. Subsequently, the F-T anti-synchronization of a specific class of delayed NNs is examined, and several F-T anti-synchronization criteria for the considered delayed NNs are presented. Finally, two numerical examples are used to verify the validity of the theoretical results obtained.},
  archive      = {J_NEUCOM},
  author       = {Yue Chen and Ailong Wu and Yan Li},
  doi          = {10.1016/j.neucom.2025.131666},
  journal      = {Neurocomputing},
  month        = {1},
  pages        = {131666},
  shortjournal = {Neurocomputing},
  title        = {Finite-time anti-synchronization of neural networks with general discrete time-varying delays},
  volume       = {659},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). A survey of machine learning and deep learning methods for vibration-based bearing fault diagnosis: The need, challenges, and potential future research directions. <em>NEUCOM</em>, <em>659</em>, 131628. (<a href='https://doi.org/10.1016/j.neucom.2025.131628'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Rolling element bearings (REBs) play a vital role in various mechanical systems, and their reliable performance is crucial for maintaining production efficiency and minimizing accidents. These bearings can develop faults, which, if not addressed, can lead to catastrophic failures, making fault diagnosis imperative for maintenance and safety. Intelligent bearing fault diagnosis systems using machine learning (ML) and deep learning (DL) approaches have been successful in accurately detecting and diagnosing faults. With rapid advancement in hardware and software technologies, there has been a parallel rapid growth in the development of intelligent fault diagnosis approaches. This survey examines the evolution and current state of various ML and DL approaches for vibration-based bearing fault diagnosis. The paper systematically reviews traditional ML algorithms, including k-Nearest Neighbors, Support Vector Machines, Artificial Neural Networks, Extreme Learning Machines, and Decision Tree-based methods, analyzing their integration with advanced signal processing and feature extraction techniques. The survey then explores DL architectures such as Convolutional Neural Networks, Deep Belief Networks, Autoencoders, Generative Adversarial Networks, Recurrent Neural Networks, and Gated Recurrent Units, highlighting their ability to automate feature extraction from raw vibration signals. A notable contribution of this survey is its in-depth examination of emerging techniques that have received limited attention in previous reviews, specifically Graph Neural Networks (GNNs) and Deep Reinforcement Learning (DRL) frameworks. The survey also covers recent advances in attention mechanisms, transformer architectures, and transfer learning strategies that enhance model performance across varying operational conditions. Furthermore, we discuss the integration of explainable AI techniques with ML and DL models to improve interpretability and highlight critical features for fault diagnosis. Through analysis of benchmark datasets and their limitations, we identify key challenges including data imbalance, domain transferability, real-time implementation constraints, and incipient fault detection. The survey concludes with future research directions emphasizing the need for physics-informed approaches, multi-modal sensor fusion, and lightweight architectures suitable for edge deployment.},
  archive      = {J_NEUCOM},
  author       = {Rohan Puntambekar and Pratyaksh Vyas and Ankit Thakkar and Dhaval Patel},
  doi          = {10.1016/j.neucom.2025.131628},
  journal      = {Neurocomputing},
  month        = {1},
  pages        = {131628},
  shortjournal = {Neurocomputing},
  title        = {A survey of machine learning and deep learning methods for vibration-based bearing fault diagnosis: The need, challenges, and potential future research directions},
  volume       = {659},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). DMPFuse: Infrared and visible image fusion via detail preservation and multi-path constraints. <em>NEUCOM</em>, <em>659</em>, 131530. (<a href='https://doi.org/10.1016/j.neucom.2025.131530'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Image fusion techniques address the inherent limitations of single-modal imaging systems by harmoniously combining complementary data modalities, thereby enhancing both image fidelity and downstream task performance. However, existing methods tend to lose texture details during feature extraction and most prioritize enhancing the visual quality of fused images. Although recent multi-task collaborative frameworks mitigate this issue, they typically introduce auxiliary task branches, substantially increasing computational costs and parameters. To address these limitations, we propose DMPFuse, a lightweight infrared-visible fusion framework that balances computational efficiency with comprehensive texture preservation and task-oriented feature extraction. The framework integrates three key innovations: (1) a Detail Enhancement Preservation (DEP) module, employing a dual-branch architecture with detail-enhancing convolutions and internal self-attention to preserve local textures and global structural features across modalities; (2) a plug-and-play Semantic-Detail Fusion Module (SDFM) that hierarchically integrates multi-scale features while mitigating cross-scale feature conflicts; and (3) a Multi-Path Constraint Module (MPCM) enforcing feature alignment through parallel supervision of fusion, reconstruction, and semantic segmentation tasks. Comparative experiments with benchmark methods demonstrate that DMPFuse significantly outperforms existing techniques in texture preservation, visual coherence, and semantic feature relevance. Quantitative metrics and qualitative analyses across segmentation and detection tasks empirically validate the practical advantages of our approach.},
  archive      = {J_NEUCOM},
  author       = {Wenlei Chen and Hanlin Qin and Xupei Zhang and Beihua Ying and Shuai Yuan and Xuefeng Bi},
  doi          = {10.1016/j.neucom.2025.131530},
  journal      = {Neurocomputing},
  month        = {1},
  pages        = {131530},
  shortjournal = {Neurocomputing},
  title        = {DMPFuse: Infrared and visible image fusion via detail preservation and multi-path constraints},
  volume       = {659},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). ADP-net: Adaptive point network with multi-scale attention mechanism for small object detection. <em>NEUCOM</em>, <em>659</em>, 131381. (<a href='https://doi.org/10.1016/j.neucom.2025.131381'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Small object detection in aerial imagery remains a significant challenge due to low resolution and sparse feature representations. While conventional object detectors achieve strong performance on medium and large objects, their accuracy for small objects can lag by up to 24 %. To address this gap, we propose AdaptivePointNet (ADP-Net), a novel keypoint-based detection framework tailored for small object recognition. Instead of relying on traditional bounding box regression, ADP-Net introduces a balanced keypoint encoder that employs heatmap representations to mitigate scale-induced biases. Additionally, an adaptive attention mechanism dynamically calibrates receptive fields, enabling the model to focus more effectively on small-scale features within coarse feature maps. To further enhance detection robustness, we incorporate a multi-scale fusion module that integrates predictions across hierarchical feature levels. Evaluated on the challenging UA-DETRAC aerial view benchmark, ADP-Net achieves state-of-the-art results, including up to 8.35 % improvement in hard-category scenarios dominated by small object instances.},
  archive      = {J_NEUCOM},
  author       = {Abenezer Girma and Abdollah Homaifar and Mahmoud Nabil Mahmoud},
  doi          = {10.1016/j.neucom.2025.131381},
  journal      = {Neurocomputing},
  month        = {1},
  pages        = {131381},
  shortjournal = {Neurocomputing},
  title        = {ADP-net: Adaptive point network with multi-scale attention mechanism for small object detection},
  volume       = {659},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Observer-based distributed adaptive neural network containment control for uncertain nonlinear multi-agent systems under DoS attacks. <em>NEUCOM</em>, <em>659</em>, 131306. (<a href='https://doi.org/10.1016/j.neucom.2025.131306'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper addresses the containment control issue in high-order nonlinear multi-agent systems (MASs) under denial of service (DoS) attacks. First, a neural network-based switching observer with adaptive mechanism is developed to reconstruct unmeasurable agent states under intermittent DoS-induced communication disruptions, establishing new theoretical pathways for directed network topologies. Second, a command-filtered backstepping control framework is proposed to circumvent the inherent complexity explosion in traditional recursive designs by eliminating redundant differentiations of virtual control laws. Ultimately, a distributed adaptive neural network containment control (DANNCC) scheme is established, ensuring all follower agents asymptotically converge into the convex hull spanned by multiple leaders. Furthermore, systematic stability analysis with constructed Lyapunov functions yields boundedness of all closed-loop signals in the system. Moreover, the containment errors can be asymptotically driven to an arbitrarily small magnitude through systematic parameter adjustment. The developed approach’s operational efficacy and real-world applicability are validated through comprehensive simulations across heterogeneous attack scenarios, demonstrating strict adherence to convergence requirements without control performance degradation.},
  archive      = {J_NEUCOM},
  author       = {Chunlong Hao and Zhi Liu and Licheng Zheng and C.L. Philip Chen and Guanyu Lai},
  doi          = {10.1016/j.neucom.2025.131306},
  journal      = {Neurocomputing},
  month        = {1},
  pages        = {131306},
  shortjournal = {Neurocomputing},
  title        = {Observer-based distributed adaptive neural network containment control for uncertain nonlinear multi-agent systems under DoS attacks},
  volume       = {659},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Measuring cognitive load by a score-based causal network model with multichannel physiological signals. <em>NEUCOM</em>, <em>659</em>, 131290. (<a href='https://doi.org/10.1016/j.neucom.2025.131290'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cognitive load significantly affects performance in safety-sensitive fields like aviation, where excessive cognitive demands are linked to increased operational errors and heightened risks. However, cognitive load measurement faces the challenge of neglecting spatio-temporal and causal dependencies in multi-channel physiological signals. We present a score-based causal representation model that captures causal structural diversity and spatio-temporal dependencies within single channels and inter-dependencies among multiple channels. By optimizing a score-based causal function under causal Markov property constraints, the model disentangles latent spatio-temporal features into causal and task-irrelevant groups. Experimental results on two public and one in-house dataset show our model significantly outperforms state-of-the-art methods.},
  archive      = {J_NEUCOM},
  author       = {Li Liu and Qiwen Pang and Shanshan Huang and Laiming Jiang and Shu Wang and Guang Wu and Guoxin Su and Qing Tao},
  doi          = {10.1016/j.neucom.2025.131290},
  journal      = {Neurocomputing},
  month        = {1},
  pages        = {131290},
  shortjournal = {Neurocomputing},
  title        = {Measuring cognitive load by a score-based causal network model with multichannel physiological signals},
  volume       = {659},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Deep semi-supervised regression with cooperative uncertainty-consistency regularization and adaptive calibration. <em>NEUCOM</em>, <em>659</em>, 131276. (<a href='https://doi.org/10.1016/j.neucom.2025.131276'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep semi-supervised learning is essential in various fields, particularly where acquiring sufficient labeled data poses a challenge. In computer vision tasks, semi-supervised classification learning has seen tremendous advancements, whereas semi-supervised regression learning remains relatively under-explored. Generally, techniques that are effective for semi-supervised classification can’t directly apply to semi-supervised regression without adaptation. This work proposes a novel semi-supervised regression method with dynamically cooperative textbfUncertainty- C onsistency R egularization and Adaptive Pseudo-Label C alibration, named UCRC . Given the limitations of consistency regularization in regression tasks, the proposed method dynamically modulates its application intensity to mitigate overfitting induced by uncertainty regularization while enhancing model robustness, and allows complete activation of uncertainty regularization to capture noisy patterns thereby improving pseudo-label quality. Furthermore, this work incorporates feature information from labeled data, adaptively selecting relevant features into the pseudo-label generation process. This effectively enhances the credibility of the pseudo-labels. Experiments demonstrate that this method achieves superior performance and robustness compared to state-of-the-art methods on two facial age prediction datasets (AgeDB and UTKFace) and a crowd counting dataset, called ShanghaiTech Part-A/B. Benefiting from the cooperative regularization loss function and adaptive pseudo label calibration technology incorporated, this method achieves R 2 scores of 69.82 %, 57.29 %, and 53.35 % on three datasets using only 10 % of the training labels. A series of ablation experiments provide detailed insights into the mechanism of the cooperative regularization and the impact of the calibration technique.},
  archive      = {J_NEUCOM},
  author       = {Xingyu Zhou and Mingqi Jiang and Yongjia Zhao and Lei Pang and Zhuo Wang},
  doi          = {10.1016/j.neucom.2025.131276},
  journal      = {Neurocomputing},
  month        = {1},
  pages        = {131276},
  shortjournal = {Neurocomputing},
  title        = {Deep semi-supervised regression with cooperative uncertainty-consistency regularization and adaptive calibration},
  volume       = {659},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Sycophancy in vision-language models: A systematic analysis and an inference-time mitigation framework. <em>NEUCOM</em>, <em>659</em>, 131217. (<a href='https://doi.org/10.1016/j.neucom.2025.131217'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Large Vision–Language Models (LVLMs) have shown significant capability in vision–language understanding. However, one critical issue that persists in these models is sycophancy, where models are unduly influenced by leading or deceptive prompts, resulting in biased outputs and hallucinations. Despite the rapid development of LVLMs, evaluating and mitigating sycophancy remains largely under-explored. In this work, we fill this gap by systematically analyzing sycophancy across multiple vision–language benchmarks and propose an inference-time mitigation framework. We curate leading queries and quantify the susceptibility of state-of-the-art LVLMs to prompt-induced bias, revealing consistent performance degradation and instability across models and tasks. Our analysis further uncovers model-specific behavioral traits, such as sentiment sensitivity and prediction polarity shifts under sycophancy. To mitigate these issues, we propose a training-free, model-agnostic framework that operates entirely at inference time. Our approach first employs a query neutralizer, leveraging a language model to suppress implicit sycophantic bias in user queries. We then introduce a sycophancy-aware contrastive decoding mechanism that dynamically recalibrates token-level output distributions by contrasting responses to neutralized and leading queries. Finally, an adaptive logits refinement module further modifies the contrasted logits by integrating both an adaptive plausibility filter and query sentiment scaler, ensuring coherent and robust generation. Extensive experiments demonstrate that this framework effectively mitigates sycophancy across all evaluated models, while maintaining performance on neutral prompts. Our results suggest that sycophancy in LVLMs is a general and urgent challenge, and that inference-time strategies offer a promising path toward trustworthy multimodal reasoning.},
  archive      = {J_NEUCOM},
  author       = {Yunpu Zhao and Rui Zhang and Junbin Xiao and Changxin Ke and Ruibo Hou and Yifan Hao and Ling Li},
  doi          = {10.1016/j.neucom.2025.131217},
  journal      = {Neurocomputing},
  month        = {1},
  pages        = {131217},
  shortjournal = {Neurocomputing},
  title        = {Sycophancy in vision-language models: A systematic analysis and an inference-time mitigation framework},
  volume       = {659},
  year         = {2026},
}
</textarea>
</details></li>
</ul>

</body>
</html>
