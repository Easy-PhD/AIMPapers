<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>NEUCOM</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="neucom">NEUCOM - 131</h2>
<ul>
<li><details>
<summary>
(2026). DSM-STWave: Enhancing traffic flow prediction for both offline and online scenarios. <em>NEUCOM</em>, <em>661</em>, 131836. (<a href='https://doi.org/10.1016/j.neucom.2025.131836'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Accurate traffic flow prediction is crucial for intelligent transportation systems. Although wave-decomposition methods effectively capture temporal dynamics, existing models such as STWave are constrained by single spatial feature extraction and static architectures . We propose DSM-STWave, a Dual Spatial and Memory-enhanced framework that overcomes these limitations with two core innovations: (1) a dual spatial attention mechanism that integrates sparse global attention O ( k N ) and local k -nearest neighbor attention O ( N log ⁡ N ) to efficiently model both long- and short-range dependencies; (2) a lightweight adaptive memory module that dynamically refines prediction strategies during inference, enabling resilience to distribution shifts in both online and offline settings. Extensive experiments on four real-world datasets highlight the practical advantages of DSM-STWave: it reduces MAE by 5.7 % on PeMSD4 ( 19.28 → 18.18 ), accelerates training by 67.8 % ( 142.1 s → 45.8 s ), and speeds up inference by 28.6 % compared with STWave, delivering both efficiency and reliability for real-time traffic prediction under dynamic conditions.},
  archive      = {J_NEUCOM},
  author       = {Yinglan Liang and Sanyang Liu and Yiguang Bai and Yudong Gong and Tianqing Zhu},
  doi          = {10.1016/j.neucom.2025.131836},
  journal      = {Neurocomputing},
  month        = {1},
  pages        = {131836},
  shortjournal = {Neurocomputing},
  title        = {DSM-STWave: Enhancing traffic flow prediction for both offline and online scenarios},
  volume       = {661},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). ML-HAN: Multi-level heterogeneous graph attention network for representation learning with semantic diversity via feature-node-semantic attention. <em>NEUCOM</em>, <em>660</em>, 131962. (<a href='https://doi.org/10.1016/j.neucom.2025.131962'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Graph neural networks (GNNs) have been widely recognized for their prominent capability in modeling complex systems. Nevertheless, developing GNNs to handle heterogeneous graphs with diverse node and link types remains a non-trivial challenge. The intrinsic diversity of nodes and edges induces semantic diversity, which further exacerbates the difficulty of representation learning in heterogeneous graphs. Thus, there is an urgent need to advance GNNs tailored for heterogeneous graph scenarios. Attention mechanisms, which have exhibited remarkable performance across multiple domains, have recently been introduced into graph representation learning to alleviate this issue. In this study, we propose a novel multi-level heterogeneous graph attention network (ML-HAN) for effective information extraction from heterogeneous networks. Specifically, ML-HAN incorporates three hierarchical attention modules: the feature-level attention module quantifies the significance of neighbors at the feature level; the node-level attention module assigns differentiated weights to distinct node features; and the semantic-level attention module identifies the optimal combination of node embeddings derived from various meta-paths. Leveraging this hierarchical framework, ML-HAN fully exploits information from three critical dimensions of heterogeneous graphs—neighbors, features, and meta-paths—thereby enabling effective extraction of rich semantic information. Additionally, we conduct three ablation experiments to validate the independent contribution of each module, and extensive experiments on three real-world datasets empirically verify the superior effectiveness of our proposed model.},
  archive      = {J_NEUCOM},
  author       = {Zhiyao La and Jinhui Shen and Yuchen Song and Shuchuan Tian and Weijun Gong},
  doi          = {10.1016/j.neucom.2025.131962},
  journal      = {Neurocomputing},
  month        = {1},
  pages        = {131962},
  shortjournal = {Neurocomputing},
  title        = {ML-HAN: Multi-level heterogeneous graph attention network for representation learning with semantic diversity via feature-node-semantic attention},
  volume       = {660},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). DCTNet: A dual-branch CNN-transformer network for SAR-optical image classification. <em>NEUCOM</em>, <em>660</em>, 131949. (<a href='https://doi.org/10.1016/j.neucom.2025.131949'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Land cover classification in remote sensing imagery remains a challenging task. It requires modeling both global scene context and fine-scale details, while dealing with complex landscapes, varied spatial resolutions, and heterogeneous target types. Many existing methods struggle to harmonize modality gaps across multi-source datasets. They also face challenges in capturing textural patterns across different scales and in modeling long-range spatial dependencies. To address these limitations, we propose a dual-branch CNN–transformer architecture, termed DCTNet. The network integrates local details with global context to produce expressive and highly discriminative representations. In our design, a three-level convolutional pipeline extracts low-level features from SAR data. Meanwhile, a hierarchical spectral–spatial network processes optical imagery. To preserve detailed structures and capture extended dependencies, we incorporate a transformer-based component. This ensures a balance between fine-grained local patterns and high-level global semantics. Comprehensive evaluations on Augsburg, Berlin, and FUSAR-Map datasets demonstrate the robustness and performance advantages of the proposed method.},
  archive      = {J_NEUCOM},
  author       = {Zhiqiang Zhao and Qi Han and Xiaofan Wang and Yuan Qiu and Xinhong Hei},
  doi          = {10.1016/j.neucom.2025.131949},
  journal      = {Neurocomputing},
  month        = {1},
  pages        = {131949},
  shortjournal = {Neurocomputing},
  title        = {DCTNet: A dual-branch CNN-transformer network for SAR-optical image classification},
  volume       = {660},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). VGen-adapter: A vision generalization adapter for stable diffusion 3. <em>NEUCOM</em>, <em>660</em>, 131948. (<a href='https://doi.org/10.1016/j.neucom.2025.131948'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent years have witnessed rapid advancements in large-scale text-to-image generation models, leading to substantial improvements in generated image quality and the continuous enrichment of the associated ecosystem. Concurrently, various control methods tailored for image generation have emerged, enabling diverse control tasks based on input images. The rise of Transformer architectures has led to the progressive replacement of traditional U-Net-based generative models with Diffusion Transformer (DiT) frameworks. However, most existing control methods face significant challenges in directly adapting to DiT architectures, commonly exhibiting limitations such as inadequate control precision, inefficient training processes, excessive parameter scales, and heavy reliance on large training datasets. To address these issues, we introduce a lightweight image control adapter method tailored for DiT-based Stable Diffusion 3 frameworks, termed VGen-adapter ( V ision Ge neralization adapter ). The proposed method incorporates an image feature extractor and optimizes the feature fusion strategy, significantly enhancing control precision. Regarding training strategy, VGen-adapter employs a parameter-efficient approach by completely freezing all parameters of the original model while implementing a lightweight design for the adapter module. Through the integration of noise perturbation for data augmentation, our approach enables high-quality image generation using limited text-image datasets. Furthermore, with only 95 M parameters (approximately 5 % of the SD3-Medium model), this architecture achieves substantial computational efficiency. Comparative experiments on the COCO dataset demonstrate that the VGen-adapter achieves excellent control performance while maintaining high efficiency and low parameter count, verifying its feasibility and effectiveness under the DiT architecture.},
  archive      = {J_NEUCOM},
  author       = {Zhenglong Xiang and Chenghao Zhou and Yu Xue},
  doi          = {10.1016/j.neucom.2025.131948},
  journal      = {Neurocomputing},
  month        = {1},
  pages        = {131948},
  shortjournal = {Neurocomputing},
  title        = {VGen-adapter: A vision generalization adapter for stable diffusion 3},
  volume       = {660},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Dynamic-aware self-attention with embedded CWT for imbalanced ECG classification in cardiac disease detection. <em>NEUCOM</em>, <em>660</em>, 131945. (<a href='https://doi.org/10.1016/j.neucom.2025.131945'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Accurate detection of arrhythmia and congestive heart failure (CHF) from electrocardiogram (ECG) signals is crucial for timely intervention, but remains challenging due to class imbalance, noise, and the need to capture both short-term and long-term temporal patterns. This study introduces a dynamic-aware self-attention network that incorporates a differentiable continuous wavelet transform (CWT) layer and an adaptive class-weighting loss. The model combines gated recurrent units (GRUs) for sequential modeling, self-attention to highlight diagnostically relevant segments, and embedded CWT for optimized time–frequency learning. Standardized nonlinear dynamical features, such as the Lyapunov exponent and sample entropy, are combined with deep features to enhance disease state discrimination. On a composite ECG dataset of 162 recordings, evaluated with stratified splits and 5-fold cross-validation, the model achieved an average accuracy of 98.17 % and 100 % sensitivity for arrhythmia, with no misclassification between normal and diseased cases. Confidence intervals confirmed the model's robustness, though the perfect sensitivity should be interpreted with caution due to the dataset size. Comparative analysis indicates that the proposed framework matches or exceeds recent state-of-the-art methods, and inference times of approximately 50 ms per 10-second ECG segment support near real-time use. These findings demonstrate the framework's potential to enable imbalance-resilient, interpretable, and end-to-end ECG analysis, supporting earlier and more reliable diagnosis of cardiovascular disease.},
  archive      = {J_NEUCOM},
  author       = {Fuad E. Alsaadi and Njud S. Alharbi},
  doi          = {10.1016/j.neucom.2025.131945},
  journal      = {Neurocomputing},
  month        = {1},
  pages        = {131945},
  shortjournal = {Neurocomputing},
  title        = {Dynamic-aware self-attention with embedded CWT for imbalanced ECG classification in cardiac disease detection},
  volume       = {660},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Diff-mamba: A diffusion-mamba framework for hyperspectral image classification. <em>NEUCOM</em>, <em>660</em>, 131930. (<a href='https://doi.org/10.1016/j.neucom.2025.131930'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The accuracy of modeling complicated spectral-spatial distribution is crucial for hyperspectral image (HSI) classification. However, most existing methods primarily focus on enhancing global and local feature representations, while neglecting the complex spectral-spatial distribution relationships in high-dimensional data. To address this issue, we propose a novel framework combining diffusion models and Mamba called Diff-Mamba for characterizing the real spectral-spatial distribution relationships from a generative perspective. Specifically, the framework comprises a spectral-spatial diffusion feature generation module based on diffusion models, a Mamba-based global feature extraction module, and a center pixel-driven semantic token generator. Firstly, the spectral-spatial diffusion feature generation module is developed to capture spectral-spatial distribution information of objects in HSI. This module consists of forward and reverse diffusion processes. Within the reverse diffusion process, we propose a Transformer-based spectral-spatial diffusion network specifically designed for HSI classification tasks, effectively capturing the complex distributional relationships inherent in HSI. In this process, iteratively denoising and explicitly constructing the data generation process are adopted to adaptively construct cross-correlation between samples and effectively generate diffusion features reflecting spectral-spatial distribution information. Secondly, the diffusion features are fed into the Mamba-based global feature extraction module to model the global correlation of spectral-spatial features with linear complexity. Finally, a center pixel-driven semantic token generator is designed to dynamically focus on the feature representation of the central pixel for patch-wise HSI classification. Experimental results on four public HSI datasets demonstrate that the proposed Diff-Mamba is competitive with state-of-the-art methods. The code will be made publicly available at https://github.com/shike801/Diff-Mamba .},
  archive      = {J_NEUCOM},
  author       = {Shuaibing Shi and Min Li and Yongqi Yin and Yujie He and Aitao Yang},
  doi          = {10.1016/j.neucom.2025.131930},
  journal      = {Neurocomputing},
  month        = {1},
  pages        = {131930},
  shortjournal = {Neurocomputing},
  title        = {Diff-mamba: A diffusion-mamba framework for hyperspectral image classification},
  volume       = {660},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). On synchronization of discontinuous competitive fuzzy neural networks with time-varying delays via non-chattering quantization. <em>NEUCOM</em>, <em>660</em>, 131920. (<a href='https://doi.org/10.1016/j.neucom.2025.131920'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper is dedicated to studying the global asymptotic synchronization of discontinuous competitive fuzzy neural networks (DCFNNs) with time-varying delays. To address the technical challenges arising from excessive communication burden and time-varying delays, an innovative non-chattering quantized controller is designed. By comprehensively applying Lyapunov functionals, differential inclusion theory, and inequality techniques, novel and practical synchronization criteria are derived. Furthermore, an adaptive non-chattering quantized controller adjustment law with less conservative control gains is constructed. Finally, the effectiveness of the aforementioned theoretical analysis is fully verified through simulation examples.},
  archive      = {J_NEUCOM},
  author       = {Yinjie Qian and Yuanhua Qiao},
  doi          = {10.1016/j.neucom.2025.131920},
  journal      = {Neurocomputing},
  month        = {1},
  pages        = {131920},
  shortjournal = {Neurocomputing},
  title        = {On synchronization of discontinuous competitive fuzzy neural networks with time-varying delays via non-chattering quantization},
  volume       = {660},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). MGC-net: Semi-supervised domain generalization in medical image segmentation via multi-granularity consistency. <em>NEUCOM</em>, <em>660</em>, 131919. (<a href='https://doi.org/10.1016/j.neucom.2025.131919'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Medical image segmentation tasks face two major challenges: the scarcity of annotated data and poor generalization on unseen domains. While semi-supervised learning and domain generalization techniques have been explored separately, existing methods often focus on only one challenge, limiting their real-world applicability. Furthermore, existing methods lack sufficient consideration of both image and feature levels. Ignoring image granularity consistency tends to generate images with pronounced style fluctuations and semantic shift. Overlooking feature granularity invariance may lead to yielding different feature representation for structures of same category but different styles. Jointly imposing both constraints allows us to effectively extract domain-invariant features. Specifically, MGC-Net integrates an SSDG segmentation branch and a self-supervised reconstruction branch to design consistency rules from the image granularity and feature granularity. The Fourier transform is utilized to achieve image style enhancement, and a reconstruction network is designed to implement the consistency constraint of the image granularity. For feature granularity consistency, a Feature Stabilization and Orthogonality (FSO) module and a Domain Consistency and Class Separation (DCCS) module are developed. FSO ensures that features learned by the segmentation network remain invariant to these style changes. DCCS aims to achieve inter-domain consistency and inter-class separability across data from different domains. Experimental results on the M&Ms and SCGM datasets demonstrate that the proposed method achieves competitive results compared with existing methods.},
  archive      = {J_NEUCOM},
  author       = {Yanfeng Li and Xiaochen Ma and Jia Sun and Houjin Chen and Bijuan Ren and Minjun Wang},
  doi          = {10.1016/j.neucom.2025.131919},
  journal      = {Neurocomputing},
  month        = {1},
  pages        = {131919},
  shortjournal = {Neurocomputing},
  title        = {MGC-net: Semi-supervised domain generalization in medical image segmentation via multi-granularity consistency},
  volume       = {660},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Multi-scale regulation of reservoir topology in echo state networks. <em>NEUCOM</em>, <em>660</em>, 131918. (<a href='https://doi.org/10.1016/j.neucom.2025.131918'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Echo State Networks (ESNs) are widely used in many fields. The core of the original ESNs is the reservoir, whose topology is random and unchanged during training. However, the random topology usually performs poorly on complex tasks. It is always an open topic to regulate the topology of the reservoir according to the given task. Inspired by the synaptic plasticity of the brain network in biology, this paper proposes a multi-scale update approach to optimize the topology of ESNs, named multi-scale echo state network (MS-ESN). Initially, based on the structural and functional connections in the brain network, we divide the reservoir connection weight matrix into the Hadamard product of a functional connectivity matrix and structural connectivity matrix. Subsequently, we propose a method to optimize the functional matrix and structural matrix across different temporal scales. Finally, experiments are conducted on five benchmark datasets. The experimental results show that the proposed MS-ESN significantly improves the performance of ESNs in time series prediction tasks.},
  archive      = {J_NEUCOM},
  author       = {Xinyu Shen and Fanjun Li and Jiayue Feng and Wenjie Li and Jingyi Chen},
  doi          = {10.1016/j.neucom.2025.131918},
  journal      = {Neurocomputing},
  month        = {1},
  pages        = {131918},
  shortjournal = {Neurocomputing},
  title        = {Multi-scale regulation of reservoir topology in echo state networks},
  volume       = {660},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Adaptive memory refinement and perception enhancement for exo-to-ego video generation. <em>NEUCOM</em>, <em>660</em>, 131917. (<a href='https://doi.org/10.1016/j.neucom.2025.131917'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The task of synthesizing cross-view videos from an exocentric (third-person) to an egocentric (first-person) perspective, referred to as the E2VG problem, remains highly challenging. This is due to the significant viewpoint differences and limited spatial overlap between the two perspectives. Current approaches often fail to capture the temporal dynamics essential for target-view synthesis, and insufficiently leverage source-view perceptual features. In this paper, we present a video-based framework, Adaptive Memory Refinement and Perception Enhancement (ARPE), to address the problem. To capture long-horizon dependencies beyond redundant short-term dynamics, we propose a Distant Temporal Dependencies (DTD) module that extracts egocentric-relevant semantics from temporally distant exocentric frames. By leveraging a sliding window, DTD aligns long-range temporal patterns across views and refines exocentric features through an egocentric-memory guidance. To enhance the focus of the model on informative content, we propose a Saliency-guided Relevance Weighting (SRW) module that adaptively highlights semantically relevant frames and spatial regions. Specifically, SRW assigns inter-frame attention to distant frames based on their relevance to the target-view reconstruction, and further applies intra-frame weighting to emphasize salient areas within each selected frame. These weights are guided by the similarity between the temporal dynamics of the two views, ensuring spatial-temporal consistency. Recognizing the need for semantic consistency across views, we propose the DINOv2 Perception Enhancement (DPE) module. It leverages DINOv2 features to capture view-invariant object-scene cues, thereby improving cross-view feature coherence. Our extensive experimental analysis demonstrates that our approach outperforms existing state-of-the-art methods, excelling in both quantitative metrics and qualitative assessments.},
  archive      = {J_NEUCOM},
  author       = {Jianhui Li and Weipeng Hu and Xingyue Wang and Jiun Tian Hoe and Ping Hu and Xudong Jiang and Yap-Peng Tan},
  doi          = {10.1016/j.neucom.2025.131917},
  journal      = {Neurocomputing},
  month        = {1},
  pages        = {131917},
  shortjournal = {Neurocomputing},
  title        = {Adaptive memory refinement and perception enhancement for exo-to-ego video generation},
  volume       = {660},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). When less is more: Sample-aware pruning of uninformative features improves neural-network regression. <em>NEUCOM</em>, <em>660</em>, 131913. (<a href='https://doi.org/10.1016/j.neucom.2025.131913'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Modern regression datasets often contain a mix of weakly informative predictors and purely noisy features. We establish, both theoretically and empirically, that discarding variables whose population covariance with the target is zero does not increase expected generalisation risk, up to a vanishing O ( log ⁡ n / n ) term under weight decay, and typically enhances performance in small-sample regimes. Risk bounds show that pruning reduces finite-sample estimation error O ( log ⁡ n / n ) without affecting approximation error, while gradients associated with noise dimensions vanish asymptotically. Controlled synthetic benchmarks spanning 27 different configurations confirm these predictions: at n = 10 2 samples with 18/20 informative features (correlation strength of 0.5), pruning reduces MSE by 44 % and increases R 2 from 0.426 to 0.678; as n grows to 10 4 , the benefit tapers, reflecting the growing influence of implicit regularisation. Attribution analysis via SHAP corroborates the oracle-level identification of relevant features. These conclusions are further validated on real-world data using the Boston Housing Dataset, where pruning just two statistically less informative features yields consistent gains in both full-sample and small-sample regimes, despite the presence of negatively correlated predictors—supporting our theoretical claim that correlation strength, not sign, determines informativeness. The safety guarantee is conditional on informativeness defined by nonzero population covariance with the target; variables that are informative only through purely non-monotonic or symmetric effects fall outside this scope. Our results caution against the automatic retention of high- p -value variables, provide sample-size-aware guidelines for feature filtering, and extend to any shrinkage-based learner, including ridge regression, kernel methods, and Gaussian processes. Code and data are publicly released to ensure full reproducibility.},
  archive      = {J_NEUCOM},
  author       = {Nicholas Christakis and Dimitris Drikakis},
  doi          = {10.1016/j.neucom.2025.131913},
  journal      = {Neurocomputing},
  month        = {1},
  pages        = {131913},
  shortjournal = {Neurocomputing},
  title        = {When less is more: Sample-aware pruning of uninformative features improves neural-network regression},
  volume       = {660},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). High-order structural attribution distillation for link prediction. <em>NEUCOM</em>, <em>660</em>, 131910. (<a href='https://doi.org/10.1016/j.neucom.2025.131910'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, Graph Neural Networks (GNNs) have achieved great success in processing graph-structured data and have shown remarkable improvements in link prediction. Despite these advances, neighborhood fetching incurred by graph dependency leads to expensive computational and time costs. Fortunately, GNN-to-MLP distillation bridges this gap by transferring graph dependency from a heavy GNN teacher into a more efficient MLP student. However, most of the existing works are devoted to learning one-hop graph structure information, which primarily focuses on local neighborhoods and fails to fully exploit higher-order relationships, making the graph-independent MLPs even more challenging to uncover complex connections and implicit patterns within the graph. Therefore, this work proposes a H igh-order S tructural A ttribution D istillation framework ( HSAD ) for link prediction task to fully leverage high-order relationships to facilitate GNN-to-MLP distillation. It primarily consists of two key processes: Structural Attribution Learning (SAL) and Structural Attribution Transfer (SAT). SAL leverages high-order adjacency matrices as input and estimates a structural similarity between nodes. By incorporating a structure extraction network (SEN) for nonlinear mapping, it learns complex structural attributions that contain local topology, enabling the model to capture implicit patterns within the graph. Subsequently, SAT employs multiple distillation strategies to incorporate the structural attribution into the student’s graph-independent node features, enhancing the student’s perception of the graph structure. Extensive experiments are conducted on seven benchmark datasets. The results demonstrate that our HSAD framework greatly improves the performance of MLP and achieves state-of-the-art results in link prediction.},
  archive      = {J_NEUCOM},
  author       = {Junyang Feng and Shunzhi Yang and Changdong Wang and Xiaowen Ma and Yibo Meng and Yunwen Chen and Zhenhua Huang},
  doi          = {10.1016/j.neucom.2025.131910},
  journal      = {Neurocomputing},
  month        = {1},
  pages        = {131910},
  shortjournal = {Neurocomputing},
  title        = {High-order structural attribution distillation for link prediction},
  volume       = {660},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Multi-scale hypergraph representation learning with channel attention for next POI recommendation. <em>NEUCOM</em>, <em>660</em>, 131908. (<a href='https://doi.org/10.1016/j.neucom.2025.131908'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the rapid development of location-based applications and mobile internet technologies, improving the performance of next Point-of-Interest (PoI) recommendation has become crucial for enhancing user experience and advancing the intelligence of applications. Addressing the limitations of existing POI recommendation methods in modeling spatio-temporal coupling features, multi-view feature fusion, and high-order relational capture, this paper proposes a novel Channel Attention-enhanced Multi-Scale Hypergraph Learning for PoI recommendation CAMSHG. CAMSHG first constructs a multi-semantic hypergraph and introduces a channel attention mechanism to dynamically model the importance of different semantic channels, thereby enhancing high-order feature representations. Second, a view-consistent alignment mechanism based on contrastive learning is employed to improve the consistency of multi-view representations. Third, a multi-scale convolutional feature fusion module is designed to fully integrate multi-view information at local and global granularities, further boosting recommendation performance. Extensive experiments and evaluations on three publicly available datasets demonstrate that CAMSHG consistently outperforms various advanced baseline methods across multiple metrics. Ablation studies and hyperparameter sensitivity analyses further validate the effectiveness of each designed module and the robustness of the overall framework. The results confirm that CAMSHG offers a unified and efficient solution for next POI recommendation in complex application scenarios, showing strong potential for practical deployment and further development. To facilitate future research, we release the code at https://github.com/Z70rain/CAMSHG .},
  archive      = {J_NEUCOM},
  author       = {Qilin Zhou and Jinkang Ye and Wei Zhou and Junhao Wen},
  doi          = {10.1016/j.neucom.2025.131908},
  journal      = {Neurocomputing},
  month        = {1},
  pages        = {131908},
  shortjournal = {Neurocomputing},
  title        = {Multi-scale hypergraph representation learning with channel attention for next POI recommendation},
  volume       = {660},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). A spiking convolutional neural network for glioma brain tumor segmentation using a spike-timing-dependent plasticity method. <em>NEUCOM</em>, <em>660</em>, 131903. (<a href='https://doi.org/10.1016/j.neucom.2025.131903'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Brain tumor segmentation efficiently can enhance patient prognosis. However, because of variations in the shape, size, and texture of gliomas from patient to patient, brain tumor segmentation is a challenging task. In this paper, brain tumors are segmented with a high rate of accuracy using a Spiking Convolutional Neural Network (SCNN). The SCNN framework utilizes a spike-time neural encoding technique that is followed by some convolutional and pooling layers. Our technique uses four feature extraction routes for exploring essential features from four MRI modalities including Flair, T1, T1C, and T2. The first layer of our model in each route applies the Difference of Gaussians (DoG) kernels for detecting contrasts in each MRI image. The input image is converted into an asynchronous spike train at the first layer that encodes the visual information in the temporal order of the spikes. Using neurons in a convolutional layer, these input spikes are mixed and emit a spike as soon as the intensity change exceeds a distinct threshold. All convolutional layers are equipped with spike-timing-dependent plasticity (STDP) for learning concealed patterns. The essential role of the first step in Spiking-based Neural Networks (SNNs) is to encode the input data into discrete spikes in the temporal domain. Verification of our model with the BraTS 2018 dataset shows the improvement of the performance parameter values compared to the state-of-the-art models. The mean of dice indexes of the SCNN framework on the BraTS 2018 dataset for the Core, Whole, and Enhanced areas are 0.92, 0.89, and 0.92, respectively.},
  archive      = {J_NEUCOM},
  author       = {Abbas Bagherian Kasgari and Soroush Sadeghi and Payam Zarbakhsh and Saeid Jafarzadeh Ghoushchi and Ramin Ranjbarzadeh},
  doi          = {10.1016/j.neucom.2025.131903},
  journal      = {Neurocomputing},
  month        = {1},
  pages        = {131903},
  shortjournal = {Neurocomputing},
  title        = {A spiking convolutional neural network for glioma brain tumor segmentation using a spike-timing-dependent plasticity method},
  volume       = {660},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). ULMGNN: Fragmented layer grouping in GUI designs through graph learning based on multimodal information. <em>NEUCOM</em>, <em>660</em>, 131894. (<a href='https://doi.org/10.1016/j.neucom.2025.131894'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Automating the generation of front-end code from UI designs can significantly accelerate the development and iteration of applications. UI components in these designs are often composed of fragmented layers, resulting in the generation of redundant view elements in the code. This not only compromises the maintainability but also the usability of the code. Existing research primarily focuses on leveraging multimodal information from UI designs to identify fragmented layers, often overlooking the structural knowledge of the UI. This paper presents ULMGNN, a multimodal graph learning-based approach that models the relationships between UI layers and incorporates multimodal information from design prototypes to effectively address the grouping of fragmented layers. Drawing inspiration from object detection tasks, we propose a novel method that classifies UI layers and concurrently regresses their corresponding component bounding boxes, enabling an end-to-end solution for fragmented layer grouping. Experimental results on two real-world datasets demonstrate that our model outperforms the current state-of-the-art baselines. User studies further validate the effectiveness of our approach in practical software engineering applications. Our end-to-end method for grouping fragmented layers offers a solution for enhancing UI-related software engineering tasks.},
  archive      = {J_NEUCOM},
  author       = {Yunnong Chen and Shuhong Xiao and Jiazhi Li and Tingting Zhou and Lingyun Sun and Liuqing Chen},
  doi          = {10.1016/j.neucom.2025.131894},
  journal      = {Neurocomputing},
  month        = {1},
  pages        = {131894},
  shortjournal = {Neurocomputing},
  title        = {ULMGNN: Fragmented layer grouping in GUI designs through graph learning based on multimodal information},
  volume       = {660},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Endogenous event-related analysis reveals dynamic brain network reorganization abnormalities in depression. <em>NEUCOM</em>, <em>660</em>, 131893. (<a href='https://doi.org/10.1016/j.neucom.2025.131893'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Current research on depression using electroencephalogram (EEG) microstates primarily focuses on static features of microstate sequences, such as duration and occurrence rate, which overlook dynamic transition processes reflecting rapid brain network reorganization. Additionally, traditional EEG acquisition paradigms relying on precise external events require patients to sustain attention to task events, resulting in excessive cognitive load and limiting their applicability in clinical diagnosis and intervention for depression. To address these dual challenges of insufficient capture of dynamic EEG features and limited clinical applicability, this study proposes an endogenous event-related analysis algorithm for the first time. This algorithm uses spontaneously occurring microstate transition events as endogenous markers, enabling dynamic EEG event-related analysis without requiring external event triggers. The method was validated on two independent public datasets (total sample size: 282). Results demonstrate that EEG microstate features exhibit consistency across datasets in both temporal and spatial dimensions, and the superimposed temporal signals associated with microstate transition events show significant cross-dataset correlations. Furthermore, analysis of depressive disorders indicates that patients with depression exhibit significant abnormal event-related synchronization or desynchronization in key brain regions (such as the occipital lobe, parietal lobe, temporal pole, supplementary motor area, limbic lobe) before and after specific microstate transition events compared to healthy subjects. Overall, the proposed endogenous event analysis algorithm and its revelation of transition-specific neural activity abnormalities in depression provide a methodological basis for developing closed-loop neurofeedback interventions targeting specific brain network state transitions, and establishes a new theoretical pathway for studying intrinsic brain dynamics.},
  archive      = {J_NEUCOM},
  author       = {Kunbo Cui and Yue Du and Lixin Zhang and Zhongqing Wu and Hua Jiang and Fuze Tian and Mingqi Zhao and Qinglin Zhao and Bin Hu},
  doi          = {10.1016/j.neucom.2025.131893},
  journal      = {Neurocomputing},
  month        = {1},
  pages        = {131893},
  shortjournal = {Neurocomputing},
  title        = {Endogenous event-related analysis reveals dynamic brain network reorganization abnormalities in depression},
  volume       = {660},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). A bio-inspired hardware implementation of an analog spike-based hippocampus memory model. <em>NEUCOM</em>, <em>660</em>, 131892. (<a href='https://doi.org/10.1016/j.neucom.2025.131892'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The need for processing at the edge of the increasing amount of data that is being produced by multitudes of sensors has led to the demand for more power-efficient computational systems, by exploring alternative computing paradigms and technologies. Neuromorphic engineering is a promising approach that can address this need by developing electronic systems that faithfully emulate the computational properties of animal brains. In particular, the hippocampus stands out as one of the most relevant brain regions for implementing auto associative memories capable of learning large amounts of information quickly and recalling it efficiently. In this work, we present a computational spike-based memory model inspired by the hippocampus that takes advantage of the features of analog electronic circuits: energy efficiency, compactness, and real-time operation. This model can learn memories, recall them from a partial fragment and forget. It has been implemented as a Spiking Neural Networks directly on a mixed-signal neuromorphic chip. We describe the details of the hardware implementation and demonstrate its operation via a series of benchmark experiments, showing how this research prototype paves the way for the development of future robust and low-power mixed-signal neuromorphic processing systems.},
  archive      = {J_NEUCOM},
  author       = {Daniel Casanueva-Morato and Alvaro Ayuso-Martinez and Giacomo Indiveri and Juan P. Dominguez-Morales and Gabriel Jimenez-Moreno},
  doi          = {10.1016/j.neucom.2025.131892},
  journal      = {Neurocomputing},
  month        = {1},
  pages        = {131892},
  shortjournal = {Neurocomputing},
  title        = {A bio-inspired hardware implementation of an analog spike-based hippocampus memory model},
  volume       = {660},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). HG-former: Utilizing multi-scale transformer links prediction based on hypergraph structure. <em>NEUCOM</em>, <em>660</em>, 131891. (<a href='https://doi.org/10.1016/j.neucom.2025.131891'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Continuous-time dynamic graphs contain rich node structure and temporal information, which can significantly improve the accuracy of link prediction. However, we argue that existing node feature construction methods in dynamic graph link prediction fail to capture the rich spatiotemporal structural information of nodes. To address this issue, we propose a novel dynamic graph link prediction model, namely HG-Former. This model incorporates the concept of H yper G raphs in dynamic graphs, and our hypergraph construction, which is different from that in discrete dynamic graphs, can better capture the high-order temporal dependencies between nodes. We also introduce a new multi-scale trans former , which dynamically adjusts according to interaction frequencies to enable the fusion learning of sequences at different scales. This method provides multi-granularity concatenated information for subsequent node connection prediction, thereby further enhancing the predictive capability of the model. Extensive experiments on seven real-world continuous-time dynamic graph datasets demonstrate that HG-Former significantly outperforms baseline models in both inductive and transductive settings. In addition, experiments show that the hypergraph structures extracted from models such as TGAT, CAWN, TCL, and GraphMixer can effectively improve the link prediction accuracy of these baseline models.},
  archive      = {J_NEUCOM},
  author       = {Ruxin Xue and Jinggui Huang and Zaitang Huang},
  doi          = {10.1016/j.neucom.2025.131891},
  journal      = {Neurocomputing},
  month        = {1},
  pages        = {131891},
  shortjournal = {Neurocomputing},
  title        = {HG-former: Utilizing multi-scale transformer links prediction based on hypergraph structure},
  volume       = {660},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). An initialization-free distributed prescribed-time optimization algorithm based on multiagent systems for solving economic dispatch problem. <em>NEUCOM</em>, <em>660</em>, 131890. (<a href='https://doi.org/10.1016/j.neucom.2025.131890'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, the economic dispatch problem (EDP) is investigated, aiming at scheduling a cluster of generators to fulfill the supply–demand balance and capability limitations at the minimized cost, where the generators in a system communicate over undirected and connected communication networks. For solving the EDP, a distributed initialization-free prescribed-time optimization approach is designed. On account of Lyapunov stability analysis and convex optimization theory, the algorithm proposed herein is proved scrupulously to converge to the optimal solutions of EDP within a prescribed time. Distinct from both the distributed finite-time optimization approaches (where the settling time heavily depends on incipient conditions) and the distributed fixed-time optimization approaches (where the settling-time cannot be arbitrarily pre-specified), the designed method herein has prescribed-time convergence and optimality. Moreover, an upper bound of the settling time function of the dynamic system is independent of controller gains and incipient conditions, implying that the convergence time of the presented algorithm can be preset under any allowable range physically. Lastly, three examples are put forward to validate the efficiency of theoretical results.},
  archive      = {J_NEUCOM},
  author       = {Yanling Zheng and Jie Zhong and Yaguan Qian and Qingshan Liu},
  doi          = {10.1016/j.neucom.2025.131890},
  journal      = {Neurocomputing},
  month        = {1},
  pages        = {131890},
  shortjournal = {Neurocomputing},
  title        = {An initialization-free distributed prescribed-time optimization algorithm based on multiagent systems for solving economic dispatch problem},
  volume       = {660},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Adversarially robust fourier-aware multimodal medical image fusion for LSCI. <em>NEUCOM</em>, <em>660</em>, 131889. (<a href='https://doi.org/10.1016/j.neucom.2025.131889'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Selecting representative frames from Laser Speckle Contrast Imaging (LSCI) sequences is crucial for accurate blood flow analysis, yet it remains highly subjective due to image instability caused by noise, motion artifacts, and illumination inconsistency. While CT/MRI and PET/MRI fusion offer complementary insights, existing methods tend to prioritize intensity information and neglect cross-modality frequency discrepancies. Due to their low intensity and high-frequency edges, blood flow regions are often diluted in fused images, despite being clinically critical. This motivates our design of an adversarially robust Fourier-aware fusion framework for enhancing blood flow saliency in multimodal medical images, supporting reliable LSCI frame selection. Specifically, a Defensive Refinement Module (DRM) introduces adversarial perturbations to improve the model’s robustness against LSCI-specific degradations. With the robust fusion, we further adopt a Fourier Enhancement Module (FEM) that selectively amplifies flow-relevant details by coupling spatial structure awareness with frequency-based refinement. A Fourier Consistency Loss guides the model to jointly constrain amplitude and phase consistency to preserve modality-specific spectral features. Expert blind evaluations demonstrate that our method notably improves the representativeness and selection efficiency of LSCI frames in practical biomedical workflows. The code is available at https://github.com/JZD151/ARFFusion .},
  archive      = {J_NEUCOM},
  author       = {Liang Zhou and Zhidong Jiao and Yuchun He and Xingyue Zhu and Weipeng Sun and Xingyuan Li and Yang Zou},
  doi          = {10.1016/j.neucom.2025.131889},
  journal      = {Neurocomputing},
  month        = {1},
  pages        = {131889},
  shortjournal = {Neurocomputing},
  title        = {Adversarially robust fourier-aware multimodal medical image fusion for LSCI},
  volume       = {660},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Discrepancy-aware contrastive learning with mixture of experts for cross-modal image-text semantic alignment. <em>NEUCOM</em>, <em>660</em>, 131887. (<a href='https://doi.org/10.1016/j.neucom.2025.131887'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the rapid growth of multimodal data, cross-modal image-text retrieval has become essential for enabling machines to align and understand heterogeneous visual and textual information. Bridging the semantic gap between these modalities is critical for tasks such as image captioning, visual question answering, and retrieval. However, current approaches often suffer from incompatible feature representations, inefficient fusion mechanisms, and poor adaptability to diverse input patterns. This paper proposes DA-MoE, a Discrepancy-Aware Mixture of Experts framework designed for accurate cross-modal semantic alignment. DA-MoE adopts a dual-stream mixture-of-experts architecture, integrating powerful pretrained vision encoders and language models through learnable projection layers. SigLIP encoders are introduced as benchmark experts to guide others and mitigate semantic bias in cross-modal transfer learning. To adaptively fuse multimodal features, DA-MoE employs a cosine similarity-based dynamic routing strategy with temperature scaling and Top-k expert selection. Additionally, a difference-aware contrastive learning strategy is proposed, which adjusts training emphasis based on sample difficulty to improve discrimination of hard examples. Extensive experiments on the Flickr30K dataset demonstrate that DA-MoE achieves Recall@10 of 99.4 % (I→T) and 95.8 % (T→I), surpassing state-of-the-art baselines by over 2 %. Analysis confirms that dynamic expert selection significantly outperforms static fusion, especially in complex and diverse image-text scenarios. This work introduces a new direction for handling modality heterogeneity in cross-modal retrieval.},
  archive      = {J_NEUCOM},
  author       = {Xin Li and Xin He and Gaigai Tang and Kaiyuan Qi and Guangfeng Su and Huiyun Zhang},
  doi          = {10.1016/j.neucom.2025.131887},
  journal      = {Neurocomputing},
  month        = {1},
  pages        = {131887},
  shortjournal = {Neurocomputing},
  title        = {Discrepancy-aware contrastive learning with mixture of experts for cross-modal image-text semantic alignment},
  volume       = {660},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Worst-case convergence analysis of relatively inexact gradient descent on smooth convex functions. <em>NEUCOM</em>, <em>660</em>, 131883. (<a href='https://doi.org/10.1016/j.neucom.2025.131883'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider the classical gradient descent algorithm with constant stepsizes, where some error is introduced in the computation of each gradient. More specifically, we assume some relative bound on the inexactness in the sense that the norm of the difference between the true gradient and its approximate value is bounded by a certain fraction of the gradient norm. This paper presents a worst-case convergence analysis of this so-called relatively inexact gradient descent on smooth convex functions, using the Performance Estimation Problem (PEP) framework. We first derive the exact worst-case behavior of the method after one step. Then we study the case of several steps and provide computable upper and lower bounds using the PEP framework. Finally, we discuss the optimal choice of constant stepsize according to the obtained worst-case convergence rates.},
  archive      = {J_NEUCOM},
  author       = {Pierre Vernimmen and François Glineur},
  doi          = {10.1016/j.neucom.2025.131883},
  journal      = {Neurocomputing},
  month        = {1},
  pages        = {131883},
  shortjournal = {Neurocomputing},
  title        = {Worst-case convergence analysis of relatively inexact gradient descent on smooth convex functions},
  volume       = {660},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Improving on early exaggeration in t-SNE: Early hierarchization better preserves global structure. <em>NEUCOM</em>, <em>660</em>, 131882. (<a href='https://doi.org/10.1016/j.neucom.2025.131882'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In dimensionality reduction, t -SNE is a local method of neighbor embedding that requires to be carefully initialized in order to preserve the global structure of data to a good extent. In standard t -SNE, the low-dimensional embedding is initialized either randomly or with PCA. Next, gradient descent runs for two successive phases to refine the embedding coordinates iteratively. In the first phase, named early exaggeration , the attractive forces between points are artificially strengthened to prevent the repulsive forces to scatter fragments of the still poorly organized embedding, before the second phase takes over with the genuine gradient, until final convergence. A novel initialization of t -SNE is proposed in this extended work. It proceeds by hierarchizing the data points into a space-partitioning binary tree that yields faithful subsamples of data with 4 , 8 , 16 , …, 2 ⌊ log 2 ⁡ N ⌋ , N points; t -SNE runs on these growing subsamples, each obtained embedding initializing the next run. Between two runs, the prototypical point in each tree branch is split into its two children and the embedding is rescaled to account for the increased population. Extended experimental results with 5 repetitions show quantitatively the effectiveness of the method on a variety of artificial and real data sets, while running times get only multiplied by a small constant factor, leaving the computational complexity unchanged. This confirms that early hierarchization can advantageously replace initialization and early exaggeration, making t -SNE a more homogeneous method with fewer meta-parameters. The proposed method is compatible with any method of neighbor embedding ( t -SNE, UMAP, etc.) with quadratic, log-linear, or even linear iterations, provided early exaggeration can be disabled and initial coordinates of the embedded data points can be specified.},
  archive      = {J_NEUCOM},
  author       = {John A. Lee and Edouard Couplet and Pierre H. Lambert and Pierre Merveille and Ludovic Journaux and Dounia Mulders and Cyril de Bodt and Michel Verleysen},
  doi          = {10.1016/j.neucom.2025.131882},
  journal      = {Neurocomputing},
  month        = {1},
  pages        = {131882},
  shortjournal = {Neurocomputing},
  title        = {Improving on early exaggeration in t-SNE: Early hierarchization better preserves global structure},
  volume       = {660},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). A survey of deep learning based knowledge tracing from cognitive processing perspective. <em>NEUCOM</em>, <em>660</em>, 131879. (<a href='https://doi.org/10.1016/j.neucom.2025.131879'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Knowledge tracing (KT) has emerged as a prominent research area in intelligent education. Its primary aim is to assess students’ knowledge mastery based on their historical interactions, ultimately predicting future performance. With the rapid advancement of deep neural networks, deep learning technologies have become integral to this domain, giving rise to Deep Learning-based Knowledge Tracing (DLKT), which is adopted by a majority of scholars. However, existing DLKT surveys mainly focus on technical advancements, ignoring educational principles. This absence of a comprehensive review of DLKT methods from a cognitive processing perspective may lead to deviations from the memory principles inherent in learning process. Therefore, this paper endeavors to provide an extensive overview of recent representative DLKT methods from a cognitive processing perspective. In pursuit of a more comprehensive understanding of the KT task, we extract three key functions and propose a unified model adaptable to most DLKT methods. Additionally, we present a general DLKT framework comprising four essential modules: educational features, input representation, architectures for learning process modeling, and output representation. Furthermore, we propose a novel classification that incorporates both technical and cognitive processing perspectives, followed by an extensive analysis of all representative DLKT methods. Finally, we review commonly employed datasets, discuss experimental results, and explore the application of visualization, interpretability, and personalized interventions.},
  archive      = {J_NEUCOM},
  author       = {Huali Yang and Junjie Hu and Shengze Hu and Zhuoran Xu and Xinjia Ou and Jing Geng and Linxia Tang and Tao Huang},
  doi          = {10.1016/j.neucom.2025.131879},
  journal      = {Neurocomputing},
  month        = {1},
  pages        = {131879},
  shortjournal = {Neurocomputing},
  title        = {A survey of deep learning based knowledge tracing from cognitive processing perspective},
  volume       = {660},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). A concept-cognitive learning approach from the perspective of causal reasoning. <em>NEUCOM</em>, <em>660</em>, 131876. (<a href='https://doi.org/10.1016/j.neucom.2025.131876'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Concept-cognitive learning (CCL) aims to simulate the formation and evolution of human conceptual understanding. However, existing CCL models often ignore causal relationships and assign equal importance to all attributes, limiting their ability to capture critical information. To address this, we propose a novel method named causal-POFSA, which integrates causal reasoning and attribute weighting into the CCL framework. Specifically: (1) We introduce a causal reasoning mechanism to identify the causal relationships between attributes and decision classes; (2) We construct a causal concept space using a partial order structure and causal power computation; (3) We generate a weighted pseudo-concept space via similarity-driven clustering and perform concept prediction based on a weighted distance measure. Experiments on nine public datasets demonstrate that the proposed method effectively captures causal relationships and constructs meaningful concept representations, and the results further indicate that causal-POFSA consistently achieves higher accuracy and balanced performance across multiple evaluation metrics, thereby confirming both its robustness and interpretability.},
  archive      = {J_NEUCOM},
  author       = {Ningqing Xie and Enliang Yan and Xinyu Yao and Qiliang Chen and Tianyong Hao},
  doi          = {10.1016/j.neucom.2025.131876},
  journal      = {Neurocomputing},
  month        = {1},
  pages        = {131876},
  shortjournal = {Neurocomputing},
  title        = {A concept-cognitive learning approach from the perspective of causal reasoning},
  volume       = {660},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). From discrete to continuous: A spatiotemporal evolution-aware adversarial diffusion framework for retinal disease progression prediction. <em>NEUCOM</em>, <em>660</em>, 131873. (<a href='https://doi.org/10.1016/j.neucom.2025.131873'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Diabetic macular edema (DME) is one of the leading causes of vision loss in patients with diabetes. Although anti-vascular endothelial growth factor (anti-VEGF)-injection is the most commonly used treatment, individual responses to treatment vary significantly. Therefore, accurately predicting treatment outcomes is crucial for developing personalized therapeutic strategies. Compared to classification or regression approaches, image prediction provides a more intuitive depiction of retinal structural changes. However, most existing methods fail to model the spatiotemporal continuity inherent in follow-up data and rely on discrete, single-time-point inputs, limiting both predictive performance and clinical interpretability. To address these limitations, we propose a novel two-stage prediction framework that integrates spatiotemporal evolution modeling with adversarial conditional diffusion generation to forecast post-treatment optical coherence tomography (OCT) images with DME. In the first stage, we innovatively adopt an implicit neural representation (INR) to learn a continuous mapping from spatiotemporal coordinates to voxel intensities, enabling the modeling of continuous retinal evolution trajectories and preliminary image prediction. In the second stage, to mitigate the oversmoothed texture issue commonly observed in INR outputs, we introduce an adversarial conditional diffusion model to enhance image detail. Specifically, we design a dual-stream alternating guidance mechanism that integrates INR predictions with features from the original follow-up images to jointly guide the generation process. Additionally, we propose a dual-supervision adversarial module that combines noise-prediction loss with a discriminative image-level constraint, thereby producing anatomically plausible and visually realistic predictions. Extensive experiments on the DME and neovascular age-related macular degeneration (nAMD) prediction tasks demonstrate that our method achieves continuous, high-fidelity OCT prediction, offering a promising tool to support individualized treatment planning for retinal disease. Our code is accessible at: https://github.com/bemyself96/STE-ACD .},
  archive      = {J_NEUCOM},
  author       = {Xiaohui Li and Yuhan Zhang and Sijie Niu and Songtao Yuan and Qiang Chen},
  doi          = {10.1016/j.neucom.2025.131873},
  journal      = {Neurocomputing},
  month        = {1},
  pages        = {131873},
  shortjournal = {Neurocomputing},
  title        = {From discrete to continuous: A spatiotemporal evolution-aware adversarial diffusion framework for retinal disease progression prediction},
  volume       = {660},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Preference modeling with multi-graph graph attention network. <em>NEUCOM</em>, <em>660</em>, 131872. (<a href='https://doi.org/10.1016/j.neucom.2025.131872'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Accurately predicting user preferences requires leveraging complex relational data from multiple sources (e.g., user-item interactions, social networks, item similarities). While Graph Neural Networks (GNNs) excel at learning from graph-structured data, existing models often fail to fully integrate and differentiate the influence of these diverse network graphs, limiting prediction accuracy. To address this, we propose a Multi-Graph Graph Attention Network (MG-GAT) specifically for preference learning. MG-GAT’s key contribution lies in its ability to effectively integrate information from multiple network graphs with a two-level attention mechanism that enables nuanced and adaptive integration of multiple graph information. At the first level, intra-network attention identifies the most informative neighbors within each graph. Then the inter-network attention dynamically weighs the contribution of each graph view to the final representation. Unlike prior multi-view GNNs, which assume shared nodes or treat all relations equally, MG-GAT preserves the source clarity of each graph and avoids mixing different views indiscriminately. This hierarchical design allows MG-GAT to preserve structural diversity and contextually prioritize relevant information, leading to more expressive embeddings of users and items. Furthermore, the inherent ability to leverage network-based relationships also provides an auxiliary benefit in mitigating cold-start scenarios for new users or items. Extensive experiments on three real-world datasets demonstrate that MG-GAT substantially outperforms state-of-the-art baselines, validating the effectiveness of multi-graph integration and two-level attention modeling.},
  archive      = {J_NEUCOM},
  author       = {Yipeng Zhuang and Chenlu Wang and Philip L.H. Yu},
  doi          = {10.1016/j.neucom.2025.131872},
  journal      = {Neurocomputing},
  month        = {1},
  pages        = {131872},
  shortjournal = {Neurocomputing},
  title        = {Preference modeling with multi-graph graph attention network},
  volume       = {660},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). DPBPRMF: A rigorous differential privacy scheme with bayesian personalized ranking for implicit recommendation. <em>NEUCOM</em>, <em>660</em>, 131871. (<a href='https://doi.org/10.1016/j.neucom.2025.131871'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The large-scale and sensitive data collection and processing in recommender systems (RSs) cause privacy issues to take a prior stage. Approaches have employed differential privacy (DP) in matrix factorization (MF) models, which play a core role in privacy-protected RSs. However, these existing methods are not well-suited for implicit feedback data. Moreover, quantitatively analyzing the sensitivity of DP under the implicit context is challenging, and the difficulty in determining the upper bound may lead to waste of the privacy budget. Thus, we propose DPBPRMF, a rigorous and implicit differential privacy MF recommendation algorithm. It leverages the Bayesian personalized ranking (BPR) and sampling techniques to enhance the efficiency of implicit data utilization. To our knowledge, the privacy loss calculation is the first meticulous analysis of the BPR-MF framework. We further present a gradient perturbation strategy to achieve ϵ -DP, meeting users’ high privacy demands. Finally, theoretical arguments and experimental results demonstrate that the proposed method provides strong privacy protection while maintaining recommendation quality.},
  archive      = {J_NEUCOM},
  author       = {Yongdong Wang and Zhiqiang Zhang and Chenhong Luo and Jiangzhou Deng and Jianmei Ye and Yong Wang and Kobiljon Kh. Khushvakhtsoda},
  doi          = {10.1016/j.neucom.2025.131871},
  journal      = {Neurocomputing},
  month        = {1},
  pages        = {131871},
  shortjournal = {Neurocomputing},
  title        = {DPBPRMF: A rigorous differential privacy scheme with bayesian personalized ranking for implicit recommendation},
  volume       = {660},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Efficient network compression via gradient-score aware pruning. <em>NEUCOM</em>, <em>660</em>, 131870. (<a href='https://doi.org/10.1016/j.neucom.2025.131870'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Convolutional neural networks (CNNs) have demonstrated significant achievements in the field of computer vision, yet their high computational demands restrict practical applications. Current pruning methods seek to mitigate this issue, which however often rely on heuristic manual approaches, encountering challenges in maintaining both significant model compression and accuracy. To address the above issues, a fast neural architecture search pruning (FNP) technique is proposed in this paper. Firstly, an importance matrix (IM) based preprocessing stage efficiently removes redundant structures by considering both weight importance and computational complexity, providing a compact baseline for subsequent pruning. Secondly, we adapt fast genetic algorithms (FGA) to identify optimally pruned model configurations. Furthermore, to accelerate the search process, we utilize a zero-shot learning approach to estimate model performance with the score of the frame (SoF), which is a gradient-based score. Compared with state-of-the-art (SOTA) pruning techniques, FNP demonstrates superior performance in terms of search duration and compression ratio. On the CIFAR-10 dataset, our method removes 95.24 % of the parameters in VGG-16 while achieving a 0.72 % accuracy improvement compared with the baseline. On the ImageNet dataset, we prune 68.98 % of the parameters in ResNet-50 and obtain a 1.2 % accuracy improvement compared with state-of-the-art (SOTA) approaches, while reducing the search time by 98.94 %. The code is available at https://github.com/aqiu1222/FNP.git},
  archive      = {J_NEUCOM},
  author       = {Qiuying Li and Zhixiang Chen and Yu Li and Zhiyuan Jiang and Shan Cao},
  doi          = {10.1016/j.neucom.2025.131870},
  journal      = {Neurocomputing},
  month        = {1},
  pages        = {131870},
  shortjournal = {Neurocomputing},
  title        = {Efficient network compression via gradient-score aware pruning},
  volume       = {660},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Invertible koopman network with external inputs: A novel data-driven modeling method for non-autonomous nonlinear systems with discrete spectrum. <em>NEUCOM</em>, <em>660</em>, 131869. (<a href='https://doi.org/10.1016/j.neucom.2025.131869'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Koopman operator theory is a popular candidate for data-driven modeling because it provides a global linearization representation of the nonlinear dynamical systems. This work develops a novel data-driven modeling method for non-autonomous systems with discrete spectra, denoted as the Invertible Koopman Network with external inputs. The Invertible Neural Network is utilized to obtain the parameterized observable function, which does not introduce additional assumptions about the observable function’s form and can explicitly provide the corresponding inverse without relying on the reconstruction loss. Additionally, the input lifting function is also realized by another Invertible Neural Network rather than assuming it to be an identity. The original system’s states and external inputs are both encoded, enabling the proposed method to better model the nonlinear state-dependent term in general nonlinear systems. Two learnable linear matrices are utilized to approximate the Koopman operator, resulting in linear dynamics in the embedding space. Based on this, linearization control schemes for the nonlinear dynamical systems are further developed. Various numerical examples demonstrate the effectiveness and superiority of our proposed approach. Finally, we examine and verify the proposed method’s data-driven modeling performance in a real-world nonlinear isolation system through a dynamic experiment.},
  archive      = {J_NEUCOM},
  author       = {Yuhong Jin and Lei Hou and Haiming Yi and Shun Zhong and Jun Li and Shijun Wang and Hao Chen},
  doi          = {10.1016/j.neucom.2025.131869},
  journal      = {Neurocomputing},
  month        = {1},
  pages        = {131869},
  shortjournal = {Neurocomputing},
  title        = {Invertible koopman network with external inputs: A novel data-driven modeling method for non-autonomous nonlinear systems with discrete spectrum},
  volume       = {660},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Synchronization mechanisms in coupled neural networks with time-varying topologies: From edge-level rewiring to network-wide collective behaviors. <em>NEUCOM</em>, <em>660</em>, 131868. (<a href='https://doi.org/10.1016/j.neucom.2025.131868'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This study investigates the synchronization issue for coupled neural networks with time-varying communication topologies, with a particular focus on establishing the cross-scale relationship between microscopic-scale edge rewiring dynamics and macroscopic network synchronization behaviors. Firstly, some characteristic parameters are introduced to quantify the dynamic rewiring behaviors of each edge within the network. Leveraging these parameters and employing the techniques such as Jensen’s inequality and Barbalat’s lemma, some synchronization conditions related to the dynamic rewiring characteristics of edges are established. Based on these conditions, the impact of edge rewiring dynamics on network synchronization is analyzed. Our analysis reveals that the synchronization becomes more likely when: (1) the time required for the switching network topologies to achieve joint connectivity is shorter, and (2) the duration each edge exists within a joint connectivity period is longer. Notably, the derived synchronization conditions impose no requirement for a positive lower bound on the communication topologies’ dwell time. In contrast to the classical average dwell-time approach, these conditions impose fewer constraints on topology switching frequency and exhibit wider applicability. Furthermore, these synchronization conditions impose weak connectivity requirements on the switching topologies, necessitating only joint connectivity. Finally, some numerical experiments confirm the validity of the theoretical results.},
  archive      = {J_NEUCOM},
  author       = {Xudong Hai and Qingyun Wang and Ting Kang and Boqiang Cao},
  doi          = {10.1016/j.neucom.2025.131868},
  journal      = {Neurocomputing},
  month        = {1},
  pages        = {131868},
  shortjournal = {Neurocomputing},
  title        = {Synchronization mechanisms in coupled neural networks with time-varying topologies: From edge-level rewiring to network-wide collective behaviors},
  volume       = {660},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). GRIFFIN: A gated relaxing interaction-aware fuzzy framework with integrated negation-aware neurocomputing to learn incomplete and non-separable rules for classification. <em>NEUCOM</em>, <em>660</em>, 131866. (<a href='https://doi.org/10.1016/j.neucom.2025.131866'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper proposes a novel gated neuro-fuzzy architecture that can gate only relevant input variables to form non-separable and incomplete fuzzy rules, considering nonlinear interactions among them, while applying relaxation to the effect of redundant features in defining rules (GRIFFIN). GRIFFIN learns to select important input variables to extract the feature space required for defining each fuzzy rule through an adaptive gating mechanism. Next, it learns to map the selected input subspace to a new feature embedding for each fuzzy rule to consider the interactions among the selected input variables related to the covered region of the fuzzy rule. Then, it decides whether the positive or negative literal for each linguistic value should be considered. Afterward, it learns to cancel the effect of irrelevant linguistic terms in the embedding space to form incomplete fuzzy rules. Consequently, the fuzzy rules are formed by taking into account the interactions among input variables while they have various numbers of positive or negative predicates in their antecedent parts. Thus, the formed fuzzy rules would be incomplete, non-separable, and in the conjunctive normal form, which makes the inference more general. The consequent parts utilize the selected features that are extracted for each fuzzy rule. This leads to providing skip connections that improve learning through error backpropagation. A novel unsupervised data-driven interaction-aware initialization method called Fuzzy Explanation Maximization (FExMax) is proposed based on maximizing the rules’ coverage. Afterward, the rules’ parameters are fine-tuned by applying error backpropagation. The performance of GRIFFIN is studied in real-world classification applications which confirm its capability to achieve the highest performance with the lowest number of rules. GRIFFIN outperforms previous SOTA with 7.9 % improvement in terms of Accuracy.},
  archive      = {J_NEUCOM},
  author       = {Armin Salimi-Badr and Aras Valizadeh and Mohammad Mahdi Parchamijalal},
  doi          = {10.1016/j.neucom.2025.131866},
  journal      = {Neurocomputing},
  month        = {1},
  pages        = {131866},
  shortjournal = {Neurocomputing},
  title        = {GRIFFIN: A gated relaxing interaction-aware fuzzy framework with integrated negation-aware neurocomputing to learn incomplete and non-separable rules for classification},
  volume       = {660},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). YCSC-UNet: A Y-shaped composite spatial channel network based on U-net for breast lesion ultrasound image segmentation. <em>NEUCOM</em>, <em>660</em>, 131865. (<a href='https://doi.org/10.1016/j.neucom.2025.131865'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Since the first global assessment of cancer burden in the 1980s, breast cancer has remained the most prevalent cancer among women. The accurate segmentation of breast tumors on ultrasound images is of significant importance for early detection of breast cancer. However, current approaches still face challenges such as contextual information loss and inadequate feature capture capabilities. To address these issues, this work proposes a Y-shaped composite spatial channel network based on U-Net (YCSC-UNet). We have four contributions: 1) Y-shaped structural branches that continuously replenish contextual information; 2) Efficient Encoding Component (EEC) to deliver effective information with minimal parameter overhead; 3) Dual Long-Range Channel Attention (DLRCA), a composite channel attention mechanism for refined feature fusion; 4) Long-Range Spatial Attention (LRSA), integrating spatial attention to comprehensively extract long-range dependencies. Extensive experiments on two public datasets demonstrate that YCSC-UNet achieves superior performance on metrics including Dice and Jaccard coefficients, outperforming multiple state-of-the-art methods. The code is available at: https://github.com/wwiinndd/YCSC-UNet .},
  archive      = {J_NEUCOM},
  author       = {Wenjun Ma and Qian Jiang and Qianqian Wang and Dongjian Yu and Yilong Huang and Bo He and Xin Jin},
  doi          = {10.1016/j.neucom.2025.131865},
  journal      = {Neurocomputing},
  month        = {1},
  pages        = {131865},
  shortjournal = {Neurocomputing},
  title        = {YCSC-UNet: A Y-shaped composite spatial channel network based on U-net for breast lesion ultrasound image segmentation},
  volume       = {660},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). LUTFormer: Lookup table transformer for image enhancement. <em>NEUCOM</em>, <em>660</em>, 131863. (<a href='https://doi.org/10.1016/j.neucom.2025.131863'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Existing image enhancement methods based on 3D lookup tables (LUTs) often yield suboptimal results by oversimplifying image context into a single global feature and disrupting the inherent geometric structure of a LUT during regression. To address these issues, we propose LUTFormer, a novel framework that reframes LUT prediction as a query-based refinement task. LUTFormer preserves geometric integrity by initializing LUT grid points as structured query tokens, which are then progressively refined by a transformer decoder. This decoder leverages a novel progressive cross-attention mechanism to inject multi-level image context, yielding a context-aware LUT transformation. Extensive experiments on multiple benchmark datasets confirm the effectiveness and efficiency of the proposed LUTFormer. The source code is available at https://github.com/Jinwon-Ko/LUTFormer .},
  archive      = {J_NEUCOM},
  author       = {Jinwon Ko and Keunsoo Ko and Hanul Kim and Chang-Su Kim},
  doi          = {10.1016/j.neucom.2025.131863},
  journal      = {Neurocomputing},
  month        = {1},
  pages        = {131863},
  shortjournal = {Neurocomputing},
  title        = {LUTFormer: Lookup table transformer for image enhancement},
  volume       = {660},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Privacy-preserving 3D human skeleton reconstruction from ankle-level 2D LiDAR using deep learning. <em>NEUCOM</em>, <em>660</em>, 131862. (<a href='https://doi.org/10.1016/j.neucom.2025.131862'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Human skeleton reconstruction enables smart monitoring and biomechanical analysis, yet RGB/RGB-D systems face privacy concerns in public spaces, illumination sensitivity, and occlusion. To overcome these limitations, we propose a privacy-preserving approach for 3D skeleton reconstruction using ankle-level 2D LiDAR-based Motion History Images (MHIs). Our method integrates a direction-guided multi-head deep learning model, leveraging ResNet18 as the backbone to extract features from the MHIs and classify walking directions (front, left, right, back), thereby improving pose estimation across diverse orientations. Experiments comparing ResNet18, ResNet50, and DenseNet121 demonstrate significant improvements in accuracy, with DenseNet121 showing the best performance in terms of joint position error, while ResNet18 offers the best trade-off between accuracy and inference speed for real-time deployment. An ablation study reveals the importance of direction classification, with the removal of this feature leading to increased errors, especially in joints such as the spine base. We further validate anatomical consistency by estimating subject-specific height from predicted skeletons, observing consistent accuracy across individuals. These results position our approach as a robust and privacy-conscious alternative to conventional methods for 3D skeleton reconstruction.},
  archive      = {J_NEUCOM},
  author       = {Md Mohibullah and Yusuke Suda and Yuhei Hironaka and Takuma Miyawaki and Ryota Suzuki and Mahmudul Hasan and Yoshinori Kobayashi},
  doi          = {10.1016/j.neucom.2025.131862},
  journal      = {Neurocomputing},
  month        = {1},
  pages        = {131862},
  shortjournal = {Neurocomputing},
  title        = {Privacy-preserving 3D human skeleton reconstruction from ankle-level 2D LiDAR using deep learning},
  volume       = {660},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Average controllability of N-duplication corona product networks. <em>NEUCOM</em>, <em>660</em>, 131860. (<a href='https://doi.org/10.1016/j.neucom.2025.131860'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The average controllability can quantify the difficulty of controlling a network based on controllability Gramian metrics in terms of energy triggered by the leaders. This work addresses the average controllability of a class of composite networks generated by factor networks via N -duplication corona product, considering the simple undirected factor networks with Laplacian dynamics within a leader-follower framework, and further provides a technique to characterize the average controllability of N -duplication corona product networks ( N -CPNs) by the spectral properties of factors, which greatly reduces the computational complexity and analytical difficulty. In addition, it is found that the ‘size’ of average controllability (i.e., trace of Gramian matrix) is dependent on N , the assignment of leaders, the number of agents and the spectral properties of factors.},
  archive      = {J_NEUCOM},
  author       = {Qiang Zhang and Junjie Huang and Bo Liu and Housheng Su and Alatancang Chen},
  doi          = {10.1016/j.neucom.2025.131860},
  journal      = {Neurocomputing},
  month        = {1},
  pages        = {131860},
  shortjournal = {Neurocomputing},
  title        = {Average controllability of N-duplication corona product networks},
  volume       = {660},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). DBTSP-net: A temporal-spatial parallel network with optuna optimization for subject-specific motor imagery EEG decoding and visualization. <em>NEUCOM</em>, <em>660</em>, 131858. (<a href='https://doi.org/10.1016/j.neucom.2025.131858'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The accuracy and stability of decoding EEG-based motor imagery (MI-EEG) is critical for achieving effective human-machine interaction and promoting motor function recovery in patients with severe motor dysfunction. In this paper, we propose a novel dual-branch temporal–spatial parallel hybrid classification network named DBTSP-Net. In addition, we introduce an adaptive weighted feature fusion method to decode MI-EEG signals on the basis of the Optuna optimization algorithm. Nine subjects were recruited to participate in the MI-EEG decoding experiment. We evaluated the classification performance of both conventional and state-of-the-art MI-EEG models using the public BCI Competition IV 2a and 2b datasets. The experimental results demonstrated that the classification performance of DBTSP-Net surpassed that of the other baseline methods, attaining average classification accuracies of 79.61 % ± 14.43 and 86.21 % ± 12.17, respectively, with corresponding kappa values of 0.7856 and 0.7189, respectively. We further conducted ablation experiments to verify the rationality of the design of each module. Additionally, EEG topological maps and t-distributed stochastic neighbor embedding (t-SNE) were utilized for feature visualization. The decoding accuracy of MI-EEG signals was increased, and a solid theoretical foundation for the future practical application of MI-BCI systems in motion control and neural rehabilitation training was obtained. The code has been released at https://github.com/xinchenPhD/DBTSPNet .},
  archive      = {J_NEUCOM},
  author       = {Xin Chen and Longjie Yu and Hongze Lin and Mingyu Du and Wei Wei and Xinyu Wu and Guanjun Bao and Shibo Cai},
  doi          = {10.1016/j.neucom.2025.131858},
  journal      = {Neurocomputing},
  month        = {1},
  pages        = {131858},
  shortjournal = {Neurocomputing},
  title        = {DBTSP-net: A temporal-spatial parallel network with optuna optimization for subject-specific motor imagery EEG decoding and visualization},
  volume       = {660},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Adaptive RBF neural network-based boundary consensus control for nonlinear reaction–diffusion agents with non-collocated measurements. <em>NEUCOM</em>, <em>660</em>, 131854. (<a href='https://doi.org/10.1016/j.neucom.2025.131854'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article proposes a constrained boundary controller to achieve practical consensus for nonlinear reaction–diffusion agents in the presence of unknown uncertainties and external disturbances. The proposed non-collocated observer, in contrast to the conventional assumption based on the Lipschitz condition, incorporates an adaptive radial basis function (RBF) neural network law to accurately estimate the nonlinear dynamics in the spatiotemporal domain. By enhancing the Lyapunov direct approach, a linear matrix inequality (LMI)-based sufficient condition is derived to ensure the convergence of consensus errors, even in the presence of unknown terms. Furthermore, an invariant set is presented by constructing the relationship between the boundary input and Lyapunov function to address the problem of actuator saturation. The comparison results ultimately demonstrate the effectiveness and merits of the proposed methodology.},
  archive      = {J_NEUCOM},
  author       = {Jiaqi Zhong and Yan Feng and Huai-Ning Wu},
  doi          = {10.1016/j.neucom.2025.131854},
  journal      = {Neurocomputing},
  month        = {1},
  pages        = {131854},
  shortjournal = {Neurocomputing},
  title        = {Adaptive RBF neural network-based boundary consensus control for nonlinear reaction–diffusion agents with non-collocated measurements},
  volume       = {660},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Dual-branch normalizing flow for anomaly detection and localization from images. <em>NEUCOM</em>, <em>660</em>, 131850. (<a href='https://doi.org/10.1016/j.neucom.2025.131850'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In visual anomaly detection, unexpected or abnormal patterns are identified from given image samples. Existing methods for visual anomaly detection are mainly divided into two types: semantic inter-class (i.e., image-level) anomaly detection and texture intra-class anomaly detection. The normalizing flow-based method can effectively map normal training data to a Gaussian distribution due to its excellent distribution expression ability, thus interpretably identifying lower likelihood abnormal data from normal data. However, with normalizing flow based methods, it is challenging to achieve high precision detection and localization of anomalies due to the unpredictable scale of the anomalies within texture anomalies. The large scale span of potential anomalous textures makes it difficult to balance the learning of distributions from both global and local features in existing normalizing flow methods. To address this limitation, we propose a dual-branch architecture to model the density mapping of global and local features, respectively. Our proposed model can achieve coarse-grained and fine-grained image anomaly detection and localization, by modeling both the global features and local texture attributes of the input images with a dual branch normalizing flow. Furthermore, we design a Dynamic Spatial Attention Module (DSAM) in each branch of the flow module to enhance the model’s ability to capture anomaly features. Extensive experiments on two public datasets have demonstrated that our model is effective in detecting various real-world sample defects, especially in unsupervised visual anomaly detection tasks, achieving substantially promising results. The codes are available at https://github.com/SYLan2019/DNFAD .},
  archive      = {J_NEUCOM},
  author       = {Yao Li and Wei Ma and Shuai He and Shiyong Lan and Wenwu Wang and Yixin Qiao and Guonan Deng},
  doi          = {10.1016/j.neucom.2025.131850},
  journal      = {Neurocomputing},
  month        = {1},
  pages        = {131850},
  shortjournal = {Neurocomputing},
  title        = {Dual-branch normalizing flow for anomaly detection and localization from images},
  volume       = {660},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). FA-seg: A fast and accurate diffusion-based method for open-vocabulary segmentation. <em>NEUCOM</em>, <em>660</em>, 131844. (<a href='https://doi.org/10.1016/j.neucom.2025.131844'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Open-vocabulary semantic segmentation (OVSS) aims to segment objects from arbitrary text categories without requiring densely annotated datasets. Although contrastive learning based models enable zero-shot segmentation, they often lose fine spatial precision at the pixel level, due to global representation bias. In contrast, diffusion-based models naturally encode fine-grained spatial features via attention mechanisms that capture both global context and local details. However, they often face challenges in balancing computational costs and the quality of the segmentation mask. In this work, we present FA-Seg, a F ast and A ccurate training-free framework for open-vocabulary segmentation based on diffusion models. FA-Seg performs segmentation using only a (1+1)-step from a pretrained diffusion model. Moreover, instead of running multiple times for different classes, FA-Seg performs segmentation for all classes at once. To further enhance the segmentation quality, FA-Seg introduces three key components: (i) a dual-prompt mechanism for discriminative, class-aware attention extraction, (ii) a Hierarchical Attention Refinement Method (HARD) that enhances semantic precision via multi-resolution attention fusion, and (iii) a Test-Time Flipping (TTF) scheme designed to improve spatial consistency. Extensive experiments show that FA-Seg achieves state-of-the-art training-free performance, achieving 43.8 % average mIoU across PASCAL VOC, PASCAL Context, and COCO Object benchmarks while maintaining superior inference efficiency. Our results demonstrate that FA-Seg provides a strong foundation for extendability, bridging the gap between segmentation quality and inference efficiency. Code is available at https://github.com/chequanghuy/FA-Seg .},
  archive      = {J_NEUCOM},
  author       = {Huy Che and Vinh-Tiep Nguyen},
  doi          = {10.1016/j.neucom.2025.131844},
  journal      = {Neurocomputing},
  month        = {1},
  pages        = {131844},
  shortjournal = {Neurocomputing},
  title        = {FA-seg: A fast and accurate diffusion-based method for open-vocabulary segmentation},
  volume       = {660},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). StyleBoost: Controlling style-content fusion with SVD for text-driven. <em>NEUCOM</em>, <em>660</em>, 131843. (<a href='https://doi.org/10.1016/j.neucom.2025.131843'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, text-driven style transfer techniques have made significant progress in the text-to-image (T2I) generation domain, demonstrating strong potential in fine-grained style control. However, existing methods still face several challenges, including overfitting to reference image styles, difficulty in effectively disentangling style from semantic content, and insufficient alignment between generated images and text prompts. These issues limit the diversity and controllability of generation results. To address these challenges, this paper proposes two strategies to enhance style representation and text alignment: First, based on Singular Value Decomposition (SVD), we analyze the content and style components within reference image embeddings and design a style extraction method that regulates the nuclear norm of the approximated reference image embeddings. This approach effectively suppresses structural and content information in the reference image, enabling accurate extraction and preservation of style features, thereby mitigating content leakage. Second, we introduce an improved cross-attention alignment module that strengthens the fusion and interaction between text and approximated reference image embeddings, boosting the generated images’ responsiveness to textual semantics and accuracy in inheriting style. A series of comprehensive experiments show that the proposed approach notably enhances both style consistency and alignment with textual prompts, all without the need for extra training or fine-tuning. It also exhibits strong generalizability and scalability and is compatible with other control modules such as ControlNet. Our approach offers an effective solution for achieving high-quality, controllable, text-guided stylized image generation. Code is available: https://github.com/math-ddup/StyleBoost .},
  archive      = {J_NEUCOM},
  author       = {Shujie Zhang and Jingyue Wang and Meiqing Wang and Yuanxiang Fang and Huimin Liu},
  doi          = {10.1016/j.neucom.2025.131843},
  journal      = {Neurocomputing},
  month        = {1},
  pages        = {131843},
  shortjournal = {Neurocomputing},
  title        = {StyleBoost: Controlling style-content fusion with SVD for text-driven},
  volume       = {660},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). A multimodal knowledge synergistic pre-training framework with masked vision-language modeling for remote sensing. <em>NEUCOM</em>, <em>660</em>, 131841. (<a href='https://doi.org/10.1016/j.neucom.2025.131841'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep learning has been widely applied to Remote Sensing Image (RSI) analysis, such as scene classification and road segmentation, with ImageNet-based pre-trained models commonly used as backbone networks. However, the significant domain shift between RSIs and natural images can result in sub-optimal performance. To address this, Self-supervised Learning (SSL) has been explored to improve performance, but the reliance on a single image modality limits its potential performance gains. Meanwhile, large vision-language foundation models have demonstrated remarkable performance, but the training typically requires large-scale datasets and poses challenges in terms of reproducibility and network customization without substantial computational resources. This raises a fundamental question: Can we achieve knowledge-enhanced pre-training without relying on large-scale datasets? To this end, we propose a novel Multimodal Knowledge Synergistic Pre-training (MKSP) framework, which encapsulates image-language multimodal knowledge into pre-trained vision models. Our MKSP framework is built upon both Masked Image Modeling (MIM) and Masked Language Modeling (MLM), while the complementarity between the image and language domains is generated by two newly proposed loss functions: Global Contrastive Loss (GCL) and Object-aware Contrastive Loss (OCL). GCL models global information communications between image and language embeddings, while OCL injects fine-grained object-level knowledge into the pre-trained model. Driven by multimodal synergistic learning, the pre-trained model demonstrates strong transferability, even with a relatively small pre-training dataset. Specifically, with MKSP pre-training, we conduct performance evaluations on challenging RSI tasks: image classification, semantic segmentation, and object detection. Extensive experimental results showcase that MKSP can learn robust feature representations despite using minimal labeled samples.},
  archive      = {J_NEUCOM},
  author       = {Meng Lou and Yunliang Qi and Shiqiang Du and Chunbo Xu and Zhen Yang and Yulin Shen},
  doi          = {10.1016/j.neucom.2025.131841},
  journal      = {Neurocomputing},
  month        = {1},
  pages        = {131841},
  shortjournal = {Neurocomputing},
  title        = {A multimodal knowledge synergistic pre-training framework with masked vision-language modeling for remote sensing},
  volume       = {660},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Hierarchical encoder-decoder for image captioning. <em>NEUCOM</em>, <em>660</em>, 131833. (<a href='https://doi.org/10.1016/j.neucom.2025.131833'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Image captioning aims to encode the visual information from images and integrate it into text decoding to generate an accurate natural language descriptions. Existing methods often neglect the hierarchical structure of visual information, fail to model global relationships between visual elements, and have not yet explored the synergistic effect of multi-level semantics, resulting in inaccurate captions. To address these issues, we propose a Hierarchical Encoder-decoder for Image Captioning (HierCap) to guide text generation with hierarchical visual information at three levels: global (encompassing positional relationships), regional (highlighting principal objects), and grid (capturing local details). Specifically, the hierarchical encoder employs three dedicated sub-encoders to build complementary visual representations at each level. For the decoder, a hierarchical fusion module with four variants is provided to explore the cross-modal synergistic fusion between hierarchical visual features and textual features. Extensive experiments demonstrate that HierCap achieves state-of-the-art performance on four datasets: COCO, NoCaps, Flickr8k, and Flickr30k. The results validate the effectiveness of hierarchical visual encoding and cross-modal hierarchical fusion in generating accurate and semantically rich descriptions. The source code is available at https://github.com/Panlizhi/HierCap .},
  archive      = {J_NEUCOM},
  author       = {Lizhi Pan and Chengtian Song and Xiaozheng Gan and Keyu Xu and Mengqian Deng},
  doi          = {10.1016/j.neucom.2025.131833},
  journal      = {Neurocomputing},
  month        = {1},
  pages        = {131833},
  shortjournal = {Neurocomputing},
  title        = {Hierarchical encoder-decoder for image captioning},
  volume       = {660},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Equivariant feature extraction: Enhancing 3D point cloud analysis with robust rotational equivariance. <em>NEUCOM</em>, <em>660</em>, 131832. (<a href='https://doi.org/10.1016/j.neucom.2025.131832'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent advancements in 3D point cloud representation techniques have significantly facilitated research in downstream tasks such as classification and segmentation. Despite the impressive capabilities of existing methods, they often struggle with rotated data due to a lack of rotation equivariance. This paper introduces the Equivariant Feature Extractor (EFE), a feature extraction method based on equivariant representations and spherical harmonics. EFE encodes 3D position data using equivariant group representations and extracts high-quality equivariant features through a series of equivariant operators. This method can be seamlessly integrated into classic neural networks, ensuring rotation equivariance while introducing only a small number of learnable parameters. Experimental results demonstrate that integrating EFE into mainstream point cloud models achieves outstanding performance, particularly on rotated test sets. This study highlights EFE’s ability to extract equivariant features and its direct applicability to classic models, contributing significantly to improving downstream task performance and advancing point cloud feature extraction techniques.},
  archive      = {J_NEUCOM},
  author       = {Qianwei Tang and Baile Xu and Jian Zhao and Furao Shen},
  doi          = {10.1016/j.neucom.2025.131832},
  journal      = {Neurocomputing},
  month        = {1},
  pages        = {131832},
  shortjournal = {Neurocomputing},
  title        = {Equivariant feature extraction: Enhancing 3D point cloud analysis with robust rotational equivariance},
  volume       = {660},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Boosting confidence in class-incremental learning via discriminative local feature enhancement. <em>NEUCOM</em>, <em>660</em>, 131829. (<a href='https://doi.org/10.1016/j.neucom.2025.131829'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Class-incremental learning aims to enable models to continuously learn new classes without accessing samples from previously learned classes, which is a challenging scenario as it requires learning new class knowledge while preventing the forgetting of knowledge related to old classes. An effective class-incremental learning approach involves isolating task-specific parameters for different tasks to mitigate interference-induced forgetting. However, this method still faces challenges, such as the model’s difficulty in accurately focusing on discriminative features of each class and its susceptibility to interference from similar background information across different classes. To address these issues, this study proposes a novel strategy for selecting discriminative local features in images. By leveraging the background information of images, the strategy identifies and extracts key local features critical for model discrimination, directing the model to prioritize these important features during training to enhance classification confidence. Additionally, the learning of visual representations is guided by semantic textual features from a pre-trained vision–language model, further improving the model’s class-incremental learning performance. Extensive empirical evaluations under various settings on multiple image classification datasets demonstrate that our method outperforms existing approaches by at least 1 % in accuracy.},
  archive      = {J_NEUCOM},
  author       = {Weizhuo Zhang and Ping Hu and Qiu Li and Jiantao Tan and Zhijun Tan and Ruixuan Wang},
  doi          = {10.1016/j.neucom.2025.131829},
  journal      = {Neurocomputing},
  month        = {1},
  pages        = {131829},
  shortjournal = {Neurocomputing},
  title        = {Boosting confidence in class-incremental learning via discriminative local feature enhancement},
  volume       = {660},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Anchor transfer learning-based multi-view subspace clustering. <em>NEUCOM</em>, <em>660</em>, 131827. (<a href='https://doi.org/10.1016/j.neucom.2025.131827'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Anchor-based multi-view subspace clustering algorithms have attracted significant attention due to their superior computational efficiency. However, existing methods face limitations when dealing with redundant noise in the original data, discrepancies in data distributions across different views, and heterogeneity between view domains. These issues significantly compromise the quality of the anchor points. To address these challenges, this paper introduces a novel algorithm called Anchor Transfer Learning-based Multi-view Subspace Clustering (ATMVC). ATMVC utilizes a projection matrix to map the original data into a low-rank subspace. This mapping helps extract informative features while minimizing the influence of redundancy and noise. A dual-anchor strategy is adopted to efficiently capture consensus and complementary information across views, ensuring information integrity. Furthermore, inspired by transfer learning, the algorithm guides both types of anchor points to learn from each other within a unified framework. This design effectively reduces the negative impact caused by inter-view domain discrepancies. Therefore, ATMVC can overcome the limitations of existing methods by enhancing anchor quality and leveraging inter-view knowledge transfer. Comparative experiments with state-of-the-art methods demonstrate that the proposed approach offers comprehensive advantages in both performance and efficiency.},
  archive      = {J_NEUCOM},
  author       = {Lin Bai and Jingxuan Liu and Qian Xue and Mengchen Sun and Xiaoying Pan},
  doi          = {10.1016/j.neucom.2025.131827},
  journal      = {Neurocomputing},
  month        = {1},
  pages        = {131827},
  shortjournal = {Neurocomputing},
  title        = {Anchor transfer learning-based multi-view subspace clustering},
  volume       = {660},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). SR-TCUR: Scalable and robust tubal CUR decomposition for large-scale multidimensional tensors. <em>NEUCOM</em>, <em>660</em>, 131826. (<a href='https://doi.org/10.1016/j.neucom.2025.131826'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Tensor factorization techniques are valuable tools in the processing and analysis of large-scale data, such as images and videos, especially the CUR factorization methods, as they offer several advantages over other decompositions, such as interpretability and computational efficiency. Moreover, tensor decompositions based on the t-product concept have attracted much attention due to their success in various applications. This paper proposes a scalable and efficient tubal CUR decomposition method based on t-product, called SR-TCUR, that samples the significant lateral and horizontal slices based on the deterministic sampling method. SR-TCUR only requires observing and loading a few data samples into memory to construct the low-rank tensor approximation, making it scalable for large-scale data tensors. Various experiments on synthetic and real-world datasets with different dimensions and characteristics are conducted to evaluate the performance of the SR-TCUR method. The experimental results demonstrate that SR-TCUR outperforms the state-of-the-art methods of deterministic tensor CUR (TCUR) based on the t-product in computational complexity. It achieves an approximation accuracy close to the standard tensor CUR decomposition based on the tensor discrete empirical interpolation selection method, but with significant computational efficiency. Moreover, we utilize SR-TCUR to solve the tensor completion problem and recover the missing data entries.},
  archive      = {J_NEUCOM},
  author       = {Muhammad A.A. Abdelgawad and Ray C.C. Cheung and Hong Yan},
  doi          = {10.1016/j.neucom.2025.131826},
  journal      = {Neurocomputing},
  month        = {1},
  pages        = {131826},
  shortjournal = {Neurocomputing},
  title        = {SR-TCUR: Scalable and robust tubal CUR decomposition for large-scale multidimensional tensors},
  volume       = {660},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Enhancing unsupervised unified anomaly detection via semi-supervised learning with multi-source uncertainty mining. <em>NEUCOM</em>, <em>660</em>, 131824. (<a href='https://doi.org/10.1016/j.neucom.2025.131824'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Semi-Supervised Learning (SSL) strategies are widely adopted in industrial applications to improve the performance of anomaly detection models. However, most existing SSL methods focus on modeling a limited range of known anomalies, requiring prior definition and labeling. Consequently, SSL has limited applicability when encountering unseen anomalies. In contrast, unsupervised anomaly detection models do not rely on prior anomaly knowledge and effectively handle unknown anomalies. In this study, we propose a semi-supervised approach to improve unsupervised anomaly detection models by gradually enhancing anomaly detection performance using a small set of unlabeled samples. Our approach comprises two plug-and-play modules: the Multi-source Uncertainty Mining Network (MUMNet) and the Anomaly Segmentation Network (ASNet). These modules aim to mitigate bias in the results of the reconstruction-based unsupervised anomaly detection model (base model). During training, MUMNet integrates multiple cross-attention modules and an uncertainty-weighted loss to iteratively identify commonalities and discrepancies in pseudo-labels generated by diverse unsupervised models. This enables MUMNet to adaptively select reliable labels, which then supervise ASNet in learning a unified decision boundary. During testing, only the synergy between ASNet and the base model is necessary to help identify unbiased differences associated with anomalies. We conducted extensive experiments on the MVTec AD dataset, demonstrating that integrating MUM-UAD led to performance enhancements of up to 16.0 % compared to the base model.},
  archive      = {J_NEUCOM},
  author       = {Borui Kang and Yuzhong Zhong and Lin Deng and Maoning Wang and Jianwei Zhang},
  doi          = {10.1016/j.neucom.2025.131824},
  journal      = {Neurocomputing},
  month        = {1},
  pages        = {131824},
  shortjournal = {Neurocomputing},
  title        = {Enhancing unsupervised unified anomaly detection via semi-supervised learning with multi-source uncertainty mining},
  volume       = {660},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). FSCFNet: Lightweight neural networks via multi-dimensional importance-aware optimization. <em>NEUCOM</em>, <em>660</em>, 131823. (<a href='https://doi.org/10.1016/j.neucom.2025.131823'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Network lightweighting has become an effective technique for compressing CNNs by eliminating redundant structures. However, several challenges remain unresolved. For instance, some convolution optimization methods decompose convolutions into multiple segments, which reduce FLOPs but simultaneously increase memory access overhead. Similarly, ensuring that quantization and pruning techniques achieve substantial improvements in computational efficiency without compromising accuracy remains a pressing challenge. Most importantly, many methods neglect hardware adaptation, resulting in no significant performance improvement. To address these limitations, we propose a framework that integrates multi-dimensional evaluation with hardware-aware optimization. 1) We introduce a multi-dimensionally important convolution module. By selectively processing only the most informative features, this module substantially reduces both Floating Point Operations per Second (FLOPs) and memory access, while maintaining accuracy. 2) We propose a mixed-precision quantization module based on multi-dimensional importance and hardware awareness. This module assigns different precision levels (high, medium, or low) to features according to their importance and how well the hardware adapts to various computational precisions. This design markedly reduces the parameter count with only marginal accuracy degradation. 3) We introduce a channel pruning module based on channel contribution assessment. Through structured pruning of channels with minimal contribution to prediction accuracy, this module further improves computational efficiency with negligible accuracy loss. We validate the proposed framework on both GPU- and CPU-based platforms. Extensive experiments demonstrate that our method improves processing speed by 2.6 times in terms of FPS and reduces the total parameter count by 71 %, all without compromising model accuracy.},
  archive      = {J_NEUCOM},
  author       = {Mengyang Nie and Jinqiu Sun and Hongsong Guoyang and Axi Niu and Yaoqi Hu and Qingsen Yan and Yu Zhu and Yanning Zhang},
  doi          = {10.1016/j.neucom.2025.131823},
  journal      = {Neurocomputing},
  month        = {1},
  pages        = {131823},
  shortjournal = {Neurocomputing},
  title        = {FSCFNet: Lightweight neural networks via multi-dimensional importance-aware optimization},
  volume       = {660},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Deep learning for analyzing chaotic dynamics in biological time series: Insights from frog heart signals. <em>NEUCOM</em>, <em>660</em>, 131820. (<a href='https://doi.org/10.1016/j.neucom.2025.131820'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The study of experimental data is a relevant task in several physical, chemical and biological applications. In particular, the analysis of chaotic dynamics in cardiac systems is crucial as it can be related to certain pathological arrhythmias. When working with short and noisy experimental time series, some standard techniques for chaos detection cannot provide reliable results because of such data characteristics. Moreover, when small datasets are available, some Deep Learning techniques cannot be applied directly (that is, using part of the data to train the network, and using the trained network to analyze the remaining dataset). To overcome all these limitations, we propose an automatic algorithm that combines Deep Learning and some selection strategies based on a mathematical model of the same nature of the experimental data. To demonstrate its performance, we test it with experimental data obtained from ex-vivo frog heart experiments, achieving highly accurate results.},
  archive      = {J_NEUCOM},
  author       = {Carmen Mayora-Cebollero and Flavio H. Fenton and Molly Halprin and Conner Herndon and Mikael J. Toye and Roberto Barrio},
  doi          = {10.1016/j.neucom.2025.131820},
  journal      = {Neurocomputing},
  month        = {1},
  pages        = {131820},
  shortjournal = {Neurocomputing},
  title        = {Deep learning for analyzing chaotic dynamics in biological time series: Insights from frog heart signals},
  volume       = {660},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Enhancing reverse distillation model for multi-class unsupervised anomaly detection via latent space regularization. <em>NEUCOM</em>, <em>660</em>, 131819. (<a href='https://doi.org/10.1016/j.neucom.2025.131819'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the field of multi-class anomaly detection (AD), widely used reconstruction-based methods suffer from the over-generalization problem, where both normal and abnormal samples are well reconstructed, leading to the failure of AD relying on reconstruction error. In response, we propose a unified reverse distillation (RD) model for multi-class AD, named UniRD4AD. Specifically, we introduce a plug-and-play cluster loss for reconstruction-based methods, leveraging latent space regularization to obtain a compact normal representation. This enhances the separability between normal and anomalous samples, thereby mitigating over-generalization. Furthermore, we identify that existing RD models neglect the reconstruction of low-level features, which can result in false positives. To address this issue, we propose a decoder with long skip connections to enhance anomaly localization. Extensive experiments on three real-world AD datasets, including MVTec-AD, VisA, and Real-IAD, demonstrate that UniRD4AD outperforms state-of-the-art AD methods while maintaining competitive inference speed. The code is available at https://github.com/Liaaaar/UniRD4AD .},
  archive      = {J_NEUCOM},
  author       = {Kaiwen Fu and Fei Qi and Xiaotian Wang and Kun Liu},
  doi          = {10.1016/j.neucom.2025.131819},
  journal      = {Neurocomputing},
  month        = {1},
  pages        = {131819},
  shortjournal = {Neurocomputing},
  title        = {Enhancing reverse distillation model for multi-class unsupervised anomaly detection via latent space regularization},
  volume       = {660},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Multimodal large language models in medical research and clinical practice: Development, applications, challenges and future. <em>NEUCOM</em>, <em>660</em>, 131817. (<a href='https://doi.org/10.1016/j.neucom.2025.131817'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the highly intertwined field of healthcare, cross-modal medical data (including diagnostic imaging profiles, physiological time series signals, clinical voice archives, electronic medical records, surgical video streams, and genomics data) play a key role in clinical diagnostic and treatment decisions and basic medical research. Despite the breakthroughs in medical knowledge management and information processing made by generalized Large Language Models (LLMs), the existing systems are mainly limited to the processing capability of text modality, and this unimodal limitation seriously restricts the practical application value of intelligent medical systems. Multimodal Large Language Models (MLLMs) can realize multidimensional patient feature modeling through the deep semantic fusion of multimodal medical data, which can significantly improve the efficiency of disease identification and medical resource scheduling, and support the formulation of precise individual treatment strategies. In this paper, we systematically describe the development, applications, chanllenges and future of MLLMs in clinical practice and medical research, and provide a theoretical framework and practical guidelines for building a new generation of intelligent healthcare infrastructure. However, data silos, modality fusion strategies, computational resource requirements, and ethical compliance are still the main bottlenecks. Through this review, it is hoped that it can help more medical researchers to understand the progress of MLLMs technology, and help more medical researchers and MLLMs developers to work closely with each other through interdisciplinary cooperation, to create more products that are beneficial to the development of human health and put them into practical applications.},
  archive      = {J_NEUCOM},
  author       = {Peng Jun Xu and Shuang Xiang Kan and Jing Jin and Zhou Jing Zhang and Ya Xin Gu and Bo Zhang and You Lang Zhou},
  doi          = {10.1016/j.neucom.2025.131817},
  journal      = {Neurocomputing},
  month        = {1},
  pages        = {131817},
  shortjournal = {Neurocomputing},
  title        = {Multimodal large language models in medical research and clinical practice: Development, applications, challenges and future},
  volume       = {660},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). CMaP-SAM: Contraction mapping prior for SAM-driven few-shot segmentation. <em>NEUCOM</em>, <em>660</em>, 131816. (<a href='https://doi.org/10.1016/j.neucom.2025.131816'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Few-shot segmentation (FSS) aims to segment new classes using few annotated images. While recent FSS methods have shown considerable improvements by leveraging Segment Anything Model (SAM), they face two critical limitations: insufficient utilization of structural correlations in query images, and significant information loss when converting continuous position priors to discrete point prompts. To address these challenges, we propose CMaP-SAM, a novel framework that introduces contraction mapping theory to optimize position priors for SAM-driven few-shot segmentation. CMaP-SAM consists of three key components: (1) a contraction mapping module that formulates position prior optimization as a Banach contraction mapping with convergence guarantees. This module iteratively refines position priors through pixel-wise structural similarity, generating a converged prior that preserves both semantic guidance from reference images and structural correlations in query images; (2) an adaptive distribution alignment module bridging continuous priors with SAM’s binary mask prompt encoder; and (3) a foreground-background decoupled refinement architecture producing accurate final segmentation masks. Extensive experiments demonstrate CMaP-SAM’s effectiveness, achieving state-of-the-art performance with 71.1 mIoU on PASCAL- 5 i and 56.1 on COCO- 20 i datasets. Code is available at https://github.com/Chenfan0206/CMaP-SAM .},
  archive      = {J_NEUCOM},
  author       = {Shuai Chen and Fanman Meng and Liming Lei and Haoran Wei and Chenhao Wu and Qingbo Wu and Linfeng Xu and Hongliang Li},
  doi          = {10.1016/j.neucom.2025.131816},
  journal      = {Neurocomputing},
  month        = {1},
  pages        = {131816},
  shortjournal = {Neurocomputing},
  title        = {CMaP-SAM: Contraction mapping prior for SAM-driven few-shot segmentation},
  volume       = {660},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Middle modality interactive feature attention learning for visible-infrared person re-identification. <em>NEUCOM</em>, <em>660</em>, 131815. (<a href='https://doi.org/10.1016/j.neucom.2025.131815'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The scarcity of paired training samples and significant cross-modality disparities poses substantial challenges for visible-infrared person re-identification (VI-ReID). Existing methods primarily focus on generating middle-modality images to alleviate the data scarcity and bridge the modality gap. However, despite progress, these approaches face limitations: the auxiliary role of middle modalities remains constrained, network structures tend to be simplistic, and the design of effective loss functions is challenging. These limitations hinder models from fully leveraging the information within the middle modality. To address these issues, this paper introduces an Interactive Feature Attention Learning Network (IFALNet). The framework first employs a Middle Modality Generator (MMG) to synthesize middle-modality images, which are then processed by the feature extraction network. Within this network, a Modal Interactive Attention (MIA) module captures discriminative information regarding modality-shared features, while a Multi-scale Feature Aggregation (MFA) block focuses on modality-specific discriminative information, collectively enhancing feature representation. Furthermore, a Triple Centers Aggregation (TCA) loss guides the feature learning across visible, infrared, and middle modalities. This loss function effectively reduces the distance between modality centers, thereby eliminating modality gaps. Extensive experiments conducted on the SYSU-MM01, RegDB, and LLCM datasets demonstrate the superiority of the proposed method. Notably, compared to the recent MMN approach of the same type, IFALNet achieves significant improvements of 6.6 % in Rank-1 accuracy and 6.2 % in mAP on the challenging SYSU-MM01 dataset. Code is available at https://github.com/ZHY-tech11/IFALNet .},
  archive      = {J_NEUCOM},
  author       = {Haoyi Zhao and Shanmin Yang and Xiaojie Li and Jing Peng and Xi Wu},
  doi          = {10.1016/j.neucom.2025.131815},
  journal      = {Neurocomputing},
  month        = {1},
  pages        = {131815},
  shortjournal = {Neurocomputing},
  title        = {Middle modality interactive feature attention learning for visible-infrared person re-identification},
  volume       = {660},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Unveiling distinguishing properties of adversarial examples through neural tangent kernels. <em>NEUCOM</em>, <em>660</em>, 131813. (<a href='https://doi.org/10.1016/j.neucom.2025.131813'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep neural networks (DNNs), in addition to their great power, exhibit significant weaknesses, with one of the most notable being their susceptibility to adversarial attacks by carefully crafted, minimally perturbed, adversarial examples. Despite numerous efforts toward creating reliable detectors and defense mechanisms, the underlying factors that make such attacks possible have not yet been fully understood. In this paper, we further explore the use of local intrinsic dimensionality (LID), and, for the first time, hubness, as indicators for detection of adversarial examples. Furthermore, we involve neural tangent kernels (NTKs) in this task, showing that NTK-induced distance measures offer more information on how to detect adversarial examples, through both LID and hubness, than distances computed on raw input data or one network layer. Furthermore, we combine LID and hubness as features into a well-performing classifier that exhibits better detection accuracy than either of the two features used on their own. Finally, we combine the LID and hubness features with state-of-the-art adversarial attack detectors, demonstrating their general usefulness for the task. Our findings offer insight into the properties and nature of adversarial examples of various types, paving the way for the construction of better detectors and, possibly, the construction of models that are more robust to such attacks.},
  archive      = {J_NEUCOM},
  author       = {Alexandros Nanopoulos and Miloš Radovanović},
  doi          = {10.1016/j.neucom.2025.131813},
  journal      = {Neurocomputing},
  month        = {1},
  pages        = {131813},
  shortjournal = {Neurocomputing},
  title        = {Unveiling distinguishing properties of adversarial examples through neural tangent kernels},
  volume       = {660},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Exploiting structure-semantic consistency for photorealistic SLAM with 3D gaussian splatting. <em>NEUCOM</em>, <em>660</em>, 131809. (<a href='https://doi.org/10.1016/j.neucom.2025.131809'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Semantic SLAM systems aim to jointly estimate camera poses, reconstruct 3D geometry, and build meaningful semantic representations of the environment. To meet these goals with high fidelity and real-time performance, 3D Gaussian Splatting (3DGS) has recently emerged as a promising scene representation, offering dense, photorealistic reconstructions and efficient rendering that make it well-suited for semantic SLAM. However, extending 3DGS to semantic SLAM requires addressing two key problems: how to initialize Gaussians in a structure-aware manner to improve reconstruction efficiency, and how to robustly integrate and align semantic information with geometry and appearance for unified multi-modal optimization. To address these issues, we propose a superpixel-guided dense semantic SLAM framework based on 3D Gaussian Splatting. Given an input RGB- d -Semantic stream, we design a semantic-consistent superpixel extraction module. It generates superpixels with geometric convexity and semantic consistency, providing reliable structural units for Gaussian initialization. A superpixel-guided strategy is employed to generate structure-aware Gaussian primitives, while semantic information is simultaneously embedded to form a unified multi-modal representation of geometry, appearance, and semantics. To improve scalability in large-scale environments, our system decomposes the scene into independently optimized submaps, within which frame-to-model tracking and local refinement are performed. By introducing a soft-mask alignment constraint, we enable progressive fusion and promote spatial consistency during cross-modal reconstruction. Extensive experiments on public datasets demonstrate that our method achieves robust camera tracking, photorealistic scene reconstruction, and accurate semantic mapping.},
  archive      = {J_NEUCOM},
  author       = {Meiyi Yang and Qianang Zhou and Hai Liu and You-fu Li and Junlin Xiong},
  doi          = {10.1016/j.neucom.2025.131809},
  journal      = {Neurocomputing},
  month        = {1},
  pages        = {131809},
  shortjournal = {Neurocomputing},
  title        = {Exploiting structure-semantic consistency for photorealistic SLAM with 3D gaussian splatting},
  volume       = {660},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Cluster synchronization of fractional-order reaction–diffusion neural networks under modular directed topologies. <em>NEUCOM</em>, <em>660</em>, 131803. (<a href='https://doi.org/10.1016/j.neucom.2025.131803'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper investigates the cluster synchronization behavior in a class of fractional-order reaction–diffusion neural networks (FORDNNs) under modular directed topologies. The considered networks allow general signed and asymmetric inter-cluster couplings, which are commonly observed in realistic neural systems but are rarely addressed in existing studies. To handle this generality, we introduce the cluster-input equivalence (CIE) condition, which enables synchronization analysis without requiring symmetry, balanced interactions, or zero-row-sum assumptions. By constructing a tailored Lyapunov function and employing fractional-order inequality techniques, we derive sufficient conditions that guarantee intra-cluster synchronization. Numerical simulations are conducted on modular FORDNNs networks to validate the theoretical results and demonstrate the effectiveness of the proposed framework.},
  archive      = {J_NEUCOM},
  author       = {Rixu Hao and Yongqing Yang and Huixian Weng and Boling Zhou},
  doi          = {10.1016/j.neucom.2025.131803},
  journal      = {Neurocomputing},
  month        = {1},
  pages        = {131803},
  shortjournal = {Neurocomputing},
  title        = {Cluster synchronization of fractional-order reaction–diffusion neural networks under modular directed topologies},
  volume       = {660},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). A comprehensive survey of adversarial defense techniques in the visual domain. <em>NEUCOM</em>, <em>660</em>, 131799. (<a href='https://doi.org/10.1016/j.neucom.2025.131799'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, deep neural networks (DNNs) have achieved widespread success in computer vision tasks, including face recognition, autonomous driving, and medical diagnosis. However, their vulnerability to adversarial attacks has also raised serious concerns regarding system security and reliability. This paper provides a comprehensive overview of recent advances in adversarial defense methods in computer vision. It explores defense strategies for Large Vision-Language Models (LVLMs) while covering traditional adversarial defense methods. The article first explains the basic concepts of adversarial samples, their generation principles, and their actual threats in white-box, black-box and physical worlds. It then systematically examines the multi-dimensional defense strategies based on model architecture design, dynamic adversarial training, and input-output space purification. Meanwhile, for the security challenges of LVLM in the era of significant models, two types of strategies, input-output space defense and dynamic training defense, are discussed. Finally, this paper summarizes the key issues facing adversarial defense and provides an outlook on future research directions.},
  archive      = {J_NEUCOM},
  author       = {Yibin Dong and Jun Lei and Sheng Long and Shuohao Li and Jun Zhang},
  doi          = {10.1016/j.neucom.2025.131799},
  journal      = {Neurocomputing},
  month        = {1},
  pages        = {131799},
  shortjournal = {Neurocomputing},
  title        = {A comprehensive survey of adversarial defense techniques in the visual domain},
  volume       = {660},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Spiking neural network analysis of MT-MST pathways in biological motion processing. <em>NEUCOM</em>, <em>660</em>, 131795. (<a href='https://doi.org/10.1016/j.neucom.2025.131795'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Understanding the neural mechanisms underlying biological motion perception remains a significant challenge in neuroscience. To further explore this mechanism, we construct the BioMotionNet model using real bio-neural data from the MT to MST regions in macaques. To characterize neuron activity within particular time windows, we propose the window learning strategy, which employs windowed learning to extract crucial information related to specific events or stimuli. By analyzing the connectivity structure of the BioMotionNet model, we identify regular projection patterns from MT to MST, reflected in the varying response characteristics of MT neurons based on their projection strength to different MST neuron populations. Our data/codes are available at https://github.com/BrainCogLab/MT_MST .},
  archive      = {J_NEUCOM},
  author       = {Yun Zhang and Ying Liu and Tingting Feng and Tao Zhang and Hong Qu and Yi Zhang},
  doi          = {10.1016/j.neucom.2025.131795},
  journal      = {Neurocomputing},
  month        = {1},
  pages        = {131795},
  shortjournal = {Neurocomputing},
  title        = {Spiking neural network analysis of MT-MST pathways in biological motion processing},
  volume       = {660},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Integrating spatio-temporal modeling of RGB video with multi-stream skeleton representations for advanced human action recognition. <em>NEUCOM</em>, <em>660</em>, 131791. (<a href='https://doi.org/10.1016/j.neucom.2025.131791'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Unimodal human action recognition techniques have undergone significant development and are widely used. Currently, RGB video and skeleton data are the most common and effective modalities, but they still have some obvious shortcomings. For example, RGB video is easily affected by redundant background information, and the skeleton lacks interaction with the scene. Multimodal learning can achieve compensation of information from both modalities. However, most existing multimodal methods are limited in building spatio-temporal features in the video and do not explore the higher-order features of the skeleton. Therefore, this paper proposes a novel method that integrates RGB spatio-temporal features and multi-stream skeleton representation. Specifically, for the RGB modality, we use the skeleton coordinates to crop the redundant background. Meanwhile, we design a fine-grained spatio-temporal modeling module to enhance the spatio-temporal features. For the skeleton modality, we introduce three representations of joint, bone, and motion to delve deeper into the skeleton features. Finally, we design a score-weighting strategy to fuse the inference results of the two modalities. The experimental results on the benchmarks NTU RGB+D, PKU-MMD, and a real-world dataset, AAD, demonstrate the effectiveness and advancement of the proposed method.},
  archive      = {J_NEUCOM},
  author       = {Yukai Zhao and Jingwei Wang and Tianyi Yin and Jiexuan Cai and Min Liu and Yunlong Ma},
  doi          = {10.1016/j.neucom.2025.131791},
  journal      = {Neurocomputing},
  month        = {1},
  pages        = {131791},
  shortjournal = {Neurocomputing},
  title        = {Integrating spatio-temporal modeling of RGB video with multi-stream skeleton representations for advanced human action recognition},
  volume       = {660},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). HGSim: High-fidelity and generalizable simulation frame-work for autonomous driving scenes. <em>NEUCOM</em>, <em>660</em>, 131784. (<a href='https://doi.org/10.1016/j.neucom.2025.131784'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Simulating autonomous driving scenarios presents significant challenges, particularly in terms of fidelity. Although game engine-based simulators demonstrate potential, they still exhibit substantial discrepancies from the real world in both visual realism and physical consistency. To address this issue, this research proposes HGSim, a high-fidelity simulation framework for autonomous driving scenario generation and generalization. First, a four-dimensional Gaussian decoupling network is developed and supervised with semantic segmentation masks to accurately separate dynamic vehicles from static backgrounds, thereby generating artifact-free and high-fidelity static scenes. Second, an extended mask-based static vehicle decoupling method is introduced to effectively isolate static vehicles from their surrounding environments. Diffusion models are then employed to reconstruct background details, enabling realistic novel view synthesis. In addition, HGSim models vehicles with fine-grained geometric structures and assigns independent feature control parameters to each vehicle, allowing flexible simulation across diverse driving scenarios. Experimental results demonstrate that HGSim substantially outperforms existing baseline methods in terms of scene realism, representing a significant advancement in the generalization capability of autonomous driving simulations.},
  archive      = {J_NEUCOM},
  author       = {Yue Tian and Wenbo Chu and Wei Zhou and Xiaolin Tang and Keqiang Li},
  doi          = {10.1016/j.neucom.2025.131784},
  journal      = {Neurocomputing},
  month        = {1},
  pages        = {131784},
  shortjournal = {Neurocomputing},
  title        = {HGSim: High-fidelity and generalizable simulation frame-work for autonomous driving scenes},
  volume       = {660},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Text enhanced curriculum supervised contrastive learning for food image recognition. <em>NEUCOM</em>, <em>660</em>, 131781. (<a href='https://doi.org/10.1016/j.neucom.2025.131781'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Food image recognition faces significant challenges due to high intra-class variations, subtle inter-class distinctions, and biased hierarchical taxonomies. Traditional methods fail to effectively capture fine-grained culinary semantics, while prevailing contrastive learning frameworks exhibit limited adaptability to the dynamic evolution of feature representations during progressive training. To address these limitations, we propose Text Enhanced Curriculum Supervised Contrastive Learning (TEC-SCL), a novel multimodal learning framework that synergizes Vision-Language Model (VLM)-generated semantic descriptions with visual features through cross-modal attention fusion. In addition, we introduce a curriculum-based scheduler to dynamically optimize contrastive pairs by prioritizing hard negatives. Extensive experiments conducted on ETH Food-101, ISIA Food-500, and UEC-Food 256 datasets demonstrate that our method achieves state-of-the-art performance, obtaining the highest Top-1 accuracy for fine-grained retrieval. The framework bridges the gap between generic vision models and domain-specific food image recognition, offering significant potential for intelligent food systems. Our code and data are released at: https://github.com/FourierAI/TEC_SCL},
  archive      = {J_NEUCOM},
  author       = {Feng Jiang and Zhipeng Ye and Lili Zhou and Jiaqi Huang},
  doi          = {10.1016/j.neucom.2025.131781},
  journal      = {Neurocomputing},
  month        = {1},
  pages        = {131781},
  shortjournal = {Neurocomputing},
  title        = {Text enhanced curriculum supervised contrastive learning for food image recognition},
  volume       = {660},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). KE-STCN: An adaptive multi-scale traffic flow prediction method based on knowledge graph. <em>NEUCOM</em>, <em>660</em>, 131771. (<a href='https://doi.org/10.1016/j.neucom.2025.131771'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Traffic flow prediction constitutes a fundamental task in intelligent traffic management systems. However, existing approaches primarily extract explicit spatio-temporal features while overlooking latent traffic flow patterns and external influencing factors, thereby constraining the performance potential of prediction models. To address this limitation, we propose a Knowledge-Enhanced Spatio-Temporal Convolutional Network (KE-STCN) that captures hidden patterns and leverages external knowledge to enhance traffic flow prediction. The proposed framework consists of two key components: a knowledge enhancement module and a spatiotemporal convolutional network. Specifically, the knowledge enhancement module constructs a comprehensive city-level traffic knowledge graph to incorporate heterogeneous external data. It employs a dynamic fusion mechanism to update traffic states in real time. The spatio-temporal network decouples temporal and spatial modeling, extracting long- and short-term temporal dependencies from global and local perspectives, respectively, while employing both dynamic and static adjacency matrices to reveal latent spatial semantic correlations. Extensive experiments were conducted on real-world datasets from Shenzhen, Manhattan, and Los Angeles. Experimental results demonstrate that KE-STCN achieves prediction accuracies of 88.9 % and 74.3 % on the Manhattan and Shenzhen datasets, respectively, when external knowledge is incorporated, substantially outperforming state-of-the-art baselines. On the Los Angeles dataset, the model achieves a 60-minute prediction Mean Absolute Error (MAE) of 3.43, highlighting its strong generalization capability. Ablation studies further validate the essential role of external knowledge and the effectiveness of individual components, offering a robust benchmark for advancing traffic flow prediction accuracy.},
  archive      = {J_NEUCOM},
  author       = {Jianrong Cao and Xing Sheng and Bingxin Yang and Zongtao Duan},
  doi          = {10.1016/j.neucom.2025.131771},
  journal      = {Neurocomputing},
  month        = {1},
  pages        = {131771},
  shortjournal = {Neurocomputing},
  title        = {KE-STCN: An adaptive multi-scale traffic flow prediction method based on knowledge graph},
  volume       = {660},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Map wisely for efficient transfer learning across heterogeneous data sources. <em>NEUCOM</em>, <em>660</em>, 131762. (<a href='https://doi.org/10.1016/j.neucom.2025.131762'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Transfer learning is an effective mechanism to learn on a small dataset if parameters can be transferred after being learned on a larger dataset, so that one can achieve decent performance despite the data scarcity in the small dataset. This mechanism has been used in developing and using foundation models, which have outperformed other classical methods. However, this paradigm of learning does not apply to small tabular data scenarios mostly dealing with classical learning methods though there are plenty of scenarios in the real world demanding such techniques. We observe an inherent challenge in adopting transfer learning for tabular datasets. Such datasets are recorded by different agencies in various locations and over a time period. There is hardly any universal norm in ordering the variables. However, parameters are very sensitive to the ordering of the variables. Forcefully transferring the parameters in such cases despite a mismatch in variables is not a very sensible approach. Our objective is to address this challenge and make it possible to transfer learn in a situation where the number of variables and the order of variables change across datasets (referred to as heterogeneous). Our approach is based on a simple statistics-based methodology. We compute proximity among variables across datasets using samples corresponding to the variables, and then align the variables based on proximity. We explore two categories of methods depending on whether the number of samples is same or different. In each category, we explore a plethora of techniques and conduct extensive studies. We validate our approach with several statistical methods, and on various types of datasets including real-world datasets. Our approach is not only more scientific than forcibly transferring the parameters but also provides empirical benefits to transfer learning with greater efficiency.},
  archive      = {J_NEUCOM},
  author       = {Snigdhatanu Acharya and Mrinal Das},
  doi          = {10.1016/j.neucom.2025.131762},
  journal      = {Neurocomputing},
  month        = {1},
  pages        = {131762},
  shortjournal = {Neurocomputing},
  title        = {Map wisely for efficient transfer learning across heterogeneous data sources},
  volume       = {660},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). No-reference image quality assessment via bidirectional feature fusion and regional distortion extraction. <em>NEUCOM</em>, <em>660</em>, 131761. (<a href='https://doi.org/10.1016/j.neucom.2025.131761'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Image quality assessment is a core technology in image processing, primarily used to evaluate the quality and distortion level of images by analyzing their characteristics. It serves as an important metric for other visual tasks. While significant progress has been made in evaluating synthetically distorted images, assessing authentically distorted images remains a challenge. Unlike synthetic distortions, which typically have uniform distribution, authentic distortions are more complex, with unevenly distributed distortion regions and varying distortion types and levels. Moreover, authentic distortions are influenced by both high-level semantic features and low-level visual features. To address these challenges, we propose a no-reference image quality assessment algorithm based on bidirectional feature fusion and regional distortion extraction. Human-perceived visual quality is influenced by both low-level visual features and high-level semantic features, a relationship that is particularly evident in authentically distorted images. To capture this, the proposed method employs a bidirectional feature fusion structure, which integrates both high-level and low-level visual information through top-down and bottom-up pathways, respectively, thereby combining multi-layered features. Synthetic distortions are usually globally distributed, while authentic distortions typically manifest regionally, which complicates their evaluation. To address this, the proposed method uses window attention to extract local distortion features and overlapping cross-window attention to strengthen the interconnections between local distortions, yielding regionally distributed distortion features. Finally, the paper trains a model using contrastive learning to extract distortion features for various distortion types and levels. A mixture-of-experts cross-attention module is introduced to fuse distortion and image features. Experimental results on two synthetic distortion datasets and three authentic distortion datasets demonstrate that the proposed method achieves competitive performance.},
  archive      = {J_NEUCOM},
  author       = {Xiong Nie and Lihua Tian and Chen Li},
  doi          = {10.1016/j.neucom.2025.131761},
  journal      = {Neurocomputing},
  month        = {1},
  pages        = {131761},
  shortjournal = {Neurocomputing},
  title        = {No-reference image quality assessment via bidirectional feature fusion and regional distortion extraction},
  volume       = {660},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Task-aware dynamic routing network for cross-domain few-shot learning. <em>NEUCOM</em>, <em>660</em>, 131752. (<a href='https://doi.org/10.1016/j.neucom.2025.131752'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cross-domain few-shot learning (CD-FSL) is a highly challenging task. It aims to learn classifiers for unseen classes and domains using limited labeled samples. Existing methods adopt learnable task-specific parameters, but the fixed-parameter paradigm faces two intertwined issues. First, significant distribution gaps between source and target domains make fixed parameters unable to flexibly adapt to target tasks: excessive parameters cause overfitting to sparse target samples, while insufficient ones fail to capture patterns required for domain transfer. Second, sparse labels hinder feature learning, leaving models struggling to extract discriminative features that are robust across domains. This problem is more severe when handling complex visual elements.Inspired by the strong correlation between task-specific parameter configurations and target task characteristics, we propose the Task-Aware dynamiC Routing Network (TACO) for CD-FSL. During training, TACO uses reinforcement learning to dynamically adjust its architecture based on discrepancies between source and target domains. This optimizes task-specific configurations to mitigate overfitting and under-adaptation. Additionally, it integrates self-attention and knowledge distillation to enhance feature extraction, enabling better representation of complex visual elements. Experiments on 8 CD-FSL datasets demonstrate that TACO outperforms existing methods in average accuracy and generalization ability. Compared with state-of-the-art results on individual datasets, our approach achieves a performance improvement of up to 3.03 %.},
  archive      = {J_NEUCOM},
  author       = {Yanan Li and Haoyang Ye and Huabing Zhou and Tao Lu and Hao Lu},
  doi          = {10.1016/j.neucom.2025.131752},
  journal      = {Neurocomputing},
  month        = {1},
  pages        = {131752},
  shortjournal = {Neurocomputing},
  title        = {Task-aware dynamic routing network for cross-domain few-shot learning},
  volume       = {660},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). AutoODL: Automated on-device learning via dynamic unfreezing and semi-supervised labeling for edge AI. <em>NEUCOM</em>, <em>660</em>, 131720. (<a href='https://doi.org/10.1016/j.neucom.2025.131720'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose AutoODL, a fully automated, semi-supervised on-device learning framework for embedded neural network fine-tuning. AutoTag generates robust pseudo-labels via class-specific dynamic thresholds with consistency regularization; Dynamic Unfreezing progressively activates layers based on loss stagnation; Batch Data Streaming uses gradient accumulation for stable large-batch-like updates under memory constraints. On CIFAR-10/100 with ResNet, SENet, and ViT, AutoODL improves accuracy over TinyOL/TinyTL/SensOL by up to 15.96 %/17.45 %/12.27 % while adding only 10–18 MB peak memory. Deployed on Raspberry Pi 4, we report step latency and average power using a measurement protocol. By seamlessly integrating these mechanisms, AutoODL achieves label-free, real-time fine-tuning on edge devices with minimal memory overhead.},
  archive      = {J_NEUCOM},
  author       = {Jun Wang and Yanwen Luo and Yanan Liu and Yifan Wang and Xinyu Yang and Huixin Zhong and Gaoyu Dai},
  doi          = {10.1016/j.neucom.2025.131720},
  journal      = {Neurocomputing},
  month        = {1},
  pages        = {131720},
  shortjournal = {Neurocomputing},
  title        = {AutoODL: Automated on-device learning via dynamic unfreezing and semi-supervised labeling for edge AI},
  volume       = {660},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Fine-grained food image classification with multi-modal collaboration. <em>NEUCOM</em>, <em>660</em>, 131714. (<a href='https://doi.org/10.1016/j.neucom.2025.131714'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Fine-grained image classification focuses on distinguishing subclasses. In food image classification, food’s shape, color, and texture are easily influenced by cooking methods, presentation, and lighting. This makes differences among the same kind of food more subtle and harder to capture. To address this, we propose a multimodal framework fusing image and text modalities. The text modality, with distinct statistical properties, data attributes, and relational structures compared to images, enables precise characterization of fine-grained food features, providing complementary semantic information for classification. Specifically, cross-modal knowledge distillation is employed to transfer textual semantics into the image feature space, while contrastive learning is leveraged to discriminate between homogeneous and heterogeneous features across modalities and facilitate feature complementarity. Experiments on benchmark fine-grained food datasets VegFru, FoodX-251, and CNFood show that the proposed method achieves classification accuracies of 97.22 %, 83.25 %, and 84.96 %, respectively, outperforming state-of-the-art (SOTA) methods and validating the effectiveness of the multimodal cross-modal fusion strategy in fine-grained food classification tasks.},
  archive      = {J_NEUCOM},
  author       = {Li Cheng and Linyi Lan and Zhongjie Xiao and Jianzhang Chen and Jiaxiong Lu and Huanrong Wang and Yi-Ping Phoebe Chen},
  doi          = {10.1016/j.neucom.2025.131714},
  journal      = {Neurocomputing},
  month        = {1},
  pages        = {131714},
  shortjournal = {Neurocomputing},
  title        = {Fine-grained food image classification with multi-modal collaboration},
  volume       = {660},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). FNCD-CS: A neurocomputing-optimized hybrid neural network for cross-domain code search. <em>NEUCOM</em>, <em>659</em>, 131847. (<a href='https://doi.org/10.1016/j.neucom.2025.131847'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Code search is pivotal for enhancing software development efficiency, yet existing deep learning models face critical limitations in cross-domain scenarios from a neural network perspective: single-modality feature extraction lacks dedicated neural modules to model code-specific syntactic dependencies, and information loss arises from gradient degradation during deep neural training—two inherent challenges of neural networks when processing structured code data. To address these issues, we propose FNCD-CS, a neurocomputing-optimized neural network architecture with a layered, synergistic design: (1) fastText initializes lexical embeddings by simulating morphological recognition in biological neural systems. It uses character n-gram splitting to resolve the out-of-vocabulary limitation of fixed-vocabulary embedding models; (2) a 2-head self-attention module constructs global syntactic dependency graphs, overcoming the local feature bias of traditional neural networks by simultaneously capturing token-level and structure-level relationships; (3) residual connections establish "short-circuit paths" for feature transmission, alleviating gradient vanishing-induced information loss and preserving 89.7 % of syntactic features in deep layers; (4) an LSTM-based domain adapter imitates synaptic plasticity via gated weight adjustment, aligning cross-domain syntactic feature spaces without retraining. Experimental results on Java (source domain) and three target domain datasets demonstrate that FNCD-CS outperforms four baseline models. It achieves 7.61 %-15.26 % higher H@1-H@3 accuracy over all baselines and a 21.90 % MRR improvement compared to the state-of-the-art cross-domain model AdaCS. This performance gain stems from the synergistic optimization of its neural modules: self-attention enhances syntactic feature discrimination, residual connections preserve feature integrity, and the LSTM adapter boosts cross-domain transferability. Notably, FNCD-CS maintains real-time inference efficiency at 50 queries per second—17.5 % faster than AdaCS—owing to optimized neural computation complexity with 234k parameters, 34.3 % fewer than AdaCS. Source code can be extracted from: https://share.weiyun.com/ngCifCi .},
  archive      = {J_NEUCOM},
  author       = {Mengge Fang and Li-e Wang and Haize Hu},
  doi          = {10.1016/j.neucom.2025.131847},
  journal      = {Neurocomputing},
  month        = {1},
  pages        = {131847},
  shortjournal = {Neurocomputing},
  title        = {FNCD-CS: A neurocomputing-optimized hybrid neural network for cross-domain code search},
  volume       = {659},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Multi-organ medical image segmentation via adaptive disentangled domain generalization collaborative learning. <em>NEUCOM</em>, <em>659</em>, 131846. (<a href='https://doi.org/10.1016/j.neucom.2025.131846'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Semi-supervised 3D medical image segmentation holds great promise for clinical practice but remains challenged by domain shifts caused by imaging protocol variations and by the difficulty of accurately segmenting small organs with sparse annotations. To address these issues, we propose a novel Adaptive Disentangled Domain Generalization Collaborative Learning (AD-DGCL) framework. It integrates two core components: a Semi-Supervised Representation Disentanglement (SSRD) module that separates domain-specific style and anatomical content via dual encoders and cross-domain contrastive learning, and a Style-induced Consistency Training (SCT) module that enhances robustness through synthetic style perturbations and consistency regularization. In addition, an adaptive region-specific loss function is designed to prioritize small organs by dynamically adjusting weights according to pixel frequency. Extensive experiments demonstrate that AD-DGCL substantially improves segmentation accuracy, particularly for small organs, while effectively mitigating domain gaps, thus showing strong potential for robust and generalizable clinical deployment.},
  archive      = {J_NEUCOM},
  author       = {Min Dong and Yishuang Liu and Ating Yang and Ye Zhang and Rongchang Zhao and Ling Liu},
  doi          = {10.1016/j.neucom.2025.131846},
  journal      = {Neurocomputing},
  month        = {1},
  pages        = {131846},
  shortjournal = {Neurocomputing},
  title        = {Multi-organ medical image segmentation via adaptive disentangled domain generalization collaborative learning},
  volume       = {659},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). WP-CrackNet: A collaborative adversarial learning framework for end-to-end weakly-supervised road crack detection. <em>NEUCOM</em>, <em>659</em>, 131845. (<a href='https://doi.org/10.1016/j.neucom.2025.131845'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Road crack detection is essential for intelligent infrastructure maintenance in smart cities. To reduce reliance on costly pixel-level annotations, we propose WP-CrackNet, an end-to-end weakly-supervised method that trains with only image-level labels for pixel-wise crack detection. WP-CrackNet integrates three components: a classifier generating class activation maps (CAMs), a reconstructor measuring feature inferability, and a detector producing pixel-wise road crack detection results. During training, the classifier and reconstructor alternate in adversarial learning to encourage crack CAMs to cover complete crack regions, while the detector learns from pseudo labels derived from post-processed crack CAMs. This mutual feedback among the three components improves learning stability and detection accuracy. To further boost detection performance, we design a path-aware attention module (PAAM) that fuses high-level semantics from the classifier with low-level structural cues from the reconstructor by modeling spatial and channel-wise dependencies. Additionally, a center-enhanced CAM consistency module (CECCM) is proposed to refine crack CAMs using center Gaussian weighting and consistency constraints, enabling better pseudo-label generation. We create three image-level datasets and extensive experiments show that WP-CrackNet achieves comparable results to supervised methods and outperforms existing weakly-supervised methods, significantly advancing scalable road inspection. The source code package and datasets are available at https://mias.group/WP-CrackNet/ .},
  archive      = {J_NEUCOM},
  author       = {Nachuan Ma and Zhengfei Song and Qiang Hu and Xiaoyu Tang and Chengxi Zhang and Rui Fan and Lihua Xie},
  doi          = {10.1016/j.neucom.2025.131845},
  journal      = {Neurocomputing},
  month        = {1},
  pages        = {131845},
  shortjournal = {Neurocomputing},
  title        = {WP-CrackNet: A collaborative adversarial learning framework for end-to-end weakly-supervised road crack detection},
  volume       = {659},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Focusing on what humans see: Robustness enhancement through adversarial supervised contrastive learning. <em>NEUCOM</em>, <em>659</em>, 131842. (<a href='https://doi.org/10.1016/j.neucom.2025.131842'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep Neural Networks (DNNs) have various vulnerabilities such as unpredictable behavior on adversarial examples. Adversarial training (AT) encourages models to learn robust, human-perceptible features rather than non-robust features present in the data distribution, which, although imperceptible to humans, are utilized for classification and lead to adversarial examples. However, while traditional AT methods achieve robustness against adversarial attacks, they suffer from various performance degradations. We hypothesize that this phenomenon is related to cross-entropy loss, and can be mitigated by using contrastive loss, which learns common features across samples through batch-wise comparisons. In response, we propose Robust Supervised Contrastive Learning (RSupCon), which extends supervised contrastive learning to the adversarial domain with two strategies: combining various augmentations and separating the anchor and contrast sets. The combined augmentations encourage the model to focus on learning robust features, and separating the contrast set reduces the learning of non-robust features. With these two strategies, RSupCon effectively helps the model discriminate robust features within images transformed in various ways and adversarial examples. Experiments on benchmark datasets demonstrate that RSupCon offers adversarial robustness comparable to traditional AT methods while mitigating performance degradation. Visual evidence further confirms the ability of our method to learn representations centered on robust features. Furthermore, several experimental results and analyses offer novel insights into non-robust features and adversarial training.},
  archive      = {J_NEUCOM},
  author       = {Keon Kim and Yongsuk Choi},
  doi          = {10.1016/j.neucom.2025.131842},
  journal      = {Neurocomputing},
  month        = {1},
  pages        = {131842},
  shortjournal = {Neurocomputing},
  title        = {Focusing on what humans see: Robustness enhancement through adversarial supervised contrastive learning},
  volume       = {659},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Collaborative filtering enhanced subgraph embedding for link direction and sign prediction. <em>NEUCOM</em>, <em>659</em>, 131840. (<a href='https://doi.org/10.1016/j.neucom.2025.131840'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The enclosing subgraph centered around a link is undoubtedly a strong indicator of the label of the link in various graph types. However, effectively representing and interpreting such link-centric subgraphs for predicting link direction and sign remains a significant challenge. In this paper, we propose a framework called C ollaborative F iltering Enhanced S ubgraph E mbedding (CFSE) for link direction and link sign prediction in (signed) directed networks. Specifically, inspired by collaborative filtering, CFSE first identifies a fundamental path within the subgraph as a key indicator of link direction and sign. Then, using a matrix representation of this path, CFSE generates subgraph embeddings to train a neural network classifier for link prediction tasks. Compared to graph learning methods based on pairwise transitivity and triad motifs, it is demonstrated that CFSE can achieve superior prediction performance across both signed and unsigned directed networks. The advantage of our approach lies in the fact that it only takes usage of one-hop neighborhood information, yet can achieve better link prediction performance than those node-centric subgraph or graph embedding approaches. The code is available at https://github.com/fang98/CFSE .},
  archive      = {J_NEUCOM},
  author       = {Zhihong Fang and Shaolin Tan and Zhe Li and Qiu Fang and Yao Chen},
  doi          = {10.1016/j.neucom.2025.131840},
  journal      = {Neurocomputing},
  month        = {1},
  pages        = {131840},
  shortjournal = {Neurocomputing},
  title        = {Collaborative filtering enhanced subgraph embedding for link direction and sign prediction},
  volume       = {659},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). A deep neural network model with physics-guided term for automatic identification of atmospheric fronts. <em>NEUCOM</em>, <em>659</em>, 131835. (<a href='https://doi.org/10.1016/j.neucom.2025.131835'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Intense weather phenomena often occur near atmospheric fronts, making accurate front identification crucial for weather analysis and forecasting. However, current deep learning methods for atmospheric front identification directly input multiple meteorological elements into networks for training and recognition. This black-box nature makes it difficult to trust that such approaches can yield highly interpretable results consistent with physical laws. These methods heavily rely on data quality and training processes, exhibiting poor generalization capability. Additionally, conflicts in front characteristics among multiple meteorological elements hinder network training and identification. To address these issues, we integrate empirical, theoretical, and computational science paradigms to propose a physics-guided intelligent atmospheric front identification model (PG-AMUnet). This model employs physics-guided terms to extract frontal physical information as prior knowledge, accelerating network training and improving identification accuracy. In subsequent network stages, an encoder-decoder architecture with attention mechanisms extracts frontal features from physical information and determines front categories at grid points. The encoder incorporates convolutional block attention modules to simultaneously consider channel and spatial information, thereby enhancing the quality of front-related features represented by physical information. The decoder utilizes scale-adaptive attention to dynamically adjust fusion weights of abstract feature maps at different scales, effectively capturing weak frontal features embedded in physical information while filtering out strong non-frontal features. Multiple experimental results demonstrate that incorporating physics-guided terms and attention mechanisms into deep learning networks for atmospheric front identification significantly promotes network training, improves recognition precision, and enhances model robustness and generalization capability. Therefore, the proposed PG-AMUnet model can be widely utilized in practical weather analysis applications.},
  archive      = {J_NEUCOM},
  author       = {Xinya Ding and Qian Li and Liang Zhang and Tianying Wang and Xiaoping Zhao and Yudi Liu and Yunpeng Zhang},
  doi          = {10.1016/j.neucom.2025.131835},
  journal      = {Neurocomputing},
  month        = {1},
  pages        = {131835},
  shortjournal = {Neurocomputing},
  title        = {A deep neural network model with physics-guided term for automatic identification of atmospheric fronts},
  volume       = {659},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Speeding up omnidirectional video quality assessment with reinforcement learning based key-frame selection. <em>NEUCOM</em>, <em>659</em>, 131834. (<a href='https://doi.org/10.1016/j.neucom.2025.131834'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Omnidirectional video quality assessment (OVQA) models typically require to extract spatiotemporal features from high-resolution video, leading to high computational complexity. To mitigate this, some existing methods employ uniform frame sampling to reduce the number of frames processed, but the sampled frames often fail to capture critical spatiotemporal degradations. In this work, we propose a key-frame selection network that adopts reinforcement learning to select distortion-aware key-frames according to inter-frame salient changes. Based on the key-frame selection network, we develop a novel OVQA framework. The resulting model samples key-frames at the original resolution for spatial feature extraction and reuses the temporal features from the key-frame selection network as inter-frame temporal representations to reduce the computational complexity of the model. Then, the computed spatio-temporal features are aggregated for quality prediction. We evaluate the performance of the proposed quality model on three publicly available OVQA databases. The experimental results show that the proposed model achieves superior performance relative to state-of-the-art quality models, especially in inference speed.},
  archive      = {J_NEUCOM},
  author       = {Ruikang Yu and Zongyao Hu},
  doi          = {10.1016/j.neucom.2025.131834},
  journal      = {Neurocomputing},
  month        = {1},
  pages        = {131834},
  shortjournal = {Neurocomputing},
  title        = {Speeding up omnidirectional video quality assessment with reinforcement learning based key-frame selection},
  volume       = {659},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Enhancing quasi-nonlinear long-term cognitive networks with temporal attention for pattern classification. <em>NEUCOM</em>, <em>659</em>, 131830. (<a href='https://doi.org/10.1016/j.neucom.2025.131830'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Long-term Cognitive Networks (LTCNs) are knowledge-based recurrent neural networks that hold significant promise in machine learning settings, particularly in structured pattern classification. These neural systems enable hybrid intelligence by allowing domain experts to encode knowledge into the network through a non-trainable weight matrix. However, LTCN-based classifiers face two key limitations that restrict their approximation capabilities. (i) Both neural concepts and weights must map to specific components of the physical system. (ii) Temporal states are not fully exploited when deriving class labels. This paper presents an enhanced LTCN-based classifier that addresses these limitations. First, we introduce a tunable quasi-nonlinear reasoning rule. Each neural concept has independent learnable unbounded parameters that evolve over iterations. Second, we propose a temporal attention mechanism that projects hidden states to a new state space and assigns different attention weights to each iteration based on its relevance. We provide theoretical evidence that this temporal attention mechanism outperforms residual-like learnable connections. Finally, we formalize a gradient-based learning algorithm to fine-tune both the quasi-nonlinear parameters and the projection matrices used by the temporal attention mechanism. Numerical simulations on real-world datasets confirm that our classifier achieves superior classification accuracy compared to tree ensembles, neurosymbolic methods, transformer-based methods, and kernel machines.},
  archive      = {J_NEUCOM},
  author       = {Gonzalo Nápoles and Yamisleydi Salgueiro},
  doi          = {10.1016/j.neucom.2025.131830},
  journal      = {Neurocomputing},
  month        = {1},
  pages        = {131830},
  shortjournal = {Neurocomputing},
  title        = {Enhancing quasi-nonlinear long-term cognitive networks with temporal attention for pattern classification},
  volume       = {659},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). LLM guided counterfactual reasoning for zero-shot knowledge based visual question answering. <em>NEUCOM</em>, <em>659</em>, 131828. (<a href='https://doi.org/10.1016/j.neucom.2025.131828'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Knowledge-Based Visual Question Answering (KB-VQA) requires not only understanding image content but also reasoning with external knowledge. Recent advancements in large language models (LLMs) have enabled zero-shot inference for KB-VQA. However, existing paradigms still suffer from three major limitations: a knowledge disconnect among the image, the caption model, and the LLM, a lack of out-of-distribution task awareness, and difficulty in capturing fine-grained visual details. To address these challenges, we propose LLM guided Counterfactual Samples Synthesizing and Training (LCSST), a unified framework that enhances caption models through counterfactual reasoning. Specifically, we design three synergistic modules: Counterfactual Caption Generation (CounCG) introduces alternative factual perspectives via vision-language models and LLMs. Counterfactual Image Synthesis (CounIS) generates counterfactual images, guided by counterfactual captions, using advanced detection, segmentation, and image synthesis techniques. CounCG and CounIS generate diverse and informative textual and visual counterfactual samples to mitigate knowledge disconnect. Additionally, we introduce a Diversity-Aware Contrastive Loss (DACL) to improve generalization and encourage fine-grained discrimination by emphasizing semantically similar and diverse negatives. Extensive experiments on OK-VQA and A-OKVQA demonstrate that LCSST substantially outperforms existing zero-shot KB-VQA methods.},
  archive      = {J_NEUCOM},
  author       = {Zhuhan Zhang and Min Jiang and Jun Kong and Jiayi Li},
  doi          = {10.1016/j.neucom.2025.131828},
  journal      = {Neurocomputing},
  month        = {1},
  pages        = {131828},
  shortjournal = {Neurocomputing},
  title        = {LLM guided counterfactual reasoning for zero-shot knowledge based visual question answering},
  volume       = {659},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Automated brain extraction on diffusion-weighted images using pseudo and cross semi-supervised method. <em>NEUCOM</em>, <em>659</em>, 131825. (<a href='https://doi.org/10.1016/j.neucom.2025.131825'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Brain extraction in diffusion-weighted images (DWI) is a primary and crucial step in lesion analysis for stroke patients, where imperfect brain region segmentation can lead to a decrease in the accuracy of downstream tasks. However, the development of DWI-specific brain extraction tools (BET) has been significantly hindered by the scarcity of publicly available DWI datasets for supervised deep learning, in stark contrast to the abundance of T1-weighted (T1w) images. In response, we proposed SSL-BET, a semi-supervised brain extraction method designed to make effective use of large amounts of unlabeled clinical DWI data, thereby reducing the reliance on costly and time-consuming manual annotations. With limited labeled training data, our method enables the construction of deep learning models for high-accuracy brain extraction from DWI scans of stroke patients. SSL-BET is designed upon the principles of consistency regularization and entropy minimization, allowing the generation of high-quality supervision from unlabeled clinical data to enhance model performance in semi-supervised settings. Our method attained a Dice Similarity Coefficient (DSC) of 97.32 %±0.59 % across 50 test samples, necessitating merely 3 labeled training samples. The experiments revealed that our approach significantly surpassed the commonly employed BET methods, such as FSL-BET, SPM-BET, and HD-BET ( p < 0.05). Compared to other semi-supervised methods, it was shown that with a single labeled data, ours achieved enhanced results (DSC: 96.46 %±0.88 %). To summarize, our method helped to alleviate the need for annotation of DWI brain extraction, thereby holding promise to facilitate the automated analysis of DWI scans for stroke patients.},
  archive      = {J_NEUCOM},
  author       = {Yibing Chen and Benqi Zhao and Jinyang Li and Zhilin Wang and Yingchun Fan and Kaiyue Su and Zhuozhao Zheng and Zhensen Chen},
  doi          = {10.1016/j.neucom.2025.131825},
  journal      = {Neurocomputing},
  month        = {1},
  pages        = {131825},
  shortjournal = {Neurocomputing},
  title        = {Automated brain extraction on diffusion-weighted images using pseudo and cross semi-supervised method},
  volume       = {659},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). A hybrid SGC-transformer network for EEG emotion recognition with historical data integration. <em>NEUCOM</em>, <em>659</em>, 131822. (<a href='https://doi.org/10.1016/j.neucom.2025.131822'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, great progress has been made in emotion recognition based on electroencephalogram (EEG). However, the complex relationships between EEG channels have not been fully explored, and existing methods rarely leverage the potential of EEG data volume, both of which impact performance of emotion recognition models. For capturing inter-channel relationships, we consider combining Simple Graph Convolutional Network (SGC) and Transformer to exploit relationships from channels’ topology and features, proposing an SGC-Transformer network for emotion recognition. Specifically, the network consists of multiple SGC-Transformer (SGCT) blocks, each incorporating one SGC layer and one Transformer architecture. The SGCT block first captures topological structure of channels through the SGC, and then utilizes the features of topological nodes to learn the long-distance dependencies between channels with Transformer, thereby enabling the complementary integration of topology-based and feature-based complex relationships. Furthermore, to maximize the utilization of EEG data, we develop an EEG-data integration strategy that incorporates historically collected data into the current training set, surprisingly finding that it significantly improves model prediction accuracy. Extensive experiments on the SEED, SEED-IV and HBUED, demonstrate that integrating both topology-based and feature-based information is more effective in exploring inter-channel relationships, compared with feature-based methods alone. Additionally, the EEG-data integration strategy provides an important reference for future studies involving increased data volumes. The relevant code can be found at https://github.com/braverSheep/SGCTNet .},
  archive      = {J_NEUCOM},
  author       = {Yong Yang and Kaibo Shi and Nan Zhou and Wenhao Wang and Shiping Wen and Ming Zhu and Yuanlun Xie},
  doi          = {10.1016/j.neucom.2025.131822},
  journal      = {Neurocomputing},
  month        = {1},
  pages        = {131822},
  shortjournal = {Neurocomputing},
  title        = {A hybrid SGC-transformer network for EEG emotion recognition with historical data integration},
  volume       = {659},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Reinforcement learning based deep fuzzy hierarchical clustering to generate personalized non-fungible token artwork. <em>NEUCOM</em>, <em>659</em>, 131821. (<a href='https://doi.org/10.1016/j.neucom.2025.131821'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the realm of information technology and software development, digital assets increasingly manifest as Non-Fungible Tokens (NFTs) on blockchain platforms, embodying intrinsic material value that enhances user satisfaction. Despite their potential, the creation of NFTs remains costly, time-consuming, and labor-intensive. To address these challenges, we introduce a digital asset generation engine specifically designed for producing NFT artwork. Utilizing real-world datasets curated by digital art experts, our engine synthesizes individual image layers into cohesive, high-quality artistic outputs. Leveraging artificial intelligence, we employ a novel Deep Fuzzy Hierarchical Clustering approach, which integrates autoencoder neural networks, fuzzy clustering, and hierarchical clustering methods. This integrated approach enables precise classification of image layers, achieving an impressive accuracy rate of 95 %. Here, we demonstrate the potential of AI-enhanced solutions in the digital art and NFT space. Our engine not only reduces costs and labor intensity in digital art production but also allows users to personalize their NFT collections by selecting desired layers and specifying rarity, arrangement order, and metadata details. This study underscores the significance of intersectional research between artificial intelligence and fine arts, opening avenues for future advancements in computational art analysis and creative AI applications.},
  archive      = {J_NEUCOM},
  author       = {Arman Daliri and Nora Mahdavi and Mahdieh Zabihimayvan and Aynaz Norouzi Baranghar and Nima Zaeimzadeh and Javad mohammadzadeh},
  doi          = {10.1016/j.neucom.2025.131821},
  journal      = {Neurocomputing},
  month        = {1},
  pages        = {131821},
  shortjournal = {Neurocomputing},
  title        = {Reinforcement learning based deep fuzzy hierarchical clustering to generate personalized non-fungible token artwork},
  volume       = {659},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Design of energy-efficient LIF neuron using CMOS compatible gate-all-around floating nanosheet FET for bio-inspired spiking neural networks. <em>NEUCOM</em>, <em>659</em>, 131814. (<a href='https://doi.org/10.1016/j.neucom.2025.131814'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The fundamental component of an artificial spiking neural network (SNN) is an electronic device designed to emulate a biological neuron effectively. However, a key concern is the high energy consumption and large area associated with these artificial neurons, rendering them highly inefficient. This article presents a CMOS-compatible Gate-All-Around Floating Nanosheet Field-Effect Transistor (GAA FNSFET)-based Leaky Integrate-and-Fire (LIF) neuron with gate length of 100 nm. This design achieves an exceptionally low energy consumption of 4.88 fJ, thereby establishing a new benchmark in the field. Utilizing well calibrated 3D TCAD simulation and the SRH recombination model, along with the Unibo2 model to represent impact ionization phenomena, the proposed GAA FNSFET LIF neuron effectively captures integration and recombination aspects within it. Moreover, it operates with a supply voltage of merely 1 V, significantly lower than conventional bulk FinFET and PD-SOI MOSFET-based LIF neurons. Also, it demonstrates spiking frequency of ∼19.3 MHz, approximately ∼10 5 times higher than that of a biological neuron, controlled by the input voltage of the device, allowing the neuron adapt dynamically. The FNSFET technology offers superior electrostatic control, minimizing parasitic capacitances and facilitating rapid switching capabilities, thereby optimizing both power and performance simultaneously. Furthermore, the effective area of the FNSFET amounts to 0.033 µm 2 making it an appealing choice for low-power, area efficient, large-scale SNN applications.},
  archive      = {J_NEUCOM},
  author       = {Yashodhan Bhatawdekar and Syed Mohammad Riyaz and Lakshmi Amrutha Yechuri and Sresta Valasa and Venkata Ramakrishna Kotha and Sunitha Bhukya and Shubham Tayal and Narendar Vadthiya},
  doi          = {10.1016/j.neucom.2025.131814},
  journal      = {Neurocomputing},
  month        = {1},
  pages        = {131814},
  shortjournal = {Neurocomputing},
  title        = {Design of energy-efficient LIF neuron using CMOS compatible gate-all-around floating nanosheet FET for bio-inspired spiking neural networks},
  volume       = {659},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). DermClinical: Clinical-oriented dataset and evaluation for computer-aided dermatological diagnosis. <em>NEUCOM</em>, <em>659</em>, 131811. (<a href='https://doi.org/10.1016/j.neucom.2025.131811'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Dermatosis is one of the most common diseases and requires a heavy workload for diagnosis due to its high incidence. Despite the development of deep-learning-based computer-aided diagnosis reported in the literature, application in real clinical practice remains a challenge. We argue that the key reason is the misalignment between the evaluation of benchmark studies and expectations of workflows in practice. In this paper, we systematically address the problem from perspectives of dataset, metric, as well as evaluation schemes, collectively denoted as an evaluation framework DermClinical. Specifically, we build a so-far largest dataset, with the introduction of the “unknown" category that measures the ability of models to deal with cases from unseen categories. On this basis, we propose a novel metric which, as opposed to common classification based metrics, directly measures the labor saved by a computational model for doctors. Finally, we design a limited-data evaluation scheme to simulate situations when unseen categories are encountered in real practice. Extensive experiments are conducted to evaluate and compare current computational methods. The dataset, metric and code will be released to facilitate the research.},
  archive      = {J_NEUCOM},
  author       = {Zihao Liu and Zhiqiang Hu and Ruiqin Xiong and Shaoting Zhang and Tingting Jiang},
  doi          = {10.1016/j.neucom.2025.131811},
  journal      = {Neurocomputing},
  month        = {1},
  pages        = {131811},
  shortjournal = {Neurocomputing},
  title        = {DermClinical: Clinical-oriented dataset and evaluation for computer-aided dermatological diagnosis},
  volume       = {659},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). SolNet and machine learning models for effective dust detection in photovoltaic systems. <em>NEUCOM</em>, <em>659</em>, 131810. (<a href='https://doi.org/10.1016/j.neucom.2025.131810'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As photovoltaic (PV) systems continue to gain popularity as a cost-effective energy solution, maintaining their efficiency has become increasingly important. One of the main challenges these systems face is dust buildup on PV panels, which can significantly lower their performance. In response to this issue, this paper proposes a machine learning-based automated dust detection system designed to optimize the cleaning process for PV modules. The study evaluates various models, including Decision Tree (DT), Support Vector Machine (SVM), Random Forest (RF), and K-Nearest Neighbor (KNN), and compares to the most state-of-the-art like SolNet + , to identify the most effective model for dust detection. Several techniques, such as oversampling, scaling, feature selection, and hyperparameter tuning, were applied to improve model performance. A new optimizer, the loin optimizer, was employed for hyperparameter tuning. In addition, these models were compared to 28 deep-learning models. Among the models tested, SolNet + emerged as the top performer, consistently achieving the highest accuracy, recall, and F1 scores for dust detection. When compared to two previous studies, SolNet + stands out, particularly in Scenario 2, where it reached an impressive 88 % accuracy—just behind EfficientNetB7 (89.8 %) and EfficientNetB4 (88.9 %). While Random Forest showed strong accuracy, it faced challenges with imbalanced data. SVM performed moderately, and KNN and Decision Tree ranked lower, especially in terms of precision and recall in imbalanced data conditions. These findings highlight the significant potential of machine learning to enhance PV maintenance by effectively detecting dust accumulation.},
  archive      = {J_NEUCOM},
  author       = {Hmeda Najemddin Musbah and Ali Othmam Albaji and Meftah Elsaraiti and Araf Taher Almasri},
  doi          = {10.1016/j.neucom.2025.131810},
  journal      = {Neurocomputing},
  month        = {1},
  pages        = {131810},
  shortjournal = {Neurocomputing},
  title        = {SolNet and machine learning models for effective dust detection in photovoltaic systems},
  volume       = {659},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Bipartite synchronization for multi-level networks with antagonistic interactions. <em>NEUCOM</em>, <em>659</em>, 131808. (<a href='https://doi.org/10.1016/j.neucom.2025.131808'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we investigate the bipartite synchronization (Bi-Syn) problem of multi-level networks which have antagonistic interactions. Firstly, we propose a novel multiweighted multi-level network model with antagonistic interactions, which generalizes existing single-layer and multi-layer network models (with or without antagonistic interactions) as special cases. Then, the multi-level network, which is described by multiweighted and signed digraphs, can be transformed into multi-level network with a single weight by rearranging variables’ order technique. Moreover, different from previous studies which restricted that the networks should be structurally balanced, the topology structure of our newly proposed multiweighted multi-level network can be structurally unbalanced and not strongly connected. What’s more, by constructing Lyapunov functions based on the Kronecker product of left eigenvectors of the multi-level coupling matrices, sufficient conditions for exponential Bi-Syn of multi-level networks are derived. Obtained criteria can reduce the conservatism of existing results, as validated by simulations.},
  archive      = {J_NEUCOM},
  author       = {Leijing Xie and Xiwei Liu},
  doi          = {10.1016/j.neucom.2025.131808},
  journal      = {Neurocomputing},
  month        = {1},
  pages        = {131808},
  shortjournal = {Neurocomputing},
  title        = {Bipartite synchronization for multi-level networks with antagonistic interactions},
  volume       = {659},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Finite-time asynchronous state estimation for two-time-scale complex networks with sojourn probabilities and event-based AF relay protocols. <em>NEUCOM</em>, <em>659</em>, 131807. (<a href='https://doi.org/10.1016/j.neucom.2025.131807'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we study the finite-time H ∞ asynchronous state estimation problem for a two-time-scale complex network with sojourn probabilities under event-based amplify-and-forward (AF) relay protocols, where the mode sojourn probabilities are partially known. An asynchronous and general state estimator is proposed, which includes four different types of state estimators. A new dynamic event-triggered mechanism (DETM) is embedded in each AF relay to reduce the energy loss of the relay and save network resources in the relay-to-estimator channel. A DETM of the same form is embedded in each sensor to save network resources in the sensor-to-relay channel. Each proposed DETM includes a multiplicative adjusting variable and an additive internal dynamic variable (IDV), allowing the overall system to save network resources while maintaining estimation performance. The aim of this study is to design an asynchronous state estimator such that the resulting error dynamics are stochastically finite-time bounded with H ∞ performance. The conditions for the state estimator design are derived by constructing a Lyapunov function associated with the singular perturbation parameter, the system mode and two sets of IDVs. A numerical example validates the effectiveness of the proposed method and the superiority of the designed DETMs.},
  archive      = {J_NEUCOM},
  author       = {Jinrong Fan and Niewen Xu and Xiongbo Wan and Leimin Wang},
  doi          = {10.1016/j.neucom.2025.131807},
  journal      = {Neurocomputing},
  month        = {1},
  pages        = {131807},
  shortjournal = {Neurocomputing},
  title        = {Finite-time asynchronous state estimation for two-time-scale complex networks with sojourn probabilities and event-based AF relay protocols},
  volume       = {659},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). FEMTL-DR: A feature-enhanced multi-task learning model for flexible drug recommendation. <em>NEUCOM</em>, <em>659</em>, 131806. (<a href='https://doi.org/10.1016/j.neucom.2025.131806'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multiple disease-based syndrome reasonings and their corresponding drug recommendations are crucial for personalized diagnosis and treatment in Traditional Chinese Medicine (TCM). However, it remains a challenging task to effectively extract and integrate various entities and multi-dimensional relationships for syndrome-based drug recommendations. This study investigates FEMTL-DR, a novel feature-enhanced multi-task learning model for flexible drug recommendation. Based on various entity characteristics, we propose a hybrid multi-entity encoding strategy to realize diverse disease, syndrome, herb, and herb property encodings. Then, the pairwise similarity between different entity encodings is calculated to reconstruct the adjacency matrix of the heterogeneous graph. A state-space transformer-based strategy is presented to enhance node features by capturing long-range dependencies and extracting local and global information. Finally, a multi-task learning framework combined with a graph neural network and transformer module is constructed to learn the relationships between enhanced node features for syndrome classification and drug recommendation. Taking Reflux Esophagitis (RE) as an instance, a series of experiments have been conducted to verify the performance of the proposed FEMTL-DR, including comparison experiments, ablation verification, and parameter sensitivity tests. The experimental results indicate that the proposed model outperforms baselines on representative evaluation metrics. Specifically, the overall evaluation (integrating syndrome classification and drug recommendation) achieves improvements of at least 2.48 % in Average Precision (AP), 2.93 % in Precision, 2.77 % in Recall, and 3.08 % in F1-score. This study provides a novel solution for personalized diagnosis and treatment for RE in TCM and can be considered a paradigm for other diseases.},
  archive      = {J_NEUCOM},
  author       = {Junyang Leng and Yin Zhang and Fang Hu and Meng Zhang and Pin-Han Ho},
  doi          = {10.1016/j.neucom.2025.131806},
  journal      = {Neurocomputing},
  month        = {1},
  pages        = {131806},
  shortjournal = {Neurocomputing},
  title        = {FEMTL-DR: A feature-enhanced multi-task learning model for flexible drug recommendation},
  volume       = {659},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Feature distribution learning based on variance transfer and center shift for long-tailed classification. <em>NEUCOM</em>, <em>659</em>, 131805. (<a href='https://doi.org/10.1016/j.neucom.2025.131805'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In real-world scenarios, data distributions often exhibit a long-tailed nature: a few categories (head classes) contain the majority of samples, while the remaining categories (tail classes) suffer from severe data scarcity. Due to the limited number of samples, the observed distributions of tail classes often fail to capture their true underlying distributions, resulting in substantially degraded generalization performance. We observe a strong negative correlation between the distances of feature centers across categories and the similarity of their covariance matrices in the feature space. Motivated by this finding, we propose estimating the true distributional shapes of tail classes by leveraging the covariance structures of head classes. Moreover, our experiments reveal that the observed feature centers of tail classes are frequently biased away from their true centers. Interestingly, this bias direction tends to align closely—with high cosine similarity—with the direction pointing toward the feature center of the nearest head class. Building on this observation, we predict and apply such bias shifts to better approximate the true feature distributions, thereby refining the decision boundaries. This approach leads to a substantial improvement in the generalization performance of tail classes.},
  archive      = {J_NEUCOM},
  author       = {Chenxi Hong and Qiang Zhao and Min He and Tao Tan and Chenggang Yan},
  doi          = {10.1016/j.neucom.2025.131805},
  journal      = {Neurocomputing},
  month        = {1},
  pages        = {131805},
  shortjournal = {Neurocomputing},
  title        = {Feature distribution learning based on variance transfer and center shift for long-tailed classification},
  volume       = {659},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Data driven modeling and stability analysis for stochastic interconnected systems with uncertainties. <em>NEUCOM</em>, <em>659</em>, 131804. (<a href='https://doi.org/10.1016/j.neucom.2025.131804'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper proposes a deep neural network (DNN)-augmented inverse optimal adaptive control for stabilizing stochastic interconnected systems, focusing on challenges posed by unmodeled dynamics and uncertainties. A structured small-gain framework is developed to address the mutual dependencies among interconnected subsystems and to ensure the input-to-state practical stability in probability. To approximate unmodeled nonlinearities, a DNN-based identifier is integrated into each subsystem, allowing real-time estimation of uncertain dynamics while preserving the analytical tractability of the controller. Through this idea, we develop an adaptive controller that guarantees uniform boundedness of the interconnected system and ensures state convergence within a small neighborhood of the origin while optimizing overall system performance. Finally, an automobile suspension system is presented to demonstrate how the proposed approach effectively achieves closed-loop stability for stochastic interconnected systems while incorporating inverse optimal control design.},
  archive      = {J_NEUCOM},
  author       = {Yu Shao and Xinyu Zhang and Runzi Liao and Shihua Li and Rongjie Liu},
  doi          = {10.1016/j.neucom.2025.131804},
  journal      = {Neurocomputing},
  month        = {1},
  pages        = {131804},
  shortjournal = {Neurocomputing},
  title        = {Data driven modeling and stability analysis for stochastic interconnected systems with uncertainties},
  volume       = {659},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Quasi-bipartite synchronization of a new stochastic impulsive reaction-diffusion network by self-triggered control approach. <em>NEUCOM</em>, <em>659</em>, 131801. (<a href='https://doi.org/10.1016/j.neucom.2025.131801'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This note aims to develop a quasi-bipartite synchronization control strategy for stochastic reaction-diffusion networks via self-triggered control technology. Based on the information of past triggered moments of nodes, a new self-triggered impulsive controller is designed, and the Zeno behavior is avoided. The impulse control rate is proposed, and a self-triggered mechanism to determine the timing of impulses is provided, taking into account the Zeno behavior at the triggering moment, addressing practical challenges in fields such as mechanical engineering and aerial vehicles. On this basis, parameter mismatch and quasi-synchronization conditions of the network are fully considered, which can be applied to multi-node nonuniform synchronization systems. Furthermore, sufficient conditions and detailed proof process for quasi-bipartite synchronization of stochastic reaction-diffusion networks are provided. Finally, the correctness of the theoretical results of quasi-bipartite synchronization is verified by means of two numerical examples.},
  archive      = {J_NEUCOM},
  author       = {Minghui Song and Yonggui Kao and Wei Xie and Chuntao Shao and Xinsong Yang},
  doi          = {10.1016/j.neucom.2025.131801},
  journal      = {Neurocomputing},
  month        = {1},
  pages        = {131801},
  shortjournal = {Neurocomputing},
  title        = {Quasi-bipartite synchronization of a new stochastic impulsive reaction-diffusion network by self-triggered control approach},
  volume       = {659},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). RA2Net: Rotated alignment and aggregation network for oriented object detection in aerial images. <em>NEUCOM</em>, <em>659</em>, 131798. (<a href='https://doi.org/10.1016/j.neucom.2025.131798'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Oriented object detection in aerial images presents unique challenges, including feature misalignment caused by rotation-agnostic convolutional backbones, severe scale and aspect ratio imbalances among densely packed objects, and instability in loss optimization due to angular discontinuity. Traditional horizontal detectors fail to capture orientation variance, while existing rotated detection methods often struggle with aligning rotated sampling points or applying uniform penalties regardless of object geometry. In this paper, we propose RA 2 Net (Rotated Alignment and Aggregation Network), a novel framework that dynamically aligns features and adaptively optimizes loss for robust oriented object detection. First, the Rotated Point Alignment (RPA) module addresses feature misalignment by predicting oriented bounding boxes (OBBs) and refining convolutional sampling points via rotated cross convolution, enabling rotation-equivariant feature extraction. Second, the Rotated Feature Aggregation (RFA) module integrates dual-branch attention to fuse multi-scale local and global features, selectively enhancing informative regions while suppressing background noise. Third, we introduce an Adaptive Geometric-aware Loss (AGL) that combines thermodynamic energy modeling for small objects with polar coordinate regression for elongated objects, thereby stabilizing training and improving localization accuracy. Extensive experiments on aerial object detection benchmarks demonstrate that RA 2 Net not only achieves high precision but also maintains robustness across diverse object scales and orientations.},
  archive      = {J_NEUCOM},
  author       = {Min Dang and Qijie Xu and Gang Liu and Hao Li and Xu Wang},
  doi          = {10.1016/j.neucom.2025.131798},
  journal      = {Neurocomputing},
  month        = {1},
  pages        = {131798},
  shortjournal = {Neurocomputing},
  title        = {RA2Net: Rotated alignment and aggregation network for oriented object detection in aerial images},
  volume       = {659},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Evidence ratio classifier: A one-pass model for fast incremental learning. <em>NEUCOM</em>, <em>659</em>, 131797. (<a href='https://doi.org/10.1016/j.neucom.2025.131797'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We introduce the Evidence Ratio Classifier (ERC), a one-pass learning rule in which every weight is a closed-form ratio of empirical co-occurrence counts. Training therefore reduces to a single linear scan of the data, and inference to a few table look-ups followed by a winner-takes-all comparison; no back-propagation, learning-rate tuning, or other iterative optimisation is required. ERC is benchmarked on four standard tabular datasets— Adult Income , Mushroom , Iris , and Credit-Card Fraud . On Adult ( 32 561 training instances) it reaches 85.1 % accuracy versus 85.9 % for a variational Bayesian neural network and 82.1 % for Gaussian Naive Bayes, while requiring about 1700 × fewer arithmetic operations end-to-end and about 11 × fewer at inference. Across the other tasks ERC matches or exceeds Naive Bayes and stays within one percentage point of the variational baseline, with a 4–8 × lower arithmetic cost on smaller datasets. Because ERC stores, for every input pattern R , the empirical conditional probability P ( class ∣ R ) , its lookup table is self-explanatory: each entry quantifies the data support for the rule R → class . This intrinsic transparency is attractive wherever auditability is required, e.g., credit scoring and anti-money-laundering (Basel III; EU AI Act, Art. 13), medical decision support (FDA SaMD guidance), insurance underwriting, or judicial risk assessment. Model updates consist solely of incrementing integer counters; no gradients or floating-point arithmetic are involved. Such integer-only updates execute on processors that lack a hardware FPU or must remain fully deterministic, as found in ultra-low-power micro-controllers, hard real-time UAV and robotic controllers, or certified safety-critical systems (DO-178C avionics, CENELEC EN 50,128 railway, IEC 62,304 medical implants). ERC thus combines accuracy, explainability, and hardware efficiency in a single, making it deployable on resource–constrained hardware.},
  archive      = {J_NEUCOM},
  author       = {F.L.Olivier Manette},
  doi          = {10.1016/j.neucom.2025.131797},
  journal      = {Neurocomputing},
  month        = {1},
  pages        = {131797},
  shortjournal = {Neurocomputing},
  title        = {Evidence ratio classifier: A one-pass model for fast incremental learning},
  volume       = {659},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Designing a hybrid optimization methodology for delineating boundary of ultrasound prostate cancer with an explainable mathematical model. <em>NEUCOM</em>, <em>659</em>, 131794. (<a href='https://doi.org/10.1016/j.neucom.2025.131794'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Accurately segmenting prostate cancer (PCa) is crucial for enhancing male survival rates, yet it poses challenges due to low-intensity contrast around the PCa outline caused by intestinal gas interference, the existence of shadow artifact, the impact of the heterogeneity among different patients, and human anatomical diversities. Our study proposes a novel ultrasound (US)-guided hybrid optimization algorithm, containing two subnetworks: 1) the first subnetwork uses a deep parallel network structure to complete the initial segmentation stage automatically; 2) the second subnetwork is used to fine-tune the initial outcome, where the initial PCa outlines are optimized via an intelligent hunting polygon tracking method linked to a modified quantum-inspired evolutionary neural network. After the neural network’s training, an explainable mathematical mapping formula based on the optimal parameters of an evolutionary neural network is adopted to produce smooth PCa outlines. Experimental outcomes prove the superiority of our proposed methodology over other recent medical image segmentation approaches, achieving an average Dice score (DS), Jaccard index (JI), and accuracy (ACC) of 83.3 + 2.5 %, 82.6 + 3.2 %, and 83.2 + 2.7 %, respectively.},
  archive      = {J_NEUCOM},
  author       = {Tao Peng and Dehui Xiang and Zhongyi Zhang and Binbin Jiang and Baoqing Nie and Fei Shi and Derun Li and Caishan Wang and Weifang Zhu and Jing Cai and Enting Gao and Xinjian Chen},
  doi          = {10.1016/j.neucom.2025.131794},
  journal      = {Neurocomputing},
  month        = {1},
  pages        = {131794},
  shortjournal = {Neurocomputing},
  title        = {Designing a hybrid optimization methodology for delineating boundary of ultrasound prostate cancer with an explainable mathematical model},
  volume       = {659},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). OV-KFA: Open-vocabulary object detection via key feature alignment. <em>NEUCOM</em>, <em>659</em>, 131790. (<a href='https://doi.org/10.1016/j.neucom.2025.131790'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Open-vocabulary object detection (OVD) leverages vision-language models (VLMs) to extend detection capabilities beyond predefined categories, enabling flexible recognition in real-world open scenarios. However, existing OVD frameworks suffer from significant performance degradation when directly transferring VLM knowledge to detection tasks. This fundamental challenge arises because VLMs are trained to match whole images with textual descriptions, whereas object detection demands precise alignment between cropped image regions and corresponding text spans. Furthermore, during training on base classes, the model may learn to align irrelevant features, leading to overfitting issues that hinder generalization to novel categories. To address this fundamental alignment challenge, we propose Open-Vocabulary Object Detection via Key Feature Alignment (OV-KFA), which introduces two key innovations: (1) Bottleneck Adapter (BA), a lightweight plug-and-play module that distills key features and enhances cross-modal alignment, and (2) Transferable Prompt (TP), a novel training paradigm that learns generalizable prompt representations without architectural modifications. Our approach seamlessly improves open vocabulary detectors’ generalization to novel classes while preserving computational efficiency during inference. Extensive evaluations on the challenging OV-COCO and OV-LVIS benchmarks demonstrate substantial performance gains, validating the effectiveness of our approach. Code will be publicly available.},
  archive      = {J_NEUCOM},
  author       = {Yunqing Jiang and Sunyuan Qiang and Wuchen Li and Huijia Zhao and Yanyan Liang},
  doi          = {10.1016/j.neucom.2025.131790},
  journal      = {Neurocomputing},
  month        = {1},
  pages        = {131790},
  shortjournal = {Neurocomputing},
  title        = {OV-KFA: Open-vocabulary object detection via key feature alignment},
  volume       = {659},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Efficient guided diffusion toward adverse weather image restoration. <em>NEUCOM</em>, <em>659</em>, 131788. (<a href='https://doi.org/10.1016/j.neucom.2025.131788'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Image restoration under adverse weather conditions aims to recover clean scenes from degraded inputs. While existing CNN-based methods often yield unrealistic results as they prioritize minimizing pixel-level distortion, which has poor correlation with human perception. Although diffusion-based models achieve commendable perceptual quality, they encounter challenges in balancing quality and efficiency. To address these issues, we propose a weather-general guided diffusion model, namely WGDiff, for efficient and high-quality image restoration under adverse weather conditions. Specifically, a dual-branch transformer-based architecture is constructed with Direct Restoration Network (DRRN) and Diffusive Restoration Network (DFRN), where DRRN employs physics-aware weather-degradation model with explicit constraints for high fidelity restoration, while DFRN leverages conditional diffusion to iteratively generated results with high perceptual quality. Additionally, an error-reduction guided diffusive restoration method is proposed, which utilizes DRRN outputs as guidance and iteratively refines intermediate diffusion results of DFRN for high-quality restoration. Furthermore, an efficient shifted-window patch sampling strategy is developed to handle the trade-off between quality and efficiency for size-agnostic image restoration, significantly boosting the sampling speed and simultaneously alleviating blocking artifacts. Extensive experimental results on synthetic and real-world datasets demonstrate that our approach achieves state-of-the-art (SOTA) performance in generating more realistic restoration results and striking a superior balance between quality and efficiency. Notably, our method outperforms SOTA diffusion-based methods with a remarkable reduction of 44.8 %–96.5 % in parameters and a significant speed enhancement of 7.9–23.7 times. The source code will be released in https://github.com/TianhangTang/WGDiff .},
  archive      = {J_NEUCOM},
  author       = {Tang Tianhang and Chen Jie and Lv Shun and Lei Ling and Liu Yiguang},
  doi          = {10.1016/j.neucom.2025.131788},
  journal      = {Neurocomputing},
  month        = {1},
  pages        = {131788},
  shortjournal = {Neurocomputing},
  title        = {Efficient guided diffusion toward adverse weather image restoration},
  volume       = {659},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Feature representation learning for image denoising. <em>NEUCOM</em>, <em>659</em>, 131787. (<a href='https://doi.org/10.1016/j.neucom.2025.131787'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Image denoising plays a vital role in enhancing image quality for various downstream vision tasks by learning robust feature representations that distinguish clean signals from noise. Recent advances in deep learning have enabled data-driven feature extraction but frequently face challenges such as spatial-channel feature redundancy, suboptimal fusion of multi-level features, and over-smoothing due to pixel-wise loss functions. To address these interconnected issues, this paper proposes the Perceptual Feature Learning Network (PFLN), a lightweight architecture explicitly designed for efficient, discriminative feature learning. PFLN introduces a Redundancy Filtering Block (RFB) to suppress redundant information, a Selective Attention Fusion Block (SAFB) to adaptively integrate complementary features, and multi-level feature constraints combining pixel, perceptual, and spatial losses for holistic optimization. Experiments demonstrate that PFLN effectively learns compact, context-aware representations that improve denoising fidelity and texture preservation while maintaining computational efficiency, providing a balanced solution for real-world image denoising tasks.},
  archive      = {J_NEUCOM},
  author       = {Yuxuan Hu and Shichao Zhang},
  doi          = {10.1016/j.neucom.2025.131787},
  journal      = {Neurocomputing},
  month        = {1},
  pages        = {131787},
  shortjournal = {Neurocomputing},
  title        = {Feature representation learning for image denoising},
  volume       = {659},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). DSRJR: A monitoring substitution framework via dual-stream reconstruction and joint representation for fault diagnosis. <em>NEUCOM</em>, <em>659</em>, 131785. (<a href='https://doi.org/10.1016/j.neucom.2025.131785'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Rotary machinery functions in harsh environments and is susceptible to failure, so dependable fault diagnosis methods are essential to ensure equipment safety. Most existing methods assume that the sensor types of inputs are consistent in the training and testing datasets. However, in practice, sensor failure or the lack of corresponding sensors installed on the equipment can cause the model to fail due to incomplete input information, which ultimately leads to monitoring interruption. Therefore, developing an effective diagnostic knowledge transfer mechanism is essential. To address fault monitoring interruptions caused by the absence of the single sensor, this study proposes a monitoring substitution framework via dual-stream reconstruction and joint representation for fault diagnosis. Dual-stream joint alignment framework (DSJA) designs unique feature extraction models according to the characteristics of each signal, and introduces locality-sensitive latent diffusion space (LSLDS) for feature reconstruction. Building on this foundation, joint representation of feature consistency and domain invariance is developed to effectively map and align feature spaces across different modalities. The monitoring-substitution of the two signals is validated using gradient-based class activation map (Grad-CAM) visualization, and the diagnostic and monitoring-substitution performance of the proposed method is assessed across four scenarios. Compared with existing methods, the proposed approach demonstrates superior fault diagnosis and monitoring-substitution capabilities.},
  archive      = {J_NEUCOM},
  author       = {Qin Han and Li Jin and Nan Li and Hui Shi and Xiaoyin Nie and Gang Xie and Haifeng Yang},
  doi          = {10.1016/j.neucom.2025.131785},
  journal      = {Neurocomputing},
  month        = {1},
  pages        = {131785},
  shortjournal = {Neurocomputing},
  title        = {DSRJR: A monitoring substitution framework via dual-stream reconstruction and joint representation for fault diagnosis},
  volume       = {659},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Variation-aware proxy learning for semantic segmentation. <em>NEUCOM</em>, <em>659</em>, 131783. (<a href='https://doi.org/10.1016/j.neucom.2025.131783'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In semantic segmentation, accurately modeling intra-class variation is essential for capturing fine-grained details and resolving ambiguity near class boundaries. While existing proxy-based embedding methods represent each class with a single prototype, they struggle to reflect diverse intra-class structures, especially in complex scenes. In this paper, we propose a novel representation learning framework called Variation-Aware Proxy Learning, which introduces a representative proxy to encode shared class semantics and multiple variation vectors to capture fine-grained intra-class variations. These components are integrated through a factorized similarity score, enabling more expressive and discriminative embedding structures. To further enhance learning in ambiguous regions, we introduce focal modulation and design a new Compositional Similarity Loss composed of attraction and repulsion terms that adaptively amplify the contribution of hard examples. Our method is model-agnostic and requires no additional inference-time cost. Extensive experiments across multiple segmentation benchmarks—Cityscapes, COCO-Stuff10k, iSAID and ADE20K—and diverse backbones including CNNs and Transformers, demonstrate consistent improvements in mIoU and boundary precision, particularly in challenging regions with high intra-class variability.},
  archive      = {J_NEUCOM},
  author       = {Haejun Bae and Byung Cheol Song},
  doi          = {10.1016/j.neucom.2025.131783},
  journal      = {Neurocomputing},
  month        = {1},
  pages        = {131783},
  shortjournal = {Neurocomputing},
  title        = {Variation-aware proxy learning for semantic segmentation},
  volume       = {659},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Low coupling and high interaction dual-branch contrastive pseudo supervision for semi-supervised medical image segmentation. <em>NEUCOM</em>, <em>659</em>, 131779. (<a href='https://doi.org/10.1016/j.neucom.2025.131779'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Fully supervised medical image segmentation methods have demonstrated remarkable success in clinical applications. However, their performance heavily relies on large volumes of high-quality annotated data. The annotation of medical images is both labor-intensive and prohibitively expensive, significantly limiting the scalability and practical applicability of fully supervised approaches. To overcome this limitation, semi-supervised learning (SSL) techniques utilize abundant unlabeled data together with a limited quantity of labeled data, thereby reducing dependence on annotations. However, current SSL-based segmentation methods face major challenges, including noisy pseudo-labels and insufficient feature space supervision, which ultimately degrade segmentation accuracy. To address these challenges, we propose a novel semi-supervised segmentation method, Low Coupling and High Interaction Dual-Branch Contrastive Pseudo Supervision (DBCPS). We design a low-coupling, high-interaction dual-branch architecture combined with a cross-consistency pseudo supervision strategy to mitigate pseudo-label noise and improve their reliability. To enhance feature representation, we incorporate a multi-scale attention aggregation module that integrates spatial and channel attention to effectively fuse contextual features across scales. Deep supervision is also applied at multiple depths to facilitate hierarchical learning. In addition, we propose a novel voxel-level boundary perception contrastive loss that improves inter-class separability, enhances intra-class compactness, and refines boundary delineation. Experiments on four public medical image datasets show that the proposed method outperforms state-of-the-art semi-supervised segmentation approaches, demonstrating its effectiveness. Future work may explore its adaptation to multi-modal data or integration with active learning strategies. The source code is available at https://github.com/UncleTom09/DBCPS.git .},
  archive      = {J_NEUCOM},
  author       = {Changlong Yu and Yunfeng Zhang and Rui Zhang and Fangxun Bao and Huijian Han},
  doi          = {10.1016/j.neucom.2025.131779},
  journal      = {Neurocomputing},
  month        = {1},
  pages        = {131779},
  shortjournal = {Neurocomputing},
  title        = {Low coupling and high interaction dual-branch contrastive pseudo supervision for semi-supervised medical image segmentation},
  volume       = {659},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). YOLO-FOD: Lightweight object detection based on multibranch and multiscale feature fusion for adverse weather. <em>NEUCOM</em>, <em>659</em>, 131778. (<a href='https://doi.org/10.1016/j.neucom.2025.131778'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Object detection plays a critical role in enhancing traffic safety and operational efficiency. However, existing methods often degrade in performance when confronted with low-quality images captured under adverse weather conditions (e.g., fog, snow, and rain), due to reduced visibility and feature ambiguity. To address these challenges, this paper proposes a lightweight real-time detection model, You Only Look Once for Foggy Object Detection (YOLO-FOD), specifically designed for road traffic applications in harsh environments. Architecturally, we introduce the Online Scaling Diverse Branch Block with Efficient Layer Aggregation Network (OSDBBELAN) module, which enhances feature extraction while minimizing computational cost. To mitigate information loss during downsampling in complex scenes, we design the Space-to-Depth with Depthwise Separable Convolution (SPD-DSC) structure. Furthermore, a novel Global Information Sharing Module with Depthwise Separable Convolution (GISM-DSC) is developed for the neck to strengthen multiscale feature fusion. In addition, we propose the F-EIoU loss function to improve bounding box regression and accelerate convergence under adverse weather conditions. Extensive experiments demonstrate that YOLO-FOD maintains its lightweight design while outperforming state-of-the-art lightweight detectors. Compared with the baseline model, YOLO-FOD achieves a 40.5 % reduction in GFLOPs and a 4.1 % increase in mAP 50 on the RTTS dataset. The proposed model also delivers notable accuracy gains across multiple challenging environmental datasets, providing an effective and efficient solution for real-time object detection in traffic management systems under severe weather conditions.},
  archive      = {J_NEUCOM},
  author       = {Yan Liu and Tongyang Yuan and Aifeng Ren and Youyu Kuo and Xiangrui Xiong},
  doi          = {10.1016/j.neucom.2025.131778},
  journal      = {Neurocomputing},
  month        = {1},
  pages        = {131778},
  shortjournal = {Neurocomputing},
  title        = {YOLO-FOD: Lightweight object detection based on multibranch and multiscale feature fusion for adverse weather},
  volume       = {659},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). DiM: Improving multivariate time series forecasting with DI embedding and multi-head graph learning mechanism. <em>NEUCOM</em>, <em>659</em>, 131777. (<a href='https://doi.org/10.1016/j.neucom.2025.131777'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Accurate multivariate time series (MTS) forecasting is crucial for applications in traffic planning, energy management, financial investment, and healthcare. The challenge of forecasting MTS lies in managing the complex temporal dynamics and inter-channel relationships. However, despite the progress achieved by previous studies, they still fall short of adeptly addressing these complexities, leaving substantial scope for further refinement. To fill this gap, this paper proposes DiM, which seamlessly integrates the difference-inverted (DI) embedding strategy and the multi-head graph learning mechanism within the Metaformer framework. Specifically, the DI embedding employs a straightforward differencing operation to compensate for the limitations of previous inverted embedding methods in capturing temporal dynamics, thereby enhancing the model’s ability to discern temporal patterns without significantly increasing computational complexity. The multi-head graph learning mechanism dynamically adjusts multiple graph structures to better represent the evolving relationships between channels, surpassing the constraints of static graph structures typically used in GNNs. The effectiveness of the DiM has been validated across eleven public datasets, where it achieved the best results in 53 out of 55 mean absolute error metrics compared to existing state-of-the-art models, establishing a new benchmark in MTS forecasting. Code is available at https://github.com/Yipengmo/DiM .},
  archive      = {J_NEUCOM},
  author       = {Yipeng Mo and Haoxin Wang and Zuhua Yao and Chengteng Yang and Bixiong Li and Yiping Jiang and Songhai Fan and Site Mo},
  doi          = {10.1016/j.neucom.2025.131777},
  journal      = {Neurocomputing},
  month        = {1},
  pages        = {131777},
  shortjournal = {Neurocomputing},
  title        = {DiM: Improving multivariate time series forecasting with DI embedding and multi-head graph learning mechanism},
  volume       = {659},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). A lightweight dual path kolmogorov-arnold convolution network for medical optical image segmentation. <em>NEUCOM</em>, <em>659</em>, 131776. (<a href='https://doi.org/10.1016/j.neucom.2025.131776'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Medical optical image segmentation (MOIS) significantly aids physicians in diagnosing and monitoring diseases by providing valuable information about the necessary anatomical regions. However, MOIS frequently incurs high computational costs because of its complex morphology, rich edge information, and high resolution. To this end, we propose a lightweight dual-path architecture U-Net called KAU-Net. The newly proposed network achieves reuse and re-exploration of historical information through information interaction between image feature paths and historical information paths, enabling deep layers to learn comprehensive features. To satisfy the lightweight requirements of MOIS, Kolmogorov-Arnold convolution (KAConv) is used to replace traditional convolution, thereby obtaining more information while greatly reducing computational costs. Based on KAConv, we further propose a dual-path information fusion (DPIF) module to improve segmentation accuracy. This module establishes information fusion mechanisms across paths, network layers, and stages, enabling KAU-Net to fully utilize and re-explore historical information. To further enhance the information extracted by the dual-path encoder, a multi-scale information fusion (MSIF) module is designed, effectively connecting the encoder and decoder. Compared to the state-of-the-art approach, KAU-Net obtains IoU metrics of 85.32 %, 85.02 %, 82.87 %, and 63.15 % on the four tasks and achieves Dice scores of 93.84 %, 91.25 %, 90.12 %, and 85.10 %, respectively. Moreover, only 15.15 M parameters, 1.72 G FLOPs and 504 M memory are required. The proposed method has the advantages of being lightweight, versatile, and highly accurate in segmentation. Our study provides a reference solution for the lightweighting of MOIS models.},
  archive      = {J_NEUCOM},
  author       = {Jun Yuan and Lijun Zhou and Meirong He and Changyu Luo and Junran Zhang},
  doi          = {10.1016/j.neucom.2025.131776},
  journal      = {Neurocomputing},
  month        = {1},
  pages        = {131776},
  shortjournal = {Neurocomputing},
  title        = {A lightweight dual path kolmogorov-arnold convolution network for medical optical image segmentation},
  volume       = {659},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Centipede optimization algorithm: Best-of-L local search with contractive consensus for constrained engineering optimization. <em>NEUCOM</em>, <em>659</em>, 131774. (<a href='https://doi.org/10.1016/j.neucom.2025.131774'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present the Centipede Optimization Algorithm (CeOA), an operator-scheduled metaheuristic that composes a per-agent best-of-L Gaussian local search with an explicit contractive consensus under box clipping for constrained engineering optimization. CeOA pairs two generic operators in a distinct per-iteration schedule: (i) a multi-proposal, best-of-L Gaussian local search per agent; and (ii) a contractive consensus step that averages each agent with the current elite under bound clipping. A brief first-order rationale (Appendix A) explains why best-of-L proposals raise expected improvement and why the consensus map is non-expansive, helping stabilize feasibility with death-penalty constraint handling. We evaluate CeOA on six standard design benchmarks (spring, welded beam, pressure vessel, cantilever beam, three-bar truss, tubular column) and on 10 representative CEC-2017 functions spanning unimodal, multimodal, hybrid, and composition families, under equal evaluation budgets. We also include four conventional evolutionary baselines GA, DE, PSO and ES. Statistical significance is assessed via paired Wilcoxon tests with Holm correction. Compared with established methods (GWO, SSA) and recent heuristics (SMA, GBO), CeOA attains higher solution quality and faster convergence, with robustness confirmed by statistical tests. The operator-level design multi-proposal search followed by a contractive consensus balances exploitation with steady progress under constraints and requires few hyperparameters. These capabilities position CeOA as a practical tool for constrained design. Future work will extend CeOA to multi-objective and dynamic settings and evaluate its scalability on higher-dimensional benchmarks, or explore its hybridization with other computational intelligence methods.},
  archive      = {J_NEUCOM},
  author       = {Mohammad Mohammadi and Yousef Bazargan Lari and Kimia Bazargan Lari},
  doi          = {10.1016/j.neucom.2025.131774},
  journal      = {Neurocomputing},
  month        = {1},
  pages        = {131774},
  shortjournal = {Neurocomputing},
  title        = {Centipede optimization algorithm: Best-of-L local search with contractive consensus for constrained engineering optimization},
  volume       = {659},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). MSER: Multi-scale event representation model for enhanced spatio-temporal feature extraction. <em>NEUCOM</em>, <em>659</em>, 131773. (<a href='https://doi.org/10.1016/j.neucom.2025.131773'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Event cameras, designed to mimic biological perception principles, are a new type of visual sensor offering advantages such as low latency, low power consumption, and high dynamic range. However, the non-uniform, discontinuous nature of their spatially sparse yet temporally dense data presents significant challenges for extracting meaningful spatio-temporal features. Representing such novel data paradigms to provide high-quality inputs for neural network models remains a formidable task. Consequently, a key consideration is how to extract spatio-temporal features from event streams effectively. Currently, most work on event data typically performs feature extraction at either a single temporal or spatial scale, which often struggles to fully utilize the complex spatio-temporal relationships, leading to inaccurate capture of dynamic changes. Inspired by the brain’s multi-scale processing mechanisms, we propose the Multi-scale Event Representation (MSER) model to enable a comprehensive understanding of spatio-temporal information. Among these, Temporal Multi-scale Encoding (TMSE) leverages a logarithmic perception algorithm with an adaptive decay factor to capture temporal features across multiple scales, effectively establishing temporal correlations. Then Spatial Multi-scale Fusion strategy (SMSF) is designed to address spatial variations by employing a feature pyramid structure with a bottom-up strategy to extract and integrate multi-scale spatial features. We validate the proposed method on a series of publicly available neuromorphic datasets, including N-CARS, N-MNIST, N-Caltech101, CIFAR10-DVS, and DvsGesture. Experimental results demonstrate that the features we extract exhibit excellent expressiveness and generalization, achieving state-of-the-art performance.},
  archive      = {J_NEUCOM},
  author       = {Wanying Xu and Ping He and Xin Zhou and Rong Xiao and Chenwei Tang and Jiancheng Lv and Huajin Tang},
  doi          = {10.1016/j.neucom.2025.131773},
  journal      = {Neurocomputing},
  month        = {1},
  pages        = {131773},
  shortjournal = {Neurocomputing},
  title        = {MSER: Multi-scale event representation model for enhanced spatio-temporal feature extraction},
  volume       = {659},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Importance estimation of hyperparameters in reinforcement learning. <em>NEUCOM</em>, <em>659</em>, 131770. (<a href='https://doi.org/10.1016/j.neucom.2025.131770'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Hyperparameters play a critical role in enabling reinforcement learning (RL) agents to achieve high performance, yet their optimization remains computationally demanding. To address this challenge, we evaluate methodologies from the field of sensitivity analysis (SA) for assessing hyperparameter importance, thereby enabling more informed resource allocation through (I) hyperparameter prioritization, (II) hyperparameter fixation, and (III) hyperparameter mapping across the value space. Following a theoretical analysis of RL-specific characteristics and available methodologies from the field of SA, we identify functional ANOVA (fANOVA) as the most promising candidate. Our empirical investigation evaluates the validity, reliability, sample efficiency, and usability of fANOVA within the RL context. The results demonstrate its overall effectiveness in achieving goals I–III, while also highlighting limitations related to the sample efficiency and especially regarding the usage of data generated during hyperparameter optimization.},
  archive      = {J_NEUCOM},
  author       = {Dominic Weller and Maximilian Moll},
  doi          = {10.1016/j.neucom.2025.131770},
  journal      = {Neurocomputing},
  month        = {1},
  pages        = {131770},
  shortjournal = {Neurocomputing},
  title        = {Importance estimation of hyperparameters in reinforcement learning},
  volume       = {659},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). An in-depth look at approximation via deep and narrow neural networks. <em>NEUCOM</em>, <em>659</em>, 131769. (<a href='https://doi.org/10.1016/j.neucom.2025.131769'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In 2017, Hanin and Sellke showed that the class of arbitrarily deep, real-valued, feed-forward and ReLU-activated networks of width w forms a dense subset of the space of continuous functions on R n , with respect to the topology of uniform convergence on compact sets, if and only if w > n holds. To show the necessity, a concrete counterexample function f : R n → R was used. In this note we actually approximate this very f by neural networks in the two cases w = n and w = n + 1 around the aforementioned threshold. We study how the approximation quality behaves if we vary the depth and what effects (spoiler alert: dying neurons) cause that behavior.},
  archive      = {J_NEUCOM},
  author       = {Joris Dommel and Sven A. Wegner},
  doi          = {10.1016/j.neucom.2025.131769},
  journal      = {Neurocomputing},
  month        = {1},
  pages        = {131769},
  shortjournal = {Neurocomputing},
  title        = {An in-depth look at approximation via deep and narrow neural networks},
  volume       = {659},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Projection-based incomplete multi-view consensus bipartite graph representation learning. <em>NEUCOM</em>, <em>659</em>, 131766. (<a href='https://doi.org/10.1016/j.neucom.2025.131766'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Incomplete Multi-View Clustering (IMVC) has garnered significant attention in recent years due to its ability to effectively handle incomplete multi-view data. Although graph-based IMVC methods are widely adopted, the reliability of the obtained affinity graphs is often compromised by the presence of missing instances. Moreover, imputation-based methods are frequently adversely affected by redundant features or noise, which can distort data relationships and structures, ultimately leading to inaccurate clustering results and limiting their practical applicability. To address these challenges, we propose a novel method termed Projection-based Incomplete Multi-view Consensus Bipartite Graph Representation Learning (PIMV_CBG). Specifically, we integrate projection learning with consensus bipartite graph construction into a unified framework, where they mutually enhance each other to reduce the impact of noise and redundancy while strengthening cross-view interactions, leading to high-quality bipartite graph representations. Furthermore, imputation is performed in the clean subspace, regularized by graph constraints derived from the consensus structure, which improves imputation accuracy and fully exploits latent information. Extensive experiments on benchmark datasets have demonstrated the effectiveness of PIMV_CBG in addressing IMVC tasks, achieving state-of-the-art clustering performance in most cases. The source code is publicly available at https://github.com/superkeranbing/PIMV_CBG/tree/main .},
  archive      = {J_NEUCOM},
  author       = {Qiuyu Ji and Hui Huang and Liang Mao and Nan Zhang},
  doi          = {10.1016/j.neucom.2025.131766},
  journal      = {Neurocomputing},
  month        = {1},
  pages        = {131766},
  shortjournal = {Neurocomputing},
  title        = {Projection-based incomplete multi-view consensus bipartite graph representation learning},
  volume       = {659},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). A sparse dynamic graph transformer for traffic flow prediction. <em>NEUCOM</em>, <em>659</em>, 131765. (<a href='https://doi.org/10.1016/j.neucom.2025.131765'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Traffic flow prediction is a crucial component of Intelligent Transportation Systems (ITS), aimed at optimizing traffic management and enhancing urban mobility. The main challenges in traffic flow prediction are accurately modeling both long-term stability and short-term dynamic spatial-temporal dependencies in traffic data. Graph Neural Networks (GNNs) and attention mechanisms have demonstrated significant potential in addressing these challenges. However, most GNN-based models capture spatial correlations in a static manner, limiting their ability to represent the dynamic characteristics of urban traffic patterns. Moreover, the self-attention mechanism, which considers the entire data range, may cause oversmoothing. To this end, we propose a Sparse Dynamic Graph Transformer (SDGFormer) model for more accurate traffic flow prediction. By incorporating long-term historical data, the model better captures periodic patterns in traffic flow. A sparse dynamic adaptive graph convolution module is appropriately designed, which adopts an adaptive adjacency matrix to capture the long-term stable traffic structures and a dynamic adjacency matrix to model the short-term dynamic spatial dependencies, respectively. This module applies top- k selection operators with varying sparsity to retain the most critical nodes for more effective feature aggregation. Additionally, we propose a frequency-decoupled temporal attention module that employs Fourier transforms to decompose complex temporal components of traffic flow series, integrating low and high-frequency information to enhance sensitivity to short-term events and mitigate oversmoothing. Experiments on seven public datasets demonstrate that SDGFormer achieves state-of-the-art performance, especially the Mean Absolute Percentage Error (MAPE) value of the PeMS07 dataset experiment is improved by 9.7 %. The code is available at https://github.com/userTGQ/SDGFormer .},
  archive      = {J_NEUCOM},
  author       = {Guoqing Teng and Han Wu and Ao He and Yu Chen and Meng Zhao},
  doi          = {10.1016/j.neucom.2025.131765},
  journal      = {Neurocomputing},
  month        = {1},
  pages        = {131765},
  shortjournal = {Neurocomputing},
  title        = {A sparse dynamic graph transformer for traffic flow prediction},
  volume       = {659},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). EHIN: Early-aware hierarchical interaction network for weakly-supervised referring image segmentation. <em>NEUCOM</em>, <em>659</em>, 131764. (<a href='https://doi.org/10.1016/j.neucom.2025.131764'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Referring image segmentation (RIS) aims to segment target regions in images based on natural language descriptions. Although weakly-supervised RIS frameworks have been proposed to reduce reliance on costly manual annotations, their performance remains limited due to both the low quality of pseudo-labels and the inherent difficulty in achieving effective interaction between visual and textual features. In this paper, we propose a novel weakly-supervised framework named Early-aware Hierarchical Interaction Network (EHIN). The proposed network includes two key components, which are designed to enhance pseudo-label generation and improve the interaction between visual and textual features for RIS, respectively. First, EHIN incorporates an Early-aware Contrastive Learning Module (ECLM) that enhances feature discrimination by leveraging contrastive learning to distinguish target features from background noise. By integrating the module early into the processing pipeline, ECLM operates on raw image features directly, preserving richer visual details while reducing reliance on labeled data and thus improving the reliability of pseudo-labels. Second, EHIN integrates a Hierarchical Interaction Prompt Module (HIPM) to facilitate comprehensive interaction between visual and textual features and enhance subsequent feature fusion. Extensive experimental results on four benchmark datasets demonstrate that the proposed EHIN outperforms the state-of-the-art RIS. Code is available at https://github.com/CDUT-DBGroup/MFP-TRIS .},
  archive      = {J_NEUCOM},
  author       = {Hongjun Li and Nan Wang and Anqing Chen and Jiang Liu and Wanli Ma and Weide Liu and Yakun Ju and Paul L. Rosin and Hantao Liu and Wei Zhou},
  doi          = {10.1016/j.neucom.2025.131764},
  journal      = {Neurocomputing},
  month        = {1},
  pages        = {131764},
  shortjournal = {Neurocomputing},
  title        = {EHIN: Early-aware hierarchical interaction network for weakly-supervised referring image segmentation},
  volume       = {659},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). A semantic driven adaptive framework for few-shot knowledge graph completion. <em>NEUCOM</em>, <em>659</em>, 131763. (<a href='https://doi.org/10.1016/j.neucom.2025.131763'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Few-shot Knowledge Graph Completion (FKGC) aims to infer missing triples under unseen relations with minimal reference data. Existing methods predominantly rely on structural patterns while overlooking semantic context, leading to suboptimal performance on sparse or ambiguous entities. In this paper, we propose Semantic Driven Adaptive Framework (SeDA), a novel FKGC model that introduces dynamic semantic-structural synergy. SeDA first extracts semantic information by dynamically selecting diverse neighborhood triples and generating entity descriptions via large language models. The entity descriptions are encoded with BERT and fused with local structural features through a graph attention network (GAT), which performs adaptive aggregation by weighting neighboring entities and relations based on task-specific attention scores. SeDA introduces a semantic-driven negative sampling strategy that categorizes relations into exclusive and inclusive types, thereby enhancing discriminative learning through the dynamic interplay between relation semantics and structural patterns. The framework effectively bridges semantic gaps, offering a scalable solution for FKGC tasks in sparse real-world knowledge graphs. Experimental results on the frequently used benchmark datasets NELL-One and FB15k237-One demonstrate that SeDA significantly outperforms existing methods.},
  archive      = {J_NEUCOM},
  author       = {ChengJia OuYang and Tinghua Zhang and Weihao Yu and Jin Huang},
  doi          = {10.1016/j.neucom.2025.131763},
  journal      = {Neurocomputing},
  month        = {1},
  pages        = {131763},
  shortjournal = {Neurocomputing},
  title        = {A semantic driven adaptive framework for few-shot knowledge graph completion},
  volume       = {659},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). New intent discovery with syntactic structure masking pretraining and density-aware contrastive learning. <em>NEUCOM</em>, <em>659</em>, 131760. (<a href='https://doi.org/10.1016/j.neucom.2025.131760'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {New intent discovery remains a critical challenge in natural language understanding. Existing methods fail to adequately incorporate the syntactic characteristics of intent data, leading to insufficient capture of deep semantic features and imbalanced sentence embedding distributions. In these distributions, embeddings of similar intents tend to cluster closely, thereby reducing discovery accuracy. This paper proposes SSM-IntentBERT (Syntax Structure Masking strategy Intent Bidirectional Encoder Representations from Transformers) to address these limitations. SSM-IntentBERT is a novel intent discovery model that leverages syntax-aware masking pretraining. Capitalizing on the interrogative-dominated nature of intent data, we design a syntax structure masking strategy. This strategy selectively masks specific words during pretraining to learn sentence representations that encode both semantic and syntactic features. Additionally, we introduce density-aware contrastive learning to maximize the similarity between positive sample pairs and the differences between negative pairs, thereby adjusting the distribution of sentence embeddings to prevent the clustering of semantically similar intents and improve discovery accuracy. Experimental results on four datasets, including Banking (financial transactions), Stackoverflow (community Q&A), Mcid (COVID-19 healthcare), and HWU64 (21 domains), demonstrate that SSM-IntentBERT outperforms baseline models (DCN, SCCL, USNID, MTP-CLNN) with average improvements of 2.43 % in Adjusted Rand Index (ARI), 2.10 % in Accuracy (ACC), and 1.61 % in Normalized Mutual Information (NMI). The code for this research work is available at: https://github.com/lllforeverlll/SSM-IntentBERT .},
  archive      = {J_NEUCOM},
  author       = {Di Wu and Liming Feng and Xiaoyu Wang},
  doi          = {10.1016/j.neucom.2025.131760},
  journal      = {Neurocomputing},
  month        = {1},
  pages        = {131760},
  shortjournal = {Neurocomputing},
  title        = {New intent discovery with syntactic structure masking pretraining and density-aware contrastive learning},
  volume       = {659},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). SynapseHD: A unified training framework for bridging spiking neural networks and hyperdimensional computing. <em>NEUCOM</em>, <em>659</em>, 131757. (<a href='https://doi.org/10.1016/j.neucom.2025.131757'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Spiking Neural Networks (SNNs) and Hyperdimensional Computing (HDC) are two prominent brain-inspired computing paradigms with complementary characteristics. SNNs effectively simulate biological neural mechanisms to achieve powerful feature extraction but typically rely on complex, gradient-based training algorithms. Conversely, HDC models the brain at a more abstract, functional level, enabling gradient-free training at the cost of less sophisticated feature extraction capabilities. The potential for a synergistic integration of these two approaches remains largely unexplored. To bridge this gap, we introduce SynapseHD, a novel deep learning framework that harmonizes the strengths of SNNs and HDC. In our architecture, an SNN module first extracts low-level features from the spatio-temporal correlations within data. These features are then mapped into a high-dimensional space by an HDC module to establish memory representations for classification. A core contribution of our work is a novel unbalanced co-training strategy that orchestrates the learning paradigm, temporal scheduling, loss transmission, and weight updating to ensure efficient and stable joint training. Experimental results demonstrate the superiority of our proposed framework. First, on the CIFAR-10 dataset, SynapseHD reduces training epochs by 30 % and improves accuracy by a remarkable 40 % compared to state-of-the-art HDC works. Furthermore, when applied to the more challenging task of few-shot class-incremental learning, SynapseHD surpasses existing methods by delivering higher accuracy with significantly fewer parameters and lower computational complexity, while maintaining a gradient-free process.},
  archive      = {J_NEUCOM},
  author       = {Lingfeng Zhou and Huiyao Wang and Bohan Wang and Jinghai Wang and Zhiyi Yu and Shanlin Xiao},
  doi          = {10.1016/j.neucom.2025.131757},
  journal      = {Neurocomputing},
  month        = {1},
  pages        = {131757},
  shortjournal = {Neurocomputing},
  title        = {SynapseHD: A unified training framework for bridging spiking neural networks and hyperdimensional computing},
  volume       = {659},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). ConDNS: A novel conditional diffusion-based negative sampling method for knowledge graph embedding. <em>NEUCOM</em>, <em>659</em>, 131751. (<a href='https://doi.org/10.1016/j.neucom.2025.131751'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Knowledge Graph Embedding (KGE) maps entities and relations into continuous vector spaces to facilitate link prediction tasks. Given the inherent inability of knowledge graphs to directly supply high-quality negative samples with multi-level difficulty, existing methods typically rely on post-sampling assessment strategies, which lack controllable generation of difficulty-calibrated negatives tailored to diverse KGE training requirements. To address these challenges, we propose ConDNS , a novel conditional diffusion-based negative sampling method for knowledge graph embedding. By adjusting the diffusion timestep, our model achieves dynamic difficulty modulation of synthetic negatives through global entity-relation information utilization. This enables generation of semantically valid samples that synergistically integrate with conventional samples, thereby overcoming single-strategy sampling bottlenecks and establishing a multiscale difficulty configuration. Experiments demonstrate that ConDNS achieves state-of-the-art performance across multiple benchmarks with minimal synthetic samples while functioning as a plug-and-play module compatible with mainstream KGE architectures. Source code is available at: https://github.com/zrj-wang/ConDNS .},
  archive      = {J_NEUCOM},
  author       = {Zhaorongjie Wang and Nan Li and Kai Chen and Aiping Li and Liqun Gao},
  doi          = {10.1016/j.neucom.2025.131751},
  journal      = {Neurocomputing},
  month        = {1},
  pages        = {131751},
  shortjournal = {Neurocomputing},
  title        = {ConDNS: A novel conditional diffusion-based negative sampling method for knowledge graph embedding},
  volume       = {659},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Sparse-view CT image reconstruction using conditional embedding fusion diffusion model. <em>NEUCOM</em>, <em>659</em>, 131748. (<a href='https://doi.org/10.1016/j.neucom.2025.131748'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The main goal of sparse-view CT reconstruction is to reconstruct high-quality images from fewer projection data, thereby reducing the radiation dose. However, the reduction in the amount of projection data also makes the reconstruction process ill-posed, leading to an increase in artifacts in the reconstructed image. Recently, diffusion models have shown strong generative capabilities, providing new ideas for sparse-view CT reconstruction. But due to the stochastic nature of the diffusion process, how to guide the generation process to achieve high-quality reconstruction remains a major challenge. To address this issue, we propose a Conditional Embedding Fusion Diffusion Model (CEF-DM) to improve reconstruction quality. Specifically, we design a FourierNet to generate an initial reconstruction, which serves as a condition to guide the CEF-DM in generating the remaining detail residuals. CEF-DM employs a conditional attention embedding module (CAEM) to comprehensively incorporate conditional input and time-step information throughout the generation process. The initial reconstruction is then summed with the residual details to obtain the final reconstruction. The experimental results show that our method outperforms existing methods in terms of reconstruction accuracy and image quality, providing an efficient and robust solution for sparse view CT reconstruction with potential clinical application value.},
  archive      = {J_NEUCOM},
  author       = {Chenchun Zhou and Yubao Sun and Jing Liang and Jia Liu and Qingshan Liu},
  doi          = {10.1016/j.neucom.2025.131748},
  journal      = {Neurocomputing},
  month        = {1},
  pages        = {131748},
  shortjournal = {Neurocomputing},
  title        = {Sparse-view CT image reconstruction using conditional embedding fusion diffusion model},
  volume       = {659},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Towards trustworthy and interpretable prediction of school bullying: A NAS-driven framework with shapley value explanation. <em>NEUCOM</em>, <em>659</em>, 131744. (<a href='https://doi.org/10.1016/j.neucom.2025.131744'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {School bullying represents a critical societal challenge with lasting psychological and academic consequences for affected students. Despite recent advances in machine learning for predicting bullying behaviors, conventional models struggle to capture the complex, non-linear relationships among contributing factors, especially under the imbalanced data distributions typical of real-world bullying cases. Furthermore, the inherent opacity of Deep Neural Networks (DNNs) restricts their application in educational contexts where interpretability and actionable insights are essential. In this paper, we propose a novel automated framework that integrates Neural Architecture Search (NAS) with Shapley value-based explanation methods to jointly address performance and interpretability challenges. Our framework automatically identifies optimal DNN architectures tailored for bullying prediction, incorporating mechanisms to handle class imbalance without extensive manual tuning. To address model transparency, we employ a Shapley value analysis pipeline that systematically attributes predictions to key risk factors, offering educators and policymakers principled and quantitative insights. Extensive experiments on publicly available datasets demonstrate that our method significantly outperforms state-of-the-art baselines, achieving notable improvements in Accuracy (+2.58 %), F1-Score (+34.52 %), and AUC (+6.47 %). Importantly, the feature importance rankings from our Shapley analysis closely align with established sociological and educational theories on bullying, affirming the model’s interpretability and practical relevance. Cross-dataset validation further verifies the framework’s generalizability to broader youth behavioral risk prediction tasks. Our code is submitted at https://github.com/clsyc/Bullyingshapley .},
  archive      = {J_NEUCOM},
  author       = {Wei Qu and Cong Chen and Wei Lu and Haodong Chen and Tao Li},
  doi          = {10.1016/j.neucom.2025.131744},
  journal      = {Neurocomputing},
  month        = {1},
  pages        = {131744},
  shortjournal = {Neurocomputing},
  title        = {Towards trustworthy and interpretable prediction of school bullying: A NAS-driven framework with shapley value explanation},
  volume       = {659},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). SVP: Stratified vertical priors for LiDAR-based 3D object detection. <em>NEUCOM</em>, <em>659</em>, 131737. (<a href='https://doi.org/10.1016/j.neucom.2025.131737'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Three-dimensional (3D) object detection is a fundamental perceptual task in autonomous driving, attracting significant attention from both academia and industry. Recent pillar-based 3D object detection algorithms have made notable progress in reducing model inference time. However, the detection performance remains unsatisfactory due to the loss of vertical dimension information. In this paper, we propose a stratified vertical priors framework to improve the pillar-based 3D object detection algorithms. Specifically, we first extract the stratified vertical information and horizontal information of the object from the point cloud. Subsequently, we design a parallel feature extraction architecture to extract the vertical features and horizontal features, respectively. A shared information fusion module is designed to extract and fuse the key features of the vertical and horizontal feature maps. Finally, we fuse vertical features into horizontal features at multiple scales to enhance the representation of horizontal features. Extensive experiments on KITTI and nuScenes datasets demonstrate that our proposed algorithm significantly improves the performance of pillar-based 3D object detection algorithms. Remarkably, it improves PointPillars by 3.45 % and 3.83 % mAP on the KITTI validation and test datasets, respectively. Code is available at https://github.com/1064783536/SVP .},
  archive      = {J_NEUCOM},
  author       = {Lei Ao and Wenkang Wan and Nan Ouyang and Jianzhao Li and Qingqing Li and Maoguo Gong and Kai Sheng},
  doi          = {10.1016/j.neucom.2025.131737},
  journal      = {Neurocomputing},
  month        = {1},
  pages        = {131737},
  shortjournal = {Neurocomputing},
  title        = {SVP: Stratified vertical priors for LiDAR-based 3D object detection},
  volume       = {659},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Facial sketch synthesis with multi-level guided latent diffusion model. <em>NEUCOM</em>, <em>659</em>, 131734. (<a href='https://doi.org/10.1016/j.neucom.2025.131734'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a facial sketch synthesis (FSS) model to generate multi-style facial sketches from photos. Recent FSS methods often rely on Generative Adversarial Networks (GANs) due to their excellent performance in generating realistic images. However, many of these methods are limited to single-style sketches, and some methods have weaknesses in abstract expression of content according to the sketch style. The great difference between photos and sketches, along with limited training samples, makes it challenging to ensure high fidelity to both content and style. Diffusion models (DMs) not only generate high-quality images comparable to those produced by GANs but also offer superior mode coverage. Inspired by this, we propose a Multi-Level Guided Latent Diffusion Model (MLGLDM). We first train a photo autoencoder and a sketch autoencoder to encode photos into the latent space and decode sketches from the latent space, respectively. A diffusion model is then applied in the learned latent space. To improve the fidelity of synthesized sketches, we introduce a Multi-Level Guidance (MLG) mechanism, which enhances the guiding effect of photos by incorporating a Conditional Information Enhancement (CIE) module into the U-Net within the latent diffusion model. Additionally, we propose a region-adaptive loss function to synthesize more precise and detailed sketches by focusing on the more challenging regions. Furthermore, we adopt a three-stage training strategy to address the challenge of limited training samples. Extensive experiments on the FS2K dataset demonstrate that our method achieves comparable or better performance beyond the state-of-the-art methods both qualitatively and quantitatively.},
  archive      = {J_NEUCOM},
  author       = {Dan Lu and Zhenxue Chen and Chengyun Liu and Yuchen Hu and Q.M.Jonathan Wu},
  doi          = {10.1016/j.neucom.2025.131734},
  journal      = {Neurocomputing},
  month        = {1},
  pages        = {131734},
  shortjournal = {Neurocomputing},
  title        = {Facial sketch synthesis with multi-level guided latent diffusion model},
  volume       = {659},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Convex space learning for tabular synthetic data generation. <em>NEUCOM</em>, <em>659</em>, 131722. (<a href='https://doi.org/10.1016/j.neucom.2025.131722'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {While synthetic data for images and text has seen significant advancements, the structured nature of tabular data presents unique challenges in generating high-utility synthetic datasets while preserving privacy. In imbalanced learning, the Convex Space Learning (CSL) approach has been widely adopted, but it has not been explored for synthetic tabular data generation in general. Moreover, deriving convex spaces (or convex hulls) for individual rows typically relies on a single distance metric; however, applying a single metric directly across data with diverse feature types limits generation quality. To address these two research gaps, we propose NextConvGeN, an extension of the ConvGeN framework that generalizes CSL for entire tabular data generation. NextConvGeN employs a generator-discriminator architecture that uses deep cooperative learning, refining synthetic data generation within local convex data neighborhoods. These neighborhoods are accessed by a nonlinear dimension reduction technique with a feature-type specific similarity metric to handle diverse feature types. We then compare several state-of-the-art synthetic tabular data generation models to assess their performances qualitatively and quantitatively in the context of privacy-utility balance. Our results show that NextConvGeN prioritizes utility preservation while incorporating privacy measures, making it a promising tool for generating high-utility synthetic data for analysis. This work advances synthetic tabular data generation by expanding convex space learning beyond imbalanced classification, strengthening the theoretical foundation of synthetic data sampling, and providing a structured evaluation of utility-driven tabular data generation, especially in the biomedical domain.},
  archive      = {J_NEUCOM},
  author       = {Manjunath Mahendra and Chaithra Umesh and Kristian Schultz and Olaf Wolkenhauer and Saptarshi Bej},
  doi          = {10.1016/j.neucom.2025.131722},
  journal      = {Neurocomputing},
  month        = {1},
  pages        = {131722},
  shortjournal = {Neurocomputing},
  title        = {Convex space learning for tabular synthetic data generation},
  volume       = {659},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). PM-adapter: MoE based dynamic denoising fine-tuning for thermal infrared object detection. <em>NEUCOM</em>, <em>659</em>, 131718. (<a href='https://doi.org/10.1016/j.neucom.2025.131718'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Thermal infrared (TIR) technology has become an indispensable component in the community of image acquisition and processing. While TIR images contain rich thermal information, they lack texture details, rendering conventional object detectors based on visible imagery less effective under low-light conditions. Additionally, noise introduced by sensor limitations and environmental factors further degrades detection performance, increasing false positives and false negatives. To address these challenges, we propose a parameter-efficient dynamic denoising fine-tuning module named PM-Adapter ( P erona- M alik diffusion-based Adapter ). Specifically, we leverage the Perona-Malik diffusion equation in the form of discrete wavelet transform to denoise TIR features, and the edge enhancement unit to enhance edge details. More crucially, to adaptively handle the complex noise characteristics across TIR images, we introduce the Mixture of Experts mechanism to dynamically allocate the most appropriate denoising expert, further improving detection performance. Comprehensive experiments on the FLIR, LLVIP and M 3 FD datasets demonstrate that the proposed PM-Adapter consistently outperforms existing methods, validating its effectiveness of the proposed approach.},
  archive      = {J_NEUCOM},
  author       = {Haijun Liu and Jun Zhang and Hang Yu and Boya Wei and Jing Nie and Suju Li and Xichuan Zhou},
  doi          = {10.1016/j.neucom.2025.131718},
  journal      = {Neurocomputing},
  month        = {1},
  pages        = {131718},
  shortjournal = {Neurocomputing},
  title        = {PM-adapter: MoE based dynamic denoising fine-tuning for thermal infrared object detection},
  volume       = {659},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Enhanced 3D tumor synthesis and segmentation framework using multiscale diffusion and hybrid SAM-swin models across diverse anatomical datasets. <em>NEUCOM</em>, <em>659</em>, 131713. (<a href='https://doi.org/10.1016/j.neucom.2025.131713'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep learning-based tumor segmentation often focuses on optimizing network architectures in 3D medical images while overlooking the potential of anatomically realistic synthetic tumor data to enhance performance. This work proposes a novel framework integrating a multiscale attention diffusion model to synthesize high-fidelity tumors and a hybrid segmentation approach using the 3D SAM-Adapter and Swin Transformer. The diffusion model generates synthetic tumors that replicate real pathological features with accurate boundaries and textures across multiple resolutions. Our framework, evaluated on diverse datasets—including public brain and liver tumor datasets, a private pelvic dataset, and others—achieves state-of-the-art segmentation performance, with Dice Similarity Coefficients of 98.61 %, 88.60 %, and 91.93 % on BraTS, DLDS, and the pelvic dataset, respectively. Our method captures global anatomical context and fine-grained local details by leveraging prompt-driven segmentation and hierarchical refinement. This reduces burdens of manual annotations, improves accuracy in segmentation, and demonstrates good generalization performance across a diverse set of anatomical sites and imaging protocols.},
  archive      = {J_NEUCOM},
  author       = {Jincao Yao and Mudassar Ali and Wenjie Zheng and Jiaqi Hu and Jing Wang and Xingze Zou and Haoji Hu and Qiong Luo and Weizeng Zheng and Neng Jin and Dong Xu},
  doi          = {10.1016/j.neucom.2025.131713},
  journal      = {Neurocomputing},
  month        = {1},
  pages        = {131713},
  shortjournal = {Neurocomputing},
  title        = {Enhanced 3D tumor synthesis and segmentation framework using multiscale diffusion and hybrid SAM-swin models across diverse anatomical datasets},
  volume       = {659},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Enhancing neural combinatorial optimization by progressive training paradigm. <em>NEUCOM</em>, <em>659</em>, 131707. (<a href='https://doi.org/10.1016/j.neucom.2025.131707'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Neural Combinatorial Optimization (NCO) methods have garnered considerable attention, due to their effectiveness in automatic algorithm design for solving combinatorial optimization problems. Current constructive NCO methods predominantly employ a one-stage training paradigm using either reinforcement learning (RL) or supervised learning (SL). The one-stage training inevitably entails the computation-intensive labeling (i.e., solving optimal solutions) in SL or less-informative sparse rewards in RL. In this work, we propose a progressive training paradigm that pre-trains a neural network on small-scale instances using SL and then fine-tunes it using RL. In the former stage, the optimal solutions as labels effectively guide the neural network training, thereby bypassing the sparse reward issue. In the latter, the neural network is trained using RL to solve large-scale problems, avoiding the labels of optimal solutions that are hard to obtain. Moreover, we propose a decomposition-based approach that enables RL training with larger problem scales, alleviating the issue of insufficient memory induced by the heavy neural network. The proposed paradigm advances existing NCO models to obtain near-optimal solutions for the Traveling Salesman Problem (TSP) and Capacitated Vehicle Routing Problem (CVRP) with up to 10,000 nodes. Additionally, it enhances the generalization performance across instances of different sizes and distributions, as well as real-world TSPLib and CVRPLib instances.},
  archive      = {J_NEUCOM},
  author       = {Zhi Cao and Yaoxin Wu and Yaqing Hou and Hongwei Ge},
  doi          = {10.1016/j.neucom.2025.131707},
  journal      = {Neurocomputing},
  month        = {1},
  pages        = {131707},
  shortjournal = {Neurocomputing},
  title        = {Enhancing neural combinatorial optimization by progressive training paradigm},
  volume       = {659},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). FGVoxel3D: Fine-grained multi-resolution voxel network for 3D object detection. <em>NEUCOM</em>, <em>659</em>, 131702. (<a href='https://doi.org/10.1016/j.neucom.2025.131702'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Three-dimensional object detection from LiDAR point clouds is an indispensable component of perception systems for autonomous vehicles. Recent advances in voxel-based 3D object detection methods demonstrate rapid progress due to their efficient point cloud processing capabilities. However, existing research primarily focuses on single-voxel approaches with limited exploration of multi-voxel methods. To our knowledge, no published work investigates multi-voxel 3D object detection approaches based on voxels of arbitrary resolutions. This paper proposes FGVoxel3D, a novel Fine-Grained Voxel Network for 3D object detection. It specifically addresses multi-voxel detection at arbitrary resolutions. The network incorporates an Arbitrary Resolution Voxel (ARV) extraction module for fine-grained multi-resolution voxel extraction. Subsequently, a Sparse-to-Dense Multi-Voxel feature extraction network (SDMV) learns robust bird’s-eye view (BEV) features from these voxels. Furthermore, a feature map scale alignment strategy precisely harmonizes feature map dimensions. These core components collectively enable FGVoxel3D to capture and utilize richer scene details for enhanced detection. Extensive experiments on the authoritative KITTI and nuScenes datasets validate the effectiveness of our method. For instance, on the KITTI testing set, FGVoxel3D improves the Car mAP of the SECOND baseline by 3.52%, and on the nuScenes validation set, it surpasses the CenterPoint baseline by 2.0% in mAP and 1.8% in NDS. It also demonstrates robust gains in detecting partially occluded objects.},
  archive      = {J_NEUCOM},
  author       = {Lei Ao and Xingzheng Wang and Wenkang Wan and Nan Ouyang and Qingqing Li and Kai Sheng},
  doi          = {10.1016/j.neucom.2025.131702},
  journal      = {Neurocomputing},
  month        = {1},
  pages        = {131702},
  shortjournal = {Neurocomputing},
  title        = {FGVoxel3D: Fine-grained multi-resolution voxel network for 3D object detection},
  volume       = {659},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). A responsive approach to multivariate time-series anomaly detection with K-distance based calibrated reconstruction. <em>NEUCOM</em>, <em>659</em>, 131689. (<a href='https://doi.org/10.1016/j.neucom.2025.131689'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Anomaly detection is of great importance in the Industrial Internet of Things (IIoT) as it enables intelligent process control, analysis, and management, etc. There is a growing demand for responsive anomaly detection models that exhibit high sensitivity and precision. Various models have been proposed to address these challenges, with reconstruction-based models currently dominating the field. These models concentrate on learning complex representations of time-series; however, representation learning can be negatively affected by anomaly contamination and may experience varying degrees of detection delay. In this paper, we introduce a responsive approach for multivariate time-series anomaly detection. We design a new distance function that emphasizes the importance of the current timestamp in calculating anomaly scores to achieve more responsive detection. Additionally, we employ a calibration method that allows the model to focus exclusively on reconstructing normal patterns and an integrated prediction mechanism to enhance detection precision. Comprehensive experiments conducted on five real-world datasets indicate that our approach surpasses the state-of-the-art methods in multivariate time-series anomaly detection, shortens detection delays, and provides support for responsive anomaly reporting.},
  archive      = {J_NEUCOM},
  author       = {Jin Fan and YanHao Bi and Jin’an Yao and Liangkang Huang and HuiFeng Wu and Jia Wu},
  doi          = {10.1016/j.neucom.2025.131689},
  journal      = {Neurocomputing},
  month        = {1},
  pages        = {131689},
  shortjournal = {Neurocomputing},
  title        = {A responsive approach to multivariate time-series anomaly detection with K-distance based calibrated reconstruction},
  volume       = {659},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Quantum annealing-based feature selection. <em>NEUCOM</em>, <em>659</em>, 131673. (<a href='https://doi.org/10.1016/j.neucom.2025.131673'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Feature selection is crucial for enhancing the accuracy and efficiency of machine learning models. This work investigates the possibility of the usefulness of a quantum annealer to tackle the challenge of selecting features based on maximizing mutual information (MI) and conditional mutual information (CMI). Calculating the optimal feature set for maximum MI and CMI remains computationally intractable for large datasets on classical computers, even with approximation methods. This study employs a Mutual Information Quadratic Unconstrained Binary Optimization (MIQUBO) formulation, enabling its solution on a quantum annealer. Importantly, the study demonstrates the relevance of this approach in identifying the best feature combinations that maximize the MI and CMI. To showcase its real-world applicability, we apply MIQUBO to forecasting the price of used excavators. Our results demonstrate that using the MIQUBO approach leads to an improvement in the prediction of machine learning models for datasets, with a smaller MI concentration on a subset of all features.},
  archive      = {J_NEUCOM},
  author       = {Daniel Pranjić and Bharadwaj Chowdary Mummaneni and Christian Tutschku},
  doi          = {10.1016/j.neucom.2025.131673},
  journal      = {Neurocomputing},
  month        = {1},
  pages        = {131673},
  shortjournal = {Neurocomputing},
  title        = {Quantum annealing-based feature selection},
  volume       = {659},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Finite-time anti-synchronization of neural networks with general discrete time-varying delays. <em>NEUCOM</em>, <em>659</em>, 131666. (<a href='https://doi.org/10.1016/j.neucom.2025.131666'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper investigates the finite-time (F-T) anti-synchronization problem of neural networks (NNs) with discrete time-varying delays. Firstly, a F-T anti-synchronization theorem is proposed for a class of non-linear dynamical systems. It is important to note that the considered non-linear dynamical systems encompass various NN models, and the obtained result is applicable to a broader range of time delays. Subsequently, the F-T anti-synchronization of a specific class of delayed NNs is examined, and several F-T anti-synchronization criteria for the considered delayed NNs are presented. Finally, two numerical examples are used to verify the validity of the theoretical results obtained.},
  archive      = {J_NEUCOM},
  author       = {Yue Chen and Ailong Wu and Yan Li},
  doi          = {10.1016/j.neucom.2025.131666},
  journal      = {Neurocomputing},
  month        = {1},
  pages        = {131666},
  shortjournal = {Neurocomputing},
  title        = {Finite-time anti-synchronization of neural networks with general discrete time-varying delays},
  volume       = {659},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). A survey of machine learning and deep learning methods for vibration-based bearing fault diagnosis: The need, challenges, and potential future research directions. <em>NEUCOM</em>, <em>659</em>, 131628. (<a href='https://doi.org/10.1016/j.neucom.2025.131628'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Rolling element bearings (REBs) play a vital role in various mechanical systems, and their reliable performance is crucial for maintaining production efficiency and minimizing accidents. These bearings can develop faults, which, if not addressed, can lead to catastrophic failures, making fault diagnosis imperative for maintenance and safety. Intelligent bearing fault diagnosis systems using machine learning (ML) and deep learning (DL) approaches have been successful in accurately detecting and diagnosing faults. With rapid advancement in hardware and software technologies, there has been a parallel rapid growth in the development of intelligent fault diagnosis approaches. This survey examines the evolution and current state of various ML and DL approaches for vibration-based bearing fault diagnosis. The paper systematically reviews traditional ML algorithms, including k-Nearest Neighbors, Support Vector Machines, Artificial Neural Networks, Extreme Learning Machines, and Decision Tree-based methods, analyzing their integration with advanced signal processing and feature extraction techniques. The survey then explores DL architectures such as Convolutional Neural Networks, Deep Belief Networks, Autoencoders, Generative Adversarial Networks, Recurrent Neural Networks, and Gated Recurrent Units, highlighting their ability to automate feature extraction from raw vibration signals. A notable contribution of this survey is its in-depth examination of emerging techniques that have received limited attention in previous reviews, specifically Graph Neural Networks (GNNs) and Deep Reinforcement Learning (DRL) frameworks. The survey also covers recent advances in attention mechanisms, transformer architectures, and transfer learning strategies that enhance model performance across varying operational conditions. Furthermore, we discuss the integration of explainable AI techniques with ML and DL models to improve interpretability and highlight critical features for fault diagnosis. Through analysis of benchmark datasets and their limitations, we identify key challenges including data imbalance, domain transferability, real-time implementation constraints, and incipient fault detection. The survey concludes with future research directions emphasizing the need for physics-informed approaches, multi-modal sensor fusion, and lightweight architectures suitable for edge deployment.},
  archive      = {J_NEUCOM},
  author       = {Rohan Puntambekar and Pratyaksh Vyas and Ankit Thakkar and Dhaval Patel},
  doi          = {10.1016/j.neucom.2025.131628},
  journal      = {Neurocomputing},
  month        = {1},
  pages        = {131628},
  shortjournal = {Neurocomputing},
  title        = {A survey of machine learning and deep learning methods for vibration-based bearing fault diagnosis: The need, challenges, and potential future research directions},
  volume       = {659},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). DMPFuse: Infrared and visible image fusion via detail preservation and multi-path constraints. <em>NEUCOM</em>, <em>659</em>, 131530. (<a href='https://doi.org/10.1016/j.neucom.2025.131530'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Image fusion techniques address the inherent limitations of single-modal imaging systems by harmoniously combining complementary data modalities, thereby enhancing both image fidelity and downstream task performance. However, existing methods tend to lose texture details during feature extraction and most prioritize enhancing the visual quality of fused images. Although recent multi-task collaborative frameworks mitigate this issue, they typically introduce auxiliary task branches, substantially increasing computational costs and parameters. To address these limitations, we propose DMPFuse, a lightweight infrared-visible fusion framework that balances computational efficiency with comprehensive texture preservation and task-oriented feature extraction. The framework integrates three key innovations: (1) a Detail Enhancement Preservation (DEP) module, employing a dual-branch architecture with detail-enhancing convolutions and internal self-attention to preserve local textures and global structural features across modalities; (2) a plug-and-play Semantic-Detail Fusion Module (SDFM) that hierarchically integrates multi-scale features while mitigating cross-scale feature conflicts; and (3) a Multi-Path Constraint Module (MPCM) enforcing feature alignment through parallel supervision of fusion, reconstruction, and semantic segmentation tasks. Comparative experiments with benchmark methods demonstrate that DMPFuse significantly outperforms existing techniques in texture preservation, visual coherence, and semantic feature relevance. Quantitative metrics and qualitative analyses across segmentation and detection tasks empirically validate the practical advantages of our approach.},
  archive      = {J_NEUCOM},
  author       = {Wenlei Chen and Hanlin Qin and Xupei Zhang and Beihua Ying and Shuai Yuan and Xuefeng Bi},
  doi          = {10.1016/j.neucom.2025.131530},
  journal      = {Neurocomputing},
  month        = {1},
  pages        = {131530},
  shortjournal = {Neurocomputing},
  title        = {DMPFuse: Infrared and visible image fusion via detail preservation and multi-path constraints},
  volume       = {659},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). ADP-net: Adaptive point network with multi-scale attention mechanism for small object detection. <em>NEUCOM</em>, <em>659</em>, 131381. (<a href='https://doi.org/10.1016/j.neucom.2025.131381'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Small object detection in aerial imagery remains a significant challenge due to low resolution and sparse feature representations. While conventional object detectors achieve strong performance on medium and large objects, their accuracy for small objects can lag by up to 24 %. To address this gap, we propose AdaptivePointNet (ADP-Net), a novel keypoint-based detection framework tailored for small object recognition. Instead of relying on traditional bounding box regression, ADP-Net introduces a balanced keypoint encoder that employs heatmap representations to mitigate scale-induced biases. Additionally, an adaptive attention mechanism dynamically calibrates receptive fields, enabling the model to focus more effectively on small-scale features within coarse feature maps. To further enhance detection robustness, we incorporate a multi-scale fusion module that integrates predictions across hierarchical feature levels. Evaluated on the challenging UA-DETRAC aerial view benchmark, ADP-Net achieves state-of-the-art results, including up to 8.35 % improvement in hard-category scenarios dominated by small object instances.},
  archive      = {J_NEUCOM},
  author       = {Abenezer Girma and Abdollah Homaifar and Mahmoud Nabil Mahmoud},
  doi          = {10.1016/j.neucom.2025.131381},
  journal      = {Neurocomputing},
  month        = {1},
  pages        = {131381},
  shortjournal = {Neurocomputing},
  title        = {ADP-net: Adaptive point network with multi-scale attention mechanism for small object detection},
  volume       = {659},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Observer-based distributed adaptive neural network containment control for uncertain nonlinear multi-agent systems under DoS attacks. <em>NEUCOM</em>, <em>659</em>, 131306. (<a href='https://doi.org/10.1016/j.neucom.2025.131306'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper addresses the containment control issue in high-order nonlinear multi-agent systems (MASs) under denial of service (DoS) attacks. First, a neural network-based switching observer with adaptive mechanism is developed to reconstruct unmeasurable agent states under intermittent DoS-induced communication disruptions, establishing new theoretical pathways for directed network topologies. Second, a command-filtered backstepping control framework is proposed to circumvent the inherent complexity explosion in traditional recursive designs by eliminating redundant differentiations of virtual control laws. Ultimately, a distributed adaptive neural network containment control (DANNCC) scheme is established, ensuring all follower agents asymptotically converge into the convex hull spanned by multiple leaders. Furthermore, systematic stability analysis with constructed Lyapunov functions yields boundedness of all closed-loop signals in the system. Moreover, the containment errors can be asymptotically driven to an arbitrarily small magnitude through systematic parameter adjustment. The developed approach’s operational efficacy and real-world applicability are validated through comprehensive simulations across heterogeneous attack scenarios, demonstrating strict adherence to convergence requirements without control performance degradation.},
  archive      = {J_NEUCOM},
  author       = {Chunlong Hao and Zhi Liu and Licheng Zheng and C.L. Philip Chen and Guanyu Lai},
  doi          = {10.1016/j.neucom.2025.131306},
  journal      = {Neurocomputing},
  month        = {1},
  pages        = {131306},
  shortjournal = {Neurocomputing},
  title        = {Observer-based distributed adaptive neural network containment control for uncertain nonlinear multi-agent systems under DoS attacks},
  volume       = {659},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Measuring cognitive load by a score-based causal network model with multichannel physiological signals. <em>NEUCOM</em>, <em>659</em>, 131290. (<a href='https://doi.org/10.1016/j.neucom.2025.131290'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cognitive load significantly affects performance in safety-sensitive fields like aviation, where excessive cognitive demands are linked to increased operational errors and heightened risks. However, cognitive load measurement faces the challenge of neglecting spatio-temporal and causal dependencies in multi-channel physiological signals. We present a score-based causal representation model that captures causal structural diversity and spatio-temporal dependencies within single channels and inter-dependencies among multiple channels. By optimizing a score-based causal function under causal Markov property constraints, the model disentangles latent spatio-temporal features into causal and task-irrelevant groups. Experimental results on two public and one in-house dataset show our model significantly outperforms state-of-the-art methods.},
  archive      = {J_NEUCOM},
  author       = {Li Liu and Qiwen Pang and Shanshan Huang and Laiming Jiang and Shu Wang and Guang Wu and Guoxin Su and Qing Tao},
  doi          = {10.1016/j.neucom.2025.131290},
  journal      = {Neurocomputing},
  month        = {1},
  pages        = {131290},
  shortjournal = {Neurocomputing},
  title        = {Measuring cognitive load by a score-based causal network model with multichannel physiological signals},
  volume       = {659},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Deep semi-supervised regression with cooperative uncertainty-consistency regularization and adaptive calibration. <em>NEUCOM</em>, <em>659</em>, 131276. (<a href='https://doi.org/10.1016/j.neucom.2025.131276'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep semi-supervised learning is essential in various fields, particularly where acquiring sufficient labeled data poses a challenge. In computer vision tasks, semi-supervised classification learning has seen tremendous advancements, whereas semi-supervised regression learning remains relatively under-explored. Generally, techniques that are effective for semi-supervised classification can’t directly apply to semi-supervised regression without adaptation. This work proposes a novel semi-supervised regression method with dynamically cooperative textbfUncertainty- C onsistency R egularization and Adaptive Pseudo-Label C alibration, named UCRC . Given the limitations of consistency regularization in regression tasks, the proposed method dynamically modulates its application intensity to mitigate overfitting induced by uncertainty regularization while enhancing model robustness, and allows complete activation of uncertainty regularization to capture noisy patterns thereby improving pseudo-label quality. Furthermore, this work incorporates feature information from labeled data, adaptively selecting relevant features into the pseudo-label generation process. This effectively enhances the credibility of the pseudo-labels. Experiments demonstrate that this method achieves superior performance and robustness compared to state-of-the-art methods on two facial age prediction datasets (AgeDB and UTKFace) and a crowd counting dataset, called ShanghaiTech Part-A/B. Benefiting from the cooperative regularization loss function and adaptive pseudo label calibration technology incorporated, this method achieves R 2 scores of 69.82 %, 57.29 %, and 53.35 % on three datasets using only 10 % of the training labels. A series of ablation experiments provide detailed insights into the mechanism of the cooperative regularization and the impact of the calibration technique.},
  archive      = {J_NEUCOM},
  author       = {Xingyu Zhou and Mingqi Jiang and Yongjia Zhao and Lei Pang and Zhuo Wang},
  doi          = {10.1016/j.neucom.2025.131276},
  journal      = {Neurocomputing},
  month        = {1},
  pages        = {131276},
  shortjournal = {Neurocomputing},
  title        = {Deep semi-supervised regression with cooperative uncertainty-consistency regularization and adaptive calibration},
  volume       = {659},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Sycophancy in vision-language models: A systematic analysis and an inference-time mitigation framework. <em>NEUCOM</em>, <em>659</em>, 131217. (<a href='https://doi.org/10.1016/j.neucom.2025.131217'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Large Vision–Language Models (LVLMs) have shown significant capability in vision–language understanding. However, one critical issue that persists in these models is sycophancy, where models are unduly influenced by leading or deceptive prompts, resulting in biased outputs and hallucinations. Despite the rapid development of LVLMs, evaluating and mitigating sycophancy remains largely under-explored. In this work, we fill this gap by systematically analyzing sycophancy across multiple vision–language benchmarks and propose an inference-time mitigation framework. We curate leading queries and quantify the susceptibility of state-of-the-art LVLMs to prompt-induced bias, revealing consistent performance degradation and instability across models and tasks. Our analysis further uncovers model-specific behavioral traits, such as sentiment sensitivity and prediction polarity shifts under sycophancy. To mitigate these issues, we propose a training-free, model-agnostic framework that operates entirely at inference time. Our approach first employs a query neutralizer, leveraging a language model to suppress implicit sycophantic bias in user queries. We then introduce a sycophancy-aware contrastive decoding mechanism that dynamically recalibrates token-level output distributions by contrasting responses to neutralized and leading queries. Finally, an adaptive logits refinement module further modifies the contrasted logits by integrating both an adaptive plausibility filter and query sentiment scaler, ensuring coherent and robust generation. Extensive experiments demonstrate that this framework effectively mitigates sycophancy across all evaluated models, while maintaining performance on neutral prompts. Our results suggest that sycophancy in LVLMs is a general and urgent challenge, and that inference-time strategies offer a promising path toward trustworthy multimodal reasoning.},
  archive      = {J_NEUCOM},
  author       = {Yunpu Zhao and Rui Zhang and Junbin Xiao and Changxin Ke and Ruibo Hou and Yifan Hao and Ling Li},
  doi          = {10.1016/j.neucom.2025.131217},
  journal      = {Neurocomputing},
  month        = {1},
  pages        = {131217},
  shortjournal = {Neurocomputing},
  title        = {Sycophancy in vision-language models: A systematic analysis and an inference-time mitigation framework},
  volume       = {659},
  year         = {2026},
}
</textarea>
</details></li>
</ul>

</body>
</html>
