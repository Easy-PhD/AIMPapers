<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>MLA</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="mla">MLA - 173</h2>
<ul>
<li><details>
<summary>
(2025). A dual-validation 3D nnU-net framework with harmonized preprocessing for robust DLBCL segamentation in PET/CT images. <em>MLA</em>, <em>22</em>, 100788. (<a href='https://doi.org/10.1016/j.mlwa.2025.100788'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Diffuse large B-cell lymphoma (DLBCL) is an aggressive and common subtype of non-Hodgkin lymphoma. The automatic segmentation of DLBCL tumors from positron emission tomography/computed tomography (PET/CT) images remains a significant challenge due to the complexity and variable appearance of tumors. In this study, we developed and evaluated a 3D nn-UNet model for the automatic segmentation of DLBCL lesions to support treatment planning and monitoring. The model was trained on 18F-FDG PET/CT scans from 217 patients. Performance was assessed using geometric metrics, resulting in a mean Dice Similarity Coefficient (DSC) of 0.85, Intersection over Union (IoU) of 0.75, sensitivity of 88.3 %, specificity of 95.7 %, and accuracy of 97.1 %. To establish clinical validity, the Total Metabolic Tumor Volume (TMTV) was derived from both ground truth and predicted segmentations. Bland-Altman analysis demonstrated strong agreement, and linear regression confirmed a high correlation between the volumes. The key novelty of our work lies in a harmonized preprocessing pipeline and a dual-validation strategy that integrates geometric metrics (DSC, IoU) with volumetric and metabolic assessments (TMTV, Standardized Uptake Value (SUVmax)). The results, supported by box plots illustrating metric distributions, confirm the model's robustness, reliability, and potential for clinical utility in managing DLBCL.},
  archive      = {J_MLA},
  author       = {Sajad Keshavarz and Elham Saeedzadeh and Dariush Sardari and Elnaz Jenabi-Haghparast and Hossein Arabi},
  doi          = {10.1016/j.mlwa.2025.100788},
  journal      = {Machine Learning with Applications},
  month        = {12},
  pages        = {100788},
  shortjournal = {Mach. Learn. Appl.},
  title        = {A dual-validation 3D nnU-net framework with harmonized preprocessing for robust DLBCL segamentation in PET/CT images},
  volume       = {22},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multi-task learning for audio scene source counting and analysis. <em>MLA</em>, <em>22</em>, 100785. (<a href='https://doi.org/10.1016/j.mlwa.2025.100785'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Audio source counting is a fundamental task of audio scene analysis related to other audio tasks such as speaker diarization and sound event detection. It is also a relatively unexplored audio task that presents a complex challenge. In particular, source counting performance is poor when the source count range is large, limiting its potential applications. This paper presents a novel approach to improve upon audio source counting through multi-task learning. We present a first of its kind empirical study on the hierarchical nature of audio source counting, introducing the coarse source counting task and a hierarchical multi-task learning framework, in order to better understand and investigate the audio source counting task through several case study scenarios. We perform multi-task learning with a ResNet architecture and demonstrate improvements to audio source counting accuracy by up to a 6% increase from the previous best result on the SARdBScene dataset. We also perform multi-task learning of audio source counting and acoustic scene classification as a step forward for robust audio scene analysis. These experimental results show improvements of up to 6% in source counting accuracy over state-of-the-art baselines, particularly in high source count scenarios. Our findings highlight that multi-task learning not only enhances accuracy, but also improves efficiency by replacing multiple task-specific models with a single robust network.},
  archive      = {J_MLA},
  author       = {Michael Nigro and Sridhar Krishnan},
  doi          = {10.1016/j.mlwa.2025.100785},
  journal      = {Machine Learning with Applications},
  month        = {12},
  pages        = {100785},
  shortjournal = {Mach. Learn. Appl.},
  title        = {Multi-task learning for audio scene source counting and analysis},
  volume       = {22},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Unsupervised detection of faults in industrial pumps from multivariate time series. <em>MLA</em>, <em>22</em>, 100784. (<a href='https://doi.org/10.1016/j.mlwa.2025.100784'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Unplanned pump failures in asset-intensive industries like pulp and paper lead to significant production losses. Data-driven predictive maintenance through anomaly detection has recently appeared to be useful in industrial settings. However, this approach is hampered by the infeasibility of manually annotating multivariate sensor data for supervised learning. While unsupervised anomaly detection offers a promising approach, a key challenge is the lack of structured ground-truth labels for evaluation derived from sparse, unstructured maintenance logs. This paper addresses this gap by introducing a fully unsupervised framework that systematically transforms window-level sensor data for model training and utilizes maintenance notifications to enable robust model evaluation. We implement this approach on a critical process pump in a paperboard mill on an industrial scale, with data that extends for a year. The framework contains a reproducible log-to-label pipeline that generates anomalous and normal time-series windows from industrial sensor and maintenance data. The framework also implements a comprehensive feature engineering process that extracts statistical, spectral, and temporal features from high-frequency sensor readings. We have implemented the framework for a comparative evaluation of five unsupervised anomaly detectors. Our experiments show the usefulness of the framework in practice, and also discuss the tradeoffs, including a critical tradeoff between detection accuracy and deployment feasibility. This work provides a practical framework for evaluating and deploying unsupervised anomaly detection models in real-world industrial settings where labeled data is almost unavailable.},
  archive      = {J_MLA},
  author       = {Faizan Shaikh and Bestoun S. Ahmed and Agne Swerin},
  doi          = {10.1016/j.mlwa.2025.100784},
  journal      = {Machine Learning with Applications},
  month        = {12},
  pages        = {100784},
  shortjournal = {Mach. Learn. Appl.},
  title        = {Unsupervised detection of faults in industrial pumps from multivariate time series},
  volume       = {22},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Implementation of knowledge distillation for onboard defect detection on an unmanned aircraft system for light aircraft general visual inspections. <em>MLA</em>, <em>22</em>, 100782. (<a href='https://doi.org/10.1016/j.mlwa.2025.100782'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Visual inspections of aircraft are a vital part of routine procedures for maintenance personnel in the aviation industry. However, these inspections take up a considerable amount of time to perform and are susceptible to human error. To mitigate this, utilising image classification for detecting defects is proposed, leveraging transfer learning and knowledge distillation within MATLAB to develop an efficient and deployable model. Transfer learning is applied to a ResNet-50 model, adapting it to classify aircraft defects using a curated dataset. This fine-tuned model is then utilised as a teacher in the knowledge distillation process, where a compact SqueezeNet model (the student) learns from both hard and soft labels to replicate its performance while significantly reducing computational demands. This allows for optimising deep-learning models for deployment on smaller hardware, making the student model suitable for use on an Unmanned Aircraft System (UAS) to filter out images that do not contain a defect, reducing workload for ground personnel. The proposed method offers a solution for improving the efficiency and accuracy of defect detection during a general visual inspection in the aviation industry. Targeted defects here are damaged_skin , missing_or_damaged_rivets , and panel_missing alongside a class denoting no_defect . The knowledge-distilled SqueezeNet model achieves 95.37% validation accuracy and 90.72% inference accuracy, with a 96.9% reduction in model size compared to ResNet-50. The teacher model has a size of 85.77 MB, while the student model is significantly smaller at 2.66 MB, making it ideal for deployment on embedded systems with limited resources.},
  archive      = {J_MLA},
  author       = {Luke Connolly and James Garland and Diarmuid O’Gorman and Edmond F. Tobin},
  doi          = {10.1016/j.mlwa.2025.100782},
  journal      = {Machine Learning with Applications},
  month        = {12},
  pages        = {100782},
  shortjournal = {Mach. Learn. Appl.},
  title        = {Implementation of knowledge distillation for onboard defect detection on an unmanned aircraft system for light aircraft general visual inspections},
  volume       = {22},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A comparative framework for multi-horizon time series forecasting: Neural networks with adaptive preprocessing. <em>MLA</em>, <em>22</em>, 100781. (<a href='https://doi.org/10.1016/j.mlwa.2025.100781'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Accurate multi-horizon time series forecasting remains a major challenge in predictive modeling due to cumulative error propagation over extended horizons. This study proposes a unified and reproducible framework that integrates adaptive signal decomposition with neural network architectures under a Multiple-Input Multiple-Output (MIMO) strategy, effectively removing recursive dependencies and improving training stability. Three representative neural models Multilayer Perceptron (MLP), Long Short-Term Memory (LSTM), and Bidirectional LSTM (BiLSTM) are systematically combined with both classical and adaptive preprocessing techniques, namely trend–fluctuation separation, Empirical Mode Decomposition (EMD), Variational Mode Decomposition (VMD), and Empirical Wavelet Transform (EWT). The framework is validated on three economic and energy-related datasets (electricity demand, natural gas prices, and CO₂ emission allowances), generating forecasts up to ten steps ahead and evaluated through RMSE, MAPE, and R² metrics. Experimental results show that adaptive decomposition, particularly VMD and EMD, yield the highest accuracy and stability across prediction horizons, while EWT provides consistent intermediate improvements and classical trend-based methods offer only marginal benefits. Moreover, computational analysis demonstrates that the proposed approach remains lightweight and efficient, with training and inference times suitable for real-world deployment. Overall, the findings confirm that coupling adaptive preprocessing with MIMO-based neural forecasting enhances accuracy, robustness, and interpretability without increasing architectural complexity, establishing a practical foundation for multi-horizon forecasting in economic and financial domains.},
  archive      = {J_MLA},
  author       = {Ana Lazcano and Julio E. Sandubete and Miguel A. Jaramillo-Morán},
  doi          = {10.1016/j.mlwa.2025.100781},
  journal      = {Machine Learning with Applications},
  month        = {12},
  pages        = {100781},
  shortjournal = {Mach. Learn. Appl.},
  title        = {A comparative framework for multi-horizon time series forecasting: Neural networks with adaptive preprocessing},
  volume       = {22},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Toward clinical reliability: Visualizing and interpreting ai-based classification in peripheral blood smear analysis. <em>MLA</em>, <em>22</em>, 100780. (<a href='https://doi.org/10.1016/j.mlwa.2025.100780'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the advent of digital microscopy, the International Council for Standardization in Hematology recommends digital imaging and artificial intelligence (AI) algorithms for automatically classifying blood cells in peripheral blood smears to enhance diagnostic efficiency and accuracy. Nevertheless, while early AI studies have shown promising results in classifying white blood cells, the prediction process often remains unclear. Herein, we aimed to build a highly accurate model and visualize the basis of its predictions. The dataset comprised peripheral blood smear images of normal cells from individuals without infections, hematological disorders, or tumors, who were not undergoing any drug treatment at the time of blood collection. The images were obtained using a CellaVision DM96 analyzer at the Core Laboratory of Hospital Clínic de Barcelona. We used VGG16 and ResNet50 with transfer learning on ImageNet and applied the Grad-CAM method to visualize the image regions on which the model focused for classification. The model effectively recognized features, such as nuclear indentation and cytoplasmic color, which are crucial for classifying promyelocytes, myelocytes, and metamyelocytes. Traditionally, the basis of AI model predictions has been opaque, posing a challenge for medical applications. Our visualized classification basis clarifies the decision-making process of the model. These insights suggest that understanding these features can make the predictions of AI models more reliable and interpretable. Our findings improve diagnostic efficiency and suggest the potential of AI-based diagnostic support systems. Future research should validate this model’s performance using more extensive datasets and different cell types to enhance its reliability and practicality.},
  archive      = {J_MLA},
  author       = {Hiroaki Iwata and Tsukie Shibayama and Miku Watanabe and Hisashi Shimohiro},
  doi          = {10.1016/j.mlwa.2025.100780},
  journal      = {Machine Learning with Applications},
  month        = {12},
  pages        = {100780},
  shortjournal = {Mach. Learn. Appl.},
  title        = {Toward clinical reliability: Visualizing and interpreting ai-based classification in peripheral blood smear analysis},
  volume       = {22},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Identifying critical fire spread to the wildland–urban interface using cellular automata and reinforcement learning. <em>MLA</em>, <em>22</em>, 100779. (<a href='https://doi.org/10.1016/j.mlwa.2025.100779'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Wildfires threatening the wildland–urban interface present significant risks to community safety, especially under conditions of inadequate vegetation management and adverse weather. Accurately identifying scenarios in which fire reaches this interface is crucial for timely evacuation planning and risk mitigation. This study presents a computational method using cellular automata and stochastic simulations to model wildfire spread. Stochastic scenarios generated through the cellular automata are employed to train a reinforcement learning model, which leverages computer vision techniques to interpret multiple layers representing diverse environmental factors. This enables the reinforcement learning agent to identify and prioritise critical fire trajectories that could impact the wildland–urban interface. The framework adapts the Rothermel surface fire spread model within a cellular automata structure, providing a simplified yet effective simulation of fire propagation under variable conditions. The proposed approach was validated using synthetic and real-world case studies, demonstrating its potential for integration with geographic information systems. Results suggest this approach enhances the identification of critical fire spread scenarios and improves computational efficiency for real-time applications. By enabling real-time recognition of high-risk events, our framework supports more informed evacuation strategies and fire management decisions around the wildland–urban interface.},
  archive      = {J_MLA},
  author       = {Javier González-Villa and David Lázaro and Arturo Cuesta and Adriana Balboa and Daniel Alvear and Mariano Lázaro},
  doi          = {10.1016/j.mlwa.2025.100779},
  journal      = {Machine Learning with Applications},
  month        = {12},
  pages        = {100779},
  shortjournal = {Mach. Learn. Appl.},
  title        = {Identifying critical fire spread to the wildland–urban interface using cellular automata and reinforcement learning},
  volume       = {22},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Time series modeling of monkeypox incidence in central africa’s endemic regions. <em>MLA</em>, <em>22</em>, 100778. (<a href='https://doi.org/10.1016/j.mlwa.2025.100778'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Monkeypox is a re-emerging zoonotic viral disease endemic to the Democratic Republic of Congo (DRC) and other Central African countries, with recurrent outbreaks posing persistent public health threats. Accurate short-term forecasts of Mpox incidence are essential for guiding surveillance, preparedness, and timely interventions. In this study, we analyzed daily confirmed Mpox cases data from May 1, 2022 to May 31, 2025, using autoregressive integrated moving average and Prophet models, complemented by wavelet analysis. Model performance varied by country, reflecting differences in reporting the quality and outbreak intensity. Prophet generally outperformed ARIMA in settings with smoother incidence trajectories, while ARIMA was more effective in capturing abrupt local fluctuations. Wavelet analysis further revealed country-specific temporal–frequency patterns, highlighting differences in epidemic periodicity across the region. These findings underscore the utility of combining statistical and decompositional approaches for Mpox forecasting in resource-limited settings. Strengthening surveillance systems, improving data quality, and adopting flexible, country-specific forecasting frameworks will be critical for developing effective early-warning systems and guiding evidence-based interventions in Central Africa.},
  archive      = {J_MLA},
  author       = {Chidozie Williams Chukwu and George Obaido and Ibomoiye Domor Mienye and Kehinde Aruleba and Ebenezer Esenogho and Cameron Modisane},
  doi          = {10.1016/j.mlwa.2025.100778},
  journal      = {Machine Learning with Applications},
  month        = {12},
  pages        = {100778},
  shortjournal = {Mach. Learn. Appl.},
  title        = {Time series modeling of monkeypox incidence in central africa’s endemic regions},
  volume       = {22},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Gradient boosting for group testing. <em>MLA</em>, <em>22</em>, 100777. (<a href='https://doi.org/10.1016/j.mlwa.2025.100777'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {When conducting disease screening within a population, it is often more efficient and cost effective to group individual specimens and test them in pools, rather than testing each specimen individually. This method, which is known as group testing, speeds up the diagnostic process and reduces costs, especially when the outcome of interest is rare. However, the data collected from group testing can be inherently complex, especially in the presence of imperfect testing, and this complexity can hinder surveillance efforts. To overcome this challenge, we propose a gradient boosting framework designed to build predictive models based on group testing data using individual-level predictors. Our framework is flexible and supports a wide range of weak learners, including regression trees, kernel smoothing, and splines. In addition, our model accommodates data arising from any group testing protocol and accounts for the effects of imperfect testing. To optimize model performance, we develop a cross-validation approach that selects optimal tuning parameters for the weak learners. We explore the performance of our approach through numerical studies and illustrate our methods using chlamydia group testing data collected by the State Hygienic Laboratory in Iowa.},
  archive      = {J_MLA},
  author       = {Erica M. Porter and Christopher S. McMahan and Joshua M. Tebbs and Christopher R. Bilder},
  doi          = {10.1016/j.mlwa.2025.100777},
  journal      = {Machine Learning with Applications},
  month        = {12},
  pages        = {100777},
  shortjournal = {Mach. Learn. Appl.},
  title        = {Gradient boosting for group testing},
  volume       = {22},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Model-informed oracle training for enhancing active learning without external knowledge. <em>MLA</em>, <em>22</em>, 100775. (<a href='https://doi.org/10.1016/j.mlwa.2025.100775'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In real-world applications of active learning frameworks, human oracles are often imperfect, and label noise is introduced into the learning process. This issue can be mitigated by further training the oracle using previous knowledge acquired by the model. However, it remains unclear whether model-informed oracle training can significantly improve performance. This study investigates whether recursive feedback between the model and the oracle can induce a knowledge augmentation effect, defined as a statistically significant improvement in model performance after receiving feedback from a self-data-trained oracle. To this end, we implemented a bidirectional active learning framework in which the model assists oracle learning by selectively transferring prior knowledge. In a closed-loop environment without external data, the model performs informative sample selection from an unlabeled pool, querying the oracle for labels, and retraining on the updated dataset. Simultaneously, the oracle is updated by learning from samples from the model’s training data that exhibit high uncertainty from the oracle’s perspective. This framework was empirically validated through a behavioral experiment involving 252 clinicians performing a medical image interpretation task. The results showed that model-informed oracle training enhanced both oracle accuracy and model performance. Moreover, when oracle learning was constrained by a fixed learning budget, a sampling strategy jointly balancing uncertainty and representativeness yielded the strongest effect. These findings provide compelling empirical evidence of the knowledge augmentation effect arising from human learning within a closed-loop active learning framework.},
  archive      = {J_MLA},
  author       = {Yujin Cha},
  doi          = {10.1016/j.mlwa.2025.100775},
  journal      = {Machine Learning with Applications},
  month        = {12},
  pages        = {100775},
  shortjournal = {Mach. Learn. Appl.},
  title        = {Model-informed oracle training for enhancing active learning without external knowledge},
  volume       = {22},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Phase-based physics-informed sequence-to-sequence neural network for typhoon intensity and trajectory prediction. <em>MLA</em>, <em>22</em>, 100774. (<a href='https://doi.org/10.1016/j.mlwa.2025.100774'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Typhoons are among the most destructive natural disasters in the Asia-Pacific region, causing extensive damage to infrastructure, agriculture, and human lives. Accurate and timely prediction of a typhoon's intensity and trajectory is crucial for effective disaster response and risk mitigation. However, existing forecasting models often struggle to capture the highly dynamic and heterogeneous nature of a typhoon’s lifecycle, leading to inaccuracies during periods of rapid intensification or abrupt trajectory shifts. This study proposes a novel phase-based, physics-informed sequence-to-sequence neural network to address these limitations. The framework decomposes the forecasting task into distinct lifecycle phases—formation, intensification, and dissipation—using specialized deep learning models for each. A key innovation is the integration of physical constraints derived from the Navier–Stokes equations directly into the model’s loss function, ensuring that predictions are both data-driven and physically consistent. Using the Digital Typhoon Dataset, the proposed method achieves a mean absolute error (MAE) of 17.83 km for trajectory prediction and 7.28 kt for intensity forecasting, representing a significant improvement over existing approaches. Moreover, the proposed method also requires only 3 h of historical data to generate forecasts, compared to more (e.g., 48) hours needed by entire-time series approaches, providing earlier inference critical for disaster preparedness and protection of infrastructure such as offshore wind farms.},
  archive      = {J_MLA},
  author       = {Ying-Yi Hong and Daryll John Medina},
  doi          = {10.1016/j.mlwa.2025.100774},
  journal      = {Machine Learning with Applications},
  month        = {12},
  pages        = {100774},
  shortjournal = {Mach. Learn. Appl.},
  title        = {Phase-based physics-informed sequence-to-sequence neural network for typhoon intensity and trajectory prediction},
  volume       = {22},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Uncertainty quantification by large language models. <em>MLA</em>, <em>22</em>, 100773. (<a href='https://doi.org/10.1016/j.mlwa.2025.100773'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As reasoning capabilities of large language models (LLMs) continue to advance, they are being integrated into increasingly complex scientific workflows, with the goal of developing agents capable of generating evidence-based explanations and testing hypotheses and theories. However, despite their rapid progress, most existing evaluations of LLM reasoning focus on accuracy or consistency rather than on uncertainty quantification (UQ), which is essential for evidence-based reasoning because it quantifies the degree of trustworthiness of evidence-based explanations. Current approaches to LLM uncertainty remain fragmented, often lacking standardized benchmarks that test models under varying task complexities. To address this gap, we introduce the first benchmark suite designed to evaluate UQ by LLM-based agents and tools. The benchmark targets one of the most fundamental UQ problem: estimating whether one quantity is probably larger than another under uncertainty. It includes two progressively complex tasks: a simple inequality test, where models judge whether one of two sets of samples is “larger,” “smaller,” or “uncertain” with 95% confidence, and a complex inequality test, where models assess interventional probabilities requiring multiple intermediate calculations. We found that reasoning models are generally capable of UQ (scores ≳ 70 % ) in the simple inequality case but do not score appreciably better than random guessing (scores ∼ 33 % ) for the complex inequality case if the UQ method and intermediate steps are not provided in the prompt. Our implementation is available at https://github.com/bekaiser-LANL/tether .},
  archive      = {J_MLA},
  author       = {Dorianis M. Perez and Bryan E. Kaiser and Ismael Boureima},
  doi          = {10.1016/j.mlwa.2025.100773},
  journal      = {Machine Learning with Applications},
  month        = {12},
  pages        = {100773},
  shortjournal = {Mach. Learn. Appl.},
  title        = {Uncertainty quantification by large language models},
  volume       = {22},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Spatio-temporal graph neural networks for human–AI collaborative decision-making. <em>MLA</em>, <em>22</em>, 100771. (<a href='https://doi.org/10.1016/j.mlwa.2025.100771'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Collaborative decision-making (CDM) is essential in different domains where integrating diverse perspectives improves classification accuracy. Traditional aggregation methods, such as majority voting (MV), are static and fail to capture the dynamic, real-time interactions among decision-makers. We propose a task- and label-independent framework based on spatio-temporal graph neural networks (STGNNs) to model CDM as an evolving process. The framework represents participants and classification options as nodes in graph sequences, capturing relational dependencies ( e.g. , agreement clusters) and temporal patterns ( e.g. , convergence to consensus). It integrates a graph neural network with a gated recurrent unit to jointly model spatial and temporal dynamics, and introduces an auxiliary loss that reinforces agreement structure and option alignment in the embedding space. We evaluated the framework on five expert-driven image classification tasks in biology and pathology using a web-based collaborative platform. In human-only settings, STGNNs achieved a global accuracy of 77.6% ( Δ = + 4 . 3 % over MV, p < 0 . 001 ). When extended to mixed human–AI teams, a meta-learning aggregator combining STGNN and AI agent predictions achieved a global accuracy of 81.4%, outperforming both human-only models and MV. These findings demonstrate the utility of STGNNs for modeling latent decision dynamics and enhancing collaborative performance in complex, ambiguous settings. The task- and label-independence of the framework suggests broad applicability across domains.},
  archive      = {J_MLA},
  author       = {Israel Mateos-Aparicio-Ruiz and Pedro Montealegre-Macias and Oscar Deniz and Gloria Bueno},
  doi          = {10.1016/j.mlwa.2025.100771},
  journal      = {Machine Learning with Applications},
  month        = {12},
  pages        = {100771},
  shortjournal = {Mach. Learn. Appl.},
  title        = {Spatio-temporal graph neural networks for human–AI collaborative decision-making},
  volume       = {22},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). On the retraining frequency of global models in retail demand forecasting. <em>MLA</em>, <em>22</em>, 100769. (<a href='https://doi.org/10.1016/j.mlwa.2025.100769'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In an era of increasing computational capabilities and growing environmental consciousness, organizations face a critical challenge in balancing the accuracy of forecasting models with computational efficiency and sustainability. Global forecasting models, lowering the computational time, have gained significant attention over the years. However, the common practice of retraining these models with new observations raises important questions about the costs of forecasting. Using ten different machine learning and deep learning models, we analyzed various retraining scenarios, ranging from continuous updates to no retraining at all, across two large retail demand datasets. We showed that less frequent retraining strategies maintain the forecast accuracy while reducing the computational costs, providing a more sustainable approach to large-scale forecasting. We also found that machine learning models are a marginally better choice to reduce the costs of forecasting when coupled with less frequent model retraining strategies as the frequency of the data increases. Our findings challenge the conventional belief that frequent retraining is essential for maintaining forecasting accuracy. Instead, periodic retraining offers a good balance between predictive performance and efficiency, both in the case of point and probabilistic forecasting. These insights provide actionable guidelines for organizations seeking to optimize forecasting pipelines while reducing costs and energy consumption.},
  archive      = {J_MLA},
  author       = {Marco Zanotti},
  doi          = {10.1016/j.mlwa.2025.100769},
  journal      = {Machine Learning with Applications},
  month        = {12},
  pages        = {100769},
  shortjournal = {Mach. Learn. Appl.},
  title        = {On the retraining frequency of global models in retail demand forecasting},
  volume       = {22},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). AI to protect AI: A modular pipeline for detecting label-flipping poisoning attacks. <em>MLA</em>, <em>22</em>, 100768. (<a href='https://doi.org/10.1016/j.mlwa.2025.100768'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Modern machine learning models are vulnerable to data poisoning attacks that compromise the integrity of their training data, with label flipping being a particularly insidious variant. In a label flipping attack, an adversary maliciously alters a fraction of the training labels to mislead the model, which can significantly degrade performance or cause targeted misclassifications while often evading simple detection. In this work, we address this threat by introducing a modular, attack-agnostic detection framework (“AI to Protect AI”) that monitors model behaviour for poisoning indicators without requiring internal access or changes to the target model. A Behaviour Monitoring Module (BMM) continuously observes the model’s outputs, extracting telltale features such as prediction probabilities, entropy, and margins for each input. These features are analysed by an ensemble of detector models, including supervised classifiers and unsupervised anomaly detectors, that collaboratively flag suspicious training samples indicative of label tampering. The proposed framework is dataset-agnostic and model-agnostic, as demonstrated across diverse image classification tasks using the MNIST (handwritten digits), CIFAR-10 (natural images), and ChestXray14 (medical X-rays) datasets. Experimental results indicate that the system reliably detects poisoned data with high accuracy (e.g., an area under the ROC curve exceeding 0.95 on MNIST, above 0.90 on CIFAR-10, and up to 0.85 on ChestXray14), while maintaining low false alarm rates. This work highlights a novel “AI to protect AI” approach, leveraging multiple lightweight detectors in concert to safeguard learning processes across different domains and thereby enhance the security and trustworthiness of AI systems.},
  archive      = {J_MLA},
  author       = {Hossein Abroshan},
  doi          = {10.1016/j.mlwa.2025.100768},
  journal      = {Machine Learning with Applications},
  month        = {12},
  pages        = {100768},
  shortjournal = {Mach. Learn. Appl.},
  title        = {AI to protect AI: A modular pipeline for detecting label-flipping poisoning attacks},
  volume       = {22},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Unsupervised deep learning for semantic segmentation using laparoscopic videos: A self-detection and self-learning approach. <em>MLA</em>, <em>22</em>, 100767. (<a href='https://doi.org/10.1016/j.mlwa.2025.100767'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Artificial intelligence (AI) and machine learning methods play a crucial role in image processing applications, particularly in semantic segmentation and localization tasks. These models rely on annotated datasets to train algorithms capable of detecting and localizing objects of interest in images. However, the manual annotation process demands substantial human effort and focus, posing significant challenges in terms of time, economic costs, and energy consumption. This paper introduces a novel unsupervised deep learning approach inspired by the psychology of human learning to address these limitations. First, the self-learning methodology is proposed to utilize only one or two annotated images to train neural networks, enabling automated segmentation and annotation of a large volume of unannotated images within the dataset. Then, to enhance the automation of this process, a complementary object detection algorithm, termed Self-Detection, is proposed. By simply clicking on an object within an image, this algorithm differentiates it from other objects in the scene, streamlining object identification and segmentation. Integrating the proposed Self-Learning and Self-Detection methods results in a fully unsupervised framework for training semantic segmentation neural networks. The key outcomes of this methodology include (1) trained neural network models capable of precise segmentation and localization of objects of interest, and (2) a fully-automatically well-annotated image dataset suitable for training other types of AI models with diverse architectures. The proposed methodology can be used for developing accurate, reliable, and interpretable deep learning models for various tasks and applications, both medical and non-medical, as well as for segmentation or localization tasks.},
  archive      = {J_MLA},
  author       = {Sina Saadati and Maryam Hashemi and Camran Nezhat},
  doi          = {10.1016/j.mlwa.2025.100767},
  journal      = {Machine Learning with Applications},
  month        = {12},
  pages        = {100767},
  shortjournal = {Mach. Learn. Appl.},
  title        = {Unsupervised deep learning for semantic segmentation using laparoscopic videos: A self-detection and self-learning approach},
  volume       = {22},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Machine learning techniques for analysing cardiotocography signals for early detection of fetal anomalies based on feature engineering methods. <em>MLA</em>, <em>22</em>, 100766. (<a href='https://doi.org/10.1016/j.mlwa.2025.100766'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Complications during childbirth are among the leading causes of infant mortality in the first months of life. Cardiotocography (CTG) is a primary tool for diagnosing and monitoring the condition of the fetus and identifying high-risk women during labor. Knowing the dynamic patterns of CTG signals requires a real-time interpretation. This is a difficult task, with significant differences in opinion between doctors. This study presents a new integrated framework that combines feature selection and dimensionality reduction methods to enhance classification of fetal health status from CTG signals. The primary contribution of this study is to combine the Shapley Additive Explanations (SHAP) method with both dimensionality reduction methods, t-distributed Stochastic Neighbor Embedding (t-SNE), and Principal Component Analysis (PCA). By implementing the combined approach, the dimensionality of the CTG data was reduced, while the most feasible features remained foundational for fetal health diagnosis. The proposed method was validated against the well-known, publicly available, and widely used CTG data. The use of a strong statistical method using the Variance Inflation Factor (VIF) facilitated a rigorous approach to reducing multicollinearity. The VIF value suggests that the data have improved quality and, therefore, reliability for the predictive model development. Various machine learning classifiers, including XGBoost, K-Nearest Neighbors (KNN), Decision Tree (DT), Random Forest (RF), and Support Vector Machine (SVM), were trained and evaluated using low-dimensional feature sets generated via PCA and t-SNE. Among the different classifiers, the RF classifier achieved the best results with PCA features, achieving average results with an accuracy of 98 %, precision of 96.13 %, recall of 96.4 %, F1 score of 96.3 %, and AUC of 99.6 %.},
  archive      = {J_MLA},
  author       = {Ibrahim Abunadi},
  doi          = {10.1016/j.mlwa.2025.100766},
  journal      = {Machine Learning with Applications},
  month        = {12},
  pages        = {100766},
  shortjournal = {Mach. Learn. Appl.},
  title        = {Machine learning techniques for analysing cardiotocography signals for early detection of fetal anomalies based on feature engineering methods},
  volume       = {22},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Hybrid stacked sparse autoencoder for robust feature extraction and classification in sparse data across multiple domains. <em>MLA</em>, <em>22</em>, 100764. (<a href='https://doi.org/10.1016/j.mlwa.2025.100764'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Tabular data is the most used data format in applied mathematics, cybersecurity, finance, and healthcare, and it presents distinct issues due to its intrinsic sparsity, with the majority of values being zero. These factors inhibit effective feature selection and reduce prediction accuracy. The Stacked Sparse Autoencoder (SSAE) model has shown great promise for feature selection in the prediction challenge. However, SSAE struggles to extract meaningful features for sparse data prediction and requires an additional machine learning classifier on the latent space for accurate predictions, thereby increasing the computational complexity. This paper presents a Hybrid-Stacked Sparse Autoencoder (HSSAE) algorithm, with a custom hybrid loss function α ( L 1 ) + ( 1 − α ) L 2 with binary cross-entropy to address these limitations. The proposed algorithm offers a unified framework that seamlessly integrates feature selection and prediction tasks in sparse data to improve feature extraction and reduce the computational complexity of sparse data. Three datasets, with sparsity levels of 43%, 53.32%, and 74.41%, were used in experiments to assess the performance of the HSSAE algorithm. Analyzed using several criteria, the HSSAE algorithm was shown to be much better than conventional SSAE latent space paired with machine learning classifiers such as Logistic Regression (LR), Support Vector Machine (SVM), XGBoost, and AdaBoost. Furthermore, HSSAE also surpasses deep learning algorithms, including Convolutional Neural Networks (CNN), Multilayer Perceptron Networks (MLP), and Recurrent Neural Networks (RNN), establishing its superiority in handling sparse data prediction tasks. The ability of the proposed HSSAE algorithm to generate effective feature selection makes the model robust and suitable for any sparse data applications, especially for sensitive applications such as healthcare and cybersecurity, which require high accuracy in prediction.},
  archive      = {J_MLA},
  author       = {Abdussamad and Said Jadid Abdulkadir and Hanita Daud and Rajalingam Sokkalingam and Iliyas Karim Khan},
  doi          = {10.1016/j.mlwa.2025.100764},
  journal      = {Machine Learning with Applications},
  month        = {12},
  pages        = {100764},
  shortjournal = {Mach. Learn. Appl.},
  title        = {Hybrid stacked sparse autoencoder for robust feature extraction and classification in sparse data across multiple domains},
  volume       = {22},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Toward AI-driven fire imagery: Attributes, challenges, comparisons, and the promise of VLMs and LLMs. <em>MLA</em>, <em>22</em>, 100763. (<a href='https://doi.org/10.1016/j.mlwa.2025.100763'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Despite recent advancements in technology-driven fire management systems, environment continues to grapple with the increasing frequency and severity of wildfires, an issue exacerbated by climate change. A recent example is the devastating California wildfire in January 2025, which burned approximately 182,197 acres, destroyed 16,306 structures, and resulted in a total economic loss of over 275 billion dollars. Such events underscore the urgent need for further investment in intelligent, data-driven fire management solutions. One transformative development in this field has been the deployment of Unmanned Aerial Systems (UAS) for wildfire monitoring and control. These systems capture multimodal imagery and sensor data, facilitating the development of advanced Artificial Intelligence (AI) models for fire detection, spread modeling and prediction, effective suppression, and post-incident damage assessment. Unfortunately, most existing wildfire datasets exhibit significant heterogeneity in terms of imaging modalities (e.g., RGB, thermal, IR), annotation quality, target applications, and geospatial attributes. This diversity often complicates the identification of appropriate datasets for new and emerging wildfire scenarios, which remains a core challenge that hampers progress in the field and limits generalizability and reusability. This paper presents a comprehensive review of prominent wildfire datasets, offering a systematic comparison across various dimensions to help researchers, especially newcomers, select the most suitable datasets for their needs. Additionally, it identifies key parameters to consider when designing and collecting new fire imagery datasets to enhance future usability. Another key contribution of this work is its exploration of how emerging Large Language Models/Vision Language Models (LLMs/VLMs) can catalyze the creation, augmentation, and application of wildfire datasets. We discuss the potential of these models to integrate global knowledge for more accurate fire detection, devise evacuation plans, and support data-driven fire control strategies.},
  archive      = {J_MLA},
  author       = {Sayed Pedram Haeri Boroujeni and Niloufar Mehrabi and Fatemeh Afghah and Connor Peter McGrath and Danish Bhatkar and Mithilesh Anil Biradar and Abolfazl Razi},
  doi          = {10.1016/j.mlwa.2025.100763},
  journal      = {Machine Learning with Applications},
  month        = {12},
  pages        = {100763},
  shortjournal = {Mach. Learn. Appl.},
  title        = {Toward AI-driven fire imagery: Attributes, challenges, comparisons, and the promise of VLMs and LLMs},
  volume       = {22},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Survey of neural network optimization methods for sustainable AI: From data preprocessing to hardware acceleration. <em>MLA</em>, <em>22</em>, 100762. (<a href='https://doi.org/10.1016/j.mlwa.2025.100762'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Technological developments in artificial intelligence and machine learning have recently been integrated into a range of daily objects to improve our lives. However, this progress has increased memory and energy consumption, causing harm to the environment. Addressing this challenge is critical for the well-being of current and future generations and for ensuring sustainability. The reduction of carbon footprint for neural networks is an essential step toward sustainable AI development. We present a structured examination of neural network optimization which covers the entire pipeline from data preprocessing to model design, compression, and hardware efficiency. Unlike prior works that focus on isolated stages, this survey integrates diverse strategies such as data labeling, feature selection, quantization, pruning, knowledge distillation, and approximate adders into a unified framework. This integration provides a cross-stage perspective that reveals synergies and trade-offs often hidden in fragmented studies, while also offering a consolidated reference for researchers through analysis and benchmarking. The novelty lies in combining a literature-wide synthesis with an interactive benchmarking platform that enables side-by-side comparison of optimization methods across metrics and deployment scenarios. A key contribution is the development of a platform compiling results from 139 peer-reviewed studies (81% from 2020 onward), enabling interactive exploration of accuracy, latency, and energy trade-offs. Validation comes from aggregated cross-study analysis, grounding insights in a broad and current evidence base rather than single experiments. This perspective is particularly valuable for guiding sustainable AI development by identifying trade-offs and synergies across optimization stages.},
  archive      = {J_MLA},
  author       = {Omar Ghoneim and Petr Dobias and Olivier Romain},
  doi          = {10.1016/j.mlwa.2025.100762},
  journal      = {Machine Learning with Applications},
  month        = {12},
  pages        = {100762},
  shortjournal = {Mach. Learn. Appl.},
  title        = {Survey of neural network optimization methods for sustainable AI: From data preprocessing to hardware acceleration},
  volume       = {22},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Neural-enhanced two-step modified Newton–Lavrentiev method: A structure-preserving deep learning approach for ill-posed inverse problems. <em>MLA</em>, <em>22</em>, 100761. (<a href='https://doi.org/10.1016/j.mlwa.2025.100761'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Ill-posed inverse problems frequently arise in scientific and medical imaging, where recovering stable and high-fidelity solutions from incomplete or noisy data remains a central challenge. Motivated by this need, we propose a novel hybrid solver framework, the Neural-Enhanced Two-Step Modified Newton–Lavrentiev Method (NE-TSMNLM) , which integrates deep neural corrections into the classical Two-Step Modified Newton–Lavrentiev Method for solving nonlinear inverse problems. Unlike black-box neural operators, our design preserves the convergence structure of the classical iteration while embedding neural modules for adaptive correction, regularization, and convergence prediction. We establish theoretical guarantees on stability and convergence: under mild assumptions, the NE-TSMNLM method inherits the convergence of the classical TSMNLM and improves the effective convergence rate to q ̃ = q 1 + β with β > 0 . This demonstrates the acceleration effect due to neural corrections, which has been theoretically proven. We validate the proposed framework on synthetic and medical inverse problems, including low-dose Computed Tomography (CT) reconstruction, where NE-TSMNLM achieves a 50% radiation dose reduction while maintaining structural fidelity. Initial implementations show promising results with slight degradation (e.g., 17.3% error increase) due to untrained modules and data scarcity. We identify clear pathways for improvement using Transformer-based modules, residual-aware training, and scalable synthetic data. These results position NE-TSMNLM as a structure-preserving neural framework with rigorous mathematical guarantees, bridging classical regularization theory and deep learning for stable, efficient, and interpretable scientific machine learning.},
  archive      = {J_MLA},
  author       = {Suresan Pareth},
  doi          = {10.1016/j.mlwa.2025.100761},
  journal      = {Machine Learning with Applications},
  month        = {12},
  pages        = {100761},
  shortjournal = {Mach. Learn. Appl.},
  title        = {Neural-enhanced two-step modified Newton–Lavrentiev method: A structure-preserving deep learning approach for ill-posed inverse problems},
  volume       = {22},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A novel explainable AI-based design optimization framework to estimate sustainability and economic impacts of reinforced concrete structures. <em>MLA</em>, <em>22</em>, 100760. (<a href='https://doi.org/10.1016/j.mlwa.2025.100760'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Commonly, structures are designed with a focus on safety and serviceability, while structural sustainability is often overlooked at the preliminary design stage. Optimizing a design that considers environmental, economic, and structural factors early in the process requires substantial time and resources. This paper introduces an innovative Explainable Artificial Intelligence (XAI) approach to optimize the environmental and economic impacts of reinforced concrete building designs at the early stage. First, machine learning (ML) models are developed to predict carbon emissions, embodied energy, and life cycle costs based on materials and basic construction information. Then, XAI techniques such as SHAP, PDP, ICE, and LIME are used to identify key input features that influence Life Cycle Assessment (LCA) and Life Cycle Cost Assessment (LCCA). Finally, the counterfactual (CF) technique optimizes design by modifying these key features. The results show that XGBoost is the best-performing model (R 2 = 0.99) for the dataset. XAI analysis identifies material quantity as the most influential variable, with other significant factors including concrete strength, distance to construction and disposal sites, vehicle capacity, and the daily volume of concrete poured. Using these insights, CF optimization reduces both LCA and LCCA by 10–20%, as predefined desired target outcomes. This study demonstrates the potential of XAI and ML to optimize the design process at the preliminary stage, balancing sustainability and economic efficiency.},
  archive      = {J_MLA},
  author       = {Nadeem Iqbal and Khurram Shabbir and Mohamed Noureldin},
  doi          = {10.1016/j.mlwa.2025.100760},
  journal      = {Machine Learning with Applications},
  month        = {12},
  pages        = {100760},
  shortjournal = {Mach. Learn. Appl.},
  title        = {A novel explainable AI-based design optimization framework to estimate sustainability and economic impacts of reinforced concrete structures},
  volume       = {22},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A machine learning approach to vulnerability detection combining software metrics and topic modelling: Evidence from smart contracts. <em>MLA</em>, <em>22</em>, 100759. (<a href='https://doi.org/10.1016/j.mlwa.2025.100759'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper introduces a methodology for software vulnerability detection that combines structural and semantic analysis through software metrics and topic modelling. We evaluate the approach using smart contracts as a case study, focusing on their structural properties and the presence of known security vulnerabilities. We identify the most relevant metrics for vulnerability detection, evaluate multiple machine learning classifiers for both binary and multi-label classification, and improve classification performance by integrating topic modelling techniques. Our analysis shows that metrics such as cyclomatic complexity, nesting depth, and function calls are strongly associated with vulnerability presence. Using these metrics, the Random Forest classifier achieved strong performance in binary classification (AUC: 0.982, accuracy: 0.977, F1-score: 0.808) and multi-label classification (AUC: 0.951, accuracy: 0.729, F1-score: 0.839). The addition of topic modelling using Non-Negative Matrix Factorisation further improved results, increasing the F1-score to 0.881. The evaluation is conducted on Ethereum smart contracts written in Solidity.},
  archive      = {J_MLA},
  author       = {Giacomo Ibba and Rumyana Neykova and Marco Ortu and Roberto Tonelli and Steve Counsell and Giuseppe Destefanis},
  doi          = {10.1016/j.mlwa.2025.100759},
  journal      = {Machine Learning with Applications},
  month        = {12},
  pages        = {100759},
  shortjournal = {Mach. Learn. Appl.},
  title        = {A machine learning approach to vulnerability detection combining software metrics and topic modelling: Evidence from smart contracts},
  volume       = {22},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Prompt design for medical question answering with large language models. <em>MLA</em>, <em>22</em>, 100758. (<a href='https://doi.org/10.1016/j.mlwa.2025.100758'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The combination of prompting technique and the choice of a foundational model determines end-to-end workflow performance on a given task. We aim to provide comprehensive guidance for the best-performing prompting techniques for various LLMs for medical question-answering. We aim to provide comprehensive guidance for the best-performing prompting techniques for a variety of LLM for medical question-answering. We evaluated 15 large LLMs (incl. Claude 3.5 Sonnet, Gemini pro, Llama, Mistral, OpenAI GPT-4o and 4.1) and 6 smaller models (incl. Gemma, Mistral Nemo, Llama 3.1, Gemini flash) across five prompting techniques on neuro-oncology exam questions. Using the established MedQA dataset and a novel neuro-oncology question set, we compared basic prompting, chain-of-thought reasoning, and more complex agent-based methods incorporating external search capabilities. Results showed that the Reasoning and Acting (ReAct) approach combined with giving LLM access to Google Search performed best on large models like Claude 3.5 Sonnet (81.7% accuracy and 85.5% for v2). We also showed that large models significantly outperformed smaller ones on the MedQA dataset (79.3% vs. 51.2% accuracy) and that complex agentic patterns like Language Agent Tree Search provided minimal benefits despite 5x higher latency. We recommend practitioners to experiment with various techniques given their specific use case and foundational model, and favor simple prompting patterns with large models, as they offer the best balance of accuracy and efficiency.},
  archive      = {J_MLA},
  author       = {Leonid Kuligin and Jacqueline Lammert and Aleksandr Ostapenko and Keno Bressem and Martin Boeker and Maximilian Tschochohei},
  doi          = {10.1016/j.mlwa.2025.100758},
  journal      = {Machine Learning with Applications},
  month        = {12},
  pages        = {100758},
  shortjournal = {Mach. Learn. Appl.},
  title        = {Prompt design for medical question answering with large language models},
  volume       = {22},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Matthews correlation coefficient-based feature ranking in recursive ensemble feature selection for high-dimensional and low-sample size data. <em>MLA</em>, <em>22</em>, 100757. (<a href='https://doi.org/10.1016/j.mlwa.2025.100757'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Identifying reliable biomarkers in omics data is challenging due to the high number of features and limited sample sizes, which often lead to overfitting, biased results, and poor reproducibility. These issues are further complicated by class imbalance, common in medical datasets. To address these challenges, we present MCC-REFS, an improved version of the Recursive Ensemble Feature Selection method. MCC-REFS uses the Matthews Correlation Coefficient (MCC) as a selection criterion, offering a more balanced evaluation of classification performance, especially in imbalanced datasets. Unlike traditional methods that require manual tuning or predefined feature counts, MCC-REFS automatically selects the most informative and compact feature sets using an ensemble of eight machine learning classifiers. We evaluated MCC-REFS on synthetic datasets and several real-world omics datasets, including mRNA expression profiles and multi-label breast cancer data. Compared to existing methods such as REFS, GRACES, DNP, and GCNN, MCC-REFS consistently achieved higher or comparable performance while selecting fewer features. Validation using independent classifiers confirmed the robustness of the selected features. Overall, MCC-REFS provides a scalable, flexible, and reliable approach for feature selection in biomedical research, with strong potential for diagnostic and prognostic applications.},
  archive      = {J_MLA},
  author       = {David Rojas-Velazquez and Aletta D. Kraneveld and Alberto Tonda and Alejandro Lopez-Rincon},
  doi          = {10.1016/j.mlwa.2025.100757},
  journal      = {Machine Learning with Applications},
  month        = {12},
  pages        = {100757},
  shortjournal = {Mach. Learn. Appl.},
  title        = {Matthews correlation coefficient-based feature ranking in recursive ensemble feature selection for high-dimensional and low-sample size data},
  volume       = {22},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). DICOMP: Deep reinforcement learning for integer compression. <em>MLA</em>, <em>22</em>, 100756. (<a href='https://doi.org/10.1016/j.mlwa.2025.100756'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper presents DICOMP (Deep Reinforcement Learning for Integer Compression), a novel approach that employs Deep Reinforcement Learning (DRL) to optimize integer compression. DICOMP is the first known approach to apply reinforcement learning specifically to integer compression, filling a significant gap in current research. Unlike traditional methods based on statistical or dictionary techniques, DICOMP formulates compression as a sequential decision-making problem. The core innovation involves a DRL agent that explores various mathematical operations to minimize an integer’s memory size. The discovered optimal strategy was dividing by a set of four prime factors, which effectively transforms its representation into a compact base-4 encoding. This process enables lossless size reduction without relying on hand-crafted strategies. Experiments on diverse datasets show that this invented strategy achieves a reduction in size of more than 80%, outperforming both traditional and other learning-based methods. Despite its learning-based nature, it maintains competitive speed and decompression efficiency, making it practical for use in resource-constrained environments. DICOMP thus represents a significant advancement in intelligent, efficient, and flexible compression techniques.},
  archive      = {J_MLA},
  author       = {Mohamad Khalil Farhat and Ji Zhang and Xiaohui Tao and Tianning Li},
  doi          = {10.1016/j.mlwa.2025.100756},
  journal      = {Machine Learning with Applications},
  month        = {12},
  pages        = {100756},
  shortjournal = {Mach. Learn. Appl.},
  title        = {DICOMP: Deep reinforcement learning for integer compression},
  volume       = {22},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Real-time detection of acoustic anomalies in drone servo motors using edge-based machine learning. <em>MLA</em>, <em>22</em>, 100755. (<a href='https://doi.org/10.1016/j.mlwa.2025.100755'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The growing demand for Unmanned Aerial Vehicles (UAVs) has led to a significant increase in their variety and usage, emphasizing the need for resilient and autonomous onboard monitoring systems. To address this, we present a lightweight, scalable solution for real-time anomaly detection focused on the mechanical servos that control UAV flight dynamics. While conventional deep learning methods offer high accuracy, they often require substantial computational and memory resources, making them unsuitable for the constrained environments of small aircraft. In this study, we introduce a real-time anomaly detection framework that combines edge computing and Internet of Things (IoT) principles to analyze acoustic signals from UAV servo motors. Our system leverages Tiny Machine Learning (TinyML) techniques to perform local data processing and inference directly on embedded hardware, minimizing latency and energy consumption. The proposed method uses a compact neural network deployed on an ultra-lightweight microcontroller (under 100 grams) to classify servo conditions. Acoustic data collected under multiple fault scenarios were minimally preprocessed and fed into the model. Experimental evaluation shows promising performance with 86% accuracy, 86% recall, and 87% precision. This edge-based AI approach supports distributed deployment across UAV fleets, reduces reliance on external infrastructure, and enhances both safety and maintenance efficiency in diverse operational environments.},
  archive      = {J_MLA},
  author       = {Tal Kfir and Sahar Tuvyahu and Boaz Ben Moshe and Or Haim Anidjar},
  doi          = {10.1016/j.mlwa.2025.100755},
  journal      = {Machine Learning with Applications},
  month        = {12},
  pages        = {100755},
  shortjournal = {Mach. Learn. Appl.},
  title        = {Real-time detection of acoustic anomalies in drone servo motors using edge-based machine learning},
  volume       = {22},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Predictive modeling for quality prediction in multi-stage manufacturing systems using artificial intelligence. <em>MLA</em>, <em>22</em>, 100754. (<a href='https://doi.org/10.1016/j.mlwa.2025.100754'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Predicting quality characteristics in multi-stage manufacturing systems (MMSs) poses challenges due to the propagation of variation across stages. In MMSs, any variation introduced at an earlier stage can be amplified in subsequent stages. Many industries rely on in-process quality inspections to monitor and adjust manufacturing processes. Based on inspection outcomes, workers often make process adjustments to maintain product specifications. These adjustments are frequently guided by individual experience rather than systematic methods. This reliance on subjective judgment introduces variability in quality outcomes, as worker evaluations may differ. Moreover, unnecessary adjustments can inadvertently increase variation, further destabilizing the process. This study examines the literature of machine learning algorithms used for quality prediction in MMSs. Selected methods include partial least squares regression, principal component regression, support vector machines with linear and radial basis functions, random forest, k-nearest neighbors XGboost and Feed Forward Neural Network. We applied these techniques to an MMS that produces aircraft engine parts. The process involves intermediate inspections using coordinate measuring machines (CMM). Our predictions rely solely on in-process inspection data, without incorporating process parameters or sensor readings. Historical quality characteristic (QC) data guides the predictions for subsequent stages, including final inspections. This enables proactive quality control and production flow optimization. The results demonstrate that the chosen models can predict the QCs’ values for both consecutive and advanced stages in the MMS. Limitations and future directions are discussed.},
  archive      = {J_MLA},
  author       = {Luis Fernando Agredano Gonzalez and Soumaya Yacout},
  doi          = {10.1016/j.mlwa.2025.100754},
  journal      = {Machine Learning with Applications},
  month        = {12},
  pages        = {100754},
  shortjournal = {Mach. Learn. Appl.},
  title        = {Predictive modeling for quality prediction in multi-stage manufacturing systems using artificial intelligence},
  volume       = {22},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Text-enhanced multimodal deep learning models for predicting chloride transport in concrete. <em>MLA</em>, <em>22</em>, 100753. (<a href='https://doi.org/10.1016/j.mlwa.2025.100753'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Reinforced concrete (RC) structures are widely used in civil engineering, and accurate prediction of chloride transport is essential for durability design and service life estimation. Existing machine learning models for predicting the chloride transport in concrete have primarily relied on researchers' expertise for feature construction. However, the factors affecting the chloride transport are numerous and highly complex, making manual feature engineering inefficient and labor-intensive. This study developed text-enhanced multimodal models that integrate natural language processing (NLP) with deep neural network (DNN) to automatically extract features from textual information, including properties of raw materials, experimental methods, chloride attack mechanisms and comments. The results demonstrate that the developed multimodal models have learned prior knowledge, which enables them to achieve significantly higher accuracy than numerical-data-only DNN models. Among these models, the multi-head self-attention model performs the best by capturing features from multiple angles and enabling parallel computation. Crucially, the text-enhanced multimodal models can maintain high accuracy even with limited numerical data.},
  archive      = {J_MLA},
  author       = {Bingbing Guo and Yujie Jiao and Yan Wang and Fengling Zhang and Yuanfei Guo and Qinghao Guan},
  doi          = {10.1016/j.mlwa.2025.100753},
  journal      = {Machine Learning with Applications},
  month        = {12},
  pages        = {100753},
  shortjournal = {Mach. Learn. Appl.},
  title        = {Text-enhanced multimodal deep learning models for predicting chloride transport in concrete},
  volume       = {22},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Classification of envisioned english speech from EEG using deep learning approaches. <em>MLA</em>, <em>22</em>, 100752. (<a href='https://doi.org/10.1016/j.mlwa.2025.100752'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The ability to use non-invasive technology to recognize imagined speech holds significant potential to enhance the well-being of individuals with motor neuron disease, especially when voluntary muscle control is limited. Despite its potential, this technology is still in its early stages of development, and there remains a significant scope for further research. Therefore, we proposed deep learning approaches to classify 26 English alphabets and 10 digits. To train and evaluate the models, we have collected electroencephalogram (EEG) signals from 20 participants in real-time while they imagined the alphabet and digits. The raw EEG data were processed using the filters, which allowed us to separate various frequency bands: δ , θ , α , β , and γ , as well as combinations of these bands δ + β + γ , and θ + α + β . The dataset was divided into training (80%), validation (10%), and testing (10%) sets and evaluated across three deep learning models: 1D CNN, BiLSTM, and a hybrid 1D CNN–BiLSTM. Among them, the BiLSTM model outperformed the other two models, achieving the highest test accuracy of 85.65% in digit classification within the δ frequency band and 83.65% in alphabet classification within the α frequency band. This research has the potential to facilitate the development of assistive communication technologies for individuals with motor neuron disorders.},
  archive      = {J_MLA},
  author       = {Arman Hossain and Tanvir Hossain Ovi and Mohammed Arif Iftakher Mahmood and Md. Fazlul Kader},
  doi          = {10.1016/j.mlwa.2025.100752},
  journal      = {Machine Learning with Applications},
  month        = {12},
  pages        = {100752},
  shortjournal = {Mach. Learn. Appl.},
  title        = {Classification of envisioned english speech from EEG using deep learning approaches},
  volume       = {22},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Estimating vessel arrival times in global supply chains. <em>MLA</em>, <em>22</em>, 100751. (<a href='https://doi.org/10.1016/j.mlwa.2025.100751'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The global maritime industry, which facilitates around 90% of the world’s trade, faces operational inefficiencies due to inconsistent Automatic Identification System (AIS) data and scalability challenges, limiting the reliability of current tracking systems. This study aims to improve the reliability of both Estimated Time of Arrival (ETA) and vessel destination predictions. We apply Machine Learning (ML) techniques to predict ETAs through both regression and classification models, while leveraging high-order Markov chains for destination prediction based on sequential maritime route patterns. To ensure the practical applicability of these models, we developed an end-to-end pipeline that incorporates waypoint-based trajectory compression, optimizing the handling of satellite-based AIS data (s-AIS). Using a real-world dataset of global shipping routes, our ML models, particularly Support Vector Regression (SVR), achieved a lower mean absolute error than captain-provided ETAs (16.01 vs. 22.15 h). When the time to arrival was more than 75 h, SVR outperformed captain-provided ETAs, whereas captain-provided ETAs were more accurate in the final three days before arrival, due to frequent manual updates. High-order Markov chains achieved a near-perfect accuracy of 99.00% (std: 1.19%) in destination prediction, confirming the regularity of cargo ship routes. These findings demonstrate the potential of combining ML models with Markov chains to enhance the accuracy and reliability of long-term maritime logistics forecasting, transforming raw s-AIS data into actionable insights for improved operational decision-making.},
  archive      = {J_MLA},
  author       = {Mathijs Pellemans and Jesper Slik and Sandjai Bhulai},
  doi          = {10.1016/j.mlwa.2025.100751},
  journal      = {Machine Learning with Applications},
  month        = {12},
  pages        = {100751},
  shortjournal = {Mach. Learn. Appl.},
  title        = {Estimating vessel arrival times in global supply chains},
  volume       = {22},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Impact of global indices on forecasting the S&P 500 index. <em>MLA</em>, <em>22</em>, 100750. (<a href='https://doi.org/10.1016/j.mlwa.2025.100750'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This study presents a hybrid Random Forest–Long Short-Term Memory (RF-LSTM) framework for forecasting the S&P 500 Index, utilizing daily data from 25 global stock indices spanning 2003 to 2024. By combining Random Forest’s feature selection with LSTM’s deep temporal modeling, the approach reveals significant geographic asymmetries in predictive influence, with North America contributing 49%, East Asia 31%, ASEAN–Oceania 10%, Europe 6%, South Asia 3%, and Latin America 1%. At the index level, the Dow Jones Industrial Average (42%), KOSPI (18%), and Russell 2000 (15%) are identified as primary predictors, highlighting both domestic and international spillover effects. Optimized using Bayesian Optimisation, the RF-LSTM model achieves superior out-of-sample performance with an R² of 0.9952, RMSE of 16.85, MAE of 13.29, and MAPE of 4.72%, reflecting error reductions of up to 32.5% compared to baseline LSTM models. The Random Forest’s permutation importance effectively isolates high-impact indices, reducing noise and dimensionality to enhance temporal modeling accuracy. In our implementation, the residual variation left after Random Forest feature selection is further modeled using LSTM, consistent with a residual-hybrid forecasting design. These findings offer investors a robust, geographically informed tool for portfolio optimization and provide policymakers with insights into global risk transmission. The results underscore the efficacy of integrating interpretable feature selection with deep learning to advance financial forecasting in a globally interconnected market.},
  archive      = {J_MLA},
  author       = {Seyed Mostafa Mostafavi and Ali Reza Hooman},
  doi          = {10.1016/j.mlwa.2025.100750},
  journal      = {Machine Learning with Applications},
  month        = {12},
  pages        = {100750},
  shortjournal = {Mach. Learn. Appl.},
  title        = {Impact of global indices on forecasting the S&P 500 index},
  volume       = {22},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Longitudinal abuse and sentiment analysis of hollywood movie dialogues using language models. <em>MLA</em>, <em>22</em>, 100749. (<a href='https://doi.org/10.1016/j.mlwa.2025.100749'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Over the past decades, there has been an increase in the prevalence of abusive and violent content in Hollywood movies. In this study, we use language models to explore the longitudinal abuse and sentiment analysis of Hollywood Oscar and blockbuster movie dialogues from 1950 to 2024. We provide an analysis of subtitles for over a thousand movies, which are categorised into four genres. We employ fine-tuned language models to examine the trends and shifts in emotional and abusive content over the past seven decades. Findings reveal significant temporal changes in movie dialogues, which reflect broader social and cultural influences. Overall, the emotional tendencies in the films are diverse, and the detection of abusive content also exhibits significant fluctuations. The results show a gradual rise in abusive content in recent decades, reflecting social norms and regulatory policy changes. Genres such as thrillers still present a higher frequency of abusive content that emphasises the ongoing narrative role of violence and conflict. At the same time, underlying positive emotions such as humour and optimism remain prevalent in most of the movies. Furthermore, the gradual increase of abusive content in movie dialogues has been significant over the last two decades, where Oscar-nominated movies overtook the top ten blockbusters.},
  archive      = {J_MLA},
  author       = {Rohitash Chandra and Guoxiang Ren},
  doi          = {10.1016/j.mlwa.2025.100749},
  journal      = {Machine Learning with Applications},
  month        = {12},
  pages        = {100749},
  shortjournal = {Mach. Learn. Appl.},
  title        = {Longitudinal abuse and sentiment analysis of hollywood movie dialogues using language models},
  volume       = {22},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Evolutionary AdaBoost ensemble: A machine learning framework for depression detection. <em>MLA</em>, <em>22</em>, 100748. (<a href='https://doi.org/10.1016/j.mlwa.2025.100748'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Depression is a prevalent and debilitating mental health disorder that often goes undiagnosed due to the lack of accessible, objective screening tools. This paper introduces EVAdaBoost, an Evolutionary AdaBoost ensemble framework designed for automated depression detection from voice signals. The method leverages a diverse set of signal processing techniques—including Fourier, Wavelet, Walsh, Hilbert–Huang, and OpenSmile, as well as time–frequency transformations for convolutional neural networks (CNNs). Each feature set is used to train a specialised AdaBoost ensemble, with Broad Learning Systems (BLS) serving as efficient weak learners. A key innovation of EVAdaBoost is its use of a quantum-inspired evolutionary algorithm to optimise the feature subsets assigned to each AdaBoost model. Instead of using all extracted features, which may include noise, redundancy, and irrelevant data, EVAdaBoost evolves to select diverse and high-performing subsets of features for each AdaBoost base learner, automatically discarding non-informative features. This evolutionary selection enhances both classification accuracy and computational efficiency. Additionally, an evolutionary pruning algorithm is employed to find the optimal subset of AdaBoost algorithms that offer the best performance at reduced computational cost. Experiments across nine feature types and multiple benchmark classifiers show that EVAdaBoost consistently outperforms state-of-the-art methods in accuracy, sensitivity (TPR), specificity (TNR), and precision (PPV). The results underscore the potential of hybrid evolutionary ensemble learning for non-invasive, speech-based mental health screening.},
  archive      = {J_MLA},
  author       = {Ruhollah Sayeri and Behnam Barzegar and Yaser Bozorgi rad and Nasser Mikaeilvand and Mohammad Hassan Tayarani Najaran},
  doi          = {10.1016/j.mlwa.2025.100748},
  journal      = {Machine Learning with Applications},
  month        = {12},
  pages        = {100748},
  shortjournal = {Mach. Learn. Appl.},
  title        = {Evolutionary AdaBoost ensemble: A machine learning framework for depression detection},
  volume       = {22},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Optimization of genomic breeding value prediction for growth traits in rongchang pigs through machine learning techniques. <em>MLA</em>, <em>22</em>, 100747. (<a href='https://doi.org/10.1016/j.mlwa.2025.100747'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Background The increasing volume of genome sequencing data poses significant challenges for traditional genome-wide prediction methods in handling large datasets. Machine learning (ML) techniques are well-suited for processing high-dimensional data, and offer promising solutions. This study aimed to identify an optimal genome-wide prediction approach for local pig breeds using 10 datasets with varying single nucleotide polymorphism (SNP) densities, derived from imputed sequencing data of 485 Rongchang pigs and the results of genome-wide association studies (GWAS). Three growth traits, namely, backfat (BF) thickness, loin and thoracic height (LTH), and girth circumference (GC), were predicted using six traditional methods and six ML-based methods, including Kernel Ridge Regression (KRR), Support Vector Regression (SVR), Random Forest, Gradient Boosting Decision Tree, Light Gradient Boosting Machine, and Adaboost. Results The efficacy of the different methods was evaluated using a five-fold cross-validation strategy and independent tests. The predictive performance of both the traditional and ML-based methods was initially enhanced through the incorporation of significantly associated SNPs and weighted data, with the KRR method exhibiting exceptional resistance to overfitting at a SNP density of 300,000. The ML-based methods outperformed the traditional methods, with improvements of 6.6–8.1 %. The integration of GWAS data enhanced the prediction accuracy of the ML-based methods. KRR and Gradient Boosting Decision Tree demonstrated significant computational efficiency, indicating their potential as promising strategies for genomic prediction in livestock breeding. Conclusions This study provides a comprehensive analysis of genome-wide predictions in Rongchang pigs, and highlights the potential of ML-based techniques in enhancing prediction accuracy and efficiency. The study provides valuable insights into GP and holds key implications for advancing genome breeding practices in local pig breeds.},
  archive      = {J_MLA},
  author       = {Pingxian Wu and Junge Wang and Xinyou Chen and Tao Wang and Zongyi Guo and Shuqi Diao and Jinyong Wang},
  doi          = {10.1016/j.mlwa.2025.100747},
  journal      = {Machine Learning with Applications},
  month        = {12},
  pages        = {100747},
  shortjournal = {Mach. Learn. Appl.},
  title        = {Optimization of genomic breeding value prediction for growth traits in rongchang pigs through machine learning techniques},
  volume       = {22},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A laplace diffusion-based transformer model for heart rate forecasting within daily activity context. <em>MLA</em>, <em>22</em>, 100746. (<a href='https://doi.org/10.1016/j.mlwa.2025.100746'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the advent of wearable Internet of Things (IoT) devices, remote patient monitoring (RPM) emerged as a promising solution for managing heart failure. However, the heart rate can fluctuate significantly due to various factors, and without correlating it to the patient's actual physical activity, it becomes difficult to assess whether changes are significant. Although Artificial Intelligence (AI) models may enhance the accuracy and contextual understanding of remote heart rate monitoring, the integration of activity data is still rarely addressed. In this paper, we propose a Transformer model combined with a Laplace diffusion technique to model heart rate fluctuations driven by physical activity of the patient. Unlike prior models that treat activity as secondary, our approach conditions the entire modeling process on activity context using specialized embeddings and attention mechanisms to prioritize activity specific historical patents. The model captures both long-term patterns and activity-specific heart rate dynamics by incorporating contextualized embeddings and dedicated encoder. The Transformer model was validated on a real-world dataset collected from 29 patients over a 4-month period. Experimental results show that our model outperforms current state-of-the-art methods, achieving a 43 % reduction in mean absolute error compared to the considered baseline models. Moreover, the coefficient of determination R 2 is 0.97 indicating the model predicted heart rate is in strong agreement with actual heart rate values. These findings suggest that the proposed model can be a practical and effective tool for supporting both healthcare providers and remote patient monitoring systems.},
  archive      = {J_MLA},
  author       = {Andrei Mateescu and Ioana Hadarau and Ionut Anghel and Tudor Cioara and Ovidiu Anchidin and Ancuta Nemes},
  doi          = {10.1016/j.mlwa.2025.100746},
  journal      = {Machine Learning with Applications},
  month        = {12},
  pages        = {100746},
  shortjournal = {Mach. Learn. Appl.},
  title        = {A laplace diffusion-based transformer model for heart rate forecasting within daily activity context},
  volume       = {22},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A comprehensive survey on deep reinforcement learning in object tracking. <em>MLA</em>, <em>22</em>, 100745. (<a href='https://doi.org/10.1016/j.mlwa.2025.100745'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The exploration of Deep Reinforcement Learning (DRL) in Object Tracking (OT) represents an emerging paradigm and is gaining traction as an alternative to conventional CNN-based methods. DRL’s ability to integrate spatial and temporal context and learn from interactions makes it particularly suited for the sequential decision-making required in OT. The survey reviews a range of DRL-based methods for OT, systematically collating and analyzing existing research to highlight trends and challenges. It also provides an evaluation of different DRL algorithms, categorizing them based on their performance in various dynamic environments. Additionally, we analyze existing evaluation benchmarks and simulators, along with the challenges, potential solutions, and trends in DRL-based OT methods. This paper aims to bridge the fragmented literature on DRL applications in OT, providing a unified view that identifies common approaches, challenges, and potential synergies.},
  archive      = {J_MLA},
  author       = {Hy Nguyen and Srikanth Thudumu and Hung Du and Rajesh Vasa and Kon Mouzakis},
  doi          = {10.1016/j.mlwa.2025.100745},
  journal      = {Machine Learning with Applications},
  month        = {12},
  pages        = {100745},
  shortjournal = {Mach. Learn. Appl.},
  title        = {A comprehensive survey on deep reinforcement learning in object tracking},
  volume       = {22},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Predicting daily stock price directions with deep learning models. <em>MLA</em>, <em>22</em>, 100744. (<a href='https://doi.org/10.1016/j.mlwa.2025.100744'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, deep learning models like LSTM, CNN and RNN were explored to predict the direction of daily stock price changes. Since stocks in the same industry sector are highly correlated, we propose to replace individual stock with their corresponding Exchange Traded Sector Funds (ETFs) and S&P 500. We show that this replacement of individual stocks by the corresponding ETF can be justified due to high degree of returns correlation and small Hamming distances of trading signals across both input and output for stocks in the same sector. We considered historical daily data spanning 25 years (2000 to 2024) on major sector ETFs, some of their components and the S&P-500 index. Our results show that LSTM consistently outperformed CNN and RNN and traditional machine learning models. As a trading strategy, LSTM significantly out-performed buy-and-hold strategy for stocks in the Technology and Consumer Durables sectors, but offered very minor improvement for stocks in other sectors. Our results suggest that predicting daily stock price directions with deep learning models should not be used for most stocks unless these stocks are in Technology or Consumer Durables sectors and LSTM-based models are used.},
  archive      = {J_MLA},
  author       = {Triparna Kundu and Eugene Pinsky},
  doi          = {10.1016/j.mlwa.2025.100744},
  journal      = {Machine Learning with Applications},
  month        = {12},
  pages        = {100744},
  shortjournal = {Mach. Learn. Appl.},
  title        = {Predicting daily stock price directions with deep learning models},
  volume       = {22},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Evaluating the impact of model scale and prompting strategies on corruption allegation classification using thai-specialized typhoon2 language models. <em>MLA</em>, <em>22</em>, 100743. (<a href='https://doi.org/10.1016/j.mlwa.2025.100743'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Corruption complaint classification is a critical yet resource-intensive task in public sector governance, particularly in low-resource linguistic environments. This study assesses the capacity of Thai-specialized large language models (LLMs) from the Typhoon2 family to automate the classification of corruption complaints submitted to Thailand’s National Anti-Corruption Commission (NACC). Three variants—Typhoon2–3B (base), Typhoon2–3B (fine-tuned), and Typhoon2–7B (base)—were evaluated under zero-shot, one-shot, and two-shot prompting strategies and benchmarked against strong traditional machine learning models (Random Forest, XGBoost) trained on TF-IDF features. Results reaffirm the competitiveness of tree-based classifiers, which delivered consistently high and stable performance. Among the LLMs, the Typhoon2–7B model with two-shot prompting achieved the most balanced performance (Macro F1 = 0.514), highlighting emergent few-shot reasoning capabilities and improved handling of class imbalance. By contrast, fine-tuning the smaller 3B model induced severe overfitting and significant degradation on minority classes. These outcomes emphasize that model scale and prompt design are more reliable drivers of performance than direct fine-tuning in small, imbalanced settings. The study contributes practical guidance for deploying scalable and ethically aligned AI in governance, demonstrating that while traditional models remain robust benchmarks, large-scale prompted LLMs represent a promising complement for future public sector innovation.},
  archive      = {J_MLA},
  author       = {Patipan Sriphon and Pattrawut Khunwipusit and Bamisaye Mayowa Emmanuel and Issara Sereewatthanawut},
  doi          = {10.1016/j.mlwa.2025.100743},
  journal      = {Machine Learning with Applications},
  month        = {12},
  pages        = {100743},
  shortjournal = {Mach. Learn. Appl.},
  title        = {Evaluating the impact of model scale and prompting strategies on corruption allegation classification using thai-specialized typhoon2 language models},
  volume       = {22},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Performance evaluation of complex-valued neural networks on real and complex-valued classification and reconstruction tasks. <em>MLA</em>, <em>22</em>, 100742. (<a href='https://doi.org/10.1016/j.mlwa.2025.100742'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Complex-Valued Neural Networks (CVNNs) are reported to be more efficient in different applications than Real-Valued Neural Networks (RVNNs) in many papers. In this study, we aim to characterize the cases when it holds true in order to assist the selection of proper tools for two specific tasks: classification and reconstruction. Among the various ways to compare CVNNs and RVNNs, we apply the one based on the number of parameters of the respective Neural Networks (NNs), assuming that a complex parameter is composed of two real ones. The performed experimentation revealed many surprising differences in the performance of CVNNs and RVNNs compared to the ones discussed in the preceding literature. This drives us to the general conclusion that the performance of RVNNs is similar or better than the performance of CVNNs in the majority of the cases, and the seldom cases when CVNNs achieve better performance are hard to characterize.},
  archive      = {J_MLA},
  author       = {Mahmood K.M. Almansoori and Miklos Telek},
  doi          = {10.1016/j.mlwa.2025.100742},
  journal      = {Machine Learning with Applications},
  month        = {12},
  pages        = {100742},
  shortjournal = {Mach. Learn. Appl.},
  title        = {Performance evaluation of complex-valued neural networks on real and complex-valued classification and reconstruction tasks},
  volume       = {22},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Hybrid machine learning model for the prediction of anaemia. <em>MLA</em>, <em>22</em>, 100741. (<a href='https://doi.org/10.1016/j.mlwa.2025.100741'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In developing countries like Tanzania, despite the national intervention, the proportion of anaemic children aged 6–59 months is seen to be high, with a prevalence of 59 %. Traditional methods, such as examining paleness in the eyes and tongue, are commonly used, but are subjective and often lead to delayed or missed diagnoses. While existing Machine learning models have attempted to predict anaemia in children and offer improved accuracy, many rely on single-model strategies and a default threshold of 0.5, which tends to favour sensitivity over specificity, leading to a high number of false positives. The study used the supervised machine learning approach within the CRISP-DM framework. A stacked hybrid approach was used, integrating Random Forest (RF) and Artificial Neural Network (ANN) as base models, using XGBoost as a meta-learner. The model's performance was evaluated using metrics such as accuracy, sensitivity, specificity, precision, and Area Under the Curve (AUC) with 95 % confidence intervals (CIs) across thresholds of 0.35, 0.4, 0.45, and 0.5, optimized using Youden’s J index. The hybrid model achieved balanced performance, especially at a 0.4 threshold with a sensitivity of 0.861 and a specificity of 0.880. Compared to standalone models, the hybrid approach outperformed in reducing false positives and false negatives, offering greater reliability and clinical safety. This study concludes that the stacking ensemble approach, along with threshold optimization, provides an effective solution for early detection of anaemia in children. Integration of hybrid Machine Learning models into Tanzania’s health screening programs could improve child health outcomes.},
  archive      = {J_MLA},
  author       = {Rabia Omar Said and Mahadia Tunga},
  doi          = {10.1016/j.mlwa.2025.100741},
  journal      = {Machine Learning with Applications},
  month        = {12},
  pages        = {100741},
  shortjournal = {Mach. Learn. Appl.},
  title        = {Hybrid machine learning model for the prediction of anaemia},
  volume       = {22},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Enhanced prediction of karst spring discharge using a hybrid LSTM-XGBoost model optimized with grid search. <em>MLA</em>, <em>22</em>, 100740. (<a href='https://doi.org/10.1016/j.mlwa.2025.100740'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Globally, intensifying droughts taxed water supplies, particularly in karst areas where it is difficult to predict spring discharge due to complex hydrology. Data-driven models represent a viable alternative, with the significance of karst aquifers to freshwater production. To enhance the accuracy of spring discharge prediction, this study introduces a new LSTM-XGBoost hybrid model for more accurate karst spring discharge prediction in Chaharmahal Bakhtiari Province, Iran. The hybrid model exploits the benefits of LSTM in capturing temporal dependency and the strength of XGBoost in modeling nonlinear relationships, and Grid Search is utilized for tuning hyperparameters. The performance of the LSTM-XGBoost model is compared with the optimized ML models. The study utilizes a dataset of 3,266 day, month, and spring discharge records of the Dehghara Springs. The results depict the excellence of the suggested LSTM-XGBoost hybrid model with the highest test R 2 = 0.8798, Explained Variance (EV) = 0.8857, and the lowest error metrics (MAE = 0.3355, RMSE = 0.5795, MAPE = 21.84%). The hybrid model outperforms both the baseline traditional and Deep Learning (DL). Feature importance analysis reveals that seasonal factors, particularly the month with an importance score of 0.919, have a significantly greater impact on spring discharge than daily variations. The proposed LSTM-XGBoost hybrid model provides a reliable and accurate tool for karst spring discharge prediction, offering valuable insights for water resource management in regions affected by climate change and increasing water demand.},
  archive      = {J_MLA},
  author       = {Xiaomei Liu},
  doi          = {10.1016/j.mlwa.2025.100740},
  journal      = {Machine Learning with Applications},
  month        = {12},
  pages        = {100740},
  shortjournal = {Mach. Learn. Appl.},
  title        = {Enhanced prediction of karst spring discharge using a hybrid LSTM-XGBoost model optimized with grid search},
  volume       = {22},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Forecasting political voting: A high dimensional machine learning approach. <em>MLA</em>, <em>22</em>, 100739. (<a href='https://doi.org/10.1016/j.mlwa.2025.100739'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present a novel machine learning approach to predict voting patterns in Brazil’s Chamber of Deputies. Using a high-dimensional dataset and a time-series methodology, our models aim to accurately forecast legislative decisions. Unlike prior studies that often focus on single ideological dimensions, our approach integrates a broad feature set, including party guidelines, proposition characteristics, and deputy voting history, to improve predictive power. We train time-series models for each legislature, comparing ensembles like Random Forests and Gradient Boosting, which are validated using three-fold chronological splits to ensure temporal integrity. Our analysis highlights the significant influence of party guidelines and pork-barrel politics on voting behavior. Additionally, we identify key predictors, including the theme and source of the legislative proposition, as well as the deputies’ voting history. This work demonstrates the feasibility of accurately forecasting legislative votes, offering a valuable tool for stakeholders to anticipate legislative outcomes and enhancing the transparency of the political process.},
  archive      = {J_MLA},
  author       = {Pedro Caiua Campelo Albuquerque and Daniel Oliveira Cajueiro},
  doi          = {10.1016/j.mlwa.2025.100739},
  journal      = {Machine Learning with Applications},
  month        = {12},
  pages        = {100739},
  shortjournal = {Mach. Learn. Appl.},
  title        = {Forecasting political voting: A high dimensional machine learning approach},
  volume       = {22},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Enhancing IDS performance through a comparative analysis of random forest, XGBoost, and deep neural networks. <em>MLA</em>, <em>22</em>, 100738. (<a href='https://doi.org/10.1016/j.mlwa.2025.100738'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Intrusion Detection Systems (IDS) face major challenges in network security, notably the need to combine a high detection rate with reliable performance. This reliability is often affected by class imbalances and inadequate hyperparameter optimization. This article addresses the issue of improving the detection rate of IDS by evaluating and comparing three machine learning algorithms: Random Forest (RF), XGBoost, and Deep Neural Networks (DNN), using the NSL-KDD dataset. In our methodology, we integrate SMOTE (Synthetic Minority Oversampling Technique) to tackle the unbalanced nature of the data, ensuring a more balanced representation of the different classes. This approach helps optimize model performance, reduce bias, and enhance robustness. Additionally, hyperparameter optimization is performed using Optuna, ensuring that each algorithm operates at its optimal level. The results show that our model, using the Random Forest algorithm, achieves an accuracy of 99.80%, surpassing the performance of XGBoost and Deep Neural Networks (DNN). This makes our approach a true asset for intrusion detection methods in computer networks.},
  archive      = {J_MLA},
  author       = {Sow Thierno Hamidou and Adda Mehdi},
  doi          = {10.1016/j.mlwa.2025.100738},
  journal      = {Machine Learning with Applications},
  month        = {12},
  pages        = {100738},
  shortjournal = {Mach. Learn. Appl.},
  title        = {Enhancing IDS performance through a comparative analysis of random forest, XGBoost, and deep neural networks},
  volume       = {22},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multi-source plume tracing via multi-agent reinforcement learning under common UAV-faults. <em>MLA</em>, <em>22</em>, 100737. (<a href='https://doi.org/10.1016/j.mlwa.2025.100737'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Hazardous airborne gas releases from accidents, leaks, or wildfires require rapid localization of emission sources under uncertain and turbulent conditions. Traditional gradient-based or biologically inspired strategies struggle in multi-source environments where odor cues are intermittent, aliased, and partially observed. We address this challenge by formulating multi-source plume tracing in three-dimensional fields as a cooperative partially observable Markov game. To solve it, we introduce an Action-Specific Double Deep Recurrent Q-Network (ADDRQN) that conditions on action–observation pairs to improve latent-state inference, and integrates teammate information through a permutation-invariant set encoder. Training follows a randomized centralized-training and decentralized-execution regime with host randomization, team-size variation, and noise injection. This yields a policy that is robust to agent failures (hardware malfunction, battery depletion, etc.), resilient to intermittent communication blackouts, and tolerant of sensor noise. Empirical evaluation in simulated Gaussian plume environments shows that ADDRQN achieves higher success rates and shorter localization times than non-action baselines, maintains strong performance under mid-mission disruptions, and scales predictably with team size.},
  archive      = {J_MLA},
  author       = {Pedro Antonio Alarcon Granadeno and Theodore Chambers and Jane Cleland-Huang},
  doi          = {10.1016/j.mlwa.2025.100737},
  journal      = {Machine Learning with Applications},
  month        = {12},
  pages        = {100737},
  shortjournal = {Mach. Learn. Appl.},
  title        = {Multi-source plume tracing via multi-agent reinforcement learning under common UAV-faults},
  volume       = {22},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Structure-aware stable diffusion for traditional architectural decoration design. <em>MLA</em>, <em>22</em>, 100735. (<a href='https://doi.org/10.1016/j.mlwa.2025.100735'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The intelligent generation of traditional architectural styles faces significant challenges in structural integrity and style consistency. While existing methods can generate numerous realistic images, they lack a deep understanding of structural elements in traditional architectural decorative design. This paper proposes a Structure-aware Stable Diffusion (SSD) model, which enhances the model's comprehension of architectural features through three key innovations. First, we design a structure-aware feature injection module that adaptively fuses extracted architectural structural information with original features during the U-net upsampling phase, enhancing the model's understanding of geometric structures. Second, we introduce a dual-path text enhancement strategy that combines structural descriptions with original descriptions to provide richer textual guidance signals for the generation process. Finally, we design a progressive injection strategy that dynamically controls the injection intensity of structural information through cosine scheduling, ultimately achieving effective internalization of structural knowledge. Experimental results show that compared to existing methods, our model effectively improves both the diversity of generated traditional architectural decorations and the rationality of their structures, thus providing an effective new technical approach for traditional architectural decorative design.},
  archive      = {J_MLA},
  author       = {Jianhong Yang and Guoyong Wang},
  doi          = {10.1016/j.mlwa.2025.100735},
  journal      = {Machine Learning with Applications},
  month        = {12},
  pages        = {100735},
  shortjournal = {Mach. Learn. Appl.},
  title        = {Structure-aware stable diffusion for traditional architectural decoration design},
  volume       = {22},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Feature importance analysis of optimized machine learning modeling for predicting customers satisfaction at the united states airlines. <em>MLA</em>, <em>22</em>, 100734. (<a href='https://doi.org/10.1016/j.mlwa.2025.100734'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Customer experience is crucial in the airline industry, as understanding passenger satisfaction helps airlines improve service quality. This study evaluates hyperparameter optimization and feature interpretability in machine learning models for predicting airline passenger satisfaction. Support Vector Machine (SVM) and Multilayer Perceptron (MLP) models were tested for binary classification, labeling passengers as ‘Satisfied’ or ‘Neutral or Dissatisfied’ using a Kaggle dataset with ∼104,000 training and ∼26,000 test records. Hyperparameter tuning used grid search with 10-fold cross-validation. For SVM, the optimal setup included the RBF kernel, C = 10, and gamma = ‘auto’, achieving a mean score of 0.9606. For MLP, the best configuration used no regularization, "he" initialization, ReLU activation, 30 epochs, batch size of 32, two hidden layers with 32 neurons each, and a learning rate of 0.001, yielding a mean score of 0.9556. Performance metrics included accuracy, precision, recall, and F1-Score, with SVM achieving a test accuracy of 0.96, precision of 0.97, and F1-Score of 0.95, slightly outperforming MLP by <1 %, though MLP was faster at 0.3 s versus SVM’s 18 s. Both models surpassed baseline models and prior studies, benefiting from robust preprocessing and a large dataset. Permutation importance analysis identified Type of Travel, Inflight Wi-Fi Service, Customer Type, and Online Boarding as key predictors, emphasizing passenger needs for digital connectivity and personalized services. These insights guide airlines to prioritize reliable Wi-Fi and efficient online boarding to enhance satisfaction, loyalty, and competitive positioning.},
  archive      = {J_MLA},
  author       = {Hamid Mirzahossein and Soheil Rezashoar},
  doi          = {10.1016/j.mlwa.2025.100734},
  journal      = {Machine Learning with Applications},
  month        = {12},
  pages        = {100734},
  shortjournal = {Mach. Learn. Appl.},
  title        = {Feature importance analysis of optimized machine learning modeling for predicting customers satisfaction at the united states airlines},
  volume       = {22},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Conformal validation: A deferral policy using uncertainty quantification with a human-in-the-loop for model validation. <em>MLA</em>, <em>22</em>, 100733. (<a href='https://doi.org/10.1016/j.mlwa.2025.100733'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Validating performance is a key challenge facing the adoption of machine learning models in high risk applications. Current validation methods assess performance marginally over the entire testing dataset, which can fail to identify regions in the distribution with insufficient performance. In this paper, we propose Conformal Validation, a systems-based approach with a calibrated form of uncertainty quantification using a conformal prediction framework as a part of the validation process to reduce performance gaps. Specifically, the policy defers a subset of observations for which the predictive model is most uncertain and provides a human with informative prediction sets to make the ancillary decision. We evaluate this policy on an image classification task where images are distorted with varying levels of gaussian blur for a quantifiable measure of added difficulty. The model is compared to human performance on the most difficult observations, i.e., those where the model is most uncertain, to simulate the scenario when a human is the alternative decision-maker. We evaluate performance on three arms: the model independently, humans with access to a set of classes the model is most confident in, and humans independently. The deferral policy is simple to understand, applicable to any predictive model, and easy to implement while, in this case, keeping humans in the loop for improved trustworthiness. Conformal Validation incorporates a risk assessment that is conditioned on the prediction set length and can be tuned to the needs of the application.},
  archive      = {J_MLA},
  author       = {Paul Horton and Alexandru Florea and Brandon Stringfield},
  doi          = {10.1016/j.mlwa.2025.100733},
  journal      = {Machine Learning with Applications},
  month        = {12},
  pages        = {100733},
  shortjournal = {Mach. Learn. Appl.},
  title        = {Conformal validation: A deferral policy using uncertainty quantification with a human-in-the-loop for model validation},
  volume       = {22},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Combining style and semantics for robust authorship verification. <em>MLA</em>, <em>22</em>, 100732. (<a href='https://doi.org/10.1016/j.mlwa.2025.100732'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Authorship Verification is a key task in Natural Language Processing, essential for applications like plagiarism detection and content authentication. This paper analyzes the use of deep learning models for Authorship Verification, focusing on combining semantic and style features to enhance model performance. We propose three models: the Feature Interaction Network, Pairwise Concatenation Network, and Siamese Network, which aim to determine if two texts are written by the same author. Each model uses RoBERTa embeddings to capture semantic content and incorporates style features such as sentence length, word frequency, and punctuation to differentiate authors based on writing style. Our results confirm that incorporating style features consistently improves model performance, with the extent of improvement varying by architecture. This demonstrates the value of combining semantic and stylistic information for Authorship Verification. While limitations such as RoBERTa’s fixed input length and the use of predefined style features exist, they do not hinder model effectiveness and point to clear opportunities for future enhancement through extended input handling and dynamic style feature extraction. In contrast to prior studies such as Bevendorff et al., (2020) and Kestemont, et al., (2022), which relied on balanced and homogeneous datasets with consistent topics and well-formed language, our work evaluates models on a more challenging, imbalanced, and stylistically diverse dataset, better reflecting real-world Authorship Verification conditions. Despite the increased difficulty, our models achieve competitive results, underscoring their robustness and practical applicability. These findings support the value of combining semantic and style features for real-world Authorship Verification.},
  archive      = {J_MLA},
  author       = {Britt van Leeuwen and Sandjai Bhulai and Rob van der Mei},
  doi          = {10.1016/j.mlwa.2025.100732},
  journal      = {Machine Learning with Applications},
  month        = {12},
  pages        = {100732},
  shortjournal = {Mach. Learn. Appl.},
  title        = {Combining style and semantics for robust authorship verification},
  volume       = {22},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Implementation of machine learning technologies in construction maintenance: A strategic analysis. <em>MLA</em>, <em>22</em>, 100731. (<a href='https://doi.org/10.1016/j.mlwa.2025.100731'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Current predictive maintenance systems in construction rely on static machine learning approaches that fail to adapt to evolving operational environments, achieving only 3%–7% performance improvements over individual models and suffering 15%–25% performance degradation when transferred across domains. This research develops and validates an Adaptive Ensemble Framework that dynamically optimizes algorithm selection through real-time data assessment and performance feedback. The framework’s meta-learning architecture continuously adapts ensemble weights using data complexity measures, temporal pattern analysis, and uncertainty quantification metrics. Unlike static approaches, the system integrates scikit-learn and TensorFlow models through dynamic optimization algorithms that respond to changing conditions without manual reconfiguration. The framework provides uncertainty-aware predictions with confidence intervals essential for safety-critical construction decisions. Comprehensive evaluation across four industries using 50,000+ maintenance records from major construction firms demonstrates substantial improvements. The adaptive ensemble achieves F1-score of 0.934 in construction delay prediction, representing 15.3% improvement over individual models and 8.7% enhancement over static ensembles. Cross-industry validation reveals successful knowledge transfer with minimal performance degradation ( < 5%). This research contributes three scholarly advances: (i) the first real-time adaptive ensemble framework eliminating manual hyperparameter tuning, (ii) uncertainty quantification mechanisms for safety-critical applications, and (iii) robust cross-industry transferability through systematic domain adaptation. The framework extends beyond construction to manufacturing, energy, and transportation sectors, demonstrating computational efficiency with sub-100ms latency and linear scaling characteristics. These contributions establish new benchmarks for adaptive machine learning in industrial predictive maintenance.},
  archive      = {J_MLA},
  author       = {Assane Lo and Aysha Alshehhi},
  doi          = {10.1016/j.mlwa.2025.100731},
  journal      = {Machine Learning with Applications},
  month        = {12},
  pages        = {100731},
  shortjournal = {Mach. Learn. Appl.},
  title        = {Implementation of machine learning technologies in construction maintenance: A strategic analysis},
  volume       = {22},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Integrating deep learning and econometrics for stock price prediction: A comprehensive comparison of LSTM, transformers, and traditional time series models. <em>MLA</em>, <em>22</em>, 100730. (<a href='https://doi.org/10.1016/j.mlwa.2025.100730'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This study presents a comprehensive empirical comparison between state-of-the-art deep learning models including Long Short-Term Memory (LSTM) networks, Transformer architectures, and traditional econometric models (ARIMA and VAR) for stock price prediction, with particular focus on performance during the COVID-19 pandemic crisis. Using daily S&P 500 data from 2015 to 2020, we rigorously evaluate model performance across multiple metrics including Root Mean Squared Error (RMSE), Mean Absolute Error (MAE), and Mean Absolute Percentage Error (MAPE). Our findings demonstrate that while Transformer models achieve the best overall performance with an RMSE of 41.87 and directional accuracy of 69.1 %, LSTM networks provide an optimal balance between performance (RMSE: 43.25) and computational efficiency. Both deep learning approaches significantly outperform traditional econometric methods, with LSTM achieving a 53.3 % reduction in RMSE compared to ARIMA models. During the COVID-19 crisis period, deep learning models demonstrated exceptional robustness, with Transformers showing only 45 % performance degradation compared to over 100 % degradation in traditional models. Through comprehensive attention analysis, we provide insights into model interpretability, revealing adaptive behavior across market regimes. The study contributes to the growing literature on artificial intelligence applications in finance by providing rigorous empirical evidence for the superiority of modern deep learning approaches, while addressing the critical need for comparison with cutting-edge Transformer architectures that have revolutionized machine learning in recent years.},
  archive      = {J_MLA},
  author       = {Eyas Gaffar A. Osman and Faisal A. Otaibi},
  doi          = {10.1016/j.mlwa.2025.100730},
  journal      = {Machine Learning with Applications},
  month        = {12},
  pages        = {100730},
  shortjournal = {Mach. Learn. Appl.},
  title        = {Integrating deep learning and econometrics for stock price prediction: A comprehensive comparison of LSTM, transformers, and traditional time series models},
  volume       = {22},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Machine-learning based li-ion cell state prediction using impedance spectroscopy. <em>MLA</em>, <em>22</em>, 100729. (<a href='https://doi.org/10.1016/j.mlwa.2025.100729'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Accurate and reliable monitoring of battery state parameters is crucial for ensuring optimal battery performance, safety, and lifetime. Existing methods have limitations, such as requiring modeling of each degradation mechanism involved or relying on direct measurement techniques that impose restrictions on field studies or end-user use. In this paper, we propose a machine learning-based approach that combines the strengths of electrochemical impedance spectroscopy (EIS) and machine learning algorithms to predict battery state parameters. We have developed an efficient prediction system that can learn from EIS data and accurately predict battery state parameters. Our approach is trained on an open dataset comprising of over 30,000 spectra, generated using an automated measurement technique that outperforms current machine learning-based models, particularly in terms of generalization across different cells and measurement setups.},
  archive      = {J_MLA},
  author       = {Carl Philipp Klemm and Till Frömling},
  doi          = {10.1016/j.mlwa.2025.100729},
  journal      = {Machine Learning with Applications},
  month        = {12},
  pages        = {100729},
  shortjournal = {Mach. Learn. Appl.},
  title        = {Machine-learning based li-ion cell state prediction using impedance spectroscopy},
  volume       = {22},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Auxiliary evaluation of marginal ridge discrepancy in periodontal disease using deep learning on periapical radiographs. <em>MLA</em>, <em>22</em>, 100727. (<a href='https://doi.org/10.1016/j.mlwa.2025.100727'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Background/Objectives : Marginal Ridge Discrepancy (MRD) is an important early indicator of periodontal disease, often resulting from tooth inclination or alveolar bone loss, leading to uneven interproximal ridge height. Although periapical radiographs commonly observe bone and root structures, image overlap and angle variation often hinder accurate clinical interpretation. This study proposes a deep learning-based system integrating image segmentation and angular evaluation to assist dentists in objectively classifying MRD severity and improving diagnostic efficiency. Methods : We adopted a Mask R-CNN model with ResNet-101 as the backbone, incorporating warm-up and learning rate scheduling strategies to ensure stable convergence. Moreover, Mask R-CNN localized the cemento-enamel junction and alveolar crest by overlapping the mask image. We also introduced a novel angular measurement method to quantify the MRD between adjacent ridges and categorize periodontal disease severity. Results : ResNet-101 achieved the best segmentation performance among tested backbones with 98.11 % pixel-wise accuracy. Recall scores reached 97.60 % for teeth, 97.24 % for crowns, and 97.53 % for bone structures. The MRD classification model achieved 93.41 % accuracy with a mean angular error of only 0.85°, demonstrating strong clinical reliability. Conclusions : The proposed method effectively addresses challenges in evaluating ridge loss on periapical radiographs. Providing accurate and objective assessment enhances early periodontal diagnosis, reduces clinical workload, and supports improved medical quality and treatment planning.},
  archive      = {J_MLA},
  author       = {Yuan-Jin Lin and Chiung-An Chen and Yi-Cheng Mao and Chin-Hao Liang and Tsung-Yi Chen and Kuo-Chen Li and Shih-Lun Chen and Wei-Chen Tu},
  doi          = {10.1016/j.mlwa.2025.100727},
  journal      = {Machine Learning with Applications},
  month        = {12},
  pages        = {100727},
  shortjournal = {Mach. Learn. Appl.},
  title        = {Auxiliary evaluation of marginal ridge discrepancy in periodontal disease using deep learning on periapical radiographs},
  volume       = {22},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Exploring the cost of equity for insurance companies in the world: Evidence from machine learning approaches. <em>MLA</em>, <em>22</em>, 100726. (<a href='https://doi.org/10.1016/j.mlwa.2025.100726'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This study investigates the determinants of the WACC for insurance firms, integrating both financial and non-financial factors through advanced machine learning techniques. Analyzing data from 2012 to 2022 for a large sample of 190 insurance companies in the world, we compare nine ML models, revealing that XGBoost and LightGBM outperform traditional methods. Key drivers of WACC include beta, dividend yield, and earnings per share, with Emission score also showing significant influence. This study fills gaps in insurance finance literature by introducing ML-based WACC modeling, enhancing predictive accuracy, and providing policy recommendations for regulatory reporting and Emission score disclosures. From a policy perspective, the global insurance sector is at a crucial turning point, where ESG integration in granular form is found to be vital for financial stability. By mandating standardized ESG disclosures in alignment with the ISSB and TCFD frameworks, regulators can reduce insurers’ cost of equity, enabling a balance between financial sustainability and environmental responsibility, while promoting long-term value creation for both investors and society.},
  archive      = {J_MLA},
  author       = {Indranarain Ramlall and Dineshwar Ramdhony},
  doi          = {10.1016/j.mlwa.2025.100726},
  journal      = {Machine Learning with Applications},
  month        = {12},
  pages        = {100726},
  shortjournal = {Mach. Learn. Appl.},
  title        = {Exploring the cost of equity for insurance companies in the world: Evidence from machine learning approaches},
  volume       = {22},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). On the latent distribution of logistic regression — An empirical study on spectroscopic profiling datasets. <em>MLA</em>, <em>22</em>, 100712. (<a href='https://doi.org/10.1016/j.mlwa.2025.100712'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Logistic regression is a simple yet widely used classification model in spectroscopic profiling analysis. Considering the model’s output represents a probability, this paper will investigate its latent distribution assumption, i.e., its inner linear regressor unit follows a standard logistic distribution. An empirical study on five spectroscopic profiling open datasets, i.e., wine, coffee, olive oil, cheese, and milk powder, was conducted to verify this latent distribution assertion. This paper measured the GoF (Goodness of Fit) of each dataset’s latent variable from three aspects, i.e., curve fitting, P–P and Q–Q plots, and K–S test. After hyper-parameter optimization and proper training, the latent variable, as a weighted sum of the original features, has demonstrated a high level of GoF on all the five datasets. This study verifies the suitability of logistic regression in spectroscopic profiling analysis and answers why the model output can be interpreted as a conditional probability.},
  archive      = {J_MLA},
  author       = {Yinsheng Zhang and Mingming He and Haiyan Wang},
  doi          = {10.1016/j.mlwa.2025.100712},
  journal      = {Machine Learning with Applications},
  month        = {12},
  pages        = {100712},
  shortjournal = {Mach. Learn. Appl.},
  title        = {On the latent distribution of logistic regression — An empirical study on spectroscopic profiling datasets},
  volume       = {22},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). CNN-CCA: A deep learning approach for anomaly detection in metro rail sensor time-series data. <em>MLA</em>, <em>21</em>, 100728. (<a href='https://doi.org/10.1016/j.mlwa.2025.100728'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Internet of Things (IoT) offers new challenges in estimating correlations among data from multiple connected devices to understand their behaviors. Canonical Correlation Analysis (CCA) can be used to measure correlations among several observed variables. Different CCA methods have been proposed in the literature including probabilistic, sparse, kernel, discriminative, and deep learning-based CCAs. However, existing CCA approaches are limited by assumptions of linearity, reliance on predefined kernels, or difficulty in modeling localized patterns in high-frequency IoT sensor data. In this research, we explore two methods, linear CCA and non-linear deep learning-based CCA. Experiments demonstrate the effectiveness of CCA in detecting correlation in synthetic and metro rail time series sensor data collected from Autonomous Train (AT) signaling systems. Also, we propose a novel Convolutional Neural Network (CNN) based CCA method to detect correlation-based mappings and combine it with statistical anomaly detection methods in collective anomaly detection. The results indicate strong performance with an F1-score of 89.0% and a sensitivity of 94.1%, which can pave the way for the application of the proposed models to real-time collective anomaly detection and CCA in IoT systems.},
  archive      = {J_MLA},
  author       = {Vignesh Rao and Amir Eskandari and Farhana Zulkernine and Mohamed K. Helwa and David Beach},
  doi          = {10.1016/j.mlwa.2025.100728},
  journal      = {Machine Learning with Applications},
  month        = {9},
  pages        = {100728},
  shortjournal = {Mach. Learn. Appl.},
  title        = {CNN-CCA: A deep learning approach for anomaly detection in metro rail sensor time-series data},
  volume       = {21},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Novel channel attention-based filter pruning methods for low-complexity semantic segmentation models. <em>MLA</em>, <em>21</em>, 100725. (<a href='https://doi.org/10.1016/j.mlwa.2025.100725'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Semantic segmentation is the area of classifying each pixel in an image using a deep learning model. Examples of widely used semantic segmentation models are the U-Net and DeeplabV3+ models. While the aforementioned models have been deemed very successful in segmenting medical targets including organs and diseases in high resolution images, the computational complexity represents a burden for the real-time application of the algorithms or the deployment of the models on resource-constrained platforms. Until recently, few methods have been introduced for optimizing or pruning of the parameters of the semantic segmentation models. In this paper, we propose two novel channel attention-based filter pruning techniques (i.e., Sub-Sampling Channel Attention (SACA) and Self-Attention Based Attention (SBCA)) in order to reduce the complexity of the semantic segmentation models while maintaining high performance with respect to the benchmark models. This is realized by recognizing the contextual importance of the feature maps in each layer of the models and the significance of each filter to the final model performance. The proposed optimization methods have been validated on the U-Net and DeeplabV3+ models using both lung and skin lesion datasets. The proposed approaches achieved a pruned model performance (i.e., dice coefficient) of up to 96%, as well as an extensively reduced complexity (i.e., percentage of remaining parameters down to 1.1%, model size down to 1.22 MB and number of GFLOPS down to 1.06), outperforming the benchmark magnitude based (i.e., l1-norm , and l2-norm ) and the attention-based (i.e., SE, ECA, and CBAM CA) filter pruning methods.},
  archive      = {J_MLA},
  author       = {Md. Bipul Hossain and Na Gong and Mohamed Shaban},
  doi          = {10.1016/j.mlwa.2025.100725},
  journal      = {Machine Learning with Applications},
  month        = {9},
  pages        = {100725},
  shortjournal = {Mach. Learn. Appl.},
  title        = {Novel channel attention-based filter pruning methods for low-complexity semantic segmentation models},
  volume       = {21},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). PyRHOH: A meta-learning analysis framework for determining the impact of compilation on malicious JavaScript identification. <em>MLA</em>, <em>21</em>, 100724. (<a href='https://doi.org/10.1016/j.mlwa.2025.100724'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Automated identification of malicious JavaScript is a core problem within modern malware analysis. Code obfuscation is a common tactic used to evade detection. This obfuscation hinders both manual and automated detection methods, including neural network techniques. In order for these methods to effectively classify malware, it is beneficial to reduce the effects of obfuscation as well as to optimize the configuration and structure of the neural network to be well suited for the task. To overcome these challenges, we present a new framework: “PyRHOH” (“Python Repeatable Hyperparameter Optimization Harness”), a meta-learning framework that implements Bayesian optimization. The automated exploration and maximization of candidate hyperparameters using a Bayesian method adds structure and rigor to the selection of neural network hyperparameters, providing the assurance that an implemented design is optimal. In this study, we used the PyRHOH framework to determine optimal recurrent neural network architectures for the differentiation of malicious and benign JavaScript samples. We then used these neural networks to measure the degree to which compilation of raw JavaScript samples into bytecode via Google’s V8 JavaScript compiler affected classification accuracy. Classifying in-the-wild samples, compilation increased the detection rate from 76.88% to 95.84%. Among uniformly obfuscated samples, compilation increased the detection rate from an average of 76.76% to an average of 91.24% e compilation was performed. This shows that pre-processing JavaScript into compiled bytecode has a clear positive impact on neural network categorization.},
  archive      = {J_MLA},
  author       = {Eli Fulkerson and Eric Yocam and Varghese Vaidyan and Mahesh Kamepalli and Yong Wang and Gurcan Comert},
  doi          = {10.1016/j.mlwa.2025.100724},
  journal      = {Machine Learning with Applications},
  month        = {9},
  pages        = {100724},
  shortjournal = {Mach. Learn. Appl.},
  title        = {PyRHOH: A meta-learning analysis framework for determining the impact of compilation on malicious JavaScript identification},
  volume       = {21},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Framework for detecting and recognizing sign language using absolute pose estimation difference and deep learning. <em>MLA</em>, <em>21</em>, 100723. (<a href='https://doi.org/10.1016/j.mlwa.2025.100723'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Computer vision has been identified as one of the key solutions for human activity recognition, including sign language recognition. Despite the success demonstrated by various studies, isolating signs from continuous video remains a challenge. The sliding window approach has been commonly used for translating continuous video. However, this method subjects the model to unnecessary predictions, leading to increased computational costs. This study proposes a framework that use absolute pose estimation differences to isolate signs from continuous videos and translate them using a model trained on isolated signs. Pose estimation features were chosen due to their proven effectiveness in various activity recognition tasks within computer vision. The proposed framework was evaluated on 10 videos of continuous signs. According to the findings, the framework achieved an average accuracy of 84%, while the model itself attained 95% accuracy. Moreover, SoftMax output analysis shows that the model exhibits higher confidence in correctly classified signs, as indicated by higher average SoftMax scores for correct predictions. This study demonstrates the potential of the proposed framework over the sliding window approach, which tends to overwhelm the model with excessive classification sequences.},
  archive      = {J_MLA},
  author       = {Kasian Myagila and Devotha Godfrey Nyambo and Mussa Ally Dida},
  doi          = {10.1016/j.mlwa.2025.100723},
  journal      = {Machine Learning with Applications},
  month        = {9},
  pages        = {100723},
  shortjournal = {Mach. Learn. Appl.},
  title        = {Framework for detecting and recognizing sign language using absolute pose estimation difference and deep learning},
  volume       = {21},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A hybrid approach to content generation based on user experience using generative AI elements. <em>MLA</em>, <em>21</em>, 100722. (<a href='https://doi.org/10.1016/j.mlwa.2025.100722'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Understanding and analyzing User Experience (UX) is a critical challenge in e-commerce. This study addresses the integration of UX analysis and Generative Artificial Intelligence (GAI) for content creation, presenting a two-phase approach. In the first phase, UX elements were extracted from user reviews of Amazon digital products and analyzed using clustering algorithms, including K-means, enhanced K-means, and Self-Organizing Maps (SOM). In the second phase, the top UX elements identified were used to generate promotional content with GPT-3.5 Turbo and Claude 3.5 Sonnet language models. The content was evaluated using the Perplexity metric, with three AI models—BERT, GPT-2, and DistilBERT. Results show that the enhanced K-means algorithm paired with GPT-3.5 Turbo achieved the best performance, yielding a Perplexity score of 3.476. This approach demonstrates the potential for leveraging real user data to create targeted and engaging content, providing businesses with actionable insights for improving audience engagement.},
  archive      = {J_MLA},
  author       = {Mohammad Javad Shayegan and Hossein Hosseinpour},
  doi          = {10.1016/j.mlwa.2025.100722},
  journal      = {Machine Learning with Applications},
  month        = {9},
  pages        = {100722},
  shortjournal = {Mach. Learn. Appl.},
  title        = {A hybrid approach to content generation based on user experience using generative AI elements},
  volume       = {21},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Enhanced early detection of dysarthric speech disabilities using stacking ensemble deep learning model. <em>MLA</em>, <em>21</em>, 100721. (<a href='https://doi.org/10.1016/j.mlwa.2025.100721'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Communication disorders, particularly dysarthria, significantly impact individuals by impairing their speech clarity, social interactions, and overall well-being. Early and accurate detection is crucial to enable timely intervention and improve speech therapy outcomes. This study introduces Adaptive Dysarthric Speech Disability Detection using Stacked Ensemble Deep Learning (ADSDD-SEDL), an innovative ensemble-based deep-learning framework for dysarthria detection. The proposed model integrates three deep learning architectures—Multi-Head Attention-based Long Short-Term Memory (MHALSTM), Deep Belief Network (DBN), and Time-Delay Neural Network (TDNN)—within a stacked ensemble model. Unlike conventional stacking methods that use fixed meta-classifiers, this study employs a Genetic Algorithm (GA)-based optimization strategy to dynamically determine optimal weight contributions of the base models, enhancing classification robustness and adaptability. The preprocessing pipeline converts speech signals from the time domain to the frequency domain by using a Short-Time Fourier Transform (STFT). Mel-Frequency Cepstral Coefficients (MFCCs) were extracted to capture the key spectral characteristics. Each base model underwent independent training, and the GA optimized the ensemble by evolving an adaptive weight distribution instead of relying on predefined fusion methods. Extensive simulations and hyperparameter tuning confirmed that the GA-optimized ADSDD-SEDL technique significantly improved detection efficiency over traditional ensemble approaches. These findings underscore the advantages of evolutionary optimization in refining speech disorder classification models. This scalable and adaptive model offers a valuable tool for healthcare professionals, enabling precise and automated early diagnosis of dysarthria. Future research could explore alternative evolutionary algorithms, reinforcement learning techniques, and hybrid deep learning approaches to enhance speech disorder classification.},
  archive      = {J_MLA},
  author       = {Jagat Chaitanya Prabhala and Ravi Ragoju and Venkatanareshbabu Kuppili and Christophe Chesneau},
  doi          = {10.1016/j.mlwa.2025.100721},
  journal      = {Machine Learning with Applications},
  month        = {9},
  pages        = {100721},
  shortjournal = {Mach. Learn. Appl.},
  title        = {Enhanced early detection of dysarthric speech disabilities using stacking ensemble deep learning model},
  volume       = {21},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Modelling the effect of vaccination on the spread of COVID-19 via a novel evolutionary ensemble learning algorithm. <em>MLA</em>, <em>21</em>, 100720. (<a href='https://doi.org/10.1016/j.mlwa.2025.100720'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The spread of the COVID-19 disease has caused a lot of problems for every country around the world. To curb the pandemic, governments have issued various policies, including vaccination. Depending on the percentage of the vaccinated population, the pandemic responds differently to the policies. This paper proposes a modelling algorithm that takes as input the percentage of the vaccinated population and the policies taken by governments and generates as output a prediction of the number of newly infected cases. Then, this model is used as the fitness function in an optimisation algorithm, which for a population with a certain percentage of vaccinated people, searches through the set of policies and finds the best set of policies that minimises the cost to society and the number of infected people. To build the model, an ensemble learning algorithm is proposed, which is a combination of different learning algorithms. In this algorithm, an evolutionary diversifier algorithm is proposed to generate the base learners. The algorithm chooses different subsets of features for each base learner to maximise diversity among them. Then, an evolutionary process is adopted to choose from the base learners a subset that optimises the prediction accuracy of the model. The proposed algorithms are tested on a well-known data set about government policies, the percentage of the population vaccinated, and the number of infected cases. Experimental studies suggest better performance for the proposed ensemble learning algorithm compared to existing ones. Multi-objective optimisation of the policies is also proposed and tested on the model, and the results are presented in this paper.},
  archive      = {J_MLA},
  author       = {Mohammad Hassan Tayarani Najaran},
  doi          = {10.1016/j.mlwa.2025.100720},
  journal      = {Machine Learning with Applications},
  month        = {9},
  pages        = {100720},
  shortjournal = {Mach. Learn. Appl.},
  title        = {Modelling the effect of vaccination on the spread of COVID-19 via a novel evolutionary ensemble learning algorithm},
  volume       = {21},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Topological data analysis for driver behavior classification driven by vehicle trajectory data. <em>MLA</em>, <em>21</em>, 100719. (<a href='https://doi.org/10.1016/j.mlwa.2025.100719'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With urbanization and rising vehicle numbers, road safety has become increasingly critical. Robust, trajectory-level risk assessment is essential for next-generation active safety systems, accident prevention, autonomous driving, and intelligent transportation networks. This paper presents a novel framework for driver behavior classification using Topological Data Analysis (TDA) — a mathematical approach for analyzing high-dimensional data — via persistent homology applied to vehicle trajectory data. Traditional methods often struggle with the complexity of such data, but TDA captures topological features that reveal subtle, meaningful behavioral patterns. Using the HighD dataset, we train a class-weighted XGBoost classifier on persistence image (PI) features, achieving 96.8% overall accuracy, macro-F 1 = 0.93, and retaining 87% F 1 on the minority Aggressive class. Unsupervised K-means clustering of the same PI features naturally separates the data into three behavioral clusters whose ANOVA-verified risk profiles align with the MOR-defined classes, confirming the behavioral relevance of the topological descriptors. These results provide empirical evidence that PI features capture safety-critical structure more effectively than raw kinematics and demonstrate the robustness and scalability of TDA for analyzing large, noisy datasets. The proposed approach shows strong potential for real-time driver monitoring, risk assessment, and data-driven transportation management, with implications for traffic safety, autonomous systems, and personalized insurance.},
  archive      = {J_MLA},
  author       = {Debbie Indah and Judith Mwakalonge and Gurcan Comert and Saidi Siuhi and Hannah Musau and Eric Osei and Paul Omulokoli and Methusela Sulle and Denis Ruganuza and Nana Kankam Gyimah},
  doi          = {10.1016/j.mlwa.2025.100719},
  journal      = {Machine Learning with Applications},
  month        = {9},
  pages        = {100719},
  shortjournal = {Mach. Learn. Appl.},
  title        = {Topological data analysis for driver behavior classification driven by vehicle trajectory data},
  volume       = {21},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A bayesian inversion supervised learning framework for the enzyme activity in graphene field-effect transistors. <em>MLA</em>, <em>21</em>, 100718. (<a href='https://doi.org/10.1016/j.mlwa.2025.100718'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Graphene Field-Effect Transistors (GFETs) are gaining prominence in enzyme detection due to their exceptional sensitivity, rapid response, and capability for real-time monitoring of enzymatic reactions. Among different catalytic systems, heme-based peroxidase enzymes such as horseradish peroxidase (HRP), and heme molecules, which can exhibit peroxidase-like activity, are noteworthy due to their significant catalytic behavior. GFETs effectively monitor and detect these enzymatic reactions by observing alterations in their electrical properties. In this study, we present a computational framework designed to determine key enzymatic parameters, including the enzyme turnover number and the Michaelis–Menten constant. Utilizing experimental reaction rate data obtained from the GFET electrical response, we apply Bayesian inversion models to estimate these parameters accurately. Additionally, we develop a novel deep neural network (multilayer perceptron) to predict enzyme behavior under various chemical and environmental conditions. The performance of this coupled computational model is compared against standard machine learning and Bayesian inversion techniques to validate its efficiency and accuracy. We present a pseudocode to explain the implementation of machine learning Bayesian inversion framework.},
  archive      = {J_MLA},
  author       = {Ehsan Khodadadian and Samaneh Mirsian and Shahrzad Shashaani and Maryam Parvizi and Amirreza Khodadadian and Peter Knees and Wolfgang Hilber and Clemens Heitzinger},
  doi          = {10.1016/j.mlwa.2025.100718},
  journal      = {Machine Learning with Applications},
  month        = {9},
  pages        = {100718},
  shortjournal = {Mach. Learn. Appl.},
  title        = {A bayesian inversion supervised learning framework for the enzyme activity in graphene field-effect transistors},
  volume       = {21},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Cross-domain fairness audit of sentiment label bias in foundation models: Comparing human and machine annotations on tweets and reviews. <em>MLA</em>, <em>21</em>, 100717. (<a href='https://doi.org/10.1016/j.mlwa.2025.100717'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This study presents a comparative fairness audit of leading foundation models (FMs), OpenAI, Gemini, DeepSeek, and LLaMA, against human labeled sentiment data across diverse text domains. Using tweet, review, and sarcasm labeled datasets, we assess model agreement, fairness metrics (e.g., Demographic Parity Difference, Equal Opportunity Difference, Disparate Impact Ratio), and statistical significance of label discrepancies. Our findings reveal performance gaps, domain sensitivity, and systematic biases, especially in sarcastic and informal texts. To address these biases, we conduct a pilot sarcasm aware multitask fine tuning experiment, which reduces misclassification disparities and improves fairness metrics on sarcastic samples. These results underscore both the necessity of fairness audits and the potential of lightweight mitigation strategies in sentiment classification tasks.},
  archive      = {J_MLA},
  author       = {Blessing Ogbuokiri and George Obaido and Chioma Kamalu and Kehinde Aruleba and Okechinyere Achilonu and Ibomoiye Domor Mienye and Stephenson Echezona and Bridget Ujah-Ogbuagu and Laleh Seyyed-Kalantari},
  doi          = {10.1016/j.mlwa.2025.100717},
  journal      = {Machine Learning with Applications},
  month        = {9},
  pages        = {100717},
  shortjournal = {Mach. Learn. Appl.},
  title        = {Cross-domain fairness audit of sentiment label bias in foundation models: Comparing human and machine annotations on tweets and reviews},
  volume       = {21},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). High-frequency stock price prediction via deep learning. <em>MLA</em>, <em>21</em>, 100716. (<a href='https://doi.org/10.1016/j.mlwa.2025.100716'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We performed a comparative analysis of deep learning methods for high-frequency stock price prediction. Instead of directly analyzing one-dimensional stock price time series data, this study employs the Gramian Angular Summation Field method (Wang and Oates, 2015) to transform high-frequency stock prices into images, which are used to train ResNet models for prediction (hereafter referred to as the image-based prediction method). In addition, the same dataset (one-dimensional time series without image conversion) is used to train Artificial Neural Network(ANN), Long Short-Term Memory(LSTM), and one-dimensional convolutional neural network(1D-CNN) models, enabling a performance comparison with the results of the image-based prediction method.},
  archive      = {J_MLA},
  author       = {Jianlong Bao and Takayuki Morimoto},
  doi          = {10.1016/j.mlwa.2025.100716},
  journal      = {Machine Learning with Applications},
  month        = {9},
  pages        = {100716},
  shortjournal = {Mach. Learn. Appl.},
  title        = {High-frequency stock price prediction via deep learning},
  volume       = {21},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). MLPro 2.0 - Online machine learning in python. <em>MLA</em>, <em>21</em>, 100715. (<a href='https://doi.org/10.1016/j.mlwa.2025.100715'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we present version 2.0 of the open-source middleware MLPro for applied machine learning in Python. Notably, it introduces the new sub-framework MLPro-OA for online machine learning, focusing on standards and templates for classic and online-adaptive data stream processing (DSP/OADSP). As part of this, we provide three novel adaptation mechanisms:The first, event-oriented adaptation, enables localized, event-driven parameter updates within individual tasks. The second, cascaded adaptation, allows adaptation events to propagate across multiple dependent tasks, creating task-spanning adjustment cascades decoupled from the forward-facing DSP. The third, reverse adaptation, allows tasks to revise prior adjustments by explicitly processing obsolete instances discarded from a preceding sliding window. Furthermore, we provide insights into the underlying design criteria of MLPro-OA, which were developed through extensive requirements engineering. In the practical part of this work, we demonstrate the essential functionalities of MLPro-OA using reproducible examples.},
  archive      = {J_MLA},
  author       = {Detlef Arend and Laxmikant Shrikant Baheti and Steve Yuwono and Syamraj Purushamparambil Satheesh Kumar and Andreas Schwung},
  doi          = {10.1016/j.mlwa.2025.100715},
  journal      = {Machine Learning with Applications},
  month        = {9},
  pages        = {100715},
  shortjournal = {Mach. Learn. Appl.},
  title        = {MLPro 2.0 - Online machine learning in python},
  volume       = {21},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A configurable intrinsic curiosity module for a testbed for developing intelligent swarm UAVs. <em>MLA</em>, <em>21</em>, 100714. (<a href='https://doi.org/10.1016/j.mlwa.2025.100714'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper introduces an Intrinsic Curiosity Module (ICM) based Reinforcement Learning (RL) framework for swarm Unmanned Aerial Vehicles (UAVs) target tracking, leveraging the actor–critic architecture to control the roll, pitch, yaw, and throttle motions of UAVs. A key challenge in RL-based UAV coordination is the delayed reward problem, which hinders effective learning in dynamic environments. Existing UAV testbeds rely primarily on extrinsic rewards and lack mechanisms for adaptive exploration and efficient UAV coordination. To address these limitations, we propose a testbed that integrates an ICM with the Asynchronous Advantage Actor-Critic (A3C) algorithm for tracking UAVs. It incorporates the Self-Reflective Curiosity-Weighted (SRCW) hyperparameter tuning mechanism for the ICM, which adaptively modifies hyperparameters based on the ongoing RL agent’s performance. In this testbed, the target UAV is guided by the Advantage Actor-Critic (A2C) model, while a swarm of two tracking UAVs is controlled by using the A3C-ICM approach. The proposed framework facilitates real-time autonomous coordination among UAVs within a simulated environment. This system is developed using the FlightGear flight simulator and the JSBSim Flight Dynamics Model (FDM), which enables dynamic simulations and continuous interaction between UAVs. Experimental results demonstrate that the tracking UAVs can effectively coordinate and maintain precise paths even under complex conditions.},
  archive      = {J_MLA},
  author       = {Jawad Mahmood and Muhammad Adil Raja and John Loane and Fergal McCaffery},
  doi          = {10.1016/j.mlwa.2025.100714},
  journal      = {Machine Learning with Applications},
  month        = {9},
  pages        = {100714},
  shortjournal = {Mach. Learn. Appl.},
  title        = {A configurable intrinsic curiosity module for a testbed for developing intelligent swarm UAVs},
  volume       = {21},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Classification of system dynamics model outputs using decision trees. <em>MLA</em>, <em>21</em>, 100713. (<a href='https://doi.org/10.1016/j.mlwa.2025.100713'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Classification of behaviours generated by mathematical models such as an ODE is an important component in modelling fields including System Dynamics. Useful for model validation, and investigation into the parameters which drive the different behaviours, a machine learning model based on a decision tree is a valuable way to interpret the behaviours returned. This research presents the creation of categorical attributes for the classification of model outputs into 13 behaviours. With a pre-given training set, it allows for the classification of unlabelled data, with hyper-parameters which can be changed for fine-tuning depending on the model presented. Where asymptotic model outputs may cause difficulty, a user-defined threshold value is available. Tested using empirical data, the results show a strong improvement on the previously available methods for behaviour classification of System Dynamics model outputs, and demonstrated using F1 scores. Our method has general applicability for classification of all time series data.},
  archive      = {J_MLA},
  author       = {Martina Curran and Enda Howley and Jim Duggan},
  doi          = {10.1016/j.mlwa.2025.100713},
  journal      = {Machine Learning with Applications},
  month        = {9},
  pages        = {100713},
  shortjournal = {Mach. Learn. Appl.},
  title        = {Classification of system dynamics model outputs using decision trees},
  volume       = {21},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A fine-tuned deep learning model for detecting japanese beetles in soybeans using unmanned aircraft systems (UAS) and mobile imaging. <em>MLA</em>, <em>21</em>, 100711. (<a href='https://doi.org/10.1016/j.mlwa.2025.100711'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Since the early 1900s, the Japanese beetle ( Popillia japonica , Newman) has invaded soybean crops in the U.S. It has emerged as a significant economic threat due to its habit of defoliating plants, often leaving them skeletal and reducing yields. The conventional approach for monitoring Japanese beetles uses visual assessments and sweep counts, which are impractical for larger soybean acreages. Furthermore, frequent manual sampling demands labor and time resources that could otherwise be allocated to enhancing soybean production. To address this challenge, we fine-tuned a deep learning model capable of automatically detecting Japanese beetles using images, thereby improving the monitoring process. The YOLOv8s model can detect Japanese beetles 87.90 % of the time on images collected from mobile devices and unmanned aircraft systems (UAS) at certain distances. The model was deployed in a web application as a prototype platform to understand the capabilities of deep learning in pest monitoring. This web application is a server that autonomously analyzes images captured by mobile devices and UAS to detect and count beetles in the soybean canopy. This study aimed to transform the traditional method of pest monitoring in soybean production by transitioning to a digital monitoring system.},
  archive      = {J_MLA},
  author       = {Ivan Grijalva and H. Braden Adams and Brian McCornack},
  doi          = {10.1016/j.mlwa.2025.100711},
  journal      = {Machine Learning with Applications},
  month        = {9},
  pages        = {100711},
  shortjournal = {Mach. Learn. Appl.},
  title        = {A fine-tuned deep learning model for detecting japanese beetles in soybeans using unmanned aircraft systems (UAS) and mobile imaging},
  volume       = {21},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Prediction of retention time in larger antisense oligonucleotide datasets using machine learning. <em>MLA</em>, <em>21</em>, 100710. (<a href='https://doi.org/10.1016/j.mlwa.2025.100710'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Antisense oligonucleotides (ASOs) are nucleic acid molecules with transformative therapeutic potential, especially for diseases that are untreatable by traditional drugs. However, the production and purification of ASOs remain challenging due to the presence of unwanted impurities. One tool successfully used to separate an ASO compound from the impurities is ion pair liquid chromatography (IPC). It is a critical step in separation, where each compound is identified by its retention time ( t R ) in the IPC. Due to the complex sequence-dependent behavior of ASOs and variability in chromatographic conditions, the accurate prediction of t R is a difficult task. This study addresses this challenge by applying machine learning (ML) to predict t R based on the sequence characteristics of ASOs. Four ML models—Gradient Boosting, Random Forest, Decision Tree, and Support Vector Regression — were evaluated on three large ASOs datasets with different gradient times. Through feature engineering and grid search optimization, key predictors were identified and compared for model accuracy using root mean square error, coefficient of determination R-squared, and run time. The results showed that Gradient Boost performance competes with the Support Vector Machine in two of the three datasets, but is 3.94 times faster to tune. Additionally, newly proposed features representing the sulfur count and the nucleotides residing at the first and last positions of a sequence found to improve the predictive power of the models. This study demonstrates the advantages of ML-based t R prediction at scale and provides insights into interpretable and efficient utilization of ML in chromatographic applications.},
  archive      = {J_MLA},
  author       = {Manal Rahal and Bestoun S. Ahmed and Christoph A. Bauer and Johan Ulander and Jörgen Samuelsson},
  doi          = {10.1016/j.mlwa.2025.100710},
  journal      = {Machine Learning with Applications},
  month        = {9},
  pages        = {100710},
  shortjournal = {Mach. Learn. Appl.},
  title        = {Prediction of retention time in larger antisense oligonucleotide datasets using machine learning},
  volume       = {21},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Optimizing travel time reliability with XAI: A virginia interstate network case using machine learning and meta-heuristics. <em>MLA</em>, <em>21</em>, 100709. (<a href='https://doi.org/10.1016/j.mlwa.2025.100709'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper applies machine learning models to predict travel time reliability in transportation networks, using XGBoost, LightGBM, and CatBoost optimized with seven metaheuristic algorithms. The models were fine-tuned with a four-year dataset (2014–2017) covering 59 interstate sections in Virginia. Key features Link Length, AADT/mile/lane, Total Rate, and PRCP/1000 were identified as influential factors for travel time index prediction. Results revealed that XGBoost optimized with Grey Wolf Optimizer (GWO) achieved the highest accuracy at 92 %, surpassing the base model. LightGBM-GWO and CatBoost-GWO also demonstrated improvements, scoring up to 89 %. GWO outperformed other optimization methods, delivering superior accuracy with fewer control parameters. Feature importance analysis highlighted Link Length and AADT/Lane.mile as critical predictors. This research enhances travel time reliability prediction, providing insights for transportation planning and management. Future work includes exploring multi-objective optimization and integrating additional features to refine model accuracy further.},
  archive      = {J_MLA},
  author       = {Navid Khorshidi and Shahriar Afandizadeh Zargari and Soheil Rezashoar and Hamid Mirzahossein},
  doi          = {10.1016/j.mlwa.2025.100709},
  journal      = {Machine Learning with Applications},
  month        = {9},
  pages        = {100709},
  shortjournal = {Mach. Learn. Appl.},
  title        = {Optimizing travel time reliability with XAI: A virginia interstate network case using machine learning and meta-heuristics},
  volume       = {21},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Predicting flow-blurring droplet size using neural networks and bayesian optimization: A data-driven approach. <em>MLA</em>, <em>21</em>, 100708. (<a href='https://doi.org/10.1016/j.mlwa.2025.100708'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Flow-blurring injectors, known for producing fine sprays in twin-fluid systems, are essential for applications involving high-viscosity fuels, such as biofuels. This study presents a data-driven approach using neural networks and Bayesian optimization to predict the Sauter Mean Diameter (SMD) of flow-blurring sprays. A dataset from the experimental literature was curated and pre-processed, with critical dimensionless parameters – including the Reynolds number, Weber number, injector’s aspect ratio, and air-to-liquid mass flow rate – used to train multi-layer perceptron (MLP) models. Through Bayesian optimization, hyperparameters such as neuron count, learning rate, and regularization were fine-tuned to enhance model accuracy and avoid overfitting. The optimized models achieved high predictive accuracy, with regression scores exceeding 97% and minimal mean-squared error (MSE), demonstrating that Bayesian-optimized neural networks can significantly reduce reliance on costly experimental and numerical methods. This approach provides a fast, accurate solution for spray modeling, offering a scalable method for optimizing injector designs in fuel systems, particularly for alternative fuel applications.},
  archive      = {J_MLA},
  author       = {S. Amirreza S. Madani and Erfan Vaezi and Seyed Sorosh Mirfasihi and Amir Keshmiri},
  doi          = {10.1016/j.mlwa.2025.100708},
  journal      = {Machine Learning with Applications},
  month        = {9},
  pages        = {100708},
  shortjournal = {Mach. Learn. Appl.},
  title        = {Predicting flow-blurring droplet size using neural networks and bayesian optimization: A data-driven approach},
  volume       = {21},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Explaining drivers of housing prices with nonlinear hedonic regressions. <em>MLA</em>, <em>21</em>, 100707. (<a href='https://doi.org/10.1016/j.mlwa.2025.100707'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Housing markets play a critical role in shaping the spatial and demographic evolution of urban areas. Simulating housing price dynamics can enhance projections of future urban development outcomes. However, traditional hedonic regressions for housing prices, which neglect nonlinear interactions among explanatory variables, often exhibit limited predictive performance. While machine learning (ML) methods can provide a more flexible representation of the relationships between predictors, they are often regarded as “black boxes” due to their complexity and lack of transparency. Interpretable ML techniques provide a promising route by combining the flexibility of ML methods with approaches to analyze the relationships between inputs and outputs. In this study, we employ interpretable ML to analyze the patterns driving the housing market in Baltimore, Maryland, USA. We train an Artificial Neural Network (ANN) to predict Baltimore housing prices based on structural characteristics (e.g., home size, number of stories) and locational attributes (e.g., distance to the city center). We then conduct sensitivity and Partial Dependence Plot (PDP) analyses to interpret the fitted ANN model. We find that the ML model achieves higher predictive accuracy and explains 16 % more of housing price variance than a traditional linear regression model. The interpretable ML model also reveals more nuanced and realistic nonlinear relationships between housing sales price and predictors as well as interactive effects underlying Baltimore home price dynamics. For instance, while the linear model indicates a steady housing price increase over time, our interpretable ML model detects a post-2008 decline, with smaller properties experiencing the sharpest drop.},
  archive      = {J_MLA},
  author       = {Heng Wan and Pranab K. Roy Chowdhury and Jim Yoon and Parin Bhaduri and Vivek Srikrishnan and David Judi and Brent Daniel},
  doi          = {10.1016/j.mlwa.2025.100707},
  journal      = {Machine Learning with Applications},
  month        = {9},
  pages        = {100707},
  shortjournal = {Mach. Learn. Appl.},
  title        = {Explaining drivers of housing prices with nonlinear hedonic regressions},
  volume       = {21},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Predicting abnormality-guided multimodal linguistic semantics arabic image captioning. <em>MLA</em>, <em>21</em>, 100706. (<a href='https://doi.org/10.1016/j.mlwa.2025.100706'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep learning has significantly advanced image captioning tasks, enabling models to generate accurate, descriptive sentences from visual content. While much progress has been made in English-language image captioning, Arabic remains underexplored despite its linguistic complexity and widespread usage. Existing Arabic image captioning systems suffer from limited datasets, insufficiently tuned models, and poor adaptation to Arabic morphology and semantics. This limitation hinders the development of accurate, coherent Arabic captions, especially in high-resource applications such as media indexing and content accessibility. This study aims to develop an effective Arabic Image Caption Generator that addresses the shortage of research and tools in this domain. The goal is to create a robust model capable of generating semantically rich, syntactically accurate Arabic captions for visual inputs. The proposed system integrates a DenseNet201 convolutional neural network (CNN) for image feature extraction with a deep Recurrent Neural Network using Long Short-Term Memory (RNN-LSTM) units for sequential caption generation. The model was trained and fine-tuned on a translated Arabic version of the Flickr8K dataset, consisting of over 8000 images, each paired with three Arabic captions. The fine-tuned DenseNet201 + LSTM model achieved BLEU-4 of 0.85, ROUGE-L of 0.90, METEOR of 0.72, CIDEr of 0.88, SPICE of 0.68, and a perplexity score of 1.1, surpassing baseline and prior models in Arabic image captioning tasks. This research provides a novel, end-to-end Arabic image captioning framework, addressing linguistic challenges through deep learning. It offers a benchmark model for future research and practical applications in Arabic-language image understanding.},
  archive      = {J_MLA},
  author       = {Nahla Aljojo and Hanin Ardah and Araek Tashkandi and Safa Habibullah},
  doi          = {10.1016/j.mlwa.2025.100706},
  journal      = {Machine Learning with Applications},
  month        = {9},
  pages        = {100706},
  shortjournal = {Mach. Learn. Appl.},
  title        = {Predicting abnormality-guided multimodal linguistic semantics arabic image captioning},
  volume       = {21},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Predictive modelling of graphene-enhanced greases using classical feedback control and quantum kernel regression. <em>MLA</em>, <em>21</em>, 100705. (<a href='https://doi.org/10.1016/j.mlwa.2025.100705'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper investigates two predictive modeling approaches for estimating the thermal and tribological performance of graphene-enhanced greases, aiming to reduce reliance on protracted endurance tests. Seven grease formulations with varying graphene concentrations (0–4 wt%) were prepared and tested under a uniform load to capture temperature evolution, wear scar area and coefficient of friction. A classical piecewise regression model, augmented by a Linear Quadratic Regulator (LQR), leverages feedback control to correct temperature predictions and subsequently estimate wear using a polynomial fit. This framework demonstrated high accuracy in tracking transient thermal behaviour, maintaining temperature deviations within ±1 °C of measured data. In parallel, a quantum-classical hybrid model employs a fidelity-based quantum kernel with support vector regression. By encoding partial early-cycle temperature measurements (e.g., from 30 to 120s) into a higher-dimensional Hilbert space, the quantum approach captures subtle nonlinearities and yields strong correlations for both final temperature and wear scar area. Moreover, consistent performance on IBM Quantum models with realistically simulated noise underscores the model’s potential for practical industrial implementation. Collectively, these results confirm the viability of advanced computational tools, both classical and quantum, for rapid, data-driven lubricant assessments. They highlight opportunities to optimize graphene content while minimizing costly trial and error testing.},
  archive      = {J_MLA},
  author       = {Ethan Stefan-Henningsen and Amirkianoosh Kiani},
  doi          = {10.1016/j.mlwa.2025.100705},
  journal      = {Machine Learning with Applications},
  month        = {9},
  pages        = {100705},
  shortjournal = {Mach. Learn. Appl.},
  title        = {Predictive modelling of graphene-enhanced greases using classical feedback control and quantum kernel regression},
  volume       = {21},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A comprehensive overview of remaining useful life prediction: From traditional literature review to scientometric analysis. <em>MLA</em>, <em>21</em>, 100704. (<a href='https://doi.org/10.1016/j.mlwa.2025.100704'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The increasing complexity of industrial systems has heightened the need for precise Remaining Useful Life (RUL) predictions. This paper provides a comprehensive overview of RUL prediction methods, processes, datasets, and tools, alongside insights from scientometric analysis. We categorize RUL prediction methods into model-based, data-driven, and hybrid methods, detailing key models and their applications. Model-based methods utilize physical failure mechanisms, enhancing interpretability, but with limited adaptability for complex systems. Data-driven methods use machine learning to extensive datasets, boosting adaptability but often at the cost of explainability. Hybrid methods aim to strike a balance between these strengths, offering both accuracy and flexibility across various applications. In addition, we outline the RUL prediction process, from data acquisition to implementation, and introduce the primary datasets used in studies of engines, bearings, and batteries as well as commonly used programming frameworks and open-source libraries. We also performed a scientometric analysis of 3442 articles from the Web of Science database, examining research areas, prominent research groups, and emerging trends. Our findings offer a comprehensive overview of the RUL prediction field, outlining its current trends and highlighting potential directions for future research.},
  archive      = {J_MLA},
  author       = {Yitong Liu and Jiarui Wen and Guoqiang Wang},
  doi          = {10.1016/j.mlwa.2025.100704},
  journal      = {Machine Learning with Applications},
  month        = {9},
  pages        = {100704},
  shortjournal = {Mach. Learn. Appl.},
  title        = {A comprehensive overview of remaining useful life prediction: From traditional literature review to scientometric analysis},
  volume       = {21},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Dual radar vision: A feature fusion approach for advanced object detection in IoT radar networks. <em>MLA</em>, <em>21</em>, 100703. (<a href='https://doi.org/10.1016/j.mlwa.2025.100703'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {60 GHz radar technology is one of the most promising movement detector solutions for Internet of Things (IoT) applications. However, challenges remain in accurately classifying different objects and detecting small objects in a multi-target scenario. This work investigates whether sensor fusion between multiple radars can enhance object detection and classification performance. A one-stage detection architecture, designed based on the features of the latest YOLO generations, is used to perform fusion based on range-Doppler (RD) maps of two non-coherent spatially separated radars. A complete physical 3D propagation simulation using ray tracing evaluates the fusion methods. This approach enables precise ground truth, as all unprocessed signal components are known, and guarantees a consistent, error-free reference. Results demonstrate that dynamic, attention-based fusion significantly improves detection and classification compared to static fusion in homogeneous and heterogeneous radar setups.},
  archive      = {J_MLA},
  author       = {Philipp Reitz and Tobias Veihelmann and Norman Franchi and Maximilian Lübke},
  doi          = {10.1016/j.mlwa.2025.100703},
  journal      = {Machine Learning with Applications},
  month        = {9},
  pages        = {100703},
  shortjournal = {Mach. Learn. Appl.},
  title        = {Dual radar vision: A feature fusion approach for advanced object detection in IoT radar networks},
  volume       = {21},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Modeling of settlement of shallow-founded rocking structures using explainable physics-guided machine learning. <em>MLA</em>, <em>21</em>, 100702. (<a href='https://doi.org/10.1016/j.mlwa.2025.100702'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Rocking foundation is an unorthodox seismic design philosophy of structures that enhances the performance of structures by absorbing and dissipating seismic energy into soil. This paper examines the application of physics-guided machine learning (PGML) technique to model the settlement of shallow-founded rocking structures during earthquake loading. An approximate physics-based model (PBM) is derived for rocking-induced total settlement as a function of critical contact area ratio and cumulative rotation of the foundation. The output of the PBM is fed as an additional input feature to machine learning (ML) algorithms to develop PGML models. The performances of PGML models are compared with the performances of purely data-driven ML models, the PBM outputs, and results obtained from an empirical relationship. To shed light on the explainability of ML and PGML models, Shapley Additive Explanations (SHAP values) are used to decipher and interpret the model predictions and their dependency on input features. It is found that PGML models, especially physics-guided gradient boosting and random forest regression, improve the prediction accuracy when compared to their purely data-driven ML counterparts by combining the knowledge extracted from experimental data with the mechanics of the problem considered. SHAP analysis reveals that the PGML model predictions and their dependency on input features are consistent with the existing domain knowledge, and that the inclusion of physics in PGML models help improve the prediction accuracy, especially in cases where other input features fail to capture the combined complex interaction among the variables involved.},
  archive      = {J_MLA},
  author       = {Sivapalan Gajan and Christopher Kantor},
  doi          = {10.1016/j.mlwa.2025.100702},
  journal      = {Machine Learning with Applications},
  month        = {9},
  pages        = {100702},
  shortjournal = {Mach. Learn. Appl.},
  title        = {Modeling of settlement of shallow-founded rocking structures using explainable physics-guided machine learning},
  volume       = {21},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Precision glass thermoforming assisted by neural networks. <em>MLA</em>, <em>21</em>, 100701. (<a href='https://doi.org/10.1016/j.mlwa.2025.100701'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Many glass products require thermoformed geometry with high precision. However, the traditional approach of developing a thermoforming process through trials and errors can cause a large waste of time and resources and often fails to produce successful outcomes. Hence, there is a need to develop an efficient predictive model, replacing the costly simulations or experiments, to assist the design of precision glass thermoforming. In this work, we report a surrogate model, based on a dimensionless back-propagation neural network (BPNN), that can adequately predict the form errors and thus compensate for these errors in mold design using geometric features and process parameters as inputs. Our trials with simulation and industrial data indicate that the surrogate model can predict forming errors with adequate accuracy. Although perception errors (mold designers’ decisions) and mold fabrication errors make the industrial training data less reliable than simulation data, our preliminary training and testing results still achieved a reasonable consistency with industrial data, suggesting that the surrogate models are directly implementable in the glass-manufacturing industry.},
  archive      = {J_MLA},
  author       = {Yuzhou Zhang and Mohan Hua and Jinan Liu and Haihui Ruan},
  doi          = {10.1016/j.mlwa.2025.100701},
  journal      = {Machine Learning with Applications},
  month        = {9},
  pages        = {100701},
  shortjournal = {Mach. Learn. Appl.},
  title        = {Precision glass thermoforming assisted by neural networks},
  volume       = {21},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multi-objective deep learning: Taxonomy and survey of the state of the art. <em>MLA</em>, <em>21</em>, 100700. (<a href='https://doi.org/10.1016/j.mlwa.2025.100700'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Simultaneously considering multiple objectives in machine learning has been a popular approach for several decades, with various benefits for multi-task learning, the consideration of secondary goals such as sparsity, or multicriteria hyperparameter tuning. However – as multi-objective optimization is significantly more costly than single-objective optimization – the recent focus on deep learning architectures poses considerable additional challenges due to the very large number of parameters, strong nonlinearities and stochasticity. On the other hand considering multiple criteria in deep learning presents many benefits, such as the just-mentioned multi-task learning, the consideration of performance versus adversarial robustness, or a more interpretable way for interactively adapting to changing preferences. This survey covers recent advancements in the area of multi-objective deep learning. We introduce a taxonomy of existing methods – based on the type of training algorithm as well as the decision maker’s needs – before listing recent advancements, and also successful applications. All three main learning paradigms supervised learning, unsupervised learning and reinforcement learning are covered, and we also address the recently very popular area of generative modeling. With a focus on the advantages and disadvantages of the existing training algorithms, this survey is formulated from an optimization perspective rather than organizing according to different learning paradigms or application areas.},
  archive      = {J_MLA},
  author       = {Sebastian Peitz and Sèdjro Salomon Hotegni},
  doi          = {10.1016/j.mlwa.2025.100700},
  journal      = {Machine Learning with Applications},
  month        = {9},
  pages        = {100700},
  shortjournal = {Mach. Learn. Appl.},
  title        = {Multi-objective deep learning: Taxonomy and survey of the state of the art},
  volume       = {21},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Forecasts and insights into japan’s fiscal future: Machine learning-based projections of city-level taxpayer numbers and total income from 2020 to 2100. <em>MLA</em>, <em>21</em>, 100699. (<a href='https://doi.org/10.1016/j.mlwa.2025.100699'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Japan’s economic landscape is undergoing profound transformations due to shifting demographic trends, including population decline, aging, and urban-rural disparities. This study applies advanced machine learning techniques and stepwise updating methodologies to predict city-level taxpayer numbers and total income across 1896 Japanese cities from 2020 to 2100. The models achieve high accuracy, with validation R 2 exceeding 98 %, ensuring robust long-term predictions. The findings reveal a 14.52 % decline in total taxpayers by 2100, closely following population trends, while total income remains relatively stable, even with an increase of 5.21 %. On the other hand, average income is projected to increase by 23.07 % by 2100. Despite an overall economic contraction, increasing labor participation helps sustain the tax base. However, spatial disparities persist, with rural areas experiencing severe declines in taxpayers and income, while metropolitan centers maintain higher resilience but still face income stagnation. These results underscore the need for regionally tailored policy interventions to mitigate the fiscal impacts of demographic shifts. The study contributes to predictive economic modeling by integrating high-resolution spatial and demographic data with explainable machine learning and offers valuable insights for policymakers navigating Japan’s long-term economic evolution.},
  archive      = {J_MLA},
  author       = {Chao Li and Alexander Ryota Keeley and Shunsuke Managi},
  doi          = {10.1016/j.mlwa.2025.100699},
  journal      = {Machine Learning with Applications},
  month        = {9},
  pages        = {100699},
  shortjournal = {Mach. Learn. Appl.},
  title        = {Forecasts and insights into japan’s fiscal future: Machine learning-based projections of city-level taxpayer numbers and total income from 2020 to 2100},
  volume       = {21},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Machine learning-based prediction of optimal antenatal care utilization among reproductive women in nigeria. <em>MLA</em>, <em>21</em>, 100698. (<a href='https://doi.org/10.1016/j.mlwa.2025.100698'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Background Despite global efforts, disparities in antenatal care (ANC) utilization persist in Nigeria, where maternal mortality remains alarmingly high (1047 deaths per 100,000 live births). Traditional statistical models often fall short in identifying complex non-linear relationships in population health data. Machine learning (ML) offers a promising alternative that uncovers hidden patterns and improves prediction accuracy. Methods This study used data from the 2018 Nigeria Demographic and Health Survey (NDHS), a nationally representative data set. After data preprocessing and feature selection, six supervised ML algorithms—Logistic Regression, Support Vector Machine, K-Nearest Neighbors, Decision Tree, Random Forest, and XGBoost—were applied using Python 3.9. The model performance was evaluated using accuracy, precision, recall, F1-score, and area under the receiver operating characteristic curve (AUROC). Feature importance was assessed using permutation importance and Gini impurity score. Results Among all models, Random Forest achieved the best performance, with 90 % accuracy, 0.90 precision and recall, an F1-score of 0.91, and an AUROC of 0.90. Permutation and Gini importance analyses identified the place of delivery, region, residence, and educational level as the most influential predictors. Other moderately important features included distance to health facilities, husband’s occupation, number of births, and healthcare decision-making autonomy—factors not highlighted by traditional statistical approaches. Conclusion Machine learning, particularly Random Forest, demonstrated strong predictive power in identifying the key determinants of ANC utilization. These findings highlight the potential of ML to inform targeted maternal health interventions and improve outcomes in low-resource settings, such as Nigeria.},
  archive      = {J_MLA},
  author       = {Jamilu Sani and Adeyemi Oluwagbemiga and Mohamed Mustaf Ahmed},
  doi          = {10.1016/j.mlwa.2025.100698},
  journal      = {Machine Learning with Applications},
  month        = {9},
  pages        = {100698},
  shortjournal = {Mach. Learn. Appl.},
  title        = {Machine learning-based prediction of optimal antenatal care utilization among reproductive women in nigeria},
  volume       = {21},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Improving neural network training using dynamic learning rate schedule for PINNs and image classification. <em>MLA</em>, <em>21</em>, 100697. (<a href='https://doi.org/10.1016/j.mlwa.2025.100697'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Training neural networks can be challenging, especially as the complexity of the problem increases. Despite using wider or deeper networks, training them can be a tedious process, especially if a wrong choice of the hyperparameter is made. The learning rate is one of such crucial hyperparameters, which is usually kept static during the training process. Learning dynamics in complex systems often requires a more adaptive approach to the learning rate. This adaptability becomes crucial to effectively navigate varying gradients and optimize the learning process during the training process. In this paper, a dynamic learning rate scheduler (DLRS) algorithm is presented that adapts the learning rate based on the loss values calculated during the training process. Experiments are conducted on problems related to physics-informed neural networks (PINNs) and image classification using multilayer perceptrons and convolutional neural networks, respectively. The results demonstrate that the proposed DLRS accelerates training and improves stability.},
  archive      = {J_MLA},
  author       = {Veerababu Dharanalakota and Ashwin Arvind Raikar and Prasanta Kumar Ghosh},
  doi          = {10.1016/j.mlwa.2025.100697},
  journal      = {Machine Learning with Applications},
  month        = {9},
  pages        = {100697},
  shortjournal = {Mach. Learn. Appl.},
  title        = {Improving neural network training using dynamic learning rate schedule for PINNs and image classification},
  volume       = {21},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Feature engineering through two-level genetic algorithm. <em>MLA</em>, <em>21</em>, 100696. (<a href='https://doi.org/10.1016/j.mlwa.2025.100696'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep learning models are widely used for their high predictive performance, but often lack interpretability. Traditional machine learning methods, such as logistic regression and ensemble models, offer greater interpretability but typically have lower predictive capacity. Feature engineering can enhance the performance of interpretable models by identifying features that optimize classification. However, existing feature engineering methods face limitations: (1) they usually do not apply non-linear transformations to features, ignoring the benefits of non-linear spaces; (2) they usually perform feature selection only once, failing to reduce uncertainty through repeated experiments; and (3) traditional methods like minimum redundancy maximum relevance (mRMR) require additional hyperparameters to define the number of selected features. To address these issues, this study proposed a hierarchical two-level feature engineering approach. In the first level, relevant features were identified using multiple bootstrapped training sets. For each training set, the features were expanded using seven non-linear transformation functions, and the minimum feature set maximizing ensemble model performance was selected using the Non-Dominated Sorting Genetic Algorithm II (NSGA-II). In the second level, candidate feature sets were aggregated using two strategies. We evaluated our approach on twelve datasets from various fields, achieving an average F1 score improvement of 1.5% while reducing the feature set size by 54.5%. Moreover, our approach outperformed or matched traditional filter-based methods. Our approach is available through a Python library ( feature-gen ), enabling others to benefit from this tool. This study highlights the utility of evolutionary algorithms to generate feature sets that enhance the performance of interpretable machine learning models.},
  archive      = {J_MLA},
  author       = {Aditi Gulati and Armin Felahatpisheh and Camilo E. Valderrama},
  doi          = {10.1016/j.mlwa.2025.100696},
  journal      = {Machine Learning with Applications},
  month        = {9},
  pages        = {100696},
  shortjournal = {Mach. Learn. Appl.},
  title        = {Feature engineering through two-level genetic algorithm},
  volume       = {21},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). DCLMA: Deep correlation learning with multi-modal attention for visual-audio retrieval. <em>MLA</em>, <em>21</em>, 100695. (<a href='https://doi.org/10.1016/j.mlwa.2025.100695'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The cross-modal retrieval task aims to retrieve audio modality information from the database that best matches the visual modality and vice versa. One of the key challenges in this field is the inconsistency of audio and visual features, which increases the complexity of capturing cross-modal information, making it difficult for machines to accurately understand visual content and retrieve suitable audio data. In this work, we propose a novel deep correlation learning with multi-modal attention (DCLMA) for visual-audio retrieval, which selectively focuses on relevant information fragments through multi-modal attention, and effectively integrates audio-visual information to enhance modal interaction and correlation representation learning capabilities. First, to achieve accurate retrieval of associated multi-modal data, we utilize multiple attention-composed models to interactively learn the complex correlation of audio and visual multi-scale features. Second, cross-modal attention is exploited to mine inter-modal correlations at the global level. Finally, we combine multi-scale and global-level representations to obtain modality-integrated representations, which enhance the representation capabilities of inputs. Furthermore, our objective function supervised model learns discriminative and modality-invariant features between samples from different semantic categories in the mutual latent space. Experimental results on cross-modal retrieval on two widely used benchmark datasets demonstrate that our proposed approach is superior in learning effective representations and significantly outperforms state-of-the-art cross-modal retrieval methods. Code is available at https://github.com/zhangjiwei-japan/cross-modal-visual-audio-retrieval},
  archive      = {J_MLA},
  author       = {Jiwei Zhang and Hirotaka Hachiya},
  doi          = {10.1016/j.mlwa.2025.100695},
  journal      = {Machine Learning with Applications},
  month        = {9},
  pages        = {100695},
  shortjournal = {Mach. Learn. Appl.},
  title        = {DCLMA: Deep correlation learning with multi-modal attention for visual-audio retrieval},
  volume       = {21},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Regularized regression outperforms trees for predicting cognitive function in the health and retirement study. <em>MLA</em>, <em>21</em>, 100694. (<a href='https://doi.org/10.1016/j.mlwa.2025.100694'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Background Generalized linear models have been favored in healthcare research due to their interpretability. In contrast, tree-based models, such as random forest or boosted trees, are often preferred in machine learning (ML) and commercial settings due to their strong predictive performance. However, for clinical applications, model interpretability remains essential for actionable results and patient understanding. This study used ML to detect cognitive decline for the purpose of timely screening and uncovering associations with psychosocial determinants. All models were interpreted to enhance transparency and understanding of their predictions. Methods Data from the 2018 to 2020 Health and Retirement Study was used to create three linear regression models and three tree-based models. Ten percent of the sample was withheld for estimating performance, and model tuning used five-fold cross validation with two repeats. Survey frequency weights were applied during tuning, training, and final evaluation. Model performance was evaluated using RMSE and R 2 and interpretability was assessed via coefficients, variable importance, and decision trees. Results The elastic net model had the best performance (RMSE = 3.520, R 2 = 0.435), followed by standard linear regression, boosted trees, random forest, multivariate adaptive regression splines, and lastly, decision trees. Across all models, baseline cognitive function and frequency of computer use were the most influential predictors. Conclusion Elastic net regression outperformed tree-based models, suggesting that cognitive outcomes may be best modeled with additive linear relationships. Its ability to remove correlated and weak predictors contributed to its balance of interpretability and predictive performance for this particular dataset.},
  archive      = {J_MLA},
  author       = {Kyle Masato Ishikawa and Deborah Taira and Joseph Keaweʻaimoku Kaholokula and Matthew Uechi and James Davis and Eunjung Lim},
  doi          = {10.1016/j.mlwa.2025.100694},
  journal      = {Machine Learning with Applications},
  month        = {9},
  pages        = {100694},
  shortjournal = {Mach. Learn. Appl.},
  title        = {Regularized regression outperforms trees for predicting cognitive function in the health and retirement study},
  volume       = {21},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). EmoFusion: An integrated machine learning model leveraging embeddings and lexicons to improve textual emotion classification. <em>MLA</em>, <em>21</em>, 100693. (<a href='https://doi.org/10.1016/j.mlwa.2025.100693'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Human emotions are complicated and intertwined with cognitive processes, influencing mental health, learning, and decision-making. The Web 2.0 era has seen a remarkable spike in the number of people sharing their experiences and emotions on online social media, mostly through posts or text messages. Due to inherent challenges associated with textual data, the issue of discovering the intricate relationships between texts and its inherent emotions is still an increasingly prevalent topic in AI and NLP. This paper presents EmoFusion , an integrated machine learning model that improves emotion classification in textual data by integrating pre-trained word embeddings and emotion lexicons. Instead of relying on a single emotion lexicon, EmoFusion integrates multiple emotion lexicons since a single lexicon might not fully cover all possible words or phrases linked with emotions. The proposed approach uses semantically related features to bridge the semantic gap between words and emotions, capturing a wide range of emotional nuances and resulting in better classification performance. The efficacy is further improved by employing emotion-specific pre-processing techniques. EmoFusion is evaluated using three benchmark datasets, namely Google AI GoEmotions, CBET, and TEC. The evaluation results demonstrate a significant improvement compared to six baselines and a state-of-the-art technique using different classifiers.},
  archive      = {J_MLA},
  author       = {Anjali Bhardwaj and Muhammad Abulaish},
  doi          = {10.1016/j.mlwa.2025.100693},
  journal      = {Machine Learning with Applications},
  month        = {9},
  pages        = {100693},
  shortjournal = {Mach. Learn. Appl.},
  title        = {EmoFusion: An integrated machine learning model leveraging embeddings and lexicons to improve textual emotion classification},
  volume       = {21},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Enhanced credit risk prediction using deep learning and SMOTE-ENN resampling. <em>MLA</em>, <em>21</em>, 100692. (<a href='https://doi.org/10.1016/j.mlwa.2025.100692'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Credit risk prediction is a vital task in financial services, ensuring that institutions can manage their lending risks effectively. This study investigates the effectiveness of deep learning (DL) models for credit risk prediction, with a focus on addressing the challenge of class imbalance and the black box nature of these models using the Synthetic Minority Over-sampling Technique - Edited Nearest Neighbor (SMOTE-ENN) resampling method and Shapley Additive Explanations (SHAP), respectively. The study compares the performance of various DL architectures, including Convolutional Neural Networks (CNN), Long Short-Term Memory networks (LSTM), Gated Recurrent Units (GRU), and Graph Neural Networks (GNN), on two real-world datasets: the Australian and German credit datasets. The findings reveal that the GRU model, enhanced with SMOTE-ENN resampling, outperforms other models in terms of accuracy, sensitivity, and specificity. The superior performance of the GRU-SMOTE-ENN model demonstrates its potential as a robust deep learning technique for financial institutions to enhance credit risk assessment. Additionally, the study demonstrates how the integration of SHAP values significantly improves the interpretability of deep learning models, making them more transparent and trustworthy for stakeholders.},
  archive      = {J_MLA},
  author       = {Idowu Aruleba and Yanxia Sun},
  doi          = {10.1016/j.mlwa.2025.100692},
  journal      = {Machine Learning with Applications},
  month        = {9},
  pages        = {100692},
  shortjournal = {Mach. Learn. Appl.},
  title        = {Enhanced credit risk prediction using deep learning and SMOTE-ENN resampling},
  volume       = {21},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Performance benchmarking of multimodal data-driven approaches in industrial settings. <em>MLA</em>, <em>21</em>, 100691. (<a href='https://doi.org/10.1016/j.mlwa.2025.100691'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Data-driven solutions are increasingly transforming the industrial sector, yet collecting large-scale, multimodal datasets remains costly and challenging. This paper presents three synthetic multimodal datasets that replicate real-world industrial conditions across varying levels of complexity, designed to benchmark multimodal machine learning models. We validate their utility through a series of experiments. Cross-modal prediction and domain adaptation demonstrate that the datasets effectively capture strong multimodal correlations. Multimodal reconstruction experiments confirm the internal consistency and richness of the fused representations, indicating that the modalities complement each other in capturing underlying structure. Additionally, multimodal regression significantly outperforms unimodal baselines, underscoring the predictive strength gained through multimodal integration. Together, these results demonstrate the utility of our datasets, establishing a solid baseline for future research and encouraging further advancements in industrial data-driven solutions.},
  archive      = {J_MLA},
  author       = {Diyar Altinses and Andreas Schwung},
  doi          = {10.1016/j.mlwa.2025.100691},
  journal      = {Machine Learning with Applications},
  month        = {9},
  pages        = {100691},
  shortjournal = {Mach. Learn. Appl.},
  title        = {Performance benchmarking of multimodal data-driven approaches in industrial settings},
  volume       = {21},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Computational argumentation based multi-agent consensual approach for semantically interpretable credit rating. <em>MLA</em>, <em>21</em>, 100690. (<a href='https://doi.org/10.1016/j.mlwa.2025.100690'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {It is well known that ensemble learning can make use of various classifiers, to capture various kinds of multimodal data features for credit rating in big data-driven financial risk analysis. However, it is very hard to provide semantic explanations for the evaluation outputs of intelligent systems when multiple classifiers come to a consensus. In this paper, a computational argumentation based multi-agent consensual approach has been proposed for credit rating. The method provides a natural means for justifying a decision, much like how people draw conclusions through argumentation. In particular, semantically interpretable consensus about the credit level of entities is provided with easily assimilated reasons in multiple classifier systems. The experimental evaluation over six benchmark real-world credit rating datasets shows that, compared with traditional strategies, not only better credit rating performance but also semantic explanation could be gained with the help of computational argumentation in ensemble learning for credit rating. Regarding multi-agent consensual credit rating, the competitive advantage offered by computational argumentation is that reasonable interpretation with arguments can be provided for financial decision makers, just like cognitive process in human intelligence.},
  archive      = {J_MLA},
  author       = {Chenhao Yu and Bohang Fu and Leilei Chang and Zhiyong Hao},
  doi          = {10.1016/j.mlwa.2025.100690},
  journal      = {Machine Learning with Applications},
  month        = {9},
  pages        = {100690},
  shortjournal = {Mach. Learn. Appl.},
  title        = {Computational argumentation based multi-agent consensual approach for semantically interpretable credit rating},
  volume       = {21},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Artificial neural networks and support vector machines for more accurate cost estimation in underground mining: A contractor's viewpoint. <em>MLA</em>, <em>21</em>, 100689. (<a href='https://doi.org/10.1016/j.mlwa.2025.100689'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Accurate cost estimation is crucial in effective decision-making and evaluation in underground mining projects. Machine learning techniques have shown enormous potential in enhancing cost estimation accuracy in various industries. This study harnesses artificial neural networks (ANN) and Support Vector Machines (SVM) to estimate operating costs in underground mining. Special emphasis is placed on cost estimation from a contractor’s perspective. Mining contractors are sensitive to deviations from the estimated costs because slight deviations may result in losing a contract bid or financial loss in an awarded project. The proposed approach can help contractors make more informed decisions and improve project management. Comprehensive data containing various parameters that impact the cost of underground mining projects, such as equipment type utilization, rock type, and cross-sectional area, were collected. This dataset was used to train and evaluate ANN and SVM models that provide more accurate cost estimation for underground mining projects. The best model achieved a mean average percentage error (MAPE) of 5.31 % for the ANN model and 3.05 % for the SVM model, outperforming traditional cost estimation methods. This study demonstrates the potential of machine learning in enhancing the performance of the cost estimation process.},
  archive      = {J_MLA},
  author       = {Juan Camilo García Vásquez and Mustafa Kumral},
  doi          = {10.1016/j.mlwa.2025.100689},
  journal      = {Machine Learning with Applications},
  month        = {9},
  pages        = {100689},
  shortjournal = {Mach. Learn. Appl.},
  title        = {Artificial neural networks and support vector machines for more accurate cost estimation in underground mining: A contractor's viewpoint},
  volume       = {21},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Hierarchical data modeling: A systematic comparison of statistical, tree-based, and neural network approaches. <em>MLA</em>, <em>21</em>, 100688. (<a href='https://doi.org/10.1016/j.mlwa.2025.100688'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Hierarchical modeling approaches have evolved significantly, yet comprehensive comparisons between fundamentally different methodological paradigms remain limited. This research presents a systematic comparative analysis of three distinct hierarchical modeling approaches: statistical (Hierarchical Mixed Model), tree-based (Hierarchical Random Forest), and neural (Hierarchical Neural Network). Based on the 2019 National Inpatient Sample — comprising more than seven million records from 4568 hospitals across four U.S. regions — the models were assessed for their ability to predict length of stay at the patient, hospital, and regional levels. The evaluation framework integrated quantitative metrics and qualitative factors, including analyses across varying sample sizes, simplified hierarchies, and a separate intensive-care dataset. Results demonstrate that tree-based approaches consistently outperform alternatives in predictive accuracy and explanation of variance while maintaining computational efficiency. These performance patterns remain generally consistent across sample sizes, simplified hierarchies, and the external dataset. Neural approaches excel at capturing group-level distinctions but require substantial computational resources and exhibit prediction bias. Statistical approaches offer rapid inference and interpretability but underperform in accuracy at intermediate hierarchical levels. Each model exhibits distinctive hierarchical information processing: neural models favor bottom-up flow, statistical models emphasize top-down constraints, and tree-based models achieve balanced integration. This research establishes practical guidelines for selecting appropriate hierarchical modeling approaches based on data characteristics, computational constraints, and analytical requirements, thereby advancing understanding of fundamental trade-offs in multilevel analysis.},
  archive      = {J_MLA},
  author       = {Marzieh Amiri Shahbazi and Nasibeh Azadeh-Fard},
  doi          = {10.1016/j.mlwa.2025.100688},
  journal      = {Machine Learning with Applications},
  month        = {9},
  pages        = {100688},
  shortjournal = {Mach. Learn. Appl.},
  title        = {Hierarchical data modeling: A systematic comparison of statistical, tree-based, and neural network approaches},
  volume       = {21},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Fine-tuned YOLO-based deep learning model for detecting malaria parasites and leukocytes in thick smear images: A tanzanian case study. <em>MLA</em>, <em>21</em>, 100687. (<a href='https://doi.org/10.1016/j.mlwa.2025.100687'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Malaria remains a serious public health concern in developing countries, where accurate diagnosis is critical for effective treatment. Reliable and timely detection of malaria parasites and leukocytes is essential for precise parasitemia quantification. However, manual identification is labor-intensive, time-consuming, and prone to diagnostic errors—particularly in resource-limited settings. To address this challenge, this paper proposes a fine-tuned deep learning model for detecting malaria parasites and leukocytes in thick smear images. The model is based on the YOLOv10 and YOLOv11 object detection architectures, each independently trained, validated, and evaluated on a custom-annotated dataset collected from hospitals in Tanzania to ensure contextual relevance. A fivefold cross-validation, followed by statistical analysis, was used to identify the best-performing model. Results demonstrate that the optimized YOLOv11m model achieved the highest performance, with a statistically significant improvement ( p < .001), attaining a mean mAP@50 of 86.2 % ± 0.3 % and a mean recall of 78.5 % ± 0.2 %. These findings highlight the potential of the proposed model to enhance diagnostic accuracy, support effective parasitemia quantification, and ultimately reduce malaria-related mortality in resource-limited settings.},
  archive      = {J_MLA},
  author       = {Beston Lufyagila and Bonny Mgawe and Anael Sam},
  doi          = {10.1016/j.mlwa.2025.100687},
  journal      = {Machine Learning with Applications},
  month        = {9},
  pages        = {100687},
  shortjournal = {Mach. Learn. Appl.},
  title        = {Fine-tuned YOLO-based deep learning model for detecting malaria parasites and leukocytes in thick smear images: A tanzanian case study},
  volume       = {21},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). AdVision: An efficient and effective deep learning based advertisement detector for printed media. <em>MLA</em>, <em>21</em>, 100686. (<a href='https://doi.org/10.1016/j.mlwa.2025.100686'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Automated advertisement detection in newspapers is a challenging task due to the diversity in print layouts, formats, and design styles. This task has critical applications in media monitoring, content analysis, and advertising analytics. To address these challenges, we introduce AdVision, a deep-learning-based solution that treats advertisements as unique visual objects. We provide a comparative study of various detection architectures, including one-stage, two-stage, and transformer-based detectors, to identify the most effective approach for detecting advertisements. Our results are validated through extensive experiments conducted under different conditions and metrics. Newspapers from four different countries — Denmark, Norway, Sweden, and the UK — were selected to demonstrate the variety of languages and print formats. Additionally, we conduct a cross-analysis to show how training on one language can generalize to another. To enhance the explainability of our results, we employ GradCAM++ (Chattopadhay et al., 2018) heatmaps. Our experiments demonstrate that the YOLOv8 model achieves superior performance, balancing high precision and recall with minimal inference latency, making it particularly suitable for high-throughput advertisement detection.},
  archive      = {J_MLA},
  author       = {Faeze Zakaryapour Sayyad and Irida Shallari and Seyed Jalaleddin Mousavirad and Mattias O’Nils and Faisal Z. Qureshi},
  doi          = {10.1016/j.mlwa.2025.100686},
  journal      = {Machine Learning with Applications},
  month        = {9},
  pages        = {100686},
  shortjournal = {Mach. Learn. Appl.},
  title        = {AdVision: An efficient and effective deep learning based advertisement detector for printed media},
  volume       = {21},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Applying deep reinforcement learning to minimize flow fluctuations in digital flow control. <em>MLA</em>, <em>21</em>, 100685. (<a href='https://doi.org/10.1016/j.mlwa.2025.100685'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This work addresses the non-linear and complex optimization challenges in digital hydraulic flow control, where piecewise behaviors and unpredictable interactions make traditional optimization methods impractical for presented system with continuous inputs. The study aims to promote real-time application of artificial intelligence algorithms for system optimization. Most off-highway construction and agriculture equipment use hydraulic valve manifolds, which offer unmatched power density and dynamics, excelling over electro actuators in high-capacity applications. The growing demand for more efficient and accurately controlled autonomous heavy machinery has driven the need for steady and precise flow control systems with reduced pressure drop. However, managing flow and pressure fluctuations when switching valves remains a significant challenge. The proposed agent effectively mitigates flow fluctuations by interactively refining valve-timing decisions over tens of thousands of possible actions. Validated under approximately 90 % of available conditions and tested against unseen pressure values, the agent achieved a median integrated flow error of less than 0.5 cm³, showcasing its potential for AI-driven optimization in digital hydraulic systems.},
  archive      = {J_MLA},
  author       = {Essam Elsaed and Matti Linjama},
  doi          = {10.1016/j.mlwa.2025.100685},
  journal      = {Machine Learning with Applications},
  month        = {9},
  pages        = {100685},
  shortjournal = {Mach. Learn. Appl.},
  title        = {Applying deep reinforcement learning to minimize flow fluctuations in digital flow control},
  volume       = {21},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). VICCA: Visual interpretation and comprehension of chest X-ray anomalies in generated report without human feedback. <em>MLA</em>, <em>21</em>, 100684. (<a href='https://doi.org/10.1016/j.mlwa.2025.100684'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As artificial intelligence (AI) becomes increasingly central to healthcare, the demand for explainable and trustworthy models is paramount. Current report generation systems for chest X-rays (CXR) often lack mechanisms for validating outputs without expert oversight, raising concerns about reliability and interpretability. To address these challenges, we propose a novel multimodal framework designed to enhance the semantic alignment between text and image context and the localization accuracy of pathologies within images and reports for AI-generated medical reports. Our framework integrates two key modules: a Phrase Grounding Model, which identifies and localizes pathologies in CXR images based on textual prompts, and a Text-to-Image Diffusion Module, which generates synthetic CXR images from prompts while preserving anatomical fidelity. By comparing features between the original and generated images, we introduce a dual-scoring system: one score quantifies localization accuracy, while the other evaluates semantic consistency between text and image features. Our approach significantly outperforms existing methods in pathology localization, achieving an 8% improvement in Intersection over Union score. It also surpasses state-of-the-art methods in CXR text-to-image generation, with a 1% gain in similarity metrics. Additionally, the integration of phrase grounding with diffusion models, coupled with the dual-scoring evaluation system, provides a robust mechanism for validating report quality, paving the way for more reliable and transparent AI in medical imaging.},
  archive      = {J_MLA},
  author       = {Sayeh Gholipour Picha and Dawood Al Chanti and Alice Caplier},
  doi          = {10.1016/j.mlwa.2025.100684},
  journal      = {Machine Learning with Applications},
  month        = {9},
  pages        = {100684},
  shortjournal = {Mach. Learn. Appl.},
  title        = {VICCA: Visual interpretation and comprehension of chest X-ray anomalies in generated report without human feedback},
  volume       = {21},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Benchmarking transformer embedding models for biomedical terminology standardization. <em>MLA</em>, <em>21</em>, 100683. (<a href='https://doi.org/10.1016/j.mlwa.2025.100683'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Biomedical text in public databases often exhibits unstandardized terminology and inconsistencies that impede machine learning applications and hinder data integration across biomedical databases. Leveraging generalized and specialized transformer/large language models (LLMs) offers a potential scalable solution for terminology standardization. We evaluated this opportunity using the National Institutes of Health Clinical Trials Registry (CTR), which contains heterogeneous, free-text records of disease from therapeutic trials. To systematically assess the ability of machine learning methods to assign biomedical terms accurately, we benchmarked 36 approaches using transformer/LLM-based text embeddings, along with traditional text-matching algorithms, against a clinical gold standard: the World Health Organization Classification of Tumours system (WHO System, also known as the WHO Blue Books). For this evaluation, we developed CANTOS (Clinical Trials Automated Nomenclature and Tumor Ontology Standardization), a computational benchmarking framework that extracts tumor names from the CTR and standardizes them using the WHO System and the National Cancer Institute Thesaurus (NCIt). We assessed standardization accuracy using a random sample of 1600 CTR tumor names manually annotated with WHO System terms. LLM/transformer-based embedding methods significantly outperformed text-matching approaches: all-MiniLM-L12-v2+Euclidean distance achieved 67.7% accuracy (WHO-5th edition), while LTE-3+Euclidean distance achieved 69.4% (WHO-all editions). Text-matching methods peaked at 32.6% accuracy. A majority voting approach combining three high-accuracy, low-agreement methods improved accuracy to 71.9% (WHO-5th) and 71.6% (WHO-all). Our findings demonstrate the effectiveness of embedding models in standardizing biomedical terminology and provide a reproducible framework for benchmarking machine learning methods against clinical gold standards using real-world datasets.},
  archive      = {J_MLA},
  author       = {Aditya Lahiri and Sangeeta Shukla and Ben Stear and Taha Mohseni Ahooyi and Katherine Beigel and Elizabeth Margolskee and Deanne Taylor},
  doi          = {10.1016/j.mlwa.2025.100683},
  journal      = {Machine Learning with Applications},
  month        = {9},
  pages        = {100683},
  shortjournal = {Mach. Learn. Appl.},
  title        = {Benchmarking transformer embedding models for biomedical terminology standardization},
  volume       = {21},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multimodal bearing fault classification under variable conditions: A 1D CNN with transfer learning. <em>MLA</em>, <em>21</em>, 100682. (<a href='https://doi.org/10.1016/j.mlwa.2025.100682'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Bearings play an integral role in ensuring the reliability and efficiency of rotating machinery — reducing friction and handling critical loads. Bearing failures that constitute up to 90% of mechanical faults highlight the imperative need for reliable condition monitoring and fault detection. This study proposes a multimodal bearing fault classification approach that relies on vibration and motor phase current signals within a one-dimensional convolutional neural network (1D CNN) framework. The method fuses features from multiple signals to enhance the accuracy of fault detection. Under the baseline condition (1500 rpm, 0.7 Nm load torque, and 1000 N radial force), the model reaches an accuracy of 96% with addition of L2 regularization. In addition, the model demonstrates robust performance across three distinct operating conditions by employing transfer learning (TL) strategies. Among the tested TL variants, the approach that preserves parameters up to the first max-pool layer and then adjusts subsequent layers achieves the highest performance. While this approach attains excellent accuracy across varied conditions, it requires more computational time due to its greater number of trainable parameters. To address resource constraints, less computationally intensive models offer feasible trade-offs, albeit at a slight accuracy cost. Overall, this multimodal 1D CNN framework with late fusion and TL strategies lays a foundation for more accurate, adaptable, and efficient bearing fault classification in industrial environments with variable operating conditions.},
  archive      = {J_MLA},
  author       = {Tasfiq E. Alam and Md Manjurul Ahsan and Shivakumar Raman},
  doi          = {10.1016/j.mlwa.2025.100682},
  journal      = {Machine Learning with Applications},
  month        = {9},
  pages        = {100682},
  shortjournal = {Mach. Learn. Appl.},
  title        = {Multimodal bearing fault classification under variable conditions: A 1D CNN with transfer learning},
  volume       = {21},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Accuracy and efficiency in financial markets forecasting using meta-learning under resource constraints. <em>MLA</em>, <em>21</em>, 100681. (<a href='https://doi.org/10.1016/j.mlwa.2025.100681'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep learning and hybrid deep learning models are widely regarded as some of the most effective predictive modeling techniques to date. Their hierarchical architecture enables them to capture complex, non-linear relationships among features and uncover hidden patterns within data, making them particularly powerful for tasks involving high-dimensional and unstructured inputs. But, these models are computationally intensive and require substantial processing time. Moreover, their predictive efficiency is highly dependent on the availability of large-scale datasets. In this study, meta learning model is employed for the prediction of two financial markets: equity market and crypto market. NASDAQ and S&P 500 index has been taken for equity market prediction. On the other hand, Bitcoin & Ethereum are considered for crypto market. Three deep learning models: LSTM, GRU and CNN are trained for the prediction of these four indices and a hybrid deep learning model of GRU and CNN is also developed. Based on RMSE, MAE and R 2 values, it is observed that meta learning yields best results among all trained models with minimum time and using scarce computation resources based on small dataset.},
  archive      = {J_MLA},
  author       = {Komal Batool and Mirza Mahmood Baig and Ubaida Fatima},
  doi          = {10.1016/j.mlwa.2025.100681},
  journal      = {Machine Learning with Applications},
  month        = {9},
  pages        = {100681},
  shortjournal = {Mach. Learn. Appl.},
  title        = {Accuracy and efficiency in financial markets forecasting using meta-learning under resource constraints},
  volume       = {21},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Active learning model used for android malware detection. <em>MLA</em>, <em>21</em>, 100680. (<a href='https://doi.org/10.1016/j.mlwa.2025.100680'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Smartphones have become one of the main products in today’s world. However, the security risks of smartphones are high compared with those of other devices. Smartphone users face threats to their privacy and property protection. Android malware identification is essential to prevent mobile applications from being compromised because a number of new techniques for Android malware assaults have appeared and evolved. Researchers are constantly improving and inventing new approaches for identifying Android malware. Modern research has extensively used machine-learning techniques. This study introduces a technique known as active learning, which analyzes device behavior and traffic monitoring on devices to discover malware. To identify malware more accurately, this hybrid architecture includes active learning, supervised machine-learning models, and threat intelligence tools. The results of the experiments demonstrated the effectiveness of our proposed model, which outperformed both supervised machine learning and the traditional machine learning technique in terms of training and test accuracies of 92.36 % and 85.9 % and loss functions of 22.5 % and 33.2 %, respectively, and achieved the highest degree of accuracy.},
  archive      = {J_MLA},
  author       = {Md․Habibullah Shakib and Md. Yeasin and Kh. Mustafizur Rahman and Fabiha Faiz Mahi and Md. Mahsin-Ul-Islam and Saddam Hossain},
  doi          = {10.1016/j.mlwa.2025.100680},
  journal      = {Machine Learning with Applications},
  month        = {9},
  pages        = {100680},
  shortjournal = {Mach. Learn. Appl.},
  title        = {Active learning model used for android malware detection},
  volume       = {21},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Connecting instance and model sparsity in multiple instance learning. <em>MLA</em>, <em>21</em>, 100679. (<a href='https://doi.org/10.1016/j.mlwa.2025.100679'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multiple instance learning (MIL) is a problem structure in which large sets of instances are grouped into ensembles/bags and labels are provided only at bag level. A general formulation is that the probability of a bag being positive is estimated as the maximum probability of being positive across its set of instances. Accurate bag label prediction depends on the effective identification of the instances with the maximum probability of being positive. These instances may be very sparse (i.e., low witness rates). We exemplify our study by structuring a data generation process mimicking an archetypal MIL problem: the prediction of a patient’s disease state based on immune receptor sequences of a very large number of adaptive immune cells, where only a few are involved in the etiology of the given disease. We thus consider cases where instances are short sequences and explore under which circumstances the sparsity of instance relevance for bag labels (low witness rate) corresponds to a sparsity of feature relevance for the determination (prediction) of bag labels. In such circumstances, we explore for which condition the different sparsities could support the detection of positive instances and relevant features, as well as exploring when such sparsity increases the effectiveness of strongly regularised LASSO models. For the task, we constructed a data-generating process where bags are composed of large sets of short sequences and systematically explored how strongly regularised logistic regression models performed across a range of data simulation parameterisations. We conclude by quantitatively reporting at which MIL problem characteristics bag label prediction is accurate even at lower witness rates. Our approach and finding provide a robust guide on the limits of multiple instance learning problems with sparse data. In particular, we provide a statistical support for experimental campaigns on immune receptors highlighting data requirements for a desirable identification accuracy of the etiology of the given disease.},
  archive      = {J_MLA},
  author       = {Enrico Riccardi and Thomas Minotto and Ingrid Hobæk Haff and Geir Kjetil Sandve},
  doi          = {10.1016/j.mlwa.2025.100679},
  journal      = {Machine Learning with Applications},
  month        = {9},
  pages        = {100679},
  shortjournal = {Mach. Learn. Appl.},
  title        = {Connecting instance and model sparsity in multiple instance learning},
  volume       = {21},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Generating impossible queries. <em>MLA</em>, <em>21</em>, 100677. (<a href='https://doi.org/10.1016/j.mlwa.2025.100677'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Modern databases are not designed to answer queries that rely on information not explicitly stored in their schemas. Here, we present a deep learning-based querying pipeline capable of generating “impossible queries”, those requiring inference over latent relationships, contextual understanding, and multimodal reasoning. Our system integrates Gemini 1.5 Pro and a fine-tuned BERT model to process complex inputs across text, image, and document modalities, enriching static records with derived features. We demonstrate that this architecture enables robust performance across basic, advanced, and integrated reasoning scenarios. Unlike conventional methods limited to explicit fields or handcrafted rules, our approach dynamically identifies subtle attributes, such as the presence of stylized imagery or implicit product features, and translates them into actionable primary key filters. We evaluate the system against state-of-the-art baselines, including CLIP, RAG, and heuristic SQL matches, achieving 99% accuracy across real-world annotated scenarios. The pipeline outperforms traditional CRUD operations in relevance, especially in multi-tenant and cloud-based environments where schema heterogeneity and data complexity are prevalent. Through experiments on Amazon product datasets, we show how our system distinguishes between nuanced cases like stylized versus real photos and infers attributes despite their absence in structured fields. These findings highlight the potential of integrating deep learning into querying frameworks to unlock latent insights and automate reasoning at scale. Our results mark a significant step toward intelligent database systems capable of reasoning beyond their schemas and responding meaningfully to real-world human intent.},
  archive      = {J_MLA},
  author       = {Mehdi Nejjar and Aymane Hassini and Yousra Chtouki},
  doi          = {10.1016/j.mlwa.2025.100677},
  journal      = {Machine Learning with Applications},
  month        = {9},
  pages        = {100677},
  shortjournal = {Mach. Learn. Appl.},
  title        = {Generating impossible queries},
  volume       = {21},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Embedded feature selection using dual-network architecture. <em>MLA</em>, <em>21</em>, 100672. (<a href='https://doi.org/10.1016/j.mlwa.2025.100672'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Feature selection is essential for eliminating noise, reducing redundancy, simplifying computational complexity, and lowering data collection and processing costs. However, existing methods often face challenges due to the complexity of feature interdependencies, uncertainty regarding the exact number of relevant features, and the need for hyperparameter optimization, which increases methodological complexity. This research proposes a novel dual-network architecture for feature selection that addresses these issues. The architecture consists of a task model and a selection model. First, redundant features are fed into the selection model, which generates a binary mask aligned with the input feature dimensions. This mask is applied to a shifted version of the original features, serving as input to the task model. The task model then uses the selected features to perform the target supervised task. Simultaneously, the selection model aims to minimize the cumulative value of the mask, thus selecting the most relevant features with minimal impact on the task model’s performance. The method is evaluated using benchmark and synthetic datasets across different supervised tasks. Comparative evaluation with state-of-the-art techniques demonstrates that the proposed approach exhibits superior or competitive feature selection capabilities, achieving a reduction of 90% or more in feature count. This is particularly notable in the presence of non-linear feature interdependencies. The key advantages of the proposed method are its ability to self-determine the number of relevant features needed for the supervised task and its simplicity, requiring the pre-definition of only a single hyperparameter, for which an estimation approach is suggested.},
  archive      = {J_MLA},
  author       = {Abderrahim Abbassi and Arved Dörpinghaus and Niklas Römgens and Tanja Grießmann and Raimund Rolfes},
  doi          = {10.1016/j.mlwa.2025.100672},
  journal      = {Machine Learning with Applications},
  month        = {9},
  pages        = {100672},
  shortjournal = {Mach. Learn. Appl.},
  title        = {Embedded feature selection using dual-network architecture},
  volume       = {21},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). AI-driven adaptive mesh refinement for thermal–hydraulic simulations in nuclear reactors. <em>MLA</em>, <em>21</em>, 100670. (<a href='https://doi.org/10.1016/j.mlwa.2025.100670'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The meshing of complex flow channels is the most time-consuming part of large-scale thermal–hydraulic simulations in nuclear reactors and often struggles to converge. Machine learning is employed to guide the optimization of the wire-wrapped fuel rod channel meshing, which has been successfully applied to large-scale fluid simulations. The main contributions of this paper are as follows: (1) A novel adaptive meshing technology based on ”adaptive meshing + machine learning algorithms” is proposed and successfully applied to predict sensitive channel meshes and achieve automatic refinement in nuclear reactors; (2) By comparing the channel mesh models before and after optimization, mesh quality was improved while maintaining the boundary integrity of the initial channel mesh model; (3) Based on the mesh refinement algorithm, a mesh refinement tool was developed and successfully coupled with classical thermal–hydraulic simulation software, enabling the thermal–hydraulic computation of a two-dimensional axial wire-wrapped flow channel in a nuclear reactor; (4) The performance of the coupled model was evaluated, demonstrating a relative speedup of 144.54 and parallel efficiency of 56.4% when scaled to 256 cores. Since this algorithm is developed based on the general characteristics of physical object discretization simulations, it holds the potential for cross-domain applications.},
  archive      = {J_MLA},
  author       = {Shuai Ren and Xue Miao and Huizhao Li and Lingyu Dong and Dandan Chen},
  doi          = {10.1016/j.mlwa.2025.100670},
  journal      = {Machine Learning with Applications},
  month        = {9},
  pages        = {100670},
  shortjournal = {Mach. Learn. Appl.},
  title        = {AI-driven adaptive mesh refinement for thermal–hydraulic simulations in nuclear reactors},
  volume       = {21},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Machine learning-based classification of geological structures from magnetic anomaly data: Case study of northern nigeria basement complex. <em>MLA</em>, <em>20</em>, 100678. (<a href='https://doi.org/10.1016/j.mlwa.2025.100678'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The geological terrain of Northern Nigeria presents a complex mineral resource landscape that requires systematic exploration. This study applies a machine learning framework to geomagnetic data to enhance the identification of subsurface mineralized structures. Through the integration of analytic signal processing with machine learning classifiers (Random Forest (RF) and Gradient Boosting (GB)), we analyze magnetic anomalies to predict subsurface geological features with a classification accuracy of 95.5%. The results identify mineral-rich zones across various depths, ranging from near-surface (280 m) to deep crustal levels (> 2000 m), with key prospective areas including Het, Kagoro, and Durbi. These regions contain mineral deposits such as monazite, tantalite, columbite, tourmaline, beryl, and kaolin. The study achieves a Pearson correlation coefficient of 0.956 between predicted and observed subsurface structures, demonstrating the effectiveness of this approach in mineral exploration. The methodology not only validates known geological features but also reveals previously unrecognized mineral-rich structures, contributing to a more data-driven strategy for resource assessment in geologically complex regions.},
  archive      = {J_MLA},
  author       = {Ema Abraham and Ayatu Usman and Ifunanya Amano},
  doi          = {10.1016/j.mlwa.2025.100678},
  journal      = {Machine Learning with Applications},
  month        = {6},
  pages        = {100678},
  shortjournal = {Mach. Learn. Appl.},
  title        = {Machine learning-based classification of geological structures from magnetic anomaly data: Case study of northern nigeria basement complex},
  volume       = {20},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). “A comprehensive review of AI-enhanced decision making: An empirical analysis for optimizing medication market business”. <em>MLA</em>, <em>20</em>, 100676. (<a href='https://doi.org/10.1016/j.mlwa.2025.100676'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Enterprise Resource Planning (ERP) systems play a critical role in integrating key business functions, including customer relationship management (CRM), inventory control, and financial operations. The integration of Artificial Intelligence (AI) techniques, particularly Machine Learning (ML), has the potential to enhance decision-making and optimize operational efficiency. This study systematically reviews AI-driven enhancements in ERP using the PRISMA methodology to identify trends, applications, and challenges. Additionally, an empirical analysis using a publicly available dataset conducted to demonstrate the impact of ML-driven sentiment analysis on demand forecasting in the pharmaceutical sector. Our findings indicate that AI-enhanced ERP systems improve forecasting accuracy, inventory management, and financial planning, leading to better alignment with market demands. Further, empirical results highlight the transformative role of AI techniques in optimizing ERP functionalities and supporting data-driven decision-making. This research provides actionable insights for enterprises aiming to integrate ML techniques into ERP systems to enhance business performance.},
  archive      = {J_MLA},
  author       = {Zainab Nadhim Jawad and Dr. Villányi Balázs János},
  doi          = {10.1016/j.mlwa.2025.100676},
  journal      = {Machine Learning with Applications},
  month        = {6},
  pages        = {100676},
  shortjournal = {Mach. Learn. Appl.},
  title        = {“A comprehensive review of AI-enhanced decision making: An empirical analysis for optimizing medication market business”},
  volume       = {20},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A credit card fraud detection approach based on ensemble machine learning classifier with hybrid data sampling. <em>MLA</em>, <em>20</em>, 100675. (<a href='https://doi.org/10.1016/j.mlwa.2025.100675'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The existing fraud detection methods present limitations such as imbalanced data, incorrect identification of fraudulent cases, limited applicability to different scenarios, and difficulties processing data in real-time. This paper proposes an ensemble machine-learning model for detecting fraud in credit card transactions. It also integrates the Synthetic Minority Oversampling Technique (SMOTE) with Edited Nearest Neighbor (ENN) to address the problem of the imbalanced datasets. The experimental results show that our approach performs better than the existing methods. Therefore, it will establish an essential framework for the ongoing investigations in developing more robust and flexible systems for fraud detection.},
  archive      = {J_MLA},
  author       = {Khanda Hassan Ahmed and Stefan Axelsson and Yuhong Li and Ali Makki Sagheer},
  doi          = {10.1016/j.mlwa.2025.100675},
  journal      = {Machine Learning with Applications},
  month        = {6},
  pages        = {100675},
  shortjournal = {Mach. Learn. Appl.},
  title        = {A credit card fraud detection approach based on ensemble machine learning classifier with hybrid data sampling},
  volume       = {20},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Bitcoin price direction prediction using on-chain data and feature selection. <em>MLA</em>, <em>20</em>, 100674. (<a href='https://doi.org/10.1016/j.mlwa.2025.100674'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Bitcoin is the most traded cryptocurrency by volume and market cap. A number of scholars have directed their research towards characterizing Bitcoin’s speculative behavior using a myriad of techniques such as technical analysis, price regression, and direction classification. For this work, research is conducted using the relatively nascent technique of on-chain data analysis. The goal of this research is to evaluate Bitcoin’s on-chain data in predicting future price direction. First, a classification process of on-chain data features that helps the reader understand their relevance is proposed. To address the curse of dimensionality, feature selection algorithms such as L1 regression, Boruta, and the dimensionality reduction algorithm Principal Component Analysis (PCA) are utilized. The research then explores advanced neural networks for next day price direction prediction, including the Convolutional Neural Network-Long-Short Term Memory (CNN-LSTM) and the Temporal Convolutional Network (TCN). Neural network models and trading strategies are then compared based on their return statistics. A comparative analysis of feature selection, learning model performance, and trading strategy performance is also conducted. Results from the research show that the Boruta feature selection algorithm combined with the CNN-LSTM model performs best compared to other combinations with a prediction accuracy of 82.03 % over the testing period. In addition, the on-chain features within the category, realized value, and unrealized value classifications have higher predictive powers for next day price direction prediction. Finally, during trade simulations, the CNN-LSTM model with a Long-Short strategy had an annualized return of 1682.7 % and a Sharpe Ratio of 6.47.},
  archive      = {J_MLA},
  author       = {Ritwik Dubey and David Enke},
  doi          = {10.1016/j.mlwa.2025.100674},
  journal      = {Machine Learning with Applications},
  month        = {6},
  pages        = {100674},
  shortjournal = {Mach. Learn. Appl.},
  title        = {Bitcoin price direction prediction using on-chain data and feature selection},
  volume       = {20},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Deep learning models for enhanced in-field maize leaf disease diagnosis. <em>MLA</em>, <em>20</em>, 100673. (<a href='https://doi.org/10.1016/j.mlwa.2025.100673'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Maize leaf diseases significantly threaten crop yields, and there is need for accurate, and accessible diagnostic tools. This research addresses this need by developing and evaluating deep learning (DL) and machine learning (ML) models for in-field classification and detection of four critical maize diseases: Maize Leaf Blight, Maize Lethal Necrosis, Maize Streak Virus, and Fall Armyworm damage. Utilizing field imagery captured via digital cameras and smartphones across Uganda, Tanzania, Ghana, and Namibia, we developed and compared custom Convolutional Neural Networks (CNNs), transfer learning (MobileNetV2, InceptionResNetV2), Vision Transformers (ViT), and classical ML models. For detection, a transformer-enhanced YOLOv10 architecture was implemented. Explainable AI (XAI) techniques (Grad-CAM, LIME) were incorporated to ensure model transparency. MobileNetV2 achieved the highest classification accuracy (97%), while the enhanced YOLOv10 reached a mean Average Precision (mAP) of 0.995 for object detection. The best models were integrated into a mobile application deployed on edge devices for real-time diagnosis by smallholder farmers in Uganda, with performance validated through field tests. This study demonstrates a powerful, interpretable, and field-deployable solution combining advanced DL, transformers, and XAI for effective maize health management.},
  archive      = {J_MLA},
  author       = {Joyce Nakatumba-Nabende and Sudi Murindanyi},
  doi          = {10.1016/j.mlwa.2025.100673},
  journal      = {Machine Learning with Applications},
  month        = {6},
  pages        = {100673},
  shortjournal = {Mach. Learn. Appl.},
  title        = {Deep learning models for enhanced in-field maize leaf disease diagnosis},
  volume       = {20},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). LARE: Latent augmentation using regional embedding with vision-language model. <em>MLA</em>, <em>20</em>, 100671. (<a href='https://doi.org/10.1016/j.mlwa.2025.100671'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, considerable research has been conducted on vision-language models (VLMs) that handle both image and text data; these models are being applied to diverse downstream tasks, such as “image-related chat,” “image recognition by instruction,” and “answering visual questions.” Vision-language models, such as Contrastive Language–Image Pre-training (CLIP), are also high-performance image classifiers, and are being developed into domain adaptation methods that can utilize language information to extend into unseen domains. However, because these VLMs embed images as a single point in a unified embedding space, they do not fully exploit the diverse domain performance of large-scale vision-language models. Therefore, in this study, we proposed the Latent Augmentation using Regional Embedding (LARE), which embeds the image as a region in the unified embedding space learned by the VLM. By sampling the augmented image embeddings from within this latent region, LARE enables data augmentation to various unseen domains, not just to specific unseen domains. LARE achieves robust image classification for domains in and out using augmented image embeddings to fine-tune VLMs. We demonstrate that LARE outperforms previous fine-tuning models in terms of image classification accuracy on three benchmarks. We also demonstrate that LARE is a more robust and general model that is valid under multiple conditions, such as unseen domains, small amounts of data, and imbalanced data.},
  archive      = {J_MLA},
  author       = {Kosuke Sakurai and Tatsuya Ishii and Ryotaro Shimizu and Linxin Song and Masayuki Goto},
  doi          = {10.1016/j.mlwa.2025.100671},
  journal      = {Machine Learning with Applications},
  month        = {6},
  pages        = {100671},
  shortjournal = {Mach. Learn. Appl.},
  title        = {LARE: Latent augmentation using regional embedding with vision-language model},
  volume       = {20},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Transformers—Messages in disguise. <em>MLA</em>, <em>20</em>, 100669. (<a href='https://doi.org/10.1016/j.mlwa.2025.100669'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Compared to human-designed algorithms, Neural Network (NN)-based cryptography can create a new NN-specific cryptographic scheme that changes every time the NN is (re)trained. NN retraining is advantageous because it requires an adversary to restart its process(es) to learn or break the cryptographic scheme every time the NN is (re)trained. However, NN-based encryption faces challenges, including communication overhead due to encoding for bit errors, quantizing the NN’s continuous-valued output, and enabling One-Time Pad encryption. With this in mind, this work introduces the Random Adversarial Data Obfuscation Model (RANDOM), an Adversarial Neural Cryptography (ANC) scheme. RANDOM is computationally efficient, can generate a new cryptographic mapping in 16 s, ensures the encrypted message is unique to the encryption key, does not induce any communication overhead, requires around 100 Kb of memory, and provides up to 2.5 Mb/s of end-to-end encrypted communication.},
  archive      = {J_MLA},
  author       = {Joshua H. Tyler and Donald R. Reising and Mohamed M.K. Fadul},
  doi          = {10.1016/j.mlwa.2025.100669},
  journal      = {Machine Learning with Applications},
  month        = {6},
  pages        = {100669},
  shortjournal = {Mach. Learn. Appl.},
  title        = {Transformers—Messages in disguise},
  volume       = {20},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Enhancing road safety: A convolutional neural network based approach for road damage detection. <em>MLA</em>, <em>20</em>, 100668. (<a href='https://doi.org/10.1016/j.mlwa.2025.100668'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Road damage poses considerable challenges for both conventional and autonomous vehicles, with obstacles such as potholes, speed bumps, cracks, and manholes increasing the risk of vehicle damage and accidents. For autonomous systems, the ability to detect these hazards in real time is essential to ensure passenger safety and protect vehicle integrity. In this paper, we introduce a comprehensive road damage dataset encompassing these four common types of damage and present the DD-CNN-23Layers model, a convolutional neural network specifically designed for road damage detection and classification. We benchmarked our model against pretrained YOLO models (versions 7 to 10), with the DD-CNN-23Layers model achieving a precision of 91.86% and a mean Average Precision (mAP) of 97.54%, outperforming all compared YOLO models. By utilizing this model, autonomous driving systems can proactively respond to road hazards, improving navigation safety and extending the lifespan of both vehicles and infrastructure.},
  archive      = {J_MLA},
  author       = {Soukaina Bouhsissin and Hamza Assemlali and Nawal Sael},
  doi          = {10.1016/j.mlwa.2025.100668},
  journal      = {Machine Learning with Applications},
  month        = {6},
  pages        = {100668},
  shortjournal = {Mach. Learn. Appl.},
  title        = {Enhancing road safety: A convolutional neural network based approach for road damage detection},
  volume       = {20},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Neural network-aided NILM (NNAN) disaggregation: Revealing appliance consumption patterns with iterative subtraction. <em>MLA</em>, <em>20</em>, 100667. (<a href='https://doi.org/10.1016/j.mlwa.2025.100667'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Non-Intrusive Load Monitoring (NILM) is a method to decompose overall electricity consumption into individual appliance-level data, utilizing the primary meter’s readings without additional sensors on each device. This article introduces a novel approach which is a Neural Network-Aided NILM (NNAN), focusing on revealing appliance consumption patterns by following a sequential subtraction method. Our goal is to tackle the issue where high-power and highly-used appliances make it difficult for neural networks to accurately separate the usage of lower-power and less-used appliances. We mainly employ Long Short-Term Memory (LSTM) and Convolutional Neural Networks (CNN) using inception blocks as key components. Our proposed architecture is validated on three public datasets that are AMPds2, ECO and UK-DALE. The NNAN model showed promising results, achieving disaggregation accuracy improvements of up to 5.13% on AMPds2, 3.79% on ECO, and 9.55% on UK-DALE compared to the reference methods. Additionally, NNAN reduces model complexity, requiring up to 74% fewer parameters than traditional deep learning approaches, leading to improved computational efficiency. Finally, NNAN demonstrated a reduced correlation between appliance usage rates and disaggregation accuracies.},
  archive      = {J_MLA},
  author       = {Yacine Belguermi and Patrice Wira and Gilles Hermann},
  doi          = {10.1016/j.mlwa.2025.100667},
  journal      = {Machine Learning with Applications},
  month        = {6},
  pages        = {100667},
  shortjournal = {Mach. Learn. Appl.},
  title        = {Neural network-aided NILM (NNAN) disaggregation: Revealing appliance consumption patterns with iterative subtraction},
  volume       = {20},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A novel unsupervised fine-tuning method for text summarization, and highlighting the limitations of ROUGE score. <em>MLA</em>, <em>20</em>, 100666. (<a href='https://doi.org/10.1016/j.mlwa.2025.100666'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The limited availability of datasets for text summarization tasks and their similar characteristics (e.g. news articles) make it crucial to focus on unsupervised learning techniques to enable summarization across different domains. Moreover, since summarization produces text output, effective methods developed for news articles can be applied to other domains lacking sufficient labeled data. This study introduces a novel target selection process to be used as an unsupervised learning method for fine-tuning text summarization models with unlabeled data. The process involves two-steps: first, generating an extractive summary (Ext-Reference) from the article, and second, using an abstractive model to create a pool of candidate summaries. The most suitable summary (to be used as the target) is then selected by calculating the cosine similarity between the Ext-Reference’s embedding and each candidate’s embedding. Furthermore, this project underscores the limitations of the ROUGE score, which assigns a relatively low score to this method. However, extended analysis with various metrics, including using GPT-4 as a judge, demonstrates the effectiveness of this technique for fine-tuning models without a specific target reference. It highlights the importance of using a combination of metrics, like those included in the SumEvaluator package released alongside this paper. SumEvaluator package on Github: https://github.com/AlaFalaki/SumEvaluator .},
  archive      = {J_MLA},
  author       = {Ala Alam Falaki and Robin Gras},
  doi          = {10.1016/j.mlwa.2025.100666},
  journal      = {Machine Learning with Applications},
  month        = {6},
  pages        = {100666},
  shortjournal = {Mach. Learn. Appl.},
  title        = {A novel unsupervised fine-tuning method for text summarization, and highlighting the limitations of ROUGE score},
  volume       = {20},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). TransTab: A transformer-based approach for table detection and tabular data extraction from scanned document images. <em>MLA</em>, <em>20</em>, 100665. (<a href='https://doi.org/10.1016/j.mlwa.2025.100665'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Table detection and content extraction are crucial tasks in document analysis. Traditional convolutional neural network (CNN) methods often face limitations when dealing with complex tables, such as cross-column, cross-row, and multi-dimensional tables. Although existing methods have shown good performance in recognizing simpler tables, the model’s effectiveness often falls short of meeting practical application needs in the case of complex layouts. The structural intricacy of tables requires more advanced recognition and extraction strategies, particularly in the precise localization and extraction of rows and columns. To address the shortcomings of traditional methods in handling complex table structures, this paper proposes an end-to-end document table detection and content extraction method based on Transformer, named TransTab. TransTab effectively overcomes the limitations of traditional CNN approaches by incorporating Vision Transformer (ViT) into the table recognition task, enabling it to handle complex table structures across columns and rows. The self-attention mechanism of ViT allows the model to capture long-range dependencies within the table, resulting in high accuracy in detecting table boundaries, cell separations, and internal table structures. This paper also introduces separate modules for table detection and column detection, which are responsible for recognizing the overall table structure and accurately positioning columns, respectively. Through this modular design, the model can better adapt to tables with diverse complex layouts, thereby improving its ability to process intricate tables. Finally, EasyOCR technology is employed to extract text from the table. Experimental results demonstrate that TransTab outperforms the state-of-the-art methods across several metrics. This research provides a novel solution for the automatic recognition and processing of document tables, paving the way for future developments in document analysis tasks.},
  archive      = {J_MLA},
  author       = {Yongzhou Wang and Wenliang Lv and Weijie Wu and Guanheng Xie and BiBo Lu and ChunYang Wang and Chao Zhan and Baishun Su},
  doi          = {10.1016/j.mlwa.2025.100665},
  journal      = {Machine Learning with Applications},
  month        = {6},
  pages        = {100665},
  shortjournal = {Mach. Learn. Appl.},
  title        = {TransTab: A transformer-based approach for table detection and tabular data extraction from scanned document images},
  volume       = {20},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Classification with reject option: Distribution-free error guarantees via conformal prediction. <em>MLA</em>, <em>20</em>, 100664. (<a href='https://doi.org/10.1016/j.mlwa.2025.100664'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Machine learning (ML) models always make a prediction, even when they are likely to be wrong. This causes problems in practical applications, as we do not know if we should trust a prediction. ML with reject option addresses this issue by abstaining from making a prediction if it is likely to be incorrect. In this work, we formalise the approach to ML with reject option in binary classification, deriving theoretical guarantees on the resulting error rate. This is achieved through conformal prediction (CP), which produce prediction sets with distribution-free validity guarantees. In binary classification, CP can output prediction sets containing exactly one, two or no labels. By accepting only the singleton predictions, we turn CP into a binary classifier with reject option. Here, CP is formally put in the framework of predicting with reject option. We state and prove the resulting error rate, and give finite sample estimates. Numerical examples provide illustrations of derived error rate through several different conformal prediction settings, ranging from full conformal prediction to offline batch inductive conformal prediction. The former has a direct link to sharp validity guarantees, whereas the latter is more fuzzy in terms of validity guarantees but can be used in practice. Error-reject curves illustrate the trade-off between error rate and reject rate, and can serve to aid a user to set an acceptable error rate or reject rate in practice.},
  archive      = {J_MLA},
  author       = {Johan Hallberg Szabadváry and Tuwe Löfström and Ulf Johansson and Cecilia Sönströd and Ernst Ahlberg and Lars Carlsson},
  doi          = {10.1016/j.mlwa.2025.100664},
  journal      = {Machine Learning with Applications},
  month        = {6},
  pages        = {100664},
  shortjournal = {Mach. Learn. Appl.},
  title        = {Classification with reject option: Distribution-free error guarantees via conformal prediction},
  volume       = {20},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). EMD-based local matching for occluded person re-identification. <em>MLA</em>, <em>20</em>, 100663. (<a href='https://doi.org/10.1016/j.mlwa.2025.100663'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Person re-identification (Re-ID) is a vital computer vision task focused on matching images of a person of interest as they move across multiple non-overlapping cameras. Thanks to advancements in deep learning models, numerous important milestones have been achieved in the field of person Re-ID. Recent efforts have concentrated on addressing a more realistic scenario where pedestrians are partially occluded. This trend indicates a promising future for the practical implementation of person Re-ID systems. This paper builds upon our previous work, which successfully addressed single-shot person Re-ID using local matching information. For this task, Earth Mover’s Distance (EMD) is employed as a metric to measure similarity between two distributions. To handle multi-shot Re-ID, the proposed framework integrates a feature block, adapting the single-shot methodology to a multi-shot setting. Unlike conventional person Re-ID methods that employ a manually determined images of person, the proposed framework takes a query tracklet as input, which is automatically generated through human detection and tracking steps. To evaluate the proposed method, FAPR dataset (Fully Automated Person ReID) is used. This dataset is one of the few publicly available datasets built specifically for an end-to-end person Re-ID system. Various scenarios are rigorously examined to demonstrate the effectiveness of the proposed framework, especially in challenging conditions with strong occlusion. Across eight experimental scenarios, the proposed method achieves matching rates at rank-1 ranging from 76.3% to 100%. These results underscore the robustness and efficacy of our approach. Our source code is made available at: https://github.com/anhnhust/emd-person-reid .},
  archive      = {J_MLA},
  author       = {Hoang-Anh Nguyen and Thuy-Binh Nguyen and Hong-Quan Nguyen and Thi-Lan Le},
  doi          = {10.1016/j.mlwa.2025.100663},
  journal      = {Machine Learning with Applications},
  month        = {6},
  pages        = {100663},
  shortjournal = {Mach. Learn. Appl.},
  title        = {EMD-based local matching for occluded person re-identification},
  volume       = {20},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Automatic and accurate measurement of cattle body based on a lightweight YOLOv8-pose model and 3D point cloud. <em>MLA</em>, <em>20</em>, 100662. (<a href='https://doi.org/10.1016/j.mlwa.2025.100662'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In animal husbandry, the transition from traditional manual and time-consuming methods to non-contact body measurements for cattle has become increasingly important. Current research predominantly relies on 3D data to extract body measurement key points; however, the presence of noise in the data often leads to significant errors. Furthermore, the complexity of deep learning models often results in poor real-time performance in practical applications. To address these challenges, this study presents the YOLOv8-Pose model with a lightweight shared convolutional detection head (YOLOv8-Pose-LSCDH), which integrates with 3D point clouds for the automatic measurement of dairy cattle. Initially, this method employs a Kinect V2 sensor to capture RGB and depth images of the cattle. The key points predicted by the YOLOv8-Pose-LSCDH on the RGB images are subsequently projected onto the 3D point cloud generated from the depth image, thereby obtaining three-dimensional body measurement key points. These key points are then used to compute the cattle's body dimensions, including withers height, hip height, chest depth, chest circumference, body length, and hip length. Experimental results demonstrate that, compared to the baseline model, the proposed YOLOv8-Pose-LSCDH model requires fewer parameters and less computational load without compromising accuracy, thus offering superior real-time performance. Additionally, when compared to manual measurement, the proposed body measurement method maintains an overall relative error of <4 %.},
  archive      = {J_MLA},
  author       = {Chunhong Yue and Xiaohang Zhang and Yajun Zhang and Haijun Jiang and Haoyuan Miao},
  doi          = {10.1016/j.mlwa.2025.100662},
  journal      = {Machine Learning with Applications},
  month        = {6},
  pages        = {100662},
  shortjournal = {Mach. Learn. Appl.},
  title        = {Automatic and accurate measurement of cattle body based on a lightweight YOLOv8-pose model and 3D point cloud},
  volume       = {20},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Hyperparameter optimization of machine learning models for predicting actual evapotranspiration. <em>MLA</em>, <em>20</em>, 100661. (<a href='https://doi.org/10.1016/j.mlwa.2025.100661'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Direct measurement of actual evapotranspiration (AET) using eddy covariance and lysimeters is challenging, particularly in large areas, due to high cost, technical complexity, and the need for specialized instrumentation. Consequently, AET data is limited, prompting the use of meteorological and soil features for prediction. This study develops and evaluates machine learning models for AET prediction based on two input combinations. The first group, selected through Pearson correlation, tolerance, and VIF scores to address multicollinearity, includes net CO 2 , sensible heat flux, air temperature, relative humidity, and wind speed. The second group, chosen for practical applicability and more accessible, consists of soil surface temperature, air temperature, relative humidity, and wind speed. Two predictive approaches are proposed: (i) deep learning models (LSTM, GRU, CNN) and (ii) classical machine learning models (SVR, RF). Hyperparameters were optimized using Bayesian optimization and compared with grid search. Bayesian optimization demonstrated higher performance and reduced computation time. Model performance was evaluated using statistical indicators (RMSE, MSE, MAE, R 2 ). Deep learning methods outperformed classical methods, with LSTM achieving the best results (Bayesian optimization: RMSE=0.0230, MSE=0.0005, MAE=0.0139, R 2 =0.8861). Performance decreased with fewer predictors. LSTM maintained superiority, achieving R 2 =0.8861 with five predictors and R 2 =0.8467 with four. LSTM also slightly outperformed SVR (R 2 = 0.8456) with fewer predictors. Overall, deep learning methods, especially with Bayesian optimization, have been shown to be more effective than classical machine learning methods for AET prediction. This findings encourage future research using varied input combinations and advanced modeling approaches for AET accurate prediction.},
  archive      = {J_MLA},
  author       = {Chalachew Muluken Liyew and Elvira Di Nardo and Stefano Ferraris and Rosa Meo},
  doi          = {10.1016/j.mlwa.2025.100661},
  journal      = {Machine Learning with Applications},
  month        = {6},
  pages        = {100661},
  shortjournal = {Mach. Learn. Appl.},
  title        = {Hyperparameter optimization of machine learning models for predicting actual evapotranspiration},
  volume       = {20},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Unified modeling language code generation from diagram images using multimodal large language models. <em>MLA</em>, <em>20</em>, 100660. (<a href='https://doi.org/10.1016/j.mlwa.2025.100660'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Unified Modeling Language is a standardized visual language widely used for modeling and documenting the design of software systems. Although many tools are available that generate UML diagrams from UML code, generating executable UML code from image-based UML diagrams remains challenging. This paper proposes a new approach to generate UML code using a large multimodal language model automatically. Synthetic UML activity and sequence diagram datasets were created to train and test the model. We compared the standard fine-tuning with LoRA techniques to optimize base models. The experiments measured the code generation accuracy across different model sizes and training strategies. These results demonstrated that domain-adapted MM-LLMs perform for UML code generation automation, whereby, at the best model, it achieved BLEU and SSIM of 0.779 and 0.942 on sequence diagrams. This will enable the modernization of legacy systems and decrease the manual effort put into software development workflows.},
  archive      = {J_MLA},
  author       = {Averi Bates and Ryan Vavricka and Shane Carleton and Ruosi Shao and Chongle Pan},
  doi          = {10.1016/j.mlwa.2025.100660},
  journal      = {Machine Learning with Applications},
  month        = {6},
  pages        = {100660},
  shortjournal = {Mach. Learn. Appl.},
  title        = {Unified modeling language code generation from diagram images using multimodal large language models},
  volume       = {20},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Going vegan with ChatGPT: Towards designing LLMs for personalized lifestyle changes. <em>MLA</em>, <em>20</em>, 100659. (<a href='https://doi.org/10.1016/j.mlwa.2025.100659'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Large language models (LLMs), one of the recent technological revolutions, have become applicable to all areas of human endeavor, including health. In the area of health, LLMs have contributed to disease management, diagnosis, stress management, and other major lifestyle-related changes. However, little is yet known about their impact in the area of nutrition and lifestyle-related changes associated with diseases such as diabetes, cardiovascular diseases, obesity, and others. In this paper, we present two case studies of ChatGPT as an LLM intervention for making lifestyle-related decisions, such as transitioning to a vegan lifestyle: 1. normal weight (healthy) and 2. obesity. Additionally, we considered three (3) dietary restrictions that could affect people in both case studies to transition to a vegan lifestyle. These include 1) allergies to nuts; 2) allergies to gluten; and 3) no allergies. We used ChatGPT to generate a one-week (seven-day) meal plan based on these dietary restrictions. We analyzed all responses from ChatGPT and found that ChatGPT provides a rich combination of vegan diets and is sensitive to these food allergies to some extent. Additionally, we found some challenges that relate to how an appropriate prompt can be employed to optimize ChatGPT’s recommendations and precisions relating to the total calories of foods recommended by ChatGPT. Furthermore, we provide recommendations to overcome these challenges in future work, including supporting user's domain-specific literacy and precision sensitivity for metrics that have an overall impact on human health.},
  archive      = {J_MLA},
  author       = {Munachiso Okenyi and Grace Ataguba and Kosi Clinton Henry and Sussan Anukem and Rita Orji},
  doi          = {10.1016/j.mlwa.2025.100659},
  journal      = {Machine Learning with Applications},
  month        = {6},
  pages        = {100659},
  shortjournal = {Mach. Learn. Appl.},
  title        = {Going vegan with ChatGPT: Towards designing LLMs for personalized lifestyle changes},
  volume       = {20},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). D3: A small language model for drug-drug interaction prediction and comparison with large language models. <em>MLA</em>, <em>20</em>, 100658. (<a href='https://doi.org/10.1016/j.mlwa.2025.100658'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Large Language Models (LLMs) have significantly advanced Natural Language Processing (NLP) applications, including healthcare. However, their high computational demands pose challenges for deployment in resource-constrained settings. Small Language Models (SLMs) offer a promising alternative, balancing performance and efficiency. In this study, we introduce D3, a compact SLM with approximately 70 million parameters, designed for Drug-Drug Interaction (DDI) prediction. Trained on a curated DrugBank dataset, D3 was compared against fine-tuned state-of-the-art LLMs, Qwen 2.5, Gemma 2, Mistral v0.3, and LLaMA 3.1, ranging from 1.5 billion to 70 billion parameters. Despite being 1000 times smaller than LLaMA 3.1, D3 achieved an F1 score of 0.86, comparable to larger models (Mistral v0.3: 0.88, LLaMA 3.1: 0.89), with no statistically significant performance difference. Expert evaluations further confirmed that D3’s predictions were clinically relevant and closely aligned with those of larger models. Our findings demonstrate that SLMs can effectively compete with LLMs in DDI prediction, achieving strong performance while significantly reducing computational requirements. Beyond DDI prediction, this work highlights the broader potential of small models in healthcare, where balancing accuracy and efficiency is critical.},
  archive      = {J_MLA},
  author       = {Ahmed Ibrahim and Abdullah Hosseini and Salma Ibrahim and Aamenah Sattar and Ahmed Serag},
  doi          = {10.1016/j.mlwa.2025.100658},
  journal      = {Machine Learning with Applications},
  month        = {6},
  pages        = {100658},
  shortjournal = {Mach. Learn. Appl.},
  title        = {D3: A small language model for drug-drug interaction prediction and comparison with large language models},
  volume       = {20},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Quantitative insights into the winnipeg rental sector: A data-driven analytical approach using geographic and property metrics. <em>MLA</em>, <em>20</em>, 100657. (<a href='https://doi.org/10.1016/j.mlwa.2025.100657'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the dynamic rental market of Winnipeg, accurately predicting rental property prices is essential for a wide range of stakeholders, including landlords, tenants, prospective renters, property managers, and urban planners. Traditional rental market assessments often fail to incorporate advanced analytical techniques, leading to less precise price forecasts and hindering strategic decision-making. This paper aims to bridge this gap by developing sophisticated predictive models using a dataset that contains rental property information as well as demographic and socio-economic information in Winnipeg. This paper highlights the importance of integrating advanced computational methods in rental market analysis, which can significantly benefit economic planning and personal investment decisions in urban environments. By utilizing both machine learning and statistical learning methods, this paper seeks to improve the accuracy of rental price estimations across different neighborhoods in Winnipeg.},
  archive      = {J_MLA},
  author       = {Lahiru Wickramasinghe and Aditya Jain},
  doi          = {10.1016/j.mlwa.2025.100657},
  journal      = {Machine Learning with Applications},
  month        = {6},
  pages        = {100657},
  shortjournal = {Mach. Learn. Appl.},
  title        = {Quantitative insights into the winnipeg rental sector: A data-driven analytical approach using geographic and property metrics},
  volume       = {20},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Application of machine learning in the determination of rock brittleness for CO2 geosequestration. <em>MLA</em>, <em>20</em>, 100656. (<a href='https://doi.org/10.1016/j.mlwa.2025.100656'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The underground storage of carbon dioxide (CO 2 ), also called CO 2 geosequestration, represents one of the most promising options for reducing greenhouse gases in the atmosphere. However, fluid-rock interactions in reservoir and cap rocks before and during CO 2 geosequestration alter their mineralogical composition, and consequently, their brittleness index which is paramount in determining the suitability of formations for CO 2 geosequestration. Therefore, it is important to monitor the brittleness of reservoir and cap rocks, to ascertain their integrity for CO 2 storage. In this study, an algorithm was developed to generate numerical simulation datasets for a more reliable machine learning model development, and an artificial neural network (ANN) model was developed to evaluate the brittleness index of rocks using data from numerical simulations of CO 2 geosequestration in sandstone and carbonate reservoirs, overlain by shale caprock. The model was developed using Python programming language. The model developed in this study predicted the brittleness index of rocks with an R 2 value greater than 99 %, and mean absolute percentage error (MAPE) <0.6 % on the training, validation, and testing datasets. Hence, the model predicts the brittleness index of rocks with high accuracy. The findings of the study revealed that the geochemical composition of formation fluids is related to the brittleness index of rocks. In terms of feature importance in predicting the brittleness index of rocks, the concentrations of SiO 2 (aq), SO 4 2 , K + , Ca 2+ , and O 2 (aq) have a stronger impact on the brittleness of rocks considered in this study.},
  archive      = {J_MLA},
  author       = {Efenwengbe Nicholas Aminaho and Mamdud Hossain and Nadimul Haque Faisal and Reza Sanaee},
  doi          = {10.1016/j.mlwa.2025.100656},
  journal      = {Machine Learning with Applications},
  month        = {6},
  pages        = {100656},
  shortjournal = {Mach. Learn. Appl.},
  title        = {Application of machine learning in the determination of rock brittleness for CO2 geosequestration},
  volume       = {20},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Complying with the EU AI act: Innovations in explainable and user-centric hand gesture recognition. <em>MLA</em>, <em>20</em>, 100655. (<a href='https://doi.org/10.1016/j.mlwa.2025.100655'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The EU AI Act underscores the importance of transparency, user-centricity, and robustness in AI systems, particularly for high-risk applications. In response, we present advancements in XentricAI, an explainable hand gesture recognition (HGR) system designed to meet these regulatory requirements. XentricAI addresses fundamental challenges in HGR, such as the opacity of black-box models using explainable AI methods and the handling of distributional shifts in real-world data through transfer learning techniques. We extend an existing radar-based HGR dataset by adding 28,000 new gestures, with contributions from multiple users across varied locations, including 24,000 out-of-distribution gestures. Leveraging this real-world dataset, we enhance XentricAI’s capabilities by integrating a variational autoencoder module for improved gesture anomaly detection, incorporating user-specific dynamic thresholding. This integration enables the identification of 11.50% more anomalous gestures. Our extensive evaluations demonstrate a 97.5% success rate in characterizing these anomalies, significantly improving system explainability. Furthermore, the implementation of transfer learning techniques has shown a substantial increase in user adaptability, with an average performance improvement of at least 15.17%. This work contributes to the development of trustworthy AI systems by providing both technical advancements and regulatory compliance, offering a commercially viable solution that aligns with the EU AI Act requirements.},
  archive      = {J_MLA},
  author       = {Sarah Seifi and Tobias Sukianto and Cecilia Carbonelli and Lorenzo Servadei and Robert Wille},
  doi          = {10.1016/j.mlwa.2025.100655},
  journal      = {Machine Learning with Applications},
  month        = {6},
  pages        = {100655},
  shortjournal = {Mach. Learn. Appl.},
  title        = {Complying with the EU AI act: Innovations in explainable and user-centric hand gesture recognition},
  volume       = {20},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Forecasting the forced van der pol equation with frequent phase shifts using reservoir computing. <em>MLA</em>, <em>20</em>, 100654. (<a href='https://doi.org/10.1016/j.mlwa.2025.100654'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We tested the performance of reservoir computing (RC) in predicting the dynamics of a specific nonautonomous dynamical system. Specifically, we considered a van der Pol oscillator subjected to a periodic external force with frequent phase shifts. The reservoir computer, trained and optimized using simulation data generated for a specific phase shift, was designed to predict the oscillation dynamics under periodic external forces with different phase shifts. The results suggest that if the training data exhibit sufficient complexity, it is possible to quantitatively predict the oscillation dynamics subjected to different phase shifts. This study was motivated by the challenge of predicting the circadian rhythm of shift workers and optimizing their shift schedules individually. Our results suggest that RC could be utilized for such applications.},
  archive      = {J_MLA},
  author       = {Sho Kuno and Hiroshi Kori},
  doi          = {10.1016/j.mlwa.2025.100654},
  journal      = {Machine Learning with Applications},
  month        = {6},
  pages        = {100654},
  shortjournal = {Mach. Learn. Appl.},
  title        = {Forecasting the forced van der pol equation with frequent phase shifts using reservoir computing},
  volume       = {20},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Building consistency in explanations: Harmonizing CNN attributions for satellite-based land cover classification. <em>MLA</em>, <em>20</em>, 100653. (<a href='https://doi.org/10.1016/j.mlwa.2025.100653'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Explainable machine learning has gained substantial attention for its role in enhancing transparency and trust in computer vision applications. Attribution methods like Grad-CAM and occlusion sensitivity analysis are frequently used to identify how features contribute to predictions of neural networks. However, a key challenge is that different attribution methods often produce different outcomes undermining trust in their results. Furthermore, the unique characteristics of remote sensing imagery pose additional challenges for attribution interpretation: it primarily comprises continuous “stuff” classes rather than objects, exhibits fine-grained spatial variability, contains mixed pixels, is often multispectral, and exhibits spatially heterogeneity. To tackle this challenge, we present a novel methodology that harmonizes attributions, resulting in: 1. greater consistency across different attribution methods; 2. more meaningful explanations when validated against known segmentation ground truth; and 3. enhanced transparency and traceability. This is achieved by coherently linking feature representations to attributions derived from analyzing the training data, enabling direct attribution assignment to features in (unseen) images. We evaluate our methodology using two satellite-based land cover classification datasets, three convolutional neural network architectures, and nine attribution methods. Harmonizing attributions increases the Pearson correlation coefficient between different attribution methods by an average of 0.18 across all datasets, models, and methods; and improves the micro F1-score — a measure of accuracy — by 12%. We demonstrate that Grad-CAM attributions are inherently well-aligned with the features, whereas other gradient-based attribution methods exhibit significant noise, mitigated through harmonization. It further enhances the resolution of occlusion-based attribution maps and adjusts misleading explanations.},
  archive      = {J_MLA},
  author       = {Timo T. Stomberg and Lennart A. Reißner and Martin G. Schultz and Ribana Roscher},
  doi          = {10.1016/j.mlwa.2025.100653},
  journal      = {Machine Learning with Applications},
  month        = {6},
  pages        = {100653},
  shortjournal = {Mach. Learn. Appl.},
  title        = {Building consistency in explanations: Harmonizing CNN attributions for satellite-based land cover classification},
  volume       = {20},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). TransFusion: Generating long, high fidelity time series using diffusion models with transformers. <em>MLA</em>, <em>20</em>, 100652. (<a href='https://doi.org/10.1016/j.mlwa.2025.100652'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The generation of high-quality, long-sequenced time-series data is essential due to its wide range of applications. In the past, standalone Recurrent and Convolutional Neural Network-based Generative Adversarial Networks (GAN) were used to synthesize time-series data. However, they are inadequate for generating long sequences of time-series data due to limitations in the architecture, such as difficulties in capturing long-range dependencies, limited temporal coherence, and scalability challenges. Furthermore, GANs are well known for their training instability and mode collapse problem. To address this, we propose TransFusion , a diffusion, and transformers-based generative model to generate high-quality long-sequence time-series data. We extended the sequence length to 384, surpassing the previous limit, and successfully generated high-quality synthetic data. Also, we introduce two evaluation metrics to evaluate the quality of the synthetic data as well as its predictive characteristics. TransFusion is evaluated using a diverse set of visual and empirical metrics, consistently outperforming the previous state-of-the-art by a significant margin.},
  archive      = {J_MLA},
  author       = {Md Fahim Sikder and Resmi Ramachandranpillai and Fredrik Heintz},
  doi          = {10.1016/j.mlwa.2025.100652},
  journal      = {Machine Learning with Applications},
  month        = {6},
  pages        = {100652},
  shortjournal = {Mach. Learn. Appl.},
  title        = {TransFusion: Generating long, high fidelity time series using diffusion models with transformers},
  volume       = {20},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Shifted hexpo activation function: An improved vanishing gradient mitigation activation function for disease classification. <em>MLA</em>, <em>20</em>, 100651. (<a href='https://doi.org/10.1016/j.mlwa.2025.100651'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Activation functions (AFs) in deep learning significantly impacts model performance. In this study, we proposed Shifted Hexpo (SHexpo), an improved variant of the Hexpo AF, designed to address limitations such as vanishing gradients and parameter sensitivity. SHexpo introduces a shifting parameter, enhancing its adaptability and performance across diverse data distributions. Using ResNet 101, DenseNet 169, 5 and 10-layer lightweight Convolutional Neural Network (CNN) trained on the SIPaKMeD dataset for cervical cancer classification, we compared SHexpo against Hexpo, ReLU, Swish, Mish, GELU and PReLU under four pre-processing techniques: zero-mean centering, normalization, their combination and ImageNet weights. Our results demonstrate that SHexpo achieves higher classification accuracy and better gradient stability than Hexpo while performing competitively with state-of-the-art AFs. Our findings indicate that SHexpo can be effectively integrated into both lightweight and deep architectures. Additionally, Grad-CAM visualizations highlight SHexpo’s capability to enhance interpretability by localizing the most relevant image regions contributing to model predictions. These results demonstrate SHexpo’s potentials for medical image analysis in low-resource settings.},
  archive      = {J_MLA},
  author       = {Joseph Otoo and Suleman Nasiru and Irene Dekomwine Angbing},
  doi          = {10.1016/j.mlwa.2025.100651},
  journal      = {Machine Learning with Applications},
  month        = {6},
  pages        = {100651},
  shortjournal = {Mach. Learn. Appl.},
  title        = {Shifted hexpo activation function: An improved vanishing gradient mitigation activation function for disease classification},
  volume       = {20},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). From pixels to letters: A high-accuracy CPU-real-time american sign language detection pipeline. <em>MLA</em>, <em>20</em>, 100650. (<a href='https://doi.org/10.1016/j.mlwa.2025.100650'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We introduce a CPU-real-time American Sign Language (ASL) recognition system designed to bridge communication barriers between the deaf community and the broader public. Our multi-step pipeline includes preprocessing, a hand detection stage, and a classification model using a MobileNetV3 convolutional neural network backbone followed by a classification head. We train and evaluate our model using a combined dataset of 252k labeled images from two distinct ASL datasets. This increases generalization on unseen data and strengthens our evaluation. We employ a two-step training: The backbone is initialized through transfer learning and frozen for the initial training of the head. A second training phase with lower learning rate and unfrozen weights yields an exceptional test accuracy of 99.98% and > 99.93% on the two datasets - setting new benchmarks for ASL detection. With an CPU-inference time under 500 ms, it ensures real-time performance on affordable hardware. We propose a straightforward method to determine the amount of data needed for validation and testing and to quantify the remaining statistical error. For this we calculate accuracy as a function of validation set size, and thus ensure sufficient data is allocated for evaluation. Model interpretability is enhanced using Gradient-weighted Class Activation Mapping (Grad-CAM), which provides visual explanations by highlighting key image regions influencing predictions. This transparency fosters trust and improves user understanding of the system’s decisions. Our system sets new benchmarks in ASL gesture recognition by closing the accuracy gap of state-of-the-art solutions, while offering broad applicability through CPU-real-time inference and interpretability of our predictions.},
  archive      = {J_MLA},
  author       = {Jonas Rheiner and Daniel Kerger and Matthias Drüppel},
  doi          = {10.1016/j.mlwa.2025.100650},
  journal      = {Machine Learning with Applications},
  month        = {6},
  pages        = {100650},
  shortjournal = {Mach. Learn. Appl.},
  title        = {From pixels to letters: A high-accuracy CPU-real-time american sign language detection pipeline},
  volume       = {20},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Optimizing translation for low-resource languages: Efficient fine-tuning with custom prompt engineering in large language models. <em>MLA</em>, <em>20</em>, 100649. (<a href='https://doi.org/10.1016/j.mlwa.2025.100649'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Training large language models (LLMs) can be prohibitively expensive. However, the emergence of new Parameter-Efficient Fine-Tuning (PEFT) strategies provides a cost-effective approach to unlocking the potential of LLMs across a variety of natural language processing (NLP) tasks. In this study, we selected the Mistral 7B language model as our primary LLM due to its superior performance, which surpasses that of LLAMA 2 13B across multiple benchmarks. By leveraging PEFT methods, we aimed to significantly reduce the cost of fine-tuning while maintaining high levels of performance. Despite their advancements, LLMs often struggle with translation tasks for low-resource languages, particularly morphologically rich African languages. To address this, we employed customized prompt engineering techniques to enhance LLM translation capabilities for these languages. Our experimentation focused on fine-tuning the Mistral 7B model to identify the best-performing ensemble using a custom prompt strategy. The results obtained from the fine-tuned Mistral 7B model were compared against several models: Serengeti, Gemma, Google Translate, and No Language Left Behind (NLLB). Specifically, Serengeti and Gemma were fine-tuned using the same custom prompt strategy as the Mistral model, while Google Translate and NLLB Gemma, which are pre-trained to handle English-to-Zulu and English-to-Xhosa translations, were evaluated directly on the test data set. This comparative analysis allowed us to assess the efficacy of the fine-tuned Mistral 7B model against both custom-tuned and pre-trained translation models. LLMs have traditionally struggled to produce high-quality translations, especially for low-resource languages. Our experiments revealed that the key to improving translation performance lies in using the correct prompt during fine-tuning. We used the Mistral 7B model to develop a custom prompt that significantly enhanced translation quality for English-to-Zulu and English-to-Xhosa language pairs. After fine-tuning the Mistral 7B model for 30 GPU days, we compared its performance to the No Language Left Behind (NLLB) model and Google Translator API on the same test dataset. While NLLB achieved the highest scores across BLEU, G-Eval (cosine similarity), and Chrf++ (F1-score), our results demonstrated that Mistral 7B, with the custom prompt, still performed competitively. Additionally, we showed that our prompt template can improve the translation accuracy of other models, such as Gemma and Serengeti, when applied to high-quality bilingual datasets. This demonstrates that our custom prompt strategy is adaptable across different model architectures, bilingual settings, and is highly effective in accelerating learning for low-resource language translation.},
  archive      = {J_MLA},
  author       = {Pitso Walter Khoboko and Vukosi Marivate and Joseph Sefara},
  doi          = {10.1016/j.mlwa.2025.100649},
  journal      = {Machine Learning with Applications},
  month        = {6},
  pages        = {100649},
  shortjournal = {Mach. Learn. Appl.},
  title        = {Optimizing translation for low-resource languages: Efficient fine-tuning with custom prompt engineering in large language models},
  volume       = {20},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Prediction of foreign currency exchange rates using an attention-based long short-term memory network. <em>MLA</em>, <em>20</em>, 100648. (<a href='https://doi.org/10.1016/j.mlwa.2025.100648'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose an a ttention-based L STM model for predicting f orex r a tes (ALFA). The prediction process consists of three stages. First, an LSTM model captures temporal dependencies within the forex time series. Next, an attention mechanism assigns different weights (importance scores) to the features of the LSTM model’s output. Finally, a fully connected layer generates predictions of forex rates. We conducted comprehensive experiments to evaluate and compare the performance of ALFA against several models used in previous work and against state-of-the-art deep learning models such as temporal convolutional networks (TCN) and Transformer. Experimental results show that ALFA outperforms the baseline models in most cases, across different currency pairs and feature sets, thanks to its attention mechanism that filters out irrelevant or redundant data to focus on important features. ALFA consistently ranks among the top three of the seven models evaluated and ranks first in most cases. We validated the effectiveness of ALFA by applying it to actual trading scenarios using several currency pairs. In these evaluations, ALFA achieves estimated annual return rates comparable to those of professional traders.},
  archive      = {J_MLA},
  author       = {Shahram Ghahremani and Uyen Trang Nguyen},
  doi          = {10.1016/j.mlwa.2025.100648},
  journal      = {Machine Learning with Applications},
  month        = {6},
  pages        = {100648},
  shortjournal = {Mach. Learn. Appl.},
  title        = {Prediction of foreign currency exchange rates using an attention-based long short-term memory network},
  volume       = {20},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Effective multimodal hate speech detection on facebook hate memes dataset using incremental PCA, SMOTE, and adversarial learning. <em>MLA</em>, <em>20</em>, 100647. (<a href='https://doi.org/10.1016/j.mlwa.2025.100647'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The proliferation of harmful information, such as hate speech and online harassment, has increased in recent years due to social media's explosive expansion. Using the Facebook Hate Meme Dataset (FBHM), we create a reliable model in this work for identifying multimodal hate speech on online platforms. To effectively address class imbalance and improve classification accuracy, our hybrid model combines ResNet for image processing with RoBERTa for text analysis, leveraging Synthetic Minority Over-sampling Technique (SMOTE) and Incremental Principal Component Analysis (PCA) combined with adversarial machine learning techniques. The combination of Incremental PCA's dimensionality reduction and SMOTE's synthetic sample creation produces a potent combination that enhances the training dataset and maximizes feature representation, resulting in improved online content moderation techniques. We achieved an accuracy of 81.80 %, and a Macro-F1 score of 81.53 % on the FBHM dataset which represents an 18 % improvement in accuracy over the base model. These results provide significant novel insights into this important field of study by demonstrating the potential of adversarial approaches in creating reliable models for automated hate speech identification that can help create a safer online environment and can significantly reduce the emotional burden on human content moderators by handling the contents quickly and accurately. This study highlights the mutually beneficial effect of combining SMOTE and incremental PCA, demonstrating how they improve the model's ability to correct class imbalance and boost performance. The source code and dataset are publicly available on GitHub to facilitate reproducibility and further research. Link to the code and dataset below: https://github.com/ludivintchokote/HatePostDetection},
  archive      = {J_MLA},
  author       = {Emmanuel Ludivin Tchuindjang Tchokote and Elie Fute Tagne},
  doi          = {10.1016/j.mlwa.2025.100647},
  journal      = {Machine Learning with Applications},
  month        = {6},
  pages        = {100647},
  shortjournal = {Mach. Learn. Appl.},
  title        = {Effective multimodal hate speech detection on facebook hate memes dataset using incremental PCA, SMOTE, and adversarial learning},
  volume       = {20},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Predicting cyberbullying victimisation in emerging markets and developing countries using the global school-based health survey. <em>MLA</em>, <em>20</em>, 100646. (<a href='https://doi.org/10.1016/j.mlwa.2025.100646'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Objectives This study aimed to identify predictors of cyberbullying victimisation among adolescents and develop predictive models to support early intervention strategies. Methods Data from the Global School-based Health Surveys (2017–2021) were analysed, focusing on emerging markets and developing countries. A simple random sampling strategy was used to ensure equal representation across countries. A multivariable logistic regression model was applied to 26 variables to identify significant predictors of cyberbullying victimisation. Subsequently, machine learning techniques were used to develop predictive models. Results This logistic regression model was statistically significant ( χ2(26)=507.96, p < 0 .001 ), explaining 19.3 % of the variance with an AUROC of 0 .758 (95 % CI, 0.739 to 0.778) . Twelve variables, including being bullied on school property, female gender, peer victimisation, early sexual debut, alcohol consumption, and suicidal ideation, were identified as significant predictors. The best-performing predictive model, a randomly over-sampled random forest classifier, achieved 82 % accuracy and an AUROC of 0 .83 (95 % CI, 0.81 to 0.85) . Conclusions The study highlights key predictors of cyberbullying victimisation and demonstrates the potential of machine learning in developing accurate predictive models. However, reliance on self-reported data may introduce biases. Future research could integrate diverse data sources to enhance model accuracy and reliability.},
  archive      = {J_MLA},
  author       = {Paulo Ricardo Vieira Braga and Katie Rose Tyrrell},
  doi          = {10.1016/j.mlwa.2025.100646},
  journal      = {Machine Learning with Applications},
  month        = {6},
  pages        = {100646},
  shortjournal = {Mach. Learn. Appl.},
  title        = {Predicting cyberbullying victimisation in emerging markets and developing countries using the global school-based health survey},
  volume       = {20},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Machine learning based risk analysis and predictive modeling of structure fire related casualties. <em>MLA</em>, <em>20</em>, 100645. (<a href='https://doi.org/10.1016/j.mlwa.2025.100645'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We analysed over 48,000 reported structure fire incidents in Oregon that occurred from January 2012 through August 2023. The dataset includes 2136 fires that led to civilian casualties including 317 confirmed fatalities. Bagged decision tree classifiers with random forest algorithm were used to quantify the importance of factors related to socioeconomic conditions, population characteristics, structural and behavioral incident details, and local infrastructure on the severity of injuries. Our results show that the age of victims, fire service response times, and availability of working smoke or fire detectors were among the most important parameters for predicting fatal outcomes of structure fires. Furthermore, a predictive Bayesian regularized neural network ensemble classifier was developed to model the severity of casualties and project a spatial risk classification on the census block level. The network model achieves a prediction accuracy of 92.5 % for the classification of structural fire-related casualty severities. With information aggregated to the census block scale and information related to specific fire incidents removed, the retrained model based solely on spatially available data reaches an 87.6 % severity classification accuracy. As the first statewide analysis of its kind, our spatial assessment provides a useful tool for resource allocation, risk factor reduction, and safety education efforts targeted to reduce the number of serious injuries or fatalities from structure fires.},
  archive      = {J_MLA},
  author       = {Andres Schmidt and Eric Gemmil and Russ Hoskins},
  doi          = {10.1016/j.mlwa.2025.100645},
  journal      = {Machine Learning with Applications},
  month        = {6},
  pages        = {100645},
  shortjournal = {Mach. Learn. Appl.},
  title        = {Machine learning based risk analysis and predictive modeling of structure fire related casualties},
  volume       = {20},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Emotional reactions towards vaccination during the emergence of the omicron variant: Insights from twitter analysis in south africa. <em>MLA</em>, <em>20</em>, 100644. (<a href='https://doi.org/10.1016/j.mlwa.2025.100644'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The emergence of the Omicron variant triggered intense emotional reactions toward vaccination in South Africa, particularly evident on platforms like Twitter. These emotions have the potential to significantly influence vaccine confidence and uptake, posing a challenge for public health efforts. However, existing research lacks a detailed understanding of how emotional dynamics during variant-specific outbreaks, such as Omicron, impact vaccination rates, especially at a province level. This gap limits the ability of policymakers to design targeted interventions. Our study addresses this problem by analyzing emotional reactions to vaccination during the Omicron outbreak using geotagged Twitter data and the Text2emotion pre-trained model. We validated the model by hand-labeling a random 10% of tweets and comparing results with BERT-labeled tweets, finding no significant differences ( p < 0 . 001 for hand-labeled, p = 0 . 002 for BERT). Using statistical methods such as χ 2 , Mann–Whitney U, Granger causality, and Jaccard similarity, we identified a strong association between emotional intensities in vaccine-related posts and vaccination rates during the Omicron period ( p < 0 . 04 ) in specific provinces. Additionally, Latent Dirichlet Allocation (LDA) was employed for topic modeling, revealing variations in emotional reactions across topics and provinces before and during the Omicron variant. Our findings provide actionable insights for health policy-making by highlighting the role of emotional dynamics in vaccine acceptance and offering a province-level analysis of Twitter discussions. This study demonstrates the potential of social media data to understand public sentiment during disease outbreaks and serves as a valuable reference for future academic research.},
  archive      = {J_MLA},
  author       = {Blessing Ogbuokiri and Ali Ahmadi and Nidhi Tripathi and Laleh Seyyed-Kalantari and Woldergebriel Assefa Woldegerima and Bruce Mellado and Jiahong Wu and James Orbinski and Ali Asgary and Jude Dzevela Kong},
  doi          = {10.1016/j.mlwa.2025.100644},
  journal      = {Machine Learning with Applications},
  month        = {6},
  pages        = {100644},
  shortjournal = {Mach. Learn. Appl.},
  title        = {Emotional reactions towards vaccination during the emergence of the omicron variant: Insights from twitter analysis in south africa},
  volume       = {20},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). MLAL: Multiple prompt learning and generation of auxiliary labeled utterances for emotion recognition in conversations. <em>MLA</em>, <em>20</em>, 100643. (<a href='https://doi.org/10.1016/j.mlwa.2025.100643'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Emotion Recognition in Conversations (ERC) is one of the most prominent research directions in the field of Natural Language Processing (NLP). It aims to accurately identify the emotional state expressed in conversations and is widely applied in psychology, education, and healthcare. However, ERC poses significant challenges due to various factors, such as conversational context, the experience of speaker, and subtle differences between similar emotion labels. Existing research primarily strives for effective sequence and graph structure to model utterance and interaction. Moreover, these methods lack comprehensive understanding of conversational contexts and precise distinction between similar emotions. To address the limitation, in this study, we propose a novel framework combining Multiple Prompt Learning and Generation of Auxiliary Labeled Utterances (MLAL). Firstly, a global prompt is constructed to facilitate the understanding of the conversational context. Specifically, utterances originating from the same speaker are identified and interactively processed. Simultaneously, taking into account the influence of speaker experience, an experience prompt is designed by retrieving and interacting with the historical utterances of speakers that display high similarity. Besides, we generate refined auxiliary labeled utterances by means of the label paraphrasing mechanism to distinguish between similar emotions. Results from experiments show that our proposed approach performs better on three datasets than the state-of-the-art techniques currently in use.},
  archive      = {J_MLA},
  author       = {Zhinan Gou and Yuxin Chen and Yuchen Long and Mengyao Jia and Zhili Liu and Jun Zhu},
  doi          = {10.1016/j.mlwa.2025.100643},
  journal      = {Machine Learning with Applications},
  month        = {6},
  pages        = {100643},
  shortjournal = {Mach. Learn. Appl.},
  title        = {MLAL: Multiple prompt learning and generation of auxiliary labeled utterances for emotion recognition in conversations},
  volume       = {20},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Spatiotemporal risk mapping of statewide weather-related traffic crashes: A machine learning approach. <em>MLA</em>, <em>20</em>, 100642. (<a href='https://doi.org/10.1016/j.mlwa.2025.100642'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Improving transportation safety statewide is key in upholding a state's economy. However, weather-related crashes, known to be one of the most severe types of crashes, poses a threat to this as lots of money is lost to lives and property damage. The goal of this study is to employ machine learning (ML) to develop a workflow on which weather-related crash risk can be better identified, predicted, and interpreted. Central to this workflow, the effects of spatiotemporal heterogeneity of weather-related crashes are studied. To demonstrate the workflow, weather-related crash events in the state of North Carolina were used. Space-time cubes were created using an optimized 5 mi x 5mi grid size and 1-month time aggregation. Equivalent property damage only (EPDO) scores were computed for each space-time cube to create a risk metric that combines both crash frequency and severity. A two-layered technique was employed for identifying and labelling crash risk zones. Subsequently, XGBoost model was used to predict crash risk zones and identify factors associated with the different risk levels. SHapley Additive exPlanations (SHAP), an explainable AI (XAI) tool, was used to interpret the model and examine the relationship between the explanatory variables and the outcome. Per the results, there are three optimal clusters with distinct variability of the impact of weather conditions that constitute the crash risk levels in the study area. The workflow can be used by transportation safety units within state departments of transportation (DOTs) to evaluate different safety risk levels, and the potential high-risk zones can be flagged for devising countermeasures (i.e., proactive risk mitigation strategies).},
  archive      = {J_MLA},
  author       = {Abimbola Ogungbire and Srinivas S. Pulugurtha},
  doi          = {10.1016/j.mlwa.2025.100642},
  journal      = {Machine Learning with Applications},
  month        = {6},
  pages        = {100642},
  shortjournal = {Mach. Learn. Appl.},
  title        = {Spatiotemporal risk mapping of statewide weather-related traffic crashes: A machine learning approach},
  volume       = {20},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). ASKSQL: Enabling cost-effective natural language to SQL conversion for enhanced analytics and search. <em>MLA</em>, <em>20</em>, 100641. (<a href='https://doi.org/10.1016/j.mlwa.2025.100641'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Natural Language to SQL (NL2SQL) for database query and search has been a significant research focus in recent years. However, existing methods have predominantly concentrated on SQL query generation, overlooking critical aspects such as enterprise cost, latency, and the overall analytical search experience. This paper presents an end-to-end NL2SQL pipeline named ASKSQL that integrates optimized and adaptable query recommendation, entity-swapping module, and skeleton-based caching to enhance the search experience. The pipeline also incorporates an intelligent schema selector for efficiently handling large schema entity selection and a fast and scalable adapter-based query generator. The proposed pipeline emphasizes minimizing Large Language Model (LLM) costs by finding search patterns in previously requested or generated queries. The pipeline can also be tuned to adapt to trends and common patterns observed from the daily search analytics. Experimental results demonstrate an average increase in accuracy by 5.83% and an overall decrease in latency by 32.6% as the usage count of this search pipeline increases highlighting its effectiveness in improving the NL2SQL search experience.},
  archive      = {J_MLA},
  author       = {Arpit Bajgoti and Rishik Gupta and Rinky Dwivedi},
  doi          = {10.1016/j.mlwa.2025.100641},
  journal      = {Machine Learning with Applications},
  month        = {6},
  pages        = {100641},
  shortjournal = {Mach. Learn. Appl.},
  title        = {ASKSQL: Enabling cost-effective natural language to SQL conversion for enhanced analytics and search},
  volume       = {20},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Deep learning-based identification of precipitation clouds from all-sky camera data for observatory safety. <em>MLA</em>, <em>20</em>, 100640. (<a href='https://doi.org/10.1016/j.mlwa.2025.100640'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {For monitoring the night sky conditions, wide-angle all-sky cameras are used in most astronomical observatories to monitor the sky cloudiness. In this manuscript, we apply a deep-learning approach for automating the identification of precipitation clouds in all-sky camera data as a cloud warning system. We construct our original training and test sets using the all-sky camera image archive of the Iranian National Observatory (INO). The training and test set images are labeled manually based on their potential rainfall and their distribution in the sky. We train our model on a set of roughly 2445 images taken by the INO all-sky camera through the deep learning method based on the EfficientNet network. Our model reaches an average accuracy of 99% in determining the cloud rainfall’s potential and an accuracy of 96% for cloud coverage. To enable a comprehensive comparison and evaluate the performance of alternative architectures for the task, we additionally trained three models—LeNet, DeiT, and AlexNet. This approach can be used for early warning of incoming dangerous clouds toward telescopes and harnesses the power of deep learning to automatically analyze vast amounts of all-sky camera data and accurately identify precipitation clouds formations. Our trained model can be deployed for real-time analysis, enabling the rapid identification of potential threats, and offering a scaleable solution that can improve our ability to safeguard telescopes and instruments in observatories. This is important now that numerous small- and medium-sized telescopes are increasingly integrated with smart control systems to reduce manual operation.},
  archive      = {J_MLA},
  author       = {Mohammad H. Zhoolideh Haghighi and Alireza Ghasrimanesh and Habib Khosroshahi},
  doi          = {10.1016/j.mlwa.2025.100640},
  journal      = {Machine Learning with Applications},
  month        = {6},
  pages        = {100640},
  shortjournal = {Mach. Learn. Appl.},
  title        = {Deep learning-based identification of precipitation clouds from all-sky camera data for observatory safety},
  volume       = {20},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Software engineering meets legal texts: LLMs for auto detection of contract smells. <em>MLA</em>, <em>20</em>, 100639. (<a href='https://doi.org/10.1016/j.mlwa.2025.100639'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Although there have been many major advances in Artificial Intelligence including its application to a wide variety of tasks, some specialized domains remain difficult to tackle. In this work, we examine parallels between software engineering and legal contract drafting and analysis. Porting well-known code smells principles to various legal contracts, we introduce ”contract smells,” text patterns that are indicative of potentially significant issues within contractual agreements. We leverage semi-auto labeling with GPT-4, prompting and expert spot checks, to create datasets for suitability testing of auto detection of these contract smells. Using transformer-based models, we explore the impact of legal domain knowledge, hyperparameters fine tuning and specific task information on detection success. We achieve high accuracy with further fine-tuning of BERT as well as LEGAL-BERT, while more consistent results were achieved using task-specific data. We further demonstrate that although multi-class detection can boost coverage of rare smells, single-class detection yields better accuracy. While this is an initial foray into the idea of contract smells, this work underscores the feasibility of applying advanced NLP techniques and LLMs to automate aspects of legal contract review, suggesting a scalable path toward standardized, machine-assisted legal drafting and analysis.},
  archive      = {J_MLA},
  author       = {Moriya Dechtiar and Daniel Martin Katz and Hongming Wang},
  doi          = {10.1016/j.mlwa.2025.100639},
  journal      = {Machine Learning with Applications},
  month        = {6},
  pages        = {100639},
  shortjournal = {Mach. Learn. Appl.},
  title        = {Software engineering meets legal texts: LLMs for auto detection of contract smells},
  volume       = {20},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). One-shot generative distribution matching for augmented RF-based UAV identification. <em>MLA</em>, <em>20</em>, 100638. (<a href='https://doi.org/10.1016/j.mlwa.2025.100638'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This work addresses the challenge of identifying Unmanned Aerial Vehicles (UAV) using radiofrequency (RF) fingerprinting in limited RF environments. The complexity and variability of RF signals, influenced by environmental interference and hardware imperfections, often render traditional RF-based identification methods ineffective. To address these complications, the study introduces the rigorous use of one-shot generative methods for augmenting transformed RF signals, offering a significant improvement in UAV identification. This approach, when utilizing a distributional distance metric, demonstrates significant promise in low-data regimes, outperforming deep generative methods such as conditional generative adversarial networks (GANs) and variational autoencoders (VAEs). The paper provides a theoretical guarantee for the effectiveness of one-shot generative models in augmenting limited data, setting a precedent for their application in limited RF environments. This research also contributes to learning techniques in low-data regime scenarios, which may include complex sequences beyond images and videos. The code and links to datasets used in this study are available at https://github.com/amir-kazemi/uav-rf-id .},
  archive      = {J_MLA},
  author       = {Amir Kazemi and Salar Basiri and Volodymyr Kindratenko and Srinivasa Salapaka},
  doi          = {10.1016/j.mlwa.2025.100638},
  journal      = {Machine Learning with Applications},
  month        = {6},
  pages        = {100638},
  shortjournal = {Mach. Learn. Appl.},
  title        = {One-shot generative distribution matching for augmented RF-based UAV identification},
  volume       = {20},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Hybrid oversampling technique for imbalanced pattern recognition: Enhancing performance with borderline synthetic minority oversampling and generative adversarial networks. <em>MLA</em>, <em>20</em>, 100637. (<a href='https://doi.org/10.1016/j.mlwa.2025.100637'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Class imbalance problems (CIP) are one of the potential challenges in developing unbiased Machine Learning models for predictions. CIP occurs when data samples are not equally distributed between two or multiple classes. Several synthetic oversampling techniques have been introduced to balance the imbalanced data by oversampling the minor samples. One of the potential drawbacks of existing oversampling techniques is that they often fail to focus on the data samples that lie at the border point and give more attention to the extreme observations, ultimately limiting the creation of more diverse data after oversampling, and that is almost the scenario for most of the oversampling strategies. As an effect, marginalization occurs after oversampling. To address these issues, in this work, we propose a hybrid oversampling technique, named Borderline Synthetic Minority Oversampling and Generative Adversarial Network (BSGAN), by combining the strengths of Borderline-Synthetic Minority Oversampling Technique (SMOTE) and Generative Adversarial Networks (GANs). This approach aims to generate more diverse data that follow Gaussian distributions, marking a significant contribution to the field of Artificial Intelligence. We tested BSGAN on ten highly imbalanced datasets, demonstrating its application in engineering, where it outperformed existing oversampling techniques, creating a more diverse dataset that follows a normal distribution after oversampling.},
  archive      = {J_MLA},
  author       = {Md Manjurul Ahsan and Shivakumar Raman and Yingtao Liu and Zahed Siddique},
  doi          = {10.1016/j.mlwa.2025.100637},
  journal      = {Machine Learning with Applications},
  month        = {6},
  pages        = {100637},
  shortjournal = {Mach. Learn. Appl.},
  title        = {Hybrid oversampling technique for imbalanced pattern recognition: Enhancing performance with borderline synthetic minority oversampling and generative adversarial networks},
  volume       = {20},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Advanced fault detection in photovoltaic panels using enhanced U-net architectures. <em>MLA</em>, <em>20</em>, 100636. (<a href='https://doi.org/10.1016/j.mlwa.2025.100636'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Fault detection in photovoltaic (PV) panels using thermal images remains a significant challenge due to the complexity of thermal patterns, environmental noise, and the subtle nature of anomalies. This paper introduces an advanced deep learning framework that enhances the U-Net architecture by integrating Residual Blocks, Atrous Spatial Pyramid Pooling (ASPP), and Attention Mechanisms. These enhancements collectively improve feature extraction, contextual understanding, and fault localization, addressing the limitations of traditional segmentation approaches and reducing false positives. Extensive experiments demonstrate that the proposed method significantly outperforms all benchmarked algorithms across key segmentation metrics, including standard U-Net, U-Net with ASPP, and DeepLabV3+. Compared to standard U-Net, the proposed model achieves more than a 29% increase in F1-score and a 62% improvement in Intersection over Union (IoU) while reducing segmentation loss by 71%. Its ability to accurately detect faults under challenging conditions establishes the framework as a state-of-the-art solution for real-time PV monitoring. These results demonstrate the effectiveness of the proposed approach in addressing the challenges of PV fault detection, offering a practical and reliable solution for ensuring the operational performance of renewable energy systems.},
  archive      = {J_MLA},
  author       = {Khalfalla Awedat and Gurcan Comert and Mustafa Ayad and Abdulmajid Mrebit},
  doi          = {10.1016/j.mlwa.2025.100636},
  journal      = {Machine Learning with Applications},
  month        = {6},
  pages        = {100636},
  shortjournal = {Mach. Learn. Appl.},
  title        = {Advanced fault detection in photovoltaic panels using enhanced U-net architectures},
  volume       = {20},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Pioneering precision in lumbar spine MRI segmentation with advanced deep learning and data enhancement. <em>MLA</em>, <em>20</em>, 100635. (<a href='https://doi.org/10.1016/j.mlwa.2025.100635'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This study presents an advanced approach to lumbar spine segmentation using deep learning techniques, focusing on addressing key challenges such as class imbalance and data preprocessing. Magnetic resonance imaging (MRI) scans of patients with low back pain are meticulously preprocessed to accurately represent three critical classes: vertebrae, spinal canal, and intervertebral discs (IVDs). By rectifying class inconsistencies in the data preprocessing stage, the fidelity of the training data is ensured. The modified U-Net model incorporates innovative architectural enhancements, including an upsample block with leaky Rectified Linear Units (ReLU) and Glorot uniform initializer, to mitigate common issues such as the dying ReLU problem and improve stability during training. Introducing a custom combined loss function effectively tackles class imbalance, significantly improving segmentation accuracy. Evaluation using a comprehensive suite of metrics showcases the superior performance of this approach, outperforming existing methods and advancing the current techniques in lumbar spine segmentation. These findings hold significant advancements for enhanced lumbar spine MRI and segmentation diagnostic accuracy.},
  archive      = {J_MLA},
  author       = {Istiak Ahmed and Md. Tanzim Hossain and Md. Zahirul Islam Nahid and Kazi Shahriar Sanjid and Md. Shakib Shahariar Junayed and M. Monir Uddin and Mohammad Monirujjaman Khan},
  doi          = {10.1016/j.mlwa.2025.100635},
  journal      = {Machine Learning with Applications},
  month        = {6},
  pages        = {100635},
  shortjournal = {Mach. Learn. Appl.},
  title        = {Pioneering precision in lumbar spine MRI segmentation with advanced deep learning and data enhancement},
  volume       = {20},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Machine learning-driven predictive modeling of mechanical properties in diverse steels. <em>MLA</em>, <em>20</em>, 100634. (<a href='https://doi.org/10.1016/j.mlwa.2025.100634'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This study explores the application of machine learning (ML) in steel design using a small dataset of various steel grades that include 13 key elements and three critical mechanical properties. Random forest (RF) models were systematically evaluated for their robustness and effectiveness in predicting the stress-strain of steel properties. Moreover, other alternative approaches, such as support vector machines, extreme gradient boosting machines, and artificial neural networks, were also evaluated to ensure that the predictions made by the RF model are as accurate as possible. To assess the bias-variance trade-off, 1-seed and random 100-seeds with 80/20 train/test split, and leave-one-out cross-validation for all datasets were conducted. The results demonstrated that the RF models are accurate and reliable, achieving low bias and variance while delivering predictions comparable to, and in some cases better than, those obtained in studies with larger datasets. The analysis revealed a trade-off between strength and ductility, with elongation negatively correlated with yield strength and ultimate tensile strength. This study highlights the feasibility of using small, realistic datasets to develop effective ML models for predicting mechanical properties in steel design. The methodology can also be readily extended to investigate processing-property relationships in other systems, offering a versatile approach for advancing materials science through data-driven techniques.},
  archive      = {J_MLA},
  author       = {Movaffaq Kateb and Sahar Safarian},
  doi          = {10.1016/j.mlwa.2025.100634},
  journal      = {Machine Learning with Applications},
  month        = {6},
  pages        = {100634},
  shortjournal = {Mach. Learn. Appl.},
  title        = {Machine learning-driven predictive modeling of mechanical properties in diverse steels},
  volume       = {20},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Application of machine learning for seam profile identification in robotic welding. <em>MLA</em>, <em>20</em>, 100633. (<a href='https://doi.org/10.1016/j.mlwa.2025.100633'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper addresses critical challenges in automated robotic welding, emphasizing precise weld groove profiling for pipe welding applications. By integrating advanced laser scanning technology with the Local Outlier Factor (LOF) algorithm, the research effectively mitigates outliers and compensates for incomplete data—persistent issues in dynamic manufacturing environments. To further enhance accuracy, a robust neural network model is employed to predict weld groove alignment, a crucial factor in maintaining weld structural integrity. The LOF algorithm was chosen for its ability to detect spatial anomalies, ensuring the exclusion of erroneous data that could compromise welding precision. Experimental results demonstrate that the combined use of LOF and neural networks significantly improves the operational efficiency of robotic welding, delivering consistently strong and precise welds across diverse manufacturing scenarios. The model achieved an average mean square error of 0.078 and an R² value of 0.995, accurately predicting 99.5 % of data. Therefore, neural network modeling enables accurate interpolation of missing data and real-time adjustments to varying operational conditions.},
  archive      = {J_MLA},
  author       = {Fatemeh Habibkhah and Mehrdad Moallem},
  doi          = {10.1016/j.mlwa.2025.100633},
  journal      = {Machine Learning with Applications},
  month        = {6},
  pages        = {100633},
  shortjournal = {Mach. Learn. Appl.},
  title        = {Application of machine learning for seam profile identification in robotic welding},
  volume       = {20},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Uncertainty quantification based on symbolic regression and probabilistic programming and its application. <em>MLA</em>, <em>20</em>, 100632. (<a href='https://doi.org/10.1016/j.mlwa.2025.100632'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The joint roughness coefficient (JRC) is critical to evaluate the strength and deformation behavior of joint rock mass in rock engineering. Various methods have been developed to estimate JRC value based on the statistical parameter of rock joints. The JRC value is uncertain due to the complex, random rock joint. Uncertainty is an essential characteristic of rock joints. However, the traditional determinative method cannot deal with uncertainty during the analysis, evaluation, and characterization of the mechanism for the rock joint. This study developed a novel JRC determination framework to estimate the JRC value and evaluate the uncertainty of rock joints based on symbolic regression and probabilistic programming. The symbolic regression was utilized to generate the general empirical equation with the unknown coefficient for the JRC determination of rock joints. The probabilistic programming was used to quantify the uncertainty of the rock joint roughness. The ten standard rock joint profiles illustrated and investigated the developed framework. And then, the developed framework was applied to the collected rock joint profile from the literature. The predicted JRC value was compared with the traditional empirical equations. The results show that the generalization performance of the developed framework is better than the traditional determinative empirical equation. It provides a scientific, reliable, and helpful to estimate the JRC value and characterize the mechanical behavior of joint rock mass.},
  archive      = {J_MLA},
  author       = {Yuyang Zhao and Hongbo Zhao},
  doi          = {10.1016/j.mlwa.2025.100632},
  journal      = {Machine Learning with Applications},
  month        = {6},
  pages        = {100632},
  shortjournal = {Mach. Learn. Appl.},
  title        = {Uncertainty quantification based on symbolic regression and probabilistic programming and its application},
  volume       = {20},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Key technical indicators for stock market prediction. <em>MLA</em>, <em>20</em>, 100631. (<a href='https://doi.org/10.1016/j.mlwa.2025.100631'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The use of technical indicators for forecasting the stock market is widespread among investors and researchers. It is crucial to determine the optimal number of input technical indicators to predict the stock market successfully. However, there is no consensus on which collection of technical indicators is most suitable. The selection of technical indicators for a given forecasting model continues to be an active area of research. To our knowledge, there is limited published work on the importance of technical indicators in various categories such as momentum, trend, volatility, and volume. To identify the key technical indicators for stock market prediction, we employed XGBoost, Random Forest, Support Vector Regression, and LSTM regression techniques using 88 technical indicators as input data. We also used the PCA method for dimension reduction. The results reveal the most significant technical indicators within the momentum, trend, volatility, and volume categories. Our findings provide evidence that the proposed model is highly effective in predicting daily prices (with and without lag in Close price) on the S&P 500 stock index.},
  archive      = {J_MLA},
  author       = {Seyed Mostafa Mostafavi and Ali Reza Hooman},
  doi          = {10.1016/j.mlwa.2025.100631},
  journal      = {Machine Learning with Applications},
  month        = {6},
  pages        = {100631},
  shortjournal = {Mach. Learn. Appl.},
  title        = {Key technical indicators for stock market prediction},
  volume       = {20},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Machine learning-based state of charge estimation: A comparison between CatBoost model and C-BLSTM-AE model. <em>MLA</em>, <em>20</em>, 100629. (<a href='https://doi.org/10.1016/j.mlwa.2025.100629'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The State of Charge (SOC) is a key metric within a Lithium-ion battery management system (BMS). Accurate SOC estimation is essential for enhancing battery longevity and ensuring user safety, making it a critical component of an effective BMS. Although SOC estimation has become an active research area for the machine learning (ML) community, only a handful of works have considered its estimation at negative temperatures. This paper proposes the application of two machine learning-based approaches for SOC estimation that perform well at wide range of temperatures (positive and negative) and varying dynamic loads. The first one is a hybrid deep learning approach based on the Convolutional BLSTM Auto-Encoder (C-BLSTM-AE) model that relies on extracting abstract features from input data. The second one is a CatBoost model that leverages the gradient boosting technique to enhance the prediction made by its constituent trees. The performance of the models is evaluated by comparing their regression accuracy and computational resource utilization. The C-BLSTM-AE model achieves a low Mean Absolute Error (MAE) of 0.52 % under fixed ambient temperature conditions and maintains a MAE of 1.03 % for variable ambient temperatures. The CatBoost model achieves a MAE of 0.69 % with fixed temperature settings and a MAE of 1.09 % under variable temperature conditions.},
  archive      = {J_MLA},
  author       = {Abderrahim Zilali and Mehdi Adda and Khaled Ziane and Maxime Berger},
  doi          = {10.1016/j.mlwa.2025.100629},
  journal      = {Machine Learning with Applications},
  month        = {6},
  pages        = {100629},
  shortjournal = {Mach. Learn. Appl.},
  title        = {Machine learning-based state of charge estimation: A comparison between CatBoost model and C-BLSTM-AE model},
  volume       = {20},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Predicting classification errors using NLP-based machine learning algorithms and expert opinions. <em>MLA</em>, <em>19</em>, 100630. (<a href='https://doi.org/10.1016/j.mlwa.2025.100630'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Various intentional and unintentional biases of humans manifest in classification tasks, such as those related to risk management. In this paper we demonstrate the role of ML algorithms when accomplishing these tasks and highlight the role of expert know-how when training the staff as well as, and very importantly, when training and fine-tuning ML algorithms. In the process of doing so and when facing well-known inefficiencies of the traditional F1 score, especially when working with unbalanced datasets, we suggest a modification of the score by incorporating human-experience-trained algorithms, which include both expert-trained algorithms (i.e., with the involvement of expert experiences in classification tasks) and staff-trained algorithms (i.e., with the involvement of experiences of those staff who have been trained by experts). Our findings reveal that the modified F1 score diverges from the traditional staff F1 score when the staff labels exhibit weak correlation with expert labels, which indicates insufficient staff training. Furthermore, the Long Short-Term Memory (LSTM) model outperforms other classifiers in terms of the modified F1 score when applied to the classification of textual narratives in consumer complaints.},
  archive      = {J_MLA},
  author       = {Peiheng Gao and Chen Yang and Ning Sun and Ričardas Zitikis},
  doi          = {10.1016/j.mlwa.2025.100630},
  journal      = {Machine Learning with Applications},
  month        = {3},
  pages        = {100630},
  shortjournal = {Mach. Learn. Appl.},
  title        = {Predicting classification errors using NLP-based machine learning algorithms and expert opinions},
  volume       = {19},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A deep learning framework for accurate COVID-19 classification in CT-scan images. <em>MLA</em>, <em>19</em>, 100628. (<a href='https://doi.org/10.1016/j.mlwa.2025.100628'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Background In response to the global COVID-19 pandemic, we have introduced a binary classification model that employs convolutional layers to differentiate between normal cases and COVID-19-infected cases. Our primary aim was to address the urgent need for a highly efficient and accurate diagnostic tool to combat the widespread outbreak of COVID-19. Methods To achieve the background, we proposed a convolutional structure that comprises 10 layers in the encoder and 3 dense layers in the decoder. We conducted comprehensive experiments and evaluations using four distinct datasets. Results The outcomes of our study consistently demonstrated remarkable performance, with our proposed model achieving an accuracy of 89.00 %, a sensitivity of 0.95, a specificity of 0.88, and an impressive AUC of 0.92. Notably, Dataset 4 yielded the most promising results among all datasets, underscoring the effectiveness of our approach. Conclusion Our research substantiates the superiority of our model over previous methodologies and pre-trained models. Furthermore, it significantly contributes to global efforts in combating COVID-19 by providing an advanced diagnostic tool. This work also paves the way for future breakthroughs in the field of medical image analysis.},
  archive      = {J_MLA},
  author       = {Shirin Kordnoori and Maliheh Sabeti and Hamidreza Mostafaei and Saeed Seyed Agha Banihashemi},
  doi          = {10.1016/j.mlwa.2025.100628},
  journal      = {Machine Learning with Applications},
  month        = {3},
  pages        = {100628},
  shortjournal = {Mach. Learn. Appl.},
  title        = {A deep learning framework for accurate COVID-19 classification in CT-scan images},
  volume       = {19},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Corrigendum to “Machine learning for sports betting: Should model selection be based on accuracy or calibration?” [Machine learning with applications volume 16, june 2024, 100539]. <em>MLA</em>, <em>19</em>, 100627. (<a href='https://doi.org/10.1016/j.mlwa.2025.100627'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_MLA},
  author       = {Conor Walsh and Alok Joshi},
  doi          = {10.1016/j.mlwa.2025.100627},
  journal      = {Machine Learning with Applications},
  month        = {3},
  pages        = {100627},
  shortjournal = {Mach. Learn. Appl.},
  title        = {Corrigendum to “Machine learning for sports betting: Should model selection be based on accuracy or calibration?” [Machine learning with applications volume 16, june 2024, 100539]},
  volume       = {19},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Noninvasive estimation of blood glucose and HbA1c using quantum machine learning technique. <em>MLA</em>, <em>19</em>, 100626. (<a href='https://doi.org/10.1016/j.mlwa.2025.100626'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we developed models with quantum and classical machine learning algorithms to detect blood glucose and HbA1c noninvasively from ten-second fingertip video by deploying a smartphone and near-infrared spectroscopy. Using our developed framework, we collected 136 participants’ ten-second fingertip videos with their baseline blood glucose and HbA1c levels after getting approval from the Institutional Review Board (IRB). We extracted 45 PPG (photoplethysmography) features from the ten-second fingertip video by using the Beer–Lambert law and applied feature engineering to select the most important features. We applied two Quantum Machine Learning (QML) based algorithms and seven Classical Machine Learning (CML) based algorithms for estimating blood glucose and HbA1c levels. The application of QML for the noninvasive estimation of blood glucose and HbA1c is a new and unexplored research area. Among all developed models, the Quantum Support Vector Machine performs best for predicting both blood glucose and HbA1c. The Quantum Support Vector Machine provides an accuracy of 89.30% and an average k-fold cross-validation score of 92.50% for blood glucose prediction and an accuracy of 96.30% and an average k-fold cross-validation score of 92.50% for HbA1c prediction. Our study signifies the potential of QML algorithms in noninvasive health monitoring, especially in the less-explored area of blood glucose and HbA1c estimation. The high performance of the developed models paves the way for advancing noninvasive techniques for measuring blood constituents. These findings offer promising applications in personalized healthcare, including continuous monitoring, early disease diagnosis, and more convenient management of chronic conditions.},
  archive      = {J_MLA},
  author       = {Parama Sridevi and Masud Rabbani and Md Hasanul Aziz and Paramita Basak Upama and Sayed Mashroor Mamun and Rumi Ahmed Khan and Sheikh Iqbal Ahamed},
  doi          = {10.1016/j.mlwa.2025.100626},
  journal      = {Machine Learning with Applications},
  month        = {3},
  pages        = {100626},
  shortjournal = {Mach. Learn. Appl.},
  title        = {Noninvasive estimation of blood glucose and HbA1c using quantum machine learning technique},
  volume       = {19},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A multiobjective continuation method to compute the regularization path of deep neural networks. <em>MLA</em>, <em>19</em>, 100625. (<a href='https://doi.org/10.1016/j.mlwa.2025.100625'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Sparsity is a highly desired feature in deep neural networks (DNNs) since it ensures numerical efficiency, improves the interpretability (due to the smaller number of relevant features), and robustness. For linear models, it is well known that there exists a regularization path connecting the sparsest solution in terms of the ℓ 1 norm, i.e., zero weights and the non-regularized solution. Recently, there was a first attempt to extend the concept of regularization paths to DNNs by means of treating the empirical loss and sparsity ( ℓ 1 norm) as two conflicting criteria and solving the resulting multiobjective optimization problem. However, due to the non-smoothness of the ℓ 1 norm and the large number of parameters, this approach is not very efficient from a computational perspective. To overcome this limitation, we present an algorithm that allows for the approximation of the entire Pareto front for the above-mentioned objectives in a very efficient manner for high-dimensional DNNs with millions of parameters. We present numerical examples using both deterministic and stochastic gradients. We furthermore demonstrate that knowledge of the regularization path allows for a well-generalizing network parametrization.},
  archive      = {J_MLA},
  author       = {Augustina Chidinma Amakor and Konstantin Sonntag and Sebastian Peitz},
  doi          = {10.1016/j.mlwa.2025.100625},
  journal      = {Machine Learning with Applications},
  month        = {3},
  pages        = {100625},
  shortjournal = {Mach. Learn. Appl.},
  title        = {A multiobjective continuation method to compute the regularization path of deep neural networks},
  volume       = {19},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Apply a deep learning hybrid model optimized by an improved chimp optimization algorithm in PM2.5 prediction. <em>MLA</em>, <em>19</em>, 100624. (<a href='https://doi.org/10.1016/j.mlwa.2025.100624'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {PM 2.5 pollution in the atmosphere not only contaminates the environment but also seriously affects human health. Therefore, studying how to accurately predict future PM 2.5 concentrations holds significant importance and practical value. This paper innovatively P M 2 . 5 proposes a high-accuracy prediction model: RF-ICHOA-CNN-LSTM-Attention. First, the Random Forest (RF) model is utilized to evaluate the importance of air pollution and meteorological features and select more suitable input features. Subsequently, a one-dimensional convolutional neural network (1DCNN) with efficient feature extraction capability is used to extract dynamic features from sequences. The extracted feature vector sequences are then fed into a Long Short-Term Memory Network (LSTM). After the LSTM, an Attention Mechanism is incorporated to assign different weights to the input features, emphasizing the role of the important features. Additionally, the Improved Chimp Optimization Algorithm (IChOA) is employed to optimize the number of neurons in the two hidden layers of LSTM, the learning rate, and the number of training epochs. The experimental results on 12 test functions demonstrate that the optimization performance of IChOA is better than that of ChOA and the representative swarm optimization algorithms used for comparison. In the case of PM 2.5 predictions in Yining and Beijing, experimental results show that the proposed model achieved the best performance in terms of RMSE, MAE, and R 2 This indicates its excellent prediction accuracy and generalization capability, Thus proving its effectiveness in predicting PM 2.5 concentration in the real world.},
  archive      = {J_MLA},
  author       = {Ming Wei and Xiaopeng Du},
  doi          = {10.1016/j.mlwa.2025.100624},
  journal      = {Machine Learning with Applications},
  month        = {3},
  pages        = {100624},
  shortjournal = {Mach. Learn. Appl.},
  title        = {Apply a deep learning hybrid model optimized by an improved chimp optimization algorithm in PM2.5 prediction},
  volume       = {19},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Machine learning techniques and multi-objective programming to select the best suppliers and determine the orders. <em>MLA</em>, <em>19</em>, 100623. (<a href='https://doi.org/10.1016/j.mlwa.2025.100623'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Selection of appropriate suppliers and allocation the orders among them have become the two key strategic decisions regarding purchasing. In this study, a two-phase integrated approach is proposed for solving supplier selection and order allocation problems. Phase 1 contains four techniques from statistics and Machine Learning (ML), including Auto-Regressive Integrated Moving Average, Random Forest, Gradient Boosting Regression, and Long Short-term Memory for forecasting the demands, using large amounts of real historical data. In Phase 2, suppliers’ qualitative weights are determined by a fuzzy logic model. Then, a new multi-objective programming model is designed, considering multiple periods and products. In this phase, the results of Phase 1 and the results of the fuzzy model are utilized as inputs for the multi-objective model. The weighted-sum method is applied for solving the multi-objective model. The results show Random Forest model leads to more accurate predictions than the other examined models in this study. In addition, based on the results, the selection of the forecasting techniques and different weights of suppliers affect both supplier selection and the related orders.},
  archive      = {J_MLA},
  author       = {Asma ul Husna and Saman Hassanzadeh Amin and Ahmad Ghasempoor},
  doi          = {10.1016/j.mlwa.2025.100623},
  journal      = {Machine Learning with Applications},
  month        = {3},
  pages        = {100623},
  shortjournal = {Mach. Learn. Appl.},
  title        = {Machine learning techniques and multi-objective programming to select the best suppliers and determine the orders},
  volume       = {19},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Safety analysis in the era of large language models: A case study of STPA using ChatGPT. <em>MLA</em>, <em>19</em>, 100622. (<a href='https://doi.org/10.1016/j.mlwa.2025.100622'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Can safety analysis leverage Large Language Models (LLMs)? This study examines the application of Systems Theoretic Process Analysis (STPA) to Automatic Emergency Brake (AEB) and Electricity Demand Side Management (DSM) systems, utilising Chat Generative Pre-Trained Transformer (ChatGPT). We investigate the impact of collaboration schemes, input semantic complexity, and prompt engineering on STPA results. Comparative results indicate that using ChatGPT without human intervention may be inadequate due to reliability issues. However, with careful design, it has the potential to outperform human experts. No statistically significant differences were observed when varying the input semantic complexity or using domain-agnostic prompt guidelines. While STPA-specific prompt engineering produced statistically significant and more pertinent results, ChatGPT generally yielded more conservative and less comprehensive outcomes. We also identify future challenges, such as concerns regarding the trustworthiness of LLMs and the need for standardisation and regulation in this field. All experimental data are publicly accessible.},
  archive      = {J_MLA},
  author       = {Yi Qi and Xingyu Zhao and Siddartha Khastgir and Xiaowei Huang},
  doi          = {10.1016/j.mlwa.2025.100622},
  journal      = {Machine Learning with Applications},
  month        = {3},
  pages        = {100622},
  shortjournal = {Mach. Learn. Appl.},
  title        = {Safety analysis in the era of large language models: A case study of STPA using ChatGPT},
  volume       = {19},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Ensembles of deep one-class classifiers for multi-class image classification. <em>MLA</em>, <em>19</em>, 100621. (<a href='https://doi.org/10.1016/j.mlwa.2025.100621'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Traditional methods for multi-class classification (MCC) involve using a monolithic feature extractor and classifier trained on data from all the classes simultaneously. These methods are dependent on the number and types of classes and are therefore rigid against changes to the class structure. For instance, if the number of classes needs to be modified or new training data becomes available, retraining would be required for optimum classification performance. Moreover, these classifiers can become biased toward classes with a large data imbalance. An alternative, more attractive framework is to consider an ensemble of one-class classifiers (EOCC) where each one-class classifier (OCC) is trained with data from a single class only, without using any information from the other classes. Although this framework has not yet systematically matched or surpassed the performance of traditional MCC approaches, it deserves further investigation for several reasons. First, it provides a more flexible framework for handling changes in class structure compared to the traditional MCC approach. Second, it is less biased toward classes with large data imbalances compared to the multi-class classification approach. Finally, each OCC can be separately optimized depending on the characteristics of the class it represents. In this paper, we have performed extensive experiments to evaluate EOCC for MCC using traditional OCCs based on Principal Component Analysis (PCA) and Auto-encoders (AE) as well as newly proposed OCCs based on Generative Adversarial Networks (GANs). Moreover, we have compared the performance of EOCC with traditional multi-class DL classifiers including VGG-19, Resnet and EfficientNet. Two different datasets were used in our experiments: (i) a subset from the Plant Village dataset plant disease dataset with high variance in the number of classes and amount of data in each class, and (ii) an Alzheimer’s disease dataset with low amounts of data and a large imbalance in data between classes. Our results show that the GAN-based EOCC outperform previous EOCC approaches and improve the performance gap with traditional MCC approaches.},
  archive      = {J_MLA},
  author       = {Alexander Novotny and George Bebis and Alireza Tavakkoli and Mircea Nicolescu},
  doi          = {10.1016/j.mlwa.2025.100621},
  journal      = {Machine Learning with Applications},
  month        = {3},
  pages        = {100621},
  shortjournal = {Mach. Learn. Appl.},
  title        = {Ensembles of deep one-class classifiers for multi-class image classification},
  volume       = {19},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multi-agent deep reinforcement learning with online and fair optimal dispatch of EV aggregators. <em>MLA</em>, <em>19</em>, 100620. (<a href='https://doi.org/10.1016/j.mlwa.2025.100620'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The growing popularity of electric vehicles (EVs) and the unpredictable behavior of EV owners have attracted attention to real-time coordination of EVs charging management. This paper presents a hierarchical structure for charging management of EVs by integrating fairness and efficiency concepts within the operations of the distribution system operator (DSO) while utilizing a multi-agent deep reinforcement learning (MADRL) framework to tackle the complexities of energy purchasing and distribution among EV aggregators (EVAs). At the upper level, DSO calculates the maximum allowable power for each EVA based on power flow constraints to ensure grid safety. Then, it finds the optimal efficiency-Jain tradeoff (EJT) point, where it sells the highest energy amount while ensuring equitable energy distribution. At the lower level, initially, each EVA acts as an agent employing a double deep Q-network (DDQN) with adaptive learning rates and prioritized experience replay to determine optimal energy purchases from the DSO. Then, the real-time smart dispatch (RSD) controller prioritizes EVs for energy dispatch based on relevant EVs information. Findings indicate the proposed enhanced DDQN outperforms deep deterministic policy gradient (DDPG) and proximal policy optimization (PPO) in cumulative rewards and convergence speed. Finally, the framework’s performance is evaluated against uncontrolled charging and the first come first serve (FCFS) scenario using the 118-bus distribution system, demonstrating superior performance in maintaining safe operation of the grid while reducing charging costs for EVAs. Additionally, the framework’s integration with renewable energy sources (RESs), such as photovoltaic (PV), demonstrates its potential to enhance grid reliability.},
  archive      = {J_MLA},
  author       = {Arian Shah Kamrani and Anoosh Dini and Hanane Dagdougui and Keyhan Sheshyekani},
  doi          = {10.1016/j.mlwa.2025.100620},
  journal      = {Machine Learning with Applications},
  month        = {3},
  pages        = {100620},
  shortjournal = {Mach. Learn. Appl.},
  title        = {Multi-agent deep reinforcement learning with online and fair optimal dispatch of EV aggregators},
  volume       = {19},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Improving mango ripeness grading accuracy: A comprehensive analysis of deep learning, traditional machine learning, and transfer learning techniques. <em>MLA</em>, <em>19</em>, 100619. (<a href='https://doi.org/10.1016/j.mlwa.2025.100619'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Bangladesh ranks among the top 10 countries globally in mango output. Mangoes can be classified based on their ripeness, with skin color being the most significant aspect. The current classification procedure is done manually, leading to mistakes and vulnerability to human error. Most research often focuses on using a single method to assess the ripeness of fruits. The study comprises a set of comprehensive tests showcasing different tactics for determining the most efficient methods through various models. One unique dataset was used for all five models: Gaussian Naive Bayes (GNB), Support Vector Machine (SVM), Gradient Boosting (GB), Random Forest (RF), and K-Nearest Neighbors (KNN). Utilizing convolutional neural networks (CNNs) and VGG16, a pre-trained CNN model, to extract features and train the dataset. Used these training datasets as input to calculate the average accuracy of the five models during testing. In addition to these experiments, these five models using standard techniques also evaluated. The study also included a comparative analysis that emphasized the best performance of each model in various scenarios. This analysis shows that the CNN model consistently performs better than the transfer learning model (VGG16) and classical machine learning methods. Except for the KNN and Naive Bayes scenarios, the VGG16 model achieved much higher accuracy compared to typical machine learning methods. In three other models, classical machine learning outperforms the VGG16 model. The Gradient Boosting model in deep learning (CNN) demonstrated the highest accuracy of 96.28 % compared to other models and techniques.},
  archive      = {J_MLA},
  author       = {Md․ Saon Sikder and Mohammad Shamsul Islam and Momenatul Islam and Md․ Suman Reza},
  doi          = {10.1016/j.mlwa.2025.100619},
  journal      = {Machine Learning with Applications},
  month        = {3},
  pages        = {100619},
  shortjournal = {Mach. Learn. Appl.},
  title        = {Improving mango ripeness grading accuracy: A comprehensive analysis of deep learning, traditional machine learning, and transfer learning techniques},
  volume       = {19},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Predicting customer subscription in bank telemarketing campaigns using ensemble learning models. <em>MLA</em>, <em>19</em>, 100618. (<a href='https://doi.org/10.1016/j.mlwa.2025.100618'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This study investigates the use of ensemble learning models bagging, boosting, and stacking to enhance the accuracy and reliability of predicting customer subscriptions in bank telemarketing campaigns. Recognizing the challenges posed by class imbalance and complex customer behaviors, we employ multiple ensemble techniques to build a robust predictive framework. Our analysis demonstrates that stacking models achieve the best overall performance, with an accuracy of 91.88% and an Receiver Operating Characteristic Area Under the Curve (ROC-AUC) score of 0.9491, indicating a strong capability to differentiate between subscribers and non-subscribers. Additionally, feature importance analysis reveals that contact duration, economic indicators like the Euro interbank offered (Euribor) rate, and customer age are the most influential factors in predicting subscription likelihood. These findings suggest that by focusing on customer engagement and economic trends, banks can improve telemarketing campaign effectiveness. We recommend the integration of advanced balancing techniques and real-time prediction systems to further enhance model performance and adaptability. Future work could explore deep learning models and interpretability techniques to gain deeper insights into customer behavior patterns. Overall, this study highlights the potential of ensemble models in predictive modeling for telemarketing, providing a data-driven foundation for more targeted and efficient customer acquisition strategies.},
  archive      = {J_MLA},
  author       = {Michael Peter and Hawa Mofi and Said Likoko and Julius Sabas and Ramadhani Mbura and Neema Mduma},
  doi          = {10.1016/j.mlwa.2025.100618},
  journal      = {Machine Learning with Applications},
  month        = {3},
  pages        = {100618},
  shortjournal = {Mach. Learn. Appl.},
  title        = {Predicting customer subscription in bank telemarketing campaigns using ensemble learning models},
  volume       = {19},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). S&P-500 vs. nasdaq-100 price movement prediction with LSTM for different daily periods. <em>MLA</em>, <em>19</em>, 100617. (<a href='https://doi.org/10.1016/j.mlwa.2024.100617'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper explores the efficiency of LSTM neural networks in predicting price movements for the two major U.S. stock indices: the S&P-500 and the Nasdaq-100 index. We consider three distinct daily periods: “overnight” (Close-to-Open), “daytime” (Open-to-Close) and “24-hour” (Close-to-Close) trading sessions. Using historical pricing data for these indices since 2000, this study shows how well the standard LSTM model captures price movement patterns to improve short-term trading strategies. The findings reveal that, for the S&P-500, a one-year training with 24-hour periods delivers a 14.5% more return over the Buy-and-Hold strategy. Moreover, combining “overnight” and “daytime” strategies delivers more than 40% return compared to passive index investing. By contrast, for the Nasdaq-100, a shorter training period of three months for “24-hour” periods delivers 90% more return than passive index investing. These results suggest that LSTM effectively learns the unique market dynamics associated with each index and different time periods, offering further insights into how deep learning can enhance financial forecasting and trading opportunities.},
  archive      = {J_MLA},
  author       = {Xiang Zhang and Eugene Pinsky},
  doi          = {10.1016/j.mlwa.2024.100617},
  journal      = {Machine Learning with Applications},
  month        = {3},
  pages        = {100617},
  shortjournal = {Mach. Learn. Appl.},
  title        = {S&P-500 vs. nasdaq-100 price movement prediction with LSTM for different daily periods},
  volume       = {19},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Unified wound diagnostic framework for wound segmentation and classification. <em>MLA</em>, <em>19</em>, 100616. (<a href='https://doi.org/10.1016/j.mlwa.2024.100616'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Chronic wounds affect millions worldwide, posing significant challenges for healthcare systems and a heavy economic burden globally. The segmentation and classification (S&C) of chronic wounds are critical for wound care management and diagnosis, aiding clinicians in selecting appropriate treatments. Existing approaches have utilized either traditional machine learning or deep learning methods for S&C. However, most focus on binary classification, with few addressing multi-class classification, often showing degraded performance for pressure and diabetic wounds. Wound segmentation has been largely limited to foot ulcer images, and there is no unified diagnostic tool for both S&C tasks. To address these gaps, we developed a unified approach that performs S&C simultaneously. For segmentation, we proposed Attention-Dense-UNet (Att- d -UNet), and for classification, we introduced a feature concatenation-based method. Our framework segments wound images using Att- d -UNet, followed by classification into one of the wound types using our proposed method. We evaluated our models on publicly available wound classification datasets (AZH and Medetec) and segmentation datasets (FUSeg and AZH). To test our unified approach, we extended wound classification datasets by generating segmentation masks for Medetec and AZH images. The proposed unified approach achieved 90% accuracy and an 86.55% dice score on the Medetec dataset and 81% accuracy and an 86.53% dice score on the AZH dataset These results demonstrate the effectiveness of our separate models and unified approach for wound S&C.},
  archive      = {J_MLA},
  author       = {Mustafa Alhababi and Gregory Auner and Hafiz Malik and Muteb Aljasem and Zaid Aldoulah},
  doi          = {10.1016/j.mlwa.2024.100616},
  journal      = {Machine Learning with Applications},
  month        = {3},
  pages        = {100616},
  shortjournal = {Mach. Learn. Appl.},
  title        = {Unified wound diagnostic framework for wound segmentation and classification},
  volume       = {19},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Combinations of distributional regression algorithms with application in uncertainty estimation of corrected satellite precipitation products. <em>MLA</em>, <em>19</em>, 100615. (<a href='https://doi.org/10.1016/j.mlwa.2024.100615'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To facilitate effective decision-making, precipitation datasets should include uncertainty estimates. Quantile regression with machine learning has been proposed for issuing such estimates. Distributional regression offers distinct advantages over quantile regression, including the ability to model intermittency as well as a stronger ability to extrapolate beyond the training data, which is critical for predicting extreme precipitation. Therefore, here, we introduce the concept of distributional regression in precipitation dataset creation, specifically for the spatial prediction task of correcting satellite precipitation products. Building upon this concept, we formulated new ensemble learning methods that can be valuable not only for spatial prediction but also for other prediction problems. These methods exploit conditional zero-adjusted probability distributions estimated with generalized additive models for location, scale and shape (GAMLSS), spline-based GAMLSS and distributional regression forests as well as their ensembles (stacking based on quantile regression and equal-weight averaging). To identify the most effective methods for our specific problem, we compared them to benchmarks using a large, multi-source precipitation dataset. Stacking was shown to be superior to individual methods at most quantile levels when evaluated with the quantile loss function. Moreover, while the relative ranking of the methods varied across different quantile levels, stacking methods, and to a lesser extent mean combiners, exhibited lower variance in their performance across different quantiles compared to individual methods that occasionally ranked extremely low. Overall, a task-specific combination of multiple distributional regression algorithms could yield significant benefits in terms of stability.},
  archive      = {J_MLA},
  author       = {Georgia Papacharalampous and Hristos Tyralis and Nikolaos Doulamis and Anastasios Doulamis},
  doi          = {10.1016/j.mlwa.2024.100615},
  journal      = {Machine Learning with Applications},
  month        = {3},
  pages        = {100615},
  shortjournal = {Mach. Learn. Appl.},
  title        = {Combinations of distributional regression algorithms with application in uncertainty estimation of corrected satellite precipitation products},
  volume       = {19},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). The association between mindfulness, psychological flexibility, and rumination in predicting mental health and well-being among university students using machine learning and structural equation modeling. <em>MLA</em>, <em>19</em>, 100614. (<a href='https://doi.org/10.1016/j.mlwa.2024.100614'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Objectives This study explores the intricate relationships between mindfulness, psychological flexibility, rumination, and their combined impact on mental health and well-being. Methods Random forest regression on survey data from 524 undergraduate students was used to identify significant predictors from a comprehensive set of psychological variables. Neural networks were then trained on various combinations of these predictors to evaluate their performance in predicting mental health and well-being outcomes. Finally, structural equation modeling (SEM) was employed to validate a model based on the identified key predictors, focusing on pathways from mindfulness through psychological flexibility to rumination and well-being. Results The random forest analysis revealed that the mindfulness variables exerted their influence partially indirectly through psychological flexibility and rumination. The deep neural network analysis supported these findings and additionally showed that the mindfulness manifold model (consisting of self-awareness, self-regulation, and self-transcendence) was superior to the Five Facet Mindfulness Questionnaire variables in predicting mental health outcomes. The SEM analysis confirmed that psychological flexibility, particularly its avoidance and acceptance components, mediated the relationship between mindfulness and mental health. The hypothesized serial mediation pathway—mindfulness affecting psychological flexibility, which then influences rumination and subsequently mental health and well-being—was supported by the data. Self-transcendence was a particularly powerful predictor of mental health outcomes. Conclusions The findings underscore the critical role of psychological flexibility and rumination in mediating the effects of mindfulness on mental health and well-being, suggesting that enhancing mindfulness and psychological flexibility might significantly reduce rumination, thereby improving overall mental health and well-being.},
  archive      = {J_MLA},
  author       = {Ruohan Feng and Vaibhav Mishra and Xin Hao and Paul Verhaeghen},
  doi          = {10.1016/j.mlwa.2024.100614},
  journal      = {Machine Learning with Applications},
  month        = {3},
  pages        = {100614},
  shortjournal = {Mach. Learn. Appl.},
  title        = {The association between mindfulness, psychological flexibility, and rumination in predicting mental health and well-being among university students using machine learning and structural equation modeling},
  volume       = {19},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Stairway to heaven: An emotional journey in divina commedia with threshold-based naïve bayes classifier. <em>MLA</em>, <em>19</em>, 100613. (<a href='https://doi.org/10.1016/j.mlwa.2024.100613'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Computational literary uses data science and computer science techniques to study literature. In this framework, we investigate how an expert system can acquire knowledge from the specific content of a narrative text without any pre-existing information about it. We utilize the Threshold-based Naïve Bayes (Tb-NB) classifier to analyze the content of Dante Alighieri’s Divina Commedia poem. Tb-NB is a probabilistic data-driven model that predicts the polarity of a binary response based on the probability of an event occurring given certain features, and assigns a log-likelihood score to each word in a text. Our first task is understanding if and how the links between lexical forms and meanings characterize the three parts of the poem (Inferno, Purgatorio and Paradiso) in order to predict if a Canto belongs to Inferno or Paradiso based on its specific content, and to determine if a Canto of Purgatorio is more similar to those of Inferno or to those of Paradiso. We show Tb-NB outperform other similar approaches and achieves the same performance of Random Forest (F1-score = 0.985) but providing much more information to interpret the specific content and the lexical forms used by Dante Alighieri in its poem. The Tb-NB’s scores are the base of knowledge for the implementation of an expert system, like a search engine, that can help users to identify the most informative verses of a Canto or by better comprehend or discover the content of the poem from a word related to a particular feeling or emotion.},
  archive      = {J_MLA},
  author       = {Maurizio Romano and Claudio Conversano},
  doi          = {10.1016/j.mlwa.2024.100613},
  journal      = {Machine Learning with Applications},
  month        = {3},
  pages        = {100613},
  shortjournal = {Mach. Learn. Appl.},
  title        = {Stairway to heaven: An emotional journey in divina commedia with threshold-based naïve bayes classifier},
  volume       = {19},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Adaptive gate residual connection and multi-scale RCNN for fake news detection. <em>MLA</em>, <em>19</em>, 100612. (<a href='https://doi.org/10.1016/j.mlwa.2024.100612'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Detection of false news based on text classification technology has significant research significance and practical value in the current information age. However, existing methods overlook the problem of uneven sample distribution in the false news dataset and fail to consider the mutual influence between news articles. In light of this, this paper proposes a new method for false news detection. Firstly, news texts are embedded using Electra (Efficiently Learning an Encoder that Classifies Token Replacements Accurately) to obtain word embedding representations. Secondly, Multi-Scale Recurrent Convolutional Neural Network (RCNN) is employed to further extract contextual information from news texts. Self-attention is introduced to calculate attention scores between news articles, allowing for mutual influence between news features. The establishment of connections between modules is achieved through adaptive gated residual connections. Finally, the focal loss function is used to balance the relationship between few-sample and multi-sample data in the dataset. Experimental results on publicly available false news detection datasets demonstrate that the proposed method achieves higher prediction accuracy than the comparative methods. This method provides a new perspective for the field of false news detection, playing a positive role in promoting information authenticity and protecting public interests.},
  archive      = {J_MLA},
  author       = {QunHui Zhou and Tijian Cai},
  doi          = {10.1016/j.mlwa.2024.100612},
  journal      = {Machine Learning with Applications},
  month        = {3},
  pages        = {100612},
  shortjournal = {Mach. Learn. Appl.},
  title        = {Adaptive gate residual connection and multi-scale RCNN for fake news detection},
  volume       = {19},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Practical classification accuracy of sequential data using neural networks. <em>MLA</em>, <em>19</em>, 100611. (<a href='https://doi.org/10.1016/j.mlwa.2024.100611'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Many existing studies on neural network accuracy utilize datasets that may not always reflect real-world conditions. While it has been demonstrated that accuracy tends to decrease as the number of benign samples increases, this effect has not been quantitatively assessed within neural networks. Moreover, its relevance to security tasks beyond malware classification remains unexplored. In this research, we refined the metric to evaluate the degradation of accuracy with an increased number of benign samples in test data. Utilizing both standard and specific neural network models, we conducted experiments to adapt this metric to neural networks and various feature extraction techniques. Using the FFRI dataset, comprising 150,000 malware and 400,000 benign samples, along with the URL dataset, containing 3143 malicious and 106,545,781 benign samples, we increased benign samples in the test set while keeping the training set’s malicious and benign samples constant. Our findings indicate that neural networks can indeed overestimate their accuracy with a smaller count of benign samples. Importantly, our refined metric is not only applicable to neural networks but is also effective for other feature extraction methods and security tasks beyond malware detection.},
  archive      = {J_MLA},
  author       = {Mamoru Mimura},
  doi          = {10.1016/j.mlwa.2024.100611},
  journal      = {Machine Learning with Applications},
  month        = {3},
  pages        = {100611},
  shortjournal = {Mach. Learn. Appl.},
  title        = {Practical classification accuracy of sequential data using neural networks},
  volume       = {19},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Demographic bias mitigation at test-time using uncertainty estimation and human–machine partnership. <em>MLA</em>, <em>19</em>, 100610. (<a href='https://doi.org/10.1016/j.mlwa.2024.100610'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Facial attribute classification algorithms frequently manifest demographic biases by obtaining differential performance across gender and racial groups. Existing bias mitigation techniques are mostly in-processing techniques, i.e., implemented during the classifier’s training stage, that often lack generalizability, require demographically annotated training sets, and exhibit a trade-off between fairness and classification accuracy. In this paper, we propose a technique to mitigate bias at the test time i.e., during the deployment stage, by harnessing prediction uncertainty and human–machine partnership. To this front, we propose to utilize those lowest percentages of test data samples identified as outliers with high prediction uncertainty. These identified uncertain samples at test-time are labeled by human analysts for decision rendering and for subsequently re-training the deep neural network in a continual learning framework. With minimal human involvement and through iterative refinement of the network with human guidance at test-time, we seek to enhance the accuracy as well as the fairness of the already deployed facial attribute classification algorithms. Extensive experiments are conducted on gender and smile attribute classification tasks using four publicly available datasets and with gender and race as the protected attributes. The obtained outcomes consistently demonstrate improved accuracy by up to 2% and 5% for the gender and smile attribute classification tasks, respectively, using our proposed approaches. Further, the demographic bias was significantly reduced, outperforming the State-of-the-Art (SOTA) bias mitigation and baseline techniques by up to 55% for both classification tasks. The demo shall be released on https://github.com/hashtaglensman/HumanintheLoop .},
  archive      = {J_MLA},
  author       = {Anoop Krishnan Upendran Nair and Ajita Rattani},
  doi          = {10.1016/j.mlwa.2024.100610},
  journal      = {Machine Learning with Applications},
  month        = {3},
  pages        = {100610},
  shortjournal = {Mach. Learn. Appl.},
  title        = {Demographic bias mitigation at test-time using uncertainty estimation and human–machine partnership},
  volume       = {19},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Application of convolutional neural networks and ensemble methods in the fiber volume content analysis of natural fiber composites. <em>MLA</em>, <em>19</em>, 100609. (<a href='https://doi.org/10.1016/j.mlwa.2024.100609'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The incorporation of natural fibers into fiber-reinforced polymer composites (FRPC) has the potential to bolster their sustainability. A critical attribute of FRPC is the fiber volume content ( FVC ), a parameter that profoundly influences their thermo-mechanical characteristics. However, the determination of FVC in natural fiber composites (NFC) through manual analysis of light microscopy images is a labor-intensive process. In this work, it is demonstrated that the pixels from light microscopy images of NFC can be utilized to predict FVC using machine learning (ML) models. In this proof-of-concept investigation, it is shown that convolutional neural network-based models predict FVC with an accuracy required in polymer engineering applications, with a mean average error of 2.72 % and an R 2 coefficient of 0.85. Finally, it is shown that much simpler ML models, non-specialized in image recognition, besides being much easier and more efficient to optimize and train, can also deliver good accuracies required for FVC characterization, which not only contributes to the sustainability, but also facilitates the access of such models by researchers in regions with little computational resources. This study marks a substantial advancement in the area of automated characterization of NFC, and democratization of knowledge, offering a promising avenue for the enhancement of sustainable materials.},
  archive      = {J_MLA},
  author       = {Florian Rothenhäusler and Rodrigo Queiroz Albuquerque and Marcel Sticher and Christopher Kuenneth and Holger Ruckdaeschel},
  doi          = {10.1016/j.mlwa.2024.100609},
  journal      = {Machine Learning with Applications},
  month        = {3},
  pages        = {100609},
  shortjournal = {Mach. Learn. Appl.},
  title        = {Application of convolutional neural networks and ensemble methods in the fiber volume content analysis of natural fiber composites},
  volume       = {19},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Detection of fraud in IoT based credit card collected dataset using machine learning. <em>MLA</em>, <em>19</em>, 100603. (<a href='https://doi.org/10.1016/j.mlwa.2024.100603'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Due in large part to the proliferation of electronic financial transactions, credit card fraud is a serious problem for customers, merchants, and banks. For this reason, a novel approach is offered to fraud detection that makes use of cutting-edge ML methods in an IoT setting. The method in this paper employs a carefully selected set of cutting-edge ML algorithms specifically designed to handle the complexities of fraud detection, in contrast to older approaches that have difficulty adapting to shifting fraud patterns. In order to address the many facets of the problem, the methodology employs a large collection of ML models. These models include deep neural networks, decision trees, support vector machines, random forests, and clustering methods. This paper provides a solution that is able to detect fraudulent activity in real time by efficiently analyzing massive amounts of transactional data thanks to the power of big data processing and cloud computing. The model is able to distinguish between valid and fraudulent transactions thanks to careful feature engineering and anomaly detection methods. Extensive experiments on a large and diverse collection of real and simulated credit card transactions, both legitimate and fraudulent, prove the success of this technique. The findings demonstrate state-of-the-art performance in fraud detection, with increased precision and recall rates compared to traditional methods. And because the presented ML models are easy to understand, they improve fraud risk management and prevention techniques. The findings of this study provide banking institutions, government agencies, and policymakers with vital information for combating the negative effects of credit card fraud on consumers, companies, and the economy as a whole. This study provides a solution to the problem of fraud in the Internet of Things (IoT) ecosystem and paves the way for future developments in this crucial area by proposing a unique ML-driven approach to the problem.},
  archive      = {J_MLA},
  author       = {Mohammed Naif Alatawi},
  doi          = {10.1016/j.mlwa.2024.100603},
  journal      = {Machine Learning with Applications},
  month        = {3},
  pages        = {100603},
  shortjournal = {Mach. Learn. Appl.},
  title        = {Detection of fraud in IoT based credit card collected dataset using machine learning},
  volume       = {19},
  year         = {2025},
}
</textarea>
</details></li>
</ul>

</body>
</html>
