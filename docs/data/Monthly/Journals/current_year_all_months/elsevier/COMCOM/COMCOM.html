<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>COMCOM</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="comcom">COMCOM - 217</h2>
<ul>
<li><details>
<summary>
(2025). Revisiting the problem of optimizing spreading factor allocations in LoRaWAN: From theory to practice. <em>COMCOM</em>, <em>243</em>, 108321. (<a href='https://doi.org/10.1016/j.comcom.2025.108321'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper revisits the problem of optimizing LoRa network success probability by proposing an optimized allocation strategy for Spreading Factors (SFs) under both uniform and Gaussian network deployments with a single or multiple gateways. More specifically, we solve the problem of finding the best SF allocations in dense network deployments whose EDs are first assigned with the minimum SF. Theoretical models are developed to quantify the success probability of transmissions, considering the capture effect as well as intra- and inter-SF interference. A mathematical optimization framework is introduced to determine the optimal SF distribution that maximizes the average probability of packet reception. The problem is solved using Mixed Integer Linear Programming (MILP), and then evaluated using simulations. Even though optimal SF allocation strategies have been proposed in the literature, no practical insights have been discovered and no real-world deployments have been considered. To this extent, the practical benefits of using improved or optimal SF settings are discovered in this paper. Simulation results confirm the theoretical findings while they demonstrate an up to 10 percentage points improvements in Packet Reception Ratio (PRR) in the real-world use-case.},
  archive      = {J_COMCOM},
  author       = {Dimitrios Zorbas and Aruzhan Sabyrbek and Luigi Di Puglia Pugliese},
  doi          = {10.1016/j.comcom.2025.108321},
  journal      = {Computer Communications},
  month        = {11},
  pages        = {108321},
  shortjournal = {Comput. Commun.},
  title        = {Revisiting the problem of optimizing spreading factor allocations in LoRaWAN: From theory to practice},
  volume       = {243},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Energy consumption optimization in UAV-assisted multi-layer mobile edge computing with active transmissive RIS. <em>COMCOM</em>, <em>243</em>, 108320. (<a href='https://doi.org/10.1016/j.comcom.2025.108320'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Unmanned Aerial Vehicle (UAV)-assisted edge computing provides low-latency and low-energy consumption computing capabilities for sparsely distributed Internet of Things (IoT) networks. In addition, the assisted UAVs provide line-of-sight links to further improve communication quality. However, the existing offloading strategies have low efficiency and high costs. Motivated by this, we propose a novel UAV-assisted multi-layer mobile edge computing network with active transmissive reconfigurable intelligent surface (RIS). The introduced an active transmissive RIS not only receives data from UAVs but also performs computing functionality. We establish an optimization to minimize the total system energy consumption under delay constraints by jointly planning UAV positions and allocating computing bits, sub-carriers, time slots, transmission power, and RIS transmission coefficient. To tackle this problem, we first use the block coordinate descent (BCD) algorithm to decouple it into four sub-problems. Then, we solve them by adopting successive convex approximation (SCA), difference-convex (DC) programming, and introducing slack variables. Experimental results demonstrate that the proposed network is superior to the other five baselines concerning energy consumption reduction. Also, the influences of system parameters are verified, including the number of IoT devices, the number of RIS elements, and the delay threshold.},
  archive      = {J_COMCOM},
  author       = {Kexin Yang and Yaxi Liu and Boxin He and Jiahao Huo and Wei Huangfu},
  doi          = {10.1016/j.comcom.2025.108320},
  journal      = {Computer Communications},
  month        = {11},
  pages        = {108320},
  shortjournal = {Comput. Commun.},
  title        = {Energy consumption optimization in UAV-assisted multi-layer mobile edge computing with active transmissive RIS},
  volume       = {243},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A preemptive task unloading scheme based on second optional unloading in cloud-fog collaborative networks. <em>COMCOM</em>, <em>243</em>, 108315. (<a href='https://doi.org/10.1016/j.comcom.2025.108315'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Long-distance data transmission between Internet of Things (IoT) devices and remote cloud center often leads to unacceptable latency for certain tasks. Fog computing has emerged as a promising solution for low-latency tasks. Consequently, the concept of cloud-fog collaborative networks has garnered significant attention. However, existing research primarily focuses on heterogeneous tasks, overlooking the crucial aspect of considering both task priority and second unloading. To address this gap, this paper proposes a novel task unloading scheme that concurrently takes preemptive priority and second optional unloading into account. In this scheme, delay-sensitive tasks (DSTs) are given preemptive priority over delay-tolerant tasks (DTTs). Furthermore, some DTTs may undergo preprocessing in the fog layer to optimize resource utilization. Moreover, tasks encountering blocking or preemption in the fog layer can also be secondarily unloaded to the cloud layer. In this framework, we devise a four-dimensional Markov chain (4DMC) to model and analyze this process. Through numerical experiments, we assess performance indicators under various parameters. Ultimately, our proposed strategy is compared with the unloading scheme that does not incorporate second unloading through both numerical analysis and simulation validation. The results indicate that our scheme notably enhances the throughput of DTTs, albeit at a marginal performance trade-off.},
  archive      = {J_COMCOM},
  author       = {Yuan Zhao and Hongmin Gao and Shuaihua Liu},
  doi          = {10.1016/j.comcom.2025.108315},
  journal      = {Computer Communications},
  month        = {11},
  pages        = {108315},
  shortjournal = {Comput. Commun.},
  title        = {A preemptive task unloading scheme based on second optional unloading in cloud-fog collaborative networks},
  volume       = {243},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). ScaleIP: A hybrid autoscaling of VoIP services based on deep reinforcement learning. <em>COMCOM</em>, <em>243</em>, 108314. (<a href='https://doi.org/10.1016/j.comcom.2025.108314'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Adaptive resource provisioning has become crucial for cloud-based applications, especially those managing real-time traffic like Voice over IP (VoIP), which experience rapidly fluctuating workloads. Traditional static provisioning methods often fall short in these dynamic environments, leading to inefficiencies and potential service disruptions. Existing solutions struggle to maintain performance under varying traffic conditions, particularly for time-sensitive applications. This paper introduces ScaleIP, a hybrid autoscaling solution for containerized VoIP services that offers real-time adaptability and efficient resource management. ScaleIP leverages Deep Reinforcement Learning to make dynamic and efficient scaling decisions, improving call latency, increasing the number of successfully routed calls, and maximizing resource utilization. We evaluated ScaleIP through extensive experiments conducted on a real testbed utilizing the customer Call Detail Record (CDR) from 2023 provided by World Direct, encompassing over 89 million calls. The results show that ScaleIP consistently maintains call latency below 2 s, increases the number of successfully routed calls by 3.26 ×, and increases the resource utilization up to 60 % compared to state-of-the-art autoscaling methods.},
  archive      = {J_COMCOM},
  author       = {Zahra Najafabadi Samani and Juan Aznar Poveda and Dominik Gratz and Rene Hueber and Philipp Kalb and Thomas Fahringer},
  doi          = {10.1016/j.comcom.2025.108314},
  journal      = {Computer Communications},
  month        = {11},
  pages        = {108314},
  shortjournal = {Comput. Commun.},
  title        = {ScaleIP: A hybrid autoscaling of VoIP services based on deep reinforcement learning},
  volume       = {243},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Beyond performance comparing the costs of applying deep and shallow learning. <em>COMCOM</em>, <em>243</em>, 108312. (<a href='https://doi.org/10.1016/j.comcom.2025.108312'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The rapid growth of mobile network traffic and the emergence of complex applications, such as self-driving cars and augmented reality, demand ultra-low latency, high throughput, and massive device connectivity, which traditional network design approaches struggle to meet. These issues were initially addressed in Fifth-Generation (5G) and Beyond-5G (B5G) networks, where Artificial Intelligence (AI), particularly Deep Learning (DL), is proposed to optimize the network and to meet these demanding requirements. However, the resource constraints and time limitations inherent in telecommunication networks raise questions about the practicality of deploying large Deep Neural Networks (DNNs) in these contexts. This paper analyzes the costs of implementing DNNs by comparing them with shallow ML models across multiple datasets and evaluating factors such as execution time and model interpretability. Our findings demonstrate that shallow ML models offer comparable performance to DNNs, with significantly reduced training and inference times, achieving up to 90% acceleration. Moreover, shallow models are more interpretable, as explainability metrics struggle to agree on feature importance values even for high-performing DNNs.},
  archive      = {J_COMCOM},
  author       = {Rafael Teixeira and Leonardo Almeida and Pedro Rodrigues and Mário Antunes and Diogo Gomes and Rui L. Aguiar},
  doi          = {10.1016/j.comcom.2025.108312},
  journal      = {Computer Communications},
  month        = {11},
  pages        = {108312},
  shortjournal = {Comput. Commun.},
  title        = {Beyond performance comparing the costs of applying deep and shallow learning},
  volume       = {243},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A scalable blockchain framework for IoT based on restaking and incentive mechanisms. <em>COMCOM</em>, <em>242</em>, 108317. (<a href='https://doi.org/10.1016/j.comcom.2025.108317'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper proposes a scalable blockchain framework based on sidechain solution for the Internet of Things (IoT). Considering the low-trust models of existing sidechains, we present a restaking-based trust aggregation method for Proof of Stake (PoS). By allowing mainchain validators to duplicate their stake on the sidechain network, we enhance the cryptoeconomics security of the sidechain while reducing costs. Given the potential conflicts between risks and rewards of trust aggregation, and the challenges posed by the heterogeneity of IoT devices for quantitative analysis, we propose an incentive analysis framework based on contract. By analyzing the optimal strategies of different risk-preference validators, design differentiated contracts to promote incentive-compatible outcomes. Additionally, we account for the uncertainty in the distribution of sidechain validators and discuss optimal configurations under various conditions. To address potential collusion attacks, we introduce a quantifiable exemption mechanism to limit the security risks. Finally, numerical simulations verify the feasibility and effectiveness of our proposed method.},
  archive      = {J_COMCOM},
  author       = {Fang Ye and Zitao Zhou and Yifan Wang and Yibing Li},
  doi          = {10.1016/j.comcom.2025.108317},
  journal      = {Computer Communications},
  month        = {10},
  pages        = {108317},
  shortjournal = {Comput. Commun.},
  title        = {A scalable blockchain framework for IoT based on restaking and incentive mechanisms},
  volume       = {242},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Proactive retention-aware online video caching scheme in mobile edge computing. <em>COMCOM</em>, <em>242</em>, 108313. (<a href='https://doi.org/10.1016/j.comcom.2025.108313'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The current massive video requests have caused severe network congestion. To reduce transmission latency and improve user Quality of Experience (QoE), caching infrastructures are deployed closer to the edge. Nowadays, most caching systems tend to cache content with a high programming voltage to ensure a long retention time, which leads to significant cache damage. However, as new videos emerge every second, the rapidly changing popularity makes long retention time wasteful in terms of caching resource. Moreover, with the rise of emerging video formats (such as virtual reality content), the diverse requirements for transmission latency across various video categories make balancing user QoE more challenging. To tackle these challenges, we propose a joint optimization framework that balances user QoE and operational costs through video category recognition and adaptive retention time selection. First, we model user QoE as transmission latency cost and further formulate the optimization problem as a Markov Decision Process (MDP) to minimize the system cost. To solve the proposed problem, we design a two-step Double Deep Q-Network (DDQN)-based scheme. The scheme first determines the optimal retention time through unifying the process of action selection and state-value evaluation. Secondly, it makes replacement decisions according to the computed caching value of each content. By validating on three datasets, the experiments show that the proposed scheme outperforms the baseline algorithms in both cache hit rate and system cost.},
  archive      = {J_COMCOM},
  author       = {Guangzhou Liu and Zhen Qian and Guanghui Li},
  doi          = {10.1016/j.comcom.2025.108313},
  journal      = {Computer Communications},
  month        = {10},
  pages        = {108313},
  shortjournal = {Comput. Commun.},
  title        = {Proactive retention-aware online video caching scheme in mobile edge computing},
  volume       = {242},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Trust-defined network: A panoramic P2P framework for distributed ledger systems. <em>COMCOM</em>, <em>242</em>, 108311. (<a href='https://doi.org/10.1016/j.comcom.2025.108311'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Blockchain technology has revolutionized distributed ledger systems by offering superior security and transparency compared to traditional centralized systems. Despite its advantages, current blockchain systems face significant challenges such as network congestion, communication errors, and scalability issues, largely due to the limitations of blockchain peer-to-peer (P2P) protocols. These problems hinder the performance, reliability, and widespread adoption of blockchain technology. In this paper, we propose a Trust-Defined Network (TDN) framework designed to solve these challenges by reflecting the physical network information to the blockchain. This approach enables the precise diagnosis of existing blockchain P2P protocol limitations and facilitates the objective verification of new improvement measures. Our proposed framework supports various blockchain network environments, particularly Ethereum-based networks, and ensures enhanced network stability and performance. Through extensive simulations and real-world case studies in IoT-enabled blockchain applications, we demonstrate that TDN significantly reduces network congestion, improves transaction finality, and enhances the reliability of blockchain communication channels. These findings highlight the framework’s potential to optimize blockchain infrastructure, making it more robust for large-scale deployment and real-world applications.},
  archive      = {J_COMCOM},
  author       = {Taehoon Yoo and Kiseok Kim and Hwangnam Kim},
  doi          = {10.1016/j.comcom.2025.108311},
  journal      = {Computer Communications},
  month        = {10},
  pages        = {108311},
  shortjournal = {Comput. Commun.},
  title        = {Trust-defined network: A panoramic P2P framework for distributed ledger systems},
  volume       = {242},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Meta-reinforcement learning driven model architecture and algorithm optimization in intelligent driving task offloading. <em>COMCOM</em>, <em>242</em>, 108310. (<a href='https://doi.org/10.1016/j.comcom.2025.108310'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the process of rapid development of intelligent driving technology, the amount of data generated by vehicles increases dramatically, while the bottleneck of storage and computation capacity of in-vehicle devices becomes more and more prominent, and task offloading becomes the key to improve the performance of intelligent driving systems. In this context, this paper proposes the MRL-ADTO algorithm, which innovatively applies meta-reinforcement learning (MRL) to the field of intelligent driving task offloading, optimizes the directed acyclic graph (DAG) synthesis logic and the task priority ranking algorithm, designs a neural network model based on the sequence to sequence (Seq2Seq) structure, and introduces the mechanism of multi-head attention at the same time. The experimental results show that MRL-ADTO can significantly reduce the task execution delay in multiple scenarios compared with the existing algorithms, and has obvious advantages in terms of training efficiency and convergence performance, providing an efficient and reliable solution for smart driving task offloading.},
  archive      = {J_COMCOM},
  author       = {Peiying Zhang and Jiamin Liu and Zhiyuan Ren and Lizhuang Tan and Neeraj Kumar and Konstantin Igorevich Kostromitin},
  doi          = {10.1016/j.comcom.2025.108310},
  journal      = {Computer Communications},
  month        = {10},
  pages        = {108310},
  shortjournal = {Comput. Commun.},
  title        = {Meta-reinforcement learning driven model architecture and algorithm optimization in intelligent driving task offloading},
  volume       = {242},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Distributed learning-based context-aware SFC deployment in the artificial intelligence of things. <em>COMCOM</em>, <em>242</em>, 108309. (<a href='https://doi.org/10.1016/j.comcom.2025.108309'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the rapid development of the Internet of Things (IoT) and Artificial Intelligence (AI) technologies, the Artificial Intelligence of Things (AIoT) has become a key driving force for realizing intelligent and automated applications. The deployment of Service Function Chains (SFCs) is crucial in dynamic AIoT environments, where efficiently and flexibly deploying SFCs to meet real-time application demands is a research focus. However, existing SFC deployment methods often face challenges such as dynamic variations and uncertainty in contextual information, resource allocation inefficiencies, and limited adaptability to changing network conditions. To address these issues, we propose a learning-based context-aware dynamic SFC deployment method tailored for AIoT environments. Specifically, we introduce an attention-based contextual feature extraction method to capture dynamic changes (e.g., link latency variations) and prioritize key contextual information, improving the rate of served requests by 17.90% (69.60% vs. 59.03% for MADDPG) and enhancing the flexibility of SFC deployment decisions. Additionally, to address resource allocation bottlenecks and adaptability challenges in SFC deployment, we propose a distributed learning-based context-aware approach that uses collaborative learning and periodic updates (every 200 ms) to adjust SFC deployment strategies in response to topology changes and load variations and optimize system performance. Extensive experimental results demonstrate the efficacy of the proposed algorithm. Numerical results demonstrate that our algorithm reduces SFC deployment latency by 8% (46 ms vs. 50 ms for MADDPG), achieves 98.3% computational resource utilization, processes 211 Mbit/s service data volume, and improves adaptability to network changes, as validated in simulations.},
  archive      = {J_COMCOM},
  author       = {Wenlin Cheng and Xingwei Wang and Fuliang Li and Bo Yi and Qiang He and Chuangchuang Zhang and Chengxi Gao and Min Huang},
  doi          = {10.1016/j.comcom.2025.108309},
  journal      = {Computer Communications},
  month        = {10},
  pages        = {108309},
  shortjournal = {Comput. Commun.},
  title        = {Distributed learning-based context-aware SFC deployment in the artificial intelligence of things},
  volume       = {242},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A multi-feature fusion approach for physical layer authentication in LEO satellites. <em>COMCOM</em>, <em>242</em>, 108308. (<a href='https://doi.org/10.1016/j.comcom.2025.108308'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Spatial information networks (SINs) have emerged as a means to enhance the expanse and dependability of communication and data transmission services. SINs rely on satellite systems to provide these services, among which low earth orbit (LEO) satellites are widely concerned because of their advantages of low orbital altitude, small network transmission delay, small path loss, and high signal strength. However, due to the frequent switching of communication links between LEO satellites and the ground, the authentication mechanism of the ground users to the satellites is vulnerable to spoofing attacks, and the traditional upper layer authentication method based on encryption usually requires a lot of overhead and delay. In this case, the lightweight physical layer authentication (PLA) mechanism utilizes the inherent distinctiveness and unpredictable nature of channel physical properties, serving as a vital application in SINs for ensuring authentication. Therefore, our work introduces a PLA method incorporating multi-feature integration, aimed at delivering effective identity verification tailored for LEO satellites. The approach employs doppler frequency shift (DS), angles of arrival (AOAs), and received power (RP) features, fusing an support vector machine (SVM) classifier, to distinguish between legal and illegal satellites in different simulation scenarios. The satellite toolkit (STK) is used to collect data from the actual orbit of satellites and assess the efficacy of the scheme. The findings indicate that the scheme offers enhanced authentication capabilities.},
  archive      = {J_COMCOM},
  author       = {Rongjun Yan and Fan Jia},
  doi          = {10.1016/j.comcom.2025.108308},
  journal      = {Computer Communications},
  month        = {10},
  pages        = {108308},
  shortjournal = {Comput. Commun.},
  title        = {A multi-feature fusion approach for physical layer authentication in LEO satellites},
  volume       = {242},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Integrating blockchain with IoT: Evaluating the feasibility of lightweight bitcoin wallets on resource-constrained devices. <em>COMCOM</em>, <em>242</em>, 108300. (<a href='https://doi.org/10.1016/j.comcom.2025.108300'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The integration of blockchain technology with IoT architectures holds immense potential for advancing application design and enhancing security properties. However, the resource constraints typically present in IoT devices pose a challenge. This paper explores the feasibility of running a lightweight Bitcoin wallet on IoT devices and identifies the minimum requirements for their successful operation. A review of the literature is used to identify existing integration architectures and derive the wallet needs. The study evaluates performance metrics such as execution time, memory usage, network data transmission, and power consumption to determine the feasibility of deploying these architectures.},
  archive      = {J_COMCOM},
  author       = {Mohsen Rahmanikivi and Cristina Pérez-Solà and Víctor Garcia-Font},
  doi          = {10.1016/j.comcom.2025.108300},
  journal      = {Computer Communications},
  month        = {10},
  pages        = {108300},
  shortjournal = {Comput. Commun.},
  title        = {Integrating blockchain with IoT: Evaluating the feasibility of lightweight bitcoin wallets on resource-constrained devices},
  volume       = {242},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Source-rate planning in self-powered wireless multi-hop D2D settings under stochasticity: A scenario-based iterative optimization approach. <em>COMCOM</em>, <em>242</em>, 108299. (<a href='https://doi.org/10.1016/j.comcom.2025.108299'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-hop Device-to-Device (D2D) communications are emerging as the foundation for numerous compelling 6G applications, enabling seamless information flow between distributed nodes. In the context of such uncertain wireless multi-hop D2D settings, jointly optimizing source data rates, routing, and transmission power decisions is both an essential task and a highly complex problem, particularly due to uncertainties introduced by the wireless channel states and the energy harvesting processes on the nodes. In the current literature, this problem is mostly tackled in a future agnostic sense, and/or using specific distributions to model the uncertainties. In contrast, in this paper, we compute a future energy and resource allocation plan of the network’s operation, using scenario-based optimization techniques to account for stochasticities. Scenarios can model generic distributions of uncertain quantities in a tractable manner. The formulated problem is inherently non-convex and to solve it, we propose CoNetPlan-E, a heuristic iterative method that at each iteration solves appropriately parameterized convex approximations of the original problem. We prove that CoNetPlan-E converges under realistic assumptions, while ensuring that the obtained solution at convergence is feasible for the original non-convex problem. Numerical evaluations showcase the effectiveness of the proposed method compared to existing baseline solutions, while considering three levels of increasing network topology complexity. Importantly, CoNetPlan-E is superior with respect to scalability and runtime while leading to close-to-optimal solutions as these are determined by the standard non-convex solver Ipopt.},
  archive      = {J_COMCOM},
  author       = {Georgia Stavropoulou and Eleni Stai and Maria Diamanti and Symeon Papavassiliou},
  doi          = {10.1016/j.comcom.2025.108299},
  journal      = {Computer Communications},
  month        = {10},
  pages        = {108299},
  shortjournal = {Comput. Commun.},
  title        = {Source-rate planning in self-powered wireless multi-hop D2D settings under stochasticity: A scenario-based iterative optimization approach},
  volume       = {242},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Distributed denial of service attack analysis and mitigation for MQTTv5 shared subscription. <em>COMCOM</em>, <em>242</em>, 108298. (<a href='https://doi.org/10.1016/j.comcom.2025.108298'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Sixth-generation (6G) networks will feature Massive IoT (M-IoT) deployments with a huge number of interconnected devices, enabling fast and reliable IoT applications. To address scalability and enhance message delivery, MQTTv5 introduces the shared subscription mechanism. However, the increased interconnectivity amplifies security vulnerabilities, posing significant risks with potentially severe consequences. In light of these challenges, this work aims to conduct a security-focused analysis of the shared subscription feature. Our study highlights the potential extent of damage from such an attack, which can potentially lead to indefinite starvation among legacy subscribers, and proposes a countermeasure to mitigate its impact. Additionally, to provide comprehensive security to the proposed mitigation mechanism, we design an Authenticated Encryption with Associated Data (AEAD)-based protection to counteract external malicious entities as well as an attacker detection mechanism based on a trust-based approach combined with the z-score statistical method to protect the proposed mitigation against internal attackers. These countermeasures are designed to accommodate the lightweight nature of MQTT and are characterized by a low protocol footprint while effectively mitigating the impact of the attack. Through an extensive experimental campaign, we tested this solution under real IoT traffic patterns to demonstrate its effectiveness and capability to restore the performance of the MQTT system targeted by the discovered attack.},
  archive      = {J_COMCOM},
  author       = {Graziano Rizzo and Mattia Giovanni Spina and Floriano De Rango},
  doi          = {10.1016/j.comcom.2025.108298},
  journal      = {Computer Communications},
  month        = {10},
  pages        = {108298},
  shortjournal = {Comput. Commun.},
  title        = {Distributed denial of service attack analysis and mitigation for MQTTv5 shared subscription},
  volume       = {242},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Applying reinforcement learning in slotted LoRaWAN: From concept to implementation. <em>COMCOM</em>, <em>242</em>, 108297. (<a href='https://doi.org/10.1016/j.comcom.2025.108297'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As Low Power Wide Area Networks (LPWANs) are increasingly adopted for Internet of Things (IoT) applications, they face significant challenges related to interference and scalability, which can lead to high collision rates and reduced network throughput. This paper presents a novel approach to enhancing the performance of LoRaWAN, one of the dominant LPWAN protocols, by leveraging Reinforcement Learning (RL). The proposed solution introduces a synchronization framework designed to operate under LoRaWAN principles, coupled with a low-cost, on-device RL mechanism that autonomously mitigates collisions. Through extensive simulations and real-world experiments, the effectiveness of the RL approach is demonstrated, showing an over 30% improvement in terms of packet delivery ratio (PDR) compared to traditional multiple access methods such as Pure-Aloha, Slotted-Aloha, and Carrier Sense Multiple Access (CSMA). Additionally, open-source implementations for both simulation and experimental validation are provided, ensuring reproducibility and facilitating further research in this domain.},
  archive      = {J_COMCOM},
  author       = {Dimitrios Zorbas and Sultan Kasenov and Kamila Salimzhanova and Dias Gaziz and Timur Ismailov and Batyrkhan Baimukhanov},
  doi          = {10.1016/j.comcom.2025.108297},
  journal      = {Computer Communications},
  month        = {10},
  pages        = {108297},
  shortjournal = {Comput. Commun.},
  title        = {Applying reinforcement learning in slotted LoRaWAN: From concept to implementation},
  volume       = {242},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Research on grouping strategy for NOMA downlink based on pointer network. <em>COMCOM</em>, <em>242</em>, 108296. (<a href='https://doi.org/10.1016/j.comcom.2025.108296'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the application of Non-Orthogonal Multiple Access (NOMA) technology in 5G and beyond communication systems, how to effectively group users to optimize power allocation has become a key challenge. This paper proposes a user grouping method based on a Pointer Network, which efficiently extracts user location information through embedding layers, encoder–decoder structures, and attention mechanisms, achieving the goal of precise grouping decisions and power optimization. The embedding layer maps users’ two-dimensional coordinates into a high-dimensional space, enhancing the model’s spatial awareness. The encoder–decoder structure, combined with Long Short-Term Memory (LSTM) networks and attention mechanisms, captures the spatiotemporal dependencies between users and dynamically selects the optimal path during the grouping process. Experimental results show that when users are located 300 meters from the base station, the recognition accuracy of a 4-user grouping reaches 94.85%, and that of a 6-user grouping reaches 89.3%. The method also demonstrates strong robustness under multipath fading channels and low signal-to-noise ratio conditions. Compared to random grouping methods, the proposed grouping strategy exhibits better adaptability and scalability in complex communication environments, significantly reducing power consumption, and providing new technical support for resource allocation and energy management in NOMA systems.},
  archive      = {J_COMCOM},
  author       = {Lingfeng Wu and Rui Zhu and Yarong Chen and Peng Chu and Juan Tian and Jiuxiao Cao},
  doi          = {10.1016/j.comcom.2025.108296},
  journal      = {Computer Communications},
  month        = {10},
  pages        = {108296},
  shortjournal = {Comput. Commun.},
  title        = {Research on grouping strategy for NOMA downlink based on pointer network},
  volume       = {242},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Private networks: Evolution, ecosystem, use cases, architecture, spectrum, and deployment challenges. <em>COMCOM</em>, <em>242</em>, 108295. (<a href='https://doi.org/10.1016/j.comcom.2025.108295'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Private networks have reshaped enterprise communications by providing unmatched control, security, and tailored solutions for various industries. This paper presents an in-depth survey of private networks, covering their evolution, current landscape, and future outlook. Key topics include the use cases, architecture, spectrum management, and deployment strategies. The study examines the transition from private 4G/LTE to private 5G networks, fueled by demands for higher data throughput and ultra-low latency across sectors. It highlights the advantages of private 5G over public mobile networks (MNOs) and Wi-Fi, with a special focus on spectrum sharing as a means to optimize frequency use. Additionally, the paper reviews global spectrum allocations for private 5G, providing an overview of regulatory frameworks and available frequency bands across countries. It also explores future prospects, including private 6G networks and emerging spectrum technologies. Key challenges such as high deployment costs, interoperability issues, and security concerns are discussed alongside potential solutions. Through this comprehensive analysis, the paper aims to provide valuable insights for researchers, practitioners, and policymakers in the field of private networks.},
  archive      = {J_COMCOM},
  author       = {Onur Sahin and Vanlin Sathya and Mehmet Yavuz},
  doi          = {10.1016/j.comcom.2025.108295},
  journal      = {Computer Communications},
  month        = {10},
  pages        = {108295},
  shortjournal = {Comput. Commun.},
  title        = {Private networks: Evolution, ecosystem, use cases, architecture, spectrum, and deployment challenges},
  volume       = {242},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Simplifying distributed application deployment at the edge through software-defined overlay networks. <em>COMCOM</em>, <em>242</em>, 108294. (<a href='https://doi.org/10.1016/j.comcom.2025.108294'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The need for low latency, bandwidth efficiency, and privacy has driven the deployment of distributed applications to the network edge. However, edge environments introduce concrete challenges such as limited infrastructure control, constrained connectivity due to NAT or firewalls, and the heterogeneity of devices and network conditions. This paper introduces a software-defined overlay networking (SDON) middleware that addresses these issues by simplifying the development and deployment of edge applications through centralized control and dynamic overlay management. SDON allows applications to define high-level requirements, such as node and link characteristics and the network topology. These requirements are translated into device-specific configurations and enforced across suitable edge devices. We implemented our SDON middleware as a fully functional software and evaluated it in two edge computing use cases: i) routing for video streaming across middleboxed edge devices and ii) computation offloading on heterogeneous edge devices. Our results show that deployments via SDON, with centrally enforced optimizations, improve application performance by reducing mean streaming latency by 20 % and computation times by 22 %.},
  archive      = {J_COMCOM},
  author       = {Heiko Bornholdt and Kevin Röbert and Stefan Schulte and Janick Edinger and Mathias Fischer},
  doi          = {10.1016/j.comcom.2025.108294},
  journal      = {Computer Communications},
  month        = {10},
  pages        = {108294},
  shortjournal = {Comput. Commun.},
  title        = {Simplifying distributed application deployment at the edge through software-defined overlay networks},
  volume       = {242},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Improving satellite network efficiency with terminal traffic prediction and SQP-SRA algorithm. <em>COMCOM</em>, <em>242</em>, 108293. (<a href='https://doi.org/10.1016/j.comcom.2025.108293'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To address the low resource utilization in satellite networks caused by heterogeneous regional traffic demands, this paper proposes a resource allocation strategy for LEO satellite internet based on terminal traffic prediction. An improved LSTM-GRU hybrid model is developed using real-world datasets to forecast ground traffic, accounting for periodic patterns and weather effects. A leaseable EOSN differentiated transmission framework is designed to enable targeted resource allocation and inter-satellite leasing, enhancing network coverage. To optimize data transmission ratios, user bandwidth, and service pricing, we introduce a sequential quadratic programming-based satellite resource allocation (SQP-SRA) algorithm that balances latency and energy consumption. Compared with LSTM, GRU, Transformer, and wavelet neural networks, the proposed model reduces traffic prediction error by approximately 26%. Simulation results demonstrate that, relative to the DDTOA, FCFS, and TOMRA algorithms, the proposed strategy improves user benefits by approximately 60% and enhances satellite service provider revenues by approximately 80%.},
  archive      = {J_COMCOM},
  author       = {Liangang Qi and Enqiang Wang and Tianfang Xu and Yuan Zhu and Yun Zhao},
  doi          = {10.1016/j.comcom.2025.108293},
  journal      = {Computer Communications},
  month        = {10},
  pages        = {108293},
  shortjournal = {Comput. Commun.},
  title        = {Improving satellite network efficiency with terminal traffic prediction and SQP-SRA algorithm},
  volume       = {242},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). An improved AVB-aware scheduling of time-triggered traffic in time-sensitive networks. <em>COMCOM</em>, <em>242</em>, 108292. (<a href='https://doi.org/10.1016/j.comcom.2025.108292'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Time-Sensitive Networking (TSN) provides deterministic services for diverse traffic types within a unified network. Among them, time-triggered (TT) traffic requires stringent timing guarantees, typically achieved through precise scheduling using Gate Control Lists (GCLs) in Time-Aware Shapers (TASs). However, most existing studies primarily focus on TT scheduling, often overlooking its impact on Audio Video Bridging (AVB) traffic, which demands worst-case delay (WCD) guarantees. This paper proposes an improved AVB-aware scheduling approach for TT traffic that enhances AVB performance without compromising TT schedulability. A rigorous network calculus analysis identifies two critical factors influencing WCD of AVB traffic: the maximum TT window length and the minimum relative offset between adjacent TT windows. Building on these insights, we develop a lightweight objective function for TT flow scheduling, enabling efficient evaluation of the impact on AVB traffic. This objective function is embedded into a Greedy Randomized Adaptive Search Procedure (GRASP)-based scheduling framework, further enhanced by a flow sorting strategy and a flexible local search mechanism that prioritizes high-impact TT flows and adaptively escapes local optima. Simulation results demonstrate that the proposed method significantly improves AVB performance and reduces runtime overhead, while consistently maintaining full TT schedulability across diverse TSN scenarios.},
  archive      = {J_COMCOM},
  author       = {Meng Wang and Yiqin Lu},
  doi          = {10.1016/j.comcom.2025.108292},
  journal      = {Computer Communications},
  month        = {10},
  pages        = {108292},
  shortjournal = {Comput. Commun.},
  title        = {An improved AVB-aware scheduling of time-triggered traffic in time-sensitive networks},
  volume       = {242},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Analyzing vertical handover of energy efficient sleep mode schemes in heterogeneous networks. <em>COMCOM</em>, <em>242</em>, 108291. (<a href='https://doi.org/10.1016/j.comcom.2025.108291'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Heterogeneous networks (HetNets) are a promising solution for the growing traffic demands of 5G. However, the continuous construction of small base stations (SBSs) increases the number of vertical handovers, which is closely related to a decrease in quality of service (QoS) due to the challenges in handover management. Therefore, many studies calculate the vertical handover rate using simulation-based or numerical methods. The sleep mode schemes, which dynamically put some SBSs into sleep mode, aim to address the concerns of excessive power consumption and also reduce the number of vertical handovers. Sleep modes are cost-effective and have become one of the popular methods for reducing energy consumption in HetNets. However, there is currently no model to analyze the number of vertical handovers when sleep modes are applied, making it difficult for internet service providers (ISPs) to estimate the impact of vertical handovers. In this paper, we analyze the number of vertical handovers in sleep mode schemes for heterogeneous networks and calculate the energy savings of three different schemes. The average errors of our proposed mathematical equations are less than 5%.},
  archive      = {J_COMCOM},
  author       = {Ting-Yu Lin and Hao-Zhong Zheng and Chun-Hao Yang and Fang-Yi Lee and Chia-Heng Tu and Meng-Hsun Tsai},
  doi          = {10.1016/j.comcom.2025.108291},
  journal      = {Computer Communications},
  month        = {10},
  pages        = {108291},
  shortjournal = {Comput. Commun.},
  title        = {Analyzing vertical handover of energy efficient sleep mode schemes in heterogeneous networks},
  volume       = {242},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Network traffic classification through high-order L-moments and multi-objective optimization. <em>COMCOM</em>, <em>242</em>, 108290. (<a href='https://doi.org/10.1016/j.comcom.2025.108290'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The exponential growth of encrypted and dynamic network traffic poses significant challenges to traditional traffic analysis methods, underscoring the need for robust and scalable solutions. Statistical approaches like L-moments have demonstrated exceptional potential in characterizing traffic flows, offering reduced sensitivity to outliers and the ability to capture higher-order distributional properties with minimal data. Building on previous work by the authors, this study introduces significant enhancements to the L-moment-based methodology for flow analysis and classification, specifically addressing limitations in feature selection and sample size requirements, aspects crucial for achieving deployable configurations in high-performance network environments. Key contributions include the integration of the fifth-order L-moment ratio ( τ 5 ) for enriched traffic representation and a multi-objective optimization framework based on a multi-objective evolutionary algorithm that balances competing goals: minimizing flow features selected for flow classification, reducing sample sizes for L-moment estimation, and maximizing classification quality. The enhanced methodology was applied to the CIC-DDoS2019 dataset, previously used in the authors’ earlier work, enabling direct comparison. Results show a reduction in sample size requirements from 200 to as few as 10, while simultaneously improving classification accuracy and selecting minimal features. These findings demonstrate the scalability and effectiveness of the proposed framework, designed for resource-constrained environments in Next-Generation Networks (NGNs), and make it publicly available for reproducibility and future research.},
  archive      = {J_COMCOM},
  author       = {Jesús Galeano-Brajones and Mihaela I. Chidean and Francisco Luna and Jesús Calle-Cancho and Javier Carmona-Murillo},
  doi          = {10.1016/j.comcom.2025.108290},
  journal      = {Computer Communications},
  month        = {10},
  pages        = {108290},
  shortjournal = {Comput. Commun.},
  title        = {Network traffic classification through high-order L-moments and multi-objective optimization},
  volume       = {242},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A blockchain solution for decentralized training in machine learning for IoT. <em>COMCOM</em>, <em>242</em>, 108289. (<a href='https://doi.org/10.1016/j.comcom.2025.108289'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The rapid growth of Internet of Things (IoT) devices and applications has led to an increased demand for advanced analytics and machine learning techniques capable of handling the challenges associated with data privacy, security, and scalability. Federated learning (FL) and blockchain technologies have emerged as promising approaches to address these challenges by enabling decentralized, secure, and privacy-preserving model training on distributed data sources. In this paper, we present a novel IoT solution that combines the incremental learning vector quantization algorithm (XuILVQ) with Ethereum blockchain technology to facilitate secure and efficient data sharing, model training, and prototype storage in a distributed environment. Our proposed architecture addresses the shortcomings of existing blockchain-based FL solutions by reducing computational and communication overheads while maintaining data privacy and security. We assess the performance of our system through a series of experiments, showing its potential to enhance the accuracy and efficiency of machine learning tasks in IoT settings.},
  archive      = {J_COMCOM},
  author       = {Carlos Beis-Penedo and Francisco Troncoso-Pastoriza and Rebeca P. Díaz-Redondo and Ana Fernández-Vilas and Manuel Fernández-Veiga and Martín González Soto},
  doi          = {10.1016/j.comcom.2025.108289},
  journal      = {Computer Communications},
  month        = {10},
  pages        = {108289},
  shortjournal = {Comput. Commun.},
  title        = {A blockchain solution for decentralized training in machine learning for IoT},
  volume       = {242},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). FLoV2T: A fine-grained malicious traffic classification method based on federated learning for AIoT. <em>COMCOM</em>, <em>242</em>, 108288. (<a href='https://doi.org/10.1016/j.comcom.2025.108288'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the rapid development of Artificial Intelligence of Things (AIoT), the network security risks associated with AIoT have surged, making precise fine-grained malicious traffic classification (MTC) technology essential, but the reliance on large datasets raises privacy concerns. Federated Learning (FL) offers a privacy-preserving alternative, but existing FL-based solutions still suffer from suboptimal classification accuracy, limited terminal resources, and the non-independent and identically distributed (non-IID) IoT data that hinder effective global model aggregation. To address these issues, this paper introduces FLoV2T — a FL-based fine-grained MTC method for AIoT. To improve classification performance, we first employ a pretrained Vision Transformer (ViT) to extract discriminative features by visualizing raw network traffic as images, thereby tackling the problem of inadequate feature representation. To alleviate the burden of resource constraints and high communication costs, we then implement a local parameter fine-tuning mechanism based on Low-Rank Adaptation (LoRA), significantly reducing the parameter for model learning and communication at the edge. Furthermore, to counteract the model bias towards clients’ non-IID data on model aggregation, we design a regularized parameter aggregation strategy to enhance global model robustness. Experimental results show that FLoV2T achieves an average accuracy of 97.26% and an F1 score of 96.99%, surpassing the baseline by 10.94% and 11.47%. Moreover, LoRA reduces parameter count by approximately 64 times while maintaining high classification performance, and under non-IID conditions, overall performance reaches an average accuracy of 96.17% and an average F1 score of 95.81%, underscoring FLoV2T’s potential in future AIoT communication networks.},
  archive      = {J_COMCOM},
  author       = {Fanyi Zeng and Chen Xu and Dapeng Man and Junhui Jiang and Wu Yang},
  doi          = {10.1016/j.comcom.2025.108288},
  journal      = {Computer Communications},
  month        = {10},
  pages        = {108288},
  shortjournal = {Comput. Commun.},
  title        = {FLoV2T: A fine-grained malicious traffic classification method based on federated learning for AIoT},
  volume       = {242},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). ILoRa: Interleaving-driven neural network for rate adaptation in LoRa communications. <em>COMCOM</em>, <em>242</em>, 108287. (<a href='https://doi.org/10.1016/j.comcom.2025.108287'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Rate adaptation in LoRa communications is crucial for improving the channel throughput by adjusting the data rate according to varying channel conditions. Existing methods typically operate at the packet or symbol level, which limits their ability to achieve fine-grained rate adaptation. In this paper, we propose ILoRa, an Interleaving-driven partial transmission method that automatically adjusts transmission rates according to real-time channel conditions. To be specific, we first introduce intra-symbol interleaving that leverages a progressive inorder traversal method to determine the transmission order within a symbol. Then inter-symbol interleaving is applied to coordinate the order across symbols. To manage the interleaving-induced partial transmission and improve communication performance under noisy conditions, we employ a multi-task convolutional recurrent neural network (MT-CRNN). This network leverages advanced data augmentation methods to further enhance channel robustness: time-spectral augmentation to mitigate information loss and synthetic noisy data to simulate various channel conditions. Extensive experimental results demonstrate that ILoRa significantly enhance transmission efficiency while maintaining reliable performance even in challenging environments.},
  archive      = {J_COMCOM},
  author       = {Xiaoke Qi and Haiyang Li and Dian Zhang and Lu Wang},
  doi          = {10.1016/j.comcom.2025.108287},
  journal      = {Computer Communications},
  month        = {10},
  pages        = {108287},
  shortjournal = {Comput. Commun.},
  title        = {ILoRa: Interleaving-driven neural network for rate adaptation in LoRa communications},
  volume       = {242},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Deep reinforcement learning based interference optimization for coordinated beamforming in ultra-dense wi-fi networks. <em>COMCOM</em>, <em>242</em>, 108286. (<a href='https://doi.org/10.1016/j.comcom.2025.108286'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Next-generation Wi-Fi networks are expected to have an ultra-dense deployment of access points (APs), thus, interference from overlapping basic service sets (OBSSs) poses challenges for interference management. Wi-Fi 8 aims at mitigating such interference using multi-access point coordination (MAPC). One of the MAPC variants is coordinated beamforming (Co-BF), where neighboring APs direct their signals towards specific users. Besides beam steering, APs can also perform null steering, which is more complex but can bring greater performance gains. In this paper, we present a centralized approach named intelligent null steering by reinforcement learning (IntelliNull), designed to reduce interference from neighboring transmitters by coordinated nulling while maximizing the signal quality at each station. We show that training the beam and null steering mechanism with a deep deterministic policy gradient (DDPG), it is possible to steer beams toward associated stations while intelligently nulling the most destructive interference from OBSS rather than nulling random interference directions. This method enhances communication between the AP and neighboring stations by reducing channel access contention, enabling transmissions at full power, and reducing worst-case latency. The proposed IntelliNull agent continuously adapts to changes in the network environment, including node mobility using channel state information (CSI) collected in real-time. We also compare our IntelliNull, which is based on beamforming plus nulling, with the baseline which is based on beamforming only. Our results demonstrate that IntelliNull outperforms the baseline by effectively mitigating interference, leading to higher throughput and better signal-to-interference-plus-noise ratio (SINR), especially in dense deployment scenarios where beamforming alone fails to sufficiently suppress OBSS interference.},
  archive      = {J_COMCOM},
  author       = {Jamshid Bacha and Anatolij Zubow and Szymon Szott and Katarzyna Kosek-Szott and Falko Dressler},
  doi          = {10.1016/j.comcom.2025.108286},
  journal      = {Computer Communications},
  month        = {10},
  pages        = {108286},
  shortjournal = {Comput. Commun.},
  title        = {Deep reinforcement learning based interference optimization for coordinated beamforming in ultra-dense wi-fi networks},
  volume       = {242},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Efficient security service function chaining based on federated learning in edge networks. <em>COMCOM</em>, <em>242</em>, 108285. (<a href='https://doi.org/10.1016/j.comcom.2025.108285'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The escalating demand for network services has prompted the evolution of Service Function Chaining (SFC) within 6G networks to deliver sophisticated, customized services while ensuring robust cybersecurity. This paper introduces an efficient and secure framework for SFC in Mobile Edge Computing (MEC) environments, termed the Federated Learning-based SFC (FL-SFC), which integrates SFC, MEC, and Federated Learning (FL) to enhance service policy decision-making and safeguard user privacy. The FL-SFC framework enables dynamic updating of service policies and optimizes communication efficiency. We propose an anomaly detection model, CNN-GRU, which combines Convolutional Neural Networks (CNNs) and Gated Recurrent Units (GRUs) to significantly improve anomaly detection performance at the network edge. Additionally, to address the high communication costs associated with service policy models, we have designed a model compression mechanism leveraging sparsification and quantization techniques, which substantially reduces communication overhead during model training. Simulation experiments demonstrated the superiority of the FL-SFC framework and the CNN-GRU model in detection performance over existing methods. Results indicate that our model excels in accuracy, precision, recall, and F1-score while significantly reducing the number of communication bits, thereby validating the effectiveness of our approach.},
  archive      = {J_COMCOM},
  author       = {Yunjian Jia and Jian Yu and Liang Liang and Fang Fang and Wanli Wen},
  doi          = {10.1016/j.comcom.2025.108285},
  journal      = {Computer Communications},
  month        = {10},
  pages        = {108285},
  shortjournal = {Comput. Commun.},
  title        = {Efficient security service function chaining based on federated learning in edge networks},
  volume       = {242},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Correctness of flow migration across network function instances. <em>COMCOM</em>, <em>242</em>, 108284. (<a href='https://doi.org/10.1016/j.comcom.2025.108284'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Network Functions (NFs) improve the safety and efficiency of networks. Flows traversing NFs may need to be migrated from a source NF instance (sNF) to a destination NF instance (dNF) to balance load, conserve energy, etc. When NFs are stateful, the information stored on an sNF per flow must be migrated to the corresponding dNF before the flow is migrated, to avoid problems of consistency. Our main contribution is to examine what it means to correctly migrate flows from a stateful NF instance. We define the property of Weak-O, where only the state information required for packets to be correctly forwarded from an sNF is migrated first to the corresponding dNF, while the remaining states are eventually migrated. Weak-O can be preserved without buffering or dropping packets, unlike existing algorithms. We propose an algorithm that preserves Weak-O and prove its correctness. Even though this may cause packet re-ordering, we experimentally demonstrate that the goodputs with and without migration are comparable when the old and new paths have the same delays and bandwidths. This is also true when the new path has larger bandwidth or at most 5 times longer delays. Thus flow migration without buffering is practical, contrary to what was thought before. We also prove that no criterion stronger than Weak-O can be preserved in a flow migration system that requires no buffering or dropping of packets and eventually synchronizes its states.},
  archive      = {J_COMCOM},
  author       = {Ranjan Patowary and Gautam Barua and Radhika Sukapuram},
  doi          = {10.1016/j.comcom.2025.108284},
  journal      = {Computer Communications},
  month        = {10},
  pages        = {108284},
  shortjournal = {Comput. Commun.},
  title        = {Correctness of flow migration across network function instances},
  volume       = {242},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). SDR: Stackelberg-based deep reinforcement learning for multi-skill spatiotemporal task allocation in AIoT systems. <em>COMCOM</em>, <em>242</em>, 108283. (<a href='https://doi.org/10.1016/j.comcom.2025.108283'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In AIoT-based multi-skill environments, task allocation is a complex process that involves multiple constraints and worker acceptance rates. However, existing studies often overlook worker acceptance rates and fail to properly balance the interests of both workers and requesters. To address this, we propose SDR, a system based on a dual Dueling DQN model in deep reinforcement learning, designed to maximize the long-term utility of all participants while considering user acceptance rates and demand constraints. SDR introduces targeted enhancements in state, action, and reward design to balance acceptance rates with spatiotemporal and skill constraints, optimizing both immediate and long-term task allocation performance. To resolve conflicts of interest, we integrate Pareto optimization into the Q-value computation and action selection. For scenarios where interests align, we adopt Stackelberg game theory to refine the reward mechanism. Extensive simulations on both synthetic and real-world datasets validate the effectiveness of our approach in improving task allocation and pricing strategies.},
  archive      = {J_COMCOM},
  author       = {Yu Li and Fengya Yin and Yihao Zheng and Wenjian Xu and Jung Yoon Kim and Zhe Peng},
  doi          = {10.1016/j.comcom.2025.108283},
  journal      = {Computer Communications},
  month        = {10},
  pages        = {108283},
  shortjournal = {Comput. Commun.},
  title        = {SDR: Stackelberg-based deep reinforcement learning for multi-skill spatiotemporal task allocation in AIoT systems},
  volume       = {242},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Proactive handover for task offloading in UAVs. <em>COMCOM</em>, <em>242</em>, 108282. (<a href='https://doi.org/10.1016/j.comcom.2025.108282'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Unmanned Aerial Vehicles (UAVs) are usually deployed alongside Internet of Things (IoT) devices in smart city applications, particularly for critical tasks such as disaster management that require continuous service. UAVs often handle resource-intensive and sensitive tasks through offloading, but unexpected task interruptions due to UAV dropouts can generate safety risks and increase costs. Although existing approaches in the literature have already addressed proactive handovers to mitigate such disruptions, their primary focus is on communication issues arising from UAV movement and are unable to handle offloading related issues. In this paper, we include in our model, in addition to communication, factors such as energy, computation requirements, and dynamic environmental conditions (e.g., wind speed and incentive), pushing toward a comprehensive solution for UAV task offloading and resource allocation. In fact, we formulate our problematic as a Markov game, which we solve using a Multi Agent Deep Q Network (MADQN). In our experiments, we assessed our approach using a federated learning scenario to illustrate its effectiveness in a realistic distributed application setting against several baselines from the state of the art. Results showed that our approach outperforms its peers in terms of system utility, and tradeoff between cost and dropout rates, leading to an improved handover management of computational and energy resources in UAV-IoT based systems. In fact, it reduces the dropout rate by approximately 45% compared to the second-best baseline, leading to a 2% improvement in model accuracy and a 50% reduction in deployment costs.},
  archive      = {J_COMCOM},
  author       = {Mohammed Riyadh Abdmeziem and Amina Ahmed Nacer and Soumeya Demil},
  doi          = {10.1016/j.comcom.2025.108282},
  journal      = {Computer Communications},
  month        = {10},
  pages        = {108282},
  shortjournal = {Comput. Commun.},
  title        = {Proactive handover for task offloading in UAVs},
  volume       = {242},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Performance model and system optimization of an energy-saving strategy based on adaptive service rate tuning in cloud data centers with micro-burst traffic. <em>COMCOM</em>, <em>242</em>, 108281. (<a href='https://doi.org/10.1016/j.comcom.2025.108281'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the increasing competition in cloud market, reducing operating costs and improving Quality of Service (QoS) are two of the key issues that cloud vendors need to consider. In order to reduce the power consumption while mitigating the negative impact of micro-burst traffic in Cloud Data Centers (CDCs) on performance, and make cloud vendors more competitive, we design an Energy-saving Strategy based on Sleep and Adaptive Service-rate Tuning (ES-SAST) in this paper. We model the arrivals of the cloud task requests as an environment-dependent R -phase Markov Arrival Process (MAP ( R ) ), and we establish a multi-server synchronous multi-vacation queue with adaptive service rate tuning. We construct a four-dimensional Markov chain to analyze the queue, and we calculate some measures to evaluate the energy efficiency and QoS in the steady state. Then we develop an objective function composed of three performance measures. Finally, we propose an Improved Fire Hawk Optimizer (IFHO) with multi-strategy integration, and IFHO jointly optimizes two system parameters. An empirical study shows that IFHO chooses a lower system expected cost, where the power consumption of the system falls by 3%, the latency of tasks decreases by 19%, and the loss rate of the system reduces by 37%, on average.},
  archive      = {J_COMCOM},
  author       = {Xuena Yan and Shunfu Jin},
  doi          = {10.1016/j.comcom.2025.108281},
  journal      = {Computer Communications},
  month        = {10},
  pages        = {108281},
  shortjournal = {Comput. Commun.},
  title        = {Performance model and system optimization of an energy-saving strategy based on adaptive service rate tuning in cloud data centers with micro-burst traffic},
  volume       = {242},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Overcoming data limitations in internet traffic forecasting: LSTM models with transfer learning and wavelet augmentation. <em>COMCOM</em>, <em>242</em>, 108280. (<a href='https://doi.org/10.1016/j.comcom.2025.108280'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Accurate internet traffic prediction in smaller ISP networks is challenged by limited data availability. This paper explores this issue using transfer learning and data augmentation techniques with two LSTM-based models, LSTMSeq2Seq and LSTMSeq2SeqAtn, initially trained on a comprehensive dataset provided by Juniper Networks, Inc. and subsequently applied to smaller datasets. The datasets represent real internet traffic telemetry, offering insights into diverse traffic patterns across different network domains. Our study found that although both models performed well in single-step predictions, multi-step forecasting was more challenging, especially regarding long-term accuracy. Empirical results demonstrated that LSTMSeq2Seq outperformed LSTMSeq2SeqAtn on smaller datasets, with improvements in forecasting accuracy by up to 36.70% in MAE and 27.66% in WAPE after applying data augmentation using Discrete Wavelet Transform. The LSTMSeq2Seq model achieved an accuracy improvement from 83% to 88% for 6-step forecasts, 82% to 88% for 9-step forecasts, and 81% to 87% for 12-step forecasts, whereas LSTMSeq2SeqAtn exhibited a more stable short-term performance but higher variability in longer forecasts. Additionally, the mean absolute percentage error (MAPE) of multi-step predictions increased over longer horizons, with LSTMSeq2Seq reaching 6.74% at 12 steps and LSTMSeq2SeqAtn at 6.77%, highlighting the challenge of long-term forecasting. Variability analysis showed that while the attention mechanism in LSTMSeq2SeqAtn improved short-term prediction consistency, it also increased uncertainty in longer forecasts, as seen in the interquartile range (IQR) rising from 0.578 at 6 steps to 1.237 at 9 steps. Outlier analysis further confirmed that LSTMSeq2Seq exhibited more stable improvements, whereas LSTMSeq2SeqAtn showed increased dispersion in forecast accuracy. These findings underscore the importance of transfer learning and data augmentation in enhancing forecasting accuracy, particularly for smaller ISP networks with limited data availability. Furthermore, our analysis highlights the trade-offs between model complexity, short-term consistency, and long-term stability in internet traffic prediction.},
  archive      = {J_COMCOM},
  author       = {Sajal Saha and Anwar Haque and Greg Sidebottom},
  doi          = {10.1016/j.comcom.2025.108280},
  journal      = {Computer Communications},
  month        = {10},
  pages        = {108280},
  shortjournal = {Comput. Commun.},
  title        = {Overcoming data limitations in internet traffic forecasting: LSTM models with transfer learning and wavelet augmentation},
  volume       = {242},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Exploring traffic pattern variability in vehicular federated learning. <em>COMCOM</em>, <em>242</em>, 108279. (<a href='https://doi.org/10.1016/j.comcom.2025.108279'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The emergence of software-defined vehicles has brought machine learning into the vehicular domain. To support these data-driven applications, techniques to incentivize users to share their vehicle data are crucial. Federated learning trains machine learning models in a distributed manner, leveraging client data without compromising its privacy. Nonetheless, in vehicular networks, the dynamic behavior of nodes affects client availability and the global model’s performance. Accordingly, this paper evaluates federated learning (FL) in a realistic vehicular network topology, accounting for real vehicle traffic in two Brazilian urban areas. The network simulation covers 3 . 7 km 2 with 1290 vehicles per hour and road speeds, based on real data. Our paper provides a comprehensive analysis of the impact that different traffic behaviors can yield during the training phase of a federated learning model. We observe that there is a performance decay in urban areas with longer vehicle permanence. Interestingly, longer vehicle participation in FL training leads to a biased final model with reduced generalization. We propose a novel approach to verify vehicle variability over time, by using the Dice-Sørensen coefficient to compare the set of clients participating in different rounds of training. By maintaining the vehicle variability over the rounds we can reduce the effect of the bias on the model, and – with a 47% reduction of the communication overhead – achieve faster learning, higher convergence in the first 15 rounds, and an equivalent final accuracy. Additionally, we extend our analysis by conducting simulations under more extreme traffic scenarios across multiple datasets, using a MobileNetV3. The results confirm that sustaining high vehicle variability – in scenarios with a brief participation of vehicles in the training – yields comparable model performance while saving up to 83.5 GB in communication costs.},
  archive      = {J_COMCOM},
  author       = {Giuliano Fittipaldi and Rodrigo S. Couto and Luís H.M.K. Costa},
  doi          = {10.1016/j.comcom.2025.108279},
  journal      = {Computer Communications},
  month        = {10},
  pages        = {108279},
  shortjournal = {Comput. Commun.},
  title        = {Exploring traffic pattern variability in vehicular federated learning},
  volume       = {242},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Delay analysis of BFT consensus: Case study of narwhal and bullshark protocols. <em>COMCOM</em>, <em>242</em>, 108278. (<a href='https://doi.org/10.1016/j.comcom.2025.108278'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Acknowledging the critical influence of consensus delays on blockchain performance, this paper presents an analytical and simulation-based exploration of delay characteristics in Byzantine Fault Tolerant (BFT) consensus mechanisms. Our focus is on SUI, a blockchain system that employs a Directed Acyclic Graph (DAG) structure to support parallel transaction execution. SUI relies on two integrated protocols: Narwhal, a mempool protocol responsible for efficient block dissemination and DAG construction; and Bullshark, which organizes DAG vertices to produce a consistent total order of transactions without incurring additional communication overhead. While our previous work modeled Narwhal’s delay characteristics under various message propagation distributions, this study shifts attention to Bullshark—the protocol responsible for reaching consensus. We propose a probabilistic analytical model that estimates the number of rounds required to reach consensus. In this model, each validator’s decision is treated as a Bernoulli trial, and we apply the binomial distribution to determine the probability of reaching quorum. This framework enables us to analyze the expected delay of the protocol. To validate our model, we implemented both Narwhal and Bullshark and conducted extensive simulations. The simulation results show strong agreement with our analytical predictions, confirming the accuracy of our model. For instance, under a Gaussian delay model with mean μ = 1 ms and standard deviation σ = 0 . 25 ms—values representative of short-range wireless communication in real-world IoT or LAN settings [1] —we predict an average round duration of approximately 3.26 ms. Furthermore, based on our binomial-based model of block commitment, the expected number of rounds to reach consensus is approximately 1 when f = 10 , indicating that blocks typically commit in a single round with high probability. To the best of our knowledge, this is the first study to model Bullshark’s consensus process using Bernoulli trials and binomial distributions. Our contributions offer a novel framework for evaluating its efficiency and provide insights that can guide future optimization and scalability efforts for DAG-based BFT protocols.},
  archive      = {J_COMCOM},
  author       = {Khouloud Hwerbi and Ichrak Amdouni and Cédric Adjih and Leila Azouz Saidane and Anis Laouiti},
  doi          = {10.1016/j.comcom.2025.108278},
  journal      = {Computer Communications},
  month        = {10},
  pages        = {108278},
  shortjournal = {Comput. Commun.},
  title        = {Delay analysis of BFT consensus: Case study of narwhal and bullshark protocols},
  volume       = {242},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Dynamic split federated learning for resource-constrained IoT systems. <em>COMCOM</em>, <em>242</em>, 108275. (<a href='https://doi.org/10.1016/j.comcom.2025.108275'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Efficient resource utilization in Internet of Things (IoT) systems is challenging due to device limitations. These limitations restrict on-device machine learning (ML) model training, leading to longer processing times and inefficient metadata analysis. Additionally, conventional centralized data collection poses privacy concerns, as raw data has to leave the device to the server for processing. Combining Federated Learning (FL) and Split Learning (SL) offers a promising solution by enabling effective machine learning on resource-constrained devices while preserving user privacy. However, the dynamic nature of IoT resources and device heterogeneity can complicate the application of these solutions, as some IoT devices cannot complete the training task on time. To address these concerns, we have developed a Dynamic Split Federated Learning (DSFL) architecture that dynamically adjusts to the real-time resource availability of individual clients. Combining real-time split-point selection with a Genetic Algorithm (GA) for client selection, tailored to heterogeneous, resource-constrained IoT devices. DSFL ensures optimal utilization of resources and efficient training across heterogeneous IoT devices and servers. Our architecture detects each IoT device’s training capabilities by identifying the number of layers it can train. Moreover, an effective Genetic Algorithm (GA) process strategically selects the clients required to complete the split federated learning round. Cooperatively, the clients and servers train their parts of the model, aggregate them, and then combine the results before moving to the next round. Our proposed architecture enables collaborative model training across devices while preserving data privacy by combining FL’s parallelism with SL’s dynamic modeling. We evaluated our architecture on sensory and image-based datasets, showing improved accuracy and reduced overhead compared to baseline methods.},
  archive      = {J_COMCOM},
  author       = {Mohamad Wazzeh and Ahmad Hammoud and Azzam Mourad and Hadi Otrok and Chamseddine Talhi and Zbigniew Dziong and Chang-Dong Wang and Mohsen Guizani},
  doi          = {10.1016/j.comcom.2025.108275},
  journal      = {Computer Communications},
  month        = {10},
  pages        = {108275},
  shortjournal = {Comput. Commun.},
  title        = {Dynamic split federated learning for resource-constrained IoT systems},
  volume       = {242},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Physical layer security in FAS-aided wireless powered NOMA systems. <em>COMCOM</em>, <em>242</em>, 108274. (<a href='https://doi.org/10.1016/j.comcom.2025.108274'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The rapid evolution of communication technologies and the emergence of sixth-generation (6G) networks have introduced unprecedented opportunities for ultra-reliable, low-latency, and energy-efficient communication. Integrating technologies like non-orthogonal multiple access (NOMA) and wireless powered communication networks (WPCNs) brings new challenges. These include energy constraints and increased security vulnerabilities. Traditional antenna systems and orthogonal multiple access schemes struggle to meet the increasing demands for performance and security in such environments. To address this gap, this paper investigates the impact of emerging fluid antenna systems (FAS) on the performance of physical layer security (PLS) in WPCNs. Specifically, we consider a scenario in which a transmitter, powered by a power beacon via an energy link, transmits confidential messages to legitimate FAS-aided users over information links while an external eavesdropper attempts to decode the transmitted signals. Additionally, users leverage the NOMA scheme, where the far user may also act as an internal eavesdropper. For the proposed model, we first derive the distributions of the equivalent channels at each node and subsequently obtain compact expressions for the secrecy outage probability (SOP) and average secrecy capacity (ASC), using the Gaussian quadrature methods. Our results reveal that incorporating the FAS for NOMA users, instead of the TAS, enhances the performance of the proposed secure WPCN.},
  archive      = {J_COMCOM},
  author       = {Farshad Rostami Ghadi and Masoud Kaveh and Kai-Kit Wong and Diego Martín and Riku Jäntti and Zheng Yan},
  doi          = {10.1016/j.comcom.2025.108274},
  journal      = {Computer Communications},
  month        = {10},
  pages        = {108274},
  shortjournal = {Comput. Commun.},
  title        = {Physical layer security in FAS-aided wireless powered NOMA systems},
  volume       = {242},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A slot-based energy storage decision-making approach for optimal off-grid telecommunication operator. <em>COMCOM</em>, <em>242</em>, 108273. (<a href='https://doi.org/10.1016/j.comcom.2025.108273'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper proposes a slot-based energy storage approach for decision-making in the context of an Off-Grid telecommunication operator. We consider network systems powered by solar panels, where harvest energy is stored in a battery that can also be sold when fully charged. To reflect real-world conditions, we account for non-stationary energy arrivals and service demands that depend on the time of day, as well as the failure states of PV panel. The network operator we model faces two conflicting objectives: maintaining the operation of its infrastructure and selling (or supplying to other networks) surplus energy from fully charged batteries. To address these challenges, we developed a slot-based Markov Decision Process (MDP) model that incorporates positive rewards for energy sales, as well as penalties for energy loss and battery depletion. This slot-based MDP follows a specific structure we have previously proven to be efficient in terms of computational performance and accuracy. From this model, we derive the optimal policy that balances these conflicting objectives and maximizes the average reward function. Additionally, we present results comparing different cities and months, which the operator can consider when deploying its infrastructure to maximize rewards based on location-specific energy availability and seasonal variations. Experimental results show that our proposed algorithm outperforms classical methods in large-scale scenarios. While Relative Value Iteration algorithm remains competitive on smaller instances, its convergence time increases significantly under strict precision requirements (e.g., ϵ < 1 0 − 10 ). In contrast, our method maintains both speed and robustness, solving MDPs with up to 2 × 1 0 5 states and 100 actions in under one hour, whereas standard approaches exceed 1 0 4 seconds.},
  archive      = {J_COMCOM},
  author       = {Youssef Ait El Mahjoub and Jean-Michel Fourneau},
  doi          = {10.1016/j.comcom.2025.108273},
  journal      = {Computer Communications},
  month        = {10},
  pages        = {108273},
  shortjournal = {Comput. Commun.},
  title        = {A slot-based energy storage decision-making approach for optimal off-grid telecommunication operator},
  volume       = {242},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A lightweight secret-sharing-based defense against model poisoning attacks in privacy-preserving federated learning. <em>COMCOM</em>, <em>242</em>, 108272. (<a href='https://doi.org/10.1016/j.comcom.2025.108272'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As Artificial Intelligence of Things (AIoT) converges with Privacy-Preserving Federated Learning (PPFL), the challenge of defending against model poisoning attacks emerges as increasingly critical. Due to PPFL’s cryptographic protocols for protecting gradient exchanges, detecting poisoning attacks becomes challenging. Traditional defense mechanisms rely on plaintext gradient analysis and thus cannot be directly applied to encrypted gradients. Although homomorphic encryption-based defense schemes enable secure computations on encrypted data, their substantial computational overhead makes them impractical for resource-constrained Internet of Things (IoT) deployments. To address these challenges, we propose a Secret-Sharing-based Defense Framework (SSDF), a lightweight scheme that enables efficient similarity calculations on encrypted gradients under secure aggregation protocols. Our scheme facilitates robust aggregation of encrypted parameters in resource-constrained edge computing environments while protecting the privacy of local model updates. Extensive experiments on four datasets demonstrate that our proposed scheme provides robust defense capabilities against poisoning attacks for both Independent and Identically Distributed (IID) and non-IID data.},
  archive      = {J_COMCOM},
  author       = {Hengheng Xiong and Jiguang Lv and Dapeng Man and Yukun Zhu and Tao Liu and Huanran Wang and Chen Xu and Wu Yang},
  doi          = {10.1016/j.comcom.2025.108272},
  journal      = {Computer Communications},
  month        = {10},
  pages        = {108272},
  shortjournal = {Comput. Commun.},
  title        = {A lightweight secret-sharing-based defense against model poisoning attacks in privacy-preserving federated learning},
  volume       = {242},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A homomorphic MAC-based verifiable secure aggregation for federated learning in cloud–edge AIoT. <em>COMCOM</em>, <em>242</em>, 108271. (<a href='https://doi.org/10.1016/j.comcom.2025.108271'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The cloud–edge collaborative Artificial Intelligence of Things (AIoT) architecture addresses challenges in managing vast data storage, intelligent information processing, device interconnectivity within the Internet of Things. For its security risks and data privacy, federated learning emerges as a promising solution for ensuring data privacy in AIoT. However, susceptibility to malicious attacks during data transmission poses a significant challenge and a semi-trusted server may deviate from the specified protocol leading to inaccurate aggregation parameters returned to clients. Our proposed solution introduces a federated learning integrity verification scheme based on homomorphic Message Authentication Code (MAC) within a cloud–edge collaborative AIoT architecture. Homomorphic MAC ensures secure aggregation and integrity verification, even when distinct clients possess different keys, emphasizing integrity verification by edge node, contributes to reduced client computing costs. Further verifying of the aggregated parameters by users prevents untrusted transmission from edge node. Leveraging data integrity verification proves effective in mitigating challenges associated with parameter security, especially in scenarios involving inaccurate aggregation of local model parameters within federated learning. Our solution is free bilinear pairing, resulting in a significant reduction in computational overhead. We evaluate accuracy on the MNIST dataset through comparison with the FedAVG plaintext scheme, showing that our approach ensures parameter integrity while maintaining model performance, numerical simulations also confirm its efficiency.},
  archive      = {J_COMCOM},
  author       = {Shufen Niu and Weiying Kong and Lihua Chen and Xusheng Zhou and Ning Wang},
  doi          = {10.1016/j.comcom.2025.108271},
  journal      = {Computer Communications},
  month        = {10},
  pages        = {108271},
  shortjournal = {Comput. Commun.},
  title        = {A homomorphic MAC-based verifiable secure aggregation for federated learning in cloud–edge AIoT},
  volume       = {242},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Performance of UAV-assisted C-V2X communications with 3D antenna beam-width fluctuations. <em>COMCOM</em>, <em>242</em>, 108267. (<a href='https://doi.org/10.1016/j.comcom.2025.108267'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The antenna’s three-dimensional (3D) beam-width orientation is crucial in assessing the effectiveness of vehicular communications. This paper investigates the influence of variations of millimeter waveband antenna 3D beam-width on the performance of un-crewed aerial vehicle (UAV)-assisted cellular vehicle-to-everything (C-V2X) communications. The cellular base-stations are represented using a two-dimensional Poisson point process (PPP), while vehicular nodes (V-Ns) are represented using a Poisson line process, and UAVs are represented using a 3D PPP. The typical transmitting V-N can connect with the nearest V-N in direct mode transmission or with the (macro base-station) MBS, line-of-sight (LOS) UAV, or non-LOS (NLOS) UAV in shared mode transmission. The efficiency of the system is measured by using the antenna’s 3D beam-width relative to coverage and spectrum efficiency. To that aim, analytical equations for the association and coverage probability of vehicular-to-vehicular, vehicular-to-MBS, vehicular-to-LOS UAV, and vehicular-to-NLOS UAV connections are obtained in the setting of variation in beam-width. The efficiency is also measured in terms of V-Ns, MBS, and UAVs. The findings revealed that our system, considering millimeter waveband-based UAV-assisted C-V2X network leveraging the benefits of MBSs and UAVs, performs better than the conventional V2X network. The findings reveal that the efficiency of the UAV-assisted C-V2X networks is affected by the variable 3D beam-width, hence, it needs to be thoroughly specified. Furthermore, the network’s performance degrades when the UAV’s beam-width variations grow.},
  archive      = {J_COMCOM},
  author       = {Mohammad Arif and Wooseong Kim and Asif Mehmood},
  doi          = {10.1016/j.comcom.2025.108267},
  journal      = {Computer Communications},
  month        = {10},
  pages        = {108267},
  shortjournal = {Comput. Commun.},
  title        = {Performance of UAV-assisted C-V2X communications with 3D antenna beam-width fluctuations},
  volume       = {242},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). ACSFL: An adaptive client selection-based federated learning with personalized differential privacy for heterogeneous AIoT environments. <em>COMCOM</em>, <em>242</em>, 108264. (<a href='https://doi.org/10.1016/j.comcom.2025.108264'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Driven by the rapid development of Artificial Intelligence (AI) and the Internet of Things (IoT), the Artificial Intelligence of Things (AIoT) is increasingly applied in smart environments. Federated Learning (FL) meets the need for intelligent data processing in these environments by providing powerful training capabilities while preserving privacy. However, AIoT environments pose new challenges for FL, particularly due to the heterogeneity of edge devices, which vary in hardware, software, network conditions, and data distribution. These factors degrade model performance and hinder convergence. Additionally, communication overhead and data privacy risks are also critical concerns. Although Differential Privacy (DP) can offer protection, they often apply uniform privacy levels, overlooking the diversity of AIoT devices. On the other hand, while current client-selection approaches partially address the heterogeneity of AIoT devices, they also tend to ignore the impact of the noising mechanisms. In this paper, we propose ACSFL, an adaptive client selection-based FL framework that integrates personalized local DP. By a novel, dynamic evaluation metric of node heterogeneity, privacy budget, and contribution, ACSFL can jointly optimize model performance, privacy preservation, and communication efficiency. We further propose a personalized local differential privacy mechanism in ACSFL, to filter and allocate each client’s budget per round, substantially enhancing privacy preservation and yielding significant accuracy gains under identical overall privacy constraints. All the above assertions are also well supported by theoretical and experimental demonstration. Specifically, our experiments show that ACSFL improves model convergence and generalization by 14% on average, achieves comparable model accuracy with 20% fewer clients, reduces communication overhead by over 25%, and saves about 26% of the privacy budget compared to other client selection methods.},
  archive      = {J_COMCOM},
  author       = {Zhousheng Wang and Junjie Chen and Hua Dai and Jian Xu and Geng Yang and Hao Zhou},
  doi          = {10.1016/j.comcom.2025.108264},
  journal      = {Computer Communications},
  month        = {10},
  pages        = {108264},
  shortjournal = {Comput. Commun.},
  title        = {ACSFL: An adaptive client selection-based federated learning with personalized differential privacy for heterogeneous AIoT environments},
  volume       = {242},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Efficient blockchain synchronization mechanism over NDN based on directed interest forwarding. <em>COMCOM</em>, <em>242</em>, 108258. (<a href='https://doi.org/10.1016/j.comcom.2025.108258'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Blockchain technology, as a decentralized technology, has been applied across various industries due to its immutability and information security features. With the increasing adoption of blockchain technology, network scale and transaction volumes have increased rapidly. The growing data transmission demands have exposed network performance issues in blockchain systems, creating a bottleneck for further improvements. While Named Data Networking (NDN) offers strong support for blockchain networks, some existing designs lack efficient synchronization methods, resulting in redundancies and limiting the full potential of NDN in blockchain networks. To address this issue, this paper proposes a directed Interest forwarding-based synchronization mechanism for NDN-based blockchain networks. In this mechanism, we design a Block Synchronous Forward Table (BSFT) to record the synchronization status of upstream and downstream nodes. Through the structure of this table, nodes can obtain information about other nodes in the network via six specifically designed NDN Interests. During synchronization, nodes dynamically select the appropriate peers to send data request Interest based on the actual network state and synchronization status, thereby reducing the large number of redundant Interest packets and corresponding response Data packets caused by Interest broadcasts. Experimental results demonstrate that our proposed synchronization mechanism can effectively reduce network traffic, lowering traffic by about 30% or more compared to traditional IP-based blockchain and other NDN-based blockchain solutions. This also accelerates the synchronization of Data packets across the entire network, thereby enhancing the overall performance of blockchain networks.},
  archive      = {J_COMCOM},
  author       = {Dehao Zhang and Jiapeng Xiu and Zhengqiu Yang and Huixin Liu and Shaoyong Guo},
  doi          = {10.1016/j.comcom.2025.108258},
  journal      = {Computer Communications},
  month        = {10},
  pages        = {108258},
  shortjournal = {Comput. Commun.},
  title        = {Efficient blockchain synchronization mechanism over NDN based on directed interest forwarding},
  volume       = {242},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Optimizing mobility prediction in 5G for enhanced C-V2X applications: A multidisciplinary research survey. <em>COMCOM</em>, <em>242</em>, 108254. (<a href='https://doi.org/10.1016/j.comcom.2025.108254'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the advent of 5G New Radio technologies, autonomous vehicles are evolving from isolated units into components of a larger, interconnected system. This transformation is enabled by robust Vehicle-to-Everything (V2X) communications, facilitating applications such as high-definition sensor data sharing and collision avoidance within the Cellular-V2X (C-V2X) framework. Nationwide, ultra-reliable, low-latency coverage is crucial for these applications, necessitating a smart, flexible network to manage mobility uncertainties effectively. To achieve this, mobility prediction will play a pivotal role by preparing the network for anticipated traffic patterns and optimizing its radio and computational resources, thereby enhancing overall efficiency. This survey provides a comprehensive review and analysis of current and emerging mobility prediction methodologies essential for enhancing these networks. We explore these methodologies along with the standards and requirements set by key organizations like the 3rd Generation Partnership Project (3GPP) and industry leaders such as the 5G Automotive Association (5GAA). By reviewing state-of-the-art mobility prediction techniques, this survey critically analyzes their role in forecasting key network performance indicators (KPIs), enabling proactive resource allocation, robust edge-computing strategies, and slice orchestration, all crucial for optimizing 5G performance and ensuring ultra-reliable, low-latency C-V2X communications.},
  archive      = {J_COMCOM},
  author       = {Mario Bou Abboud and Maroua Drissi and Oumaya Baala and Sylvain Allio},
  doi          = {10.1016/j.comcom.2025.108254},
  journal      = {Computer Communications},
  month        = {10},
  pages        = {108254},
  shortjournal = {Comput. Commun.},
  title        = {Optimizing mobility prediction in 5G for enhanced C-V2X applications: A multidisciplinary research survey},
  volume       = {242},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Optimizing bandwidth allocation in mmWave/sub-THz cellular networks using maximum flow algorithms. <em>COMCOM</em>, <em>242</em>, 108221. (<a href='https://doi.org/10.1016/j.comcom.2025.108221'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The exploitation of millimeter wave (mmWave) and sub-Terahertz (sub-THz) bands is expected to be one of the main pillars for the development of future cellular networks due to the high available bandwidth they provide. The existence of Line-of-Sight (LOS) link between a user equipment (UE) and an access point (AP) is a prerequisite for connection establishment in these networks, as the wireless links in these bands are very sensitive to blockage effects. This can be achieved by densifying APs within a network area. An arising challenge is the efficient exploitation of the available bandwidth of a given network. In this paper, the maximization of the number of served UEs in modern mmWave and sub-THz cellular networks is investigated and achieved by deploying a Maximum Flow Algorithm for UE-AP association (MFUA) to optimize bandwidth allocation, assuming that every AP will have a finite and predefined amount of bandwidth which they can share among UEs. MFUA determines the maximum flow between two given nodes of a graph corresponding to a specific network, where the capacity of its edges is known. An extensive simulation campaign was carried out revealing that the use of MFUA utilizes bandwidth more effectively compared to the reference method and improves the system performance, leading to the maximization of number of served UEs. The examined test cases include static and time-evolving scenarios.},
  archive      = {J_COMCOM},
  author       = {Kyriakos N. Manganaris and Panagiotis Promponas and Aris Tsolis and Fotis I. Lazarakis and Kostas P. Peppas},
  doi          = {10.1016/j.comcom.2025.108221},
  journal      = {Computer Communications},
  month        = {10},
  pages        = {108221},
  shortjournal = {Comput. Commun.},
  title        = {Optimizing bandwidth allocation in mmWave/sub-THz cellular networks using maximum flow algorithms},
  volume       = {242},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Accountable privacy-enhanced multi-authority attribute-based authentication scheme for cloud services. <em>COMCOM</em>, <em>242</em>, 108205. (<a href='https://doi.org/10.1016/j.comcom.2025.108205'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Current attribute-based authentication (ABA) schemes have three major drawbacks: first, the single attribute authority (AA) becomes the system bottleneck, i.e., if the AA is corrupted, the entire system will stop working; second, user privacy is not completely secured; and third, malicious users may exploit their anonymity. To overcome these defects, we improved a previously established privacy-preserving decentralized ciphertext policy attribute-based encryption (PPD-CP-ABE) scheme, obtaining a PPD-CP-ABE with verifiable outsourced decryption (PPD-CP-ABE-VOD). This improved scheme uses outsourced decryption, secure two-party computation protocol, and zero-knowledge proofs. We transformed the PPD-CP-ABE-VOD scheme into a new privacy-enhanced multi-authority ABA scheme using an identity tracing mechanism based on linear encryption. This new scheme has the following advantages over similar schemes. First, it introduces multiple AAs and does not require users to trust AA fully. Second, it protects users’ attributes, global identifiers, and access behavior, thus strengthening user privacy protection. Finally, it balances user privacy protection and user accountability. Theoretical and experimental analyses have shown that the new scheme is comparable to recently proposed ABA systems in terms of performance in the key generation and authentication phases, despite appending multiple security properties.},
  archive      = {J_COMCOM},
  author       = {Xin Liu and Hao Wang and Bo Zhang and Bin Zhang},
  doi          = {10.1016/j.comcom.2025.108205},
  journal      = {Computer Communications},
  month        = {10},
  pages        = {108205},
  shortjournal = {Comput. Commun.},
  title        = {Accountable privacy-enhanced multi-authority attribute-based authentication scheme for cloud services},
  volume       = {242},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Hybrid mobility in opportunistic networks: Insights into enhanced PIPeR variants for subway settings. <em>COMCOM</em>, <em>241</em>, 108277. (<a href='https://doi.org/10.1016/j.comcom.2025.108277'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper explores the performance of the Power and Interest Aware PeopleRank (PIPeR) algorithm, a prominent opportunistic network forwarding algorithm, within a new context namely, subway mobility environments. While PIPeR has demonstrated strong performance in pedestrian mobility models by accounting for both interest in disseminated content and power conservation, its application in subway settings—characterized by hybrid mobility and unique challenges—has yet to be explored. Subway mobility scenarios are particularly relevant for contexts such as emergency response for civilians use during crises, where fixed network infrastructure is limited or unavailable. In this study, PIPeR is implemented and evaluated using the AnyLogic simulator, which accurately models subway passenger flows under hybrid mobility. Additionally, five enhanced variants of the original PIPeR algorithm are proposed, designed to address the unique challenges of subway environments and any other environments of similar mobility patterns, aiming to enhance the algorithm's overall efficiency. The best-performing variant is then identified and rigorously tested through experiments to evaluate its robustness under varying conditions, including various interest distributions, battery distributions, user density, and message volume per user. The results reveal that the PIPeR algorithm in the subway environment achieves a notable 64 % increase in the F-measure and a 63 % reduction in delay compared to the pedestrian mobility environment, but at the cost of increased power consumption and cost. The proposed variants mitigate these challenges, achieving an impressive 83 % reduction in power consumption and a 38 % decrease in cost, with a trade-off of a 20 % reduction in F-measure. These findings highlight a significant step towards green computing and sustainability in opportunistic networks. Moreover, the best-performing variant, when tested in a challenging scenario with a majority of uninterested users and a lack of intermediate forwarders, demonstrates excellent performance, further underscoring its adaptability and robustness.},
  archive      = {J_COMCOM},
  author       = {Sara ElSingergy and Soumaia Al Ayyat and Sherif G. Aly},
  doi          = {10.1016/j.comcom.2025.108277},
  journal      = {Computer Communications},
  month        = {9},
  pages        = {108277},
  shortjournal = {Comput. Commun.},
  title        = {Hybrid mobility in opportunistic networks: Insights into enhanced PIPeR variants for subway settings},
  volume       = {241},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Maritime monitoring through LoRaWAN: Resilient decentralised mesh networks for enhanced data transmission. <em>COMCOM</em>, <em>241</em>, 108276. (<a href='https://doi.org/10.1016/j.comcom.2025.108276'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Resilient communication networks from ocean-deployed buoys are crucial for maritime applications. However, wireless data transmission in these environments faces significant challenges due to limited buoy battery capacity, harsh weather conditions, and potential interference from maritime vessels. LoRaWAN technology, known for its low power consumption and long-range communication capabilities, presents a promising solution. Nevertheless, the standard LoRaWAN framework lacks native support for multi-hop routing, which is essential for enhancing network efficiency by relaying data between buoys. This paper introduces two novel multi-hop routing protocols designed for resilient LoRaWAN mesh networks in maritime environments. The first, Opportunistic Smart Routing over a Decentralised LoRaWAN Mesh (OSR-DLM), employs a cross-layer design with a hybrid routing strategy and balanced metric selection. The second, Beacon-Forwarding LoRaWAN with Channel-Aware Path Selection (BF-LoRaCAPS), maintains continuous device awareness using a scheduling mechanism and integrates the OSR-DLM strategy for further optimisation. We evaluate these protocols through extensive simulations that model the detrimental effects of severe weather on data transmission, validated by analysing varied parameter settings in massive Maritime of Things (MoT) scenarios. Key performance metrics, including packet delivery ratio, end-to-end latency, throughput, and traffic intensity for each hop-ratio, are analysed. The results show the superiority of both OSR-DLM and BF-LoRaCAPS over conventional Geographic Routing Protocol (GRP) variants under realistic marine channel conditions. Notably, BF-LoRaCAPS exhibits superior network coverage and resilience, outperforming both OSR-DLM and GRP variants, albeit with slightly increased latency.},
  archive      = {J_COMCOM},
  author       = {Salah Eddine Elgharbi and Mauricio Iturralde and Yohan Dupuis and Alain Gaugue},
  doi          = {10.1016/j.comcom.2025.108276},
  journal      = {Computer Communications},
  month        = {9},
  pages        = {108276},
  shortjournal = {Comput. Commun.},
  title        = {Maritime monitoring through LoRaWAN: Resilient decentralised mesh networks for enhanced data transmission},
  volume       = {241},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Mitigating IoT botnet attacks: An early-stage explainable network-based anomaly detection approach. <em>COMCOM</em>, <em>241</em>, 108270. (<a href='https://doi.org/10.1016/j.comcom.2025.108270'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As the Internet of Things (IoT) continues to expand, botnet-driven threats pose a growing and severe risk to the security of IoT-enabled infrastructures. These threats exploit large numbers of compromised devices to establish covert control channels and, eventually, launch large-scale cyberattacks such as Distributed Denial of Service (DDoS), capable of severely disrupting critical services and causing substantial economic damage. This paper highlights the urgent need for detecting botnets at an early stage, particularly by identifying stealthy command and control (C&C) traffic that precedes the execution of such attacks. We propose an anomaly-based detection framework that combines semi-supervised learning with explainable Artificial Intelligence (XAI). Unlike most existing approaches, our method requires only benign traffic for training, thereby enabling the detection of previously unseen or evolving botnet threats without relying on labeled malicious data. The framework supports multiple traffic representations, including raw bytes, packet-level data, and unidirectional or bidirectional flows, enriched with diverse network features to enhance detection coverage and adaptability. Experimental evaluations using the IoT-23 dataset demonstrate a 99.51% detection rate and a 1.09% false positive rate for stealthy C&C communications, underscoring the method’s effectiveness and robustness. The integration of XAI enhances transparency and interpretability, enabling security professionals to better understand model decisions and refine detection strategies.},
  archive      = {J_COMCOM},
  author       = {Abdelaziz Amara Korba and Alaeddine Diaf and Mouhamed Amine Bouchiha and Yacine Ghamri-Doudane},
  doi          = {10.1016/j.comcom.2025.108270},
  journal      = {Computer Communications},
  month        = {9},
  pages        = {108270},
  shortjournal = {Comput. Commun.},
  title        = {Mitigating IoT botnet attacks: An early-stage explainable network-based anomaly detection approach},
  volume       = {241},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A pareto-based genetic algorithm for online task allocation in mobile crowdsensing. <em>COMCOM</em>, <em>241</em>, 108269. (<a href='https://doi.org/10.1016/j.comcom.2025.108269'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Mobile CrowdSensing (MCS) is a widely adopted sensing paradigm that utilizes the strength of multiple mobile users to carry out various location-based sensing tasks. The task allocation problem, which involves assigning tasks to suitable mobile users, is a critical issue in the design of MCS systems. Many existing studies neglect the time constraints associated with both participants and tasks, often ignoring task execution times and assuming that a task is considered completed once the participant arrives at the task location. In order to take execution time into consideration, this study suggests an online heterogeneous task allocation problem with time constraints. The objective is to minimize the reward while simultaneously maximizing sensing quality. To solve this problem, we offer an enhanced approach that is based on a genetic algorithm and uses a bipartite network to account for the assignability link between participants and tasks. Additionally, we employ Pareto optimization to balance the dual objectives of minimizing reward and maximizing task quality. Results from experiments show that the suggested algorithm works better than baseline methods in many experimental configurations.},
  archive      = {J_COMCOM},
  author       = {Xiangling Wu and Wenming Ma and Xiao Zhu and Shengyang Sun and Xiaoang Zhu},
  doi          = {10.1016/j.comcom.2025.108269},
  journal      = {Computer Communications},
  month        = {9},
  pages        = {108269},
  shortjournal = {Comput. Commun.},
  title        = {A pareto-based genetic algorithm for online task allocation in mobile crowdsensing},
  volume       = {241},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A novel fuzzy-logic-based adaptive gate-controlled scheduling algorithm for time-aware shaper in TSN. <em>COMCOM</em>, <em>241</em>, 108268. (<a href='https://doi.org/10.1016/j.comcom.2025.108268'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Time-sensitive networking (TSN) is critical for real-time, industrial, and mission-critical applications that require deterministic communication. Scheduling time-triggered flows in TSN’s time-aware shaper (TAS) mechanism constitute an NP-hard problem, where the inherent trade-off between computational complexity and scheduling optimality persists. Exact algorithms achieve precision via exhaustive search mechanisms at prohibitive costs, while heuristic algorithms sacrifice fidelity to accelerate execution under complex network scenarios. This paper addresses these challenges through a novel rule-based framework that employs a fuzzy logic system to dynamically select algorithms, ensuring adaptation to complex requirements in diverse scenarios. In addition, a dynamic switching algorithm is proposed to intelligently select the most suitable scheduling method based on real-time network conditions and task requirements. Compared with traditional exact algorithms, our approach reduces computation time by over 35% in large-scale networks while meeting time constraints. In small-scale networks, it increases the scheduling success ratio by 20% compared to heuristic methods, particularly when higher accuracy is required. The proposed framework establishes an innovative analytical perspective for TAS traffic scheduling challenges by enabling self-adaptive algorithm matching across varying scheduling demands, rather than constraining specific algorithms to predefined operational scenarios.},
  archive      = {J_COMCOM},
  author       = {Daqian Liu and Zhewei Zhang and Yuntao Shi and Yingying Wang and Zhenwu Lei},
  doi          = {10.1016/j.comcom.2025.108268},
  journal      = {Computer Communications},
  month        = {9},
  pages        = {108268},
  shortjournal = {Comput. Commun.},
  title        = {A novel fuzzy-logic-based adaptive gate-controlled scheduling algorithm for time-aware shaper in TSN},
  volume       = {241},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Latency-optimized multi-task collaborative computing mechanism based on NOMA-D2D for AIoT. <em>COMCOM</em>, <em>241</em>, 108266. (<a href='https://doi.org/10.1016/j.comcom.2025.108266'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the development of communication technologies such as NOMA-D2D, computing power and network resource sharing in the task collaboration process have shown great potential in AIoT. However, in the face of concurrent and low-latency task requirements, improper scheduling of computing and communication resources will lead to task timeout problems. This study proposes a latency-optimized multi-task collaborative computing mechanism, focusing on task offloading and resource optimization. First, a multi-task collaborative computing architecture based on a multi-hop NOMA-D2D cellular network is constructed, and a multi-task collaborative computing delay model is designed by analyzing co-frequency reuse interference. Secondly, the HGCG algorithm is designed to achieve the optimal alliance grouping based on hedonic game by comprehensively considering factors such as distance and resource supply and demand under dynamic network state changes. In addition, the HGCG-MAPPO algorithm is designed to achieve joint optimization of task offloading, transmission power and channel resource allocation decisions under complex environments. Simulation results show that the proposed method can dynamically adapt to network state changes, effectively reduce task completion delay by at least 14.97% compared with other methods, and show strong robustness under high task loads.},
  archive      = {J_COMCOM},
  author       = {Sujie Shao and Lili Su and Shaoyong Guo and Siya Xu and Xuesong Qiu},
  doi          = {10.1016/j.comcom.2025.108266},
  journal      = {Computer Communications},
  month        = {9},
  pages        = {108266},
  shortjournal = {Comput. Commun.},
  title        = {Latency-optimized multi-task collaborative computing mechanism based on NOMA-D2D for AIoT},
  volume       = {241},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Autonomous cyber defense for AIoT using graph attention network-enhanced reinforcement learning. <em>COMCOM</em>, <em>241</em>, 108265. (<a href='https://doi.org/10.1016/j.comcom.2025.108265'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The rapid development of the Artificial Intelligence of Things (AIoT) has established a highly interconnected device ecosystem, enhancing user convenience while exposing it to increasingly severe Advanced Persistent Threats (APTs). To address these challenges, defenders have increasingly adopted Deep Reinforcement Learning (DRL) methods, leveraging their self-learning and adaptability to strengthen network security. However, experiments reveal that existing DRL algorithms, which rely on one-dimensional observational data, often face challenges in adequately interpreting network topologies and inter-device interactions, thus impacting their defensive performance. To overcome this limitation, we propose integrating the weighting mechanism of Graph Attention Networks (GAT) to process complex network connectivity data, enabling intelligent defense agents to dynamically capture dependencies and interactions within intricate AIoT architectures and execute precise, rapid defensive actions. Accordingly, we combine GAT with DRL, integrating it with value-based Deep Q-Network (DQN) and policy-based Proximal Policy Optimization (PPO) algorithms to develop GAT-DQN and GAT-PPO. Extensive experiments on the Yawning Titan network security simulation platform demonstrate that GAT-DQN and GAT-PPO significantly outperform traditional methods, particularly as the complexity of scenarios increases, graph attention-enhanced DRL algorithms demonstrate greater stability and robustness in performance. These results highlight the effectiveness of graph-enhanced DRL in mitigating APTs, underscoring its potential for achieving autonomous network defense in future AIoT communication networks.},
  archive      = {J_COMCOM},
  author       = {Yihang Shi and Huajun Zhang and Lin Shi and Shoukun Xu},
  doi          = {10.1016/j.comcom.2025.108265},
  journal      = {Computer Communications},
  month        = {9},
  pages        = {108265},
  shortjournal = {Comput. Commun.},
  title        = {Autonomous cyber defense for AIoT using graph attention network-enhanced reinforcement learning},
  volume       = {241},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). IoT-ONDDQN: A detection model based on deep reinforcement learning for IoT data security. <em>COMCOM</em>, <em>241</em>, 108263. (<a href='https://doi.org/10.1016/j.comcom.2025.108263'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Internet of Things (IoT) devices are frequently targeted by malicious traffic, leading to data leaks and even device malfunctions. The variety and rapid evolution of IoT malicious traffic attacks pose a significant challenge to existing intrusion detection systems (IDS). Therefore, this paper uses the CIC-IoT2023 malicious traffic dataset specifically designed for IoT devices and builds detection models for OneR Deep Q Network (IoT-ODQN) and OneR Noisynet Double DQN (IoT-ONDDQN) based on Deep Reinforcement Learning (DRL). They use the OneR classifier for feature engineering, providing well-performing feature inputs for model training, and utilizing the experience replay mechanism to reuse and train samples repeatedly. IoT-ONDDQN replaces the ɛ -greedy strategy with Noisynet for more dynamic learning. The performance of these two detection models is compared with several existing Machine Learning (ML) and Deep Learning (DL) models. Experimental results show that IoT-ODQN and IoT-ONDDQN outperform the comparison models in terms of loss values, accuracy, precision, recall, and F1 score for the 8-classification and 34-classification tasks, DRL has better detection ability compared to ML and DL models. In addition, the improved IoT-ONDDQN outperforms the IoT-ODQN base model in all evaluation metrics and has better stability and robustness.},
  archive      = {J_COMCOM},
  author       = {Yutao Hu and Yongxin Feng and Yuntao Zhao and Xuedong Mao},
  doi          = {10.1016/j.comcom.2025.108263},
  journal      = {Computer Communications},
  month        = {9},
  pages        = {108263},
  shortjournal = {Comput. Commun.},
  title        = {IoT-ONDDQN: A detection model based on deep reinforcement learning for IoT data security},
  volume       = {241},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Dynamic data partitioning strategy for distributed learning on heterogeneous edge system. <em>COMCOM</em>, <em>241</em>, 108262. (<a href='https://doi.org/10.1016/j.comcom.2025.108262'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Distributed machine learning on edge systems has attracted attention due to the development of artificial intelligence and edge computing. One challenge is straggler problem for synchronous updates during training, in which some edge nodes that complete training first have to wait for the nodes that complete training later. This results in long waiting time and downgrades the performance of distributed learning. In this paper, we investigate dynamic data partition for load balance among heterogeneous edge nodes. We propose experience-driven algorithms based on actor–critic deep reinforcement learning to optimize model training in distributed edge systems. It can learn the network environment and the computing capabilities of edge nodes, and thus strategically allocate training data to edge nodes. We conduct experiments on two commonly used datasets, i.e., MNIST and CIFAR-10, to evaluate the performance of the proposed method. The results show that the proposed DDPS can significantly reduce training latency, compared to random partition strategy, even partition strategy, greedy partition strategy and A2C strategy.},
  archive      = {J_COMCOM},
  author       = {Kun Yu and Weiwen Zhang},
  doi          = {10.1016/j.comcom.2025.108262},
  journal      = {Computer Communications},
  month        = {9},
  pages        = {108262},
  shortjournal = {Comput. Commun.},
  title        = {Dynamic data partitioning strategy for distributed learning on heterogeneous edge system},
  volume       = {241},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A novel caching framework for information-centric IoT using deep reinforcement proximal policy optimization. <em>COMCOM</em>, <em>241</em>, 108261. (<a href='https://doi.org/10.1016/j.comcom.2025.108261'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Internet of Things (IoT) continues to evolve rapidly, necessitating innovative approaches to content delivery as the number of connected devices increases. Integrating Information-Centric Networking (ICN) within IoT environments offers a transformative solution, shifting from host-centric to content-centric architectures. This shift is particularly suitable for the distributed nature of IoT applications, which enhances content retrieval and distribution efficiency. However, the dynamic and diverse patterns of IoT networks require intelligent and adaptive caching mechanisms. This paper proposes an enhanced centrally controlled cache (ECCC) scheme that integrates the Proximal Policy Optimization (PPO) algorithm to optimize caching decisions in ICN-IoT environments. The ECCC scheme adapts in real time, adjusting caching strategies based on network conditions, resulting in improved network performance, higher energy efficiency, and reduced server load. The ECCC demonstrated an average energy savings of 15% and a cache-hit ratio improvement of 10% compared to traditional schemes. Extensive simulations demonstrate that ECCC outperforms traditional caching schemes, significantly improving the efficiency of the IoT application network and resource management. Furthermore, this work opens up new opportunities for smart cities, autonomous systems, and edge computing applications, where real-time data access and efficient resource management are critical.},
  archive      = {J_COMCOM},
  author       = {Hamid Asmat and Fasee Ullah and Arfat Ahmad Khan and Farman Ali and Muhammad Ismail Mohmand},
  doi          = {10.1016/j.comcom.2025.108261},
  journal      = {Computer Communications},
  month        = {9},
  pages        = {108261},
  shortjournal = {Comput. Commun.},
  title        = {A novel caching framework for information-centric IoT using deep reinforcement proximal policy optimization},
  volume       = {241},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Load-balanced scheduling optimization strategy for high-communication tasks in kubernetes with RDMA. <em>COMCOM</em>, <em>241</em>, 108260. (<a href='https://doi.org/10.1016/j.comcom.2025.108260'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Remote Direct Memory Access (RDMA) is a low-latency, high-bandwidth communication technology. Efficient and balanced RDMA load balancing and bandwidth utilization are critical for optimizing the performance of high-communication tasks in heterogeneous resource environments. However, the existing Kubernetes scheduling strategies fall short in balancing the RDMA resource loads and optimizing the distribution of TCP-Pods and RDMA-Pods, thereby hindering the performance of high-communication tasks that rely on RDMA bandwidth. To address these challenges, this paper proposes a load-balanced scheduling optimization strategy that integrates RDMA bandwidth, CPU, memory utilization, and node load balancing metrics for high-communication tasks in Kubernetes with RDMA. Specifically, the proposed strategy extends Kubernetes’ resource management capabilities to support dynamic monitoring of RDMA node bandwidth states. During the filtering phase, the nodes are categorized into RDMA nodes and TCP nodes, while Pods are classified as RDMA-Pods or TCP-Pods based on their communication requirements, corresponding to high-communication and ordinary tasks, respectively. In the scoring phase, a comprehensive scoring mechanism incorporates fairness factors and resource utilization metrics to ensure that high-communication tasks are preferentially allocated to appropriate RDMA nodes, thereby avoiding resource contention and suboptimal distribution. Additionally, the distribution of ordinary tasks is optimized to reduce interference with RDMA node resources. Experimental results show that the proposed scheduling strategy significantly improves the RDMA Load Balancing Rate, meets the performance demands of high-communication tasks, and enhances the overall resource utilization and scheduling efficiency within the cluster.},
  archive      = {J_COMCOM},
  author       = {Donglei Xiao and Wenhui Shen and Huiyue Yi and Wuxiong Zhang},
  doi          = {10.1016/j.comcom.2025.108260},
  journal      = {Computer Communications},
  month        = {9},
  pages        = {108260},
  shortjournal = {Comput. Commun.},
  title        = {Load-balanced scheduling optimization strategy for high-communication tasks in kubernetes with RDMA},
  volume       = {241},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Flexible and efficient privacy-preserving fine-grained access control scheme for data sharing in cloud-edge collaborative IoT. <em>COMCOM</em>, <em>241</em>, 108259. (<a href='https://doi.org/10.1016/j.comcom.2025.108259'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {—The cloud-edge collaborative Internet of Things (IoT) can provide a promising solution for real-time services. However, widespread concerns about the security of private data arise owing to the involvement of interest-oriented third parties. Although ciphertext-policy attribute-based encryption (CP-ABE) can achieve the fine-grained access control for outsourced data, existing schemes are unsuitable for cloud-edge collaborative IoT. Specifically, there are three main issues: 1) access policies in the form of plaintext or semi-ciphertext may reveal the privacy; 2) dynamically updating access policies for outsourced data is challenging, let alone in the state where the policies are fully hidden; and 3) the massive complex operations involved impose an enormous burden on resource-constrained end devices. To address these challenges, we proposed a flexible and efficient privacy-preserving ABE (FE2P-ABE) scheme that enabled the flexible policy updating in the fully hidden policy state. FE2P-ABE supports the large attribute universe and expressive access structure, and is friendly to resource-constrained devices, which greatly alleviates the computational burden on end devices by integrating online/offline encryption and verifiable outsourced decryption. Additionally, FE2P-ABE was formally proven to be selectively indistinguishable against the chosen plaintext attack, and the theoretical analysis and experimental simulation confirmed its superior performance in terms of storage and computation overhead while achieving comprehensive functionality.},
  archive      = {J_COMCOM},
  author       = {Sensen Li and Meigen Huang and Chen Fang and Bin Yu},
  doi          = {10.1016/j.comcom.2025.108259},
  journal      = {Computer Communications},
  month        = {9},
  pages        = {108259},
  shortjournal = {Comput. Commun.},
  title        = {Flexible and efficient privacy-preserving fine-grained access control scheme for data sharing in cloud-edge collaborative IoT},
  volume       = {241},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Altitude control and power allocation with reinforcement learning for sum capacity maximization in high-data-rate swarm drone interventions. <em>COMCOM</em>, <em>241</em>, 108257. (<a href='https://doi.org/10.1016/j.comcom.2025.108257'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Several efforts have been made to study the achievable performance gains of UAV-assisted communications, especially in emergency and disaster scenarios, as an alternative to traditional cellular communication, which is prone to damage. In such situations, first-hand responders require an aerial view of the terrain for situational awareness to assess the extent of damage and plan the nature and level of intervention required. Such applications require high data rates and proper UAV altitude control. Existing work on the 3D location of aerial base stations (ABS) does not study high-data-rate applications involving UAV interventions, which are critical and sometimes life-saving. Furthermore, existing work does not provide sufficient details on the mapping of state representations. It uses a fixed representation of the state space, which limits the exploration capabilities of the agent in the optimization process. Also, mutual distances affecting interference were not considered in the state space, whereas interference severely affects the sum capacity performance. In this paper, a reinforcement learning-based altitude control framework for sum capacity maximization using UAVs as ABSs is also proposed. In the scenario studied, portable ground stations, such as those that can be set up in an ad hoc manner, are assumed to assess the situation via the aerial footage of the ABS for situational awareness. In this case, the ABS should ascend whenever its location in 2D space is very close to those of the ground stations to provide extended coverage of their footage. A control mechanism is developed for UAV altitude. A new reward function, as well as, a fine-grained representation of the state transition and a more precise design of the state space, are introduced. ABS are positioned via the k -means algorithm with a large discrete search space. These ABS learn to select the transmit power that maximizes the sum capacity during training while also deploying the proposed altitude control mechanism. The proposed optimization framework improves the state-of-the-art Q-learning-based scheme without altitude control and equal power allocation algorithms, by over 20% and 55% for 8 and 16 UAVs, respectively.},
  archive      = {J_COMCOM},
  author       = {Oluwatosin Ahmed Amodu and Rosdiadee Nordin and Nor Fadzilah Abdullah and Asma’ Abu-Samah},
  doi          = {10.1016/j.comcom.2025.108257},
  journal      = {Computer Communications},
  month        = {9},
  pages        = {108257},
  shortjournal = {Comput. Commun.},
  title        = {Altitude control and power allocation with reinforcement learning for sum capacity maximization in high-data-rate swarm drone interventions},
  volume       = {241},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). UAV hovering location optimization for maximizing the throughput of IPv6 packet broadcast in wireless powered sensor network. <em>COMCOM</em>, <em>241</em>, 108256. (<a href='https://doi.org/10.1016/j.comcom.2025.108256'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In a Wireless Powered Sensor Network (WPSN) assisted by Unmanned Aerial Vehicles (UAVs), the UAVs provide wireless charging to the nodes in order to maintain uninterrupted functionality. Effective dissemination of IPv6 packets is essential in WPSN for many Internet of Things (IoT) applications, such as smart agriculture. Improving the efficiency of wireless charging to increase the transmission speed of IPv6 packets in WPSN is a critical problem. In this paper, we suggest using a Particle Swarm Optimization (PSO)-based approach to optimize the placement of UAVs. This method aims to optimize the places where UAVs hover in order to enhance the efficiency of charging nodes and ultimately boost network throughput. Our initial approach involves creating a method for broadcasting IPv6 packets in a WPSN by utilizing the combined power supply from several UAVs. This approach utilizes network coding technologies to improve the reliability of packet broadcasting. In addition, we transform the problem of IPv6 broadcasting into a unicast equivalent, resulting in a derived equation for throughput. Consequently, we establish an optimization problem where the positions of UAVs serve as variables, with the goal of maximizing network throughput as the objective function. An approach based on Particle Swarm Optimization (PSO) is developed to handle this optimization problem. The simulation results demonstrate that our method provides a throughput performance enhancement ranging from 10.39% to 70.46% compared to IFA (Improved Firefly Algorithm), Fixed, and Random solutions under various parameter configurations.},
  archive      = {J_COMCOM},
  author       = {Shuwei Qiu and Haiyan Shi and Mahammad Humayoo and Bin Qiu and Jianzhong Li and Xiaoqing Dong and Yinghui Zhu and Wei Huang},
  doi          = {10.1016/j.comcom.2025.108256},
  journal      = {Computer Communications},
  month        = {9},
  pages        = {108256},
  shortjournal = {Comput. Commun.},
  title        = {UAV hovering location optimization for maximizing the throughput of IPv6 packet broadcast in wireless powered sensor network},
  volume       = {241},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Maritime communication networks: A survey on architecture, key technologies, and challenges. <em>COMCOM</em>, <em>241</em>, 108255. (<a href='https://doi.org/10.1016/j.comcom.2025.108255'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Maritime Communication Networks (MCNs) are crucial for ensuring marine security, facilitating global trade, and advancing maritime scientific research. However, establishing a stable and efficient communication network is challenging due to the complex marine environment, such as tides, wind, waves, sea surface scattering, refraction, and evaporation ducts. Additionally, the non-uniform distribution of marine terminals and the differentiated service requirements further exacerbate its complexity. Recent advancements in multi-domain technologies (shipborne/airborne/satellite systems) have driven MCNs toward cognitive and adaptive architectures. This survey begins by outlining the current applications, demands and challenges of maritime communications. Next, we sequentially overview five MCN architectures, including the maritime mobile ad-hoc network (M-MANET), onshore BSs-assisted, satellite-assisted, UAV-assisted, and space–air–ground–sea integrated network (SAGSIN) architectures. Then, we elaborate on recent advancements in key generic technologies under different network architectures, covering spectrum strategies, beam management, multi-access edge computing, and routing. Finally, we discuss and analyze the open issues and challenges in MCNs, such as cross-domain resources collaborative optimization. This survey is intended to serve as a reference for researchers interested in MCNs.},
  archive      = {J_COMCOM},
  author       = {Ziqi Shang and Xian Zhang and Xuehua Li},
  doi          = {10.1016/j.comcom.2025.108255},
  journal      = {Computer Communications},
  month        = {9},
  pages        = {108255},
  shortjournal = {Comput. Commun.},
  title        = {Maritime communication networks: A survey on architecture, key technologies, and challenges},
  volume       = {241},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). UAV-assisted small base station ON-OFF switching in 6G cellular networks considering backhaul energy consumption. <em>COMCOM</em>, <em>241</em>, 108253. (<a href='https://doi.org/10.1016/j.comcom.2025.108253'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The emergence of 6th Generation (6G) cellular networks presents an opportunity to redefine Key Performance Indicators (KPIs) necessary for high-quality communications in the 2030s. 6G aims to innovate through novel architectural designs and the utilization of higher frequency bands, alongside incorporating aerial coverage to establish a three-dimensional network framework in contrast to its predecessor, 5G. Central to this innovation are Unmanned Aerial Vehicles (UAVs), which can be used as Drone Base Stations (DBSs). Despite the energy required for UAVs to hover, they can significantly decrease energy consumption and environmental impact by replacing terrestrial cellular infrastructure and switching off underutilized or inefficient Small Base Stations (SBSs) in Ultra-Dense Networks (UDNs). This work presents an energy-efficient UAV-assisted On-Off switching methodology that considers energy usage of DBSs’ backhaul links, in contrast to previous studies. By optimizing DBS placement, user association, and power control, the approach aims to improve energy efficiency. The problem is formulated as a Mixed-Integer Nonlinear Programming (MINLP) optimization, which is then decomposed into three manageable sub-problems that are solved using proposed algorithms. This methodological framework not only alleviates the complexity associated with the original problem but also enables practical implementations in energy-constrained UAV systems, ultimately leading to improved energy efficiency compared to existing approaches. Simulation results demonstrate about 90 % improvement of energy efficiency compared to prior studies even when fewer SBSs are switched off. Furthermore, the proposed approach exhibits 95 % better energy efficiency rather than previous methods when the serving time of UAVs increases.},
  archive      = {J_COMCOM},
  author       = {Samaneh Khadem Khorasani and Behrouz Shahgholi Ghahfarokhi and Naser Movahhedinia},
  doi          = {10.1016/j.comcom.2025.108253},
  journal      = {Computer Communications},
  month        = {9},
  pages        = {108253},
  shortjournal = {Comput. Commun.},
  title        = {UAV-assisted small base station ON-OFF switching in 6G cellular networks considering backhaul energy consumption},
  volume       = {241},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Securing ICS networks: SDN-based automated traffic control and MTD defensive framework against DDoS attacks. <em>COMCOM</em>, <em>241</em>, 108252. (<a href='https://doi.org/10.1016/j.comcom.2025.108252'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Industrial Control Systems (ICS) are increasingly targeted by distributed denial-of-service (DDoS) attacks, posing significant risks to system availability and reliability. This research proposes a novel defensive framework for ICS networks based on Software-Defined Networking (SDN). The main objectives are to enhance resilience against DDoS attacks and maintain critical system functions. Our framework combines automated traffic control (ATC) to filter and bypass malicious traffic dynamically, and Moving Target Defense (MTD) techniques such as proactive IP shuffling and network redundancy to protect critical nodes. Experimental results show that the proposed approach effectively reduces CPU load, improves round-trip time (RTT), and lowers packet drop rate (PDR) during DDoS scenarios. These findings demonstrate that integrating SDN-based ATC and MTD strategies can significantly strengthen ICS security and ensure system availability, providing a robust solution for critical infrastructure protection.},
  archive      = {J_COMCOM},
  author       = {Xingsheng Qin and Robin Doss and Frank Jiang and Xingguo Qin and Biyue Long},
  doi          = {10.1016/j.comcom.2025.108252},
  journal      = {Computer Communications},
  month        = {9},
  pages        = {108252},
  shortjournal = {Comput. Commun.},
  title        = {Securing ICS networks: SDN-based automated traffic control and MTD defensive framework against DDoS attacks},
  volume       = {241},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Fuzzy logic-based IDS (FLIDS) for the detection of different types of jamming attacks in IoT networks. <em>COMCOM</em>, <em>241</em>, 108251. (<a href='https://doi.org/10.1016/j.comcom.2025.108251'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Jamming attacks pose a critical threat to the security and reliability of Internet of Things (IoT) networks by disrupting communication and degrading network performance. Existing intrusion detection systems often struggle to provide real-time and adaptive defense mechanisms against sophisticated jamming techniques. This study introduces the Fuzzy Logic Intrusion Detection System (FLIDS), designed for efficient, real-time detection of jamming attacks in IoT environments. This work significantly extends previous research by providing a more comprehensive validation of FLIDS across a broader range of attack scenarios and system evaluations. FLIDS demonstrates high accuracy in identifying various jamming strategies, including constant, deceptive, random, reactive, and complex jammers, achieving detection rates of up to 97.51% for reactive jammers and 96.73% for deceptive jammers as detailed in this extended study. The system operates efficiently in resource-constrained IoT environments, enabling autonomous and distributed detection with minimal computational overhead. New analyses further explore its scalability and performance under dynamic conditions. Extensive simulations in Contiki OS and Cooja validate FLIDS’s effectiveness across diverse network topologies. These results establish FLIDS as a robust and practical solution for enhancing IoT security against jamming attacks.},
  archive      = {J_COMCOM},
  author       = {Michael Savva and Iacovos Ioannou and Vasos Vassiliou},
  doi          = {10.1016/j.comcom.2025.108251},
  journal      = {Computer Communications},
  month        = {9},
  pages        = {108251},
  shortjournal = {Comput. Commun.},
  title        = {Fuzzy logic-based IDS (FLIDS) for the detection of different types of jamming attacks in IoT networks},
  volume       = {241},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Robustness of decentralised learning to nodes and data disruption. <em>COMCOM</em>, <em>241</em>, 108250. (<a href='https://doi.org/10.1016/j.comcom.2025.108250'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the active landscape of AI research, decentralised learning is gaining momentum. Decentralised learning allows individual nodes to keep data locally where they are generated and to share knowledge extracted from local data among themselves through an interactive process of collaborative refinement. This paradigm supports scenarios where data cannot leave the data owner node due to privacy or sovereignty reasons or real-time constraints imposing proximity of models to locations where inference has to be carried out. The distributed nature of decentralised learning implies significant new research challenges with respect to centralised learning. Among them, in this paper, we focus on robustness issues. Specifically, we study the effect of nodes’ disruption on the collective learning process. Assuming a given percentage of “central” nodes disappear from the network, we focus on different cases, characterised by (i) different distributions of data across nodes and (ii) different times when disruption occurs with respect to the start of the collaborative learning task. Through these configurations, we are able to show the non-trivial interplay between the properties of the network connecting nodes, the persistence of knowledge acquired collectively before disruption or lack thereof, and the effect of data availability pre- and post-disruption. Our results show that decentralised learning processes are remarkably robust to network disruption. As long as even minimum amounts of data remain available somewhere in the network, the learning process is able to recover from disruptions and achieve significant classification accuracy. This clearly varies depending on the remaining connectivity after disruption, but we show that even nodes that remain completely isolated can retain significant knowledge acquired before the disruption.},
  archive      = {J_COMCOM},
  author       = {Luigi Palmieri and Chiara Boldrini and Lorenzo Valerio and Andrea Passarella and Marco Conti and János Kertész},
  doi          = {10.1016/j.comcom.2025.108250},
  journal      = {Computer Communications},
  month        = {9},
  pages        = {108250},
  shortjournal = {Comput. Commun.},
  title        = {Robustness of decentralised learning to nodes and data disruption},
  volume       = {241},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). BRI-MAC: Broadcast receiver initiated MAC protocol for internet of things. <em>COMCOM</em>, <em>241</em>, 108249. (<a href='https://doi.org/10.1016/j.comcom.2025.108249'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Efficient energy management in IoT networks, particularly at the MAC layer, is essential due to the excessive energy consumption of traditional protocols caused by retransmissions, idle listening, and control overhead. To address these challenges, this paper proposes BRI-MAC (Broadcast Receiver-Initiated MAC), a multi-hop duty cycle MAC protocol that enhances energy efficiency, reduces latency, and optimizes packet delivery. BRI-MAC employs a rendezvous beacon for transmission scheduling and integrates a collision resolution mechanism to minimize energy waste. In addition to performance improvements, BRI-MAC strengthens IoT security by mitigating DDoS, Sybil, and jamming attacks through adaptive scheduling and resilient multi-hop broadcasting. Simulation results demonstrate that BRI-MAC reduces end-to-end delay by 20 %, achieves a 97 % packet reception rate, maintains low energy consumption (30 %), and offers excellent scalability (90 %), compared to existing MAC protocols such as RI-MAC and EnRI-MAC. These findings highlight its superiority over conventional MAC protocols, making it a compelling solution for large-scale, energy-constrained IoT deployments.},
  archive      = {J_COMCOM},
  author       = {Lakhdar Goudjil and Samir Fenanir and Fouzi Semchedine and Mounir Zerroug},
  doi          = {10.1016/j.comcom.2025.108249},
  journal      = {Computer Communications},
  month        = {9},
  pages        = {108249},
  shortjournal = {Comput. Commun.},
  title        = {BRI-MAC: Broadcast receiver initiated MAC protocol for internet of things},
  volume       = {241},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A self-adaptive hybrid network anomaly detection engine. <em>COMCOM</em>, <em>241</em>, 108248. (<a href='https://doi.org/10.1016/j.comcom.2025.108248'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Network anomaly detection faces new challenges with the rising complexity of enterprise networks and cyber threats. This paper presents a Self-adaptive Hybrid Network Anomaly Detection Engine (denoted SHADE), that operates over the Activity and Event Networks (AEN) graph model. SHADE stems from the combination three key modules: (1) a two-stage detection framework that combines unsupervised pattern mining with supervised classification, (2) a self-adaptive threshold mechanism that dynamically adjusts to network behavior changes, and (3) a transfer learning module that enables zero-day attack detection. The unsupervised stage uses an autoencoder with Hierarchical Density-Based Spatial Clustering of Applications with Noise (HDBSCAN) to identify potential anomalies in AEN event sequences. The supervised stage employs an ensemble classifier to validate and categorize these anomalies. SHADE is evaluated using three widely-used public datasets: CICIDS2018, IoT-23, and UNSW-NB15, containing a combined total of 1 0 6 network events. Our results show that SHADE achieves 94.3% detection accuracy and 0.8% false positive rate. It is also demonstrated that SHADE achieves 87.2% accuracy in detecting previously unseen attacks.},
  archive      = {J_COMCOM},
  author       = {Amir Mohammadi Bagha and Isaac Woungang and Issa Traore and Danda B. Rawat},
  doi          = {10.1016/j.comcom.2025.108248},
  journal      = {Computer Communications},
  month        = {9},
  pages        = {108248},
  shortjournal = {Comput. Commun.},
  title        = {A self-adaptive hybrid network anomaly detection engine},
  volume       = {241},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). QUICL: Disruption-tolerant networking via a QUIC convergence layer. <em>COMCOM</em>, <em>241</em>, 108247. (<a href='https://doi.org/10.1016/j.comcom.2025.108247'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Disruption-tolerant networks (DTNs) have a wide range of applications, including emergencies where traditional communication infrastructure has been destroyed, remote rural deployments where communication infrastructure does not exist, and environmental monitoring in which animals are equipped with sensors and transmit data whenever they come into contact with a base station. Using the de-facto DTN protocol standard, Bundle Protocol version 7, nodes transmit data using Convergence Layer Protocols, which serve as abstractions for the underlying communication technology. In this article, we introduce QUICL , a novel convergence layer for disruption-tolerant networks. QUICL is built on the QUIC transport protocol, which offers advantages over TCP in a disruption-tolerant setting. In particular, it improves congestion control, supports multiplexing, ensures reliable transmission, effectively manages unstable networks, and encrypts traffic by default. Our implementation, already merged upstream, is based on the free and open-source DTN7-go -protocol suite and the QUIC-go -library. Our experimental evaluation shows that even in challenging situations such as bundle transmission over 63 hops with a packet loss of 30% on each hop, QUICL still delivers data where most other DTN software/convergence layer combinations fail to transmit any data.},
  archive      = {J_COMCOM},
  author       = {Markus Sommer and Artur Sterz and Markus Vogelbacher and Hicham Bellafkir and Bernd Freisleben},
  doi          = {10.1016/j.comcom.2025.108247},
  journal      = {Computer Communications},
  month        = {9},
  pages        = {108247},
  shortjournal = {Comput. Commun.},
  title        = {QUICL: Disruption-tolerant networking via a QUIC convergence layer},
  volume       = {241},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Analysis and implementation of discrete time markov chain based method for pilot decontamination in cognitive radio based ultra-dense networks. <em>COMCOM</em>, <em>241</em>, 108246. (<a href='https://doi.org/10.1016/j.comcom.2025.108246'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the context of beyond 5G (B5G) systems, the deployment of ultra-dense networks (UDNs) equipped with a large number of base-station antennas holds the promise of achieving high spectral efficiency (SE) through Cognitive Radio Network (CRN) technology. However, this approach introduce the issue of pilot contamination (PC), stemming from the reuse of pilots in adjacent cells, which has a detrimental impact on channel estimation and overall network performance. To address this issue comprehensively, we propose the SD-GraW-PD scheme for CRNs. Our scheme begins by categorizing users into two groups: cell-centered and cell-edged users. To further mitigate the effects of PC, we introduce a Discrete Time Markov Chain (DTMC)-based approach that leverages mutually orthogonal Graeco-Latin squares matrix pilot allocation. Additionally, we apply DTMC analysis to implement the Weighted Graph-Coloring Pilot Decontamination (WGC-PD) technique, enhancing the decontamination process, particularly for cell-edged users. Through extensive numerical simulations, we evaluate the proposed methodology. The results unequivocally demonstrate significant improvements in network performance, including enhancements in uplink achievable rate and SINR, in comparison to existing methods. Our DTMC-based approach emerges as an efficient and effective solution to the persistent PC problem within CR-based UDNs, offering promising prospects for achieving higher SE in the context of B5G systems.},
  archive      = {J_COMCOM},
  author       = {Subrat Kumar Sethi and Arunanshu Mahapatro},
  doi          = {10.1016/j.comcom.2025.108246},
  journal      = {Computer Communications},
  month        = {9},
  pages        = {108246},
  shortjournal = {Comput. Commun.},
  title        = {Analysis and implementation of discrete time markov chain based method for pilot decontamination in cognitive radio based ultra-dense networks},
  volume       = {241},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Crowd counting with WiFi sensing based on iterative attentional feature fusion. <em>COMCOM</em>, <em>241</em>, 108245. (<a href='https://doi.org/10.1016/j.comcom.2025.108245'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {—Crowd counting has great appeal for a variety of applications, such as public transportation, disaster management and building automation. Recently, WiFi-based crowd counting has gained dominance due to its ubiquitous and non-invasive advantages. However, current WiFi-based crowd counting systems have a limitation in that they do not consider the effect of dynamic crowds and static crowds on crowd counting. In contrast to previous studies, this paper investigates the effect of crowds in different states on crowd counting performance, and proposes a WiFi-based multi-state crowd counting system, which can not only count dynamic or static crowds, but also count joint dynamic and static crowds. By analyzing the effect of crowd states on the signal, we demonstrate that the channel state information (CSI) subcarrier distribution can indicate the count of crowds in different states. To this end, we adopt an iterative attentional feature fusion (IAFF) which allows for the fusion of amplitude and phase information from multiple antennas and adaptively assigns weights to amplitude and phase on multiple subcarriers, thus enabling the counting of crowds in various states. The experimental results show that the system has recognition accuracy of 99.38 % for static crowds, 95.94 % for dynamic crowds, and 97.57 % for joint dynamic and static crowds.},
  archive      = {J_COMCOM},
  author       = {BeiMing Yan and Yong Li and LiMeng Dong and ZeRong Ren and HuiMin Liu and Xiang Gao and Wei Cheng},
  doi          = {10.1016/j.comcom.2025.108245},
  journal      = {Computer Communications},
  month        = {9},
  pages        = {108245},
  shortjournal = {Comput. Commun.},
  title        = {Crowd counting with WiFi sensing based on iterative attentional feature fusion},
  volume       = {241},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Simulating extended reality traffic: An empirical model from user behavior to network packets. <em>COMCOM</em>, <em>241</em>, 108244. (<a href='https://doi.org/10.1016/j.comcom.2025.108244'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Several components in the design of next-generation networks, including user profiling and network slicing, rely on accurate models of traffic load. In this context, recent studies have focused on various video traffic categories, while traffic associated with extended reality (XR) services has received limited attention. This paper introduces a novel empirical model for 3D XR traffic, developed by encoding real Point Clouds using a standard-compliant codec, and able to account for the dynamic of service sessions and user behaviors over an entire session. Our methodology encompasses multiple temporal scales, ranging from milliseconds to minutes, to account for different phenomena related to both user behavior and encoder settings. Initially, we investigate the packet size distribution at the time scale of a semantic unit, corresponding to the encoding of a single point cloud. We verify that it can be effectively represented by a heavy-tailed Gamma distribution. Then, we illustrate how this insight can be leveraged to model application-layer phenomena. Specifically, we demonstrate the applicability of a general semi-hidden Markov model to capture both the temporal dynamics of service sessions and user behaviors. We provide results in terms of comparison of the empirical and fitting traffic distributions, based on quantile to quantile analysis and statistical tests. We also show how the model can be trained on real data and we provide a pseudo-code demonstrating the model application within a network simulator.},
  archive      = {J_COMCOM},
  author       = {Luca Mastrandrea and Alessandro Priviero and Gaetano Scarano and Stefania Colonnese and Tiziana Cattai},
  doi          = {10.1016/j.comcom.2025.108244},
  journal      = {Computer Communications},
  month        = {9},
  pages        = {108244},
  shortjournal = {Comput. Commun.},
  title        = {Simulating extended reality traffic: An empirical model from user behavior to network packets},
  volume       = {241},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multi-dimensional resource placement algorithm based on parallel genetic algorithm. <em>COMCOM</em>, <em>241</em>, 108235. (<a href='https://doi.org/10.1016/j.comcom.2025.108235'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the advancement of cloud-native technologies, container cluster management systems such as Kubernetes, Swarm, and Mesos have emerged. Due to its superior container orchestration capabilities, Kubernetes has been widely adopted across diverse domains and is now the industry-preferred solution for container cluster management. However, Kubernetes primarily relies on a single resource dimension for Pod placement, which often leads to imbalanced resource utilization and single-resource bottlenecks. To address this limitation, we optimize the Pod placement strategy in Kubernetes by designing a parallel genetic algorithm based on the island model, which accounts for multi-dimensional resource consumption in cloud-native environments. The genetic algorithm is tailored to the cloud-native context through enhancements in genetic coding design, initial population generation, and objective function formulation. By integrating the island model with genetic algorithms, our parallel optimization approach improves computational efficiency and addresses the NP-hard challenge of resource placement in cloud environments. Experimental results demonstrate that the proposed algorithm reduces the average single-prediction time by 42.5 %, achieves a cluster resource utilization rate of 93.77 %, and attains a parallel speedup ratio of 3.681. Furthermore, it mitigates resource imbalance and enhances utilization efficiency across clusters of varying scales.},
  archive      = {J_COMCOM},
  author       = {Qinlu He and Fan Zhang and Genqing Bian and Weiqi Zhang and Zhen Li},
  doi          = {10.1016/j.comcom.2025.108235},
  journal      = {Computer Communications},
  month        = {9},
  pages        = {108235},
  shortjournal = {Comput. Commun.},
  title        = {Multi-dimensional resource placement algorithm based on parallel genetic algorithm},
  volume       = {241},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Joint estimation method for space–time frequency parameters of frequency-hopping network station in the case of low-quality data. <em>COMCOM</em>, <em>241</em>, 108234. (<a href='https://doi.org/10.1016/j.comcom.2025.108234'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The parameter estimation of frequency-hopping networks is an important research direction in the field of communication electronic countermeasures(CECM). However, in urban scenarios, non-line-of-sight transmission and noise interference are common, leading to data loss in sensor networks during information acquisition, which poses significant challenges for the parameter estimation of frequency-hopping networks. To address the issue of missing observation data, this paper proposes an atomic norm soft thresholding method that combines time–frequency and angle information as joint feature vectors(AST-JFV) to improve the accuracy of parameter estimation under conditions of partial data loss. It achieves high-precision estimation of hopping time, instantaneous frequency, and direction of arrival for frequency-hopping signals. Experimental results show that with 20% data loss and a SNR of 10 dB, the proposed method achieves an accuracy of over 90% in estimating hopping time, an estimation error of less than 0.1 MHz for instantaneous frequency, and an angle estimation error controlled within 0.5°, with a computation time reduction of over 80%.},
  archive      = {J_COMCOM},
  author       = {Yuxiao Yang and Chang Xu and Xinyue Zhao and Junjie Li and Xiaoyu Dang},
  doi          = {10.1016/j.comcom.2025.108234},
  journal      = {Computer Communications},
  month        = {9},
  pages        = {108234},
  shortjournal = {Comput. Commun.},
  title        = {Joint estimation method for space–time frequency parameters of frequency-hopping network station in the case of low-quality data},
  volume       = {241},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Joint analysis of outage probability and energy efficiency in underlay cognitive radio-inspired NOMA systems using the AF relay in a downlink transmission. <em>COMCOM</em>, <em>241</em>, 108233. (<a href='https://doi.org/10.1016/j.comcom.2025.108233'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we investigate the performance of the underlay Cognitive Radio-Inspired Non-Orthogonal Multiple Access (NOMA) system in a downlink transmission. In our proposed model, a primary transmitter (PT) operating over a weak channel shares its frequency band with a secondary transmitter (ST) to enhance overall spectrum utilization. This work employs a coordinated direct and relay transmission (CDRT) strategy, where the relay operates in amplify-and-forward (AF) mode. We considered co-channel interference affecting both primary and secondary users. We present a novel decoding technique for the symbols of the primary and secondary users based on a combined signal-to-interference plus noise ratio (SINR) framework. This technique significantly improves the outage probability for both user types, resulting in enhanced system performance overall. The main advantage of this method lies in utilizing the combined SINR from both transmission phases, which enables a more robust decoding process. In this study, we derived a closed-form analytical expression for the outage probability of both PU and secondary users (SU). Additionally, we provided an asymptotic approximation of the outage probability at high signal-to-noise ratio (SNR) regimes. Our analytical results show that, at a high SNR, the system's performance based on the asymptotic approximation matches the simulation results, which validates the accuracy of our model. An essential aspect of this study is analyzing power allocation coefficients, as they directly affect system efficiency and energy utilization. In this regard, we derived the power allocation coefficients for the primary and secondary users and presented the corresponding analyses. Based on these coefficients, we calculated the SU's throughput, and system's overall energy efficiency. Our findings indicate that the proposed model reduces outage probability and increases throughput while also enhancing energy efficiency, which is crucial for sustainability and optimal resource utilization. Ultimately, this work can significantly enhance the performance of NOMA and cognitive radio (CR) systems, making it more applicable to the next generation of communication networks.},
  archive      = {J_COMCOM},
  author       = {Nourollah Davoudian and Hamidreza Bakhshi},
  doi          = {10.1016/j.comcom.2025.108233},
  journal      = {Computer Communications},
  month        = {9},
  pages        = {108233},
  shortjournal = {Comput. Commun.},
  title        = {Joint analysis of outage probability and energy efficiency in underlay cognitive radio-inspired NOMA systems using the AF relay in a downlink transmission},
  volume       = {241},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multi-objective optimization of advanced sleep mode for energy saving in cognitive radio network. <em>COMCOM</em>, <em>241</em>, 108232. (<a href='https://doi.org/10.1016/j.comcom.2025.108232'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Advanced Sleep Modes (ASM) concept corresponds to entering the Base Station (BS) progressively deeper and less energy-intensive states to reduce energy consumption. Introducing the ASM can mitigate energy wastage during low-traffic periods in the Cognitive Radio Network (CRN). In this study, we propose a strategy for integrating ASM within the CRN architecture to effectively handle primary and secondary traffic across varying ASM sleep states. Additionally, we study the general scenario of CRN with heterogeneous secondary users, imperfect sensing, and unreliable BS due to the arrival of negative packets (virus attack). By modeling the entire system as a three-dimensional discrete-time Markov chain, we conduct the transient analysis of the proposed model. Through numerical demonstrations involving reliability and queueing analyses, we substantiate the validity of the proposed model and examine the impact of reliability on its performance. Then, we showcased the effectiveness of the ASM strategy by comparing it with the Sleep Mode (SM) strategy in terms of the waiting time and blocking probability of the secondary user and the degree of energy savings. Also, simulation experiments are conducted to corroborate the accuracy and validity of the numerical results. Finally, we formulate the Cost Benefit Function (CBF), which depends on both the successful transmission and waiting time of secondary packets. Subsequently, we obtain the Pareto optimal solution for CBF and the degree of energy saving using the Non-dominated Sorting Genetic Algorithm II (NSGA-II) and Multi-Objective Particle Swarm Optimization (MOPSO) techniques for multi-objective optimization.},
  archive      = {J_COMCOM},
  author       = {Ajay Singh and Rakhee Kulshrestha and Vijaypal Poonia},
  doi          = {10.1016/j.comcom.2025.108232},
  journal      = {Computer Communications},
  month        = {9},
  pages        = {108232},
  shortjournal = {Comput. Commun.},
  title        = {Multi-objective optimization of advanced sleep mode for energy saving in cognitive radio network},
  volume       = {241},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Security optimization and beamforming design for active RIS-assisted UAV relaying NOMA networks. <em>COMCOM</em>, <em>241</em>, 108220. (<a href='https://doi.org/10.1016/j.comcom.2025.108220'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Physical layer security (PLS) in non-orthogonal multiple access (NOMA) networks faces critical challenges, especially for eavesdropping risk of cell-edge users in conventional passive reconfigurable intelligent surface (RIS)-assisted unmanned aerial vehicles (UAV) relaying systems, which may be increased because of channel sharing and multiplicative fading. Unlike ground-based RIS deployments limited by installation constraints and multi-path signal degradation, this work proposes an active RIS-assisted UAV (U-ARIS) for multi-user multiple-input single-output (MU-MISO) NOMA systems with two legitimate users and one eavesdropper. Moreover, we formulate a joint optimization problem to maximize sum secrecy rate by coordinating base station beamforming and U-ARIS reflection beamforming. Due to the multivariate nature of optimization variables and complex non-convexity, an alternating optimization (AO) algorithm is designed to decompose the problem into two sub-problems, which can be solved by semidefinite relaxation (SDR) and successive convex approximation (SCA). Simulations demonstrate that the proposed U-ARIS-assisted NOMA scheme achieves secrecy rate improvements of 12.5% and 50% compared to UAV-passive RIS (U-PRIS) and conventional OMA systems, respectively.},
  archive      = {J_COMCOM},
  author       = {Ruoyu Zhang and Songlin Cheng and Niansheng Chen and Guangyu Fan and Lei Rao and Xiaoyong Song and Dingyu Yang},
  doi          = {10.1016/j.comcom.2025.108220},
  journal      = {Computer Communications},
  month        = {9},
  pages        = {108220},
  shortjournal = {Comput. Commun.},
  title        = {Security optimization and beamforming design for active RIS-assisted UAV relaying NOMA networks},
  volume       = {241},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Digital twin-assisted multi-layer networks for low-latency and energy-efficient communication. <em>COMCOM</em>, <em>241</em>, 108219. (<a href='https://doi.org/10.1016/j.comcom.2025.108219'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The sixth-generation (6G) wireless networks are expected to provide ubiquitous connectivity, high data rate, low latency, energy efficiency, and edge intelligence for Internet of Things (IoT) applications. Digital twin technology is a promising solution to enable multi-layer wireless networks that incorporate IoT devices on the ground, unmanned aerial vehicles (UAVs) as mobile edge computing (MEC) servers, and cloud servers. Multi-layer processing can handle time-sensitive and computationally intensive tasks from IoT devices. This paper proposes a digital twin-assisted multi-layer network for low-latency and energy-efficient communication and computation. We mathematically formulate an optimization problem to minimize the latency and energy consumption of IoT devices by optimizing their association with the UAV-MECs, computation resources, communication resources, and offloading portions of tasks. We propose a two-stage scheme based on the K-means method and the deep neural network approach to solve the above optimization problem. We compare the proposed two-stage scheme with existing schemes to highlight the scalability of the proposed solution. Simulation results demonstrate that the proposed multi-layer network achieved optimization results comparable to existing schemes with less computational cost, highlighting its usefulness in achieving low latency and energy-efficient computation and communication.},
  archive      = {J_COMCOM},
  author       = {Muhammad Adnan Qadir and Muhammad Naeem and Waleed Ejaz},
  doi          = {10.1016/j.comcom.2025.108219},
  journal      = {Computer Communications},
  month        = {9},
  pages        = {108219},
  shortjournal = {Comput. Commun.},
  title        = {Digital twin-assisted multi-layer networks for low-latency and energy-efficient communication},
  volume       = {241},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). From 5G to 6G: Empowering vertical industries with next-gen technologies and trial facilities. <em>COMCOM</em>, <em>241</em>, 108218. (<a href='https://doi.org/10.1016/j.comcom.2025.108218'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the fifth-generation (5G) era, there are opportunities for innovation in various vertical industries, including Transport and Logistics (T&L) and automotive, among others. For instance, the automotive sector can advance with edge computing nodes connected through network slices, providing more reliable teleoperation of vehicles, and enhancing vehicle-to-everything (V2X) use cases. However, widespread adoption of these technologies (e.g., edge computing, teleoperation of vehicles, V2X) has been limited, partly due to a lack of resources to test and explore among verticals to experience the potential improvements that 5G and Beyond (B5G) networks can offer to them. Thus, establishing B5G trial facilities is crucial. These facilities enable real-life B5G deployments across various verticals and serve as collaborative ecosystems where industries, telecom providers, and academia can co-create tailored services to meet specific operational needs. As the sixth-generation (6G) era approaches, it is important to equip trial facilities with cutting-edge B5G technologies to support advanced vertical services. This article examines key 5G and B5G technologies for vertical industries, analyzing 40 major trial facilities to assess their capabilities. In addition, offering insights and recommendations to enhance trial facilities for testing and validating innovative vertical use cases in the 6G era.},
  archive      = {J_COMCOM},
  author       = {Vincent Charpentier and Miguel Camelo and Johann M. Marquez-Barja and Nina Slamnik-Kriještorac},
  doi          = {10.1016/j.comcom.2025.108218},
  journal      = {Computer Communications},
  month        = {9},
  pages        = {108218},
  shortjournal = {Comput. Commun.},
  title        = {From 5G to 6G: Empowering vertical industries with next-gen technologies and trial facilities},
  volume       = {241},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). AIRSDN: AI based routing in software-defined networks for multimedia traffic transmission. <em>COMCOM</em>, <em>240</em>, 108222. (<a href='https://doi.org/10.1016/j.comcom.2025.108222'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the rapid increase in internet usage and the number of network-connected devices, network management and optimization have become increasingly challenging, particularly for high-bandwidth applications such as video streaming. The decentralized structure of traditional networks and the lack of standardization further complicate these challenges. Software Defined Networking (SDN) has emerged as a solution, enabling a more flexible and programmable architecture by centralizing network control. However, existing SDN controllers typically determine the optimal path based on simple metrics such as hop count and bandwidth, which can be insufficient in high-traffic scenarios. To overcome these limitations, this study proposes a novel artificial intelligence (AI)-based routing algorithm. Operating within the SDN framework, the proposed algorithm analyzes network traffic levels and dynamically selects the most efficient data transmission paths. The proposed algorithm is simulated in Mininet, a virtual network environment, using a network model inspired by real-world internet structures (NSFNET). Simulations are conducted under varying traffic conditions, with TCP (Transport Control Protocol) data and video transmission scenarios. Key performance metrics are observed, including round-trip time (RTT), throughput, packet loss, and video quality (measured using PSNR and SSIM). The machine learning model was trained using a custom dataset consisting of 876 records generated in the Mininet environment. Although the dataset size is sufficient for the simulation environment, caution should be exercised when generalizing the results to real-world network conditions. Future studies may aim to enhance the model's reliability by exploring data augmentation techniques and utilizing larger datasets that include real-world data. To classify traffic levels, machine learning models are trained, and the best-performing model (Logistic Regression) is integrated into the proposed routing algorithm. The results demonstrate that the proposed AI-based routing algorithm significantly improves network performance compared to both traditional hop-count-based and QoS-aware routing. Particularly in high-traffic scenarios, it achieves lower latency, higher throughput, and better video quality. Additionally, resource usage was analyzed on a Raspberry Pi 5 device, revealing stable RAM consumption (∼50 %) and fluctuating CPU utilization (10–90 %), indicating the feasibility of lightweight deployment with awareness of processing load. This study highlights the potential of AI-driven SDN frameworks for adaptive and efficient network traffic management in high-demand applications, offering a robust solution for dynamic routing.},
  archive      = {J_COMCOM},
  author       = {Anıl Dursun İpek and Murtaza Cicioğlu and Ali Çalhan},
  doi          = {10.1016/j.comcom.2025.108222},
  journal      = {Computer Communications},
  month        = {8},
  pages        = {108222},
  shortjournal = {Comput. Commun.},
  title        = {AIRSDN: AI based routing in software-defined networks for multimedia traffic transmission},
  volume       = {240},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Periodic data collection in hybrid energy-harvesting sensor networks. <em>COMCOM</em>, <em>240</em>, 108217. (<a href='https://doi.org/10.1016/j.comcom.2025.108217'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We investigate wireless sensor networks with energy harvesting and periodic data collection by mobile sinks. We propose a Markov model to study the trade-off between the value of information at the mobile sink and the cost of data collection. In particular, we focus on the impact of the limited ability of sensor nodes to collect energy by incorporating an energy buffer with discretised energy chunks as an abstraction of the battery and energy harvesting dynamics. We also use a value information framework that accounts for the timeliness of the data by discounting the value of the information at the sensor over time.},
  archive      = {J_COMCOM},
  author       = {Dieter Fiems and Kishor Patil and Koen De Turck},
  doi          = {10.1016/j.comcom.2025.108217},
  journal      = {Computer Communications},
  month        = {8},
  pages        = {108217},
  shortjournal = {Comput. Commun.},
  title        = {Periodic data collection in hybrid energy-harvesting sensor networks},
  volume       = {240},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Disaster identification scheme based on federated learning and cognitive internet of vehicles. <em>COMCOM</em>, <em>240</em>, 108216. (<a href='https://doi.org/10.1016/j.comcom.2025.108216'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The rate of disaster occurrences has been increasing over the last decade due to the alarming effects of global warming. A major challenge with such disasters is identifying their nature before substantial loss of lives and property occurs. Existing systems often fail to determine the type of disaster until significant damage has been done. This research proposes a novel scheme to identify disasters as they occur, leveraging Federated Learning (FL) and Cognitive Internet of Vehicles (CIoV) since vehicles are a common presence in disaster scenarios. The proposed scheme utilizes various machine learning (ML) and deep learning algorithms to predict disaster types in real-time. Additionally, it introduces a custom federated averaging algorithm to maintain result privacy. The research evaluated the scheme’s performance using a data set of recorded disasters from various countries, training different algorithms to determine optimal results. The results indicate that the proposed scheme can achieve a 90% accuracy in disaster-type identification using deep learning and random forest algorithms.},
  archive      = {J_COMCOM},
  author       = {Muhammad Junaid Anjum and Muhammad Shoaib Farooq and Tariq Umer and Momina Shaheen},
  doi          = {10.1016/j.comcom.2025.108216},
  journal      = {Computer Communications},
  month        = {8},
  pages        = {108216},
  shortjournal = {Comput. Commun.},
  title        = {Disaster identification scheme based on federated learning and cognitive internet of vehicles},
  volume       = {240},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). CoDDoS: Detecting and mitigating diverse DDoS attacks with programmable switches. <em>COMCOM</em>, <em>240</em>, 108215. (<a href='https://doi.org/10.1016/j.comcom.2025.108215'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Over the past decades, DDoS attacks have dramatically evolved in attack scale and patterns. However, state-of-the-art DDoS defenses encounter challenges in maintaining high performance while handling large-scale network traffic. Most require rerouting traffic to a centralized collector (or analyzer), introducing additional processing latency and cost. Emerging programmable switches have become a promising means for conducting flexible DDoS defense directly on the data plane. However, due to the limited resources on each device and the lack of monitoring statistics from a network-wide perspective, these works have not provided sufficient capabilities to detect and mitigate diverse DDoS attacks. In this paper, we propose CoDDoS, a Collaborative DDoS defense system that combines device-local and network-wide monitoring information to defend against diverse DDoS attacks with programmable switches. For device-local estimation, we propose a resource-efficient sketch DBMin, to recognize suspicious DDoS attack flows from tremendous ongoing traffic. It is fully deployed on the data plane and achieves much lower memory consumption than existing approaches. We further initiate an In-band Network Telemetry mechanism to perform the network-wide measurement on suspicious flows detected by DBMin sketch. To defend against diverse DDoS attacks, CoDDoS enables a comprehensive investigation of the collected statistics and sets appropriate mitigation strategies for different DDoS attacks. The experiment results show that CoDDoS can achieve high detection accuracy for endpoint-based DDoS defense referring to F1 score/FNR/FPR metrics and effectively detect and mitigate link-based DDoS attacks.},
  archive      = {J_COMCOM},
  author       = {Jiqiang Xia and Le Tian and Yuxiang Hu and Ziyong Li and Penghao Sun and Jianhua Peng},
  doi          = {10.1016/j.comcom.2025.108215},
  journal      = {Computer Communications},
  month        = {8},
  pages        = {108215},
  shortjournal = {Comput. Commun.},
  title        = {CoDDoS: Detecting and mitigating diverse DDoS attacks with programmable switches},
  volume       = {240},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Secrecy analysis of orthogonal/nonorthogonal multiple access in energy scavenging-enabled short-packet transmission. <em>COMCOM</em>, <em>240</em>, 108214. (<a href='https://doi.org/10.1016/j.comcom.2025.108214'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Energy scavenging (ES)-enabled short-packet transmission (SPT) aims to attain low latency, high energy efficiency, high reliability by combining radio frequency ES with SPT. However, ES-enabled SPT (ESSPT) is susceptible to malicious overhearing attributable to broadcast feature of wireless transmission. To have prompt insights into secrecy capability of nonorthogonal/orthogonal multiple access (NOMA/OMA) in ESSPT, this paper proposes an analytical framework under realistic imperfections, such as arbitrary fading severity, hardware imperfection (HWi), nonlinear ES (nlES), channel state information imperfection (CSIi), and successive interference cancellation imperfection (SICi). Monte-Carlo simulations validate the proposed framework and expose the significant degradation of the secrecy capability of NOMA/OMA in ESSPT due to these imperfections. Moreover, the secrecy capability of NOMA/OMA in ESSPT can be maximized by selecting properly the transmit power and ES duration. Remarkably, in ESSPT, average effective secrecy rate sum and average secrecy throughput sum of OMA are almost double those of NOMA, indicating a significant superiority of OMA over NOMA in terms of security.},
  archive      = {J_COMCOM},
  author       = {Toi Le-Thanh and Khuong Ho-Van and Thiem Do-Dac},
  doi          = {10.1016/j.comcom.2025.108214},
  journal      = {Computer Communications},
  month        = {8},
  pages        = {108214},
  shortjournal = {Comput. Commun.},
  title        = {Secrecy analysis of orthogonal/nonorthogonal multiple access in energy scavenging-enabled short-packet transmission},
  volume       = {240},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Adaptive traffic splitting for transmission power minimization with QoS enhancement in 5G HetNets. <em>COMCOM</em>, <em>240</em>, 108206. (<a href='https://doi.org/10.1016/j.comcom.2025.108206'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we propose an adaptive data rate splitting algorithm for 5G HetNets comprised of both macro cells and small cells. Dual connectivity with data rate splitting has been considered a solution for achieving energy efficiency and enhancing network throughput. However, previous work did not optimize both transmission power and throughput for dual connectivity. For the minimization of transmission power with quality of service (QoS) enhancement, we employ a data rate splitting ratio for dual connectivity users. To that end, we formulate the problem of minimizing transmission power based on users’ minimum rates, and we derive a theoretical expression to obtain the optimal data rate splitting ratio. An algorithm is proposed for adaptive data rate splitting considering channel conditions and the UE data rate demands. The algorithm finds the data rate split ratio iteratively to minimize total transmission power and fulfill QoS requirements. Using NS-3, we verify the analysis and show that the proposed algorithm minimizes total transmission power while satisfying QoS requirements. In addition, we show the impact of user mobility and the number of small cells on the data rate splitting ratio.},
  archive      = {J_COMCOM},
  author       = {Abdul Manan and Syed Maaz Shahid and Nam-I Kim and Sungoh Kwon},
  doi          = {10.1016/j.comcom.2025.108206},
  journal      = {Computer Communications},
  month        = {8},
  pages        = {108206},
  shortjournal = {Comput. Commun.},
  title        = {Adaptive traffic splitting for transmission power minimization with QoS enhancement in 5G HetNets},
  volume       = {240},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A methodology for extracting and decoding smart contracts data. <em>COMCOM</em>, <em>240</em>, 108204. (<a href='https://doi.org/10.1016/j.comcom.2025.108204'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Blockchain technology has been widely adopted to enhance the security and the decentralisation of smart applications in large-scale pervasive systems. In such a context, data extraction is crucial as it provides a better understanding of the system’s behaviours. However, several challenges arise in automatically extracting data, due to the variety of data sources, such as transactions, events, contract storage, and the complexity of the blockchain structure. In particular, retrieving smart contract state changes remains unexplored despite its potential usage for discovering unexpected behaviour. For such reasons, in this work, we propose a novel methodology and a supporting application for extracting smart contract state changes and other execution-related data. The obtained data is then decoded and offered in a standard format to be easily reused. The methodology provides additional functionalities such as transaction filtering and capabilities for querying over extracted data. The effectiveness and the performance of the methodology were evaluated on three real-world projects from different EVM-based blockchains.},
  archive      = {J_COMCOM},
  author       = {Flavio Corradini and Alessandro Marcelletti and Andrea Morichetta and Barbara Re},
  doi          = {10.1016/j.comcom.2025.108204},
  journal      = {Computer Communications},
  month        = {8},
  pages        = {108204},
  shortjournal = {Comput. Commun.},
  title        = {A methodology for extracting and decoding smart contracts data},
  volume       = {240},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Analysis of supervised machine-learning techniques in computer networks attack detection. <em>COMCOM</em>, <em>240</em>, 108203. (<a href='https://doi.org/10.1016/j.comcom.2025.108203'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In an era marked by an increasing reliance on technology in our daily lives, the imperative to ensure the availability and security of infrastructures supporting system operations is evident. This commitment is crucial for guaranteeing service quality, delivering a positive end-user experience, and optimizing resource utilization. Against this backdrop, the integration of new technologies, such as artificial intelligence and machine-learning, becomes essential to enhance the agility of problem detection and mitigate potential impacts. The study presented in this paper delves into an analysis of various supervised classifier machine-learning methods applied to data collected from network equipment, specifically switches. The primary objective is to detect attacks within the network infrastructure of a higher education institution. The attacks were categorized into distinct signatures, forming datasets instrumental in the comparative assessment of machine-learning techniques. The models derived from these methods demonstrated promising results, achieving an impressive 99.88% in the Weighted F1 metric and 99.23% in Balanced Accuracy. Beyond traditional metrics, the study also considered critical factors such as training time, prediction time, and saved file size for a comprehensive evaluation of the methods. This multifaceted analysis aids in identifying the most suitable method, taking into account not only classification performance but also practical considerations associated with real-world deployment.},
  archive      = {J_COMCOM},
  author       = {Domingos S.F. Paes and Carlos H.V. de Moraes and Bruno G. Batista},
  doi          = {10.1016/j.comcom.2025.108203},
  journal      = {Computer Communications},
  month        = {8},
  pages        = {108203},
  shortjournal = {Comput. Commun.},
  title        = {Analysis of supervised machine-learning techniques in computer networks attack detection},
  volume       = {240},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Time varying channel estimation for RIS assisted network with outdated CSI: Looking beyond coherence time. <em>COMCOM</em>, <em>240</em>, 108202. (<a href='https://doi.org/10.1016/j.comcom.2025.108202'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The channel estimation (CE) overhead for unstructured multipath-rich channels increases linearly with the number of reflective elements of reconfigurable intelligent surface (RIS). This results in a significant portion of the channel coherence time being spent on CE, reducing data communication time. Furthermore, due to the mobility of the user equipment (UE) and the time consumed during CE, the estimated channel state information (CSI) may become outdated during actual data communication. In recent studies, the timing for CE has been primarily determined based on the coherence time interval, which is dependent on the velocity of the UE. However, the effect of the current channel condition and pathloss of the UEs can also be utilized to control the duration between successive CE to reduce the overhead while still maintaining the quality of service. Furthermore, for multi-user systems, the appropriate coherence time intervals of different users may be different depending on their velocities. Therefore CE carried out ignoring the difference in coherence time of different UEs may result in the estimated CSI being detrimentally outdated for some users. In contrast, others may not have sufficient time for data communication. To this end, based on the throughput analysis on outdated CSI, an algorithm has been designed to dynamically predict the next time instant for CE after the current CSI acquisition. In the first step, optimal RIS phase shifts to maximize channel gain is computed. Based on this and the amount of degradation of signal to interference plus noise ratio due to outdated CSI, transmit powers and bandwidth are allocated for the UEs and finally the next time instant for CE is predicted such that the aggregated throughput is maximized. Simulation results confirm that our proposed algorithm outperforms the coherence time-based strategies and an existing algorithm that adaptively changes inter CE duration.},
  archive      = {J_COMCOM},
  author       = {Souvik Deb and Sasthi C. Ghosh},
  doi          = {10.1016/j.comcom.2025.108202},
  journal      = {Computer Communications},
  month        = {8},
  pages        = {108202},
  shortjournal = {Comput. Commun.},
  title        = {Time varying channel estimation for RIS assisted network with outdated CSI: Looking beyond coherence time},
  volume       = {240},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Standardizing the evaluation framework for ECG-based authentication in IoT devices. <em>COMCOM</em>, <em>240</em>, 108201. (<a href='https://doi.org/10.1016/j.comcom.2025.108201'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Devices on the Internet of Things (IoT) often have constrained resources and operate in diverse environments, making them vulnerable to unauthorized access and cyber threats. Electrocardiogram (ECG) signals have emerged as a promising biometric for authenticating users in such settings. However, current ECG-based authentication studies lack a standardized evaluation framework tailored to resource-limited IoT contexts and long-term usage, making it difficult to assess their practical reliability. In this paper, we introduce a new evaluation framework for ECG-based authentication on IoT devices and construct a standardized dataset to facilitate rigorous testing. We categorize performance metrics into four key dimensions: scalability, adaptability, efficiency, and cancelability. Using this framework, we evaluate four representative ECG authentication algorithms for IoT devices. The results show that these algorithms struggle to maintain consistent performance under cross-session authentication scenarios. These findings highlight the critical importance of addressing the temporal variability of ECG signals and the current gap in robust ECG-based authentication for IoT devices. We believe the proposed framework will guide future research toward more resilient and secure ECG authentication systems for the IoT.},
  archive      = {J_COMCOM},
  author       = {Bonan Zhang and Lin Li and Chao Chen and Ickjai Lee and Kyungmi Lee and Kok-Leong Ong},
  doi          = {10.1016/j.comcom.2025.108201},
  journal      = {Computer Communications},
  month        = {8},
  pages        = {108201},
  shortjournal = {Comput. Commun.},
  title        = {Standardizing the evaluation framework for ECG-based authentication in IoT devices},
  volume       = {240},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A privacy-enhancing and lightweight framework for device-free localization-based AIoT system. <em>COMCOM</em>, <em>240</em>, 108200. (<a href='https://doi.org/10.1016/j.comcom.2025.108200'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the growing demand for location-based services in smart cities, Artificial Intelligence of Things (AIoT)-enabled device-free methods have gained attention for their ability to address privacy and usability challenges. WiFi-based target localization, leveraging channel state information, offers advantages such as ease of deployment and obstacle penetration but faces privacy and computational challenges in centralized training. To address these issues, we propose a privacy-enhancing and lightweight federated device-free localization framework (PLDFL). The PLDFL integrates local differential privacy in federated learning to safeguard user data, uses the Fisher Information Matrix for model pruning to reduce model complexity, and employs three-dimensional convolutional neural network (3DCNN) for efficient feature extraction. Experimental results on real-world data validate its effectiveness in achieving accurate, private, and lightweight device-free localization.},
  archive      = {J_COMCOM},
  author       = {Haoda Wang and Chen Zhang and Lingjun Zhao and Huakun Huang and Chunhua Su},
  doi          = {10.1016/j.comcom.2025.108200},
  journal      = {Computer Communications},
  month        = {8},
  pages        = {108200},
  shortjournal = {Comput. Commun.},
  title        = {A privacy-enhancing and lightweight framework for device-free localization-based AIoT system},
  volume       = {240},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). An analysis of pervasive payment channel networks for central bank digital currencies. <em>COMCOM</em>, <em>240</em>, 108199. (<a href='https://doi.org/10.1016/j.comcom.2025.108199'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The recent advancement of blockchain technology presents interesting opportunities that are worth a systematic investigation for their potential use in a Central Bank Digital Currency (CBDC). A blockchain alone has known scalability issues that can be overcome by, e.g., a layer-2 payment channel network (PCN). However, not all aspects of such a PCN are easy to specify and optimize. Therefore, its overall behavior, given the multitude of decentralized and individually configured nodes, is challenging to fully comprehend. In this paper, we consider a two-layer hypothetical CBDC in which the wholesale layer utilizes a permissioned blockchain, which ensures high integrity and verifiability, while the retail layer leverages an off-ledger PCN model (with pervasive nodes distributed on a large-scale) that supports instant, privacy-preserving, and retail payments. We systematically analyze the performances of two families of PCNs, namely SF-PCNs and SH-PCNs, characterized respectively by a Scale-Free topology and a Semi-Hierarchical topology. Through extensive simulations and analyses, we offer insights into optimizing PCN structures for CBDCs by exploring the trade-offs among liquidity locked by market operators, payment success rate, throughput, payment completion time, as well as load on the underlying blockchain. Although both SH-PCNs and SF-PCNs can offer state-of-the-art guarantees of fault-tolerance and integrity, we demonstrate that SH-PCNs are better suited for handling large volumes of payments, scale better with the number of network nodes, are more aligned with the anatomy of the current monetary and financial system, and therefore should be preferred in CBDC designs.},
  archive      = {J_COMCOM},
  author       = {Marco Benedetti and Francesco De Sclavis and Marco Favorito and Giuseppe Galano and Sara Giammusso and Antonio Muci and Matteo Nardelli},
  doi          = {10.1016/j.comcom.2025.108199},
  journal      = {Computer Communications},
  month        = {8},
  pages        = {108199},
  shortjournal = {Comput. Commun.},
  title        = {An analysis of pervasive payment channel networks for central bank digital currencies},
  volume       = {240},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Empowering disaster response: Advanced network slicing solutions for reliable wi-fi and 5G communications. <em>COMCOM</em>, <em>240</em>, 108198. (<a href='https://doi.org/10.1016/j.comcom.2025.108198'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the context of increasing global challenges, such as climate change, geopolitical unrest, and a range of natural disasters, the need for robust Public Protection and Disaster Relief (PPDR) management strategies has emerged to face phases such as risk mitigation, preparations, and recovery mechanisms. Within such a scenario, communication technologies are an essential part of emergency-response strategies. Both cellular and non-cellular networks, such as 5G and Wi-Fi, are crucial in supporting PPDR operations. 5G networks are essential for covering extensive areas, for instance, enabling video surveillance through drones, whereas Wi-Fi networks are better suited for localized applications, such as in temporary shelters or field hospitals. During environmental disasters, these networks often experience substantial traffic loads due to the high demand for diverse services, each with varying network requirements, such as enhanced Mobile Broadband (eMBB) and Ultra-Reliable Low-Latency Communications (URLLC). The increased network load poses a risk of impairing PPDR services, as the network may be unable to meet the stringent requirements necessary for first responders. Consequently, the implementation of Network Slicing techniques becomes critical to ensure flexibility, isolation, and dynamic prioritization of disaster management services, thereby guaranteeing the necessary network performance for effective emergency response. In this paper, we deploy real-life Network Slicing mechanism for Wi-Fi and 5G networks, to guarantee the network requirements for PPDR services. We evaluate the performance of Wi-Fi and 5G networks (throughput, latency, and packet loss), separately, for different scenarios. First, we introduce a dynamic Network Slicing mechanism for Wi-Fi networks. This mechanism, based on Software-Defined Networking (SDN) and In-band Network Telemetry (INT), incorporates an algorithm that mimics human-like reasoning to dynamically allocate network resources, such as airtime, using physical Wi-Fi equipment in real-world environments. Subsequently, we present a Network Slicing configuration for a real-world 5G network, deployed using actual hardware, utilizing a modular Open Radio Access Network (O-RAN) architecture.},
  archive      = {J_COMCOM},
  author       = {Xhulio Limani and Gilson Miranda Jr. and Joao Nunes Pinheiro and Xiaoman Shen and Chun Pan and Xingfeng Jiang and Chi Zhang and Johann M. Marquez-Barja and Nina Slamnik-Kriještorac},
  doi          = {10.1016/j.comcom.2025.108198},
  journal      = {Computer Communications},
  month        = {8},
  pages        = {108198},
  shortjournal = {Comput. Commun.},
  title        = {Empowering disaster response: Advanced network slicing solutions for reliable wi-fi and 5G communications},
  volume       = {240},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). The application of 5G networks on construction sites and in underground mines: Successful outcomes from field trials - Extended version. <em>COMCOM</em>, <em>240</em>, 108175. (<a href='https://doi.org/10.1016/j.comcom.2025.108175'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The fifth generation of mobile communications, 5G, has been introduced to various application domains, enabling significant progress towards networked and adaptive systems. As the development of 5G was specifically tailored towards the requirements of industry, a broad knowledge of the technology has been built up in industrial production while also contributing to increasing the spread of data-driven technologies like machine learning. Other application domains are still lacking the widespread use and adoption of digital and data-driven technologies. The objective of the research project 5G.NAMICO is to utilize the gained expertise and knowledge from industrial production and contribute to the adoption of 5G in the application domains of construction and underground mining. We set up trial sites on a construction site and in an underground mine to determine how a 5G network must be designed to meet the domain-specific requirements. Use cases were designed and implemented to verify the functionality as well as the benefits of the employed 5G networks. First network tests were conducted showing the potential of 5G to enable an end-to-end coverage, which provides the basis for the use of digital and data-driven technologies in the application domains of construction and underground mining.},
  archive      = {J_COMCOM},
  author       = {Johannes Josef Emontsbotz and Hyung Joo Lee and Sarah Schmitt and Maximilian Brochhaus and Ajith Krishnan and Johannes Lukas Sieger and Victoria Jung and Sigrid Brell-Cokcan and Niels König and Robert H. Schmitt},
  doi          = {10.1016/j.comcom.2025.108175},
  journal      = {Computer Communications},
  month        = {8},
  pages        = {108175},
  shortjournal = {Comput. Commun.},
  title        = {The application of 5G networks on construction sites and in underground mines: Successful outcomes from field trials - Extended version},
  volume       = {240},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). LEBFL: Lightweight authentication and efficient consensus for blockchained federated learning in vehicle–road cooperation systems with AIoT. <em>COMCOM</em>, <em>239</em>, 108196. (<a href='https://doi.org/10.1016/j.comcom.2025.108196'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Artificial Intelligence Internet of Things (AIoT) technology is gradually overcoming challenges related to traffic data transmission and processing in vehicle-road cooperative systems. However, the dynamism and openness of the vehicle-road cooperative networks make it susceptible to potential attacks, where attackers might intercept or tamper with transmitted local model parameters, thereby compromising the integrity of the models and leaking user privacy. Although existing solutions such as differential privacy and encryption can address these issues, they may reduce data availability or increase computational complexity. To tackle these challenges, we propose a lightweight authentication and efficient consensus for blockchained federated learning in vehicle–road cooperation systems(LEBFL), which provides lightweight privacy-enhanced authentication and efficient consensus while ensuring the privacy of local models and datasets. Specifically, we first introduce a blockchain-based federated learning architecture that enhances privacy and efficient consensus, utilizing the consortium blockchain to replace the centralized server. Subsequently, we design a lightweight anonymous authentication and key agreement protocol using efficient cryptographic primitives to establish secure session keys for the transmission of local models. Furthermore, we propose a utility-based Raft consensus algorithm, which selects the optimal fog server as the leader node using a resource matrix and weight vector, and enhances the performance of the blockchain network by leveraging the idle computing resources of fog servers. Security analysis and experimental results confirm that the proposed scheme shows superior performance without sacrificing security.},
  archive      = {J_COMCOM},
  author       = {Peng Liu and Qian He and Sen Li and Yiting Chen and Anfeng Liu},
  doi          = {10.1016/j.comcom.2025.108196},
  journal      = {Computer Communications},
  month        = {7},
  pages        = {108196},
  shortjournal = {Comput. Commun.},
  title        = {LEBFL: Lightweight authentication and efficient consensus for blockchained federated learning in vehicle–road cooperation systems with AIoT},
  volume       = {239},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Adaptive layer-wise personalized federated learning via dual delay update in future communication networks. <em>COMCOM</em>, <em>239</em>, 108195. (<a href='https://doi.org/10.1016/j.comcom.2025.108195'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The future communication networks refers to large-scale mass-connected networks consisting of billions of cloud, edge, and end devices, which are expected to support the ever-growing communication demands. In the future communication networks, billions of end devices generate massive amount of data that needs to be processed and analyzed ( e.g. , model training). Artificial Intelligence of Things (AIoT) is a groundbreaking technology that leverages artificial intelligence models to process and analyze data generated by a large number of internet of things devices. As an emerging AIoT method, personalized Federated Learning (pFL) has emerged prominently in distributed model training using massive data from the future communication networks. However, it is challenging to accomplish high-performance and communication-efficient model training by existing pFL methods in the future communication networks, due to the following limitations. a) Dynamic role differences in each layer of a multi-layer model are neglected, leading to poor accuracy in customized models deployed on end devices. b) Owing to numerous end devices in the future communication networks, the communication frequency between a cloud server and end devices is extremely high in each communication round, resulting in expensive communication cost. To solve these two limitations, this paper presents a novel pFL framework for distributed model training in the future communication networks, called Adaptive Layer-wise personalized Federated Learning via Dual Delay Update (ALpFLDDU). First, in end devices, a layer-wise aggregation scheme based on an adaptive weight calculation mechanism is designed to capture the dynamic role differences of model layers. Second, in each communication round, we develop a dual delay update strategy to reduce communication frequency between a cloud server and end devices while ensuring model performance. Simulation experiments on text and image classification datasets are conducted. The experimental results show that ALpFLDDU realizes higher classification precision and lower communication cost than advanced pFL benchmarks on various classification tasks.},
  archive      = {J_COMCOM},
  author       = {Haowen Xu and Yingchi Mao and Si Chen and Yi Rong and Tasiu Muazu and Xiaoming He},
  doi          = {10.1016/j.comcom.2025.108195},
  journal      = {Computer Communications},
  month        = {7},
  pages        = {108195},
  shortjournal = {Comput. Commun.},
  title        = {Adaptive layer-wise personalized federated learning via dual delay update in future communication networks},
  volume       = {239},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Identification of path congestion status for network performance tomography using deep spatial–temporal learning. <em>COMCOM</em>, <em>239</em>, 108194. (<a href='https://doi.org/10.1016/j.comcom.2025.108194'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Network tomography plays a crucial role in assessing the operational status of internal links within networks through end-to-end path-level measurements, independently of cooperation from the network infrastructure. However, the accuracy of performance inference in internal network links heavily relies on comprehensive end-to-end path performance data. Most network tomography algorithms employ conventional threshold-based methods to identify congestion along paths, while these methods encounter limitations stemming from network complexities, resulting in inaccuracies such as misidentifying abnormal links and overlooking congestion attacks, thereby impeding algorithm performance. This paper introduces the concept of Additive Congestion Status to address these challenges effectively. Using a framework that combines Adversarial Autoencoders (AAE) with Long Short-Term Memory (LSTM) networks, this approach robustly categorizes (as uncongested, single-congested, or multiple-congested) and quantifies (regarding the number of congested links) the Additive Congestion Status. Leveraging prior path information and capturing spatio-temporal characteristics of probing flows, this method significantly enhances the localization of congested links and the inference of link performance compared to conventional network tomography algorithms, as demonstrated through experimental evaluations.},
  archive      = {J_COMCOM},
  author       = {Chengze Du and Zhiwei Yu and Xiangyu Wang},
  doi          = {10.1016/j.comcom.2025.108194},
  journal      = {Computer Communications},
  month        = {7},
  pages        = {108194},
  shortjournal = {Comput. Commun.},
  title        = {Identification of path congestion status for network performance tomography using deep spatial–temporal learning},
  volume       = {239},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Disjoint end-to-end walks for service function chain provisioning and protection. <em>COMCOM</em>, <em>239</em>, 108193. (<a href='https://doi.org/10.1016/j.comcom.2025.108193'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Service Function Chains (SFCs) enable traffic to flow through Virtual Network Functions (VNFs) deployed on physical servers to deliver network services. While SDN (Software Defined Network)/NFV (Network Function Virtualization) provides flexible network management, ensuring service continuity remains challenging and requires robust protection mechanisms. Network failures can be addressed through local or end-to-end protection approaches. Local protection relies on multiple detours, potentially protecting against failures of the same components, which leads to complex management and multiple detour activations for single failures. End-to-end protection ensures service continuity through two end-to-end disjoint SFC provisioning paths, significantly reducing both route maintenance overhead and resource allocations. This paper addresses the challenge of finding two end-to-end disjoint paths for SFC provisioning and protection. We first model the problem using ILP and prove its NP -completeness, even in over-resourced networks where single SFC provisioning is polynomial-time solvable. To address this complexity, we propose a novel three-step heuristic that enhances protection through transient route computation that is intended to “leave room” and facilitate disjoint provisioning identification. Recomputation of the transient route is established to enhance the resource allocation while improving the protection efficiency. Our extensive simulations demonstrate significant improvements over conventional approaches, showing notable enhancement in both protection efficiency and SFC path quality without incurring additional costs.},
  archive      = {J_COMCOM},
  author       = {Mohand Yazid Saidi},
  doi          = {10.1016/j.comcom.2025.108193},
  journal      = {Computer Communications},
  month        = {7},
  pages        = {108193},
  shortjournal = {Comput. Commun.},
  title        = {Disjoint end-to-end walks for service function chain provisioning and protection},
  volume       = {239},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). An anonymous and privacy-preserving lightweight authentication protocol for secure communication in UAV-assisted IoAV networks. <em>COMCOM</em>, <em>239</em>, 108192. (<a href='https://doi.org/10.1016/j.comcom.2025.108192'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the rapid proliferation of the Internet of Things (IoT), autonomous vehicles (AVs), or self-driving cars, rely heavily on real-time data sharing and message exchanges over wireless networks. AVs use sensors, artificial intelligence, machine learning, and advanced algorithms to perform various functions, enabling users to operate without human intervention. Owing to the flexibility and high mobility of drones, they could aid in the operations of AVs. However, the security and privacy are the main concerns; specifically, the threat of physical capture and violation of anonymity are the main hurdles for realization of secure communication among the AVs and drones. To address these challenges, we propose an anonymous and provably Secure Lightweight Authentication Protocol for unmanned-aerial-vehicle-assisted Internet of Autonomous Vehicles (SLAP-IoAV). The proposed protocol uses cryptographic primitives such as exclusive-OR operations, elliptic-curve cryptography, collision-resistant one-way hashing, and concatenation to ensure robust security. An informal security analysis found that SLAP-IoAV is secure against several known attacks, and a performance analysis established that the protocol has less computational and communication overhead than existing competitive protocols. Additionally, Scyther simulation results confirm that no security vulnerabilities are present. Overall, our protocol delivers superior security and performance, making it well-suited to real-world applications in the AV industry.},
  archive      = {J_COMCOM},
  author       = {Mohd Shariq and Norziana Jamil and Gopal Singh Rawat and Shehzad Ashraf Chaudhry and Mehedi Masud and Angelo Cangelosi},
  doi          = {10.1016/j.comcom.2025.108192},
  journal      = {Computer Communications},
  month        = {7},
  pages        = {108192},
  shortjournal = {Comput. Commun.},
  title        = {An anonymous and privacy-preserving lightweight authentication protocol for secure communication in UAV-assisted IoAV networks},
  volume       = {239},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). FedBH: Efficient federated learning through shared base and adaptive hyperparameters in heterogeneous systems. <em>COMCOM</em>, <em>239</em>, 108190. (<a href='https://doi.org/10.1016/j.comcom.2025.108190'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Federated learning is a distributed learning framework that protects client data privacy. However, in real-world applications, due to the significant differences in data distribution among different clients, a single global model often fails to meet personalized needs. Moreover, existing personalized federated learning methods often struggle to balance computational efficiency and model personalization while inadequately addressing the heterogeneous computational capabilities of clients. To address these challenges, we propose FedBH—an innovative personalized federated learning method that combines the feature extraction capabilities of deep learning with an adaptive dynamic training mechanism. FedBH decomposes each client’s model into a global base layer and client-specific local heads, enhancing both computational efficiency and personalized training. To further optimize learning, FedBH incorporates a dynamic adjustment mechanism that adapts training parameters based on each client’s specific conditions. In each local training round, the algorithm samples a subset of the dataset and explores different configurations of local head and global base training rounds. The optimal configuration is determined based on validation loss and then applied for full training. This adaptive mechanism enables FedBH to dynamically adjust to varying client demands, ensuring efficient utilization of computational resources while maintaining robust personalization. Experimental results across multiple benchmark datasets and diverse data distribution scenarios show that FedBH achieves faster convergence and higher accuracy in most cases. These findings demonstrate that FedBH not only effectively adapts to heterogeneous data environments but also significantly improves training efficiency, showing great potential in addressing issues related to diverse and non-IID data distributions.},
  archive      = {J_COMCOM},
  author       = {Zediao Liu and Yayu Luo and Tongzhijun Zhu and Ziyi Chen and Tenglong Mao and Huan Pi and Ying Lin},
  doi          = {10.1016/j.comcom.2025.108190},
  journal      = {Computer Communications},
  month        = {7},
  pages        = {108190},
  shortjournal = {Comput. Commun.},
  title        = {FedBH: Efficient federated learning through shared base and adaptive hyperparameters in heterogeneous systems},
  volume       = {239},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). VESPACE: A verifiable blockchain-based data space solution to empower the data economy. <em>COMCOM</em>, <em>239</em>, 108180. (<a href='https://doi.org/10.1016/j.comcom.2025.108180'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the rapidly evolving data economy, the ability to securely and efficiently share data between organizations has become paramount, unlocking new opportunities for innovation and growth. In this context, different initiatives have worked on conceptual proposals and enabling technological building blocks, addressing design aspects of data spaces. However, the current landscape lacks practical implementations and integration of secure data-sharing primitives supporting a decentralized data ecosystem. To this end, we conduct an analysis of previous efforts and initiatives, identifying gaps. We then introduce VESPACE , a blockchain-based platform for data spaces that enables participants to selectively and securely share verifiable data with authorized users while maintaining control over their access. Our framework incorporates data sovereignty principles implemented through Decentralized Identifiers (DIDs), Verifiable Credentials (VCs), and blockchain technology, qualifying decentralized identity and access control as key features to establish user trust in decentralized data ecosystems. We present a prototype system implementation of VESPACE , evaluating the design choices, showcasing the feasibility of our proposal.},
  archive      = {J_COMCOM},
  author       = {Andrea Roberta Costagliola and Carlo Mazzocca and Armir Bujari and Rebecca Montanari and Paolo Bellavista},
  doi          = {10.1016/j.comcom.2025.108180},
  journal      = {Computer Communications},
  month        = {7},
  pages        = {108180},
  shortjournal = {Comput. Commun.},
  title        = {VESPACE: A verifiable blockchain-based data space solution to empower the data economy},
  volume       = {239},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Community-oriented edge computing platform. <em>COMCOM</em>, <em>239</em>, 108179. (<a href='https://doi.org/10.1016/j.comcom.2025.108179'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Democratizing the edge by capitalizing the underutilized computational resources of end devices, referred to as Extreme Edge Devices (EEDs), can foster various IoT applications. In this paper, we propose the Community Edge Platform (CEP). CEP is the first platform that exploits business, institutional, and social relationships to build communities of requesters and EEDs to eliminate recruitment costs and preserve privacy in EED-enabled environments. CEP promotes service-for-service exchange and utilizes a hierarchical control paradigm to prioritize the enrollment of nearby devices as workers. CEP also considers the fact that community-imposed constraints can lead to unbalanced work distribution. To alleviate this issue, we propose the Community-Oriented Resource Allocation (CORA) scheme. CORA accounts for community restrictions and strives to minimize the execution time and makespan while retaining a reasonable scheduler runtime. Towards that end, we formulate the resource allocation problem as a Bipartite Graph Matching problem. Comprehensive qualitative evaluations demonstrate the superiority of CEP compared to 12 prominent edge computing platforms in terms of various system architecture and performance features. Additionally, extensive simulations show that CORA outperforms six prominent resource allocation schemes by up to 44% and 7% in terms of makespan and execution time, respectively, while achieving a much faster runtime, outperforming the best of the six baseline resource allocation schemes by a factor of six.},
  archive      = {J_COMCOM},
  author       = {Abdalla A. Moustafa and Sara A. Elsayed and Hossam S. Hassanein},
  doi          = {10.1016/j.comcom.2025.108179},
  journal      = {Computer Communications},
  month        = {7},
  pages        = {108179},
  shortjournal = {Comput. Commun.},
  title        = {Community-oriented edge computing platform},
  volume       = {239},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A methodology for reproducible and portable experiment workflows. <em>COMCOM</em>, <em>239</em>, 108178. (<a href='https://doi.org/10.1016/j.comcom.2025.108178'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Testbeds allow the creation of research prototypes to test new ideas through practical experiments. This central role in validating ideas makes them irreplaceable tools for data-driven research in computer science. Various testbeds were created to provide testbeds for the scientific community. To simplify testbed usage, frameworks help to authenticate users, allocate resources, and run experiments. Each testbed typically implements its own framework using a specific API to realize experiments. Such an experiment design impedes the portability of experiments between different testbeds. In this paper, we present a solution where we port the pos experiment controller to the Chameleon and CloudLab testbed. The well-structured pos experiment workflow allows the creation of inherently reproducible experiments. Previously, the experiments using the pos workflow were only possible in dedicated testbeds. By introducing the portability feature, these experiments can run on Chameleon and CloudLab. We demonstrate that experiments can be executed on any of the mentioned platforms without changing the experiment definition. Based on these results, we discuss how the portability feature will be used in the upcoming SLICES-RI testbeds to create reproducible and easily-shareable experiments.},
  archive      = {J_COMCOM},
  author       = {Henning Stubbe and Sebastian Gallenmüller and Georg Carle},
  doi          = {10.1016/j.comcom.2025.108178},
  journal      = {Computer Communications},
  month        = {7},
  pages        = {108178},
  shortjournal = {Comput. Commun.},
  title        = {A methodology for reproducible and portable experiment workflows},
  volume       = {239},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multi-agent reinforcement learning with graph representation for green edge–cloud computation offloading. <em>COMCOM</em>, <em>239</em>, 108176. (<a href='https://doi.org/10.1016/j.comcom.2025.108176'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Edge–Cloud Computing (ECC) stands as a widely adopted distributed computing architecture that facilitates the offloading of computation-intensive tasks from Internet of Things (IoT) devices to edge servers. The growing emphasis on energy conservation and environmental protection raises the concerns of green edge–cloud computation offloading technology. However, conventional computation offloading methods have difficulties in making real-time offloading decisions and adapting to dynamic environmental changes, such as the communication channels. In response to these challenges, we propose a multi-agent reinforcement learning method with graph representation to address the edge–cloud computing offloading schedule problem. Our approach constructs a multi-agent computation offloading reinforcement learning scenario and utilizes graph neural networks to represent the connectivity features between devices and edge servers. Experimental results demonstrate that our proposed method outperforms other algorithms in reducing system energy consumption and response delay. Furthermore, the time-consuming of our approach is significantly shorter compared to heuristic genetic algorithms, with a reduction of 10–20 times.},
  archive      = {J_COMCOM},
  author       = {Yifan Bo and Jinghan Feng and Shou Zhang and Biao Leng},
  doi          = {10.1016/j.comcom.2025.108176},
  journal      = {Computer Communications},
  month        = {7},
  pages        = {108176},
  shortjournal = {Comput. Commun.},
  title        = {Multi-agent reinforcement learning with graph representation for green edge–cloud computation offloading},
  volume       = {239},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A novel energy-efficient cross-layer design for scheduling and routing in 6TiSCH networks. <em>COMCOM</em>, <em>239</em>, 108173. (<a href='https://doi.org/10.1016/j.comcom.2025.108173'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The 6TiSCH protocol stack plays a vital role in enabling reliable and energy-efficient communications for the Industrial Internet of Things (IIoT). However, it faces challenges, including prolonged network formation, inefficient parent switching, high control packet overhead, and suboptimal resource utilization. To tackle these issues, we propose in this paper a novel cross-layer optimization framework aiming to enhance the coordination between the Scheduling Function (SF), the Routing Protocol for Low-Power and Lossy Networks (RPL), and queue management. Our solution introduces a slot-aware parent switching mechanism, early slot reservation to mitigate queue overflow, and a refined slot locking strategy to improve slot availability. To reduce control overhead, the proposed method merges 6P cell reservation information into RPL control packets (DIO/DAO), thus minimizing control exchanges during parent switching and node joining. Optimized slot selection further reduces latency and jitter. Through extensive simulations on the 6TiSCH simulator and under varying network densities and traffic loads, we demonstrate significant improvements over the standard 6TiSCH benchmark in terms of traffic load, joining time, latency, and energy efficiency. These enhancements make the proposed solution suitable for time-sensitive IIoT applications.},
  archive      = {J_COMCOM},
  author       = {Ahlam Hannachi and Wael Jaafar and Salim Bitam and Nabil Ouazene},
  doi          = {10.1016/j.comcom.2025.108173},
  journal      = {Computer Communications},
  month        = {7},
  pages        = {108173},
  shortjournal = {Comput. Commun.},
  title        = {A novel energy-efficient cross-layer design for scheduling and routing in 6TiSCH networks},
  volume       = {239},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Deterministic networking (DetNet): Performance evaluation of the packet ordering functions. <em>COMCOM</em>, <em>239</em>, 108171. (<a href='https://doi.org/10.1016/j.comcom.2025.108171'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Industrial Internet of Things applications need strict Quality of Service guarantees, including when they use wireless networks. While existing standardization efforts, such as Deterministic Networks (IETF) and Time-Sensitive Networking (TSN), have focused mainly on wired networks, more recent activities address wireless networks, like the Reliable and Available Wireless (RAW) Working Group of the IETF. Duplicating packets over multiple paths can improve availability and reliability, but this can potentially result in more out-of-order packets. Existing reordering algorithms, including two recently standardized by the IETF, are not always well suited to wireless networks. As a result, guaranteeing a bounded end-to-end latency and/or maximum consecutive packet losses is still particularly challenging. We propose a novel packet reordering algorithm for deterministic networking that includes wireless segments to achieve lower end-to-end latency while providing high end-to-end reliability. We simulate a 6TiSCH wireless network to compare its performance with the existing ones. Furthermore, we also extend existing Network Calculus bounds on the reordering timer to obtain path-dependent values that can reduce the average latency. We also introduce a probabilistic model to compute the reordering ratio when there are no retransmissions that fits the simulation results.},
  archive      = {J_COMCOM},
  author       = {Juan-Cruz Piñero and Alberto Blanc and José Ignacio Alvarez-Hamelin and Georgios Z. Papadopoulos},
  doi          = {10.1016/j.comcom.2025.108171},
  journal      = {Computer Communications},
  month        = {7},
  pages        = {108171},
  shortjournal = {Comput. Commun.},
  title        = {Deterministic networking (DetNet): Performance evaluation of the packet ordering functions},
  volume       = {239},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A multipath redundancy communication framework for enhancing 5G mobile communication quality. <em>COMCOM</em>, <em>239</em>, 108157. (<a href='https://doi.org/10.1016/j.comcom.2025.108157'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As networks increasingly become the backbone of modern society, the demands placed on them by various applications have become more complex. In particular, the demand for high-capacity, low-latency services such as real-time streaming is increasing every year. Although 5G has been deployed to meet these needs, its effectiveness can vary significantly by location and time, and sometimes falls short of requirements. Traditionally, much of the research to improve communication stability has focused on TCP-based systems, which do not translate well to real-time UDP streaming applications. To address the above challenges, we propose a multipath redundant communication framework designed to improve the quality of real-time media streaming. This framework has been tested using multipath redundant communication over two mobile networks with a moving vehicle in an urban environment. Using a real-time streaming application based on WebRTC, our framework demonstrates a significant reduction in packet loss and an increase in bitrate, outperforming existing multipath redundant communication systems without interfering with the application’s congestion control mechanisms.},
  archive      = {J_COMCOM},
  author       = {Koki Ito and Jin Nakazato and Romain Fontugne and Manabu Tsukada and Esaki Hiroshi},
  doi          = {10.1016/j.comcom.2025.108157},
  journal      = {Computer Communications},
  month        = {7},
  pages        = {108157},
  shortjournal = {Comput. Commun.},
  title        = {A multipath redundancy communication framework for enhancing 5G mobile communication quality},
  volume       = {239},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Allocation of computing resources based on multi-objective strategy and performance improvement in 5G networks. <em>COMCOM</em>, <em>238</em>, 108197. (<a href='https://doi.org/10.1016/j.comcom.2025.108197'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The increase in mobile communication has escalated network load, necessitating more intelligent resource management in 5G and forthcoming networks. This research introduces a novel methodology to enhance network performance by integrating bird swarm optimization (BSO) with deep learning (DL). We investigate a configuration in which several users inside a single cell solicit services from proximate edge servers and remote cloud servers, employing Non-Orthogonal Multiple Access (NOMA) to optimize radio spectrum utilization. BSO, influenced by avian flocking patterns, aggregates users and distributes workloads among servers to reduce energy consumption, service latency, and operational expenses. Simultaneously, DL examines historical network data to forecast traffic and user requirements, enabling BSO to make immediate, educated decisions. We utilize queuing theory to model this system, addressing a complicated optimization problem that concurrently reduces energy, latency, and costs. In contrast to conventional solutions, our technology dynamically adjusts to fluctuating network conditions, providing an effective remedy for the requirements of 5G. Simulations demonstrate that the integrated BSO-DL approach diminishes energy usage, delays, and expenses by around 54 %, substantiating its efficacy in improving broadband performance. This research facilitates the development of more efficient and sustainable 5G networks, addressing the increasing demands of contemporary mobile communication through a scalable and intelligent approach.},
  archive      = {J_COMCOM},
  author       = {Qinghui Yuan and Zhiyong Liu and Xueying Jiang and Huijin Hu and Yunpeng Yang and Juxiao Li},
  doi          = {10.1016/j.comcom.2025.108197},
  journal      = {Computer Communications},
  month        = {6},
  pages        = {108197},
  shortjournal = {Comput. Commun.},
  title        = {Allocation of computing resources based on multi-objective strategy and performance improvement in 5G networks},
  volume       = {238},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). KPI-aware service provisioning for remote industrial control systems management. <em>COMCOM</em>, <em>238</em>, 108191. (<a href='https://doi.org/10.1016/j.comcom.2025.108191'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The advent of B5G/6G communication infrastructures answers to the requirements posed by emerging use cases and business models, such as Industry 4.0 and autonomous guided vehicles, which impose a set of highly demanding Key Performance Indicators (KPIs). In this regard, time-engineered applications are of particular importance. Said applications, while do not may impose very demanding latencies, they impose a strict control over them to ensure a deterministic service performance. This has given rise to the concept of Deterministic Networking that relies on adding deterministic capabilities to all involved network infrastructures. In general, optical networks are seen as one of the main enablers of latency and jitter-bounded communications, due to their intrinsic deterministic nature. In the framework of Industry 4.0 use cases, such capability can be exploited to enable remote control of optically interconnected smart factories, contributing this way to reduce both CAPEX and OPEX. However, the estimation of KPIs across optically interconnected smart factories/client domains and the provisioning of end-to-end (E2E) services arises several challenges. In this work, we present firstly multiple service provisioning schemas (single-vs multi-path) leveraging the capabilities of the Software Defined Networking (SDN) paradigm. Then, we provide tools and algorithms for best-case KPI estimation and resource allocation decision and analyze the impact of supporting heterogeneous KPIs into the obtained overall network performance.},
  archive      = {J_COMCOM},
  author       = {Albert Pagès and Enric Guasch and Fernando Agraz and Salvatore Spadaro},
  doi          = {10.1016/j.comcom.2025.108191},
  journal      = {Computer Communications},
  month        = {6},
  pages        = {108191},
  shortjournal = {Comput. Commun.},
  title        = {KPI-aware service provisioning for remote industrial control systems management},
  volume       = {238},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Exploiting O-RAN neutral hosting for first responders connectivity in emergency scenarios. <em>COMCOM</em>, <em>238</em>, 108188. (<a href='https://doi.org/10.1016/j.comcom.2025.108188'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Ensuring seamless connectivity for first responders represents a critical issue in emergency scenarios, where disrupted infrastructures and network congestion can significantly hinder the communication. Open Radio Access Network (O-RAN) promises a level of openness in Radio Access Networks (RANs) that could be advantageous under such critical conditions, allowing for enhanced integration across different operators’ networks to mitigate resource scarcity. This paper introduces an architectural solution based on O-RAN that leverages neutral hosting to provide a unified gateway to RAN infrastructures. Our approach facilitates seamless access for emergency responders and optimizes the allocation of resources at both the RAN and Core levels within a specified area by exploiting the deployment of near-real-time Applications (xApps) and non-real-time Applications (rApps) managed by the neutral host. The results demonstrate the effectiveness of O-RAN neutral hosting in enhancing first responders’ connectivity, showcasing significant improvements in throughput and latency, especially under disaster scenarios.},
  archive      = {J_COMCOM},
  author       = {A. Piccioni and A. Marotta and C. Rinaldi and C. Centofanti and K. Kondepu and D. Cassioli and F. Graziosi},
  doi          = {10.1016/j.comcom.2025.108188},
  journal      = {Computer Communications},
  month        = {6},
  pages        = {108188},
  shortjournal = {Comput. Commun.},
  title        = {Exploiting O-RAN neutral hosting for first responders connectivity in emergency scenarios},
  volume       = {238},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Modeling and performance analysis of slotted ALOHA with interference cancellation for mMTC. <em>COMCOM</em>, <em>238</em>, 108177. (<a href='https://doi.org/10.1016/j.comcom.2025.108177'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose an analytical model to evaluate the performance of slotted ALOHA with NOMA for mMTC under the assumption that base stations and devices are distributed according to mutually independent stationary point processes. For NOMA, we consider a simple scenario in which perfect interference cancellation can be conducted at each base station. As performance metrics, we focus on the probability of a device successfully transmitting data to the nearest base station (transmission success probability) and the expected number of devices from which a base station can correctly decode the signals (throughput of base station). We establish analytical expressions for the two performance metrics for three schemes: simple slotted ALOHA, slotted ALOHA with interference cancellation, and slotted ALOHA with power control. Through several numerical experiments, we show that the application of interference cancellation improves the throughput by up to 20%; however, the application of interference cancellation does not solve the near-far problem. We also show that the application of power control solves the near-far problem but significantly degrades the performance of slotted ALOHA.},
  archive      = {J_COMCOM},
  author       = {Yuki Ichimura and Shigeo Shioda and Takeshi Hirai},
  doi          = {10.1016/j.comcom.2025.108177},
  journal      = {Computer Communications},
  month        = {6},
  pages        = {108177},
  shortjournal = {Comput. Commun.},
  title        = {Modeling and performance analysis of slotted ALOHA with interference cancellation for mMTC},
  volume       = {238},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A novel joint short-packet transmission in satellite–terrestrial NOMA networks. <em>COMCOM</em>, <em>238</em>, 108174. (<a href='https://doi.org/10.1016/j.comcom.2025.108174'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper proposes an innovative communication framework that integrates Non-Orthogonal Multiple Access (NOMA) with satellite and short-packet technologies, addressing the emerging demands of Sixth-Generation (6G) networks. A dedicated transmission channel model for short-packets in Satellite–Terrestrial NOMA networks is introduced, which tackles the unique challenges presented by these environments. Through a comprehensive performance analysis, key metrics such as Block Error Rate (BLER), throughput, and goodput are evaluated, enhancing the understanding of system performance under varying conditions. Extensive Monte-Carlo simulations validate the theoretical models, confirming their applicability in real-world scenarios. The findings demonstrate that an increase in the Signal-to-Noise Ratio (SNR) significantly reduces both outage probability and average BLER, highlighting the critical role of SNR in enhancing system reliability. Furthermore, the study reveals that increasing the number of satellite antennas substantially improves system performance, underscoring the importance of antenna configuration. The analysis of blocklength and hardware noise provides essential insights for optimizing system design. Overall, this work offers valuable perspectives for the design and optimization of future communication systems, emphasizing the potential of combining NOMA with satellite and short-packet technologies to enhance performance in next-generation 6G networks.},
  archive      = {J_COMCOM},
  author       = {Huu Q. Tran},
  doi          = {10.1016/j.comcom.2025.108174},
  journal      = {Computer Communications},
  month        = {6},
  pages        = {108174},
  shortjournal = {Comput. Commun.},
  title        = {A novel joint short-packet transmission in satellite–terrestrial NOMA networks},
  volume       = {238},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multi-agent proximal policy optimization based efficient user association and resource allocation in UAV-assisted heterogeneous cellular networks. <em>COMCOM</em>, <em>238</em>, 108172. (<a href='https://doi.org/10.1016/j.comcom.2025.108172'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The rapid growth in wireless communication demands has placed unprecedented pressure on modern networks, particularly concerning capacity enhancement and coverage expansion. Heterogeneous Cellular Networks (HCNs), with their flexible multi-layered architecture, present a promising solution to these challenges. However, these networks face considerable complexity and resource constraints. Consequently, developing efficient User Association and Resource Allocation (UARA) schemes is essential for establishing sustainable, high-performance wireless communication systems. In this work, we establish a paradigm for an Unmanned Aerial Vehicle (UAV)-assisted HCN with multiple User Equipment (UEs). An optimization scheme for UAV deployment and UARA policy is proposed to maximize the long-term system utility while ensuring quality of service for UEs. Specifically, we employ an efficient greedy deployment algorithm to dynamically update UAV locations and maximize the coverage utility. For UARA decision-making, we introduce a dynamic non-convex mixed-integer nonlinear programming problem and model it as a partially observable Markov decision process. Subsequently, an Independent Reward-based Hybrid action space Multi-Agent Proximal Policy Optimization algorithm (IRH-MAPPO) is proposed. Additionally, we utilize a centralized training and distributed execution framework and incorporate value normalization and action masking to improve its efficiency. Experimental results demonstrate that our algorithm significantly outperforms the baselines in terms of system utility, particularly in Spectral Efficiency (SE) and Energy Efficiency (EE), which confirms its effectiveness, superiority, and scalability.},
  archive      = {J_COMCOM},
  author       = {Yueqian Song and Qingtian Zeng and Geng Chen and Guiyuan Yuan and Hua Duan},
  doi          = {10.1016/j.comcom.2025.108172},
  journal      = {Computer Communications},
  month        = {6},
  pages        = {108172},
  shortjournal = {Comput. Commun.},
  title        = {Multi-agent proximal policy optimization based efficient user association and resource allocation in UAV-assisted heterogeneous cellular networks},
  volume       = {238},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A two-way dynamic adaptive pricing resource allocation model based on combinatorial double auctions in computational network. <em>COMCOM</em>, <em>238</em>, 108170. (<a href='https://doi.org/10.1016/j.comcom.2025.108170'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Efficient resource allocation in computing networks is essential for managing fluctuating demands and optimizing system performance. Traditional auction and pricing models often fail to adapt to diverse demands and supply–demand fluctuations, resulting in inefficiencies. This paper proposes a bidirectional price-adaptive bundled resource auction model that considers not only the autonomous adjustment of sellers’ quotations in response to supply–demand fluctuations but also the impact of these fluctuations on buyers’ willingness to pay and bidding behavior. The model integrates Combinatorial Double Auction (CDA) mechanism and Genetic Algorithm (GA), constructing a bundled resource auction mechanism that accommodates diverse resource demands and an adaptive pricing strategy that dynamically responds to real-time supply–demand variations. This approach enhances resource allocation accuracy in dynamic and competitive computing network environments. Furthermore, a reserve price mechanism and a delay compensation strategy are introduced to ensure that the proposed mechanism satisfies individual rationality, budget balance, and incentive compatibility while maintaining computational efficiency. Simulation results demonstrate that, compared to traditional methods, the proposed model not only improves allocation efficiency and enhances resource utilization but also helps reduce operational costs. Specifically, resource allocation dispersion decreases by 4.26%, while service providers’ revenue increases by 7.47%. This study provides a scalable and adaptive solution for dynamic resource allocation in cloud and edge computing platforms. It contributes significantly to the development of resource management and flexible pricing strategies in markets characterized by diverse demands and fluctuating conditions.},
  archive      = {J_COMCOM},
  author       = {Yanjun Xu and Chunqi Tian and Wei Wang and Lizhi Bai and Xuhui Xia},
  doi          = {10.1016/j.comcom.2025.108170},
  journal      = {Computer Communications},
  month        = {6},
  pages        = {108170},
  shortjournal = {Comput. Commun.},
  title        = {A two-way dynamic adaptive pricing resource allocation model based on combinatorial double auctions in computational network},
  volume       = {238},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). TokenGuard: A novel framework for robust access management in SDN controllers. <em>COMCOM</em>, <em>238</em>, 108169. (<a href='https://doi.org/10.1016/j.comcom.2025.108169'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Software-defined networks (SDNs) are increasingly popular due to their simplified network management and centralized control through an SDN controller. However, ensuring secure authentication and authorization for REST web services in SDN controllers is a critical challenge. This paper introduces TokenGuard, a novel security framework designed to enhance the protection of REST web services in SDN controllers. TokenGuard uses dynamic and unique access tokens for each REST request between network applications and the SDN controller. These tokens are generated using a specialized mathematical model, the Fractional Logistic Map (FLM), which incorporates a fixed memory length. This approach significantly improves the robustness of SDN controllers against REST replay attacks involving stolen access tokens. Extensive simulations demonstrate that TokenGuard outperforms standard and federated token-based authentication systems in terms of performance and security. Specifically, TokenGuard achieves approximately 10.5% faster response times than standard token-based systems and 78.1% faster than federated token-based systems. Additionally, TokenGuard’s content sizes are 1.95% smaller than standard token-based systems and 35.28% smaller than federated token-based systems. Moreover, TokenGuard handles requests per second 1.03 times more efficiently than standard token-based systems and 4.82 times more efficiently than federated token-based systems. By employing dynamic access token sequences, TokenGuard significantly mitigates the risks associated with token replay attacks and stolen access tokens, offering a substantial security advantage over the static single-token mechanisms used in traditional systems. This paper also addresses the challenges and limitations of current SDN controllers and highlights how TokenGuard fills these gaps. Practical aspects of deploying TokenGuard in real-world SDN environments are discussed, including its scalability, performance impact, and interoperability.},
  archive      = {J_COMCOM},
  author       = {Mahmoud Elzoghbi and Hui He},
  doi          = {10.1016/j.comcom.2025.108169},
  journal      = {Computer Communications},
  month        = {6},
  pages        = {108169},
  shortjournal = {Comput. Commun.},
  title        = {TokenGuard: A novel framework for robust access management in SDN controllers},
  volume       = {238},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Enhanced load balancing technique for SDN controllers: A multi-threshold approach with migration of switches. <em>COMCOM</em>, <em>238</em>, 108167. (<a href='https://doi.org/10.1016/j.comcom.2025.108167'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deploying multiple controllers in the control panel of software-defined networks increases scalability, availability, and performance, but it also brings challenges, such as controller overload. To address this, load-balancing techniques are employed in software-defined networks. Controller load balancing can be categorized into two main approaches: (1) single-level thresholds and (2) multi-level thresholds. However, previous studies have predominantly relied on single-level thresholds, which result in an imprecise classification of controllers or have assumed uniform controller capacities in multi-level threshold methods. This study explores controller load balancing with a focus on utilizing multi-level thresholds to accurately assess controller status. Switch migration operations are utilized to achieve load balancing, considering factors such as the degree of load imbalance of the target controller and migration efficiency. This includes evaluating the post-migration status of the target controller and the distance between the migrating switch and the target controller to select the appropriate target controller and migrating switch. The proposed scheme reduces controller response time, migration costs, communication overhead, and throughput rate. Results demonstrate that our scheme outperforms others regarding response time and overall performance.},
  archive      = {J_COMCOM},
  author       = {Mohammad Kazemiesfeh and Somaye Imanpour and Ahmadreza Montazerolghaem},
  doi          = {10.1016/j.comcom.2025.108167},
  journal      = {Computer Communications},
  month        = {6},
  pages        = {108167},
  shortjournal = {Comput. Commun.},
  title        = {Enhanced load balancing technique for SDN controllers: A multi-threshold approach with migration of switches},
  volume       = {238},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). On flexible association and placement in disaggregated RAN designs. <em>COMCOM</em>, <em>238</em>, 108166. (<a href='https://doi.org/10.1016/j.comcom.2025.108166'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In Open RAN architectures, the classic gNB radio protocol stack is disaggregated into virtualized components: the Centralized Unit (CU), the Distributed Unit (DU), and the Radio Unit (RU). Each unit is deployed throughout a cloud-enabled RAN infrastructure to meet users’ Quality of Service (QoS) requirements. In this framework, we propose Open RAN unit placement methods that maximize User Equipment (UE) admission while ensuring their QoS needs. We focus on two primary tasks: (i) establishing UE-RU associations and (ii) placing CUs and DUs across the network’s cloud hosts. We formulate the joint association-placement UE-DU-CU optimization problem as an Integer Linear Programming (ILP) model and propose two resolution approaches besides the optimal one: (i) an algorithm that decomposes and sequentially solves the ILP model and (ii) a Recurrent Neural Network (RNN) heuristic that emulates the joint optimization model. We assess the optimal model’s performance across varying network resource availability. Compared to baseline models, simulations demonstrate that our approaches ensure higher admissibility levels while minimizing deployment costs and increasing fairness. The RNN heuristic presents a small optimality gap, with up to 9% fewer admissions while reducing the execution time by up to 99.98%, making it suitable for real-time implementation.},
  archive      = {J_COMCOM},
  author       = {Hiba Hojeij and Guilherme Iecker Ricardo and Mahdi Sharara and Sahar Hoteit and Véronique Vèque and Stefano Secci},
  doi          = {10.1016/j.comcom.2025.108166},
  journal      = {Computer Communications},
  month        = {6},
  pages        = {108166},
  shortjournal = {Comput. Commun.},
  title        = {On flexible association and placement in disaggregated RAN designs},
  volume       = {238},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A survey on security enhancing digital twins: Models, applications and tools. <em>COMCOM</em>, <em>238</em>, 108158. (<a href='https://doi.org/10.1016/j.comcom.2025.108158'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A Digital Twin (DT) is a virtual representation of both cyber and physical objects. Initially, the concept was introduced to optimize manufacturing products, aimed at validating and verifying their behavior along their lifecycle. However, in the recent years and with the advent of Industry 4.0, it has also gained huge attention in the cybersecurity arena. Indeed, using a DT is seen as an strategy to automate, monitor, simulate, and diagnose cyber–physical systems behavior, aimed at making them to be secure. The DT can provide security in multiple domains such as manufacturing, aerospace, automotive, construction, smart grid, smart cities, and smart transportation. The fundamentals of a DT is a combination of data and a model of the system. To this end, this survey aims to explore current efforts on DT modeling techniques, and how these DT models capture the behavior of the system to enhance cybersecurity. The study focuses on the construction of behavioral models, techniques, tools, and technologies of DT used for specific security applications. The purpose of the discussion carried out in this survey is to illustrate the reader on how these behavioral models adapt to different security applications. In addition, the paper also analyzes and classifies current DT literature based on use cases, type of DT study, tools, and various DT operation modes with their specific security application. The findings of this survey describe security applications being enhanced by DT, and how the combination of key technologies facilitating Industry 4.0 (i.e., AI, IoT, Big Data and Cloud Computing, among others), are accelerating the adoption of DT as a solution for next generation security applications. Finally, this survey highlights relevant research gaps that need to be addressed before the wide adoption of DT in cybersecurity.},
  archive      = {J_COMCOM},
  author       = {Abdul Rehman Qureshi and Adrián Asensio and Muhammad Imran and Jordi Garcia and Xavi Masip-Bruin},
  doi          = {10.1016/j.comcom.2025.108158},
  journal      = {Computer Communications},
  month        = {6},
  pages        = {108158},
  shortjournal = {Comput. Commun.},
  title        = {A survey on security enhancing digital twins: Models, applications and tools},
  volume       = {238},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Mind the paths you choose: Speeding up segment routing-based traffic engineering with path preprocessing. <em>COMCOM</em>, <em>238</em>, 108156. (<a href='https://doi.org/10.1016/j.comcom.2025.108156'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Many state-of-the-art Segment Routing (SR) Traffic Engineering (TE) algorithms rely on Linear Program (LP)-based optimization. However, the poor scalability of the latter and the resulting high computation times impose severe restrictions on the practical usability of such approaches for many use cases. A promising way to address these issues is to preemptively limit the number of SR paths considered during optimization by employing certain preprocessing strategies. In the first part of this paper, we conduct an extensive literature review of such preprocessing approaches together with a large-scale comparative performance study on a plethora of real-world topologies, including recent data from a Tier-1 Internet Service Provider (ISP). In the second part, we then use the insights gained from the former study to develop a novel combined preprocessing approach which also guarantees to not interfere with the satisfiability of practically important latency bound constraints. Our approach is able to reduce the number of SR paths to consider during optimization by as much as 97%–99%, while still allowing to achieve close to optimal solutions. This facilitates an around 10 × speedup for different LP-based SR TE algorithms, which is more than twice as good as what is achievable with any of the previously existing methods. Finally, we also study the applicability of the path preprocessing paradigm to the use case of tactical TE, showing that it facilitates an around 37% speedup in this context as well. All in all, this constitutes a major improvement over the current state-of-the-art and further facilitates the reliable use of LP-based TE optimization for large segment-routed networks.},
  archive      = {J_COMCOM},
  author       = {Alexander Brundiers and Timmy Schüller and Nils Aschenbruck},
  doi          = {10.1016/j.comcom.2025.108156},
  journal      = {Computer Communications},
  month        = {6},
  pages        = {108156},
  shortjournal = {Comput. Commun.},
  title        = {Mind the paths you choose: Speeding up segment routing-based traffic engineering with path preprocessing},
  volume       = {238},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Optimizing virtual payment channel establishment in the face of on-path adversaries. <em>COMCOM</em>, <em>238</em>, 108155. (<a href='https://doi.org/10.1016/j.comcom.2025.108155'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Payment channel networks (PCNs) are among the most promising solutions to the scalability issues in permissionless blockchains, allowing parties to pay each other off-chain through a path of payment channels (PCs). However, the cost of routing transactions is proportional to the number of intermediaries since each charges a fee. Analogous to other networks, malicious intermediaries on the path can lead to security/privacy threats. Virtual channels (VCs), i.e., bridges over PC paths, mitigate the above PCN issues: Intermediaries participate only in the VC setup but in no future VC payments. However, creating a VC has a cost that must be paid out of the bridged PCs’ balance. Currently, we are missing guidelines on how/where to set up VCs. Ideally, VCs should minimize transaction costs while mitigating security and privacy threats from on-path adversaries. In this work, we address for the first time the VC setup problem, formalizing it as an optimization problem. We present an integer linear program (ILP) computing the globally optimal VC setup strategy in terms of cost, security, and privacy. We accompany this expensive ILP with a fast, greedy algorithm. Our model and algorithms can be used with any on-path adversary whose strategy can be expressed as a set of corrupted nodes. We evaluate the greedy algorithm over a snapshot of the Lightning Network (LN), the largest Bitcoin-based PCN. Our results confirm that the greedy strategy minimizes costs while protecting against security and privacy threats and may serve the LN community as guidelines for VC deployment.},
  archive      = {J_COMCOM},
  author       = {Lukas Aumayr and Esra Ceylan and Yannik Kopyciok and Matteo Maffei and Pedro Moreno-Sanchez and Iosif Salem and Stefan Schmid},
  doi          = {10.1016/j.comcom.2025.108155},
  journal      = {Computer Communications},
  month        = {6},
  pages        = {108155},
  shortjournal = {Comput. Commun.},
  title        = {Optimizing virtual payment channel establishment in the face of on-path adversaries},
  volume       = {238},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Estimating autonomous system risk levels by analyzing IXP route server RIB. <em>COMCOM</em>, <em>237</em>, 108154. (<a href='https://doi.org/10.1016/j.comcom.2025.108154'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The security of Border Gateway Protocol (BGP) operations at Internet Exchange Points (IXPs) is critical to ensuring the integrity of data exchanges between Internet Service Providers (ISPs). A key challenge in BGP is its trust-based route sharing, which introduces vulnerabilities that attackers can exploit to hijack or disrupt traffic. While mechanisms like Internet Routing Registries (IRRs) and the Resource Public Key Infrastructure (RPKI) have been developed to mitigate these risks, their effectiveness is often undermined by inherent design flaws that limit their reliability. This paper presents a novel tool designed to address these security gaps in IXP infrastructures. By analyzing the Routing Information Base (RIB) of an IXP’s route server, the tool identifies possible prefix hijacking attacks. These prefixes serve as input for calculating a Risk Level metric for each Autonomous System (AS), offering IXP operators insights into anomalous behaviors. The effectiveness of the metric is validated through its application to well-known attack scenarios. Finally, we showcase the application of this tool through an analysis of real-world data from the route server of a major Italian IXP.},
  archive      = {J_COMCOM},
  author       = {Stefano Servillo and Pietro Spadaccino and Flavio Luciani and Francesca Cuomo},
  doi          = {10.1016/j.comcom.2025.108154},
  journal      = {Computer Communications},
  month        = {5},
  pages        = {108154},
  shortjournal = {Comput. Commun.},
  title        = {Estimating autonomous system risk levels by analyzing IXP route server RIB},
  volume       = {237},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). How mature is 5G deployment? a cross-sectional, year-long study of 5G uplink performance. <em>COMCOM</em>, <em>237</em>, 108153. (<a href='https://doi.org/10.1016/j.comcom.2025.108153'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {After a rapid deployment worldwide over the past few years, 5G is expected to have reached a mature deployment stage to provide measurable improvement of network performance and user experience over its predecessors. In this study, we aim to assess 5G deployment maturity via three conditions: (1) Does 5G performance remain stable over a long time span (1 year)? (2) Does 5G provide better performance than its predecessor Long-Term Evolution (LTE)? (3) Does the technology offer similar performance across diverse geographic areas and cellular operators? We answer this important question by conducting two year-long measurement campaigns of 5G uplink performance leveraging a custom Android app: one crowd-sourced, cross-sectional campaign spanning 8 major cities in 7 countries and two different continents (Europe and North America), and one controlled campaign focusing on mmWave deployment at a fixed location in the downtown area of Boston, MA. Our datasets show that 5G deployment in major cities appears to have matured, with no major performance improvements observed over a one-year period, but 5G does not provide consistent, superior measurable performance over LTE, especially in terms of latency, and further there exists clear uneven 5G performance across the 8 cities. Our study suggests that, while 5G deployment appears to have stagnated, it is short of delivering its promised performance and user experience gain over its predecessor.},
  archive      = {J_COMCOM},
  author       = {Imran Khan and Moinak Ghoshal and Joana Angjo and Sigrid Dimce and Mushahid Hussain and Paniz Parastar and Yenchia Yu and Xueting Deng and Sumit Hawal and Shirui Huang and Ameya Rane and Yin Wang and Claudio Fiandrino and Charalampos Orfanidis and Shivang Aggarwal and Ana C. Aguiar and Ozgu Alay and Carla Fabiana Chiasserini and Falko Dressler and Y. Charlie Hu and Joerg Widmer},
  doi          = {10.1016/j.comcom.2025.108153},
  journal      = {Computer Communications},
  month        = {5},
  pages        = {108153},
  shortjournal = {Comput. Commun.},
  title        = {How mature is 5G deployment? a cross-sectional, year-long study of 5G uplink performance},
  volume       = {237},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A hybrid swarm intelligence approach for optimizing multimodal large language models deployment in edge-cloud-based federated learning environments. <em>COMCOM</em>, <em>237</em>, 108152. (<a href='https://doi.org/10.1016/j.comcom.2025.108152'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The combination of Federated Learning (FL), Multimodal Large Language Models ( MLLMs ), and edge-cloud computing enables distributed and real-time data processing while preserving privacy across edge devices and cloud infrastructure. However, the deployment of MLLMs in FL environments with resource-constrained edge devices presents significant challenges, including resource management, communication overhead, and non-IID data. To address these challenges, we propose a novel hybrid framework wherein MLLMs are deployed on edge devices equipped with sufficient resources and battery life, while the majority of training occurs in the cloud. To identify suitable edge devices for deployment, we employ Particle Swarm Optimization ( PSO ), and Ant Colony Optimization ( ACO ) is utilized to optimize the transmission of model updates between edge and cloud nodes. This proposed swarm intelligence-based framework aims to enhance the efficiency of MLLM training by conducting extensive training in the cloud and fine-tuning at the edge, thereby reducing energy consumption and communication costs. The framework is particularly applicable in Unmanned Vehicle System (UVS)-based edge-cloud computing environments, where efficient resource management and real-time decision-making are critical for autonomous operations. The autonomous nature of UVS requires efficient resource management and a reliable communication system. The proposed swarm intelligence-based framework aims to satisfy these requirements through optimized edge device selection, while minimizing communication overhead. Our experimental results show that the proposed method significantly improves system performance, achieving an accuracy of 92%, reducing communication cost by 30%, and enhancing client participation compared to traditional FL methods. These results make the proposed approach highly suitable for large-scale edge-cloud computing systems.},
  archive      = {J_COMCOM},
  author       = {Gaith Rjoub and Hanae Elmekki and Saidul Islam and Jamal Bentahar and Rachida Dssouli},
  doi          = {10.1016/j.comcom.2025.108152},
  journal      = {Computer Communications},
  month        = {5},
  pages        = {108152},
  shortjournal = {Comput. Commun.},
  title        = {A hybrid swarm intelligence approach for optimizing multimodal large language models deployment in edge-cloud-based federated learning environments},
  volume       = {237},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). AssociateChain: Scaling blockchain in cloud–edge-enabled metaverse via associative sharding. <em>COMCOM</em>, <em>237</em>, 108150. (<a href='https://doi.org/10.1016/j.comcom.2025.108150'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Blockchain has been extensively employed to strengthen the security and privacy of cloud–edge-enabled Metaverse due to its immutability, decentralization, and other key characteristics. However, limited scalability remains a significant barrier to its broader adoption. Sharding offers a promising solution for scaling blockchain, but existing schemes perform state and node sharding separately, ignoring the fact that, in cloud–edge computing, end devices are typically serviced by the nearest edge nodes to achieve a high quality of service (QoS). We define this characteristic as device–edge proximity. As a result, device states may be assigned to distant edge node shards, requiring transactions to be relayed over the wide-area network before reaching their assigned shards, thus introducing additional relay overhead and increasing the overall transaction latency. To address this challenge, we propose AssociateChain, which employs a two-stage associative sharding approach to minimize number of relay transactions. In the first stage, device states are grouped into shards based on historical transaction patterns. In the second stage, the edge node sharding process is modeled as a stable matching problem, optimizing the assignment of edge nodes based on the first-stage results and device–edge proximity. Furthermore, we introduce a double-check mechanism to strengthen resilience against potential takeover attacks. Although AssociateChain slightly increases sharding complexity from O ( n ) to O ( k ( n + 1 ) ) , where n is the number of nodes and k is the number of shards, experiments demonstrate that it reduces the relay transaction ratio by 90%, lowers relay delay by two orders of magnitude, and decreases total cost by 70%, significantly outperforming existing sharding schemes.},
  archive      = {J_COMCOM},
  author       = {Yi Li and Jinsong Wang and Hongwei Zhang and Zening Zhao and Yuemin Ding},
  doi          = {10.1016/j.comcom.2025.108150},
  journal      = {Computer Communications},
  month        = {5},
  pages        = {108150},
  shortjournal = {Comput. Commun.},
  title        = {AssociateChain: Scaling blockchain in cloud–edge-enabled metaverse via associative sharding},
  volume       = {237},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). LFIoTDI: A lightweight and fine-grained device identification approach for IoT security enhancement. <em>COMCOM</em>, <em>237</em>, 108149. (<a href='https://doi.org/10.1016/j.comcom.2025.108149'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The rapid development of the Internet of Things (IoT) has brought new challenges in device identification. Accurately identifying IoT devices connected to a network is vital for effective resource management, network planning, security threat detection, and handling anomalous traffic. However, existing traffic-based device identification approaches have shortcomings in terms of accuracy, stability, identification granularity, etc. In this study, we introduce LFIoTDI, a lightweight and fine-grained device identification method leveraging machine learning to enhance IoT security. Based on an innovative feature set, LFIoTDI can accomplish device identification on resources-constraint IoT devices with just a single network-layer packet. Additionally, a key feature of LFIoTDI is its use of the Message Queuing Telemetry Transport (MQTT) protocol for real-time updates to the device identification model, greatly enhancing the model’s scalability. Extensive evaluation experiments on the CIC, UNSW, and SMPS datasets demonstrate LFIoTDI’s exceptional performance, achieving accuracies of 99.08%, 98.15%, and 95.28%, respectively, while maintaining minimal system overhead. These results highlight its broad effectiveness in the IoT environment.},
  archive      = {J_COMCOM},
  author       = {Zaiting Xu and Qian Lu and Fei Chen and Hequn Xian},
  doi          = {10.1016/j.comcom.2025.108149},
  journal      = {Computer Communications},
  month        = {5},
  pages        = {108149},
  shortjournal = {Comput. Commun.},
  title        = {LFIoTDI: A lightweight and fine-grained device identification approach for IoT security enhancement},
  volume       = {237},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Analysis of attack-defense game for advanced malware propagation control in cloud. <em>COMCOM</em>, <em>237</em>, 108148. (<a href='https://doi.org/10.1016/j.comcom.2025.108148'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In modern society, cloud computing has emerged as an indispensable infrastructure. Nevertheless, as the cloud ecosystem grows increasingly vast and complex, a series of novel security challenges have surfaced, among which artificial intelligence (AI)-empowered advanced malware has provided network attackers with even more stealthy and potent weapons. While existing malware detection technologies can still maintain a certain level of defense against traditional security threats, the instant detection and response to these sophisticated AI-crafted threats become exceedingly difficult, consuming substantial remediation time and security resources. To address the balance between control costs and effectiveness, recognizing the intricately intertwined and dynamically interactive nature of the offensive and defensive parties, this paper introduces the framework of differential game theory, delving into the strategies for controlling the propagation of advanced malware in cloud environments. Firstly, we construct an advanced malware propagation control model targeting each virtual machine. On this basis, we define the specific categories of strategy selection for both the offensive and defensive sides, as well as their respective cost-benefit relationships, and formulate an attack-defense game problem. Subsequently, we rigorously demonstrate, from a mathematical theoretical perspective, that the optimal solution (i.e., Nash equilibrium) to the attack-defense game problem is indeed attainable, and we devise a dedicated accelerated algorithm for its solution. Finally, we conduct comparative experiments on three real-world datasets using three distinct strategies, and the analysis results show the effectiveness of our proposed method.},
  archive      = {J_COMCOM},
  author       = {Liang Tian and Chenquan Gan and Jiabin Lin and Fengjun Shang and Qingyi Zhu},
  doi          = {10.1016/j.comcom.2025.108148},
  journal      = {Computer Communications},
  month        = {5},
  pages        = {108148},
  shortjournal = {Comput. Commun.},
  title        = {Analysis of attack-defense game for advanced malware propagation control in cloud},
  volume       = {237},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Introducing and evaluating SWI-FEED: A smart water IoT framework designed for large-scale contexts. <em>COMCOM</em>, <em>237</em>, 108146. (<a href='https://doi.org/10.1016/j.comcom.2025.108146'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The digitalization of Water Distribution Systems (WDSs) is becoming a key objective in modern society. The increasing complexity of contemporary WDSs, driven by urbanization, fluctuating consumer demand, and limited resources, makes their management particularly challenging, especially in large-scale scenarios. This paper proposes the SWI-FEED framework designed to facilitate the widespread deployment of the Internet of Things (IoT) for enhanced monitoring and optimization of WDSs. The framework aims to investigate the utilization of massive IoT in monitoring and optimizing WDSs in different contexts, with a particular focus on four use cases such as optimal node activation, IoT gateways deployment, distributed leakage detection and water demand disaggregation. SWI-FEED has been tested with predefined network models available in the Open Water Analytics community public repository. Specifically, the four use cases are evaluated using a large network consisting of 4,419 sensor nodes, 3 tanks and 5,066 pipes. Overall, this comprehensive framework provides a holistic approach to address possible challenges of a WDS and optimize the efficiency of large-scale IoT deployments. It reduces the energy consumption of IoT devices within the WDS while enhancing leak detection and localization capabilities in real-world water networks. Our adopted theoretical methodology is based on graph theory, which allows IoT gateways to be strategically positioned to maximize network coverage and minimize infrastructure redundancy. This makes it possible to significantly reduce the number of gateways required and, consequently, the overall system energy consumption.},
  archive      = {J_COMCOM},
  author       = {Antonino Pagano and Domenico Garlisi and Fabrizio Giuliano and Tiziana Cattai and Redemptor Jr Laceda Taloma and Francesca Cuomo},
  doi          = {10.1016/j.comcom.2025.108146},
  journal      = {Computer Communications},
  month        = {5},
  pages        = {108146},
  shortjournal = {Comput. Commun.},
  title        = {Introducing and evaluating SWI-FEED: A smart water IoT framework designed for large-scale contexts},
  volume       = {237},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). ASAP 2.0: Autonomous & proactive detection of malicious applications for privacy quantification in 6G network services. <em>COMCOM</em>, <em>237</em>, 108145. (<a href='https://doi.org/10.1016/j.comcom.2025.108145'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {While 6G networks, reliant on software, promise significant advancements, the proliferation of diverse applications deployed closer to users poses considerable privacy challenges. To counter this, privacy-first software development, as advocated by DevPrivOps, becomes essential. While Privacy-Enhancing Technologies (PETs) are frequently used, their limitations are well-documented. DevPrivOps strives to reinforce software privacy through prioritization, compliance, transparency, optimization, and informed decision-making. A promising alternative to PETs involves quantifying privacy to guide development and pinpoint potential threats, thus enhancing application privacy before deployment on OpenSlice network services. Privacy-centric malicious application detection, amongst other features, is a key component of this privacy quantification framework, serving to inform users of potential harm from such applications. In this study, we focus on privacy-centric malicious application detection. ASAP 2.0, an autonomous system, identifies these threats by scrutinizing requested application permissions. Building on its antecedent, ASAP 2.0 employs a tuned autoencoder trained via unsupervised learning. By analyzing reconstruction errors, it differentiates between potentially harmful and benign applications. A dynamically adjusted threshold assists in the decision-making process. Our model, validated on three public datasets, achieved an average Matthews Correlation Coefficient (MCC) of 0.976, outperforming baseline models such as Logistic Regression and Decision Trees.},
  archive      = {J_COMCOM},
  author       = {Catarina Silva and João Felisberto and João Paulo Barraca and Paulo Salvador},
  doi          = {10.1016/j.comcom.2025.108145},
  journal      = {Computer Communications},
  month        = {5},
  pages        = {108145},
  shortjournal = {Comput. Commun.},
  title        = {ASAP 2.0: Autonomous & proactive detection of malicious applications for privacy quantification in 6G network services},
  volume       = {237},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). NR-U and wi-fi unlicensed spectrum sharing: Design challenges and solutions. <em>COMCOM</em>, <em>237</em>, 108143. (<a href='https://doi.org/10.1016/j.comcom.2025.108143'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper presents an in-depth analysis of the Listen-Before-Talk (LBT) procedures required for cellular technologies operating in the unlicensed spectrum. We specifically investigate the 5G New Radio Unlicensed Standalone (NR-U SA) design under the most recent 3GPP standard-compliant Type 1 Channel Access Procedure (CAP). We highlight and address key challenges in adapting the asynchronous Type 1 CAP (i.e., load-based CAP) to NR-U’s synchronous slot-based framework and propose the Scheduled Type 1 CAP, which introduces a novel scheduling mechanism and implementation of additional sensing to facilitate seamless integration. We carry out our performance evaluation using a full-stack end-to-end network simulator and construct a realistic 3GPP-compliant coexistence scenario in which 5G NR-U SA and Wi-Fi networks operate indoors in unlicensed sub-7 GHz bands. The users of the 5G NR-U SA and Wi-Fi networks demand eXtended Reality applications. Through an extensive evaluation study, we analyse the interplay between the NR-U numerologies and the CAPs used in shared channel access and evaluate their impact on the end-to-end performance of both technologies by considering various quality of service metrics. The results reveal some of the main pitfalls of the LBT procedure defined by 3GPP in TS 37.213 and demonstrate the clear advantages of the proposed Scheduled Type 1 CAP. Our work provides valuable insights into CAPs for shared spectrum, proposes standard-compliant solutions, and lays the groundwork for future research by introducing a pioneering open-source network simulation platform for evaluating NR-U SA coexistence scenarios with the Scheduled Type 1 CAP.},
  archive      = {J_COMCOM},
  author       = {George V. Frangulea and Philippos Assimakopoulos and Biljana Bojović and Sandra Lagén},
  doi          = {10.1016/j.comcom.2025.108143},
  journal      = {Computer Communications},
  month        = {5},
  pages        = {108143},
  shortjournal = {Comput. Commun.},
  title        = {NR-U and wi-fi unlicensed spectrum sharing: Design challenges and solutions},
  volume       = {237},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Protecting an entity by hiding its role in anonymity networks. <em>COMCOM</em>, <em>237</em>, 108109. (<a href='https://doi.org/10.1016/j.comcom.2025.108109'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Knowing the role of entities in a network undermines anonymity and facilitates the identification of their behavioral patterns. In many existing low-latency anonymity network structures, the creation and use of tunnels for packet transmission allow adversaries to discern the roles of entities. This paper explores the impact of identifying these tunnels, specifically how such identification can expose an entity’s role in relation to a particular message, potentially reducing the level of anonymity in the network. To explore this, we first discuss two key aspects of anonymity networks: the distribution of information and the homogeneity of roles. We then analyze several low-latency anonymity structures to assess vulnerabilities related to tunnel identification, evaluating their strengths and weaknesses based on the aforementioned aspects. In addition, we propose a novel network structure that addresses these vulnerabilities by eliminating the conventional tunnel mechanism, which typically requires a tunnel establishment message. This change prevents adversaries from identifying an entity’s role. In the proposed structure, the sender and intermediate relays work together to select distinct routes for each packet, removing the need for the sender to establish a dedicated data tunnel. To provide a deeper analysis, we will describe in detail the information an attacker can obtain by tracing tunneling messages and how this compromises anonymity by exposing the roles of entities. We will also evaluate how these changes affect the degree of anonymity based on the attacker’s knowledge. Our evaluation shows that the proposed technique effectively eliminates traffic patterns that would normally reveal the roles of entities, thus neutralizing the attacker’s ability to compromise anonymity. As a result, the average level of anonymity is significantly improved compared to previous structures. Overall, our findings suggest that the proposed approach offers a more effective strategy for concealing the roles of entities compared to earlier methods.},
  archive      = {J_COMCOM},
  author       = {Reza Mirzaei and Nasser Yazdani and Mohammad Sayad Haghighi},
  doi          = {10.1016/j.comcom.2025.108109},
  journal      = {Computer Communications},
  month        = {5},
  pages        = {108109},
  shortjournal = {Comput. Commun.},
  title        = {Protecting an entity by hiding its role in anonymity networks},
  volume       = {237},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Research on intelligent ship resilient network architecture based on SDN. <em>COMCOM</em>, <em>236</em>, 108151. (<a href='https://doi.org/10.1016/j.comcom.2025.108151'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the extensive adoption of information and communication technology (ICT) in the maritime field, intelligent ships are increasingly dependent on system integration, control, and data collection from devices. Real-time data transmission is essential for ensuring stable ship system operations. However, communication link failures frequently become key factors impacting data transmission. To this end, we propose an SDN-based intelligent ship network architecture, SDN-Intelligent Ship Network Architecture (SDISN), to simplify network management and enable centralized control of intelligent ships. On this basis, we design a link failure recovery model tailored for different maritime communication services to address the issue of sudden communication link failures. The model begins by collecting the status of the intelligent ship network and pre-defining backup flow rules for different maritime communication service flows. Considering the service flow characteristics, the optimization aims to minimize transmission delay and maximize switch TCAM utilization for life-safety communication flows and ship operational communication flows, respectively. For life-safety communication flows, we introduce a heuristic algorithm that progressively relaxes constraints. Meanwhile, we preload backup flow rules into switches. For ship operational communication flows, we apply a two-stage optimization algorithm, storing the relevant backup flow rules in the controller. Additionally, we propose a backup storage strategy for commercial communication flows based on dynamically adjusting the memory load of switches. Compared to existing approaches, the SDISN satisfies the need for real-time data transmission in intelligent ships while balancing resource consumption and fault response time in its link failure recovery mechanism. Lastly, experiments conduct on a testbed in a real network environment further validate the model's efficacy and efficiency.},
  archive      = {J_COMCOM},
  author       = {Qing Hu and Jiabing Liu and Zhengfei Wang and Haoyu Si and Sinian Jin and Ying Zhang and Jinhai Li},
  doi          = {10.1016/j.comcom.2025.108151},
  journal      = {Computer Communications},
  month        = {4},
  pages        = {108151},
  shortjournal = {Comput. Commun.},
  title        = {Research on intelligent ship resilient network architecture based on SDN},
  volume       = {236},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). LBMDTE: Multi-domain traffic engineering in distributed software-defined networks. <em>COMCOM</em>, <em>236</em>, 108147. (<a href='https://doi.org/10.1016/j.comcom.2025.108147'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Large-scale Software-Defined Networks (SDN) applications rely on a distributed control architecture to manage network resources collaboratively among multiple subdomains. This requires multi-domain traffic engineering (TE) for reliable, comprehensive, and efficient traffic scheduling. However, the impact of control message traffic on link load has been ignored in previous multi-domain TE studies. Here, we explore a multi-objective load balancing scheme to address the traffic scheduling imbalance problem for the flat distributed architecture. First, we introduce four types of control message traffic and rules for intra-domain and inter-domain communication. Second, we develop a traffic optimization model to balance the controller load and minimize the maximum link utilization. Third, we propose a hierarchical routing algorithm to compute inter-domain routing, and then propose a heuristic Load Balancing Based Multi-Domain Traffic Engineering (LBMDTE) algorithm to address the optimization objective. Experiments conducted on three real networks and one synthetic network demonstrate that the control link traffic accounts for up to 11.32% of the total link traffic. Our proposed LBMDTE is able to jointly balance the controller load and the link load in comparison with other TE mechanisms.},
  archive      = {J_COMCOM},
  author       = {Kun Wang and Guanghong Lv},
  doi          = {10.1016/j.comcom.2025.108147},
  journal      = {Computer Communications},
  month        = {4},
  pages        = {108147},
  shortjournal = {Comput. Commun.},
  title        = {LBMDTE: Multi-domain traffic engineering in distributed software-defined networks},
  volume       = {236},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A comprehensive survey of network digital twin architecture, capabilities, challenges, and requirements for Edge–Cloud continuum. <em>COMCOM</em>, <em>236</em>, 108144. (<a href='https://doi.org/10.1016/j.comcom.2025.108144'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Network Digital Twin (NDT) collects data from physical, virtual, and software components and supports real-time network performance analysis, emulation, and intelligent physical network control. This paper surveys the current state of NDT specifications and explores NDT benefits for Network Operators (NOs) and its possible roles in future network management. It discusses the NDT key components, architecture, and integration of Machine Learning and Artificial Intelligence models in the NDT. Further, it covers virtualization technology management, suitability of Software-Defined Networking capabilities, and simulation tools to empower NDT. Two perspectives make the position of this survey different from existing studies; first, it highlights NDT limitations regarding Edge–Cloud Continuum (ECC) contextualization. ECC is a purposeful trending integration of Edge and Cloud Computing, involving multiple stakeholders like Service Providers, Customers, and Platform or Infrastructure Providers. However, current NDT specifications have not mentioned the ways to benefit stakeholders other than NOs. We also discuss notable computing and communication technologies transformations necessary to consider during NDT modeling, the existing data models, and reusable vocabularies that can be extended to achieve a detailed ECC representation for all stakeholders, essentially for Service Providers and Customers. Secondly, a data model is proposed that covers descriptive and prescriptive features and aims to provide a granular representation of ECC components to meet stakeholders’ requirements and render particular user information views. Different explored NDT perspectives, and proposed data model reduces the impact of existing NDT limitations in ECC representation.},
  archive      = {J_COMCOM},
  author       = {Syed Mohsan Raza and Roberto Minerva and Noel Crespi and Maira Alvi and Manoj Herath and Hrishikesh Dutta},
  doi          = {10.1016/j.comcom.2025.108144},
  journal      = {Computer Communications},
  month        = {4},
  pages        = {108144},
  shortjournal = {Comput. Commun.},
  title        = {A comprehensive survey of network digital twin architecture, capabilities, challenges, and requirements for Edge–Cloud continuum},
  volume       = {236},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). INT-LLPP: Lightweight in-band network-wide telemetry with low-latency and low-overhead path planning. <em>COMCOM</em>, <em>236</em>, 108142. (<a href='https://doi.org/10.1016/j.comcom.2025.108142'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the increasing complexity of networks, network telemetry becomes a critical part of network management. However, existing network telemetry systems still suffer from excessive control overhead, forwarding overhead, and latency. In this paper, we propose INT-LLPP, a novel in-band network-wide telemetry system with low-latency and low-overhead path planning. The network telemetry architecture of INT-LLPP is unique in that it only requires a set of probes to collect telemetry items for multiple service flows. Moreover, the proposed Probe Path Generation (PPG) algorithm optimizes the probe paths to reduce the forwarding overhead and achieve full network coverage. To balance the telemetry latency and control overhead, we propose an efficient algorithm called the Simulated Annealing Maximum Latency Setting (SAMLS) algorithm, which controls the length of the probe paths. Simulation results show that INT-LLPP can reduce network telemetry control overhead by over 50% and reduce forwarding overhead by 5% to 10%. Moreover, INT-LLPP can lower telemetry latency by 30% to 40%.},
  archive      = {J_COMCOM},
  author       = {Penghui Zhang and Hua Zhang and Yuqi Dai and Cheng Zeng and Jingyu Wang and Jianxin Liao},
  doi          = {10.1016/j.comcom.2025.108142},
  journal      = {Computer Communications},
  month        = {4},
  pages        = {108142},
  shortjournal = {Comput. Commun.},
  title        = {INT-LLPP: Lightweight in-band network-wide telemetry with low-latency and low-overhead path planning},
  volume       = {236},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Efficient paths determining strategies in mobile crowd-sensing networks with AI-based sensors forwarding data. <em>COMCOM</em>, <em>236</em>, 108138. (<a href='https://doi.org/10.1016/j.comcom.2025.108138'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Selecting a sufficient number of mobile users to collect and upload collected data to the server is a critical issue in the Mobile Crowd-sensing Networks (MCN). Previous studies have assumed that mobile users upload collected data over cellular networks, which could cause heavily burden to users. This work focus on how to forward collected data by pre-deployed wireless sensors which can fuse collected data and operate as edge nodes. Specifically, given the reward paid to each mobile user depends on the time he spends on data collection and uploading, this work investigates the problem how to select Points of Interest (PoIs) and edge nodes for participants who already have schedules with the objective of minimizing the total reward paid to all participants. We boil down this problem to the problem of determining path for each participant which connects participant’s initial location to PoI, then to an edge node and finally to participant’s destination. We formulate it as Paths determination with Cost Minimization problem. We can prove that this problem is an NP-Complete problem. Considering that the sensors acted as edge nodes which may be rechargeable or have limited energy, we design three heuristic algorithms: Minimum Cost Algorithm (MCA), Minimum Cost with Energy Consideration Algorithm (MCECA), and Energy Balance Algorithm (EBA) to address this problem. Finally, we conduct extensive simulations to validate the efficiency of the proposed algorithms. The results demonstrate that MCA finds paths for users with lower cost, while EBA effectively balances the energy consumption of edge nodes.},
  archive      = {J_COMCOM},
  author       = {Jiaoyan Chen and Jin Liu and Zhehao Cheng and Laurence Tianruo Yang and Xianjun Deng and Yihong Chen},
  doi          = {10.1016/j.comcom.2025.108138},
  journal      = {Computer Communications},
  month        = {4},
  pages        = {108138},
  shortjournal = {Comput. Commun.},
  title        = {Efficient paths determining strategies in mobile crowd-sensing networks with AI-based sensors forwarding data},
  volume       = {236},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). PDSLS: An approximation SINR-based shortest link scheduling algorithm with power control. <em>COMCOM</em>, <em>236</em>, 108137. (<a href='https://doi.org/10.1016/j.comcom.2025.108137'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we examine the Shortest Link Scheduling (SLS) issue in wireless networks within the context of the Signal-to-Interference-plus-Noise-Ratio ( SINR ) interference model with oblivious power control, and propose an approximation Diamond-based SLS with Power control (PDSLS) algorithm. Given that numerous nodes within wireless networks operate on battery power, minimizing the transmission power not only reduces interference but also conserves energy. We adopt a novel link classification approach, reducing nodes’ transmission power without generating “black and gray” links. We schedule links belonging to each class by dividing the link deployment plane into diamonds. The validity and efficacy of our algorithm are demonstrated through theoretical analysis and simulation outcomes. Numerical analysis shows that our approximation ratio is tighter than the best known ones in state-of-the-art algorithms. Simulation results demonstrate that the proposed algorithm effectively reduces the transmission power and the number of required time slots compared to the state-of-the-art algorithms.},
  archive      = {J_COMCOM},
  author       = {Neda Mohammadi and Bahram Sadeghi Bigham and Mehdi Kadivar},
  doi          = {10.1016/j.comcom.2025.108137},
  journal      = {Computer Communications},
  month        = {4},
  pages        = {108137},
  shortjournal = {Comput. Commun.},
  title        = {PDSLS: An approximation SINR-based shortest link scheduling algorithm with power control},
  volume       = {236},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). AI-powered resilience: A dual-approach for outage management in dense cellular networks. <em>COMCOM</em>, <em>236</em>, 108129. (<a href='https://doi.org/10.1016/j.comcom.2025.108129'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As 5G evolves to 6G, network management faces growing challenges with increasing base station density, leading to more frequent outages. To address this, we introduce a robust, automated two-tier framework for outage management. The first tier involves an artificial intelligence-based outage detection scheme using an enhanced XGBoost model (Impv-XGBoost), which incorporates autoencoder outputs for hyperparameter tuning. The analysis shows Impv-XGBoost’s superior performance in high shadowing conditions and with sparse data, outperforming existing methods. The second tier adopts an actor–critic reinforcement learning strategy for outage compensation by adjusting the tilt of the neighboring base station and power. To prevent service declines to connected user equipment, our compensation scheme accounts for both outage-affected users and those connected to compensating base stations. We design a reward scheme that combines Jain’s fairness index and the geometric mean of the reference signal received power to ensure fairness and enhance convergence. Performance evaluations for single and multiple base station failures show coverage improvements for outage-affected users without compromising the coverage of the users in compensating base stations.},
  archive      = {J_COMCOM},
  author       = {Waseem Raza and Muhammad Umar Bin Farooq and Aneeqa Ijaz and Marvin Manalastas and Ali Imran},
  doi          = {10.1016/j.comcom.2025.108129},
  journal      = {Computer Communications},
  month        = {4},
  pages        = {108129},
  shortjournal = {Comput. Commun.},
  title        = {AI-powered resilience: A dual-approach for outage management in dense cellular networks},
  volume       = {236},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). SQID: A deep learning and network design synergy for next-generation IoT resource allocation management. <em>COMCOM</em>, <em>236</em>, 108128. (<a href='https://doi.org/10.1016/j.comcom.2025.108128'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The exponential growth of mobile broadband and Internet of Things (IoT) devices has pushed traditional IoT models to their operational limits, necessitating more efficient data management strategies. This research introduces the SQID framework, a solution that integrates advanced techniques, including Sierpinski triangle design (STD) for network optimization, quantum density peak clustering (QDPC) for intelligent device clustering, and improved deep deterministic policy gradient (IDDPG) for deep learning-driven traffic prediction. By utilizing STD to optimize device communication, the framework applies the QDPC algorithm to efficiently cluster devices, ensuring balanced packet distribution and minimizing latency. Additionally, IDDPG enhances network performance by enabling accurate traffic prediction and resource allocation, optimizing data transmission. Extensive simulations reveal that SQID outperforms existing methods in critical metrics such as time efficiency, latency reduction, throughput maximization, and packet loss. These results indicate that SQID has the potential to significantly improve data management in IoT networks, paving the way for next-generation IoT advancements.},
  archive      = {J_COMCOM},
  author       = {Ali. M.A. Ibrahim and Zhigang Chen and Yijie Wang and Hala A. Eljailany},
  doi          = {10.1016/j.comcom.2025.108128},
  journal      = {Computer Communications},
  month        = {4},
  pages        = {108128},
  shortjournal = {Comput. Commun.},
  title        = {SQID: A deep learning and network design synergy for next-generation IoT resource allocation management},
  volume       = {236},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Target wake time in IEEE 802.11 WLANs: Survey, challenges, and opportunities. <em>COMCOM</em>, <em>236</em>, 108127. (<a href='https://doi.org/10.1016/j.comcom.2025.108127'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Wi-Fi has become the most widely used Wireless Local Area Network (WLAN) technology, but as the density of WLAN deployments and the number of devices per network increase, congestion has become a significant issue. Scheduling features in Wi-Fi have the potential to alleviate these issues by managing medium access more efficiently. While there have been other scheduling features in Wi-Fi in the past, none were widely adopted and were limited in functionality. Target Wake Time (TWT) introduces powerful scheduling capabilities to Wi-Fi as part of Wi-Fi 6, representing a fundamental shift in how Wi-Fi Access Points (APs) manage channel access for Stations (STAs) while improving energy efficiency. TWT is extremely versatile and is poised to play a crucial role in reducing contention and enhancing performance, especially in dense network environments. The potential benefits are particularly valuable for Internet of Things (IoT) scenarios, where low power consumption and efficient medium access are essential due to the large number of connected devices. This paper presents an in-depth survey of the research on TWT, categorizing and analyzing existing literature while identifying practical challenges often overlooked due to idealized assumptions. We introduce comprehensive models of infrastructure-mode APs and STAs to facilitate discussions of current and future work. We provide a detailed analysis of the challenges and issues that must be addressed to fully realize the performance gains promised by TWT. Furthermore, we explore the potential future applications of TWT, particularly in conjunction with upcoming features in IEEE 802.11 such as multi-link operation (MLO) and multi-AP coordination, offering insights into novel uses of TWT for enhancing Wi-Fi performance.},
  archive      = {J_COMCOM},
  author       = {Shyam Krishnan Venkateswaran and Ching-Lun Tai and Atif Ahmed and Raghupathy Sivakumar},
  doi          = {10.1016/j.comcom.2025.108127},
  journal      = {Computer Communications},
  month        = {4},
  pages        = {108127},
  shortjournal = {Comput. Commun.},
  title        = {Target wake time in IEEE 802.11 WLANs: Survey, challenges, and opportunities},
  volume       = {236},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Towards green networking: Efficient dynamic radio resource management in open-RAN slicing using deep reinforcement learning and transfer learning. <em>COMCOM</em>, <em>236</em>, 108126. (<a href='https://doi.org/10.1016/j.comcom.2025.108126'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Next Generation Wireless Networks (NGWNs) are characterized by agility and flexibility. It introduces new technologies such as network slicing (NS) and Open Radio Access Network (O-RAN). NS supports multiple services with different requirements whereas O-RAN supports different network suppliers and provides Mobile Network Operators (MNOs) more intelligent control. Deep Reinforcement Learning (DRL) techniques have been presented to address resource management and other problems in NGWNs in recent years. However, instability and lateness in convergence are the main obstacles against their adoption in live networks. Moreover, deep learning models consume lots of energy and emit significant amounts of carbon dioxide which badly impacts climate. This paper addresses solving the dynamic radio resource management (RRM) problem in O-RAN slicing with DRL and Transfer Learning (TL), focusing on proposing a green model that minimizes power and energy consumption, decreasing the carbon footprint. A new latency-and-reliability-based reward function is designed. Then, a variable threshold action filtration mechanism is proposed, and a policy TL approach is proposed to accelerate the performance in commercial networks. Compared with the state-of-the-art, this work significantly improved exploration stability, convergence speed, Quality of Service (QoS) satisfaction, power and energy consumption, and emitted carbon footprint.},
  archive      = {J_COMCOM},
  author       = {Heba Sherif and Eman Ahmed and Amira M. Kotb},
  doi          = {10.1016/j.comcom.2025.108126},
  journal      = {Computer Communications},
  month        = {4},
  pages        = {108126},
  shortjournal = {Comput. Commun.},
  title        = {Towards green networking: Efficient dynamic radio resource management in open-RAN slicing using deep reinforcement learning and transfer learning},
  volume       = {236},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). IoT edge network interoperability. <em>COMCOM</em>, <em>236</em>, 108125. (<a href='https://doi.org/10.1016/j.comcom.2025.108125'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Network interoperability is crucial for achieving seamless communication across Internet of Things (IoT) environments. IoT comprises heterogeneous devices and systems supporting diverse technologies, protocols, and manufacturers. Enabling devices to communicate and exchange data effectively, regardless of underlying protocols, is key to building cohesive and integrated IoT networks. IoT has transformed multiple sectors ranging from home automation to healthcare—by harnessing a vast array of sensors and actuators that communicate through cloud, fog, and edge layers. However, the variety in device manufacturing and communication standards demands interoperable interfaces, and most current solutions depend on cloud-based centralised architectures. These architectures introduce latency and scalability challenges, particularly for resource-constrained IoT devices that often struggle to communicate with the cloud due to limited resources. This paper addresses network interoperability at the IoT edge level, focusing on resource-efficient communication by integrating Wi-Fi and Bluetooth, two commonly used protocols in IoT ecosystems. We have implemented a network edge interoperability solution that supports effective data exchange between devices operating on these distinct protocols, enhancing the overall efficiency, flexibility, and scalability of IoT systems. Our approach allows devices interoperate by addressing network latency and bandwidth limitations, incorporating an integrated controller to facilitate broader applications and enhance performance across IoT networks. Our findings illustrate how bridging protocol differences can foster more resilient and adaptable IoT solutions, advancing the deployment of IoT applications across various domains and use cases.},
  archive      = {J_COMCOM},
  author       = {Tanzima Azad and M.A. Hakim Newton and Jarrod Trevathan and Abdul Sattar},
  doi          = {10.1016/j.comcom.2025.108125},
  journal      = {Computer Communications},
  month        = {4},
  pages        = {108125},
  shortjournal = {Comput. Commun.},
  title        = {IoT edge network interoperability},
  volume       = {236},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A stochastic analysis of the gasper protocol. <em>COMCOM</em>, <em>236</em>, 108123. (<a href='https://doi.org/10.1016/j.comcom.2025.108123'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Ethereum has recently switched to a Proof of Stake consensus protocol called Gasper. We analyze Gasper using PRISM+ , an extension of the probabilistic model checker PRISM with primitives for modeling blockchain data types. PRISM+ is therefore used to rapidly and automatically analyze the robustness of Gasper when tuning, up or down, several basic parameters of the protocol, such as network latencies and number of validators. We also study the effectiveness of Gasper in updating stakes and its resilience to three attacks: the balance, bouncing and time attacks.},
  archive      = {J_COMCOM},
  author       = {Cosimo Laneve and Adele Veschetti},
  doi          = {10.1016/j.comcom.2025.108123},
  journal      = {Computer Communications},
  month        = {4},
  pages        = {108123},
  shortjournal = {Comput. Commun.},
  title        = {A stochastic analysis of the gasper protocol},
  volume       = {236},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Energy efficient LEO satellite communications: Traffic-aware payload switch-off techniques. <em>COMCOM</em>, <em>236</em>, 108122. (<a href='https://doi.org/10.1016/j.comcom.2025.108122'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Low Earth orbit (LEO) satellite constellations have a pivotal role in shaping the future of communication networks by providing extensive global coverage. However, ensuring the long-term viability of LEO constellations relies on addressing significant challenges, particularly in the domains of energy efficiency and maximizing the lifespan of satellites. This paper introduces a novel approach that considers user traffic demands to optimize power consumption. By implementing a traffic-aware strategy, redundant satellites can be intelligently switched-off, resulting in significant power savings within the LEO constellation. To accomplish this objective, we formulate the problem of joint satellite beam assignment and beam power allocation as a mixed binary integer optimization problem while carefully considering the constraints imposed by satellite-user visibility and the need to fulfill the data traffic requirements of all ground users. To tackle the formulated problem, we employ a framework called the Difference of Convex Programming and Multiplier Penalty (DCMP) based convexification approach, which ensures convergence to a local optimum. The reformulated convex problem is solved using the low-complexity iterative algorithm, Successive Convex Approximation (SCA). Additionally, we propose a heuristic algorithm based on slant distance, which offers a simplified and efficient solution to the joint problem. To corroborate the effectiveness and validity of the proposed techniques, we assess and compare their performance via simulations, considering practical constellation patterns and realistic user traffic distribution. It has been shown that approximately 43% of the satellite nodes can be switched-off for energy saving, and thus, extending the constellation lifetime and reducing the aggregated interference from multi-beam satellites.},
  archive      = {J_COMCOM},
  author       = {Vaibhav Kumar Gupta and Hayder Al-Hraishawi and Eva Lagunas and Symeon Chatzinotas},
  doi          = {10.1016/j.comcom.2025.108122},
  journal      = {Computer Communications},
  month        = {4},
  pages        = {108122},
  shortjournal = {Comput. Commun.},
  title        = {Energy efficient LEO satellite communications: Traffic-aware payload switch-off techniques},
  volume       = {236},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A defense mechanism for federated learning in AIoT through critical gradient dimension extraction. <em>COMCOM</em>, <em>236</em>, 108114. (<a href='https://doi.org/10.1016/j.comcom.2025.108114'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Leveraging the distributed nature of the Internet of Things (IoT), Federated Learning (FL) facilitates knowledge transfer among heterogeneous IoT devices, enhancing the capabilities of Artificial Intelligence of Things (AIoT) while preserving data privacy. However, FL is susceptible to poisoning attacks such as label flipping, Gaussian, and backdoor attacks. Most existing defense strategies rely on robust aggregation algorithms that use the statistical properties of gradient vectors to counteract poisoning attacks, however, they often overlook the non-independent and identically distributed (non-iid) nature of client data, limiting their effectiveness in the IoT. We propose a method that combines cross-node Top-k gradient vector compression and Principal Component Analysis (PCA) dimensionality reduction to extract critical gradient dimensions. By clustering these essential dimensions and performing filtering, our approach effectively distinguishes malicious from benign clients in non-iid data scenarios. Additionally, we introduce a client trust-score assessment mechanism that continuously monitors client behavior and applies secondary filtering, further improving the identification of malicious clients. Experimental results on the CIFAR-10, MNIST, DomainNet, and Flowers102 datasets demonstrate that our method achieves higher model accuracy and robustness in non-iid data settings compared to existing defense strategies.},
  archive      = {J_COMCOM},
  author       = {Jian Xu and Bing Guo and Fei Chen and Yan Shen and Shengxin Dai and Cheng Dai and Yuchuan Hu},
  doi          = {10.1016/j.comcom.2025.108114},
  journal      = {Computer Communications},
  month        = {4},
  pages        = {108114},
  shortjournal = {Comput. Commun.},
  title        = {A defense mechanism for federated learning in AIoT through critical gradient dimension extraction},
  volume       = {236},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Optimizing UAV delivery for pervasive systems through blockchain integration and adversarial machine learning. <em>COMCOM</em>, <em>236</em>, 108113. (<a href='https://doi.org/10.1016/j.comcom.2025.108113'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Unmanned Aerial Vehicles (UAVs), play a significant role in the advancement of pervasive systems by providing efficient, scalable, and innovative solutions in various sectors, such as smart cities or location-based services. However, the current UAV delivery scenario presents various challenges for recipients, including lengthy identity verification processes, privacy concerns, and risks of fraud and theft. In response to these issues, this paper proposes an innovative system that leverages Blockchain technology and Adversarial Machine Learning (AML) to tackle these problems effectively. The proposed system streamlines the verification process, enhances privacy safeguards, and reduces fraud risks. The integration of AML is crucial as it enables users to have greater control over their personal data, boosting privacy and security. AML also plays a critical role in this system by creating test scenarios that reinforce the machine learning model against adversarial threats, ensuring its precision and dependability in the face of malicious manipulations. The paper also provides details on the practical implementation and evaluation of this system in real-life adversarial situations. The evaluation results demonstrate superior performance on selected metrics, highlighting the potential of this system as an effective solution for verifying recipients in UAV delivery.},
  archive      = {J_COMCOM},
  author       = {Chengzu Dong and Shantanu Pal and Aiting Yao and Frank Jiang and Shiping Chen and Xiao Liu},
  doi          = {10.1016/j.comcom.2025.108113},
  journal      = {Computer Communications},
  month        = {4},
  pages        = {108113},
  shortjournal = {Comput. Commun.},
  title        = {Optimizing UAV delivery for pervasive systems through blockchain integration and adversarial machine learning},
  volume       = {236},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Decentralized coordination for resilient federated learning: A blockchain-based approach with smart contracts and decentralized storage. <em>COMCOM</em>, <em>236</em>, 108112. (<a href='https://doi.org/10.1016/j.comcom.2025.108112'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Machine Learning (ML) in distributed environments increasingly deals with sensitive data (like healthcare or financial records) that cannot be centrally stored or processed due to privacy concerns. Federated Learning (FL) addresses this by enabling model training across decentralized devices, but faces significant challenges including system reliability, node failures, and trust issues among participants. Traditional FL approaches often rely on centralized coordinators, creating single points of failure and potential security vulnerabilities. This paper presents a novel approach to FL that leverages smart contracts, blockchain, and decentralized storage to enhance the traceability and reliability of the learning process. Our proposed system architecture is fully decentralized, eliminating single points of failure and promoting cooperation through a rewarding mechanism. Unlike previous approaches that neglect node fault tolerance, we introduce a smart contract based scheme for managing node failures and electing the aggregator node. The presence of the smart contract, executed on a decentralized permissioned blockchain, provides reliability guarantees and eliminates the need for costly distributed algorithms in terms of message exchange. An experimental study is conducted to evaluate various aspects of the FL system. We present results related to the accuracy and effectiveness of the FL system on ML models. We also examine the performance related to the distribution of the weights of the ML model based on the use of IPFS. Furthermore, we analyze the performance of the smart contract in terms of gas consumption. Lastly, we investigate the impact of failures combined with incentive policies and aggregator election algorithms on the FL system. Our findings demonstrate the viability of the proposed approach, paving the way for more robust, reliable, and efficient FL systems.},
  archive      = {J_COMCOM},
  author       = {Stefano Ferretti and Lorenzo Cassano and Gabriele Cialone and Jacopo D’Abramo and Filippo Imboccioli},
  doi          = {10.1016/j.comcom.2025.108112},
  journal      = {Computer Communications},
  month        = {4},
  pages        = {108112},
  shortjournal = {Comput. Commun.},
  title        = {Decentralized coordination for resilient federated learning: A blockchain-based approach with smart contracts and decentralized storage},
  volume       = {236},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Resiliency focused proactive lifecycle management for stateful microservices in multi-cluster containerized environments. <em>COMCOM</em>, <em>236</em>, 108111. (<a href='https://doi.org/10.1016/j.comcom.2025.108111'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Containerization has become fundamental to deploying cloud-native applications, allowing for the packaging and independent execution of applications. This approach speeds up deployment processes and facilitates the creation of various environments for feature testing. However, the ephemeral nature of containers poses a significant challenge to data persistence, especially during container restarts or migrations across different hosts. This paper proposes a proactive zero-touch management solution for stateful microservices applications, ensuring seamless application lifecycle management. Our solution integrates seamlessly with container platforms such as Kubernetes and supports multi-cluster environments, enhancing fault tolerance and data persistence in stateful applications. The solution has been thoroughly tested on different hardware configurations in the public cloud and with our on-premises servers.},
  archive      = {J_COMCOM},
  author       = {Abd Elghani Meliani and Mohamed Mekki and Adlen Ksentini},
  doi          = {10.1016/j.comcom.2025.108111},
  journal      = {Computer Communications},
  month        = {4},
  pages        = {108111},
  shortjournal = {Comput. Commun.},
  title        = {Resiliency focused proactive lifecycle management for stateful microservices in multi-cluster containerized environments},
  volume       = {236},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Authenticated data visualization for hybrid blockchain-based digital product passports. <em>COMCOM</em>, <em>236</em>, 108110. (<a href='https://doi.org/10.1016/j.comcom.2025.108110'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Digital Product Passport (DPP), introduced by the European Green Deal in 2022, is a key innovation designed to improve product sustainability and circularity by enabling secure and transparent communication among stakeholders. Despite its potential, existing blockchain-based implementations of the DPP face significant limitations, such as scalability challenges and usability issues, which hinder widespread adoption. To address these shortcomings, this paper proposes a hybrid blockchain-based implementation of the DPP that enhances data transparency, integrity, and accessibility while minimizing common drawbacks. The proposed solution utilizes a hybrid blockchain architecture, where data is collected and managed within a private blockchain network and notarized on a public blockchain. Additionally, the central problem of authenticated blockchain data visualization is addressed by proposing a new solution that not only ensures the provenance, integrity, and history consistency of DPP data, but also preserves these properties throughout data processing and visualization. Our experiments demonstrates the effectiveness of our approach, achieving low time consumption and storage overhead. To further promote transparency and collaboration, a selection of the implementation has been made available as open-source projects. We show that hybrid blockchains offer a promising path for realizing the full potential of the Digital Product Passport.},
  archive      = {J_COMCOM},
  author       = {Domenico Tortola and Claudio Felicioli and Andrea Canciani and Fabio Severino},
  doi          = {10.1016/j.comcom.2025.108110},
  journal      = {Computer Communications},
  month        = {4},
  pages        = {108110},
  shortjournal = {Comput. Commun.},
  title        = {Authenticated data visualization for hybrid blockchain-based digital product passports},
  volume       = {236},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Active queue management in 5G and beyond cellular networks using machine learning. <em>COMCOM</em>, <em>236</em>, 108108. (<a href='https://doi.org/10.1016/j.comcom.2025.108108'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper proposes a state-of-the-art framework for adapting Active Queue Management (AQM) in 5G and beyond cellular networks with disaggregated Radio Access Network (RAN) deployments. While existing AQM algorithms effectively mitigate bufferbloat in monolithic RAN deployments, their potential in disaggregated ones remains largely unexplored. This gap particularly relates to AQM algorithms relying on communication between layers distributed across distinct network entities to operate. Our research explores the current literature on AQM, identifies the gaps regarding disaggregated deployments, and introduces a comprehensive framework that employs Artificial Intelligence (AI) and Machine Learning (ML) within the RAN Intelligent Controller (RIC) for adapting AQM in such deployments. We evaluate our novel solution on a previously proposed AQM algorithm which requires cross-layer communication, using OpenAirInterface5G (OAI5G) to deploy a disaggregated RAN and a connected User Equipment (UE) that experiences realistic network conditions, including noise and mobility. Finally, we assess its accuracy through the Quality of Service (QoS) achieved for our disaggregated deployment on the NITOS testbed.},
  archive      = {J_COMCOM},
  author       = {Alexandros Stoltidis and Kostas Choumas and Thanasis Korakis},
  doi          = {10.1016/j.comcom.2025.108108},
  journal      = {Computer Communications},
  month        = {4},
  pages        = {108108},
  shortjournal = {Comput. Commun.},
  title        = {Active queue management in 5G and beyond cellular networks using machine learning},
  volume       = {236},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). In-time conditional handover for B5G/6G. <em>COMCOM</em>, <em>236</em>, 108107. (<a href='https://doi.org/10.1016/j.comcom.2025.108107'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Conditional Handover (CHO) by the 3rd Generation Partnership Project (3GPP) enables efficient user mobility between Base Stations (BSs) by preselecting and preparing Target BSs (T-BSs). However, CHO relies on signal strength for T-BS selection, leading to resource blocking on multiple T-BSs due to signal fluctuations. Existing state-of-the-art methods use deep learning to narrow the list of T-BSs but still lack an effective method for resource reservation timing. This paper presents in-time CHO (iCHO) which exploits historical mobility data to estimate user dwell time at the current BS to reduce resource reservation duration. The proposed iCHO employs a Multivariate Multi-output Single-step Prediction (MMSP) model that leverages a multi-task learning approach to simultaneously predict the minimal list of required T-BSs together with the user dwell time. The model demonstrates remarkable performance across two mobility datasets of different scales, achieving T-BS prediction accuracies of 98% and 95%. It also ensures a 100% handover success rate with a minimum of three and four predicted T-BSs for both datasets, respectively, significantly limiting the list of T-BSs. Moreover, the MMSP model achieves a Mean Absolute Error (MAE) of 19 s and 45 s when predicting the user’s dwell time at the current BS. By utilizing these predictions, iCHO reserves resources at the minimum number of T-BSs immediately before handover. Thus, iCHO can save up to 99% of resources from blockage as compared to the CHO, enabling operators to increase revenue by serving up to eighteen more users with the saved resources.},
  archive      = {J_COMCOM},
  author       = {Sardar Jaffar Ali and Syed M. Raza and Huigyu Yang and Duc Tai Le and Rajesh Challa and Moonseong Kim and Hyunseung Choo},
  doi          = {10.1016/j.comcom.2025.108107},
  journal      = {Computer Communications},
  month        = {4},
  pages        = {108107},
  shortjournal = {Comput. Commun.},
  title        = {In-time conditional handover for B5G/6G},
  volume       = {236},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Enhancing the ACME protocol to automate the management of all x.509 web certificates (Extended version). <em>COMCOM</em>, <em>236</em>, 108106. (<a href='https://doi.org/10.1016/j.comcom.2025.108106'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {X.509 Public Key Infrastructures (PKIs) are widely used for managing X.509 Public Key Certificates (PKCs) to allow for secure communications and authentication on the Internet. PKCs are issued by a trusted third-party Certification Authority (CA), which is responsible for verifying the certificate requester’s information. Recent developments in web PKI show a high proliferation of Domain Validated (DV) certificates but a decline in Extended Validated (EV) certificates, indicating poor authentication of the entities behind web services. The ACME protocol facilitates the deployment of Web Certificates by automating their management. However, it is only limited to DV certificates. This paper proposes an enhancement to the ACME protocol for automating all types of Web X.509 PKCs by using W3C Verifiable Credentials (VCs) to assert a requester’s claims. We argue that any CA’s requirements for issuing a PKC can be expressed as a set of VCs returned in a Verifiable Presentation (VP) that could facilitate the issuance of high-profile certificates such as EV certificates. We also propose a generic communication workflow to request and present VPs, which interact with our ACME enhancement. In this regard, we present proof of our approach by using the OpenID for Verifiable Presentation protocol (OID4VP) to request and present VPs. To assess the feasibility of our solution, we conduct a complexity analysis, evaluating both computational and communication overhead compared to the standard ACME protocol. Finally, we present an implementation of our solution as proof-of-concept.},
  archive      = {J_COMCOM},
  author       = {David A. Cordova Morales and Ahmad Samer Wazan and David W. Chadwick and Romain Laborde and April Rains Reyes Maramara},
  doi          = {10.1016/j.comcom.2025.108106},
  journal      = {Computer Communications},
  month        = {4},
  pages        = {108106},
  shortjournal = {Comput. Commun.},
  title        = {Enhancing the ACME protocol to automate the management of all x.509 web certificates (Extended version)},
  volume       = {236},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multi-layer guided reinforcement learning task offloading based on softmax policy in smart cities. <em>COMCOM</em>, <em>235</em>, 108105. (<a href='https://doi.org/10.1016/j.comcom.2025.108105'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Edge computing is an effective measure for addressing the high demand for computing power on the end-side due to dense task distribution in the mobile Internet. In the case of limited device resources and computing power, how to optimize the task offloading decision has become an important issue for improving computing efficiency. We improve the heuristic algorithm by combining the characteristics of intensive tasks, and optimize the task offloading decision at a lower cost. To overcome the limitation of requiring a large amount of real-time information, we utilize the RL algorithm and design a new reward function to enable the agent to learn from its interactions with the environment. Aiming at the poor performance of the system in the uncertain initial environment, we propose a Q-learning scheme based on the Softmax strategy for the multi-layer agent RL framework. The offloading process is optimized by coordinating agents with different views of the environment between each layer, while balancing the exploration and utilization relationship to improve the performance of the algorithm in a more complex dynamic environment. The experimental results show that in the mobile environment with high device density and diverse tasks, the proposed algorithm achieves significant improvements in key indicators such as task success rate, waiting time, and energy consumption. In particular, it exhibits excellent robustness and efficiency advantages in complex dynamic environments, far exceeding the current benchmark algorithm.},
  archive      = {J_COMCOM},
  author       = {Bin Wu and Liwen Ma and Yu Ji and Jia Cong and Min Xu and Jie Zhao and Yue Yang},
  doi          = {10.1016/j.comcom.2025.108105},
  journal      = {Computer Communications},
  month        = {4},
  pages        = {108105},
  shortjournal = {Comput. Commun.},
  title        = {Multi-layer guided reinforcement learning task offloading based on softmax policy in smart cities},
  volume       = {235},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). DFFL: A dual fairness framework for federated learning. <em>COMCOM</em>, <em>235</em>, 108104. (<a href='https://doi.org/10.1016/j.comcom.2025.108104'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Federated learning (FL) is an emerging paradigm of distributed machine learning that facilitates collaborative training of a global model across multiple clients while preserving client-side data privacy. However, current equality fairness methodologies aim to maintain a more uniform performance distribution across clients, but they fail to consider the varying contributions of different clients. In contrast, collaboration fairness takes into account the contributions of clients but may exclude low-contributing clients in pursuit of the interests of high-contributing clients. To address these concerns, this paper proposes a novel Dual Fair Federated Learning (DFFL) framework. Specifically, we combine the concept of cosine annealing to evaluate each client’s contribution from two perspectives. Then, we utilize client’s contribution as the aggregation weight of the global model to improve the global model accuracy. Additionally, we introduce a personalized design and utilize client’s contribution as a regularization coefficient to achieve dual fairness. Furthermore, we conduct a theoretical analysis of the convergence of the global model. Finally, through comprehensive experiments on benchmark datasets, we demonstrate that our method achieves competitive predictive accuracy and dual fairness.},
  archive      = {J_COMCOM},
  author       = {Kaiyue Qi and Tongjiang Yan and Pengcheng Ren and Jianye Yang and Jialin Li},
  doi          = {10.1016/j.comcom.2025.108104},
  journal      = {Computer Communications},
  month        = {4},
  pages        = {108104},
  shortjournal = {Comput. Commun.},
  title        = {DFFL: A dual fairness framework for federated learning},
  volume       = {235},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Cloud-edge-end integrated artificial intelligence based on ensemble learning. <em>COMCOM</em>, <em>235</em>, 108103. (<a href='https://doi.org/10.1016/j.comcom.2025.108103'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep neural networks (DNNs) have been extensively used in the domains of artificial intelligence (AI) applications. Their inherent complexity primarily drives the deployment of DNN models in cloud environments. However, the geographical distance between the cloud and the end-users fails to meet the low-latency requirements of time-sensitive applications. Edge computing has emerged as a viable way to address this issue, nevertheless, the inherent constraints of limited resources on edge servers pose challenges in supporting intricate models. Solutions relying on network compression or model segmentation often fall short in meeting both performance and reliability needs. For the few ensemble-based solutions, the diversity between base models is not fully explored, and the low-latency advantage of edge computing is not fully utilized. In this paper, we propose a cloud–edge-end integrated approach for building an efficient and reliable DNN inference platform based on ensemble learning. In this design, heterogeneous models are trained on the cloud according to the resource constraints of edge servers, and the inference process is performed independently on each edge server, whose outputs are combined at the end-user side to get the final result. Furthermore, a diversity-based deployment scheme is proposed to build a user-centric network for edge AI. The generation of base models is explored, and the effectiveness of the proposed approach is demonstrated through two case studies.},
  archive      = {J_COMCOM},
  author       = {Zhen Gao and Daning Su and Shuang Liu and Yuqi Zhang and Chenyang Wang and Cheng Zhang and Xiaofei Wang and Tarik Taleb},
  doi          = {10.1016/j.comcom.2025.108103},
  journal      = {Computer Communications},
  month        = {4},
  pages        = {108103},
  shortjournal = {Comput. Commun.},
  title        = {Cloud-edge-end integrated artificial intelligence based on ensemble learning},
  volume       = {235},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Load-balanced multi-user mobility-aware task offloading in multi-access edge computing. <em>COMCOM</em>, <em>235</em>, 108102. (<a href='https://doi.org/10.1016/j.comcom.2025.108102'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In scenarios with dense user network service requests, multi-access edge computing demonstrates significant advantages in reducing user device load and decreasing service response time. However, the dynamic changes in user trajectories cause edge server load fluctuations, inevitably impacting the overall service processing performance. To tackle this problem, this paper introduces a load-balanced multi-user mobility-aware service request offloading method, achieving efficient service request offloading in mobile user scenarios. Specifically, this paper divides the service request offloading problem into two stages: dynamic edge server allocation and real-time offloading decision generation. In the first stage, users are allocated edge servers based on their location distribution, implementing an adaptive decreasing variance optimization server load balancing algorithm to achieve edge server load balancing. In the second stage, based on the edge server allocation results from the first stage, a latency performance self-optimizing task offloading decision-making algorithm is employed to minimize the processing latency of user requests, utilizing dueling double deep Q-network to generate real-time decisions on whether to offload service requests to the corresponding edge servers. According to experimental results, the proposed algorithm markedly decreases the processing latency of user network service requests in scenarios of different scales, with an average task completion rate of 99.94%. This effectively addresses the problem of inefficient processing requests caused by load fluctuations due to user movement in multi-access edge computing.},
  archive      = {J_COMCOM},
  author       = {Shanchen Pang and Meng Zhou and Haiyuan Gui and Xiao He and Nuanlai Wang and Luqi Wang},
  doi          = {10.1016/j.comcom.2025.108102},
  journal      = {Computer Communications},
  month        = {4},
  pages        = {108102},
  shortjournal = {Comput. Commun.},
  title        = {Load-balanced multi-user mobility-aware task offloading in multi-access edge computing},
  volume       = {235},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Secrecy performance optimization for UAV-based relay NOMA systems with friendly jamming. <em>COMCOM</em>, <em>235</em>, 108086. (<a href='https://doi.org/10.1016/j.comcom.2025.108086'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Friendly jamming and relay are effective schemes in physical layer security (PLS) for enhancing security in wireless communication. By deploying unmanned aerial vehicle (UAV)-assisted Non-Orthogonal Multiple Access (NOMA) transmission can extend coverage and enhancing spectrum efficiency. This paper studies the physical layer security of an UAV-based relay NOMA system, consisting of a source, multiple users, and an eavesdropper. To enhance secrecy performance, an additional UAV is employed to transmit jamming signals to the eavesdropper. Moreover, for a more practical approach, we also consider the imperfect collaboration between the jammer device and the legitimate user. The minimum average secrecy rate (MASR) of the users is maximized, assuming that the eavesdropper is capable of intercepting signals both from the source and from the relay UAV. An efficient iterative algorithm is proposed to solve the MASR maximum problem by optimizing UAV trajectories, transmit power, and power allocation coefficients. Simulation results demonstrate that the proposed system achieves 238% better MASR than the system without friendly jamming signals and 633% better than the non-optimal system. In addition, the ability to decode the received signal using successive interference cancellation also significantly affects the MASR of users in the system.},
  archive      = {J_COMCOM},
  author       = {Thanh Trung Nguyen and Tran Manh Hoang and Phuong T. Tran},
  doi          = {10.1016/j.comcom.2025.108086},
  journal      = {Computer Communications},
  month        = {4},
  pages        = {108086},
  shortjournal = {Comput. Commun.},
  title        = {Secrecy performance optimization for UAV-based relay NOMA systems with friendly jamming},
  volume       = {235},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Incentive mechanisms for non-proprietary vehicles in vehicular crowdsensing with budget constraints. <em>COMCOM</em>, <em>235</em>, 108083. (<a href='https://doi.org/10.1016/j.comcom.2025.108083'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Vehicular crowdsensing (VCS) utilizes the onboard sensors and computational capabilities of smart vehicles to collect data across diverse regions. Non-dedicated vehicles, due to their lower cost and broad distribution, have emerged as a central focus in VCS research. However, their trajectories are often concentrated in urban areas, resulting in uneven data coverage. Existing incentive mechanisms primarily rely on platforms to dynamically adjust task allocation based on vehicle trajectory predictions. Yet, they frequently neglect the influence of geographic locations on vehicle routing choices and fail to incentivize proactive route planning. To address this, we propose a novel two-phase incentive mechanism that, for the first time, incorporates a willingness to traverse factor. This mechanism aims to maximize spatial coverage within a limited budget by encouraging vehicles to voluntarily traverse remote areas to complete tasks. In the initial phase, a multi-agent deep reinforcement learning algorithm dynamically adjusts each vehicle’s route and quote price, which is then reported to the platform. In the second phase, the platform allocates tasks and adjusts compensation based on the provided routes and quotes to optimize overall platform benefits. Experimental results show that our mechanism effectively balances platform and vehicle benefits, achieving optimal outcomes even under budget constraints.},
  archive      = {J_COMCOM},
  author       = {Zhirui Feng and Yantao Yu and Guojin Liu and Yang Jiang and TianCong Huang},
  doi          = {10.1016/j.comcom.2025.108083},
  journal      = {Computer Communications},
  month        = {4},
  pages        = {108083},
  shortjournal = {Comput. Commun.},
  title        = {Incentive mechanisms for non-proprietary vehicles in vehicular crowdsensing with budget constraints},
  volume       = {235},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Efficient post-quantum attribute-based access control scheme for blockchain-empowered metaverse data management. <em>COMCOM</em>, <em>234</em>, 108092. (<a href='https://doi.org/10.1016/j.comcom.2025.108092'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Driven by recent advances in mobile networks and distributed computing, the Metaverse provides photorealistic services where humans can experience different virtual landscapes through avatars derived from abundant personal user data. To address significant concerns about privacy breaches in private digital assets, access control based on cryptography systems has become the focus of common research. However, existing designs have scalability and efficiency issues, appealing to more investigation in real-world implementation. In response to this security challenge using the prevalent cryptography tool, this paper proposes an Attribute-Based Access Control mechanism for Metaverse Data management (ABAC-MD). It provides a flexible and secure data-sharing framework that integrates the ciphertext-policy attribute-based encryption scheme with the polynomial function technique on lattice. Reliable outsourcing decryption based on blockchain facilitates efficient data processing by employing an attribute-associated access tree. It exploits a pragmatic solution to guarantee fine-grained data privacy control and fortify resilience against quantum attacks. Simulated experiment with relevant schemes based on custom-made avatars proves that the proposed scheme reduces ciphertext size by 43.6% and improve efficiency by at least 25.4%. With higher security, lower storage costs, and reduced computational complexity, the ABAC-MD is more practical for privacy preservation in Metaverse services.},
  archive      = {J_COMCOM},
  author       = {Yuxuan Pan and Rui Jin and Yu Liu and Lin Zhang},
  doi          = {10.1016/j.comcom.2025.108092},
  journal      = {Computer Communications},
  month        = {3},
  pages        = {108092},
  shortjournal = {Comput. Commun.},
  title        = {Efficient post-quantum attribute-based access control scheme for blockchain-empowered metaverse data management},
  volume       = {234},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). 5GMap: Enabling external audits of access security and attach procedures in real-world cellular deployments. <em>COMCOM</em>, <em>234</em>, 108091. (<a href='https://doi.org/10.1016/j.comcom.2025.108091'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In cellular networks, security vulnerabilities often arise from misconfigurations and improper implementations of protection mechanisms. Typically, ensuring proper security configurations is the responsibility of network operators. The tool described in this paper, called 5GMap, empowers legitimate subscribers, equipped with software-defined radios (Ettus B210 or X310), with innovative means and methodologies for auditing security configurations of the access networks they are connecting to. Specifically, 5GMap allows to evaluate negotiable ciphers, predictability of temporary identifiers (TMSI), resilience against disclosure of privacy-sensitive identifiers (IMSI, IMEI), and susceptibility to downgrade attacks. 5GMap achieves this by iterating access and attach primitives using either carefully crafted signaling messages requiring specific cryptographic configuration, as well as custom methodologies such as using predictable TMSIs and querying the network with non-standard signaling message sequences to detect potential departures from the expected protocol specification. Extensive testing over four mobile network operators and three virtual network operators reveals significant security and privacy issues: many networks allow unencrypted or even unauthenticated communication, TMSI randomness and IMSI concealment are not consistently ensured across all operators tested, and many other fine-grained concerns emerge among different operators. We believe that our findings highlight the usefulness of tools like 5GMap to assess (and ultimately improve, through responsible disclosure) the security posture of 4G and 5G cellular networks in the wild.},
  archive      = {J_COMCOM},
  author       = {Andrea Paci and Matteo Chiacchia and Giuseppe Bianchi},
  doi          = {10.1016/j.comcom.2025.108091},
  journal      = {Computer Communications},
  month        = {3},
  pages        = {108091},
  shortjournal = {Comput. Commun.},
  title        = {5GMap: Enabling external audits of access security and attach procedures in real-world cellular deployments},
  volume       = {234},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Modeling and predicting starlink throughput with fine-grained burst characterization. <em>COMCOM</em>, <em>234</em>, 108090. (<a href='https://doi.org/10.1016/j.comcom.2025.108090'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Leveraging a dataset of almost half a billion packets with high-precision packet times and sizes, we extract characteristics of the bursts emitted over Starlink’s Ethernet interface. The structure of these bursts directly reflects the physical layer reception of OFDMA frames on the satellite link. We study these bursts by analyzing their rates, and thus indirectly also the transition between different physical layer rates. The results highlight that there is definitive structure in the transition behavior, and we note specific behaviors such as particular transition steps associated with rate switching, and that rate switching occurs mainly to neighboring rates. We also study the joint burst rate and burst duration transitions, noting that transitions occur mainly within the same rate, and that changes in burst duration are often performed with an intermediate short burst in-between. Furthermore, we examine the configurations of the three factors burst rate, burst duration, and inter-burst silent time, which together determine the effective throughput of a Starlink connection. We perform pattern mining on these three factors, and we use the patterns to construct a dynamic N-gram model predicting the characteristics of the next upcoming burst, and by extension, the short-term future throughput. We further train a Deep Learning time-series model which shows improved prediction performance.},
  archive      = {J_COMCOM},
  author       = {Johan Garcia and Matthias Beckerle and Simon Sundberg and Anna Brunstrom},
  doi          = {10.1016/j.comcom.2025.108090},
  journal      = {Computer Communications},
  month        = {3},
  pages        = {108090},
  shortjournal = {Comput. Commun.},
  title        = {Modeling and predicting starlink throughput with fine-grained burst characterization},
  volume       = {234},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Heterogeneous multi-agent deep reinforcement learning based low carbon emission task offloading in mobile edge computing. <em>COMCOM</em>, <em>234</em>, 108089. (<a href='https://doi.org/10.1016/j.comcom.2025.108089'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Mobile edge computing is an emerging computing paradigm in the Internet of Things. Task offloading is a critical method in mobile edge computing to alleviate computational resource constraints. Nowadays, the rising number of tasks is placing greater demands on computing resources. The increasing consumption of computing resources leads to high carbon emission. Achieving environmentally friendly mobile edge computing while effectively managing low-carbon task offloading poses a significant challenge. Recently, deep reinforcement learning has made certain progress in many research fields. However, there are few deep reinforcement learning methods that consider the carbon emission in task offloading. In this paper, we propose a deep reinforcement learning based low carbon emission task offloading algorithm for minimizing carbon emission in mobile edge computing. Firstly, since different base stations exist in the mobile edge computing environment, we consider the mobile edge computing environment with multiple heterogeneous agents. Secondly, to minimize carbon emission, we consider the carbon intensity of the base station as an optimization factor. We conclude the task offloading strategy to minimize carbon emission, consequently achieving the minimization of carbon emission. Moreover, our proposed algorithm allows user devices to decide their own preference for task offloading. Based on the specific requirements and preferences of user devices, our proposed algorithm can dynamically adjust the weights of delay, energy consumption, and carbon emission, respectively. Experiments indicate that the proposed algorithm can accurately and quickly conclude the task offloading strategy to minimize carbon emission.},
  archive      = {J_COMCOM},
  author       = {Xiongjie Zhou and Xin Guan and Di Sun and Xiaoguang Zhang and Zhaogong Zhang and Tomoaki Ohtsuki},
  doi          = {10.1016/j.comcom.2025.108089},
  journal      = {Computer Communications},
  month        = {3},
  pages        = {108089},
  shortjournal = {Comput. Commun.},
  title        = {Heterogeneous multi-agent deep reinforcement learning based low carbon emission task offloading in mobile edge computing},
  volume       = {234},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A deep reinforcement learning-based UAV-smallcell system for mobile terminals geolocalization in disaster scenarios. <em>COMCOM</em>, <em>234</em>, 108088. (<a href='https://doi.org/10.1016/j.comcom.2025.108088'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep reinforcement learning (DRL) techniques have the potential to significantly improve the ability of Unmanned Aerial Vehicles (UAVs) for mobile device localization in disaster scenarios by optimizing flight paths and enhancing signal detection accuracy using Reference Signal Received Power (RSRP) measurements. DRL allows UAVs to learn optimal navigation strategies autonomously in dynamic and complex environments, leading to more efficient and accurate localization of mobile devices. The integration between UAVs and 4G/5G technology allows for more accurate and timely localization of mobile devices under the rubble, thereby improving the overall effectiveness of the system. Smallcells, low-power cellular base stations, are used to enhance coverage and capacity. In this study, we propose a DRL-based UAV-Smallcell system that can quickly and efficiently localize devices in large disaster areas. The performance of the proposed system is evaluated through an extensive simulation campaign to demonstrate that our approach significantly improves the effectiveness of mobile device localization compared to other state-of-the-art approaches.},
  archive      = {J_COMCOM},
  author       = {Roberta Avanzato and Francesco Beritelli and Raoul Raftopoulos and Giovanni Schembra},
  doi          = {10.1016/j.comcom.2025.108088},
  journal      = {Computer Communications},
  month        = {3},
  pages        = {108088},
  shortjournal = {Comput. Commun.},
  title        = {A deep reinforcement learning-based UAV-smallcell system for mobile terminals geolocalization in disaster scenarios},
  volume       = {234},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). An anomaly-based approach for cyber–physical threat detection using network and sensor data. <em>COMCOM</em>, <em>234</em>, 108087. (<a href='https://doi.org/10.1016/j.comcom.2025.108087'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Integrating physical and cyber realms, Cyber–Physical Systems (CPSs) expand the potential attack surface for intruders. Given their deployment in critical infrastructures like Industrial Control Systems (ICSs), ensuring robust security is imperative. Current research has developed various Intrusion Detection techniques to identify and counter malicious activities. However, traditional methods often encounter challenges in detecting several attack types due to reliance on a single data source such as time series data from sensors and actuators. In this study, we meticulously design advanced Deep Learning (DL) anomaly-based techniques trained on either sensor/actuator data or network traffic statistics in an unsupervised setting. We evaluate these techniques on network and physical data collected concurrently from a real-world CPS. Through meticulous hyperparameter tuning, we identify the optimal parameters for each model and compare their efficiency and effectiveness in detecting different types of attacks. In addition to demonstrating superior performance compared to various baselines, we showcase the best model for each data source. Eventually, we show how utilizing diverse data sources can enhance cyber-threat detection, recognizing different kinds of attacks.},
  archive      = {J_COMCOM},
  author       = {Roberto Canonico and Giovanni Esposito and Annalisa Navarro and Simon Pietro Romano and Giancarlo Sperlí and Andrea Vignali},
  doi          = {10.1016/j.comcom.2025.108087},
  journal      = {Computer Communications},
  month        = {3},
  pages        = {108087},
  shortjournal = {Comput. Commun.},
  title        = {An anomaly-based approach for cyber–physical threat detection using network and sensor data},
  volume       = {234},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A multi-objective task allocation scheme with privacy-preserving and regional heat in mobile crowdsensing. <em>COMCOM</em>, <em>234</em>, 108085. (<a href='https://doi.org/10.1016/j.comcom.2025.108085'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In Mobile Crowdsensing, platforms typically require all users to upload their location information during the user recruitment phase, then select a subset of users to perform tasks based on location and reputation. However, this approach results in users who upload their location information but do not participate in tasks essentially providing their location data without compensation, posing a risk of location data leakage. If users repeatedly upload location information without receiving compensation for tasks, they may lose confidence in the platform and consequently leave it. Therefore, this paper proposes a multi-objective task allocation scheme based on differential privacy and regional heat, named MTADPRH. During the user recruitment phase, the MTADPRH scheme uses the Optimized Unary Encoding (OUE) mechanism to statistically analyze the distribution of all users, providing privacy protection that meets local differential privacy. In the location upload phase, the scheme adds planar Laplace noise to the location coordinates of participating users to achieve geo-indistinguishability. During the task allocation phase, MTADPRH employs the multi-objective evolutionary algorithm C3M to find Pareto optimal solutions, aiming to maximize the reward per unit distance for users and the revenue for the platform. The experimental results show that, with privacy protect, the MTADPRH scheme achieves the best results in terms of platform revenue, task completion rate, and per-unit distance compensation for users, and it provides a superior Pareto solution.},
  archive      = {J_COMCOM},
  author       = {Yanming Fu and Jiayuan Chen and Haodong Lu and Bocheng Huang and Weigeng Han},
  doi          = {10.1016/j.comcom.2025.108085},
  journal      = {Computer Communications},
  month        = {3},
  pages        = {108085},
  shortjournal = {Comput. Commun.},
  title        = {A multi-objective task allocation scheme with privacy-preserving and regional heat in mobile crowdsensing},
  volume       = {234},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Prioritization of smart meters based on data monitoring for enhanced grid resilience. <em>COMCOM</em>, <em>234</em>, 108082. (<a href='https://doi.org/10.1016/j.comcom.2025.108082'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Smart meters (SM) generate critical data that provides real-time insights into energy consumption, grid performance, and load management, which are essential for improving grid reliability, energy efficiency, and renewable energy integration. However, achieving effective communication between smart meters and the control center remains a challenge due to limitations in Advanced Metering Infrastructure (AMI), including communication delays, metering technology constraints, and restricted data storage and processing capabilities. These limitations hinder the precision and timeliness of real-time data delivery, negatively impacting the efficiency of energy management and grid operations. While existing research predominantly focuses on optimizing communication network algorithms, the critical issue of comprehensive SM data scheduling has received limited attention. Moreover, current methods often fail to account for the complexity of communication networks and the dynamic nature of information flow. To address this gap, this paper introduces a novel method for scheduling SM data access by leveraging real-time data assessment and analysis. A quality metric termed mismatch probability evaluates data quality, and the Hungarian algorithm is employed to optimize meter scheduling. The proposed method is validated using real-world data from a Danish grid, demonstrating significant improvements in information quality for real-time monitoring compared to heuristic-based scheduling approaches.},
  archive      = {J_COMCOM},
  author       = {Asma Farooq and Kamal Shahid and Rasmus Løvenstein Olsen},
  doi          = {10.1016/j.comcom.2025.108082},
  journal      = {Computer Communications},
  month        = {3},
  pages        = {108082},
  shortjournal = {Comput. Commun.},
  title        = {Prioritization of smart meters based on data monitoring for enhanced grid resilience},
  volume       = {234},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multi-agent deep reinforcement learning-based partial offloading and resource allocation in vehicular edge computing networks. <em>COMCOM</em>, <em>234</em>, 108081. (<a href='https://doi.org/10.1016/j.comcom.2025.108081'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The advancement of intelligent transportation systems and the increase in vehicle density have led to a need for more efficient computation offloading in vehicular edge computing networks (VECNs). However, traditional approaches are unable to meet the service demand of each vehicle due to limited resources and overload. Therefore, in this paper, we aim to minimize the long-term computation overhead (including delay and energy consumption) of vehicles. First, we propose combining the computational resources of local vehicles, idle vehicles, and roadside units (RSUs) to formulate a computation offloading strategy and resource allocation scheme based on multi-agent deep reinforcement learning (MADRL), which optimizes the dual offloading decisions for both total and residual tasks as well as system resource allocation for each vehicle. Furthermore, due to the high mobility of vehicles, we propose a task migration strategy (TMS) algorithm based on communication distance and vehicle movement speed to avoid failure of computation result delivery when a vehicle moves out of the communication range of an RSU service node. Finally, we formulate the computation offloading problem for vehicles as a Markov game process and design a Partial Offloading and Resource Allocation algorithm based on the collaborative Multi-Agent Twin Delayed Deep Deterministic Policy Gradient (PORA-MATD3). PORA-MATD3 optimizes the offloading decisions and resource allocation for each vehicle through a centralized training and distributed execution approach. Simulation results demonstrate that PORA-MATD3 significantly reduces the computational overhead of each vehicle compared to other baseline algorithms in VECN scenarios.},
  archive      = {J_COMCOM},
  author       = {Jianbin Xue and Luyao Wang and Qingda Yu and Peipei Mao},
  doi          = {10.1016/j.comcom.2025.108081},
  journal      = {Computer Communications},
  month        = {3},
  pages        = {108081},
  shortjournal = {Comput. Commun.},
  title        = {Multi-agent deep reinforcement learning-based partial offloading and resource allocation in vehicular edge computing networks},
  volume       = {234},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Enhancing healthcare infrastructure resilience through agent-based simulation methods. <em>COMCOM</em>, <em>234</em>, 108070. (<a href='https://doi.org/10.1016/j.comcom.2025.108070'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Critical infrastructures face demanding challenges due to natural and human-generated threats, such as pandemics, workforce shortages or cyber-attacks, which might severely compromise service quality. To improve system resilience, decision-makers would need intelligent tools for quick and efficient resource allocation. This article explores an agent-based simulation model that intends to capture a part of the complexity of critical infrastructure systems, particularly considering the interdependencies of healthcare systems with information and telecommunication systems. Such a model enables to implement a simulation-based optimization approach in which the exposure of critical systems to risks is evaluated, while comparing the mitigation effects of multiple tactical and strategical decision alternatives to enhance their resilience. The proposed model is designed to be parameterizable, to enable adapting it to risk scenarios with different severity, and it facilitates the compilation of relevant performance indicators enabling monitoring at both agent level and system level. To validate the agent-based model, a literature-supported methodology has been used to perform cross-validation, sensitivity analysis and test the usefulness of the proposed model through a use case. The use case analyzes the impact of a concurrent pandemic and a cyber-attack on a hospital and compares different resiliency-enhancing countermeasures using contingency tables. Overall, the use case illustrates the feasibility and versatility of the proposed approach to enhance resiliency.},
  archive      = {J_COMCOM},
  author       = {David Carramiñana and Ana M. Bernardos and Juan A. Besada and José R. Casar},
  doi          = {10.1016/j.comcom.2025.108070},
  journal      = {Computer Communications},
  month        = {3},
  pages        = {108070},
  shortjournal = {Comput. Commun.},
  title        = {Enhancing healthcare infrastructure resilience through agent-based simulation methods},
  volume       = {234},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). RP-DFC: Responsive probes and dynamic flow classification based load balancing in datacenter networks. <em>COMCOM</em>, <em>234</em>, 108069. (<a href='https://doi.org/10.1016/j.comcom.2025.108069'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Datacenter networks achieve high bandwidth by establishing multiple accessible paths between hosts. This necessitates a load-balancing approach to effectively select optimal paths for data flows, minimizing transmission delays and path congestion. The current congestion awareness and load-balancing methods that rely on active detection encounter issues with substantial bandwidth overhead from probes and overlook the distribution characteristics of network traffic. This paper introduces RP-DFC as a distributed load-balancing approach that utilizes responsive probes and flow classification within the data plane. RP-DFC employs in-band network telemetry and active detection to create a responsive probe congestion awareness mechanism. This mechanism can adaptively adjust the detection frequency according to the network congestion status, significantly decreasing the bandwidth overhead of active detection. RP-DFC further enhances network performance in high-load scenarios by employing advanced large-flow and small-flow classification techniques tailored to data centers’ unique traffic distribution characteristics. This strategic implementation at the network edge optimizes traffic management, significantly outperforming existing methods. RP-DFC exhibits a substantial 30% performance improvement over HULA under high-load conditions, concurrently reducing probe overhead by an impressive 98%. Moreover, when benchmarked against alternative methods like W-ECMP, RP-DFC achieves a notable 29% enhancement in performance, highlighting its effectiveness in optimizing data center network operations.},
  archive      = {J_COMCOM},
  author       = {Bo Li and Qiang Li and Bo Peng and Ji Zhao and Shunhua Tan},
  doi          = {10.1016/j.comcom.2025.108069},
  journal      = {Computer Communications},
  month        = {3},
  pages        = {108069},
  shortjournal = {Comput. Commun.},
  title        = {RP-DFC: Responsive probes and dynamic flow classification based load balancing in datacenter networks},
  volume       = {234},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Deeply fused flow and topology features for botnet detection based on a pretrained GCN. <em>COMCOM</em>, <em>233</em>, 108084. (<a href='https://doi.org/10.1016/j.comcom.2025.108084'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The characteristics of botnets are mainly reflected in their network behaviors and the intercommunication relationships among their bots. The existing botnet detection methods typically use only one kind of feature, i.e., flow features or topological features; each feature type overlooks the other type of features and affects the resulting model performance. In this paper, for the first time, we propose a botnet detection model that uses a graph convolutional network (GCN) to deeply fuse flow features and topological features. We construct communication graphs from network traffic and represent node attributes with flow features. The extreme sample imbalance phenomenon exhibited by the existing public traffic datasets makes training a GCN model impractical. To address this problem, we propose a pretrained GCN framework that utilizes a public balanced artificial communication graph dataset to pretrain the GCN model, and the feature output obtained from the last hidden layer of the GCN model containing the flow and topology information is input into the Extra Tree classification model. Furthermore, our model can effectively detect command-and-control (C2) and peer-to-peer (P2P) botnets by simply adjusting the number of layers in the GCN. The experimental results obtained on public datasets demonstrate that our approach outperforms the current state-of-the-art botnet detection models. In addition, our model also performs well in real-world botnet detection scenarios.},
  archive      = {J_COMCOM},
  author       = {Xiaoyuan Meng and Bo Lang and Yuhao Yan and Yanxi Liu},
  doi          = {10.1016/j.comcom.2025.108084},
  journal      = {Computer Communications},
  month        = {3},
  pages        = {108084},
  shortjournal = {Comput. Commun.},
  title        = {Deeply fused flow and topology features for botnet detection based on a pretrained GCN},
  volume       = {233},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Leveraging decentralized communication for privacy-preserving federated learning in 6G networks. <em>COMCOM</em>, <em>233</em>, 108072. (<a href='https://doi.org/10.1016/j.comcom.2025.108072'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Artificial intelligence (AI) is a fundamental pillar in developing next-generation networks. Federated learning (FL) emerges as a promising solution to address data privacy concerns during AI model training within the network. However, training AI models on user equipment raises challenges regarding battery consumption, unreliable connections, and communication overhead. This paper proposes Zenoh, a data-centric communication middleware, as an alternative to the traditional Message Passing Interface (MPI) for FL applications. Zenoh’s decentralized nature and low communication overhead make it suitable for resource-constrained devices and unreliable network connections. The paper compares Zenoh and MPI in a realistic FL scenario, demonstrating Zenoh’s potential to outperform MPI in terms of flexibility, communication efficiency, and system complexity.},
  archive      = {J_COMCOM},
  author       = {Rafael Teixeira and Gabriele Baldoni and Mário Antunes and Diogo Gomes and Rui L. Aguiar},
  doi          = {10.1016/j.comcom.2025.108072},
  journal      = {Computer Communications},
  month        = {3},
  pages        = {108072},
  shortjournal = {Comput. Commun.},
  title        = {Leveraging decentralized communication for privacy-preserving federated learning in 6G networks},
  volume       = {233},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). UAV-assisted mobile edge computing model for cognitive radio-based IoT networks. <em>COMCOM</em>, <em>233</em>, 108071. (<a href='https://doi.org/10.1016/j.comcom.2025.108071'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The explosive growth in Internet of Things (IoT) in terms of number of applications and deployed devices has created many challenges over the past decade. Among the most critical of which are the increasing demand on spectrum resources, the growing computation and data processing cost, and the limited energy resources. In this paper, we present a model for IoT networks that incorporates the technologies of cognitive radio (CR), mobile edge computing (MEC), unmanned aerial vehicles (UAVs), and radio-frequency energy harvesting to address the aforementioned challenges. In this model, UAVs provide computation and energy recharging services to IoT devices. These services can be requested/delivered through multiple spectrum bands by exploiting the CR technology. Specifically, aim at scheduling the task offloading and energy transmission/harvesting activities over time and frequency so that the maximum energy consumption rate among IoT devices is minimized. A mixed integer linear program was formulated to find such schedule. A greedy sub-optimal algorithm was also proposed, where our results show that it is within ≈ 11 % of the optimal solution. We also investigate the maximum energy consumption rate among IoT devices under several settings regarding number of UAV MEC servers, task size, task offloading cost, and task local computation cost.},
  archive      = {J_COMCOM},
  author       = {Hisham M. Almasaeid},
  doi          = {10.1016/j.comcom.2025.108071},
  journal      = {Computer Communications},
  month        = {3},
  pages        = {108071},
  shortjournal = {Comput. Commun.},
  title        = {UAV-assisted mobile edge computing model for cognitive radio-based IoT networks},
  volume       = {233},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). On the right choice of data from popular datasets for internet traffic classification. <em>COMCOM</em>, <em>233</em>, 108068. (<a href='https://doi.org/10.1016/j.comcom.2025.108068'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Machine learning (ML) models used to analyze Internet traffic, similar to models in all other fields of ML, need to be fed by training datasets. Many such sets consist of labeled samples of the collected traffic data from harmful and benign traffic classes captured from the actual traffic. Since the traffic recording tools capture all the transmitted data, they contain much information related to the registration process that is irrelevant to the actual traffic class. Moreover, they are not fully anonymized. Thus, there is a need to preprocess the data before proper modeling, which should always be addressed in related studies, but often, this is not done. In our paper, we focus on the dependence of the efficiency of threat detection ML models by selecting the appropriate data samples from the training sets during preprocessing. We are analyzing three popular datasets: USTC-TFC2016, VPN-nonVPN, and TOR-nonTOR, which are widely used in traffic classification, security, and privacy-enhancing technologies research. We show that some choices of data sample pieces, although maximizing the model’s efficiency, would not result in similar outcomes in the case of traffic data other than the learning set. The reason is that, in these cases, models are biased due to learning incidental correlations that appear in the datasets used for training the model, introduced by auxiliary data related to the network traffic capturing and transmission process. They are present in popular datasets but may never appear in traffic data. Consequently, the models trained on such datasets, without any preprocessing and anonymization, would never reach the accuracy levels of the training data. Our paper introduces five consecutive levels of anonymization of the traffic data and points out that only the highest provide correct learning results. We validate the results by applying decision trees, random forests, and extra tree models. Having found the optimal part of the header data that may safely be used, we focus on the length of the remaining part of the traffic data to find its minimal length, which preserves good detection accuracy.},
  archive      = {J_COMCOM},
  author       = {Jacek Krupski and Marcin Iwanowski and Waldemar Graniszewski},
  doi          = {10.1016/j.comcom.2025.108068},
  journal      = {Computer Communications},
  month        = {3},
  pages        = {108068},
  shortjournal = {Comput. Commun.},
  title        = {On the right choice of data from popular datasets for internet traffic classification},
  volume       = {233},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Evaluating conditional handover for 5G networks with dynamic obstacles. <em>COMCOM</em>, <em>233</em>, 108067. (<a href='https://doi.org/10.1016/j.comcom.2025.108067'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To enhance seamless connectivity in millimetre wave New Radio networks, Conditional handover has evolved as a promising solution. Unlike A3 handover where handover execution is certain after receiving handover command from the serving access network, in Conditional handover, handover execution is conditional on Reference signal received power measurements from current and target access networks, as well as on handover parameters such as preparation and execution offsets. Presence of dynamic obstacles may block the signal from serving and (or) target access networks, which results in violation of the conditions for handover preparation/execution. Moreover, signal blockage by dynamic obstacles may cause radio link failure, which may cause handover failure as well. Analytic evaluation of Conditional handover in the presence of dynamic obstacles is quite limited in the existing literature. In this work, handover performance of Conditional handover has been analysed in terms of handover latency, handover packet loss and handover failure probability. A Markov model accounting the effect of dynamic obstacles, handover parameters (e.g., execution offset, preparation offset, time-to-preparation and time-to-execution), user velocity and channel fading characteristics has been proposed to characterize handover failure. Results obtained from the proposed analytic model have been validated against simulation results. Our study reveals that optimal configuration of handover parameters is actually conditional on the presence of dynamic obstacles, user velocity and fading characteristics. This study will be helpful for the mobile operators to configure handover parameters for New Radio systems where dynamic obstacles are present.},
  archive      = {J_COMCOM},
  author       = {Souvik Deb and Megh Rathod and Rishi Balamurugan and Shankar K. Ghosh and Rajeev Kumar Singh and Samriddha Sanyal},
  doi          = {10.1016/j.comcom.2025.108067},
  journal      = {Computer Communications},
  month        = {3},
  pages        = {108067},
  shortjournal = {Comput. Commun.},
  title        = {Evaluating conditional handover for 5G networks with dynamic obstacles},
  volume       = {233},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A multi-agent enhanced DDPG method for federated learning resource allocation in IoT. <em>COMCOM</em>, <em>233</em>, 108066. (<a href='https://doi.org/10.1016/j.comcom.2025.108066'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the Internet of Things (IoT), federated learning (FL) is a distributed machine learning method that significantly improves model performance by utilizing local device data for collaborative training. However, applying FL in IoT also presents new challenges: the significant differences in computing and communication capabilities among IoT devices and the limited resources make efficient resource allocation crucial. This paper proposes a multi-agent enhanced deep deterministic policy gradient method (MAEDDPG) based on deep reinforcement learning to obtain the optimal resource allocation strategy. Firstly, MAEDDPG introduces long short-term memory networks to address the local observation problem in multi-agent settings. Secondly, noise networks are employed during training to enhance exploration, preventing the model from getting stuck in local optima. Finally, an enhanced double critic network is designed to reduce the error in value function estimation. MAEDDPG effectively obtains the optimal resource allocation strategy, coordinating the computing and communication resources of various IoT devices, thereby balancing FL training time and IoT device energy consumption. The experimental results show that the proposed MAEDDPG method outperforms the state-of-the-art method in IoT, reducing the average system cost by 12.4%.},
  archive      = {J_COMCOM},
  author       = {Yue Sun and Hui Xia and Chuxiao Su and Rui Zhang and Jieru Wang and Kunkun Jia},
  doi          = {10.1016/j.comcom.2025.108066},
  journal      = {Computer Communications},
  month        = {3},
  pages        = {108066},
  shortjournal = {Comput. Commun.},
  title        = {A multi-agent enhanced DDPG method for federated learning resource allocation in IoT},
  volume       = {233},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). DPS-IIoT: Non-interactive zero-knowledge proof-inspired access control towards information-centric industrial internet of things. <em>COMCOM</em>, <em>233</em>, 108065. (<a href='https://doi.org/10.1016/j.comcom.2025.108065'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The advancements in 5G/6G communication technologies have enabled the rapid development and expanded application of the Industrial Internet of Things (IIoT). However, the limitations of traditional host-centric networks are becoming increasingly evident, especially in meeting the growing demands of the IIoT for higher data speeds, enhanced privacy protections, and improved resilience to disruptions. In this work, we present the ZK-CP-ABE algorithm, a novel security framework designed to enhance security and efficiency in distributing content within the IIoT. By integrating a non-interactive zero-knowledge proof (ZKP) protocol for user authentication and data validation into the existing Ciphertext-Policy Attribute-Based Encryption (CP-ABE), the ZK-CP-ABE algorithm substantially improves privacy protections while efficiently managing bandwidth usage. Furthermore, we propose the Distributed Publish-Subscribe Industrial Internet of Things (DPS-IIoT) system, which uses Hyperledger Fabric blockchain technology to deploy access policies and ensure the integrity of ZKP from tampering and cyber-attacks, thus enhancing the security and reliability of IIoT networks. To validate the effectiveness of our approach, extensive experiments were conducted, demonstrating that the proposed ZK-CP-ABE algorithm significantly reduces bandwidth consumption, while maintaining robust security against unauthorized access. Experimental evaluation shows that the ZK-CP-ABE algorithm and DPS-IIoT system significantly enhance bandwidth efficiency and overall throughput in IIoT environments.},
  archive      = {J_COMCOM},
  author       = {Dun Li and Noel Crespi and Roberto Minerva and Wei Liang and Kuan-Ching Li and Joanna Kołodziej},
  doi          = {10.1016/j.comcom.2025.108065},
  journal      = {Computer Communications},
  month        = {3},
  pages        = {108065},
  shortjournal = {Comput. Commun.},
  title        = {DPS-IIoT: Non-interactive zero-knowledge proof-inspired access control towards information-centric industrial internet of things},
  volume       = {233},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). AI-based malware detection in IoT networks within smart cities: A survey. <em>COMCOM</em>, <em>233</em>, 108055. (<a href='https://doi.org/10.1016/j.comcom.2025.108055'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The exponential expansion of Internet of Things (IoT) applications in smart cities has significantly pushed smart city development forward. Intelligent applications have the potential to enhance systems' efficiency, service quality, and overall performance. Smart cities, intelligent transportation networks, and other influential infrastructure are the main targets of cyberattacks. These attacks have the potential to undercut the security of important government, commercial, and personal information, placing privacy and confidentiality at risk. Multiple scientific studies indicate that Smart City cyberattacks can result in millions of euros in financial losses due to data compromise and loss. The importance of anomaly detection rests in its ability to identify and analyze illegitimacy within IoT data. Unprotected, infected, or suspicious devices may be unsafe for intrusion attacks, which have the potential to enter several machines within a network. This interferes with the network's provision of customer service in terms of privacy and safety. The objective of this study is to assess procedures for detecting malware in the IoT using artificial intelligence (AI) approaches. To identify and prevent threats and malicious programs, current methodologies use AI algorithms such as support vector machines, decision trees, and deep neural networks. We explore existing studies that propose several methods to address malware in IoT using AI approaches. Finally, the survey highlights current issues in this context, including the accuracy of detection and the cost of security concerns in terms of detection performance and energy consumption.},
  archive      = {J_COMCOM},
  author       = {Mustafa J.M. Alhamdi and Jose Manuel Lopez-Guede and Jafar AlQaryouti and Javad Rahebi and Ekaitz Zulueta and Unai Fernandez-Gamiz},
  doi          = {10.1016/j.comcom.2025.108055},
  journal      = {Computer Communications},
  month        = {3},
  pages        = {108055},
  shortjournal = {Comput. Commun.},
  title        = {AI-based malware detection in IoT networks within smart cities: A survey},
  volume       = {233},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). COIChain: Blockchain scheme for privacy data authentication in cross-organizational identification. <em>COMCOM</em>, <em>233</em>, 108054. (<a href='https://doi.org/10.1016/j.comcom.2025.108054'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In cross-institutional user authentication, users’ personal privacy information is often exposed to the risk of disclosure and abuse. Users should have the right to decide on their own data, and others should not be able to use user data without users’ permission. In this study, we adopted a user-centered framework, so that users can obtain authorization among different resource owners through qualification proof, avoiding the dissemination of users’ personal privacy data. We have developed a blockchain-based cross-institutional authorization architecture where users can obtain identity authentication between different entities by structuring transactions. Through the selective disclosure algorithm, the user’s private information is hidden during the user identity authentication, and the authenticity of the user’s private information is verified by disclosing the user’s non-private information and authentication credentials. The architecture supports the generation of identity credentials of constant size based on atomic properties. We prototype the system on Ethereum. The prototype of the system is tested. The experiment proves that the sum of user information processing and verification time is about 80ms, and the time fluctuation of user information processing is very small. The results show that our data flow scheme can effectively avoid the privacy leakage problem in the user cross-agency authentication scenario with a small cost.},
  archive      = {J_COMCOM},
  author       = {Zhexuan Yang and Xiao Qu and Zeng Chen and Guozi Sun},
  doi          = {10.1016/j.comcom.2025.108054},
  journal      = {Computer Communications},
  month        = {3},
  pages        = {108054},
  shortjournal = {Comput. Commun.},
  title        = {COIChain: Blockchain scheme for privacy data authentication in cross-organizational identification},
  volume       = {233},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Just a little human intelligence feedback! unsupervised learning assisted supervised learning data poisoning based backdoor removal. <em>COMCOM</em>, <em>233</em>, 108052. (<a href='https://doi.org/10.1016/j.comcom.2025.108052'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Backdoor attacks on deep learning (DL) models are recognized as one of the most alarming security threats, particularly in security-critical applications. A primary source of backdoor introduction is data outsourcing such as when data is aggregated from third parties or end Internet of Things (IoT) devices, which are susceptible to various attacks. Significant efforts have been made to counteract backdoor attacks through defensive measures. However, the majority of them are ineffective to either evolving trigger types or backdoor types. This study proposes a poisoned data detection method, termed as LABOR (unsupervised L earning A ssisted supervised learning data poisoning based B ackd O r R emoval), by incorporating a little human intelligence feedback. LABOR is specifically devised to counter backdoor induced by dirty-label data poisoning on the most common classification tasks. The key insight is that regardless of the underlying trigger types (e.g., patch or imperceptible triggers) and intended backdoor types (e.g., universal or partial backdoor), the poisoned samples still preserve the semantic features of their original classes. By clustering these poisoned samples based on their original categories through unsupervised learning, with category identification assisted by human intelligence, LABOR can detect and remove poisoned samples by identifying discrepancies between cluster categories and classification model predictions. Extensive experiments on eight benchmark datasets, including an intrusion detection dataset relevant to IoT device protection, validate LABOR ’s effectiveness in combating dirty-label poisoning-based backdoor attacks. LABOR ’s robustness is further demonstrated across various trigger and backdoor types, as well as diverse data modalities, including image, audio and text.},
  archive      = {J_COMCOM},
  author       = {Ting Luo and Huaibing Peng and Anmin Fu and Wei Yang and Lihui Pang and Said F. Al-Sarawi and Derek Abbott and Yansong Gao},
  doi          = {10.1016/j.comcom.2025.108052},
  journal      = {Computer Communications},
  month        = {3},
  pages        = {108052},
  shortjournal = {Comput. Commun.},
  title        = {Just a little human intelligence feedback! unsupervised learning assisted supervised learning data poisoning based backdoor removal},
  volume       = {233},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A two-stage federated learning method for personalization via selective collaboration. <em>COMCOM</em>, <em>232</em>, 108053. (<a href='https://doi.org/10.1016/j.comcom.2025.108053'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As an emerging distributed learning method, Federated learning has received much attention recently. Traditional federated learning aims to train a global model on a decentralized dataset, but in the case of uneven data distribution, a single global model may not be well adapted to each client, and even the local training performance of some clients may be superior to the global model. Under this background, clustering resemblance clients into the same group is a common approach. However, there is still some heterogeneity of clients within the same group, and general clustering methods usually assume that clients belong to a specific class only, but in real-world scenarios, it is difficult to accurately categorize clients into one class due to the complexity of data distribution. To solve these problems, we propose a two-stage fed erated learning method for personalization via s elective c ollaboration (FedSC). Different from previous clustering methods, we focus on how to independently exclude other clients with significant distributional differences for each client and break the restriction that clients can only belong to one category. We tend to select collaborators for each client who are more conducive to achieving local mission goals and build a collaborative group for them independently, and every client engages in a federated learning process only with group members to avoid negative knowledge transfer. Furthermore, FedSC performs finer-grained processing within each group, using an adaptive hierarchical fusion strategy of group and local models instead of the traditional approach’s scheme of directly overriding local models. Extensive experiments show that our proposed method considerably increases model performance under different heterogeneity scenarios.},
  archive      = {J_COMCOM},
  author       = {Jiuyun Xu and Liang Zhou and Yingzhi Zhao and Xiaowen Li and Kongshang Zhu and Xiangrui Xu and Qiang Duan and RuRu Zhang},
  doi          = {10.1016/j.comcom.2025.108053},
  journal      = {Computer Communications},
  month        = {2},
  pages        = {108053},
  shortjournal = {Comput. Commun.},
  title        = {A two-stage federated learning method for personalization via selective collaboration},
  volume       = {232},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Reinforcement learning based offloading and resource allocation for multi-intelligent vehicles in green edge-cloud computing. <em>COMCOM</em>, <em>232</em>, 108051. (<a href='https://doi.org/10.1016/j.comcom.2025.108051'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Green edge-cloud computing (GECC) collaborative service architecture has become one of the mainstream frameworks for real-time intensive multi-intelligent vehicle applications in intelligent transportation systems (ITS). In GECC systems, effective task offloading and resource allocation are critical to system performance and efficiency. Existing works on task offloading and resource allocation for multi-intelligent vehicles in GECC systems focus on designing static methods, which offload tasks once or a fixed number of times. This offloading manner may lead to low resource utilization due to congestion on edge servers and is not suitable for ITS with dynamically changing parameters such as bandwidth. To solve the above problems, we present a dynamic task offloading and resource allocation method, which allows tasks to be offloaded an arbitrary number of times under time and resource constraints. Specifically, we consider the characteristics of tasks and propose a remaining model to obtain the states of vehicles and tasks in real-time. Then we present a task offloading and resource allocation method considering both time and energy according to a designed real-time multi-agent deep deterministic policy gradient (RT-MADDPG) model. Our approach can offload tasks in arbitrary number of times under resource and time constraints, and can dynamically adjust the task offloading and resource allocation solutions according to changing system states to maximize system utility, which considers both task processing time and energy. Extensive simulation results indicate that the proposed RT-MADDPG method can effectively improve the utility of ITS compared to 2 benchmarking methods.},
  archive      = {J_COMCOM},
  author       = {Liying Li and Yifei Gao and Peiwen Xia and Sijie Lin and Peijin Cong and Junlong Zhou},
  doi          = {10.1016/j.comcom.2025.108051},
  journal      = {Computer Communications},
  month        = {2},
  pages        = {108051},
  shortjournal = {Comput. Commun.},
  title        = {Reinforcement learning based offloading and resource allocation for multi-intelligent vehicles in green edge-cloud computing},
  volume       = {232},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Graph convolutional networks and deep reinforcement learning for intelligent edge routing in IoT environment. <em>COMCOM</em>, <em>232</em>, 108050. (<a href='https://doi.org/10.1016/j.comcom.2025.108050'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The rapid growth of the Internet of Things (IoT) has increased the demand for Quality of Service (QoS) in various applications. Intelligent routing algorithms have emerged to meet these high QoS requirements. However, existing algorithms face challenges such as long training time, limited generalization capabilities, and difficulties in handling high-dimensional continuous action spaces, which hinder their ability to achieve optimal routing solutions. To address these challenges, this paper proposes a novel intelligent edge routing optimization (RO) algorithm that integrates node classification (NC) using a graph convolutional network (GCN) with path selection (PS) based on deep reinforcement learning (DRL). This approach aims to intelligently select optimal paths while meeting high QoS requirements in complex, dynamically changing IoT Edge Network Environments (IENEs). The NC module reduces the computational complexity and enhances the generalization capability of the RO algorithm by transforming network topology and link state information into node features, effectively filtering out low-performing nodes. To cope with high-dimensional continuous action spaces and meet QoS requirements, the PS module utilizes the refined network topology and state information from NC to determine optimal routing paths. Simulation results show that the proposed algorithm outperforms state-of-the-art methods in key performance metrics such as average network delay, packet loss rate, and throughput. In addition, it shows significant improvements in convergence speed and generalization ability.},
  archive      = {J_COMCOM},
  author       = {Zhi Wang and Bo Yi and Saru Kumari and Chien Ming Chen and Mohammed J.F. Alenazi},
  doi          = {10.1016/j.comcom.2025.108050},
  journal      = {Computer Communications},
  month        = {2},
  pages        = {108050},
  shortjournal = {Comput. Commun.},
  title        = {Graph convolutional networks and deep reinforcement learning for intelligent edge routing in IoT environment},
  volume       = {232},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Resource allocation and UAV deployment for a UAV-assisted URLLC system. <em>COMCOM</em>, <em>232</em>, 108049. (<a href='https://doi.org/10.1016/j.comcom.2025.108049'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The unmanned aerial vehicle (UAV)-assisted transmission in ultra-reliable low-latency communication (URLLC) can achieve precise control in environments where communication infrastructures are unavailable, with enormous benefits in military and commercial applications. This paper investigates a three-hop decode-and-forward UAV-assisted system to guarantee the stringent quality-and-service requirements in long-distance URLLC. First, the block error rate (BLER) is derived for air-to-ground and air-to-air channels. Then, the transmit power, blocklength, and UAV deployment in three-dimensional space are optimized together to jointly minimize the overall BLER and UAV communication energy consumption. The formulated non-convex problem is divided into subproblems and an iterative algorithm is proposed to tackle it by utilizing the block coordinate descent. Different search techniques and the block successive convex approximation approach are used to conquer the subproblems. Finally, simulations are conducted to demonstrate the system performance and the effectiveness of the proposed algorithm.},
  archive      = {J_COMCOM},
  author       = {Xinyue Gu and Hong Jiang and Hao Yang},
  doi          = {10.1016/j.comcom.2025.108049},
  journal      = {Computer Communications},
  month        = {2},
  pages        = {108049},
  shortjournal = {Comput. Commun.},
  title        = {Resource allocation and UAV deployment for a UAV-assisted URLLC system},
  volume       = {232},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). DBVA: Double-layered blockchain architecture for enhanced security in VANET vehicular authentication. <em>COMCOM</em>, <em>232</em>, 108048. (<a href='https://doi.org/10.1016/j.comcom.2025.108048'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Vehicular ad-hoc networks (VANET) are crucial for improving road safety and traffic management in Intelligent Transportation Systems (ITS). However, these networks face significant security and privacy challenges due to their dynamic and decentralized nature. Traditional authentication methods, such as Public Key Infrastructure (PKI) and centralized systems, struggle with scalability, single points of failure, and privacy issues. To address these issues, this paper introduces DBVA, a Double-Layered Blockchain Architecture that integrates private and consortium blockchains to create a robust and scalable authentication framework for VANET. The DBVA framework segregates public transactions, such as traffic data, from private transactions, such as identity and location information, into separate blockchain layers, preserving privacy and enhancing security. Additionally, DBVA introduces strict access control smart contracts for the decentralized revocation of unauthorized vehicle privileges, minimizing communication risks and enhancing system resilience. A dynamic pseudonym identity generation mechanism with periodic updates further strengthens privacy by segregating real and pseudonymous identities into separate blockchain layers. Comprehensive performance evaluations reveal that DBVA significantly enhances computational efficiency, reducing the computational cost to 18.34 ms, lowering communication overhead to 992 bits per message, and minimizing storage requirements to just 50 units, making it competitive among contemporary schemes. Extensive security analysis and formal proof confirm that DBVA effectively meets all essential privacy and security requirements, making it a robust, reliable, and scalable solution for enhancing the security, privacy, and resilience of VANET.},
  archive      = {J_COMCOM},
  author       = {Samuel Akwasi Frimpong and Mu Han and Usman Ahmad and Otu Larbi-Siaw and Joseph Kwame Adjei},
  doi          = {10.1016/j.comcom.2025.108048},
  journal      = {Computer Communications},
  month        = {2},
  pages        = {108048},
  shortjournal = {Comput. Commun.},
  title        = {DBVA: Double-layered blockchain architecture for enhanced security in VANET vehicular authentication},
  volume       = {232},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). 5G for connected and automated mobility - Network level evaluation on real neighboring 5G networks - The greece: Turkey cross border corridor. <em>COMCOM</em>, <em>232</em>, 108047. (<a href='https://doi.org/10.1016/j.comcom.2025.108047'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The automotive industry has been one of the vertical sectors eagerly waiting for the extended availability of 5G connectivity in order to deliver Connected and Automated Mobility (CAM) services. These services require extremely fast and reliable, uninterrupted communication to guarantee the safety of the drivers and other road users. Even though extended analysis and evaluation of the expected performance of 5G for CAM services has taken place in the past years via simulation studies and local trials based on 5G experimental testbeds, performance evaluation based on real 5G networks has been extremely limited, due to their unavailability until recently. Even more so in rural/highway conditions, as the 5G deployments so far have been focused on urban environments with greater population coverage. This article is among the first to present evaluation data and the corresponding analysis of the 5G Non-Stand Alone (NSA) network performance for CAM services based on neighboring 5G (overlay) networks in the cross-border corridor between Greece and Turkey, by one of the leading global 5G vendors and two of the top national operators. The performance evaluation focuses on the effect of inter-PLMN (Public Land Mobile Network) Handovers on the throughput, latency and interruption time experienced by a mobile user, and the network metrics achievable under various network configurations.},
  archive      = {J_COMCOM},
  author       = {Konstantinos Trichias and Serhat Col and Ioannis Masmanidis and Afrim Berisha and Foteini Setaki and Panagiotis Demestichas and Symeon Papavassiliou and Nikolaos Mitrou},
  doi          = {10.1016/j.comcom.2025.108047},
  journal      = {Computer Communications},
  month        = {2},
  pages        = {108047},
  shortjournal = {Comput. Commun.},
  title        = {5G for connected and automated mobility - Network level evaluation on real neighboring 5G networks - The greece: Turkey cross border corridor},
  volume       = {232},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Next-generation cloudlet federation for internet of things in healthcare: Enhancing response time and energy efficiency. <em>COMCOM</em>, <em>232</em>, 108046. (<a href='https://doi.org/10.1016/j.comcom.2025.108046'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The integration of the Internet of Things (IoT) with edge computing has revolutionized various sectors, including healthcare, by enabling timely patient monitoring and service availability. These services encompass emergency responses, regular treatment, and physician assistance. Healthcare systems leverage mobile edge computing, fog computing, and cloudlet computing to offload data and computations and minimize delays and energy consumption. However, edge computing solutions face limitations such as restricted area coverage, diverse service requirements, and insufficient computational power, making them unsuitable for large-scale data processing and computation. To address the growing need for seamless patient information sharing among doctors and specialists across different hospitals, a resource-sharing solution, such as the cloudlet federation, is essential. Nonetheless, integrating IoT systems with cloudlet federations in healthcare presents challenges, primarily because of the necessity for mobile nodes such as ambulances, in addition to the fixed-edge nodes and brokers typical of traditional cloudlet federations. This study proposes a modified cloudlet federation model designed to overcome these challenges by effectively managing data from heterogeneous nodes. Our approach enhanced response times, reduced energy consumption, and ensured the availability of comprehensive patient information. The experimental results demonstrate a 49% improvement in response time and a 51% reduction in energy consumption compared with existing cloud-based solutions.},
  archive      = {J_COMCOM},
  author       = {Rahima Tanveer and Muhammad Ziad Nayyer and Muhammad Hasan Jamal and Imran Raza and Claude Fachkha},
  doi          = {10.1016/j.comcom.2025.108046},
  journal      = {Computer Communications},
  month        = {2},
  pages        = {108046},
  shortjournal = {Comput. Commun.},
  title        = {Next-generation cloudlet federation for internet of things in healthcare: Enhancing response time and energy efficiency},
  volume       = {232},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). DCDIM: Diversified influence maximization on dynamic social networks. <em>COMCOM</em>, <em>232</em>, 108045. (<a href='https://doi.org/10.1016/j.comcom.2025.108045'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The problem of influence maximization (IM) identifies the top k nodes that can maximize the expected influence in social networks. IM has many applications, such as viral marketing, business strategy, and profit maximization. However, most of the existing studies focus on maximizing the influenced node in the static social network, and overlook the diversity of the influenced nodes. To address this issue, this work proposes a framework to diversify the influenced node in dynamic social networks. Utilizing the framework, our DCDIM algorithm identifies the communities dynamically and maximizes the communities of influential nodes using a proposed objective function. We prove that the proposed objective function is Monotonic, Submodular, and NP-Hard. The experiments have been conducted on four datasets, and the experimental results show that the proposed approach achieves the maximum number of communities and gives competitive influenced node with the benchmark algorithms.},
  archive      = {J_COMCOM},
  author       = {Sunil Kumar Meena and Shashank Sheshar Singh and Kuldeep Singh},
  doi          = {10.1016/j.comcom.2025.108045},
  journal      = {Computer Communications},
  month        = {2},
  pages        = {108045},
  shortjournal = {Comput. Commun.},
  title        = {DCDIM: Diversified influence maximization on dynamic social networks},
  volume       = {232},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). GNNetSlice: A GNN-based performance model to support network slicing in B5G networks. <em>COMCOM</em>, <em>232</em>, 108044. (<a href='https://doi.org/10.1016/j.comcom.2025.108044'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Network slicing is gaining traction in Fifth Generation (5G) deployments and Beyond 5G (B5G) designs. In a nutshell, network slicing virtualizes a single physical network into multiple virtual networks or slices, so that each slice provides a desired network performance to the set of traffic flows (source–destination pairs) mapped to it. The network performance, defined by specific Quality of Service (QoS) parameters (latency, jitter and losses), is tailored to different use cases, such as manufacturing, automotive or smart cities. A network controller determines whether a new slice request can be safely granted without degrading the performance of existing slices, and therefore fast and accurate models are needed to efficiently allocate network resources to slices. Although there is a large body of work of network slicing modeling and resource allocation in the Radio Access Network (RAN), there are few works that deal with the implementation and modeling of network slicing in the core and transport network. In this paper, we present GNNetSlice, a model that predicts the performance of a given configuration of network slices and traffic requirements in the core and transport network. The model is built leveraging Graph Neural Networks (GNNs), a kind of Neural Network specifically designed to deal with data structured as graphs. We have chosen a data-driven approach instead of classical modeling techniques, such as Queuing Theory or packet-level simulations due to their balance between prediction speed and accuracy. We detail the structure of GNNetSlice, the dataset used for training, and show how our model can accurately predict the delay, jitter and losses of a wide range of scenarios, achieving a Symmetric Mean Average Percentage Error (SMAPE) of 5.22%, 1.95% and 2.04%, respectively.},
  archive      = {J_COMCOM},
  author       = {Miquel Farreras and Jordi Paillissé and Lluís Fàbrega and Pere Vilà},
  doi          = {10.1016/j.comcom.2025.108044},
  journal      = {Computer Communications},
  month        = {2},
  pages        = {108044},
  shortjournal = {Comput. Commun.},
  title        = {GNNetSlice: A GNN-based performance model to support network slicing in B5G networks},
  volume       = {232},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A contextual aware enhanced LoRaWAN adaptive data rate for mobile IoT applications. <em>COMCOM</em>, <em>232</em>, 108042. (<a href='https://doi.org/10.1016/j.comcom.2024.108042'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Long range wide area network (LoRaWAN) utilize Adaptive Data Rate (ADR) for static Internet of Things (IoT) applications such as smart parking in smart city. Blind ADR (BADR) has been introduced for end devices to manage the resources of mobile applications such as assets tracking. However, the predetermined mechanism of allocating the spreading factors (SFs) to mobile end devices is not adequate in terms of energy depletion. Recently, AI-based solutions to resource allocation have been introduced in the existing literature. However, implementing complex models directly on low-power devices is not ideal in terms of energy and processing power. Therefore, considering these challenges, in this paper, we present a novel Contextual Aware Enhanced LoRaWAN Adaptive Data Rate (CA-ADR) for mobile IoT Applications. The proposed CA-ADR comprises two modes offline and online. In offline mode, we compile a dataset based on successful acknowledgments received by the end devices. Later, dataset is modified by implementing contextual rule-based learning (CRL), following which we train a hybrid CNN-LSTM model. In the online mode, we utilize pre-trained model for efficient resource allocation (e.g., SF) to static and mobile end devices. The proposed CA-ADR has been implemented using TinyML, recommended for low-power and computational devices, which has shown improved results in terms of packet success ratio and energy consumption.},
  archive      = {J_COMCOM},
  author       = {Muhammad Ali Lodhi and Lei Wang and Arshad Farhad and Khalid Ibrahim Qureshi and Jenhu Chen and Khalid Mahmood and Ashok Kumar Das},
  doi          = {10.1016/j.comcom.2024.108042},
  journal      = {Computer Communications},
  month        = {2},
  pages        = {108042},
  shortjournal = {Comput. Commun.},
  title        = {A contextual aware enhanced LoRaWAN adaptive data rate for mobile IoT applications},
  volume       = {232},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). UAVs deployment optimization in cell-free aerial communication networks. <em>COMCOM</em>, <em>232</em>, 108041. (<a href='https://doi.org/10.1016/j.comcom.2024.108041'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper tackles the joint problem of user association, channel assignment, UAV placement, and transmit power allocation in cell-free wireless networks. Each user can either be directly connected to a ground base station (GBS) or through UAVs acting as relays. To address this, we formulate the problem mathematically as a mixed-integer non-convex program, to minimize the number of deployed UAVs under data rate requirements and coverage constraints. Since the problem is NP -hard, we propose a model-free algorithm utilizing the deep deterministic policy gradient method to handle UAV deployment and positioning within a continuous space domain. We also propose efficient heuristic and meta-heuristic algorithms for comparison purposes. Simulation results demonstrate the benefits of the cell-free concept in satisfying users and improving the performance of UAV-assisted wireless networks. They also validate the effectiveness of the proposed algorithms in minimizing the required number of deployed UAVs to meet stringent user requirements.},
  archive      = {J_COMCOM},
  author       = {Aya Ahmed and Cirine Chaieb and Wessam Ajib and Halima Elbiaze and Roch Glitho},
  doi          = {10.1016/j.comcom.2024.108041},
  journal      = {Computer Communications},
  month        = {2},
  pages        = {108041},
  shortjournal = {Comput. Commun.},
  title        = {UAVs deployment optimization in cell-free aerial communication networks},
  volume       = {232},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Blockchain-empowered multi-skilled crowdsourcing for mobile web 3.0. <em>COMCOM</em>, <em>232</em>, 108037. (<a href='https://doi.org/10.1016/j.comcom.2024.108037'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As the next generation of the world wide web, web 3.0 is envisioned as a decentralized internet which improves data security and self-sovereign identity. The mobile web 3.0 mainly focuses on decentralized internet for mobile users and mobile applications. With the rapid development of mobile crowdsourcing research, existing mobile crowdsourcing models can achieve efficient allocation of tasks and responders. Benefiting from the inherent decentralization and immutability, more and more crowdsourcing models over mobile web 3.0 have been deployed on blockchain systems to enhance data verifiability. However, executing these crowdsourcing-oriented smart contracts on a blockchain may incur a large amount of gas consumption, leading to significant costs for the system and increasing users’ expenses. In addition, the existing crowdsourcing model does not take into account the expected quality of task completion in the matching link between tasks and responders, which will cause some tasks to fail to achieve effects and damage the interests of task publishers. In order to solve these problems, this paper proposes a decentralized multi-skill mobile crowdsourcing model with guaranteed task quality and gas optimization (DMCQG), which performs task matching while considering skill coverage and expected quality of task completion, and guarantees the final completion quality of each task. In addition, DMCQG also optimizes the gas value consumed by smart contracts at the code level, reducing the cost of crowdsourcing task participation. In order to verify whether DMCQG is effective, we deployed the model on the Ethereum platform for testing. Through inspection, it was proved that the final expected quality of the tasks matched by DMCQG was better than other models. And it is verified that after optimization, the gas consumption of DMCQG is significantly reduced.},
  archive      = {J_COMCOM},
  author       = {Yu Li and Yueheng Lu and Xinyu Yang and Wenjian Xu and Zhe Peng},
  doi          = {10.1016/j.comcom.2024.108037},
  journal      = {Computer Communications},
  month        = {2},
  pages        = {108037},
  shortjournal = {Comput. Commun.},
  title        = {Blockchain-empowered multi-skilled crowdsourcing for mobile web 3.0},
  volume       = {232},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Distributed cooperative task allocation for heterogeneous UAV swarms under complex constraints. <em>COMCOM</em>, <em>231</em>, 108043. (<a href='https://doi.org/10.1016/j.comcom.2024.108043'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper investigates the dynamic task allocation problem for a heterogeneous UAV swarm conducting reconnaissance and strike (RAS) tasks while considering constraints on critical task time, communication range, and task resource requirements. The main challenge is to reconnaissance and strike all unknown targets within the mission area, which involves managing the UAV's changing states, task information, and variable communication with neighboring nodes. It is also important to overcome the limitations of current consensus-based heuristic task allocation approaches, which often lead to sub-optimal solutions due to being trapped in a local optimum within a distributed computing framework. To solve these problems, a novel heterogeneous UAV swarm task allocation model is developed first to maximize task benefits and minimize path planning costs. Second, we propose a two-phase consensus-based group bundling algorithm (CBGBA), which enables UAVs to reach consensus on task allocation results in a dynamic environment. In the task inclusion phase, we create feasible time slots for newly added tasks by optimizing task delay and sequence revenue, thus preventing the occurrence local optima problems under the critical task time constraint. In the consensus procedure phase, we employ a block-information-sharing (BIS) strategy to establish local networks, resolving consensus conflicts due to communication range constraints. Additionally, we propose an improved consensus principle that facilitates dynamic task allocation among distributed heterogeneous UAVs, meeting task resource requirements. Finally, the simulation results demonstrate the effectiveness and superiority of our proposed algorithm. Furthermore, CBGBA exhibits a performance enhancement of up to 14.2 % compared to the consensus-based synergy algorithm (CBSA).},
  archive      = {J_COMCOM},
  author       = {Wei Yue and Xiaoyong Zhang and Zhongchang Liu},
  doi          = {10.1016/j.comcom.2024.108043},
  journal      = {Computer Communications},
  month        = {2},
  pages        = {108043},
  shortjournal = {Comput. Commun.},
  title        = {Distributed cooperative task allocation for heterogeneous UAV swarms under complex constraints},
  volume       = {231},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Blockchain-based VANET edge computing-assisted cross-vehicle enterprise authentication scheme. <em>COMCOM</em>, <em>231</em>, 108040. (<a href='https://doi.org/10.1016/j.comcom.2024.108040'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Vehicular Ad hoc Network (VANET) is considered one of the feasible solutions to improve the efficiency and safety of modern transportation systems, and it provides new opportunities for creating a safe and efficient traffic environment. In recent years, this technology has attracted extensive attention from the academic community. However, VANET is an open network with frequent information interaction, and users are vulnerable to security and privacy threats. The existing schemes mainly consider the identity authentication of vehicles in vehicle enterprises (VEs). Due to concerns about the leakage of core data, VEs lack the motivation to establish a cross-vehicle enterprise identity authentication framework. Based on the above analysis, we propose a cross-vehicle enterprise authentication architecture by designing a two-stage certificate generation mechanism where certificate authority (CA) and VEs cooperate to generate identity credentials for vehicles. To address the concerns of VEs, we establish distributed trust and enable information sharing across VEs by introducing a consortium blockchain composed of car companies, CA, and pseudonym certificate authority (PCA). Considering the need for vehicles to access road traffic information, we use a public blockchain to store public information, and the practical byzantine fault tolerant (PBFT) algorithm is used to reach consensus. Instead of using computationally complex bilinear pairing and mapping-to-point hashing operations, the proposed scheme uses an elliptic curve cryptosystem (ECC), considering the limited hardware resources of the vehicle and RSU. In addition, our scheme integrates edge computing to solve complex computing tasks that cannot be performed locally and further reduces system latency. Security analysis and performance analysis show that our scheme has better performance than existing schemes in terms of security, computational overhead, and communication overhead.},
  archive      = {J_COMCOM},
  author       = {Jiaming Lai and Xiaohong Zhang and Shuling Liu and Shaojiang Zhong and Ata Jahangir Moshayedi},
  doi          = {10.1016/j.comcom.2024.108040},
  journal      = {Computer Communications},
  month        = {2},
  pages        = {108040},
  shortjournal = {Comput. Commun.},
  title        = {Blockchain-based VANET edge computing-assisted cross-vehicle enterprise authentication scheme},
  volume       = {231},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Attribute-based policies through microservices in a smart home scenario. <em>COMCOM</em>, <em>231</em>, 108039. (<a href='https://doi.org/10.1016/j.comcom.2024.108039'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Application containerization allows for efficient resource utilization and improved performance when compared to traditional virtualization techniques. However, managing multiple containers and providing services such as load balancing, fault tolerance and security represent challenging tasks in the emerging microservices architectures. In this context, Kubernetes platform allows to build resilient distributed containers. Besides its efficiency in terms of configuration and architectural resiliency, it must also guarantee the access control to the managed resources. In fact, information must be protected throughout the different microservices which compose an application. To cope with such an issue, this paper proposes the definition of attribute-based policies able to regulate data disclosure within a Kubernetes-based microservices network. Simulations are carried out in a local Minikube environment, considering a smart residence scenario. The investigated metrics include response time, required memory, CPU load, and disk usage.},
  archive      = {J_COMCOM},
  author       = {Alessandra Rizzardi and Sabrina Sicari and Alberto Coen-Porisini},
  doi          = {10.1016/j.comcom.2024.108039},
  journal      = {Computer Communications},
  month        = {2},
  pages        = {108039},
  shortjournal = {Comput. Commun.},
  title        = {Attribute-based policies through microservices in a smart home scenario},
  volume       = {231},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Deadline-constrained routing based on power-law and exponentially distributed contacts in DTNs. <em>COMCOM</em>, <em>231</em>, 108038. (<a href='https://doi.org/10.1016/j.comcom.2024.108038'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {During a large-scale disaster, there is a severe destruction to physical infrastructures such as telecommunication and power lines, which result in the disruption of communication, making timely emergency response challenging. Since Delay Tolerant Networks (DTNs) are infrastructure-less, they tolerate physical destruction and thus can serve as an emergency response network during a disaster scenario. To be effective, DTNs need a routing protocol that maximizes the number of messages delivered within deadline. One obvious approach is to broadcast messages everywhere. However, this approach is impractical as DTNs are resource-constrained. In this work, we propose a cost-effective routing protocol based on the expected delivery delay that optimizes the number of messages delivered within deadline with a significantly low network overhead. Simulations using real-life mobility traces show that with our scheme, up to 95% of messages are delivered within deadline, while requiring on average less than three message copies.},
  archive      = {J_COMCOM},
  author       = {Tuan Le},
  doi          = {10.1016/j.comcom.2024.108038},
  journal      = {Computer Communications},
  month        = {2},
  pages        = {108038},
  shortjournal = {Comput. Commun.},
  title        = {Deadline-constrained routing based on power-law and exponentially distributed contacts in DTNs},
  volume       = {231},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Aegis: A cloud-edge computing based multi-disaster crowd evacuation model using improved deep reinforcement learning. <em>COMCOM</em>, <em>231</em>, 108036. (<a href='https://doi.org/10.1016/j.comcom.2024.108036'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Crowd evacuation is an important measure for urban disaster management, which can provide effective evacuation guidelines for victims and safeguard their lives. However, most of existing methods are designed for single-disaster scenarios, ignoring the fact that disasters often erupt in multiple locations simultaneously. Thus, a multi-disaster crowd evacuation model, Aegis, is proposed based on cloud-edge computing and improved deep reinforcement learning. Firstly, the multi-disaster crowd evacuation problem is modeled as a multi-objective optimization problem, which considers shelter load balancing and dangerous area crossing issues. Secondly, an improved deep reinforcement learning model is proposed in this paper to solve it. The model utilizes Attention mechanism, Gated Recurrent Unit (GRU) and Graph Attention Network (GAT) to obtain the embedding of raw data. Then, the model maps the embedded information to the evacuation plan by an attention-based decoder. The model parameters are optimized using a Policy Gradient method. Thirdly, a cloud-edge computing framework is also introduced for Aegis, featuring a three-tier architecture that includes cloud, edge, and terminal levels. This design allows for the seamless integration of the model into smart city management. The experimental results show that Aegis outperforms other baseline methods, especially in reducing evacuation costs and optimizing shelter loads. In experiments with four different scales, Aegis reduces the evacuation costs by 58.87 %, 64.56 %, 65.59 %, and 67.79 %.},
  archive      = {J_COMCOM},
  author       = {Jinbo Zhao and Xiaolong Xu and Fu Xiao},
  doi          = {10.1016/j.comcom.2024.108036},
  journal      = {Computer Communications},
  month        = {2},
  pages        = {108036},
  shortjournal = {Comput. Commun.},
  title        = {Aegis: A cloud-edge computing based multi-disaster crowd evacuation model using improved deep reinforcement learning},
  volume       = {231},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). An efficient sharding consensus protocol for improving blockchain scalability. <em>COMCOM</em>, <em>231</em>, 108032. (<a href='https://doi.org/10.1016/j.comcom.2024.108032'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A consortium blockchain facilitates establishment credit among supply and demand agents on a cloud platform. HotStuff, a Byzantine fault-tolerance consensus protocol, predominates the consortium blockchains and has undergone extensive research and practical applications. However, its scalability remains limited with an increased number of nodes, making it unsuitable for large-scale transactions. Consequently, an improved sharding consensus protocol (IShard) is proposed to consider decentralization, security, and scalability within the consortium blockchain. First, IShard employs the jump consistent hash algorithm for reasonable node allocation within the network, thus reducing data migration resulting from shard modifications. Second, a credit mechanism is devised to reflect credit based on the behavior of nodes, optimizing consensus nodes to enhance performance. Third, a credit-based consensus protocol is introduced to concurrently handle transactions through sharding among multiple shards, distributing transactions to each shard to alleviate the overall burden, thus enhancing the scalability of the blockchain. Fourth, a node removal mechanism is devised to identify and eliminate Byzantine nodes, minimizing view changes and ensuring efficient system operation in an environment susceptible to Byzantine faults. Finally, IShard has demonstrated its ability to ensure security and liveness in shard transactions, subject to particular constraints regarding Byzantine nodes. In addition, transaction processes involving supply and demand agents are designed to enhance data reliability. Experimental results demonstrate that IShard surpasses current leading protocols, achieving a communication complexity of O( n ) and superior throughput and scalability.},
  archive      = {J_COMCOM},
  author       = {Li Lu and Linfu Sun and Yisheng Zou},
  doi          = {10.1016/j.comcom.2024.108032},
  journal      = {Computer Communications},
  month        = {2},
  pages        = {108032},
  shortjournal = {Comput. Commun.},
  title        = {An efficient sharding consensus protocol for improving blockchain scalability},
  volume       = {231},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Enhancing fog load balancing through lifelong transfer learning of reinforcement learning agents. <em>COMCOM</em>, <em>231</em>, 108024. (<a href='https://doi.org/10.1016/j.comcom.2024.108024'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Fog computing is a promising paradigm for processing Internet of Things (IoT) data. Load balancing (LB) optimizes Fog performance through efficient resource allocation, improving resource utilization, latency for real-time IoT applications, and users’ quality of service. In this work, we enhance the learning process of privacy-aware Reinforcement Learning (PARL), which requires significant training to minimize waiting delays by reducing the number of queued requests without explicitly observing Fog load or resource capabilities. To achieve this, we explore different Transfer Learning (TL) techniques for efficient adaptation to variations in demand, triggering a fine-tuning process when abrupt surges in generation rates are detected. This exploration highlights the advantages and disadvantages of reusing previously learned policies (knowledge) and interactions (experience) over multiple learning epochs with increased difficulties. Our results show that Full TL (using knowledge and experience) enhances the learning and generalization of the PARL agent, allowing it to consistently converge to the optimal solution with 80% less training compared to without TL. Additionally, we propose a lifelong learning framework for practical agent deployment in frequently changing environments. Introducing TL in this framework significantly reduces the computationally expensive training phase compared to training from scratch. Instead of continuous adaptation through ongoing training, balancer resources are preserved to provide faster decisions via a lightweight inference model. In case of significant system changes, the model is swiftly fine-tuned using TL. Furthermore, the framework leverages existing (expert) or simulation-trained agents to initialize newly deployed agents in the network, reducing failure probability in new environments compared to learning from scratch. To our knowledge, no existing efforts in the literature use TL to address lifelong learning for practical RL-based Fog LB. This gap highlights the need for a practical yet efficient solution that minimizes the cost of continuous adaptation to changing conditions.},
  archive      = {J_COMCOM},
  author       = {Maad Ebrahim and Abdelhakim Hafid and Mohamed Riduan Abid},
  doi          = {10.1016/j.comcom.2024.108024},
  journal      = {Computer Communications},
  month        = {2},
  pages        = {108024},
  shortjournal = {Comput. Commun.},
  title        = {Enhancing fog load balancing through lifelong transfer learning of reinforcement learning agents},
  volume       = {231},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Combining transformer with a latent variable model for radio tomography based robust device-free localization. <em>COMCOM</em>, <em>231</em>, 108022. (<a href='https://doi.org/10.1016/j.comcom.2024.108022'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Radio tomographic imaging (RTI) is a promising device-free localization (DFL) method for reconstructing the signal attenuation caused by physical objects in wireless networks. In this paper, we use the received signal strength (RSS) difference between the current and baseline measurements captured by a wireless network to achieve the RTI based DFL in a predefined monitoring area. RTI is formulated as solving a badly conditioned problem under complex noise. And the end-to-end deep learning method based on Transformers and latent variable models (LVMs) is considered to address the RTI problem. The data grouping strategy is designed to divide the RSS data into multiple spatially-correlated groups, and a Transformer-based convolutional neural network (TCNN) model is firstly developed for RTI, in which the Transformer blocks are able to help the model learn the more expressive feature for the environmental image reconstruction task. The RTI system is influenced by both sensor noise and environmental noise simultaneously. In order to improve the performance of the RTI method, a Transformer-based latent variable model (TLVM) is proposed further, where the robustness to interference can be enhanced by controlling the capacity of the latent variables. The comparative numerical experiments are conducted for RTI based DFL, and the efficacy of the proposed TCNN and TLVM based RTI methods is verified by the experimental results.},
  archive      = {J_COMCOM},
  author       = {Hongzhuang Wu and Cheng Cheng and Tao Peng and Hongzhi Zhou and Tao Chen},
  doi          = {10.1016/j.comcom.2024.108022},
  journal      = {Computer Communications},
  month        = {2},
  pages        = {108022},
  shortjournal = {Comput. Commun.},
  title        = {Combining transformer with a latent variable model for radio tomography based robust device-free localization},
  volume       = {231},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). DCP and VarDis: An ad-hoc protocol stack for dynamic swarms and formations of drones. <em>COMCOM</em>, <em>231</em>, 108021. (<a href='https://doi.org/10.1016/j.comcom.2024.108021'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, swarms or formations of drones have received increased interest both in the literature and in applications. To dynamically adapt to their operating environment, swarm members need to communicate wirelessly for control and coordination tasks. One fundamental communication pattern required for basic safety purposes, such as collision avoidance, is beaconing, where drones frequently transmit information about their position, speed, heading, and other operational data to a local neighbourhood, using a local broadcast service. In this paper, we propose and analyse a protocol stack which allows to use the recurring-beaconing primitive for additional purposes. In particular, we propose the VarDis (Variable Dissemination) protocol, which creates the abstraction of variables to which all members of a drone swarm have (read) access, and which can naturally be used for centralized control of a swarm, amongst other applications. We describe the involved protocols and provide a performance analysis of VarDis.},
  archive      = {J_COMCOM},
  author       = {Samuel Pell and Andreas Willig},
  doi          = {10.1016/j.comcom.2024.108021},
  journal      = {Computer Communications},
  month        = {2},
  pages        = {108021},
  shortjournal = {Comput. Commun.},
  title        = {DCP and VarDis: An ad-hoc protocol stack for dynamic swarms and formations of drones},
  volume       = {231},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Novel low-latency data gathering scheduling for multi-radio wireless multi-hop networks. <em>COMCOM</em>, <em>231</em>, 108020. (<a href='https://doi.org/10.1016/j.comcom.2024.108020'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Due to the complicated terrain, wireless multi-hop networks (WMNs) are often needed in large industrial environment. As the distribution of WMNs’ nodes in practical industrial environment are usually quite uneven, WMNs are prone to suffer from serious congestion during the data gathering. Employing multiple radios can help to mitigate the congestion. This is due to their flexible capability to concurrently schedule each node’s transmission and reception. Most existing works for multi-radio WMNs study the scheduling of gathering traffic flow at each node, while little work studies the scheduling of gathering a fixed amount of data at the nodes. The latter is quite typical in practice, quite complicated as the transmission load of each node in this scenario is dynamic and related to the descendant nodes’ scheduling. In this paper, we propose a novel low-latency scheduling for gathering a fixed amount of data spread in multi-radio WMNs. The proposed scheduling dynamically assigns each node’s radios for transmission, reception and their targets, according to the current amount of local data and the assignment of neighboring nodes’ radios. Extensive simulations are conducted and the results show the remarkable performance of the proposed scheduling, when compared to related scheduling methods.},
  archive      = {J_COMCOM},
  author       = {Chi Zhang and Honglie Li and Mi Yan and Peng Guo and Yiyi Zhang and Zhe Tian},
  doi          = {10.1016/j.comcom.2024.108020},
  journal      = {Computer Communications},
  month        = {2},
  pages        = {108020},
  shortjournal = {Comput. Commun.},
  title        = {Novel low-latency data gathering scheduling for multi-radio wireless multi-hop networks},
  volume       = {231},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). GDD-geo: IPv6 geolocation by graph dual decomposition. <em>COMCOM</em>, <em>231</em>, 108019. (<a href='https://doi.org/10.1016/j.comcom.2024.108019'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {IP geolocation is a technique used to infer the location of an IP address through its network measurement features. It is widely used in network security, network management, and location-based services. To improve geolocation accuracy in IPv6 networks, especially when landmarks are sparse, we propose an IPv6 geolocation method based on graph dual decomposition called GDD-Geo. GDD-Geo models an IPv6 address by its network measurement attributes, including paths, delay values, and addresses. The geolocation process involves comparing the similarity of these attributes. GDD-Geo comprises two sub-algorithms, GDD-CGeo and GDD-SGeo, which provide city-level and street-level (oriented) geolocation results, respectively. Particularly, we design two graph decomposition algorithms to transform the paths represented by router interfaces into the paths represented by subgraphs based on the characteristics of IPv6 address distribution and delay distribution. The former decomposition is the support for GDD-CGeo, while the latter decomposition is conducted on the results of the former decomposition and supports GDD-SGeo. Due to the aggregation and reconstruction effects of paths derived from graph decomposition, GDD-Geo can reduce the dependence on landmarks and thus can cope with the landmark-sparse scenarios. Experimental results of city-level geolocation show that GDD-CGeo can accurately geolocate the IPv6 targets at the city level. Street-level (oriented) geolocation results in six cities within different countries show that the median errors of GDD-SGeo are 1.66–5.27 km, and the mean errors are 2.55–5.88 km. Compared with popular algorithms SLG and MLP-Geo, GDD-SGeo performs significantly better on sparse landmark datasets, with at least a 60% decrease in errors.},
  archive      = {J_COMCOM},
  author       = {Chong Liu and Ruosi Cheng and Fuxiang Yuan and Shichang Ding and Yan Liu and Xiangyang Luo},
  doi          = {10.1016/j.comcom.2024.108019},
  journal      = {Computer Communications},
  month        = {2},
  pages        = {108019},
  shortjournal = {Comput. Commun.},
  title        = {GDD-geo: IPv6 geolocation by graph dual decomposition},
  volume       = {231},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). SMKA: Secure multi-key aggregation with verifiable search for IoMT. <em>COMCOM</em>, <em>231</em>, 108012. (<a href='https://doi.org/10.1016/j.comcom.2024.108012'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Internet of Medical Things (IoMT) aggregates numerous smart medical devices and fully employs the collected health data to enhance patients’ experiences. In IoMT, patients generate various encryption keys and receive keyword information sent by data requesters to securely share selected health data, which increases the potential risk of key leakage. Besides, the cloud server may tamper with search results. The existing schemes did not consider that patients may inadvertently disclose the keyword information of data requesters. Additionally, these schemes entail a significant cost for verifying the search results. To deal with these challenges, we innovatively propose a secure multi-key aggregation (SMKA) scheme with verifiable search for IoMT. Firstly, the SMKA scheme is built upon key-aggregate searchable encryption, utilizing an oblivious search request and blockchain technology to achieve secure key aggregation. Secondly, a dual verifiable algorithm is integrated into the scheme to provide lightweight verification for the search results. The proposed scheme can achieve access control, requester privacy, accountability, and dual verification while ensuring secure search. Furthermore, the security analysis and proof have shown the effectiveness of the proposed protocol in achieving the intended security goals. Finally, the performance analysis indicates the significant feasibility and scalability of the proposed scheme.},
  archive      = {J_COMCOM},
  author       = {Xueli Nie and Aiqing Zhang and Yong Wang and Weiqi Wang},
  doi          = {10.1016/j.comcom.2024.108012},
  journal      = {Computer Communications},
  month        = {2},
  pages        = {108012},
  shortjournal = {Comput. Commun.},
  title        = {SMKA: Secure multi-key aggregation with verifiable search for IoMT},
  volume       = {231},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A review on machine learning based user-centric multimedia streaming techniques. <em>COMCOM</em>, <em>231</em>, 108011. (<a href='https://doi.org/10.1016/j.comcom.2024.108011'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The multimedia content and streaming are a major means of information exchange in the modern era and there is an increasing demand for such services. This coupled with the advancement of future wireless networks B5G/6G and the proliferation of intelligent handheld mobile devices, has facilitated the availability of multimedia content to heterogeneous mobile users. Apart from the conventional video, the 360 ° videos have gained significant attention and are quickly emerging as the popular multimedia format for virtual reality experiences. All formats of videos (conventional and 360 ° ) undergo processing, compression, and transmission across dynamic wireless channels with restricted bandwidth to facilitate the streaming services. This causes video impairments, leading to quality degradation and poses challenges for the content providers in delivering good Quality-of-Experience (QoE) to the viewers. The QoE is a prominent subjective measure of quality, which has become a crucial component in assessing multimedia services and operations. So, there has been a growing preference for QoE-aware multimedia services over heterogeneous networks with a need to address design issues like how to evaluate and quantify end-to-end QoE. Efficient multimedia streaming techniques can improve the service quality while dealing with dynamic network and end-user challenges. A paradigm shift in user-centric multimedia services is envisioned with a focus on Machine Learning (ML) based QoE modeling and streaming strategies. This survey paper presents a comprehensive overview of the overall and continuous, time varying QoE modeling for the purpose of QoE management in multimedia services. It also examines the recent research on intelligent and adaptive multimedia streaming strategies, with a special emphasis on ML based techniques for video (conventional and 360 ° ) streaming. This paper discusses the overall and continuous QoE modeling to optimize the end-user viewing experience, efficient video streaming with a focus on user-centric strategies, associated datasets for modeling and streaming, along with existing shortcoming and open challenges.},
  archive      = {J_COMCOM},
  author       = {Monalisa Ghosh and Chetna Singhal},
  doi          = {10.1016/j.comcom.2024.108011},
  journal      = {Computer Communications},
  month        = {2},
  pages        = {108011},
  shortjournal = {Comput. Commun.},
  title        = {A review on machine learning based user-centric multimedia streaming techniques},
  volume       = {231},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Towards proactive rumor control: When a budget constraint meets impression counts. <em>COMCOM</em>, <em>230</em>, 108010. (<a href='https://doi.org/10.1016/j.comcom.2024.108010'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The proliferation of rumors in online networks poses significant public safety risks and economic repercussions. Addressing this, we investigate the understudied aspect of rumor control: the interplay between influence block effect and user impression counts under budget constraints. We introduce two problem variants, RCIC and RCICB, tailored for diverse application contexts. Our study confronts two inherent challenges: the NP-hard nature of the problems and the non-submodularity of the influence block, which precludes direct greedy approaches. We develop a novel branch-and-bound framework for RCIC, achieving a ( 1 − 1 / e − ϵ ) approximation ratio, and enhance its efficacy with a progressive upper bound estimation, refining the ratio to ( 1 − 1 / e − ϵ − ρ ). Extending these techniques to RCICB, we attain approximation ratios of ( 1 2 ( 1 − 1 / e ) − ϵ ) and ( 1 2 ( 1 − 1 / e − ρ ) − ϵ ). We conduct experiments on real-world datasets to verify the efficiency, effectiveness, and scalability of our methods.},
  archive      = {J_COMCOM},
  author       = {Pengfei Xu and Zhiyong Peng and Liwei Wang},
  doi          = {10.1016/j.comcom.2024.108010},
  journal      = {Computer Communications},
  month        = {1},
  pages        = {108010},
  shortjournal = {Comput. Commun.},
  title        = {Towards proactive rumor control: When a budget constraint meets impression counts},
  volume       = {230},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Trustless privacy-preserving data aggregation on ethereum with hypercube network topology. <em>COMCOM</em>, <em>230</em>, 108009. (<a href='https://doi.org/10.1016/j.comcom.2024.108009'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The privacy-preserving data aggregation is a critical problem for many applications where multiple parties need to collaborate with each other privately to arrive at certain results. Blockchain, as a database shared across the network, provides an underlying platform on which such aggregations can be carried out with a decentralized manner. Therefore, in this paper, we have proposed a scalable privacy-preserving data aggregation protocol for summation on the Ethereum blockchain by integrating several cryptographic primitives including commitment scheme, asymmetric encryption and zero-knowledge proof along with the hypercube network topology. The protocol consists of four stages as contract deployment , user registration , private submission and proof verification . The analysis of the protocol is made with respect to two main perspectives as security and scalability including computational, communicational and storage overheads. In the paper, the zero-knowledge proof, smart contract and web user interface models for the protocol are provided. We have performed an experimental study in order to identify the required gas costs per individual and per system. The general formulation is provided to characterize the changes in gas costs for the increasing number of users. The zero-knowledge proof generation and verification times are also measured.},
  archive      = {J_COMCOM},
  author       = {Goshgar C. Ismayilov and Can Özturan},
  doi          = {10.1016/j.comcom.2024.108009},
  journal      = {Computer Communications},
  month        = {1},
  pages        = {108009},
  shortjournal = {Comput. Commun.},
  title        = {Trustless privacy-preserving data aggregation on ethereum with hypercube network topology},
  volume       = {230},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A survey on authentication protocols of dynamic wireless EV charging. <em>COMCOM</em>, <em>230</em>, 108008. (<a href='https://doi.org/10.1016/j.comcom.2024.108008'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Electric Vehicles (EVs) are considered the predominant method of decreasing fossil fuels as well as greenhouse gas emissions. With the drastic growth of EVs, the future smart grid is expected to extensively incorporate dynamic wireless charging (DWC) systems, a significant advancement over traditional charging methods. DWC, offering the unique ability to charge vehicles in motion, introduces new infrastructures, complex network models and consequently, a massive attack surface. To accomplish the goal of such an enormous smart grid accompanying DWCs, the security of EV charging infrastructures has become a deciding factor. EV charging is vulnerable to cyberattacks as it has many attack vectors and many challenges to combat. Unlike the traditional charging services provided in a typical static charging station, the DWC has a complex network architecture which makes it vulnerable to many forms of cyberattacks. Authentication plays a crucial role in safeguarding the frontline security of this ecosystem. However, within the domain of DWC, the current academic landscape has seen limited attention dedicated to authentication protocols. This background signifies the necessity of a comprehensive survey to cover the authentication protocols of dynamic wireless EV charging environments. This review paper examines the security requirements and the network model of the DWC, providing comprehensive insights into existing authentication protocols by scrutinizing a proper classification. Furthermore, the paper addresses existing challenges in authentication schemes within DWC and explores potential future research tendencies aiming to strengthen the security framework of this emerging technology.},
  archive      = {J_COMCOM},
  author       = {Nethmi Hettiarachchi and Saqib Hakak and Kalikinkar Mandal},
  doi          = {10.1016/j.comcom.2024.108008},
  journal      = {Computer Communications},
  month        = {1},
  pages        = {108008},
  shortjournal = {Comput. Commun.},
  title        = {A survey on authentication protocols of dynamic wireless EV charging},
  volume       = {230},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Trajectory design of UAV-aided energy-harvesting relay networks in the terahertz band. <em>COMCOM</em>, <em>230</em>, 108007. (<a href='https://doi.org/10.1016/j.comcom.2024.108007'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Unmanned aerial vehicle (UAV)-aided relaying benefits from easy deployment, strong communication channels, and mobility compared with traditional ground relaying, thereby enhancing the wireless connectivity of future industrial Internet of Things networks. In this paper, a UAV-assisted relay network capable of harvesting energy from a source is designed by exploiting the radio frequency band and transmitting information between the transmitter and corresponding receiver utilizing the terahertz (THz) band. Subsequently, the channel capacity is analytically derived using the finite blocklength theorem for THz communication. In addition, we formulate an optimization problem to determine the optimal location of the UAV to maintain the minimum channel capacity between the transmitter and receiver pair. To determine the optimal location, we employ the augmented Lagrange multiplier approach. Regarding the optimal location, we propose an algorithm for two UAV trajectories, namely forward and backward trajectories, that employs modified minimal jerk trajectories. The numerical results indicate that the backward trajectory provides better system performance in terms of channel capacity. Moreover, the simulation findings show that in urban, dense urban, and high-rise areas, the backward trajectory improves upon the forward trajectory by approximately 41.07%, 59.02%, and 76.47%, respectively, while using a blocklength of 400 bytes.},
  archive      = {J_COMCOM},
  author       = {Saifur Rahman Sabuj and Yeongi Cho and Mahmoud Elsharief and Han-Shin Jo},
  doi          = {10.1016/j.comcom.2024.108007},
  journal      = {Computer Communications},
  month        = {1},
  pages        = {108007},
  shortjournal = {Comput. Commun.},
  title        = {Trajectory design of UAV-aided energy-harvesting relay networks in the terahertz band},
  volume       = {230},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A dual-tier adaptive one-class classification IDS for emerging cyberthreats. <em>COMCOM</em>, <em>229</em>, 108006. (<a href='https://doi.org/10.1016/j.comcom.2024.108006'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In today’s digital age, our dependence on IoT (Internet of Things) and IIoT (Industrial IoT) systems has grown immensely, which facilitates sensitive activities such as banking transactions and personal, enterprise data, and legal document exchanges. Cyberattackers consistently exploit weak security measures and tools. The Network Intrusion Detection System (IDS) acts as a primary tool against such cyber threats. However, machine learning-based IDSs, when trained on specific attack patterns, often misclassify new emerging cyberattacks. Further, the limited availability of attack instances for training a supervised learner and the ever-evolving nature of cyber threats further complicate the matter. This emphasizes the need for an adaptable IDS framework capable of recognizing and learning from unfamiliar/unseen attacks over time. In this research, we propose a one-class classification-driven IDS system structured on two tiers. The first tier distinguishes between normal activities and attacks/threats, while the second tier determines if the detected attack is known or unknown. Within this second tier, we also embed a multi-classification mechanism coupled with a clustering algorithm. This model not only identifies unseen attacks but also uses them for retraining them by clustering unseen attacks. This enables our model to be future-proofed, capable of evolving with emerging threat patterns. Leveraging one-class classifiers (OCC) at the first level, our approach bypasses the need for attack samples, addressing data imbalance and zero-day attack concerns and OCC at the second level can effectively separate unknown attacks from the known attacks. Our methodology and evaluations indicate that the presented framework exhibits promising potential for real-world deployments.},
  archive      = {J_COMCOM},
  author       = {Md. Ashraf Uddin and Sunil Aryal and Mohamed Reda Bouadjenek and Muna Al-Hawawreh and Md. Alamin Talukder},
  doi          = {10.1016/j.comcom.2024.108006},
  journal      = {Computer Communications},
  month        = {1},
  pages        = {108006},
  shortjournal = {Comput. Commun.},
  title        = {A dual-tier adaptive one-class classification IDS for emerging cyberthreats},
  volume       = {229},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A deep dive into cybersecurity solutions for AI-driven IoT-enabled smart cities in advanced communication networks. <em>COMCOM</em>, <em>229</em>, 108000. (<a href='https://doi.org/10.1016/j.comcom.2024.108000'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The integration of the Internet of Things (IoT) and artificial intelligence (AI) in urban infrastructure, powered by advanced information communication technologies (ICT), has paved the way for smart cities. While these technologies promise enhanced quality of life, economic growth, and improved public services, they also introduce significant cybersecurity challenges. This article comprehensively examines the complex factors in securing AI-driven IoT-enabled smart cities within the framework of future communication networks. Our research addresses critical questions about the evolving threat, multi-layered security approaches, the role of AI in enhancing cybersecurity, and necessary policy frameworks. We conduct an in-depth analysis of cybersecurity solutions across service, application, network, and physical layers, evaluating their effectiveness and integration potential with existing systems. The study offers a detailed examination of AI-driven security approaches, particularly ML and DL techniques, assessing their applicability and limitations in smart city environments. We incorporate real-world case studies to illustrate successful strategies and show areas requiring further research, especially considering emerging communication technologies. Our findings contribute to the field by providing a multi-layered classification of cybersecurity solutions, assessing AI-driven security approaches, and exploring future research directions. Additionally, we investigate the essential role played by policy and regulatory frameworks in safeguarding smart city security. Based on our analysis, we offer recommendations for technical implementations and policy development, aiming to create a holistic approach that balances technological advancements with robust security measures. This study also provides valuable insights for scholars, professionals, and policymakers, offering a comprehensive perspective on the cybersecurity challenges and solutions for AI-driven IoT-enabled smart cities in advanced communication networks.},
  archive      = {J_COMCOM},
  author       = {Jehad Ali and Sushil Kumar Singh and Weiwei Jiang and Abdulmajeed M. Alenezi and Muhammad Islam and Yousef Ibrahim Daradkeh and Asif Mehmood},
  doi          = {10.1016/j.comcom.2024.108000},
  journal      = {Computer Communications},
  month        = {1},
  pages        = {108000},
  shortjournal = {Comput. Commun.},
  title        = {A deep dive into cybersecurity solutions for AI-driven IoT-enabled smart cities in advanced communication networks},
  volume       = {229},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). The pupil outdoes the master: Imperfect demonstration-assisted trust region jamming policy optimization against frequency-hopping spread spectrum. <em>COMCOM</em>, <em>229</em>, 107993. (<a href='https://doi.org/10.1016/j.comcom.2024.107993'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Jamming decision-making is a pivotal component of modern electromagnetic warfare, wherein recent years have witnessed the extensive application of deep reinforcement learning techniques to enhance the autonomy and intelligence of wireless communication jamming decisions. However, existing researches heavily rely on manually designed customized jamming reward functions, leading to significant consumption of human and computational resources. To this end, under the premise of obviating designing task-customized reward functions, we propose a jamming policy optimization method that learns from imperfect demonstrations to effectively address the complex and high-dimensional jamming resource allocation problem against frequency hopping spread spectrum (FHSS) communication systems. To achieve this, a policy network is meticulously architected to consecutively ascertain jamming schemes for each jamming node, facilitating the construction of the dynamic transition within the Markov decision process. Subsequently, anchored in the dual-trust region concept, we design policy improvement and policy adversarial imitation phases. During the policy improvement phase, the trust region policy optimization method is utilized to refine the policy, while the policy adversarial imitation phase employs adversarial training to guide policy exploration using information embedded in demonstrations. Extensive simulation results indicate that our proposed method can approximate the optimal jamming performance trained under customized reward functions, even with rough binary reward settings, and also significantly surpass demonstration performance.},
  archive      = {J_COMCOM},
  author       = {Ning Rao and Hua Xu and Zisen Qi and Dan Wang and Yue Zhang and Xiang Peng and Lei Jiang},
  doi          = {10.1016/j.comcom.2024.107993},
  journal      = {Computer Communications},
  month        = {1},
  pages        = {107993},
  shortjournal = {Comput. Commun.},
  title        = {The pupil outdoes the master: Imperfect demonstration-assisted trust region jamming policy optimization against frequency-hopping spread spectrum},
  volume       = {229},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Symbol-level scheme for combating eavesdropping: Symbol conversion and constellation adjustment. <em>COMCOM</em>, <em>229</em>, 107992. (<a href='https://doi.org/10.1016/j.comcom.2024.107992'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the non-orthogonal multiple access scenario, users may suffer inter-multiuser eavesdropping due to the feature of successive interference cancellation, and the conditions of eavesdropping suppression methods in the traditional schemes may not be satisfied. To combat this eavesdropping, we consider physical layer security and propose a novel scheme by specially designing symbol conversion and constellation adjustment methods. Based on these methods, the amplitudes and phases of symbols are properly changed. When each user intercepts information as an eavesdropper, he/she has to accept high error probability, or he/she has to undergo exorbitant overhead. Analytical and numerical results demonstrate that the proposed scheme can protect the privacy of information, and this protection does not destruct the execution of successive interference cancellation and symbol transmission.},
  archive      = {J_COMCOM},
  author       = {Datong Xu and Chaosheng Qiu and Wenshan Yin and Pan Zhao and Mingyang Cui},
  doi          = {10.1016/j.comcom.2024.107992},
  journal      = {Computer Communications},
  month        = {1},
  pages        = {107992},
  shortjournal = {Comput. Commun.},
  title        = {Symbol-level scheme for combating eavesdropping: Symbol conversion and constellation adjustment},
  volume       = {229},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Automating 5G network slice management for industrial applications. <em>COMCOM</em>, <em>229</em>, 107991. (<a href='https://doi.org/10.1016/j.comcom.2024.107991'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The transition to Industry 4.0 introduces new use cases with unique communication requirements, demanding wireless technologies capable of dynamically adjusting their performance to meet various demands. Leveraging network slicing, 5G technology offers the flexibility to support such use cases. However, the usage and deployment of network slices in networks are complex tasks. To increase the adoption of 5G, there is a need for mechanisms that automate the deployment and management of network slices. This paper introduces a design for a network slice manager capable of such mechanisms in 5G networks. This design adheres to related standards, facilitating interoperability with other software, while also considering the capabilities and limitations of the technology. The proposed design can provision custom slices tailored to meet the unique requirements of verticals, offering communication performance across the spectrum of the three primary 5G services (eMBB, URLLC, and mMTC/mIoT). To access the proposed design, a Proof-of-Concept (PoC) prototype was developed and evaluated. The evaluation results demonstrate the flexibility of the proposed solution for deploying slices adjusted to the vertical use cases. Additionally, the slices generated by the PoC maintain a high TRL (Technology Readiness Level) equivalent to that of the commercial-graded network used.},
  archive      = {J_COMCOM},
  author       = {André Perdigão and José Quevedo and Rui L. Aguiar},
  doi          = {10.1016/j.comcom.2024.107991},
  journal      = {Computer Communications},
  month        = {1},
  pages        = {107991},
  shortjournal = {Comput. Commun.},
  title        = {Automating 5G network slice management for industrial applications},
  volume       = {229},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). High-performance BFT consensus for metaverse through block linking and shortcut loop. <em>COMCOM</em>, <em>229</em>, 107990. (<a href='https://doi.org/10.1016/j.comcom.2024.107990'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, the Metaverse has captured increasing attention. As the foundational technologies for these digital realms, blockchain systems and their critical component – the Byzantine Fault Tolerance (BFT) consensus protocol – significantly influence the performance of Metaverse. Due to vulnerabilities to network attacks, synchronous and partially synchronous consensus protocols often face compromises in their liveness or security. Consequently, recent efforts in BFT consensus have shifted towards asynchronous consensus protocols, notably the Multi-valued Validated Binary Agreement (MVBA) protocols, with sMVBA being particularly prominent. Despite its advances, sMVBA struggles to meet the high-performance demands of Metaverse applications. Each sMVBA instance commits only one block, discarding all others, which severely restricts throughput. Moreover, if a leader in a given view crashes, nodes must rebroadcast blocks in the subsequent view, resulting in increased latency. To overcome these challenges, this paper introduces Mercury , a protocol designed to enhance throughput under various conditions and reduce latency in less favorable scenarios where leaders are crashed. Mercury incorporates a mechanism whereby each block contains hashes from blocks of a previous instance, linking blocks across instances. This structure ensures that once a block is committed, all its linked blocks are also committed, thereby boosting throughput. Additionally, Mercury integrates a ‘shortcut loop’ mechanism, allowing nodes to bypass the last phase of the current view and the block broadcasting in the next view, significantly decreasing latency. Our experimental evaluations of Mercury confirm its superior performance. Compared to the cutting-edge protocols, sMVBA, CKPS, and AMS, Mercury boosts throughput by 1.03X, 1.65X, and 2.51X, respectively.},
  archive      = {J_COMCOM},
  author       = {Rui Hao and Chaozheng Ding and Xiaohai Dai and Hao Fan and Jianwen Xiang},
  doi          = {10.1016/j.comcom.2024.107990},
  journal      = {Computer Communications},
  month        = {1},
  pages        = {107990},
  shortjournal = {Comput. Commun.},
  title        = {High-performance BFT consensus for metaverse through block linking and shortcut loop},
  volume       = {229},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). MDTA: An efficient, scalable and fast multiple disjoint tree algorithm for dynamic environments. <em>COMCOM</em>, <em>229</em>, 107989. (<a href='https://doi.org/10.1016/j.comcom.2024.107989'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Emerging applications such as telemedicine, the tactile Internet or live streaming place high demands on low latency to ensure a satisfactory Quality of Experience (QoE). In these scenarios the use of trees can be particularly interesting to efficiently deliver traffic to groups of users because they further enhance network performance by providing redundancy and fault tolerance, ensuring service continuity when network failure or congestion scenarios occur. Furthermore, if trees are isolated from each other (they do not share common communication elements as links and/or nodes), their benefits are further enhanced since events such as failures or congestion in one tree do not affect others. However, the challenge of computing fully disjoint trees (both link- and node-disjoint) introduces significant mathematical complexity, resulting in longer computation times, which negatively impacts latency-sensitive applications. In this article, we propose a novel algorithm designed to rapidly compute multiple fully (either link- or node-) disjoint trees while maintaining efficiency and scalability, specifically focused on targeting the low-latency requirements of emerging services and applications. The proposed algorithm addresses the complexity of ensuring disjointedness between trees without sacrificing performance. Our solution has been tested in a variety of network environments, including both wired and wireless scenarios. The results showcase that our proposed method is approximately 100 times faster than existing techniques, while achieving a comparable success rate in terms of number of obtained disjoint trees. This significant improvement in computational speed makes our approach highly suitable for the low-latency requirements of next-generation networks.},
  archive      = {J_COMCOM},
  author       = {Diego Lopez-Pajares and Elisa Rojas and Mankamana Prasad Mishra and Parveen Jindgar and Joaquin Alvarez-Horcajo and Nicolas Manso and Jonathan Desmarais},
  doi          = {10.1016/j.comcom.2024.107989},
  journal      = {Computer Communications},
  month        = {1},
  pages        = {107989},
  shortjournal = {Comput. Commun.},
  title        = {MDTA: An efficient, scalable and fast multiple disjoint tree algorithm for dynamic environments},
  volume       = {229},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Communication-efficient heterogeneous multi-UAV task allocation based on clustering. <em>COMCOM</em>, <em>229</em>, 107986. (<a href='https://doi.org/10.1016/j.comcom.2024.107986'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The heterogeneous unmanned aerial vehicle (UAV) system aims to achieve higher-level task coordination and execution by integrating UAVs of different types, functionalities, and scales. Addressing the diverse and complex requirements of tasks, the allocation algorithm for decentralized multi-UAV systems often encounters communication redundancy, leading to the issue of excessive communication overhead. This paper proposes a clustering-based Consensus-Based Bundle Algorithm (Clustering-CBBA), which introduces a novel bundle construction, an improved consensus strategy, and a distance-based UAV grouping approach. Specifically, utilizing the k-means++ method based on distance factors, UAVs are initially partitioned into different clusters, breaking down the large-scale problem into smaller ones. Subsequently, the first UAV in each cluster is designated as the leader UAV. The proposed algorithm can handle multi-UAV tasks by improving the task bundle construction method and consensus algorithm. Additionally, intra-cluster UAVs employ an internal conflict resolution method to gather the latest information, while inter-cluster UAVs use an external conflict resolution method to ensure conflict-free task allocation, continuing until the algorithm converges. Experimental results demonstrate that the proposed method, compared to DMCHBA, G-CBBA, and baseline CBBA, significantly reduces communication overhead across different task scales and UAV quantities. Moreover, it maintains ideal performance regarding task completion and global task reward, showcasing higher efficiency and practicality.},
  archive      = {J_COMCOM},
  author       = {Na Dong and Shuai Liu and Xiaoming Mai},
  doi          = {10.1016/j.comcom.2024.107986},
  journal      = {Computer Communications},
  month        = {1},
  pages        = {107986},
  shortjournal = {Comput. Commun.},
  title        = {Communication-efficient heterogeneous multi-UAV task allocation based on clustering},
  volume       = {229},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Safe load balancing in software-defined-networking. <em>COMCOM</em>, <em>229</em>, 107985. (<a href='https://doi.org/10.1016/j.comcom.2024.107985'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {High performance, reliability and safety are crucial properties of any Software-Defined-Networking (SDN) system. Although the use of Deep Reinforcement Learning (DRL) algorithms has been widely studied to improve performance, their practical applications are still limited as they fail to ensure safe operations in exploration and decision-making. To fill this gap, we explore the design of a Control Barrier Function (CBF) on top of Deep Reinforcement Learning (DRL) algorithms for load-balancing. We show that our DRL-CBF approach is capable of meeting safety requirements during training and testing while achieving near-optimal performance in testing. We provide results using two simulators: a flow-based simulator, which is used for proof-of-concept and benchmarking, and a packet-based simulator that implements real protocols and scheduling. Thanks to the flow-based simulator, we compared the performance against the optimal policy, solving a Non Linear Programming (NLP) problem with the SCIP solver. Furthermore, we showed that pre-trained models in the flow-based simulator, which is faster, can be transferred to the packet simulator, which is slower but more accurate, with some fine-tuning. Overall, the results suggest that near-optimal Quality-of-Service (QoS) performance in terms of end-to-end delay can be achieved while safety requirements related to link capacity constraints are guaranteed. In the packet-based simulator, we also show that our DRL-CBF algorithms outperform non-RL baseline algorithms. When the models are fine-tuned over a few episodes, we achieved smoother QoS and safety in training, and similar performance in testing compared to the case where models have been trained from scratch.},
  archive      = {J_COMCOM},
  author       = {Lam Dinh and Pham Tran Anh Quang and Jérémie Leguay},
  doi          = {10.1016/j.comcom.2024.107985},
  journal      = {Computer Communications},
  month        = {1},
  pages        = {107985},
  shortjournal = {Comput. Commun.},
  title        = {Safe load balancing in software-defined-networking},
  volume       = {229},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). 5G core network control plane: Network security challenges and solution requirements. <em>COMCOM</em>, <em>229</em>, 107982. (<a href='https://doi.org/10.1016/j.comcom.2024.107982'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The control plane of the 5G Core Network (5GCN) is essential for ensuring reliable and high-performance 5G communication. It provides critical network services such as authentication, user credentials, and privacy-sensitive signaling. However, the security threat landscape of the 5GCN control plane has largely expanded and it faces serious security threats from various sources and interfaces. In this paper, we analyze the new features and vulnerabilities of the 5GCN service-based architecture (SBA) with a focus on the control plane. We investigate the network threat surface in the 5GCN and outline potential vulnerabilities in the control plane. We develop a threat model to illustrate the potential threat sources, vulnerable interfaces, possible threats and their impacts. We provide a comprehensive survey of the existing security solutions, identify their challenges and propose possible solution requirements to address the network security challenges in the control plane of 5GCN and beyond.},
  archive      = {J_COMCOM},
  author       = {Rajendra Patil and Zixu Tian and Mohan Gurusamy and Joshua McCloud},
  doi          = {10.1016/j.comcom.2024.107982},
  journal      = {Computer Communications},
  month        = {1},
  pages        = {107982},
  shortjournal = {Comput. Commun.},
  title        = {5G core network control plane: Network security challenges and solution requirements},
  volume       = {229},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Assessing the impact of the burst size in the FTM ranging procedure in COTS wi-fi devices. <em>COMCOM</em>, <em>229</em>, 107980. (<a href='https://doi.org/10.1016/j.comcom.2024.107980'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The wide availability of the Wi-Fi infrastructure together with the recent integration of the IEEE 802.11mc capability in common-off-the-shelf (COTS) devices, have contributed to increasing the interest of the research community in the fine time measurement (FTM) technique, which allows two Wi-Fi devices to estimate distances between each other. However, one of the main issues yet to be solved is how it scales with an increasing number of Wi-Fi devices injecting location-specific traffic in the shared medium. While the recently released IEEE 802.11az standard will still take time before being integrated in COTS devices, this paper aims at assessing the impact of the burst size on the ranging performance of COTS Wi-Fi Android devices running release 12. While increasing the burst size is expected to bring higher stability in the observed distance, on the other hand a longer transmission period for location-only purposes may jeopardize the transmission of data traffic among Wi-Fi users. Several models of smartphones and APs, and different frequency bands, have been considered in order to evaluate the behavior of the FTM procedure in real devices, showing that not always the newest or most expensive ones perform better. Also, it is shown that using the minimum burst size significantly decreases the performance and it is thus not recommended. While bursts longer than 8 may no be always supported by all the models and/or frequency bands, the small improvements in ranging estimations obtained when they are used do not always justify the increased location traffic injected in the network.},
  archive      = {J_COMCOM},
  author       = {Enrica Zola and Israel Martin-Escalona},
  doi          = {10.1016/j.comcom.2024.107980},
  journal      = {Computer Communications},
  month        = {1},
  pages        = {107980},
  shortjournal = {Comput. Commun.},
  title        = {Assessing the impact of the burst size in the FTM ranging procedure in COTS wi-fi devices},
  volume       = {229},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Achieving manet protection without the use of superfluous fictitious nodes. <em>COMCOM</em>, <em>229</em>, 107978. (<a href='https://doi.org/10.1016/j.comcom.2024.107978'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Mobile ad-hoc networks ( manet s) are everywhere. They are the basis for many current technologies (including vanet s, i o t , etc.), and used in multiple domains (including military, disaster zones, etc.). For them to function, routing protocols have been defined, taking into account the high mobility of network nodes. These protocols, however, are vulnerable to devastating attacks. Many solutions have been proposed for various attacks, including dcfm (Denial Contradictions with Fictitious nodes Mechanism) for the node isolation and gray-hole variants. In this work we present a refinement for dcfm , calculate its cost, and compare alternative algorithms. It will be shown that the entire fictitious mechanism is superfluous for some required security level. Examination of the results when under attack show that using dcfm ’s contradiction rules alone achieves the best cost-benefit ratio for networks with and without movement. In terms of packet delivery ratio ( pdr ), however, the proposed algorithm achieves 93% for a 50-node static network, stabilizing on 100% for 90 nodes and above. When movement is present, the success drops to 67%, which is slightly better than the alternatives examined.},
  archive      = {J_COMCOM},
  author       = {Nadav Schweitzer and Liad Cohen and Tirza Hirst and Amit Dvir and Ariel Stulman},
  doi          = {10.1016/j.comcom.2024.107978},
  journal      = {Computer Communications},
  month        = {1},
  pages        = {107978},
  shortjournal = {Comput. Commun.},
  title        = {Achieving manet protection without the use of superfluous fictitious nodes},
  volume       = {229},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A hierarchical adaptive federated reinforcement learning for efficient resource allocation and task scheduling in hierarchical IoT network. <em>COMCOM</em>, <em>229</em>, 107969. (<a href='https://doi.org/10.1016/j.comcom.2024.107969'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The increasing demand for processing numerous data from IoT devices in a hierarchical IoT network drives researchers to propose different resource allocation methods in the edge hosts efficiently. Traditional approaches often compromise on one of these aspects: either prioritizing local decision-making at the edge, which lacks global system insights or centralizing decisions in cloud systems, which raises privacy concerns. Additionally, most solutions do not consider scheduling tasks at the same time to effectively complete the prioritized task accordingly. This study introduces the hierarchical adaptive federated reinforcement learning (HAFedRL) framework for robust resource allocation and task scheduling in hierarchical IoT networks. At the local edge host level, a primal–dual update based deep deterministic policy gradient (DDPG) method is introduced for effective individual task resource allocation and scheduling. Concurrently, the central server utilizes an adaptive multi-objective policy gradient (AMOPG) which integrates a multi-objective policy adaptation (MOPA) with dynamic federated reward aggregation (DFRA) method to allocate resources across connected edge hosts. An adaptive learning rate modulation (ALRM) is proposed for faster convergence and to ensure high performance output from HAFedRL. Our proposed HAFedRL enables the effective integration of reward from edge hosts, ensuring the alignment of local and global optimization goals. The experimental results of HAFedRL showcase its efficacy in improving system-wide utility, average task completion rate, and optimizing resource utilization, establishing it as a robust solution for hierarchical IoT networks.},
  archive      = {J_COMCOM},
  author       = {A.S.M. Sharifuzzaman Sagar and Amir Haider and Hyung Seok Kim},
  doi          = {10.1016/j.comcom.2024.107969},
  journal      = {Computer Communications},
  month        = {1},
  pages        = {107969},
  shortjournal = {Comput. Commun.},
  title        = {A hierarchical adaptive federated reinforcement learning for efficient resource allocation and task scheduling in hierarchical IoT network},
  volume       = {229},
  year         = {2025},
}
</textarea>
</details></li>
</ul>

</body>
</html>
