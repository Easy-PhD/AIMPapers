<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>PR</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="pr">PR - 556</h2>
<ul>
<li><details>
<summary>
(2026). Condense loss: Exploiting vector magnitude during person re-identification training process. <em>PR</em>, <em>172</em>, 112443. (<a href='https://doi.org/10.1016/j.patcog.2025.112443'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The magnitudes of features and weights significantly affect the gradients during the training process. L2 normalized softmax losses (such as NormFace, CosFace, ArcFace, etc.) and Naive softmax losses both reduce the magnitudes of image features in the training process and achieve good results in face recognition and person re-identification tasks, respectively. In this paper, we fully utilize the feature vector magnitudes and propose Condense loss for Re-ID tasks, which replaces the inner production of Naive softmax loss with the negative Euclidean distance. Condense loss generates negative radial gradients when updating weight parameters to push all features compacter. Because the coefficients of tangential gradients (the tangential component of the gradients) are related to feature magnitudes, it ideally provides monotonically decreasing tangential gradients, resulting in gradually diminishing updates that enhance the stability of the training process. We also introduce a margin parameter into Condense loss to enlarge inter-class distances and thus help the model learn more discriminative features. Mathematical analysis is given in this paper, and we have conducted sufficient experiments focusing on Re-ID tasks to prove the corresponding conclusion. The experimental results demonstrate that the Condense loss achieves competitive results compared to the state-of-the-art methods in the person re-identification task. At the same time, it also has a good performance in face recognition tasks.},
  archive      = {J_PR},
  author       = {Xi Yang and Wenjiao Dong and Yingzhi Tang and Gu Zheng and Nannan Wang and Xinbo Gao},
  doi          = {10.1016/j.patcog.2025.112443},
  journal      = {Pattern Recognition},
  month        = {4},
  pages        = {112443},
  shortjournal = {Pattern Recognition},
  title        = {Condense loss: Exploiting vector magnitude during person re-identification training process},
  volume       = {172},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Buffer-free class-incremental learning with out-of-distribution detection. <em>PR</em>, <em>172</em>, 112441. (<a href='https://doi.org/10.1016/j.patcog.2025.112441'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Class-incremental learning (CIL) poses significant challenges in open-world scenarios, where models must learn new classes over time without forgetting previous ones and handle inputs from unknown classes that a closed-set model would misclassify. In this paper, we present an in-depth analysis of post-hoc OOD detection methods and investigate their potential to eliminate the need for a memory buffer. When post hoc OOD detection is applied at inference time, we discover that it can effectively replace buffer-based strategies. We examine the performance of these methods in terms of classification accuracy of seen samples and rejection rates of unseen samples. We show that our approach achieves competitive performance compared to recent multi-head and single-head methods that rely on memory buffers and other buffer-free approaches. The results show that the proposed approach outperforms them in a closed-world setting and detects unseen samples while being significantly resource-efficient. Experimental results on CIFAR-10, CIFAR-100, and Tiny ImageNet support our findings and offer new insights into the design of efficient and privacy-preserving CIL systems for open-world settings.},
  archive      = {J_PR},
  author       = {Srishti Gupta and Daniele Angioni and Maura Pintor and Ambra Demontis and Lea Schönherr and Fabio Roli and Battista Biggio},
  doi          = {10.1016/j.patcog.2025.112441},
  journal      = {Pattern Recognition},
  month        = {4},
  pages        = {112441},
  shortjournal = {Pattern Recognition},
  title        = {Buffer-free class-incremental learning with out-of-distribution detection},
  volume       = {172},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Corrigendum to “DiffTrajectory: Mitigating cumulative errors and enhancing inference efficiency in diffusion-based trajectory prediction” [Pattern recognition 172 (2026) 112339]. <em>PR</em>, <em>172</em>, 112440. (<a href='https://doi.org/10.1016/j.patcog.2025.112440'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_PR},
  author       = {Chengcheng Li and Luqi Gong and Leiheng Xu and Xin Wang},
  doi          = {10.1016/j.patcog.2025.112440},
  journal      = {Pattern Recognition},
  month        = {4},
  pages        = {112440},
  shortjournal = {Pattern Recognition},
  title        = {Corrigendum to “DiffTrajectory: Mitigating cumulative errors and enhancing inference efficiency in diffusion-based trajectory prediction” [Pattern recognition 172 (2026) 112339]},
  volume       = {172},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). A progressive attention network with transformer for multi-label image recognition. <em>PR</em>, <em>172</em>, 112439. (<a href='https://doi.org/10.1016/j.patcog.2025.112439'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent research typically improves the performance of multi-label image recognition by constructing higher-order pairwise label correlations. However, these methods lack the ability to effectively learn multi-scale features, which makes it difficult to distinguish small-scale objects. Moreover, most current attention-based methods to capture local salient features may ignore many useful non-salient features. To address the aforementioned issues, we propose a Transformer-based Progressive Attention Network (TPANet) for multi-label image recognition. Specifically, we first design a new adaptive multi-scale feature attention (AMSA) module to learn cross-scale features in multi-level features. Then, to excavate various useful object features, we introduce the transformer encoder to construct a semantic spatial attention (ESA) module and also propose a context-aware feature enhanced (CAFE) module. The former ESA module is used to discover complete object regions and capture discriminative features, and the latter CAFE module leverages object-local features to enhance pixel-level global features. The proposed TPANet model can generate more accurate object labels in three popular benchmark datasets (i.e., MS-COCO 2014, Pascal VOC 2007 and Visual Genome), and is competitive to state-of-the-art models (e.g., SST and FL-Tran, etc.).},
  archive      = {J_PR},
  author       = {Sulan Zhang and Zhenwen Liao and Jianeng Li and Lihua Hu and Jifu Zhang},
  doi          = {10.1016/j.patcog.2025.112439},
  journal      = {Pattern Recognition},
  month        = {4},
  pages        = {112439},
  shortjournal = {Pattern Recognition},
  title        = {A progressive attention network with transformer for multi-label image recognition},
  volume       = {172},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Layer-wise correlation and attention discrepancy distillation for semantic segmentation. <em>PR</em>, <em>172</em>, 112438. (<a href='https://doi.org/10.1016/j.patcog.2025.112438'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Knowledge distillation (KD) has recently garnered increased attention in segmentation tasks due to its effective balance between accuracy and computational efficiency. Nonetheless, existing methods mainly rely on structured knowledge from a single layer, overlooking the valuable discrepant knowledge that captures the diversity and distinctiveness of features across various layers, which is essential for the KD process. We present Layer-wise Correlation and Attention Discrepancy Distillation (LCADD) to tackle this issue, training compact and accurate semantic segmentation networks by considering layer-wise discrepancy knowledge. Specifically, we employ two distillation schemes: (i) correlation discrepancy distillation, which constructs a pixel-wise correlation discrepancy matrix across various layers to seize more detailed spatial dependencies, and (ii) attention discrepancy self-distillation, which aims to guide the shallower layers of the student network to emulate the attention discrepancy maps of the deeper layers, facilitating self-learning of attention discrepancy knowledge within the student network. Each proposed method is designed to work collaboratively in learning discrepancy knowledge, allowing the student network to better imitate the teacher from the perspective of layer-wise discrepancy. Our method has demonstrated superior performance on various semantic segmentation datasets, including Cityscapes, Pascal VOC 2012, and CamVid, compared to the latest knowledge distillation techniques, thereby validating its effectiveness.},
  archive      = {J_PR},
  author       = {Jianping Gou and Kaijie Chen and Cheng Chen and Weihua Ou and Xin Luo and Zhang Yi},
  doi          = {10.1016/j.patcog.2025.112438},
  journal      = {Pattern Recognition},
  month        = {4},
  pages        = {112438},
  shortjournal = {Pattern Recognition},
  title        = {Layer-wise correlation and attention discrepancy distillation for semantic segmentation},
  volume       = {172},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Gradient semi-masking for improving adversarial robustness. <em>PR</em>, <em>172</em>, 112433. (<a href='https://doi.org/10.1016/j.patcog.2025.112433'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In gradient masking, certain complex signal processing and probabilistic optimization strategies exhibit favorable characteristics such as nonlinearity, irreversibility, and feature preservation, thereby providing new solutions for adversarial defense. Inspired by this, this paper proposes a plug-and-play gradient semi-masking module ( GSeM ) to improve the adversarial robustness of neural networks. GSeM primarily contains a feature straight-through pathway that allows for normal gradient propagation and a feature mapping pathway that interrupts gradient flow. The multi-pathway and semi-masking characteristics cause GSeM to exhibit opposing behaviors when processing data and gradients. Specifically, during data processing, GSeM compresses the state space of features while introducing white noise augmentation. However, during gradient processing, it leads to inefficient updates to certain parameters and ineffective generation of training examples. To address this shortcoming, we correct gradient propagation and introduce gradient-corrected adversarial training. Extensive experiments demonstrate that GSeM differs fundamentally from earlier gradient masking methods: it can genuinely enhance the adversarial defense performance of neural networks, surpassing previous state-of-the-art approaches.},
  archive      = {J_PR},
  author       = {Xinlei Liu and Tao Hu and Peng Yi and Baolin Li and Jichao Xie and Hailong Ma},
  doi          = {10.1016/j.patcog.2025.112433},
  journal      = {Pattern Recognition},
  month        = {4},
  pages        = {112433},
  shortjournal = {Pattern Recognition},
  title        = {Gradient semi-masking for improving adversarial robustness},
  volume       = {172},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Structural-prior guided bi-generative network for image inpainting. <em>PR</em>, <em>172</em>, 112432. (<a href='https://doi.org/10.1016/j.patcog.2025.112432'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Image inpainting is a great challenge when reconstructed with realistic textures and required to enhance the consistency of semantic structures in large-scale missing regions. However, popular structural prior guidance methods primarily rely on the reconstruction of structural features. Due to the Markovian property inherent in purely feedforward architectures, noise undergoes persistent accumulation and propagation in early network layers. Without intermediate feedback mechanisms, minor artifacts in shallow layers would be nonlinearly amplified through successive convolution operations and cannot be timely corrected, thereby hindering the extraction of valid structural information. To this end, we presents a bi-generative network (Bi-GNet) guided by specific semantic structures, including an auxiliary network N s and an inpainting network N inp . Here N s provides the structural prior information to N inp for reconstructing the texture details of images. Additionally, we provide the spatial coordinate attention (SCA) and the adaptive feature filtering (AFF) module to ensure structural consistency and texture plausibility in the reconstructed content. Experiments demonstrate that Bi-GNet significantly outperforms other state-of-the-art approaches on three datasets and achieves good inpainting results on the Mogao Grottoes mural dataset.},
  archive      = {J_PR},
  author       = {Jiajun Zhang and Jizhao Liu and Huaikun Zhang and Jibao Zhang and Jing Lian},
  doi          = {10.1016/j.patcog.2025.112432},
  journal      = {Pattern Recognition},
  month        = {4},
  pages        = {112432},
  shortjournal = {Pattern Recognition},
  title        = {Structural-prior guided bi-generative network for image inpainting},
  volume       = {172},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Learning from majority label: A novel problem in multi-class multiple-instance learning. <em>PR</em>, <em>172</em>, 112425. (<a href='https://doi.org/10.1016/j.patcog.2025.112425'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The paper proposes a novel multi-class Multiple-Instance Learning (MIL) problem called Learning from Majority Label (LML). In LML, the majority class of instances in a bag is assigned as the bag-level label. The goal of LML is to train a classification model that estimates the class of each instance using the majority label. This problem is valuable in a variety of applications, including pathology image segmentation, political voting prediction, customer sentiment analysis, and environmental monitoring. To solve LML, we propose a Counting Network trained to produce bag-level majority labels, estimated by counting the number of instances in each class. Furthermore, analysis experiments on the characteristics of LML revealed that bags with a high proportion of the majority class facilitate learning. Based on this result, we developed a Majority Proportion Enhancement Module (MPEM) that increases the proportion of the majority class by removing minority class instances within the bags. Experiments demonstrate the superiority of the proposed method on four datasets compared to conventional MIL methods. Moreover, ablation studies confirmed the effectiveness of each module. The code is available at here .},
  archive      = {J_PR},
  author       = {Kaito Shiku and Shinnosuke Matsuo and Daiki Suehiro and Ryoma Bise},
  doi          = {10.1016/j.patcog.2025.112425},
  journal      = {Pattern Recognition},
  month        = {4},
  pages        = {112425},
  shortjournal = {Pattern Recognition},
  title        = {Learning from majority label: A novel problem in multi-class multiple-instance learning},
  volume       = {172},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Feature subset weighting for distance-based supervised learning. <em>PR</em>, <em>172</em>, 112424. (<a href='https://doi.org/10.1016/j.patcog.2025.112424'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper introduces feature subset weighting using monotone measures for distance-based supervised learning. The Choquet integral is used to define a distance function that incorporates these weights. This integration enables the proposed distances to effectively capture non-linear relationships and account for interactions both between conditional and decision attributes and among conditional attributes themselves, resulting in a more flexible distance measure. In particular, we show how this approach ensures that the distances remain unaffected by the addition of duplicate and strongly correlated features. Another key point of this approach is that it makes feature subset weighting computationally feasible, since only m feature subset weights should be calculated each time instead of calculating all feature subset weights ( 2 m ), where m is the number of attributes. Next, we also examine how the use of the Choquet integral for measuring similarity leads to a non-equivalent definition of distance. The relationship between distance and similarity is further explored through dual measures. Additionally, symmetric Choquet distances and similarities are proposed, preserving the classical symmetry between similarity and distance. Finally, we introduce a concrete feature subset weighting distance, evaluate its performance in a k -nearest neighbours (KNN) classification setting, and compare it against Mahalanobis distances and weighted distance methods.},
  archive      = {J_PR},
  author       = {Adnan Theerens and Yvan Saeys and Chris Cornelis},
  doi          = {10.1016/j.patcog.2025.112424},
  journal      = {Pattern Recognition},
  month        = {4},
  pages        = {112424},
  shortjournal = {Pattern Recognition},
  title        = {Feature subset weighting for distance-based supervised learning},
  volume       = {172},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Adaptive integration of textual context and visual embeddings for underrepresented vision classification. <em>PR</em>, <em>172</em>, 112420. (<a href='https://doi.org/10.1016/j.patcog.2025.112420'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The advancement of deep learning has significantly improved image classification performance; however, handling long-tail distributions remains challenging due to the limited data available for rare classes. Existing approaches predominantly focus on visual features, often neglecting the valuable contextual information provided by textual data, which can be especially beneficial for classes with sparse visual examples. In this work, we introduce a novel method addressing this limitation by integrating textual data generated by advanced language models with visual inputs through our newly proposed Adaptive Integration Block for Vision-Text Synergy (AIB-VTS). Specifically designed for Vision Transformer architectures, AIB-VTS adaptively balances visual and textual information during inference, effectively utilizing textual descriptions generated from large language models. Extensive experiments on benchmark datasets demonstrate substantial performance improvements across all class groups, particularly in underrepresented (tail) classes. These results confirm the effectiveness of our approach in leveraging textual context to mitigate data scarcity issues and enhance model robustness.},
  archive      = {J_PR},
  author       = {Seongyeop Kim and Hyung-Il Kim and Yong Man Ro},
  doi          = {10.1016/j.patcog.2025.112420},
  journal      = {Pattern Recognition},
  month        = {4},
  pages        = {112420},
  shortjournal = {Pattern Recognition},
  title        = {Adaptive integration of textual context and visual embeddings for underrepresented vision classification},
  volume       = {172},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). PCFFusion: Progressive cross-modal feature fusion network for infrared and visible images. <em>PR</em>, <em>172</em>, 112419. (<a href='https://doi.org/10.1016/j.patcog.2025.112419'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Infrared and visible image fusion (IVIF) aims to fuse thermal target information in infrared images and spatial texture information in visible images, improving the observability and comprehensibility of the fused images. Currently, most IVIF methods suffer from the loss of salient target information and texture details in fused images. To alleviate this problem, a progressive cross-modal feature fusion network (PCFFusion) for IVIF is proposed, which comprises two stages: feature extraction and feature fusion. In the feature extraction stage, to enhance the network’s feature representation capability, a feature decomposition module (FDM) is constructed to extract two modal features of different scales by defining a feature decomposition operation (FDO). In addition, by establishing correlations between the high- frequency and low-frequency components of two modal features, a cross-modal feature enhancement module (CMFEM) is built to realize correction and enhancement of the two features at each scale. The feature fusion stage achieves the fusion of two modal features at each scale and the supplementation of adjacent scale features by constructing three cross-domain fusion module (CDFMs). To constrain the fused results preserve more salient targets and richer texture details, a dual-feature fidelity loss function is defined by constructing a salient weight map to balance the two loss terms. Extensive experiments demonstrate that fusion results of the proposed method highlight prominent targets from infrared images while retaining rich background details from visible images, and the performance of PCFFusion is superior to some advanced methods. Specifically, compared to the optimal results obtained by other comparison methods, the proposed network achieves an average increase of 30.35 % and 10.9 % in metrics Mutual Information (MI) and Standard deviation (SD) on the TNO dataset, respectively.},
  archive      = {J_PR},
  author       = {Shuying Huang and Kai Zhang and Yong Yang and Weiguo Wan},
  doi          = {10.1016/j.patcog.2025.112419},
  journal      = {Pattern Recognition},
  month        = {4},
  pages        = {112419},
  shortjournal = {Pattern Recognition},
  title        = {PCFFusion: Progressive cross-modal feature fusion network for infrared and visible images},
  volume       = {172},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Time series adaptive mode decomposition (TAMD): Method for improving forecasting accuracy in the apparel industry. <em>PR</em>, <em>172</em>, 112417. (<a href='https://doi.org/10.1016/j.patcog.2025.112417'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Accurate forecasting of apparel sales is critical for inventory management, supply chain optimization, and market strategy planning. However, existing forecasting models often struggle to effectively capture the complex characteristics of apparel sales data, such as distinct seasonality, cyclicality, and strongly nonlinear fluctuations, which significantly hinder prediction accuracy and generalization ability. To address these challenges, this study introduces a novel Time series Adaptive Mode Decomposition (TAMD)-based forecasting algorithm. The proposed method: (1) employs Density-Based Spatial Clustering of Applications with Noise (DBSCAN) and sample entropy-guided Variational Mode Decomposition (VMD) to separate the input time series into noise components and multiple smooth Intrinsic Mode Functions (IMFs), to better capture intrinsic data dynamics; (2) refines the sub-series distribution features via an adaptive module guided by sample entropy, dividing each sub-series into subsequences with maximal distribution difference to improve adaptability to periodic changes and market volatility; (3) predicts each subsequence with adaptive distribution matching based on discontinuous random subsequence combinations, and then linearly superposes the prediction results as a final output, thereby boosting accuracy and generalizability. Comprehensive experiments on both public and self-constructed datasets (including four years of Taobao sales data for dresses, jeans, sweatshirts, and sweaters, totaling over 44.7 million records) demonstrate that TAMD outperforms existing methods significantly, highlighting its effectiveness in revealing the complexity of apparel market data and enhancing prediction performance.},
  archive      = {J_PR},
  author       = {Guangbao Zhou and Pengliang Liu and Quanle Lin and Miao Qian and Zhong Xiang and Zeyu Zheng and Lixian Liu},
  doi          = {10.1016/j.patcog.2025.112417},
  journal      = {Pattern Recognition},
  month        = {4},
  pages        = {112417},
  shortjournal = {Pattern Recognition},
  title        = {Time series adaptive mode decomposition (TAMD): Method for improving forecasting accuracy in the apparel industry},
  volume       = {172},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). A framework for bias-aware dataset evaluation in soft facial attribute recognition. <em>PR</em>, <em>172</em>, 112416. (<a href='https://doi.org/10.1016/j.patcog.2025.112416'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Soft Facial Attribute Recognition (FAR) remains largely unexplored in terms of demographic fairness. To the best of our knowledge, this study presents one of the first comprehensive analyses of demographic bias in FAR, proposing a systematic framework to detect, quantify, and promote awareness of both representational and stereotypical biases, supporting their mitigation. Leveraging established taxonomies, we evaluate state-of-the-art datasets using a rigorous set of interpretable bias metrics to uncover hidden demographic imbalances. To support reliable fairness assessment, we first enrich the datasets with standardized demographic annotations using the FairFace model. We then address label inconsistencies through the integration of predictions from advanced Vision-Language Models (VLMs). Our analysis reveals substantial imbalances across gender, age, and racial categories-specifically White, Black, and Asian- affecting dataset composition. Furthermore, we show that conventional fairness metrics often yield divergent assessments, highlighting the importance of multi-metric evaluation. This study provides a replicable methodology and actionable insights to support bias-aware facial analysis.},
  archive      = {J_PR},
  author       = {Lucia Cascone and Michele Nappi and Chiara Pero and Xinggang Wang},
  doi          = {10.1016/j.patcog.2025.112416},
  journal      = {Pattern Recognition},
  month        = {4},
  pages        = {112416},
  shortjournal = {Pattern Recognition},
  title        = {A framework for bias-aware dataset evaluation in soft facial attribute recognition},
  volume       = {172},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Fast multi-view discrete clustering with two solvers. <em>PR</em>, <em>172</em>, 112415. (<a href='https://doi.org/10.1016/j.patcog.2025.112415'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-view graph clustering follows a three-phase process: constructing view-specific similarity graphs, fusing information from different views, and conducting eigenvalue decomposition followed by post-processing to obtain the clustering indicators. However, it encounters two key challenges: the high computational cost of graph construction and eigenvalue decomposition, and the inevitable information deviation introduced by the last process. To tackle these obstacles, we propose Fast Multi-view Discrete Clustering with two solvers (FMDC), to directly and efficiently solve the multi-view graph clustering problem. FMDC involves: (1) generating a compact set of representative anchors to construct anchor graphs, (2) automatically weighting them into a symmetric and doubly stochastic aggregated similarity matrix, (3) executing clustering on the aggregated form with the discrete indicator matrix directly computed through two efficient solvers that we devised. The linear computational complexity of FMDC w.r.t. data size is a notable improvement over traditional quadratic or cubic complexity. Extensive experiments confirm the superior performance of FMDC both in efficiency and in effectiveness.},
  archive      = {J_PR},
  author       = {Qianyao Qiang and Bin Zhang and Jason Chen Zhang and Feiping Nie},
  doi          = {10.1016/j.patcog.2025.112415},
  journal      = {Pattern Recognition},
  month        = {4},
  pages        = {112415},
  shortjournal = {Pattern Recognition},
  title        = {Fast multi-view discrete clustering with two solvers},
  volume       = {172},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Asymmetric simulation-enhanced flow reconstruction for incomplete multimodal learning. <em>PR</em>, <em>172</em>, 112413. (<a href='https://doi.org/10.1016/j.patcog.2025.112413'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Incomplete multimodal learning addresses the common real-world challenge of missing modalities, which undermines the performance of standard multimodal methods. Existing solutions struggle with distribution mismatches between reconstructed and observed data, asymmetric cross-modal structures, and insufficient cross-modal knowledge sharing. To tackle these issues, we propose an asymmetric simulation-enhanced flow reconstruction (ASE-FR) framework, which contains following contributions: (1) Distribution-consistent flow reconstruction module that align available and missing modality distributions by normalizing flows; (2) Asymmetric simulation module that perturbs and randomly masks features to mimic real-world modality absence and improve robustness; (3) Modal-shared knowledge distillation that transfers shared representations from teacher encoders to a student encoder through contrastive learning. This framework is applicable to a range of real-world scenarios, such as multi-sensor networks in smart manufacturing, medical diagnostic systems combining imaging and electronic health records, and autonomous driving platforms that integrate camera and LiDAR data. The experimental results show that our ASE-FR method achieves 94.71 %, 41.85 % and 81.90 % accuracy on Audiovision-MNIST, MM-IMDb and IEMOCAP datasets, as well as 1.1376 error rate on CMU-MOSI dataset, which exhibits competitive performance.},
  archive      = {J_PR},
  author       = {Jiacheng Yao and Jing Zhang and Yixiao Wang and Li Zhuo},
  doi          = {10.1016/j.patcog.2025.112413},
  journal      = {Pattern Recognition},
  month        = {4},
  pages        = {112413},
  shortjournal = {Pattern Recognition},
  title        = {Asymmetric simulation-enhanced flow reconstruction for incomplete multimodal learning},
  volume       = {172},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Benchmarking the spatial robustness of DNNs via natural and adversarial localized corruptions. <em>PR</em>, <em>172</em>, 112412. (<a href='https://doi.org/10.1016/j.patcog.2025.112412'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The robustness of deep neural networks is a crucial factor in safety-critical applications, particularly in complex and dynamic environments (e.g., medical or driving scenarios) where localized corruptions can arise. While previous studies have evaluated the robustness of semantic segmentation (SS) models under whole-image natural or adversarial corruptions, a comprehensive investigation into the spatial robustness of dense vision models under localized corruptions remained underexplored. This paper fills this gap by introducing novel, region-aware metrics for benchmarking the spatial robustness of segmentation models, along with an evaluation framework to assess the impact of natural localized corruptions. Furthermore, it uncovers the inherent complexity of evaluating worst-case spatial robustness using only a single localized adversarial attack. To address this, the work proposes a region-aware multi-attack adversarial analysis to systematically assess model robustness across specific image regions. The proposed metrics and analysis were exploited to evaluate 14 segmentation models in driving scenarios, uncovering key insights into the effects of localized corruption in both natural and adversarial forms. The results reveal that models respond to these two types of threats differently; for instance, transformer-based segmentation models demonstrate notable robustness to localized natural corruptions but are highly vulnerable to adversarial ones, and vice versa for CNN-based models. Consequently, we also address the challenge of balancing robustness to both natural and adversarial localized corruptions by means of ensemble models, thereby achieving a broader threat coverage and improved reliability for dense vision tasks.},
  archive      = {J_PR},
  author       = {Giulia Marchiori Pietrosanti and Giulio Rossolini and Alessandro Biondi and Giorgio Buttazzo},
  doi          = {10.1016/j.patcog.2025.112412},
  journal      = {Pattern Recognition},
  month        = {4},
  pages        = {112412},
  shortjournal = {Pattern Recognition},
  title        = {Benchmarking the spatial robustness of DNNs via natural and adversarial localized corruptions},
  volume       = {172},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Preserving privacy without compromising accuracy: Machine unlearning for handwritten text recognition. <em>PR</em>, <em>172</em>, 112411. (<a href='https://doi.org/10.1016/j.patcog.2025.112411'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Handwritten Text Recognition (HTR) is crucial for document digitization, but handwritten data can contain user-identifiable features, like unique writing styles, posing privacy risks. Regulations such as the “right to be forgotten” require models to remove these sensitive traces without full retraining. We introduce a practical encoder-only transformer baseline as a robust reference for future HTR research. Building on this, we propose a two-stage unlearning framework for multihead transformer HTR models. Our method combines neural pruning with machine unlearning applied to a writer classification head, ensuring sensitive information is removed while preserving the recognition head. We also present Writer-ID Confusion (WIC), a method that forces the forget set to follow a uniform distribution over writer identities, unlearning user-specific cues while maintaining text recognition performance. We compare WIC to Random Labeling, Fisher Forgetting, Amnesiac Unlearning, and DELETE within our prune-unlearn pipeline and consistently achieve better privacy and accuracy trade-offs. This is the first systematic study of machine unlearning for HTR. Using metrics such as Accuracy, Character Error Rate (CER), Word Error Rate (WER), and Membership Inference Attacks (MIA) on the IAM and CVL datasets, we demonstrate that our method achieves state-of-the-art or superior performance for effective unlearning. These experiments show that our approach effectively safeguards privacy without compromising accuracy, opening new directions for document analysis research. Our code is publicly available at https://github.com/leitro/WIC-WriterIDConfusion-MachineUnlearning .},
  archive      = {J_PR},
  author       = {Lei Kang and Xuanshuo Fu and Lluis Gomez and Alicia Fornés and Ernest Valveny and Dimosthenis Karatzas},
  doi          = {10.1016/j.patcog.2025.112411},
  journal      = {Pattern Recognition},
  month        = {4},
  pages        = {112411},
  shortjournal = {Pattern Recognition},
  title        = {Preserving privacy without compromising accuracy: Machine unlearning for handwritten text recognition},
  volume       = {172},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Federated automatic latent variable selection in multi-output gaussian processes. <em>PR</em>, <em>172</em>, 112410. (<a href='https://doi.org/10.1016/j.patcog.2025.112410'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper explores a federated learning approach that automatically selects the number of latent processes in multi-output Gaussian processes (MGPs). The MGP has seen great success as a transfer learning tool when data is generated from multiple sources/units/entities. A common approach in MGPs to transfer knowledge across units involves gathering all data from each unit to a central server and extracting common independent latent processes to express each unit as a linear combination of the shared latent patterns. However, this approach poses key challenges in (i) determining the adequate number of latent processes and (ii) relying on centralized learning which leads to potential privacy risks and significant computational burdens on the central server. To address these issues, we propose a hierarchical model that places spike-and-slab priors on the coefficients of each latent process. These priors help automatically select only needed latent processes by shrinking the coefficients of unnecessary ones to zero. To estimate the model while avoiding the drawbacks of centralized learning, we propose a variational inference-based approach, that formulates model inference as an optimization problem compatible with federated settings. We then design a federated learning algorithm that allows units to jointly select and infer the common latent processes without sharing their data. We also discuss an efficient learning approach for a new unit within our proposed federated framework. Simulation and case studies on Li-ion battery degradation and air temperature data demonstrate the advantageous features of our proposed approach.},
  archive      = {J_PR},
  author       = {Jingyi Gao and Seokhyun Chung},
  doi          = {10.1016/j.patcog.2025.112410},
  journal      = {Pattern Recognition},
  month        = {4},
  pages        = {112410},
  shortjournal = {Pattern Recognition},
  title        = {Federated automatic latent variable selection in multi-output gaussian processes},
  volume       = {172},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Adaptive multi-view consistency clustering via structure-enhanced contrastive learning. <em>PR</em>, <em>172</em>, 112409. (<a href='https://doi.org/10.1016/j.patcog.2025.112409'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Current state-of-the-art deep multi-view clustering methods resort to contrastive learning to learn consensus representations with Cross-View Consistency ( CVC ). However, contrastive learning has inherent limitations when being applied to the multi-view clustering. On one hand, contrastive learning suffers from class collision issue, compromising the discriminability of consensus representation. On the other hand, contrastive alignment of two views of different quality could lead to representation degradation for the higher-quality view, weakening the robustness of the consensus representation. To alleviate these issues, this paper presents an Adaptive Multi-view consistency clustering method via structure-enhanced contrastive learning ( A da M ), which learns multi-faceted consensus representation that balances view-consistency, discriminability and robustness, forming an optimal consensus representation. Specifically, we first design a view fusion module and a structural learning module to learn view weights and structural relationships among samples, respectively, to derive the consensus representation. Second, beyond CVC , we propose a novel clustering framework called Adaptive Multi-View Consistency ( AMVC ), which adaptively aligns specific view representation with consensus representation based on the learned view weights. Furthermore, compared to CVC , we theoretically demonstrate the superiority of AMVC in learning robust consensus representation. Third, A da M leverages the structural relationships among samples to refine the conventional contrastive loss, further enhancing the discriminability of the consensus representation. Extensive experimental results on eight datasets demonstrate the superior performance of A da M over eight advanced multi-view clustering baselines.},
  archive      = {J_PR},
  author       = {Xuqian Xue and Qi Cai and Zhanwei Zhang and Yiming Lei and Hongming Shan and Junping Zhang},
  doi          = {10.1016/j.patcog.2025.112409},
  journal      = {Pattern Recognition},
  month        = {4},
  pages        = {112409},
  shortjournal = {Pattern Recognition},
  title        = {Adaptive multi-view consistency clustering via structure-enhanced contrastive learning},
  volume       = {172},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). A text-only weakly supervised learning framework for text spotting via text-to-polygon generator. <em>PR</em>, <em>172</em>, 112408. (<a href='https://doi.org/10.1016/j.patcog.2025.112408'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Advanced text spotting methods typically rely on large-scale, meticulously labeled datasets to achieve satisfactory performance. However, annotating fine-grained positional information of texts in real-world scene images is extremely costly and time-consuming. Although some weakly supervised methods have been developed to reduce annotation costs, they face two major challenges: 1) their performance significantly lags behind the fully supervised counterparts, and 2) They are tightly coupled with specific text spotting models, meaning that switching to a different model would require retraining and incur substantial computational costs. To address these limitations, we propose a novel text-only weakly supervised learning framework for text spotting via text-to-polygon generator. In the first stage, we pretrain a text-to-polygon generator on an auxiliary dataset, e.g., synthetic or public datasets, where full annotations are readily accessible. In the second stage, given real-world target datasets annotated with text-only labels, we employ the pretrained generator to produce pseudo polygon labels, thereby constructing a pseudo-labeled supervised dataset for training text spotting models. To ensure high-quality pseudo polygon labels, the text-to-polygon generator first identifies all candidate text regions, then filters those that are relevant to the target text, and finally predicts their precise spatial locations. Notably, this generator requires only a single pretraining session and can subsequently be applied to any text spotting model and target text-only dataset without incurring additional costs. Extensive experiments on public benchmarks demonstrate that our method can significantly reduce labeling costs while maintaining competitive performance.},
  archive      = {J_PR},
  author       = {Gege Zhang and Zhiyong Gan and Ling Deng and Shuaicheng Niu and Zhenghua Peng and Gang Dai and Shuangping Huang and Xiangmin Xu},
  doi          = {10.1016/j.patcog.2025.112408},
  journal      = {Pattern Recognition},
  month        = {4},
  pages        = {112408},
  shortjournal = {Pattern Recognition},
  title        = {A text-only weakly supervised learning framework for text spotting via text-to-polygon generator},
  volume       = {172},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Leveraging synthetic data for zero–shot and few–shot circle detection in real–world domains. <em>PR</em>, <em>172</em>, 112407. (<a href='https://doi.org/10.1016/j.patcog.2025.112407'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Circle detection plays a pivotal role in computer vision, underpinning applications from industrial inspection and bioinformatics to autonomous driving. Traditional methods, however, often struggle with real–world complexities, as they demand extensive parameter tuning and adaptation across different domains. In this paper, we present the Synthetic Circle Dataset (SynCircle), a large synthetic image dataset designed to train a YOLO v10 network for circle detection. The YOLO v10 network, pre–trained solely on synthetic data, demonstrates remarkable off–the–shelf performance that surpasses conventional methods in various practical scenarios. Furthermore, we show that incorporating just a few labeled real images for fine–tuning can significantly boost performance, reducing the need for large annotated datasets. To promote reproducibility and streamline adoption, we publicly release both the trained YOLO v10 weights and the full SynCircle dataset.},
  archive      = {J_PR},
  author       = {Paolo Andreini and Marco Tanfoni and Simone Bonechi and Monica Bianchini},
  doi          = {10.1016/j.patcog.2025.112407},
  journal      = {Pattern Recognition},
  month        = {4},
  pages        = {112407},
  shortjournal = {Pattern Recognition},
  title        = {Leveraging synthetic data for zero–shot and few–shot circle detection in real–world domains},
  volume       = {172},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). DURN: Data uncertainty-driven robust network for mural sketch detection. <em>PR</em>, <em>172</em>, 112404. (<a href='https://doi.org/10.1016/j.patcog.2025.112404'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Mural sketches reveal both the content and structure of the murals and are crucial for the preservation of murals. However, existing methods lack robustness, making it difficult to suppress noise while preserving sketches on damaged murals and fully capturing details on clear murals. To address this, we propose a Data Uncertainty-Driven Robust Network (DURN) for mural sketch detection. DURN uses uncertainty to quantify noise in the murals, converting prediction into a learnable normal distribution, where the mean represents the sketch and the variance denotes the uncertainty. This enables the model to learn both the sketch and the noise simultaneously, achieving noise suppression while preserving the sketches. To enhance sketches, we design an Adaptive Fusion Feature Enhancement Module (AFFE) to dynamically adjust the fusion strategy according to the contribution of features at different scales and reduce the information loss caused by feature dimensionality reduction to maximize the utility of each feature. We develop a novel Deep-Shallow Supervision (DSS) module to mitigate background noise using deep semantic information to guide shallow features without adding parameters. Additionally, we achieve model lightweighting through pruning techniques, ensuring competitive performance while reducing the number of parameters to only 4.5 % of the original. The experimental results show an improvement of 10. 4 % AP over existing methods, demonstrating the robustness of DURN for complex and damaged murals. The source code is available at https://github.com/TIVEN-Z/DURN .},
  archive      = {J_PR},
  author       = {Shenglin Peng and Xingguo Zhao and Jun Wang and Lin Wang and Shuyi Qu and Jingye Peng and Xianlin Peng},
  doi          = {10.1016/j.patcog.2025.112404},
  journal      = {Pattern Recognition},
  month        = {4},
  pages        = {112404},
  shortjournal = {Pattern Recognition},
  title        = {DURN: Data uncertainty-driven robust network for mural sketch detection},
  volume       = {172},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Learn depth space from light field via a distance-constraint query mechanism. <em>PR</em>, <em>172</em>, 112403. (<a href='https://doi.org/10.1016/j.patcog.2025.112403'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Light Field (LF) captures both spatial and angular information of scenes, enabling precise depth estimation. Recent advancements in deep learning have led to significant success in this field; however, existing methods primarily focus on modeling surface characteristics (e.g., depth maps) while overlooking the depth space, which contains additional valuable information. The depth space consists of numerous space points and provides substantially more geometric data than a single depth map. In this paper, we conceptualize depth prediction as a spatial modeling problem, aiming to learn the entire depth space rather than merely a single depth map. Specifically, we define space points as signed distances relative to the scene surface and propose a novel distance-constraint query mechanism for LF depth estimation. To model the depth space effectively, we first develop a mixed sampling strategy to approximate its data representation. Subsequently, we introduce an encoder-decoder network architecture to query the distances of each point, thereby implicitly embedding the depth space. Finally, to extract the target depth map from this space, we present a generation algorithm that iteratively invokes the decoder network. Through extensive experiments, our approach achieves the highest performance on LF depth estimation benchmarks, and also demonstrates superior performance on various synthetic and real-world scenes.},
  archive      = {J_PR},
  author       = {Hao Sheng and Rongshan Chen and Ruixuan Cong and Da Yang and Zhenglong Cui and Sizhe Wang},
  doi          = {10.1016/j.patcog.2025.112403},
  journal      = {Pattern Recognition},
  month        = {4},
  pages        = {112403},
  shortjournal = {Pattern Recognition},
  title        = {Learn depth space from light field via a distance-constraint query mechanism},
  volume       = {172},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Unsupervised instance segmentation with superpixels. <em>PR</em>, <em>172</em>, 112402. (<a href='https://doi.org/10.1016/j.patcog.2025.112402'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Instance segmentation is essential for numerous computer vision applications, including robotics, human-computer interaction, and autonomous driving. Currently, popular models bring impressive performance in instance segmentation by training with a large number of human annotations, which are costly to collect. For this reason, we present a new framework that efficiently and effectively segments objects without the need for human annotations. Firstly, a MultiCut algorithm is applied to self-supervised features for coarse mask segmentation. Then, a mask filter is employed to obtain high-quality coarse masks. To train the segmentation network, we compute a novel superpixel-guided mask loss, comprising hard loss and soft loss, with high-quality coarse masks and superpixels segmented from low-level image features. Lastly, a self-training process with a new adaptive loss is proposed to improve the quality of predicted masks. We conduct experiments on public datasets in instance segmentation and object detection to demonstrate the effectiveness of the proposed framework. The results show that the proposed framework outperforms previous state-of-the-art methods.},
  archive      = {J_PR},
  author       = {Cuong Manh Hoang},
  doi          = {10.1016/j.patcog.2025.112402},
  journal      = {Pattern Recognition},
  month        = {4},
  pages        = {112402},
  shortjournal = {Pattern Recognition},
  title        = {Unsupervised instance segmentation with superpixels},
  volume       = {172},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). MIGF-net: Multimodal interaction-guided fusion network for image aesthetics assessment. <em>PR</em>, <em>172</em>, 112401. (<a href='https://doi.org/10.1016/j.patcog.2025.112401'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the development of social media, people like to post images and comments to share their ideas, which provides rich visual and textural semantic information for image aesthetics assessment (IAA). However, most previous works either extracted the unimodal aesthetic features from image due to the difficulty of obtaining comments, or combined multimodal information together but ignoring the interactive relationship between image and comment, which limits the overall performance. To solve the above problem, we propose a Multimodal Interaction-Guided Fusion Network (MIGF-Net) for image aesthetics assessment based on both image and comment semantic information, which can not only solve the challenge of comment generating, but also provide the multimodal feature interactive information. Specifically, considering the coupling mechanism of the image theme, we construct a visual semantic fusion module to extract the visual semantic feature based on the visual attributes and the theme features. Then, a textural semantic feature extractor is designed to mine the semantic information hidden in comments, which not only addresses the issue of missing comments but also effectively complements the visual semantic features. Furthermore, we establish a Dual-Stream Interaction-Guided Fusion module to fuse the semantic features of images and comments, fully exploring the interactive relationship between images and comments in the human brain’s perception mechanism. Experimental results on two public image aesthetics evaluation datasets demonstrate that our model outperforms the current state-of-the-art methods. Our code will be released at https://github.com/wenzhipeng123/MIGF-Net .},
  archive      = {J_PR},
  author       = {Yun Liu and Zhipeng Wen and Leida Li and Peiguang Jing and Daoxin Fan},
  doi          = {10.1016/j.patcog.2025.112401},
  journal      = {Pattern Recognition},
  month        = {4},
  pages        = {112401},
  shortjournal = {Pattern Recognition},
  title        = {MIGF-net: Multimodal interaction-guided fusion network for image aesthetics assessment},
  volume       = {172},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). IRIS: An information path planning method based on reinforcement learning and information-directed sampling. <em>PR</em>, <em>172</em>, 112400. (<a href='https://doi.org/10.1016/j.patcog.2025.112400'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Information Path Planning (IPP) is a critical aspect of robotics, aimed at intelligently selecting information-rich paths to optimize robot trajectories and significantly enhance the efficiency and quality of data collection. However, in the process of maximizing information acquisition, IPP must also account for energy consumption, time constraints, and physical obstacles, which often lead to inefficiencies. To address these challenges, we propose an Information Path Planning method based on Reinforcement Learning and Information-Directed Sampling (IRIS). This model is the first to integrate Reinforcement Learning (RL) with Information-Directed Sampling (IDS), ensuring both immediate rewards and the potential for greater information gain through exploratory actions. IRIS employs an off-policy deep reinforcement learning framework, effectively overcoming the limitations observed in on-policy methods, thereby enhancing the model’s adaptability and efficiency. Simulation results demonstrate that the IRIS algorithm performs exceptionally well across various IPP scenarios. Once training stabilizes, IDS will dominate decision-making with a probability of approximately 1.3 % to yield better outcomes, highlighting its significant potential in this field. The relevant code is available at https://github.com/SUTLZY/IRIS .},
  archive      = {J_PR},
  author       = {Ziyuan Liu and Yan Zhuang and Peng Wu and Yuanchang Liu},
  doi          = {10.1016/j.patcog.2025.112400},
  journal      = {Pattern Recognition},
  month        = {4},
  pages        = {112400},
  shortjournal = {Pattern Recognition},
  title        = {IRIS: An information path planning method based on reinforcement learning and information-directed sampling},
  volume       = {172},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Knowledge tailoring: Bridging the teacher-student gap in semantic segmentation. <em>PR</em>, <em>172</em>, 112399. (<a href='https://doi.org/10.1016/j.patcog.2025.112399'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Knowledge distillation transfers knowledge from a high-capacity teacher network to a compact student network, but a large capacity gap often limits the student’s ability to fully benefit from the teacher’s guidance. In semantic segmentation, another major challenge is the difficulty in predicting accurate object boundaries, as even strong teacher models can produce ambiguous or imprecise outputs. To address both challenges, we present Knowledge Tailoring, a novel distillation framework that adapts the teacher’s knowledge to better match the student’s representational capacity and learning dynamics. Much like a tailor adjusts an oversized suit to fit the wearer’s shape, our method reshapes the teacher’s abundant but misaligned knowledge into a form more suitable for the student. KT introduces feature tailoring, which restructures intermediate features based on channel-wise correlation to narrow the representation gap, and logit tailoring, which improves boundary prediction by refining class-specific logits. The tailoring strategy evolves throughout training, offering guidance that aligns with the student’s progress. Experiments on Cityscapes, Pascal VOC, and ADE20K confirm that KT consistently enhances performance across a variety of architectures including DeepLabV3, PSPNet, and SegFormer. Our code is available for https://github.com/seok-hwa/KT .},
  archive      = {J_PR},
  author       = {Seokhwa Cheung and Seungbeom Woo and Taehoon Kim and Wonjun Hwang},
  doi          = {10.1016/j.patcog.2025.112399},
  journal      = {Pattern Recognition},
  month        = {4},
  pages        = {112399},
  shortjournal = {Pattern Recognition},
  title        = {Knowledge tailoring: Bridging the teacher-student gap in semantic segmentation},
  volume       = {172},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Understanding and tackling the modality imbalance problem in multimodal survival prediction. <em>PR</em>, <em>172</em>, 112398. (<a href='https://doi.org/10.1016/j.patcog.2025.112398'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Benefiting from the in-depth integration of multimodal data, survival prediction has emerged as a pivotal task in cancer prognosis by facilitating personalized treatment planning and medical resource allocation. In this study, we report an intriguing phenomenon of inter-modality capability gap (ICG) enlargement during joint survival modelling of genomics data and pathology images. This observation, supported by our dedicated theoretical analysis, uncovers a previously unrecognized modality imbalance problem, where pathology modality suffers from limited gradient propagation and insufficient learning while genomics modality dominates in reducing survival loss. To tackle this problem, we further propose a balanced multimodal learning approach for survival prediction named BMLSurv, which introduces two innovative auxiliary learning strategies: self-enhancement learning (SEL) and peer-assistance learning (PAL). The SEL strategy exploits a real-time imbalance measure to guide extra task-aware supervision, therefore dynamically strengthening pathology-specific gradient propagation in a self-enhanced manner. Meanwhile, the PAL strategy leverages the stronger genomics modality as a “helpful peer” to assist the sufficient learning of pathology modality via a new risk-ranking distillation technique. Extensive experiments on representative cancer datasets demonstrate that by successfully address the modality imbalance problem, BMLSurv remarkably narrows the ICG in joint survival modelling and consistently outperforms state-of-the-art methods by a large margin. These results underscore the potential of BMLSurv to advance multimodal survival prediction and enhance clinical decision-making in cancer prognosis.},
  archive      = {J_PR},
  author       = {Chicheng Zhou and Minghui Wang and Yi Shi and Anli Zhang and Ao Li},
  doi          = {10.1016/j.patcog.2025.112398},
  journal      = {Pattern Recognition},
  month        = {4},
  pages        = {112398},
  shortjournal = {Pattern Recognition},
  title        = {Understanding and tackling the modality imbalance problem in multimodal survival prediction},
  volume       = {172},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Hyper-network curvature: A new representation method for high-order brain network analysis. <em>PR</em>, <em>172</em>, 112397. (<a href='https://doi.org/10.1016/j.patcog.2025.112397'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Human brain is a complex system and contains abundant high-order interactions among multiple brain regions, which can be described by brain hyper-network. In brain hyper-networks, nodes represent brain regions of interest (ROIs), while edges describe the interactions of multiple ROIs, providing important high-order information for brain disease analysis and diagnosis. However, most of the existing hyper-network studies focused on the hyper-connection (i.e. hyper-edge) analysis and ignored the local topological information on nodes. To address this problem, we propose a new representation method (i.e., hyper-network curvature) for brain hyper-network analysis. Compared with the existing hyper-network representation methods, the proposed hyper-network curvature can be used to analyze the local topologies of nodes in brain hyper-networks. Based on hyper-network curvature, we further propose a novel graph kernel called brain hyper-network curvature kernel to measure the similarity of a pair of brain hyper-networks. We have proved that the proposed hyper-network curvature is bounded and brain hyper-network curvature kernel is positive definite. To evaluate the effectiveness of our proposed method, we perform the classification experiments on functional magnetic resonance imaging data of brain diseases. The experimental results demonstrate that our proposed method can significantly improve classification accuracy compared to the state-of-the-art graph kernels and graph neural networks for classifying brain diseases.},
  archive      = {J_PR},
  author       = {Kai Ma and Tianyu Du and Qi Zhu and Xuyun Wen and Jiashuang Huang and Xibei Yang and Daoqiang Zhang},
  doi          = {10.1016/j.patcog.2025.112397},
  journal      = {Pattern Recognition},
  month        = {4},
  pages        = {112397},
  shortjournal = {Pattern Recognition},
  title        = {Hyper-network curvature: A new representation method for high-order brain network analysis},
  volume       = {172},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Federated cross-source learning for lung nodule segmentation with data characteristic-aware weight optimization. <em>PR</em>, <em>172</em>, 112396. (<a href='https://doi.org/10.1016/j.patcog.2025.112396'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Federated learning enables multiple medical institutions to undertake distributed training while protecting patient privacy. Nevertheless, the significant variance in data distributions across diverse sites results in imbalanced knowledge acquisition, thereby affecting the performance of the global model. To tackle this challenge, we propose a novel federated algorithm for lung nodule segmentation, incorporating a Cross-source Learning (CSL) method. This method generates pseudo nodules by synthesizing the nodule phase spectrum with the nodule amplitude spectrum from other clients. These pseudo nodules are subsequently embedded into pulmonary regions to augment the data. By incorporating knowledge from various clients, which alleviates the challenges posed by non-IID data. On the server side, a Data Characteristic-aware Weight Optimization (DCWO) method is proposed to incorporate client data quality assessment and the size of lung nodule volume as weights to optimize both model performance and fairness. On the client side, we design a Multi-scale Attention Dynamic Convolution (MADC) lightweight network, which dynamically adapts attention to different spatial regions and extracts features at multiple scales. The performance of our method is superior to the state-of-the-art methods on six public and in-house CT datasets of lung cancer.},
  archive      = {J_PR},
  author       = {Xinjun Bian and Huan Lin and Yumeng Wang and Lingqiao Li and Zhenbing Liu and Huadeng Wang and Zhenwei Shi and Yi Qian and Zaiyi Liu and Rushi Lan and Xipeng Pan},
  doi          = {10.1016/j.patcog.2025.112396},
  journal      = {Pattern Recognition},
  month        = {4},
  pages        = {112396},
  shortjournal = {Pattern Recognition},
  title        = {Federated cross-source learning for lung nodule segmentation with data characteristic-aware weight optimization},
  volume       = {172},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Joint luminance-chrominance learning for quality assessment of low-light image enhancement. <em>PR</em>, <em>172</em>, 112395. (<a href='https://doi.org/10.1016/j.patcog.2025.112395'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Existing methods for low-light enhancement quality assessment (LEQA) often underperform across diverse scenarios. One reason is that most of them rely on shallow feature respresentations, while another is that deep-learning-based counterparts fail to make full use of the unique characteristics of low-light enhanced images (LEIs), such as luminance enhancement and color refinement. In this paper, we propose a novel Joint Luminance-Chrominance Learning Network (JLCLNet) for LEQA to comprehensively assess the effects of low-light image enhancement (LLIE) algorithms. Specifically, we construct a two-branch network architecture consisting of a luminance learning branch and a chrominance learning branch. In the luminance learning branch, the low- and high-frequency subbands of the luminance channel in the CIELAB color space, derived from the dual-tree complex wavelet transform (DTCWT), focus on measuring contrast enhancement and structure preservation. Meanwhile, the chrominance learning branch addresses potential color distortions by integrating perceptual information from the two parallel chrominance channels of the CIELAB color space. Finally, the complementary features from both branches are fused to predict quality scores. Experimental results on four public LEQA databases demonstrate the performance advantages of the proposed method compared to the state-of-the-art approaches. The source code of JLCLNet is available at https://github.com/li181119/JLCLNET .},
  archive      = {J_PR},
  author       = {Tuxin Guan and Qiuping Jiang and Xiongli Chai and Chaofeng Li},
  doi          = {10.1016/j.patcog.2025.112395},
  journal      = {Pattern Recognition},
  month        = {4},
  pages        = {112395},
  shortjournal = {Pattern Recognition},
  title        = {Joint luminance-chrominance learning for quality assessment of low-light image enhancement},
  volume       = {172},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). A graph contrastive learning network for change detection with heterogeneous remote sensing images. <em>PR</em>, <em>172</em>, 112394. (<a href='https://doi.org/10.1016/j.patcog.2025.112394'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Land cover change detection (LCCD) with heterogeneous remote sensing images (Hete-RSIs) is an attractive topic in the community of remote sensing applications. Intuitively, Hete-RSIs are acquired with different remote sensors, and they cannot be compared directly for LCCD because of the different imaging modalities. In this paper, a graph contrastive learning network (GCLN) is proposed for LCCD with bitemporal Hete-RSIs. First, with the motivation of smoothing the noise and utilizing contextual information, the k-nearest neighbor algorithm is used to improve the spectral homogeneity of the pixels within a superpixel. Then, a pairwise graph is constructed on the basis of each superpixel from spectral similarity and dissimilarity perspectives, and a graph feature learning network is designed to learn the near-far dependencies of graph features for change detection. Finally, the similarity and dissimilarity loss functions are coupled as a contrastive loss function to expand the difference between similar and dissimilar features. Comparisons with seven advanced methods on five pairs of Hete-RSIs demonstrate the feasibility and superiority of the proposed GCLN for LCCD with Hete-RSIs. For example, the improvements on the five datasets are 3.63 % , 8.47 % , 4.17 % , 8.23 % , and 4.98 % in terms of overall accuracy. The code of the proposed approach can be available at: https://github.com/ImgSciGroup/2024-GCLN .},
  archive      = {J_PR},
  author       = {Zhiyong Lv and Sizhe Cheng and Linfu Xie and Junhuai Li and Minghua Zhao},
  doi          = {10.1016/j.patcog.2025.112394},
  journal      = {Pattern Recognition},
  month        = {4},
  pages        = {112394},
  shortjournal = {Pattern Recognition},
  title        = {A graph contrastive learning network for change detection with heterogeneous remote sensing images},
  volume       = {172},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Edge craft odyssey: Navigating guided super-resolution with a fast, precise, and lightweight network. <em>PR</em>, <em>172</em>, 112392. (<a href='https://doi.org/10.1016/j.patcog.2025.112392'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Thermal imaging technology is exceptionally valuable in environments where visibility is limited or nonexistent. However, the high cost and technological limitations of high-resolution thermal imaging sensors restrict their widespread use. Many thermal cameras are now paired with high-resolution visible cameras, which can help improve low-resolution thermal images. However, aligning thermal and visible images is challenging due to differences in their spectral ranges, making pixel-wise alignment difficult. Therefore, we present the Edge Craft Odyssey Network (ECONet), a lightweight transformer-based network designed for Guided Thermal Super-Resolution (GTSR) to address these challenges. Our approach introduces a Progressive Edge Prediction module that extracts edge features from visible images using an adaptive threshold within our innovative Edge-Weighted Gradient Blending technique. This technique provides precise control over the blending intensity between low-resolution thermal and visible images. Additionally, we introduce a lightweight Cascade Deep Feature Extractor that focuses on efficient feature extraction and edge weight highlighting, enhancing the representation of high-frequency details. Experimental results show that ECONet outperforms state-of-the-art methods across various datasets while maintaining a relatively low computational and memory requirements. ECONet improves performance by up to 0.20 to 1.3 dB over existing methods and generates super-resolved images in a fraction of a second, approximately 91 % faster than the other methods. The code is available at https://github.com/Rm1n90/ECONet .},
  archive      = {J_PR},
  author       = {Armin Mehri and Parichehr Behjati and Dario Carpio and Angel D. Sappa},
  doi          = {10.1016/j.patcog.2025.112392},
  journal      = {Pattern Recognition},
  month        = {4},
  pages        = {112392},
  shortjournal = {Pattern Recognition},
  title        = {Edge craft odyssey: Navigating guided super-resolution with a fast, precise, and lightweight network},
  volume       = {172},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Jailbreak attack with multimodal virtual scenario hypnosis for vision-language models. <em>PR</em>, <em>172</em>, 112391. (<a href='https://doi.org/10.1016/j.patcog.2025.112391'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Due to the inherent vulnerabilities of large Vision-Language Models (VLMs), security governance has emerged as a critical concern, particularly given the risks posed by noisy and biased training data as well as adversarial attacks, including data poisoning and prompt injection. These perturbations can significantly degrade model performance and introduce multifaceted societal risks. To verify the safe robustness of VLMs and further inspire the design of defensive AI frameworks, we propose Virtual Scenario Hypnosis (VSH), a multimodal prompt injection jailbreak method that embeds malicious queries into prompts through a deceptive narrative framework. This approach strategically distracts the model while compromising its resistance to jailbreak attempts. Our methodology features two key innovations: 1) Targeted adversarial image prompts that transform textual content into visual layouts through optimized typographic designs, circumventing safety alignment mechanisms to elicit harmful responses; and 2) An information veil encrypted In-Context Learning (ICL) method for text prompts that systematically evades safety detection protocols. To streamline evaluation, we employ Large Language Models (LLMs) to facilitate an efficient assessment of jailbreak success rates, supported by a meticulously designed prompt template incorporating multi-dimensional scoring rules and evaluation metrics. Extensive experiments demonstrate the efficacy of VSH, achieving an overall success rate exceeding 82% on 500 harmful queries spanning multiple domains when tested against LLaVA-v1.5-13B and GPT-4o mini.},
  archive      = {J_PR},
  author       = {Xiayang Shi and Shangfeng Chen and Gang Zhang and Wei Wei and Yinlin Li and Zhaoxin Fan and Jingjing Liu},
  doi          = {10.1016/j.patcog.2025.112391},
  journal      = {Pattern Recognition},
  month        = {4},
  pages        = {112391},
  shortjournal = {Pattern Recognition},
  title        = {Jailbreak attack with multimodal virtual scenario hypnosis for vision-language models},
  volume       = {172},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). A locality-sensitive hashing based instance selection method with its application to acceleration of feature selection. <em>PR</em>, <em>172</em>, 112390. (<a href='https://doi.org/10.1016/j.patcog.2025.112390'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As one of important data preprocessing techniques, feature selection aims to remove redundant and irrelevant features and has been extensively applied to many fields. At present, however, the evaluation of existing feature selection algorithms focuses mainly on the scale of the selected features and the performance of models formulated by the selected features, while the running time of feature selection algorithms is usually neglected. It is noted that the computation complexity of the majority of feature selection algorithms is the square order of the number of instances, resulting in an exponential increase of the running time for large-scale data. In this paper, we propose an algorithm of core instance selection based on the locality-sensitive hashing (CISLSH) to improve the computation efficiency of feature selection algorithms by alleviating the instances used for feature selection. Specifically, all the instances are firstly considered to map them into the one-dimensional integer space using a locality-sensitive hashing (LSH) function. Given a set of hash functions families, a bucket index matrix is constructed to integrate all the mapping results of the set of hash functions families. Then, a voting mechanism is designed according to the bucket index matrix, which motivates to present a novel data partitioning method dividing similar instances into the same bucket (partition) as many as possible. Furthermore, the CISLSH algorithm is developed by selecting a core instance from each non-empty bucket. Finally, numerical experiments are conducted to assess the performance of CISLSH. The experimental results show that the execution of feature selection using the representative instances selected by CISLSH can not only significantly reduce the running time of feature selection but also guarantee the effectiveness of the selected features.},
  archive      = {J_PR},
  author       = {Fan Song and Xiao Zhang and Jinhai Li and Changlin Mei},
  doi          = {10.1016/j.patcog.2025.112390},
  journal      = {Pattern Recognition},
  month        = {4},
  pages        = {112390},
  shortjournal = {Pattern Recognition},
  title        = {A locality-sensitive hashing based instance selection method with its application to acceleration of feature selection},
  volume       = {172},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Adaptive latent disease state learning for multimodal alzheimer’s disease biomarker detection with missing modalities. <em>PR</em>, <em>172</em>, 112389. (<a href='https://doi.org/10.1016/j.patcog.2025.112389'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multimodal neuroimaging genetics is a crucial approach for identifying biomarkers of Alzheimer’s disease (AD) by leveraging the inherent relationships between genetic and neuroimaging data. However, existing methods are limited by susceptibility to input noises, underutilization of complementary information across neuroimaging modalities, and ineffective handling of samples with incomplete modalities. To address these challenges, we propose an Adaptive Latent Disease State Learning (ALDSL) method, which integrates noise reduction, latent space learning, adaptive regularization, and feature selection into a unified framework for detecting AD biomarkers from incomplete multimodal data. ALDSL introduces a noise reduction strategy based on inter-variable correlations and tailored distance metrics to eliminate noises in the input data, thereby obtaining high-quality representations for each modality. Additionally, latent disease state learning with adaptive regularization is proposed to capture inter-modality correlations by projecting the high-quality representations from multiple modalities into a common latent space. To utilize samples with incomplete modalities, we design a modality-specific weight matrix that accounts for the missing information in the latent disease state learning. Furthermore, an adaptive weighting determination strategy is developed to ensure that the modalities with different data types and varying sample sizes contribute on the same scale. We develop an efficient alternating optimization algorithm to solve the objective function of ALDSL. Experimental results on synthetic datasets and the ADNI GO/2 dataset demonstrate the effectiveness of ALDSL in detecting AD biomarkers.},
  archive      = {J_PR},
  author       = {Zhi Chen and Fengli Zhang and Yun Zhang and Jiajing Zhu and Qiaoqin Li and Yongguo Liu},
  doi          = {10.1016/j.patcog.2025.112389},
  journal      = {Pattern Recognition},
  month        = {4},
  pages        = {112389},
  shortjournal = {Pattern Recognition},
  title        = {Adaptive latent disease state learning for multimodal alzheimer’s disease biomarker detection with missing modalities},
  volume       = {172},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). MMP: Enhancing unsupervised graph anomaly detection with multi-view message passing. <em>PR</em>, <em>172</em>, 112388. (<a href='https://doi.org/10.1016/j.patcog.2025.112388'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The complementary and conflicting relationships between views are two fundamental issues when applying Graph Neural Networks (GNNs) to multi-view attributed graph anomaly detection. Most existing approaches do not address the inherent multi-view properties in the attribute space or leverage complementary information through simple representation fusion, which overlooks the conflicting information among different views. In this paper, we argue that effectively applying GNNs to multi-view anomaly detection necessitates reinforcing complementary information between views and, more importantly, managing conflicting information. Building on this perspective, this paper introduces Multi-View Message Passing (MMP), a novel and effective message passing paradigm specifically designed for multi-view anomaly detection. In the multi-view aggregation phase of MMP, views containing different types of information are integrated using view-specific aggregation functions. This approach enables the model to dynamically adjust the amount of information aggregated from complementary and conflicting views, thereby mitigating issues arising from insufficient complementary information and excessive conflicting information, which can lead to suboptimal representation learning. Furthermore, we propose an innovative aggregation loss mechanism that enhances model performance by optimizing the reconstruction differences between aggregated representations and the original views, thereby improving both detection accuracy and model interpretability. Extensive experiments on synthetic and real-world datasets validate the effectiveness and robustness of our method. The source code is available at https://github.com/weihus/MMP .},
  archive      = {J_PR},
  author       = {Weihu Song and Lei Li and Mengxiao Zhu and Yue Pei and Haogang Zhu},
  doi          = {10.1016/j.patcog.2025.112388},
  journal      = {Pattern Recognition},
  month        = {4},
  pages        = {112388},
  shortjournal = {Pattern Recognition},
  title        = {MMP: Enhancing unsupervised graph anomaly detection with multi-view message passing},
  volume       = {172},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). HopGAT: A multi-hop graph attention network with heterophily and degree awareness. <em>PR</em>, <em>172</em>, 112387. (<a href='https://doi.org/10.1016/j.patcog.2025.112387'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In highly heterophilic graphs, where nodes frequently connect across categories, the attention learning mechanism by dynamically adjusting neighboring node weights, may struggle to capture intricate node relationships. Furthermore, first-hop neighbor information is usually insufficient to encompass the global structure, but multi-hop increases complexity. To address these challenges, we propose HopGAT, a multi-hop graph attention network with heterophily and degree awareness. Firstly, we design heterophily-based neighbor sampling to sequentially filter high-hop neighbors by degree. Next, to obtain comprehensive global information, we construct a multi-hop recursive learning method with head and tail attention vectors to learn multi-hop neighbor features. Finally, we combine the average node degree of the graph with hop decay modeling to learn importance coefficients at different hops and adaptively aggregate the learned multi-hop features. Experimental results demonstrate that HopGAT significantly improves performance across 9 benchmark datasets with various heterophily and different average degrees.},
  archive      = {J_PR},
  author       = {Han Zhang and Huan Wang and Mingjing Han},
  doi          = {10.1016/j.patcog.2025.112387},
  journal      = {Pattern Recognition},
  month        = {4},
  pages        = {112387},
  shortjournal = {Pattern Recognition},
  title        = {HopGAT: A multi-hop graph attention network with heterophily and degree awareness},
  volume       = {172},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). A boundary-enhanced and target-driven deformable convolutional network for abdominal multi-organ segmentation. <em>PR</em>, <em>172</em>, 112386. (<a href='https://doi.org/10.1016/j.patcog.2025.112386'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {It is crucial to accurately segment organs from abdominal CT images for clinical diagnosis, treatment planning, and surgical guidance, which remains an extremely challenging task due to low contrast between organs and surrounding tissues and the difference of organ size and shape. Previous works mainly focused on complex network architectures or task-specific modules but frequently failed to learn irregular boundaries and did not consider that different slices from the same case might contain targets of different numbers of categories. To tackle these issues, this paper proposes UAMSNet for abdominal multi-organ segmentation. In UAMSNet, a hybrid receptive field extraction (HRFE) module is introduced to adaptively learn the features of irregular targets, which has an adaptive dilation factor containing distance information to facilitate spatial and channel attention. The HRFE module can simultaneously learn multiple scales and deformations of different organs. Furthermore, a multi-organ boundary-enhanced attention (MBA) module in the encoder and decoder is designed to provide effective boundary information for feature extraction based on the large peak of the organ edge. Finally, the difference in the number of organ categories between different slices is first considered using a loss function, which can adjust the loss computation based on organ categories in the image. The loss function mitigates the effect of false positives during training to ensure the model can adapt to small organ segmentation. Experimental results on WORD and Synapse datasets demonstrate that our UAMSNet outperforms the existing state-of-the-art methods. Ablation experiments confirm the effectiveness of our designed modules and loss function. Our code is publicly available on https://github.com/HeyJGJu/UAMSNet .},
  archive      = {J_PR},
  author       = {Jianguo Ju and Menghao Liu and Wenhuan Song and Tongtong Zhang and Jindong Liu and Pengfei Xu and Ziyu Guan},
  doi          = {10.1016/j.patcog.2025.112386},
  journal      = {Pattern Recognition},
  month        = {4},
  pages        = {112386},
  shortjournal = {Pattern Recognition},
  title        = {A boundary-enhanced and target-driven deformable convolutional network for abdominal multi-organ segmentation},
  volume       = {172},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Distantly supervised reinforcement localization for real-world object distribution estimation. <em>PR</em>, <em>172</em>, 112385. (<a href='https://doi.org/10.1016/j.patcog.2025.112385'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Predicting the distribution of objects in the real world from monocular images is a challenging task due to the disparity between object distributions in perspective images and reality. Many researchers focus on predicting object distributions by converting perspective images into Bird’s-Eye View (BEV) images. In scenarios where camera parameter information is unavailable, the prediction of vanishing lines becomes critical for performing inverse perspective transformations. However, accurately predicting vanishing lines necessitates accounting for variations in object size, which cannot be effectively captured through simple regression models. Therefore, this paper proposes a size variation-aware method, utilizing expert knowledge from object detection to build a reinforcement learning framework for predicting vanishing lines in traffic scenes. Specifically, this method leverages size information from trained detectors to convert perspective images into BEV images without the need for additional camera intrinsic parameters. First, we design a novel reward mechanism that utilizes prior knowledge of scale differences between similar objects in perspective images, allowing the network to automatically update and learn specific vanishing line positions. Second, we propose a fast inverse perspective transformation method, which accelerates the training speed of the proposed approach. To evaluate the effectiveness of the method, experiments are conducted on two traffic flow datasets. The experimental results demonstrate that the proposed algorithm accurately predicts vanishing line positions and successfully transforms perspective images into BEV images. Furthermore, the proposed algorithm performs competitively with directly supervised methods. The code is available at: https://github.com/HotChieh/DDRL.},
  archive      = {J_PR},
  author       = {Haojie Guo and Junyu Gao and Yuan Yuan},
  doi          = {10.1016/j.patcog.2025.112385},
  journal      = {Pattern Recognition},
  month        = {4},
  pages        = {112385},
  shortjournal = {Pattern Recognition},
  title        = {Distantly supervised reinforcement localization for real-world object distribution estimation},
  volume       = {172},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). A variable gaussian kernel scale active contour model based on jeffreys divergence for ICT image segmentation. <em>PR</em>, <em>172</em>, 112384. (<a href='https://doi.org/10.1016/j.patcog.2025.112384'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In industrial computed tomography (ICT), factors like beam scattering, insufficient beam intensity, and detector dark current often lead to weak edges, scattering artifacts, and severe Gaussian noise in ICT images. These issues pose significant difficulties for accurate segmentation of high-density complex structures using existing active contour models (ACMs). To address these limitations, this paper presents a variable Gaussian kernel scale active contour model based on Jeffreys divergence (VGJD). Firstly, the Jeffreys divergence (JD) is incorporated into the energy function to replace the conventional Euclidean distance, enhancing the contour’s ability to quantify pixel value disparity during evolution. Additionally, a filter weight is introduced to minimize the impact of noise. Moreover, a variable Gaussian kernel scale strategy is adopted to effectively integrate both global and local image information, thereby enhancing the robustness of the initial contour and improving the precision of detail segmentation. Finally, optimized length and regularity terms are employed to enforce constraints on the level set function. Extensive experimental results demonstrate that the VGJD model can effectively segment various complex ICT images, achieving superior precision in comparison to other ACM models. The code is available at https://github.com/LiuZX599/ACM-VGJD.git},
  archive      = {J_PR},
  author       = {Zexin Liu and Qi Li and Junyao Wang and Tingyuan Deng and Rifeng Zhou and Yufang Cai and Fenglin Liu},
  doi          = {10.1016/j.patcog.2025.112384},
  journal      = {Pattern Recognition},
  month        = {4},
  pages        = {112384},
  shortjournal = {Pattern Recognition},
  title        = {A variable gaussian kernel scale active contour model based on jeffreys divergence for ICT image segmentation},
  volume       = {172},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Robust spatio-temporal graph neural networks with sparse structure learning. <em>PR</em>, <em>172</em>, 112383. (<a href='https://doi.org/10.1016/j.patcog.2025.112383'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper focuses on the problem of spatio-temporal graph classification by introducing sparse structure learning to enhance its robustness and explainability. Spatio-temporal graph neural networks (STGNN) integrate spatial structure and temporal sequential features into GNN learning, resulting in promising performance in many applications. However, current STGNN models often fail to capture the discriminative sparse substructure and the smooth distribution of these samples. To this end, this paper introduces RostGNN, robust spatio-temporal graph neural networks, for achieving more discriminative graph representations. Concretely, RostGNN extracts the spatial and temporal features by performing gated recurrent units on the given time series data and calculating adjacent matrixes for graphs. Then, we impose the iterative hard-thresholding approach on the final association matrix to obtain a sparse graph. Meanwhile, we calculate a similarity matrix from the side information of samples to smooth the achieved data representations and use fully connected networks for graph classification. We finally applied RostGNN to brain graph classification in experiments on real-world datasets. The results demonstrate that RostGNN delivers robust and discriminative graph representations and performs better than compared methods, benefiting from the sparsity and manifold regularizers. Furthermore, RostGNN can potentially yield useful findings for data understanding.},
  archive      = {J_PR},
  author       = {Yupei Zhang and Yuxin Li and Shuhui Liu and Xuequn Shang},
  doi          = {10.1016/j.patcog.2025.112383},
  journal      = {Pattern Recognition},
  month        = {4},
  pages        = {112383},
  shortjournal = {Pattern Recognition},
  title        = {Robust spatio-temporal graph neural networks with sparse structure learning},
  volume       = {172},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). DiscDC: Unsupervised discriminative deep image clustering via confidence-driven self-labeling. <em>PR</em>, <em>172</em>, 112382. (<a href='https://doi.org/10.1016/j.patcog.2025.112382'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep clustering, as an important research topic in machine learning and data mining, has been widely applied in many real-world scenarios. However, existing deep clustering methods primarily rely on implicit optimization objectives such as contrastive learning or reconstruction, which do not explicitly enforce cluster-level discrimination. This limitation restricts their ability to achieve compact intra-cluster structures and distinct inter-cluster separations. To overcome this limitation, we propose a novel unsupervised discriminative deep clustering (discDC) method, which explicitly integrates cluster-level discrimination into the learning process. The proposed discDC framework projects data into a nonlinear latent space with compact and well-separated cluster representations. It explicitly optimizes clustering objectives by minimizing intra-cluster discrepancy and maximizing inter-cluster discrepancy. Additionally, to tackle the lack of label information in unsupervised scenarios, we introduce a confidence-driven self-labeling mechanism, which iteratively derives reliable pseudo-labels to enhance discriminative analysis. Extensive experiments on five benchmark datasets demonstrate the superiority of discDC over state-of-the-art deep clustering approaches.},
  archive      = {J_PR},
  author       = {Jinyu Cai and Wenzhong Guo and Yunhe Zhang and Jicong Fan},
  doi          = {10.1016/j.patcog.2025.112382},
  journal      = {Pattern Recognition},
  month        = {4},
  pages        = {112382},
  shortjournal = {Pattern Recognition},
  title        = {DiscDC: Unsupervised discriminative deep image clustering via confidence-driven self-labeling},
  volume       = {172},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). MCoCa: Towards fine-grained multimodal control in image captioning. <em>PR</em>, <em>172</em>, 112381. (<a href='https://doi.org/10.1016/j.patcog.2025.112381'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Controllable image captioning (CIC) models have traditionally focused on generating controlled descriptions using specific text styles. However, these approaches are limited as they rely solely on text control signals, which often fail to align with complex human intentions, such as selecting specific areas in images. To enhance multimodal interactivity, we propose to augment current CIC systems with diverse and joint visual-text controls. To achieve this, we first create a comprehensive Multimodal Controllable Image Captioning Corpus (MCoCa) dataset by leveraging language rewriting ability of GPT-3.5, containing 0.97M image-captions pairs along with 21 visual-text control signals. By training the visual and textual adapters equipped on the multimodal large language model with newly proposed instructional prompts on MCoCa, we observe emergent combinatory multimodal controllability and significant improvement in text controllability. We present exhaustive quantitative and qualitative results, benchmarking our trained model’s state-of-the-art zero-shot captioning performance on SentiCap and FlickrStyle10K in terms of both fidelity and controllability. For regional understanding ability of visual-controlled captioning, our method achieves obvious improvement compared with the baseline models.},
  archive      = {J_PR},
  author       = {Shanshan Zhao and Teng Wang and Jinrui Zhang and Xiangchen Wang and Feng Zheng},
  doi          = {10.1016/j.patcog.2025.112381},
  journal      = {Pattern Recognition},
  month        = {4},
  pages        = {112381},
  shortjournal = {Pattern Recognition},
  title        = {MCoCa: Towards fine-grained multimodal control in image captioning},
  volume       = {172},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Learning interpretable binary codes via semantic alignment for customized image retrieval. <em>PR</em>, <em>172</em>, 112380. (<a href='https://doi.org/10.1016/j.patcog.2025.112380'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The single modality hashing (SMH) has achieved impressive performance on image retrieval task in recent years. The only fly in the ointment is that most of the methods mainly measure the image similarity based on the high-level class labels. The retrieval needs in the real world are diverse in form of different subsets of the semantics (not only the category labels) presented in the query image. However, existing SMH methods fail to account for such customized image retrieval task that allows users to select visual semantics or their combinations present in the query and retrieve similar images based on such selected semantic descriptions. To address such practical issues, we propose a deep hashing to learn Interpretable Binary Codes (IBC), endowing the hashing bits with semantic interpretability rather than purely entangling the class information in the whole codes, i.e., aligning the criteria of binary space partition of each bit with a particular visual semantic concept. Specifically, binary encoding is a highly non-linear operation of dimension reduction, the semantic and spatial information of which has respectively been abstract and lost heavily. In light of the rich semantic interpretability and binary concept detection ability of convolutional filters, we innovatively transfer the semantic knowledge from filters to hashing bits by align the distributions of the binary codes and filter activations that capture the presence/absence of visual patterns in images. To further improve the semantics of filters/bits, the shared and learnable classification rules are introduced and optimized to disentangle the sparse composition between the category label and encoded semantics in filters/bits. With high interpretability, we can selectively combine bits corresponding to the target semantics during retrieval, thereby enabling flexible and customized similarity searches. Extensive experiments on several large-scale datasets covering general objects and scenes, single and multiple label scenarios, demonstrate the interpretability and functionalities of learned binary codes for the customized image retrieval tasks.},
  archive      = {J_PR},
  author       = {Shishi Qiao and Ruiping Wang and Xilin Chen},
  doi          = {10.1016/j.patcog.2025.112380},
  journal      = {Pattern Recognition},
  month        = {4},
  pages        = {112380},
  shortjournal = {Pattern Recognition},
  title        = {Learning interpretable binary codes via semantic alignment for customized image retrieval},
  volume       = {172},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). AFFusion: Atmospheric scattering enhancement and frequency integrated spatial-channel attention for infrared and visible image fusion. <em>PR</em>, <em>172</em>, 112379. (<a href='https://doi.org/10.1016/j.patcog.2025.112379'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Infrared and visible image fusion (IVIF) seeks to generate fused images that combine rich texture details with distinct thermal radiation features by integrating and leveraging complementary information from multiple sources. However, existing fusion methods frequently neglect the challenges posed by illumination degradation and inaccurate color contrast, which arise due to light energy loss and light scattering during atmospheric transmission. To address these limitations, this study introduces an innovative IVIF framework, termed AFFusion, which integrates an atmospheric scattering physical model with a frequency-domain feature component. By accurately predicting and estimating two key physical parameters-the transmission map and atmospheric light-within the scattering model, AFFusion harnesses atmospheric scattering principles to produce enhanced visible images, thereby mitigating the adverse effects of energy attenuation and scattering. Furthermore, to resolve artifacts and texture loss caused by traditional atmospheric scattering models, AFFusion incorporates Fourier transform in conjunction with spatial and channel attention mechanisms to selectively amplify amplitude and phase features in the frequency domain, thereby enhancing texture fidelity and detail representation within the fused images. Comprehensive experimental evaluations demonstrate that AFFusion surpasses state-of-the-art methods in both qualitative and quantitative performance metrics, while also providing robust support for high-level visual tasks. The implementation code is publicly accessible at https://github.com/cici0206/AFFusion .},
  archive      = {J_PR},
  author       = {Jiwei Hu and Chengcheng Song and Qiwen Jin and Kin-Man Lam},
  doi          = {10.1016/j.patcog.2025.112379},
  journal      = {Pattern Recognition},
  month        = {4},
  pages        = {112379},
  shortjournal = {Pattern Recognition},
  title        = {AFFusion: Atmospheric scattering enhancement and frequency integrated spatial-channel attention for infrared and visible image fusion},
  volume       = {172},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Vision-by-prompt: Context-aware dual prompts for composed video retrieval. <em>PR</em>, <em>172</em>, 112378. (<a href='https://doi.org/10.1016/j.patcog.2025.112378'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Composed video retrieval (CoVR) is a challenging task of retrieving relevant videos in a corpus by using a query that integrates both a relative change text and a reference video. Most existing CoVR models simply rely on the late-fusion strategy to combine visual and change text. Furthermore, various methods have been proposed to generate pseudo-word tokens from the reference video, which are then integrated into the relative change text for CoVR. However, these pseudo-word-based techniques exhibit limitations when the target video involves complex changes from the reference video, e.g. , object removal. In this work, we propose a novel CoVR framework that learns context information via context-aware dual prompts for relative change text to achieve effective composed video retrieval. The dual prompts cater to two aspects: 1) Global descriptive prompts generated from the pretrained V-L models, e.g. , BLIP-2, to get concise textual representations of the reference video. 2) Local target prompts to learn the target representations that the change text pays attention to. By connecting these prompts with relative change text, one can easily use existing text-to-video retrieval models to enhance CoVR performance. Our proposed framework can be flexibly used for both composed video retrieval (CoVR) and composed image retrieval (CoIR) tasks. Moreover, we take a pioneering approach by adopting the CoVR model to achieve zero-shot CoIR for remote sensing. Experiments on four datasets show that our approach achieves state-of-the-art performance in both CoVR and zero-shot CoIR tasks, with improvements of as high as around 3.5 % in terms of recall@K=1 score.},
  archive      = {J_PR},
  author       = {Hao Wang and Fang Liu and Licheng Jiao and Jiahao Wang and Shuo Li and Lingling Li and Puhua Chen and Xu Liu},
  doi          = {10.1016/j.patcog.2025.112378},
  journal      = {Pattern Recognition},
  month        = {4},
  pages        = {112378},
  shortjournal = {Pattern Recognition},
  title        = {Vision-by-prompt: Context-aware dual prompts for composed video retrieval},
  volume       = {172},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Efficient and compact tensor wheel decomposition for tensor completion. <em>PR</em>, <em>172</em>, 112377. (<a href='https://doi.org/10.1016/j.patcog.2025.112377'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Tensor wheel (TW) decomposition has recently emerged as a powerful technique for achieving state-of-the-art recovery performance in tensor completion tasks. However, its widespread application has been hindered by issues related to rank sensitivity and high computational cost. To address these limitations, we introduce an efficient and compact TW decomposition method for low-rank tensor completion. Specifically, we demonstrate that the model complexity of TW decomposition is controlled simultaneously by two elements, namely, the explicit TW rank and implicit sparsity in the core tensor. Therefore, low-rank and sparsity regularization are introduced to ring factors and core factor, respectively, to achieve a compact TW decomposition. Furthermore, to alleviate the computational bottleneck of TW decomposition, we propose a novel generalized inverse operation, which reduces the computational complexity of vanilla TW decomposition from O ( I N R 2 N ) to O ( I N R N ) . Subsequently, we develop an efficient alternating direction method of multipliers (ADMM) algorithm with theoretical convergence guarantees. Numerical tensor completion experiments on color images, multispectral images, and color videos demonstrate that the proposed method achieves superior performance while significantly reducing runtime compared to state-of-the-art methods. The code is available at: https://github.com/justicbro/TWLRS .},
  archive      = {J_PR},
  author       = {Peilin Yang and Yuning Qiu and Zhenhao Huang and Guoxu Zhou and Qibin Zhao},
  doi          = {10.1016/j.patcog.2025.112377},
  journal      = {Pattern Recognition},
  month        = {4},
  pages        = {112377},
  shortjournal = {Pattern Recognition},
  title        = {Efficient and compact tensor wheel decomposition for tensor completion},
  volume       = {172},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Learning to complement with multiple humans. <em>PR</em>, <em>172</em>, 112376. (<a href='https://doi.org/10.1016/j.patcog.2025.112376'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Solution for addressing real-world image classification challenges. Human-AI collaborative classification (HAI-CC) aims to synergise the efficiency of machine learning classifiers and the reliability of human experts to support decision making. Learning to defer (L2D) has been one of the promising HAI-CC approaches, where the system assesses a sample and decides to defer to one of human experts when it is not confident. Despite recent progress, existing L2D methods rely on the strong assumption of ground truth label availability for training, while in practice, most datasets often contain multiple noisy annotations per data sample without well-curated ground truth labels. In addition, current L2D methods either consider the setting of a single human expert or defer the decision to one human expert, even though there may be multiple experts available, resulting in a suboptimal utilisation of available resources. Furthermore, current HAI-CC evaluation frameworks often overlook processing costs, making it difficult to assess the trade-off between computational efficiency and performance when benchmarking different methods. To address these gaps, this paper introduces LECOMH – a new HAI-CC method that learns from noisy labels without depending on clean labels for training, simultaneously maximising collaborative accuracy with either one or multiple human experts, while minimising the cost of human collaboration. The paper also introduces benchmarks featuring multiple noisy labels per data sample for both training and testing to evaluate HAI-CC methods. Through quantitative comparisons on these benchmarks, LECOMH consistently outperforms HAI-CC methods and baselines, including human experts alone, multi-rater learning and noisy-label learning methods across both synthetic and real-world datasets.},
  archive      = {J_PR},
  author       = {Zheng Zhang and Cuong Nguyen and Kevin Wells and Thanh-Toan Do and Gustavo Carneiro},
  doi          = {10.1016/j.patcog.2025.112376},
  journal      = {Pattern Recognition},
  month        = {4},
  pages        = {112376},
  shortjournal = {Pattern Recognition},
  title        = {Learning to complement with multiple humans},
  volume       = {172},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Noise-aware state-space method for underwater object detection. <em>PR</em>, <em>172</em>, 112375. (<a href='https://doi.org/10.1016/j.patcog.2025.112375'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Underwater Object Detection (UOD) faces significant challenges due to complex degradation factors, such as color shifts caused by light absorption and scattering, spatially varying noise induced by plankton and sea snow, and motion blur resulting from dynamic water currents. Among existing methods, Convolutional Neural Networks (CNNs) are limited by fixed receptive fields, making it difficult to model long-range noise patterns; while Transformers excel at modeling global dependencies, they suffer from high computational complexity and weak capability in restoring fine-grained local features. Neither can effectively address the demands of detecting underwater-specific noise and small objects. To tackle these issues, we propose UOD-Mamba, a state space model (SSM)-based framework for underwater object detection. At its core is the Noise-Aware Dual-path Mamba (NADM) module, which integrates a global-local dual-path fusion strategy to enable both long-range noise modeling and local feature enhancement. The global path balances noise in input features through the Noise-Balanced Preprocessing Module (NBPM) and leverages Mamba’s long-range modeling capability to extract global noise patterns; the local path fuses the Underwater Enhanced Multi-scale Attention Module (UEMA) with CSP convolution to model edge and detail features at a fine-grained level, thereby compensating for the loss of local information. By explicitly learning the distribution characteristics of underwater noise and capturing the differences between noise and target features, the framework enhances detection robustness in noisy environments. Experimental validation on the DUO and RUOD datasets demonstrates that UOD-Mamba sets a new state-of-the-art in detection performance. It also exhibits advantages in explicit modeling of diverse noises, preservation of local details, and computational efficiency across multi-noise scenarios, enabling effective handling of complex underwater interference environments.},
  archive      = {J_PR},
  author       = {Jingchun Zhou and Xudong Wang and Mingjie Li and Zongxin He and Wentian Xin and Xiuguo Zhang},
  doi          = {10.1016/j.patcog.2025.112375},
  journal      = {Pattern Recognition},
  month        = {4},
  pages        = {112375},
  shortjournal = {Pattern Recognition},
  title        = {Noise-aware state-space method for underwater object detection},
  volume       = {172},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Cross-domain-aware deep unfolding transformer for hyperspectral image super-resolution. <em>PR</em>, <em>172</em>, 112374. (<a href='https://doi.org/10.1016/j.patcog.2025.112374'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Fusing low-spatial-resolution hyperspectral images with high-spatial-resolution (HSR) multispectral images is pivotal for generating HSR hyperspectral images (HSR-HSIs). While current deep unrolling-based multi-stage frameworks have shown notable advancements due to their robustness and interpretability, they still exhibit limitations in adequately harnessing the HSI prior knowledge. This deficiency is principally attributed to three factors: (1) prior knowledge learned from training samples often overlooks target-specific characteristics; (2) insufficient feature representation within and across stages; and (3) insufficient modeling of spatial–spectral dependencies. To address these issues, we propose a novel Cross-domain-aware Transformer (CaFormer). Specifically, a cross-domain aware attention mechanism is investigated to capture intrinsic joint spatial–spectral dependencies through unified cross-domain feature representation. The attention mechanism models HSI eigenfeatures to derive spatial and spectral representations while preserving their mutual correlations. Furthermore, we introduce a Fourier Domain Perception Block to enhance structural and semantic representations by exploiting amplitude and phase components in the frequency domain, thereby strengthening feature aggregation across stages. To further improve adaptability while preserving the interpretability of deep unrolling networks, CaFormer employs a dual-stage prior learning strategy, transferring prior knowledge learned from general training data to the specific observed scene. Our experimental evaluations on four public datasets and Worldview-2 satellite images confirm that our proposed method outperformed eleven state-of-the-art methods. The code is available at https://github.com/Caoxuheng/HIFtool .},
  archive      = {J_PR},
  author       = {Xuheng Cao and Xuquan Wang and Xiong Dun and Yusheng Lian and Xinbin Cheng and Xiaopeng Hao},
  doi          = {10.1016/j.patcog.2025.112374},
  journal      = {Pattern Recognition},
  month        = {4},
  pages        = {112374},
  shortjournal = {Pattern Recognition},
  title        = {Cross-domain-aware deep unfolding transformer for hyperspectral image super-resolution},
  volume       = {172},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Dual-teacher self-distillation registration for multi-modality medical image fusion. <em>PR</em>, <em>172</em>, 112373. (<a href='https://doi.org/10.1016/j.patcog.2025.112373'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Misaligned multimodal medical images pose challenges to the fusion task, resulting in structural distortions and edge artifacts in the fusion results. Existing registration networks primarily consider single-scale deformation fields at each stage, thereby neglecting long-range connections between non-adjacent stages. Moreover, in the fusion task, due to the quadratic computational complexity faced by Transformers during feature extraction, they are unable to effectively capture long-range correlated features. To address these problems, we propose an image registration and fusion method called DTMFusion. DTMFusion comprises two main networks: a Dual-Teacher Self-Distillation Registration (DTSDR) network and a Mamba-Conv-based Fusion (MCF) network. The registration network employs a pyramid progressive architecture to generate independent deformation fields at each layer. We introduce a dual-teacher self-distillation scheme that leverages past learning history and the current network structure as teacher guidance to constrain the generated deformation fields. For the fusion network, we introduced Mamba to address the quadratic complexity problem of Transformers. Specifically, the fusion network involves two key components: the Shallow Fusion Module (SFM) and the Cross-Modality Fusion Module (CFM). The SFM achieves lightweight cross-modality interaction through channel exchange, while the CFM leverages inherent cross-modality relationships to enhance the representation capability of fusion results. Through the collaborative effort of these components, the network can effectively integrate cross-modality complementary information and maintain appropriate apparent strength from a global perspective. Extensive experimental analysis demonstrates the superiority of this method in fusing misaligned medical images.},
  archive      = {J_PR},
  author       = {Aimei Dong and Jingyuan Xu and Long Wang},
  doi          = {10.1016/j.patcog.2025.112373},
  journal      = {Pattern Recognition},
  month        = {4},
  pages        = {112373},
  shortjournal = {Pattern Recognition},
  title        = {Dual-teacher self-distillation registration for multi-modality medical image fusion},
  volume       = {172},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). FSF-net: Enhance 4D occupancy forecasting with coarse BEV scene flow for autonomous driving. <em>PR</em>, <em>172</em>, 112372. (<a href='https://doi.org/10.1016/j.patcog.2025.112372'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {4D occupancy forecasting is one of the important techniques for autonomous driving, which can avoid potential risk in the complex traffic scenes. Scene flow is a crucial element to describe 4D occupancy map tendency. However, an accurate scene flow is difficult to predict in the real scene. In this paper, we find that BEV scene flow can approximately represent 3D scene flow in most traffic scenes. And coarse BEV scene flow is easy to generate. Under this thought, we propose 4D occupancy forecasting method FSF-Net based on coarse BEV scene flow. At first, we develop a general occupancy forecasting architecture based on coarse BEV scene flow. Then, to further enhance 4D occupancy feature representation ability, we propose a vector quantized based Mamba (VQ-Mamba) network to mine spatial-temporal structural scene feature. After that, to effectively fuse coarse occupancy maps forecasted from BEV scene flow and latent features, we design a U-Net based quality fusion (UQF) network to generate the fine-grained forecasting result. Extensive experiments are conducted on public Occ3D dataset. FSF-Net has achieved IoU and mIoU 9.56 % and 10.87 % higher than state-of-the-art method. Hence, we believe that proposed FSF-Net benefits to the safety of autonomous driving.},
  archive      = {J_PR},
  author       = {Erxin Guo and Pei An and You Yang and Qiong Liu and An-An Liu},
  doi          = {10.1016/j.patcog.2025.112372},
  journal      = {Pattern Recognition},
  month        = {4},
  pages        = {112372},
  shortjournal = {Pattern Recognition},
  title        = {FSF-net: Enhance 4D occupancy forecasting with coarse BEV scene flow for autonomous driving},
  volume       = {172},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). A novel image enhancement method based on image decomposition and deep neural networks. <em>PR</em>, <em>172</em>, 112371. (<a href='https://doi.org/10.1016/j.patcog.2025.112371'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Image decomposition and deep learning are active research areas in computer vision tasks, such as cartoon texture decomposition, low-light image enhancement, rain streak removal, image recovery, etc. This paper proposes a novel low-light image enhancement method by joining image decomposition and deep neural network techniques. We introduce a new image decomposition-based optimization model by incorporating the Tikhonov regularization and multi-scale convolutional sparse coding (MSCSC) to enhance image visual effects. To enhance robustness performance, we introduce a noise-free image decomposition error term to effectively suppress noise in low-light images. To effectively implement the proposed method, we incorporate a deep-unfolding neural network and an adaptive denoiser into the alternating direction method of multipliers (ADMM) framework. Since the deep unfolding network can effectively simulate the optimization algorithm process, the interpretability of the network model is increased. Moreover, through end-to-end training, we can automatically estimate the two priors and parameter settings from training samples. Finally, qualitative and quantitative experiments demonstrate that the proposed method outperforms state-of-the-art image enhancement methods in terms of visual quality and robustness. The source code is available at https://github.com/cassiopeia-yxx/LLIE .},
  archive      = {J_PR},
  author       = {Yao Xiao and Youshen Xia},
  doi          = {10.1016/j.patcog.2025.112371},
  journal      = {Pattern Recognition},
  month        = {4},
  pages        = {112371},
  shortjournal = {Pattern Recognition},
  title        = {A novel image enhancement method based on image decomposition and deep neural networks},
  volume       = {172},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Nonuniform low-light image enhancement via noise-aware decomposition and adaptive correction. <em>PR</em>, <em>172</em>, 112370. (<a href='https://doi.org/10.1016/j.patcog.2025.112370'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Images captured under low-illumination conditions often exhibit low brightness with nonuniform distribution, low contrast, and noise, negatively affecting the human visual experience and the accuracy of image-based computer vision tasks. Enhancing nonuniform low-light images is challenging considering the requirement of simultaneously reducing noise, enhancing low-light regions, and suppressing high-light regions. To address these challenges, we innovatively propose a noise-aware decomposition and adaptive correction method (NDAC) to enhance the nonuniform low-light images without the need for paired high-quality training data. Specifically, a noise-aware image decomposition network (NIDNet) is first presented to decompose the input images into illumination, reflection, and noise components, while suppressing the noise in the reflection component through a variable gradient operator and estimating the noise component. Besides, we devise a novel nonlinear adaptive brightness mapping function (NABM), whose parameters are optimized via a designed automatic light enhancement network (ALENet) to brighten the illumination component. The enhancements are obtained by fusing the noiseless reflection component with the brightened illumination component. Extensive experiments on both public and industrial datasets demonstrate that the proposed NDAC method outperforms state-of-the-art approaches in both qualitative and quantitative evaluations.},
  archive      = {J_PR},
  author       = {Jiancai Huang and Zhaohui Jiang and Xingjian Liu and Yap-Peng Tan and Weihua Gui},
  doi          = {10.1016/j.patcog.2025.112370},
  journal      = {Pattern Recognition},
  month        = {4},
  pages        = {112370},
  shortjournal = {Pattern Recognition},
  title        = {Nonuniform low-light image enhancement via noise-aware decomposition and adaptive correction},
  volume       = {172},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Fourier-enhanced semi-supervised proxy learning for ultra-fine-grained novel class discovery. <em>PR</em>, <em>172</em>, 112369. (<a href='https://doi.org/10.1016/j.patcog.2025.112369'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Operating in open-world environments requires recognizing known categories and discovering new ones, especially in ultra-fine-grained task, where distinguishing similar categories is challenging. The task of Ultra-Fine-Grained Novel Class Discovery (UFG-NCD) intensifies this challenge by requiring systems to identify previously unseen classes within unlabeled data. However, existing UFG-NCD methods fall short in extracting critical visual cues and efficiently transferring knowledge from known to novel categories. To overcome these limitations, this paper proposes Fourier-Enhanced Semi-supervised Proxy Learning (FESPL), a novel framework for UFG-NCD. FESPL incorporates a Fourier amplitude guided block that leverages frequency domain analysis to capture high-frequency details often missed by traditional approaches, enhancing ultra-fine-grained discrimination. Additionally, the semi-supervised proxy learning strategy maximizes information extraction from limited labeled data and promotes robust generalization across known and unseen categories. Our approach achieves substantial improvements in both novel category discovery and known category classification on seven popular UFG-NCD datasets, with average performance gains of 10.41 % in the accuracy of the old class and 4.27 % in the accuracy of the new class in task-agnostic evaluation, while with average performance gains of 4.40 % in clustering accuracy on the unlabeled training data in task-aware evaluation.},
  archive      = {J_PR},
  author       = {Qiupu Chen and Hongkui Jiang and Lin Jiao and Zhou Li and Taosheng Xu and Xue Wang and Rujing Wang},
  doi          = {10.1016/j.patcog.2025.112369},
  journal      = {Pattern Recognition},
  month        = {4},
  pages        = {112369},
  shortjournal = {Pattern Recognition},
  title        = {Fourier-enhanced semi-supervised proxy learning for ultra-fine-grained novel class discovery},
  volume       = {172},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). An adaptive weighted active contour based HRNet for underwater image segmentation. <em>PR</em>, <em>172</em>, 112368. (<a href='https://doi.org/10.1016/j.patcog.2025.112368'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Acquiring optimal results in underwater environments remains challenging due to light absorption, scattering, and suspended particles. Furthermore, the low-resolution outputs from traditional semantic segmentation often result in spatial information loss and blurred segmentation boundaries. To address these issues, we first propose a low-level image enhancement preprocessing module as an independent preliminary stage to improve underwater image quality, thereby enhancing subsequent high-level semantic segmentation performance. Second, leveraging the region-based active contour model-which is independent of image gradients and adept at handling complex contour topology changes-we design a novel level set function to serve as the level set in the geometric active contour model. While this new level set exhibits formal similarity to classical level sets in representing binary segmentation contours, its formulation is derived from network prediction outputs. Third, we construct an adaptive weighted active contour energy function as a loss function within HRNet for multi-class segmentation. This loss function preserves geometric information while penalizing deviations between network-predicted probabilities and ground truth, effectively mitigating spatial information loss and optimizing boundary. Comparative experiments demonstrate that our model outperforms classical methods on objective metrics including mIoU and mPA.},
  archive      = {J_PR},
  author       = {Bo Chen and Jing Ji and Junwei Li and Xiaoli Sun and Feng Gong},
  doi          = {10.1016/j.patcog.2025.112368},
  journal      = {Pattern Recognition},
  month        = {4},
  pages        = {112368},
  shortjournal = {Pattern Recognition},
  title        = {An adaptive weighted active contour based HRNet for underwater image segmentation},
  volume       = {172},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). BORT2: Bi-level optimization for robust target training in multi-source domain adaptation. <em>PR</em>, <em>172</em>, 112367. (<a href='https://doi.org/10.1016/j.patcog.2025.112367'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Both conventional and source-free multi-source domain adaptation (MSDA) tasks often face bias toward source domains, because more numerous labeled data in these domains, compared with a single unlabeled target domain, can dominate the training process. To alleviate the bias, target adaptation techniques train a target model on the pseudo-labeled target domain data only, with the source-domain-biased models used as the labeling function for pseudo-label generation. However, the pseudo labels may contain noise and harm performance when directly used for supervision. To tackle label noise, we introduce a novel Bi-level Optimization for Robust Target Training (BORT 2 ) scheme. BORT 2 trains a noise-robust target model on pseudo-labeled target data only and meanwhile updates the labeling function (i.e., the source-domain-biased models) to improve pseudo-label quality. Specifically, the target model is a stochastic network designed to be robust to label noise. Such a stochastic network exploits a Gaussian distribution to model the feature of each target instance and deploys an entropy maximization regularizer to the Gaussian to quantify the uncertainty of each pseudo-label, where the uncertainty is utilized to mitigate the negative effects of label noise. In addition, BORT 2 leverages the entropy to update the labeling function for better pseudo-label quality. Updating both the labeling function and the stochastic network involves a nested bi-level optimization problem, addressed using implicit differentiation. Extensive experiments demonstrate that BORT 2 achieves state-of-the-art performance for both conventional and source-free MSDA, as verified on Office-Home, Office-Caltech, PACS, Digit-Five, and the large-scale DomainNet datasets.},
  archive      = {J_PR},
  author       = {Zhongying Deng and Da Li and Xiaojiang Peng and Yi-Zhe Song and Tao Xiang},
  doi          = {10.1016/j.patcog.2025.112367},
  journal      = {Pattern Recognition},
  month        = {4},
  pages        = {112367},
  shortjournal = {Pattern Recognition},
  title        = {BORT2: Bi-level optimization for robust target training in multi-source domain adaptation},
  volume       = {172},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Ultra-efficient 3D shape reconstruction: Line-coded absolute phase unwrapping algorithm. <em>PR</em>, <em>172</em>, 112366. (<a href='https://doi.org/10.1016/j.patcog.2025.112366'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Absolute phase unwrapping-based fringe projection profilometry (APU-FPP) has the advantages of pixel-wise calculation, high precision, and full-field sensing of 3D shape information. To the best of our knowledge, existing APU-FPP methods have a general contradiction between accuracy and efficiency because of projecting extra auxiliary coded fringes (ACFs). In this paper, a line-coded absolute phase unwrapping (LCAPU) algorithm is presented for absolute 3D shape reconstruction of the scene with non-uniform reflectivity and complex surfaces. Firstly, a sequence of single-pixel lines is successively embedded into two sets of 3-step phase-shifting patterns to mark fringe periods, which can thoroughly avoid extra ACFs to disrupt the coherence of adjacent morphological information. Secondly, two line-coded phase-shifting patterns with the same phase shift are used to recognize the corresponding coded lines containing the fringe order cue, which can be simultaneously used to guide fringe mutual compensation, thereby extracting a high-quality phase. Finally, according to the pixel positions and the fringe indices of the decoded lines, a multi-layer decoding (MLD) algorithm is developed to iteratively generate a fringe order map, which can adapt to the randomness of morphological changes. Compared to other methods, the proposed LCAPU can not only perform a one-shot 3D shape reconstruction with a single image acquisition, but also automatically correct phase errors, balancing ultra-efficiency and high accuracy. Experimental results demonstrate the superior performance and the practical application potential in dynamic complex scenes.},
  archive      = {J_PR},
  author       = {Haihua An and Yiping Cao and Hechen Zhang},
  doi          = {10.1016/j.patcog.2025.112366},
  journal      = {Pattern Recognition},
  month        = {4},
  pages        = {112366},
  shortjournal = {Pattern Recognition},
  title        = {Ultra-efficient 3D shape reconstruction: Line-coded absolute phase unwrapping algorithm},
  volume       = {172},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Learning label-specific features for multi-dimensional classification. <em>PR</em>, <em>172</em>, 112365. (<a href='https://doi.org/10.1016/j.patcog.2025.112365'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In multi-dimensional classification (MDC), instances are associated with multiple class variables that are assumed in the output space, and each class variable corresponds to one heterogeneous class space and characterizes the objects’ semantics from one dimension. Learning from MDC examples poses challenges due to the heterogeneity of class spaces, since the outputs from different class spaces are not directly comparable. Moreover, existing approaches often use identical data representation for all labels in a class, which may lead to suboptimal results as each label might be determined by its own specific characteristics. Critically, the inherent incomparability of raw heterogeneous labels prevents existing methods from effectively capturing label correlations, which are essential for guiding feature learning. In this paper, we propose a novel algorithm named LEAD, i.e., learning Label-spEcific feAtures for multi-Dimensional classification. LEAD first resolves label heterogeneity by transforming the original output space into a unified encoded label space through one-hot label encoding. This critical alignment enables explicit extraction of label correlations from the encoded space. To enhance the reliability of the estimation of label correlations, LEAD then leverages feature-space manifold structures via locally linear embedding, propagating labeling information across similar instances to counteract sparsity. Finally, LEAD jointly learns label-specific feature representations and constructs the classifier through sparse learning while incorporating label correlations. Experimental comparisons on fifteen datasets demonstrate that our proposed method outperforms state-of-the-art multi-dimensional classification methods. The code is available at https://github.com/ZhangZan-source/LEAD .},
  archive      = {J_PR},
  author       = {Zan Zhang and Jialin Zhou and Jialu Yao and Lin Liu and Jiuyong Li and Lei Li and Xindong Wu},
  doi          = {10.1016/j.patcog.2025.112365},
  journal      = {Pattern Recognition},
  month        = {4},
  pages        = {112365},
  shortjournal = {Pattern Recognition},
  title        = {Learning label-specific features for multi-dimensional classification},
  volume       = {172},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). TSAR: A two-stage approach to motion artifact reduction in OCTA images. <em>PR</em>, <em>172</em>, 112364. (<a href='https://doi.org/10.1016/j.patcog.2025.112364'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Optical Coherence Tomography Angiography (OCTA) is an innovative and non-invasive imaging technique that leverages motion contrast imaging to generate angiographic images from high-resolution volumetric blood flow data rapidly. However, OCTA imaging is vulnerable to various artifacts induced by eye movements, including displacement artifacts, duplicated scanning artifacts, and white line artifacts. Previous methods that attempted to mitigate eye motion artifacts necessitated costly hardware upgrades. However, despite the availability of advanced eye-tracking hardware and software correction in commercial machines, motion artifacts persist in real-world usage. Recently developed cost-effective learning-based methods only focus on the removal of white line artifacts while neglecting the displacement artifacts and duplicated scanning artifacts. To address this challenge, we propose a comprehensive framework, TSAR, to remove three types of eye motion artifacts in OCTA images. In the first stage, we leverage the intrinsic axial and directional attributes of these artifacts in the first phase to develop an innovative hierarchical transformer network. This network is designed to capture global-wise, local-wise, and vertical-wise features effectively while also removing displacement and duplicate scanning artifacts. Afterward, we leverage the contextual information and develop a residual conditional diffusion model (RCDM) to remove the white line artifacts. By applying our TSAR to the degraded OCTA images, we aim to eliminate all three types of motion artifacts. We evaluate the superior performance of our proposed methodology in artifact removal and image quality enhancement compared to other methods by conducting experiments on both synthetic and real-world OCTA images. The code is available at https://github.com/btma48/TSAR},
  archive      = {J_PR},
  author       = {Benteng Ma and Xiaomeng Li and Xu Lin and Xiaoyu Bai and Dongping Shao and Chubin Ou and Lin An and Jia Qin and Kwang-Ting Cheng},
  doi          = {10.1016/j.patcog.2025.112364},
  journal      = {Pattern Recognition},
  month        = {4},
  pages        = {112364},
  shortjournal = {Pattern Recognition},
  title        = {TSAR: A two-stage approach to motion artifact reduction in OCTA images},
  volume       = {172},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Quaternionic reweighted amplitude flow for phase retrieval in image reconstruction. <em>PR</em>, <em>172</em>, 112363. (<a href='https://doi.org/10.1016/j.patcog.2025.112363'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Quaternionic signal processing provides powerful tools for efficiently managing color signals by preserving the intrinsic correlations among signal dimensions through quaternion algebra. In this paper, we address the quaternionic phase retrieval problem by systematically developing novel algorithms based on an amplitude-based model. Specifically, we propose the Quaternionic Reweighted Amplitude Flow (QRAF) algorithm, which is further enhanced by three of its variants: incremental, accelerated, and adapted QRAF algorithms. In addition, we introduce the Quaternionic Perturbed Amplitude Flow (QPAF) algorithm, which has linear convergence. Extensive numerical experiments on both synthetic data and real images demonstrate that our proposed methods significantly improve recovery performance and computational efficiency compared to state-of-the-art approaches.},
  archive      = {J_PR},
  author       = {Ren Hu and Pan Lian},
  doi          = {10.1016/j.patcog.2025.112363},
  journal      = {Pattern Recognition},
  month        = {4},
  pages        = {112363},
  shortjournal = {Pattern Recognition},
  title        = {Quaternionic reweighted amplitude flow for phase retrieval in image reconstruction},
  volume       = {172},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Robust scene text understanding with OCR token and word alignment for text-VQA and text-caption. <em>PR</em>, <em>172</em>, 112362. (<a href='https://doi.org/10.1016/j.patcog.2025.112362'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To solve vision-language tasks incorporating scene text, such as Text-VQA and Text-Caption, recognizing and understanding scene text within image is the first priority. However, the scene text recognized by Optical Character Recognition (OCR) systems often includes spelling errors, such as “pepsi” being recognized as “peosi”. These OCR errors are one of the major challenges for Text-VQA and Text-Caption systems. To address this, we propose a novel multi-modal OCR Token and Word Alignment (TWA) method to alleviate OCR errors in these tasks. First, we artificially create the misspelled OCR tokens and render them onto the RGB images, which can effectively simulates OCR errors. Second, we propose an OCR token-word contrastive learning task to pre-train OCR token representation, making the system more robust to OCR errors. Finally, we introduce a vocabulary predictor with character-level semantic matching, which enables the model to recover the correct word from the vocabulary even with misspelled OCR tokens. A variety of experimental evaluations demonstrate that our method outperforms the state-of-the-art methods on both Text-VQA and Text-Caption datasets.},
  archive      = {J_PR},
  author       = {Zan-Xia Jin and Pinle Qin and Suzhen Lin and Jia Qin and Shuangjiao Zhai and Jianchao Zeng and Xu-Cheng Yin},
  doi          = {10.1016/j.patcog.2025.112362},
  journal      = {Pattern Recognition},
  month        = {4},
  pages        = {112362},
  shortjournal = {Pattern Recognition},
  title        = {Robust scene text understanding with OCR token and word alignment for text-VQA and text-caption},
  volume       = {172},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Prior tokenization-based interactive segmentation with vision transformers. <em>PR</em>, <em>172</em>, 112361. (<a href='https://doi.org/10.1016/j.patcog.2025.112361'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Effectively leveraging the provided priors is crucial for interactive segmentation. Existing approaches typically encode clicks via distance-based maps, which are then concatenated or added to the original image as network input. However, these methods do not fully exploit the semantic information embedded in the provided priors, leading to confusion in the feature distribution of different targets and reducing the segmentation quality. To address this issue, we propose a prior tokenization-based interactive segmentation method that uses simple Vision Transformers. By extending the original image tokens with prior tokens, each token represents the semantic features of the foreground and background related to the priors. These tokens participate in the self-attention operation alongside regular image tokens, gradually extracting semantic features from the image tokens to the prior tokens. In addition, we introduce a discriminative loss function to enforce inter-class separation and intra-class compactness of the prior tokens. Subsequently, we employ a cross-attention mechanism to couple the prior tokens with the regular image block token features, ensuring that the features extracted by the network are aligned with the user’s intent. Finally, we use the register method to suppress artifacts and enhance the segmentation performance further. Extensive experiments demonstrate that our method achieves superior interaction efficiency, robustness, and generalization ability across various medical image segmentation benchmarks. The source codes are available at https://github.com/dzyha2011/PT-SimpleClick},
  archive      = {J_PR},
  author       = {Zongyuan Ding and Boyu Wang and Hongyuan Wang and Tao Wang},
  doi          = {10.1016/j.patcog.2025.112361},
  journal      = {Pattern Recognition},
  month        = {4},
  pages        = {112361},
  shortjournal = {Pattern Recognition},
  title        = {Prior tokenization-based interactive segmentation with vision transformers},
  volume       = {172},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). CTNet: Color transformation network for low-light image enhancement. <em>PR</em>, <em>172</em>, 112360. (<a href='https://doi.org/10.1016/j.patcog.2025.112360'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Low-light images are often plagued by low visibility, poor contrast, and high noise levels, which significantly impair both subjective visual quality and the performance of downstream tasks. Existing enhancement methods typically struggle with color-related degradations such as color casting, artifacts, and distortion. To address these challenges, we propose an end-to-end Color Transformation Network for low-light image enhancement, with a specific focus on improving color restoration. By leveraging the complementary strengths of the HSV and RGB color spaces in capturing color attributes, our approach enables effective interaction between these color spaces at the feature level. The HSV branch simultaneously enhances the V component while extracting features from the H and S components, thereby providing a more comprehensive set of cues for color recovery. To facilitate interaction, we design a learnable Color Transformation Block that bridges the HSV and RGB feature domains, effectively simulating the HSV-to-RGB conversion. Furthermore, a Cross-Integration Block, employing an attention-based cross-guidance mechanism, enables bi-directional information flow between the two color spaces. Extensive experiments on both real and synthetic datasets demonstrate that our method achieves superior performance, surpassing existing approaches both qualitatively and quantitatively. The project is available at https://github.com/1013990424/CTNet .},
  archive      = {J_PR},
  author       = {Lidong Xie and Runmin Cong and Ju Dai and Wenhan Yang and Junjun Pan and Hao Wu},
  doi          = {10.1016/j.patcog.2025.112360},
  journal      = {Pattern Recognition},
  month        = {4},
  pages        = {112360},
  shortjournal = {Pattern Recognition},
  title        = {CTNet: Color transformation network for low-light image enhancement},
  volume       = {172},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Joint adversarial attack: An effective approach to evaluate robustness of 3D object tracking. <em>PR</em>, <em>172</em>, 112359. (<a href='https://doi.org/10.1016/j.patcog.2025.112359'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep neural networks (DNNs) have widely been used in 3D object tracking, thanks to its superior capabilities to learn from geometric training samples and locate tracking targets. Although the DNN based trackers show vulnerability to adversarial examples, their robustness in real-world scenarios with potentially complex data defects has rarely been studied. To this end, a joint adversarial attack method against 3D object tracking is proposed, which simulates defects of the point cloud data in the form of point filtration and perturbation simultaneously. Specifically, a voxel-based point filtration module is designed to filter points of the tracking template, which is described by the voxel-wise binary distribution regarding the density of the point cloud. Furthermore, a voxel-based point perturbation module adds voxel-wise perturbations to the filtered template, whose direction is constrained by local geometrical information of the template. Experiments conducted on popular 3D trackers demonstrate that the proposed joint attack have decreased the success and precision of existing 3D trackers by 30.2% and 35.4% respectively in average, which made an improvement of 30.5% over existing attack methods.},
  archive      = {J_PR},
  author       = {Riran Cheng and Xupeng Wang and Ferdous Sohel and Hang Lei},
  doi          = {10.1016/j.patcog.2025.112359},
  journal      = {Pattern Recognition},
  month        = {4},
  pages        = {112359},
  shortjournal = {Pattern Recognition},
  title        = {Joint adversarial attack: An effective approach to evaluate robustness of 3D object tracking},
  volume       = {172},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Cross-channel blur invariants of color and multispectral images. <em>PR</em>, <em>172</em>, 112358. (<a href='https://doi.org/10.1016/j.patcog.2025.112358'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The paper deals with the recognition of blurred color/multispectral images directly without any deblurring. We present a general theory of invariants of multispectral images with respect to blur. The paper is a significant non-trivial extension of the recent theory of blur invariants of graylevel images. The main original contribution of the paper lies in introducing cross-channel blur invariants in Fourier domain. We also developed an algorithm for their stable and fast calculation in the moment domain. Moreover, the cross-channel invariants can be found for blurs for which single-channel invariants do not exist. The experiments on simulated and real data demonstrate that incorporating the new cross-channel invariants significantly improves the recognition power and surpasses other existing approaches. The outlook for a possible implementation of the blur invariants into neural networks is briefly sketched in the conclusion.},
  archive      = {J_PR},
  author       = {Václav Košík and Jan Flusser and Filip Šroubek},
  doi          = {10.1016/j.patcog.2025.112358},
  journal      = {Pattern Recognition},
  month        = {4},
  pages        = {112358},
  shortjournal = {Pattern Recognition},
  title        = {Cross-channel blur invariants of color and multispectral images},
  volume       = {172},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). UniForCE: The unimodality forest method for clustering and estimation of the number of clusters. <em>PR</em>, <em>172</em>, 112357. (<a href='https://doi.org/10.1016/j.patcog.2025.112357'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Estimating the number of clusters k while clustering the data is a challenging task. An incorrect cluster assumption indicates that the number of clusters k gets wrongly estimated. Consequently, the model fitting becomes less important. In this work, we focus on the concept of unimodality and propose a flexible cluster definition called locally unimodal cluster . A locally unimodal cluster extends for as long as unimodality is locally preserved across pairs of subclusters of the data. Then, we propose the UniForCE method for locally unimodal clustering. The method starts with an initial overclustering of the data and relies on the unimodality graph that connects subclusters forming unimodal pairs. Such pairs are identified using an appropriate statistical test. UniForCE identifies maximal locally unimodal clusters that are statistically significant by computing a spanning forest in the unimodality graph. Experimental results on both real and synthetic datasets illustrate that the proposed methodology is particularly flexible and robust in discovering regular and highly complex cluster shapes. Most importantly, it automatically provides an adequate estimation of the number of clusters.},
  archive      = {J_PR},
  author       = {Georgios Vardakas and Argyris Kalogeratos and Aristidis Likas},
  doi          = {10.1016/j.patcog.2025.112357},
  journal      = {Pattern Recognition},
  month        = {4},
  pages        = {112357},
  shortjournal = {Pattern Recognition},
  title        = {UniForCE: The unimodality forest method for clustering and estimation of the number of clusters},
  volume       = {172},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). SequencePAR: Understanding pedestrian attributes via a sequence generation paradigm. <em>PR</em>, <em>172</em>, 112356. (<a href='https://doi.org/10.1016/j.patcog.2025.112356'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Current pedestrian attribute recognition (PAR) algorithms use multi-label or multi-task learning frameworks with specific classification heads. These models often struggle with imbalanced data and noisy samples. Inspired by the success of generative models, we propose Sequence Pedestrian Attribute Recognition (SequencePAR), a novel sequence generation paradigm for PAR. SequencePAR extracts pedestrian features using a language-image pre-trained model and embeds the attribute set into query tokens guided by text prompts. A Transformer decoder generates human attributes by integrating visual features and attribute query tokens. The masked multi-head attention layer in the decoder prevents the model from predicting the next attribute during training. The extensive experiments on multiple PAR datasets validate the effectiveness of SequencePAR. Specifically, we achieve 84.92 %, 90.44 %, 90.73 %, and 90.46 % in accuracy, precision, recall, and F1-score on the PETA dataset.},
  archive      = {J_PR},
  author       = {Jiandong Jin and Xiao Wang and Yin Lin and Chenglong Li and Lili Huang and Aihua Zheng and Jin Tang},
  doi          = {10.1016/j.patcog.2025.112356},
  journal      = {Pattern Recognition},
  month        = {4},
  pages        = {112356},
  shortjournal = {Pattern Recognition},
  title        = {SequencePAR: Understanding pedestrian attributes via a sequence generation paradigm},
  volume       = {172},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). SATE: Efficient knowledge distillation with implicit student-aware teacher ensembles. <em>PR</em>, <em>172</em>, 112355. (<a href='https://doi.org/10.1016/j.patcog.2025.112355'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent findings suggest that with the same teacher architecture, a fully converged or “stronger” checkpoint surprisingly leads to a worse student. This can be explained by the Information Bottleneck (IB) principle, as the features of a weaker teacher transfer more “dark” knowledge because they maintain higher mutual information with the inputs. Meanwhile, various works have shown that severe teacher-student structural disparity or capability mismatch often leads to worse student performance. To deal with these issues, we propose a generalizable and efficient Knowledge Distillation (KD) framework with implicit Student-Aware Teacher Ensembles (SATE). The SATE framework simultaneously trains a student network and a student-aware intermediate teacher as a learning companion. With the proposed co-training strategy, the intermediate teacher is trained gradually and forms implicit ensembles of weaker teachers along the learning process. Such a design enables the student model to retain more dark knowledge for better generalization ability. The proposed framework improves the training scheme in a plug-and-play way so that it can be applied to improve various classic and state-of-the-art KD methods on both intra-domain (up to 2.184 % ) and cross-domain (up to 7.358 % ) settings, under a diversified configurations on teacher-student architectures, and achieves a major efficient advantage over other generic frameworks. The code is available at https://github.com/diqichen91/SATE.git .},
  archive      = {J_PR},
  author       = {Diqi Chen and Yang Li and Jiajun Liu and Jun Zhou and Yongsheng Gao},
  doi          = {10.1016/j.patcog.2025.112355},
  journal      = {Pattern Recognition},
  month        = {4},
  pages        = {112355},
  shortjournal = {Pattern Recognition},
  title        = {SATE: Efficient knowledge distillation with implicit student-aware teacher ensembles},
  volume       = {172},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Discriminative attention based weighted sparse representation of visual objects in complex scenarios. <em>PR</em>, <em>172</em>, 112354. (<a href='https://doi.org/10.1016/j.patcog.2025.112354'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Sparse subspace representation (SSR) is an attractive technique for subspace segmentation of high-dimensional data through a self-representation manner to reveal its algebraic structure. Numerous generalizations of SSR have been developed to meet different applications. However, a fatal limitation in those extensions is their neglect of feature weights in visual samples, which play crucial roles in segmenting or recognizing specific objects. This paper introduces a discriminative attention based weighted SSR model to tackle visual objects. In the proposed model, the prior information is empirically constructed for intra-cluster features and inter-cluster ones, aided by the sparse representation of samples. An attention mechanism is introduced to learn weights of features of samples. The attention based weights of objects in samples and sparse representation of samples are collaboratively learned from the prior information. A hard version and a soft one of attention based sparse subspace representation, abbreviated as HDAWSSR and SDAWSSR, are specified by assigning attention of features by a Boolean matrix and a fuzzy matrix. Algorithms for solving both models are meticulously developed, respectively. Applications of both algorithms in clustering and moving object detection within high-dimensional image data are investigated. Experimental results show that both models outperform the state-of-the-art subspace based segmentation methods.},
  archive      = {J_PR},
  author       = {Ge Yang and Tingquan Deng and Ming Yang and Changzhong Wang},
  doi          = {10.1016/j.patcog.2025.112354},
  journal      = {Pattern Recognition},
  month        = {4},
  pages        = {112354},
  shortjournal = {Pattern Recognition},
  title        = {Discriminative attention based weighted sparse representation of visual objects in complex scenarios},
  volume       = {172},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Multi-scale feature sharing and collaborative sampling for unsupervised vehicle re-identification. <em>PR</em>, <em>172</em>, 112353. (<a href='https://doi.org/10.1016/j.patcog.2025.112353'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Vehicle Re-identification (Re-ID) retrieves target vehicle images from non-overlapping cameras. To address label noise in pseudo-labels, we propose the Multi-scale Feature Sharing and Collaborative Sampling (MFSCS) method. Specifically, the designed multi-scale feature sharing module moves beyond reliance on global features, efficiently promoting the exchange of characteristics between global and local aspects. This shared feature approach collectively mitigates the label noise arising from clustering. Recognizing that clustering methods are highly sensitive to outliers, we introduce a collaborative sampling module that cooperatively combines samples in the clustering process before training the model. This cooperative sampling module is better equipped to handle outliers in the samples and update label information more efficiently. As a result, it asymptotically improves the accuracy and stability of the model. The effectiveness of the proposed method in terms of performance is demonstrated through extensive experiments conducted on both the latest challenging truck Re-ID dataset, Truck-ID and VeRi-776.},
  archive      = {J_PR},
  author       = {Jia-Jia Li and Si-Bao Chen and Chris Ding and Bin Luo},
  doi          = {10.1016/j.patcog.2025.112353},
  journal      = {Pattern Recognition},
  month        = {4},
  pages        = {112353},
  shortjournal = {Pattern Recognition},
  title        = {Multi-scale feature sharing and collaborative sampling for unsupervised vehicle re-identification},
  volume       = {172},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). You look from old classes: Towards accurate few shot class-incremental learning. <em>PR</em>, <em>172</em>, 112352. (<a href='https://doi.org/10.1016/j.patcog.2025.112352'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Few-shot class incremental learning (FSCIL) is a common but difficult task that faces two challenges: catastrophic forgetting of old classes and insufficient learning of new classes with limited samples. Recent wisdom focuses on preventing catastrophic forgetting yet overlooks the limited samples issue, resulting in poor new class performance. In this paper, we argue that old class samples contain rich knowledge, which can be exploited to supplement the learning of new classes. To this end, we propose to Look from Old Classes (YLOC) for FSCIL, enhancing both the base and incremental sessions. In the base session, we develop a prototype centered loss (PCL) to obtain a compact distribution of old classes. During incremental sessions, we devise a prototype augmentation learning (PAL) method to aid the learning of new classes by exploiting old classes. Extensive experiments on three FSCIL benchmark datasets demonstrate the superiority of our method.},
  archive      = {J_PR},
  author       = {Yijie Hu and Kaizhu Huang and Wei Wang and Xiaowei Huang and Qiufeng Wang},
  doi          = {10.1016/j.patcog.2025.112352},
  journal      = {Pattern Recognition},
  month        = {4},
  pages        = {112352},
  shortjournal = {Pattern Recognition},
  title        = {You look from old classes: Towards accurate few shot class-incremental learning},
  volume       = {172},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Dual-decoder collaborative learning with multi-hybrid view augmentation for self-supervised 3D action recognition. <em>PR</em>, <em>172</em>, 112351. (<a href='https://doi.org/10.1016/j.patcog.2025.112351'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Self-supervised methods, including contrastive learning and masked skeleton modeling, have demonstrated considerable potential in the field of skeleton-based action recognition. While contrastive learning captures fine-grained details at the instance level, masked skeleton modeling emphasizes joint-level features. Recent studies have begun to combine these two approaches. However, existing combination methods primarily focus on integrating the tasks within the skeleton space. Moreover, existing contrastive learning methods often fail to exploit the comprehensive interaction information in skeletal structures, resulting in suboptimal performance when recognizing actions involving multiple individuals. To overcome these limitations, we introduce the Dual-Decoder Collaborative Learning (DDC) with Multi-Hybrid View Augmentation (MHGNA) method, which connects these two tasks across multiple spaces. Specifically, the masked skeleton modeling task provides diverse views for the contrastive learning task in the skeleton space, while the contrastive method aligns the features generated by both tasks within the feature space. We further present an innovative view augmentation method that enhances the model’s capacity to understand human interaction relationships by shuffling and replacing data across temporal, spatial, and personal dimensions. Extensive experiments on four downstream tasks across three large-scale datasets demonstrate that DDC exhibits stronger representational capabilities compared to state-of-the-art methods. Our code is available at https://github.com/Yingfei-Wu/DDC .},
  archive      = {J_PR},
  author       = {Wenming Cao and Yingfei Wu and Xinpeng Yin},
  doi          = {10.1016/j.patcog.2025.112351},
  journal      = {Pattern Recognition},
  month        = {4},
  pages        = {112351},
  shortjournal = {Pattern Recognition},
  title        = {Dual-decoder collaborative learning with multi-hybrid view augmentation for self-supervised 3D action recognition},
  volume       = {172},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). D3PD: Dual distillation and dynamic fusion for camera-radar 3D perception. <em>PR</em>, <em>172</em>, 112350. (<a href='https://doi.org/10.1016/j.patcog.2025.112350'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Autonomous driving perception is driving rapid advancements in Bird’s-Eye-View (BEV) technology. The synergy of surround-view imagery and radar is seen as a cost-friendly approach that enhances the understanding of driving scenarios. However, current methods for fusing radar and camera features lack effective environmental perception guidance and dynamic adjustment capabilities, which restricts their performance in real-world scenarios. In this paper, we introduce the D3PD framework, which combines fusion techniques with knowledge distillation to tackle the dynamic guidance deficit in existing radar-camera fusion methods. Our method includes two key modules: Radar-Camera Feature Enhancement (RCFE) and Dual Distillation Knowledge Transfer. The RCFE module enhances the areas of interest in BEV, addressing the poor object perception performance of single-modal features. The Dual Distillation Knowledge Transfer includes four distinct modules: Camera Radar Sparse Distillation (CRSD) for sparse feature knowledge transfer and teacher-student network feature alignment. Position-guided Sampling Distillation(SamD) for refining the knowledge transfer of fused features through dynamic sampling. Detection Constraint Result Distillation (DcRD) for strengthening the positional correlation between teacher and student network outputs in forward propagation, achieving more precise detection perception. and Self-learning Mask Focused Distillation (SMFD) for focusing perception detection results on knowledge transfer through self-learning, concentrating on the reinforcement of local key areas. The D3PD framework outperforms existing methods on the nuScenes benchmark, achieving 49.6 % mAP and 59.2 % NDS performance. Moreover, in the occupancy prediction task, D3PD-Occ has achieved an advanced performance of 37.94 % mIoU. This provides insights for the design and model training of camera and radar-based 3D object detection and occupancy network prediction methods. The code will be available at https://github.com/no-Name128/D3PD .},
  archive      = {J_PR},
  author       = {Junyin Wang and Chenghu Du and Tongao Ge and Bingyi Liu and Shengwu Xiong},
  doi          = {10.1016/j.patcog.2025.112350},
  journal      = {Pattern Recognition},
  month        = {4},
  pages        = {112350},
  shortjournal = {Pattern Recognition},
  title        = {D3PD: Dual distillation and dynamic fusion for camera-radar 3D perception},
  volume       = {172},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). TBiGAN-based parallel networks for remaining useful life prediction of multi-stage degraded bearings. <em>PR</em>, <em>172</em>, 112349. (<a href='https://doi.org/10.1016/j.patcog.2025.112349'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Accurate prediction of the remaining useful life (RUL) of rolling bearings is crucial for ensuring the safe and reliable operation of rotating machinery. However, existing methods generally overlook the correlation between different degradation stages and RUL, thereby limiting the accuracy of RUL prediction for rolling bearings. To address this challenge, a novel adaptive RUL prediction method for multi-stage degrading rolling bearings is proposed. Specifically, a new Transformer-based network is designed to classify the degradation stages of bearings. Additionally, a parallel RUL prediction model incorporating attention mechanisms is introduced, which integrates Temporal Convolutional Networks (TCN) and Bidirectional Gated Recurrent Units (BiGRU) to capture degradation features from multiple dimensions automatically and enhance the model’s ability to capture long-term dependencies in sequence tasks. Finally, the RUL prediction results from different stages are adaptively integrated using a smoothing technique to generate the final RUL. The accuracy and superiority of the proposed method are validated on the PHM2012 bearing dataset.},
  archive      = {J_PR},
  author       = {Zheng Jianfei and Chen Dongnan and Hu Changhua and Han Qihui and Pei Hong},
  doi          = {10.1016/j.patcog.2025.112349},
  journal      = {Pattern Recognition},
  month        = {4},
  pages        = {112349},
  shortjournal = {Pattern Recognition},
  title        = {TBiGAN-based parallel networks for remaining useful life prediction of multi-stage degraded bearings},
  volume       = {172},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). MChartQA and mChartQABench: A multimodal-only solution for complex chart question-answering. <em>PR</em>, <em>172</em>, 112348. (<a href='https://doi.org/10.1016/j.patcog.2025.112348'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multimodal chart question-answering (QA) is essential for applications such as financial report analysis, decision support, and invoice parsing. Current methods typically convert charts to text for processing by large language models (LLMs) or use direct multimodal processing. This raises an important question: under what conditions is a multimodal approach essential for chart question-answering? We observe that these traditional approaches often struggle with complex color patterns, structural intricacies, and implicit numerical data. Yet, limited research addresses these challenges. To bridge this gap, we introduce a new multimodal chart dataset, mChartQABench, constructed by consolidating data from existing open-source datasets to address challenges with color, structure, and textless chart data. To handle these complex multimodal scenarios effectively, we propose mChartQA, a framework integrating the advanced language processing of LLMs with a state-of-the-art table-to-text engine. This framework excels in aligning visual and textual data, enhancing deep reasoning and contextual understanding within charts. Experimental results show that mChartQA achieves superior performance across four datasets, with over 20 % overall accuracy improvement on mChartQABench.},
  archive      = {J_PR},
  author       = {Jingxuan Wei and Nan Xu and Guiyong Chang and Yin Luo and Bihui Yu and Ruifeng Guo},
  doi          = {10.1016/j.patcog.2025.112348},
  journal      = {Pattern Recognition},
  month        = {4},
  pages        = {112348},
  shortjournal = {Pattern Recognition},
  title        = {MChartQA and mChartQABench: A multimodal-only solution for complex chart question-answering},
  volume       = {172},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Copula-based conformal prediction for prioritized heterogeneous multi-task learning. <em>PR</em>, <em>172</em>, 112347. (<a href='https://doi.org/10.1016/j.patcog.2025.112347'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Conformal prediction (CP) has emerged as a standard for finite-sample and distribution-free uncertainty quantification (UQ). Although CP is widely used as a post-processing step (on the outputs of machine learning models) to produce reliable set-valued predictions, it is still challenging to post-process heterogeneous (i.e., categorical & numerical) predictions since the traditional CP procedures are either exclusively designed for classification or only tailored to regression. This article proposes the use of a simple yet novel copula-based CP method that jointly produces (discrete) set-valued predictions and (continuous) interval-valued predictions. This approach offers flexibility by allowing the prioritization of specific outputs’ reliability and applies to general heterogeneous multi-task problems. We demonstrate its effectiveness in the context of autonomous driving, on two popular multi-class object detection benchmarks, where it effectively infers set values for object classes and bounding boxes with the specified confidence levels. Experimental results validate our method’s ability in handling heterogeneous multi-task conformal predictions: we achieve high confidence levels without losing the informativeness of the prediction regions.},
  archive      = {J_PR},
  author       = {Bruce Cyusa Mukama and Soundouss Messoudi and Sébastien Destercke and Sylvain Rousseau},
  doi          = {10.1016/j.patcog.2025.112347},
  journal      = {Pattern Recognition},
  month        = {4},
  pages        = {112347},
  shortjournal = {Pattern Recognition},
  title        = {Copula-based conformal prediction for prioritized heterogeneous multi-task learning},
  volume       = {172},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). A deep spatio-temporal architecture for dynamic ECN analysis with granger causality based causal discovery. <em>PR</em>, <em>172</em>, 112346. (<a href='https://doi.org/10.1016/j.patcog.2025.112346'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Neurobrain science provides the motivation for research on causal modeling. The existing causal discovery methods have shown promising results in effective connectivity network analysis, however, they often overlook the dynamics of causality, in addition to the incorporation of spatio-temporal information in data. Dynamic effective connectivity networks (dECNs) reveal the changing directed brain activity and the dynamic causal influences among brain regions, which facilitate the identification of individual differences and enhance the understanding of human brain. To learn dynamic causality, we propose a deep spatio-temporal fusion architecture, which employs a dynamic causal deep encoder to incorporate spatio-temporal information into dynamic causality modeling, and a dynamic causal deep decoder to verify the discovered causality. The effectiveness of the proposed method is first illustrated with simulated data. Then, experimental results from Philadelphia Neurodevelopmental Cohort (PNC) demonstrate the superiority of the proposed method in inferring dECNs, which reveal the dynamic evolution of directed flow between brain regions. The analysis shows the difference of dECNs between young adults and children. Specifically, the directed brain functional networks transit from fluctuating undifferentiated systems to more stable specialized networks as one grows. This observation provides further evidence on the modularization and adaptation of brain networks during development, leading to higher cognitive abilities observed in young adults.},
  archive      = {J_PR},
  author       = {Faming Xu and Yiding Wang and Gang Qu and Vince D. Calhoun and Julia M. Stephen and Tony W. Wilson and Yu-Ping Wang and Chen Qiao},
  doi          = {10.1016/j.patcog.2025.112346},
  journal      = {Pattern Recognition},
  month        = {4},
  pages        = {112346},
  shortjournal = {Pattern Recognition},
  title        = {A deep spatio-temporal architecture for dynamic ECN analysis with granger causality based causal discovery},
  volume       = {172},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Unsupervised domain adaptation via style-aware self-intermediate domain. <em>PR</em>, <em>172</em>, 112344. (<a href='https://doi.org/10.1016/j.patcog.2025.112344'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Unsupervised Domain Adaptation (UDA) has garnered significant attention for its ability to transfer knowledge from a label-rich source domain to a related but unlabeled target domain, with minimizing inter-domain discrepancies being crucial, especially when a substantial gap exists between the domains. To address this, we introduce the novel Style-aware Self-Intermediate Domain (SSID), which effectively bridges large domain gaps by facilitating knowledge transfer while preserving class-discriminative information. Inspired by human transitive inference and learning capabilities, SSID connects seemingly unrelated concepts through a sequence of intermediate, auxiliary synthesized concepts. Meanwhile, an external memory bank is designed to store and update designated labeled features, ensuring the stability of class-specific and class-wise style features. Additionally, we also proposed a novel intra- and inter-domain loss functions that enhance class recognition and feature compatibility, with their convergence rigorously validated through a novel analytical approach. Comprehensive experiments demonstrate that SSID achieves accuracies of 85.4 % and 85.3 % on two widely recognized UDA benchmarks, outperforming the second-best methods by 0.94 % and 1.17 %, respectively. As a plug-and-play solution, SSID integrates seamlessly with various backbone networks, showcasing its effectiveness and versatility in domain adaptation scenarios.},
  archive      = {J_PR},
  author       = {Lianyu Wang and Meng Wang and Daoqiang Zhang and Huazhu Fu},
  doi          = {10.1016/j.patcog.2025.112344},
  journal      = {Pattern Recognition},
  month        = {4},
  pages        = {112344},
  shortjournal = {Pattern Recognition},
  title        = {Unsupervised domain adaptation via style-aware self-intermediate domain},
  volume       = {172},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Enhancing visual representation of untrimmed videos by counteracting visuality threatening content. <em>PR</em>, <em>172</em>, 112343. (<a href='https://doi.org/10.1016/j.patcog.2025.112343'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the remarkable growth of the video platform industry and the surge in video uploads, Content-Based Video Retrieval (CBVR), which finds videos on desired topics from a collection of untrimmed videos using solely the visual modality, is gaining increased attention. However, the challenge of accurate retrieval persists due to the varied and complex content in untrimmed videos, and there has been a lack of discussion on which types of content compromise visual representations. In this paper, we found that text and blur texture are of this nature, grounded in empirical observations. Indeed, in models focusing on the visual modality, both the visual structure of text (without semantics) and the smoothness of blur texture (with few edges and corners) interfere with decision-making. To address them, we propose two strategies: text-masking learning, which excludes the effect of text in the descriptor for inputs that may contain text content, and blur texture filtering, a re-scaling strategy that mitigates the impact of blur textures by exploiting the neural network’s insensitivity to the smoothed pixel-wise gradients. Furthermore, through empirical observations, we demonstrate that our proposed method effectively handles visuality-threatening content. Additionally, we show that our method can lead to state-of-the-art performance across multiple benchmarks of untrimmed videos.},
  archive      = {J_PR},
  author       = {Gwangjin Lee and Won Jo and Hyunwoo Kim and Yukyung Choi},
  doi          = {10.1016/j.patcog.2025.112343},
  journal      = {Pattern Recognition},
  month        = {4},
  pages        = {112343},
  shortjournal = {Pattern Recognition},
  title        = {Enhancing visual representation of untrimmed videos by counteracting visuality threatening content},
  volume       = {172},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Texture-aware transformer with pose-patch mapping for occluded person re-identification. <em>PR</em>, <em>172</em>, 112341. (<a href='https://doi.org/10.1016/j.patcog.2025.112341'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Occluded person re-identification (re-ID) aims to retrieve the target person from occluded images captured by different cameras, where the challenges lie in identity loss caused by different types of occlusion. To alleviate the occlusion interference, some methods rely on external clues or generate more occlusion samples. However, these methods fail to address the issues of pose misalignment under extreme occlusion and identity confusion caused by non-target pedestrian occlusion. To solve these problems, we design a novel T exture-Aware T ransformer with P ose-Patch M apping (TTPM), which does not require generating any occlusion samples. Specifically, a Multi-patch Feature Encoder is proposed to encode discriminative features from inter patches and intra patches. Afterwards, the Pose-Patch Mapping is designed to construct a positional mapping between poses and patches, which highlights human patches and weakens the impact of occluded patches. Finally, to mitigate the non-target pedestrian occlusion, a Texture-Aware Decoder is introduced to perceive texture features and leverage their distinctiveness to enhance the representation of important regions. Extensive experiments show that our method achieves state-of-the-art results on Occluded-Duke and Occluded-REID datasets.},
  archive      = {J_PR},
  author       = {Dengwen Wang and Guanyu Xing and Yanli Liu},
  doi          = {10.1016/j.patcog.2025.112341},
  journal      = {Pattern Recognition},
  month        = {4},
  pages        = {112341},
  shortjournal = {Pattern Recognition},
  title        = {Texture-aware transformer with pose-patch mapping for occluded person re-identification},
  volume       = {172},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). An efficient community-aware pre-training method for graph neural networks. <em>PR</em>, <em>172</em>, 112340. (<a href='https://doi.org/10.1016/j.patcog.2025.112340'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {While graph neural networks (GNNs) have demonstrated widespread success in various domains, their pre-training techniques lag behind those in computer vision and natural language processing, typically exhibiting limited performance gains and high computational costs. This paper introduces Community-Aware Pre-training (CAP), a novel approach that leverages the inherent community structures prevalent in real-world networks to enhance GNN pre-training efficiency and effectiveness. CAP employs a self-supervised contrastive learning framework to learn node representations that are highly discriminative of their respective communities. To further optimize the pre-training process, we introduce a Monte Carlo Tree Search-based community sampler that efficiently extracts representative subgraphs, mitigating noise and enhancing sample quality. CAP is versatile and can be applied to a broad range of node classification tasks due to the commonly existing community structures within networks. Extensive evaluations on diverse node classification benchmarks demonstrate that CAP consistently outperforms state-of-the-art methods, achieving accuracy improvements of up to 4.34 % while significantly reducing pre-training time by up to 14.87 times compared to existing techniques. Furthermore, CAP enhances the predictive confidence and visualization distinctiveness of node representations, paving a new path for effective and efficient GNN pre-training.},
  archive      = {J_PR},
  author       = {Zhenhua Huang and Wenhao Zhou and Yihang Jiang and Zhaohong Jia and Linyuan Lü and Yunjie Ma},
  doi          = {10.1016/j.patcog.2025.112340},
  journal      = {Pattern Recognition},
  month        = {4},
  pages        = {112340},
  shortjournal = {Pattern Recognition},
  title        = {An efficient community-aware pre-training method for graph neural networks},
  volume       = {172},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). DiffTrajectory: Mitigating cumulative errors and enhancing inference efficiency in diffusion-based trajectory prediction. <em>PR</em>, <em>172</em>, 112339. (<a href='https://doi.org/10.1016/j.patcog.2025.112339'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Diffusion models have made significant progress in trajectory prediction tasks but still face several critical challenges. The ordinary differential equation (ODE) solving methods used in standard diffusion models often suffer from error accumulation during multi-step iterations. Additionally, the denoising process is highly time-consuming due to the large number of computational steps, which significantly hinders inference efficiency and makes real-time applications challenging. To address these issues, we propose a diffusion-based method, DiffTrajectory, which integrates the Runge-Kutta (RK4) method, a Leap Initializer Module (LIM), and an Adaptive Dynamic Step-size Strategy (ADSS) to enhance generation accuracy and greatly optimize inference efficiency. Specifically, to tackle the problem of error accumulation, DiffTrajectory formalizes the denoising process as an ODE-solving problem and adopts the RK4 as a numerical solution. By computing multiple intermediate points at each iteration, this approach significantly reduces error accumulation. To improve the efficiency of the denoising process, DiffTrajectory introduces LIM, which leverages a pre-trained initial model to quickly generate a high-quality starting point for denoising, thereby reducing the computational burden during the initial denoising stages. Furthermore, we design the ADSS that adjusts the step size dynamically based on the results of each denoising stage, ensuring the quality of the generated results while substantially shortening inference time. Extensive experiments on the ETH/UCY and NBA datasets demonstrate that DiffTrajectory achieves substantial improvements in both accuracy and efficiency.},
  archive      = {J_PR},
  author       = {Chengcheng Li and Luqi Gong and Leiheng Xu and Xin Wang},
  doi          = {10.1016/j.patcog.2025.112339},
  journal      = {Pattern Recognition},
  month        = {4},
  pages        = {112339},
  shortjournal = {Pattern Recognition},
  title        = {DiffTrajectory: Mitigating cumulative errors and enhancing inference efficiency in diffusion-based trajectory prediction},
  volume       = {172},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). A mondrian conformal predictive system with improved decision trees for uncertainty quantification under heteroscedasticity. <em>PR</em>, <em>172</em>, 112338. (<a href='https://doi.org/10.1016/j.patcog.2025.112338'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the increasing application of machine learning in industrial production, model uncertainty quantification has become a critical tool to evaluate the prediction reliability and guide decision-making. The Conformal Predictive System (CPS), which generates Cumulative Distribution Functions (CDFs), provides valuable support for uncertainty quantification. However, CPS faces limitations when addressing heteroskedasticity. This paper proposes a Mondrian Conformal Predictive System (LWT-MCPS) based on an enhanced Decision Tree. The proposed approach constructs decision trees using splitting criteria derived from Levene’s test and Welch’s t -test, ensuring that the variance and mean within each partition remain as homogeneous as possible. Furthermore, it incorporates predicted values and prediction variances estimated using the k -Nearest Neighbors (KNN) as splitting features, effectively mitigating the impact of high-dimensional data on tree partitioning and enhancing the model’s ability to identify heterogeneous regions. Experiments conducted on simulated data, public datasets, and blast furnace ironmaking data demonstrate that LWT-MCPS generates CDFs with lower Continuous Ranked Probability Scores (CRPS) than traditional CPS. These results validate its significant advantages in addressing heteroskedasticity challenges.},
  archive      = {J_PR},
  author       = {Ruiyao Zhang and Ping Zhou},
  doi          = {10.1016/j.patcog.2025.112338},
  journal      = {Pattern Recognition},
  month        = {4},
  pages        = {112338},
  shortjournal = {Pattern Recognition},
  title        = {A mondrian conformal predictive system with improved decision trees for uncertainty quantification under heteroscedasticity},
  volume       = {172},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Spatial multi-semantic features guided spectral-friendly transformer network for hyperspectral image classification. <em>PR</em>, <em>172</em>, 112337. (<a href='https://doi.org/10.1016/j.patcog.2025.112337'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Hyperspectral image classification (HSIC) is a foundational topic in remote sensing. However, the high correlations between bands and the spectral correlations often result in redundant data. Moreover, traditional convolutional neural networks (CNNs) compress spatial dimensions through pooling layers or strides during spatial information extraction, resulting in the loss of spatial information. To overcome these challenges, we propose a spatial multi-semantic features guided spectral-friendly Transformer network (SFTN), which effectively extracts the spectral and spatial features of HSIs. Specifically, a multi-semantic spatial attention (MsSA) module applies unidirectional spatial compression along the height and width dimensions. Thus, this module maintains spatial structure in one direction while aggregating global spatial information, thereby minimizing information loss during compression. It then employs multi-scale depth-shared 1D convolutions to capture multi-semantic spatial information. Furthermore, the spectral-friendly Transformer replaces the traditional multi-head self-attention (MHSA) with spectral correlation self-attention (ECSa), which effectively captures spectral differences and thus reduces the redundancy of spectral information. Extensive experiments on several HSI datasets show that the proposed SFTN method outperforms other state-of-the-art methods in HSIC applications. The source code for this work will be released later.},
  archive      = {J_PR},
  author       = {Xiaoyan Yu and Mingzhu Tai and Yuyang Wang and Zhenqiu Shu and Liehuang Zhu},
  doi          = {10.1016/j.patcog.2025.112337},
  journal      = {Pattern Recognition},
  month        = {4},
  pages        = {112337},
  shortjournal = {Pattern Recognition},
  title        = {Spatial multi-semantic features guided spectral-friendly transformer network for hyperspectral image classification},
  volume       = {172},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Backdoor defense based on adversarial prediction proximity and contrastive knowledge distillation. <em>PR</em>, <em>172</em>, 112336. (<a href='https://doi.org/10.1016/j.patcog.2025.112336'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep neural networks (DNNs) have become indispensable across various fields; however, their susceptibility to backdoor attacks poses significant security risks. In this paper, we propose a backdoor defense scheme based on adversarial prediction proximity and contrastive knowledge distillation. This scheme not only detects poisoned models and labels but also effectively unlearns backdoors while preserving the model’s benign functionality. Based on the observation that untargeted adversarial examples and poisoned samples exhibit proximity in feature space within poisoned models (i.e., adversarial prediction proximity), we first detect backdoors by analyzing changes in the prediction behavior of untargeted adversarial examples for models before and after fine-tuning. Next, we purify the poisoned model using a triplet loss that incorporates clean samples and untargeted adversarial examples. This process is guided by contrastive knowledge distillation, where a fine-tuned model acts as a “benign teacher”, and a backdoor-retained model serves as a “malicious teacher”, encouraging the poisoned model to align its feature representations with clean behavior. Comprehensive experimental results demonstrate that our scheme achieves high accuracy in detecting poisoned models and labels, even with limited access to clean samples. Furthermore, our scheme provides effective backdoor purification, while preserving the integrity and performance of models.},
  archive      = {J_PR},
  author       = {Lin Huang and Leo Yu Zhang and Ching-Chun Chang and Wei Wang and Chuan Qin},
  doi          = {10.1016/j.patcog.2025.112336},
  journal      = {Pattern Recognition},
  month        = {4},
  pages        = {112336},
  shortjournal = {Pattern Recognition},
  title        = {Backdoor defense based on adversarial prediction proximity and contrastive knowledge distillation},
  volume       = {172},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Hyperspectral space transformations for texture classification. <em>PR</em>, <em>172</em>, 112335. (<a href='https://doi.org/10.1016/j.patcog.2025.112335'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {3D color space transformations are widely used in color imaging to enhance the results of various tasks, including image segmentation, object recognition, and texture classification. However, such useful transformations are much more limited in hyperspectral imaging, where images contain hundreds to thousands of spectral bands. To improve the performance of hyperspectral image analysis, we introduce four new hyperspectral space transformations in this paper: the Hyper-Chrominance-Luminance (H-CL), the Hyper-Hue-Chroma-Luminance (H-HCL), the Hyper-Hue-Saturation-Intensity (H-HSI), and the Hyper-Hue-Saturation-Value (H-HSV). These transformations extend the corresponding CL, HCL, HSI, and HSV 3D color spaces to multiple dimensions. To investigate their suitability in the context of texture classification, several well-known texture descriptors, including both theory-driven (handcrafted) and data-driven (deep learning) methods, are used in the experiments. Ten hyperspectral datasets are considered: HyTexila, SpecTex, HyperPlastic, and seven datasets extracted from the Timbers database. Among these datasets, six new ones are introduced in this paper. The proposed H-CL, H-HCL, H-HSI, and H-HSV transformations are also compared with state-of-the-art transformation strategies. The experiments conducted in this paper demonstrate the efficacy of the proposed space transformations with an accuracy improvement that can reach +43.47 %.},
  archive      = {J_PR},
  author       = {Alice Porebski and Souraya Ouaidar Hadir and Thierry Gensane and Nicolas Vandenbroucke},
  doi          = {10.1016/j.patcog.2025.112335},
  journal      = {Pattern Recognition},
  month        = {4},
  pages        = {112335},
  shortjournal = {Pattern Recognition},
  title        = {Hyperspectral space transformations for texture classification},
  volume       = {172},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Multi-target federated backdoor attack based on feature aggregation. <em>PR</em>, <em>172</em>, 112333. (<a href='https://doi.org/10.1016/j.patcog.2025.112333'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Current federated backdoor attacks focus on collaboratively training backdoor triggers, where multiple compromised clients train their local trigger patches and then merge them into a global trigger during the inference phase. However, these methods require careful design of the shape and position of trigger patches and lack the feature interactions between trigger patches during training, resulting in poor backdoor attack success rates. Moreover, the pixels of the patches remain untruncated, thereby making abrupt areas in backdoor examples easily detectable by the detection algorithm. To this end, we propose a novel benchmark for the federated backdoor attack based on feature aggregation. Specifically, we align the dimensions of triggers with images, constrain the trigger’s pixel boundaries so that it is within a small range to avoid being detected, and aggregate trigger features from multiple compromised clients to enhance the global trigger’s ability to capture distributed data patterns. Furthermore, leveraging the intra-class attack strategy to train specific triggers for each class of samples, we propose the simultaneous generation of backdoor triggers for all target classes, significantly reducing the overall production time for triggers across all target classes and increasing the risk of the federated model being attacked. Experiments demonstrate that our method can not only bypass the detection of defense methods while patch-based methods fail, but also achieve a zero-shot backdoor attack with a success rate of 77.39 %. To the best of our knowledge, our work is the first to implement such a zero-shot attack in federated learning. Finally, we evaluate attack performance by varying the trigger’s training factors, including poison location, ratio, pixel bound, and trigger training duration (local epochs and communication rounds).},
  archive      = {J_PR},
  author       = {Lingguag Hao and Kuangrong Hao and Bing Wei and Xue-Song Tang},
  doi          = {10.1016/j.patcog.2025.112333},
  journal      = {Pattern Recognition},
  month        = {4},
  pages        = {112333},
  shortjournal = {Pattern Recognition},
  title        = {Multi-target federated backdoor attack based on feature aggregation},
  volume       = {172},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). LayerMix: Enhanced data augmentation for robust deep learning. <em>PR</em>, <em>172</em>, 112332. (<a href='https://doi.org/10.1016/j.patcog.2025.112332'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep Learning (DL) models have demonstrated remarkable performance across various computer vision tasks, yet their vulnerability to distribution shifts remains a critical challenge. Despite sophisticated neural network architectures, existing models often struggle to maintain consistent performance when confronted with Out-of-Distribution (OOD) samples, including natural corruptions, adversarial perturbations, and anomalous patterns. We introduce LayerMix, an innovative Data Augmentation (DA) approach that systematically enhances model robustness through structured fractal-based image synthesis. By meticulously integrating structural complexity into training datasets, our method generates semantically consistent synthetic samples that significantly improve neural network generalization capabilities. Unlike traditional augmentation techniques that rely on random transformations, LayerMix employs a structured mixing pipeline that preserves original image semantics while introducing controlled variability. Extensive experiments across multiple benchmark datasets, including CIFAR-10, CIFAR-100, ImageNet-200, and ImageNet-1K demonstrate LayerMix’s superior performance in classification accuracy and substantially enhances critical Machine Learning (ML) safety metrics, including resilience to natural image corruptions, robustness against adversarial attacks, improved model calibration and enhanced prediction consistency. LayerMix represents a significant advancement toward developing more reliable and adaptable artificial intelligence systems by addressing the fundamental challenges of DL generalization. The code is available at https://github.com/ahmadmughees/layermix .},
  archive      = {J_PR},
  author       = {Hafiz Mughees Ahmad and Dario Morle and Afshin Rahimi},
  doi          = {10.1016/j.patcog.2025.112332},
  journal      = {Pattern Recognition},
  month        = {4},
  pages        = {112332},
  shortjournal = {Pattern Recognition},
  title        = {LayerMix: Enhanced data augmentation for robust deep learning},
  volume       = {172},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). A query-driven twin network framework with optimization-based meta-learning for few-shot hyperspectral image classification. <em>PR</em>, <em>172</em>, 112331. (<a href='https://doi.org/10.1016/j.patcog.2025.112331'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep learning has achieved remarkable results in hyperspectral image (HSI) classification due to its powerful deep feature extraction and nonlinear relationship processing capabilities. However, the success of deep learning methods is largely dependent on extensive labeled samples, which is both time-consuming and labor-intensive. To address this issue, a novel query-driven meta-learning twin network (QMTN) framework is proposed for HSI few-shot learning. QMTN uses two meta-learning channels, allowing for the comprehensive learning of meta-knowledge across diverse meta-tasks and enhancing learning efficiency. Within the QMTN framework, a lightweight spectral-spatial attention residual network is proposed for extraction of HSI features. The network incorporates a residual mechanism in both spectral and spatial feature extraction processes and includes an attention block to improve network performance by focusing on key locations in the spatial features. To maximize the use of the limited samples for constructing diverse meta-tasks, two meta-task generation approaches are employed, with and without simulated noise. Experiments on three public HSI datasets demonstrate that the QMTN framework effectively reduces the dependence on labeled samples in a single scene and significantly improves the classification performance and convergence of the internal network. The meta-task generation method with simulated noise can improve the classification performance of the QMTN.},
  archive      = {J_PR},
  author       = {Jian Zhu and Pengxin Wang and Jian Hui and Xin Ye},
  doi          = {10.1016/j.patcog.2025.112331},
  journal      = {Pattern Recognition},
  month        = {4},
  pages        = {112331},
  shortjournal = {Pattern Recognition},
  title        = {A query-driven twin network framework with optimization-based meta-learning for few-shot hyperspectral image classification},
  volume       = {172},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Reliable classification through rank-based conformal prediction sets. <em>PR</em>, <em>172</em>, 112330. (<a href='https://doi.org/10.1016/j.patcog.2025.112330'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Machine learning classification tasks often benefit from predicting a set of possible labels with confidence scores to capture uncertainty. However, existing methods struggle with the high-dimensional nature of the data and the lack of well-calibrated probabilities from modern classification models. We propose a novel conformal prediction method that utilizes a rank-based score function suitable for classification models that predict the order of labels correctly, even if not well-calibrated. Our approach constructs prediction sets that achieve the desired coverage rate while managing their size. We provide a theoretical analysis of the expected size of the conformal prediction sets based on the rank distribution of the underlying classifier. Through extensive experiments, we demonstrate that our method outperforms existing techniques on various datasets, providing reliable uncertainty quantification. Our contributions include a novel conformal prediction method, theoretical analysis, and empirical evaluation. This work advances the practical deployment of machine learning systems by enabling reliable uncertainty quantification.},
  archive      = {J_PR},
  author       = {Rui Luo and Zhixin Zhou},
  doi          = {10.1016/j.patcog.2025.112330},
  journal      = {Pattern Recognition},
  month        = {4},
  pages        = {112330},
  shortjournal = {Pattern Recognition},
  title        = {Reliable classification through rank-based conformal prediction sets},
  volume       = {172},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Unsupervised domain adaptation for cardiac MRI segmentation via adversarial learning in latent space. <em>PR</em>, <em>172</em>, 112328. (<a href='https://doi.org/10.1016/j.patcog.2025.112328'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Late gadolinium enhancement (LGE) cardiac magnetic resonance (CMR) imaging is crucial for visualizing myocardial infarction (MI), with accurate segmentation of the ventricles and myocardium being essential for effective MI treatment. However, due to the complex myocardial structure and the limited availability of pixel-level annotations in LGE CMR images, accurate segmentation using supervised deep learning methods remains challenging. To address this, we propose an unsupervised domain adaptation framework for LGE CMR segmentation, utilizing CMR images from other modalities. First, we transform balanced Steady-State Free Precession (bSSFP) CMR images, which have abundant annotations, into LGE-like images using an enhanced CycleGAN. This CycleGAN incorporates an adversarial sample mining technique in the latent space to improve the quality of synthetic images. Next, we modify the nnU-Net architecture by introducing non-local blocks to train on these synthetic images, enabling precise segmentation of the myocardium and ventricular regions. We evaluate our method on the MS-CMRSeg 2019 dataset and MyoPS 2020 dataset, achieving an average Dice score of 88.0 % and 82.6 % respectively. Our experimental results demonstrate superior performance compared to state-of-the-art methods. The code for our approach is available at https://github.com/Lucarqi/Adv-CycleGAN .},
  archive      = {J_PR},
  author       = {Fan Zheng and Hengfei Cui and Yanning Zhang and Yong Xia},
  doi          = {10.1016/j.patcog.2025.112328},
  journal      = {Pattern Recognition},
  month        = {4},
  pages        = {112328},
  shortjournal = {Pattern Recognition},
  title        = {Unsupervised domain adaptation for cardiac MRI segmentation via adversarial learning in latent space},
  volume       = {172},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Efficacy of varying sensing features for enhanced performance of deep-learning-informed multidimensional force platform. <em>PR</em>, <em>172</em>, 112327. (<a href='https://doi.org/10.1016/j.patcog.2025.112327'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep learning (DL)-informed vision-based 3D force platforms have demonstrated significant potential for simultaneous assessment of pressure and shear stresses. However, the enhancement of force decoupling capacity is widely recognized as a difficult challenge in the field. For vision-based designs, the marker-embedded sensing layer serves as the pivotal element of the force platform, unveiling diverse sensing characteristics throughout the learning process. However, none of the previous studies have thoroughly investigated the differences among these sensing features and leveraged them to optimize DL models for enhanced performance in multidimensional force detection. This study addresses this gap by systematically evaluating five distinct features (including optical flow, original images, and their derivatives) using four classic CNN architectures. Our comparative analysis reveals a clear feature-force specialization: gray images are most effective for pressure decoupling, while arrow images are superior in decoupling shear stress. Based on this finding, we proposed and validated a dual-branch DL model that fuses these two specialized features. The model achieves a strong, comprehensive performance on both tasks simultaneously, demonstrating the efficacy of our evidence-based feature-fusion strategy. This study provides new insights into sensing feature selection and evidence-based neural network design for vision-based multidimensional force platforms. These advancements have the potential to expedite the deployment of high-performance multidimensional force platforms in real-life applications.},
  archive      = {J_PR},
  author       = {Hu Luo and Yuxin Ma and Zesheng Wang and Jiewen Li and Xin Ma and Wen-Ming Chen},
  doi          = {10.1016/j.patcog.2025.112327},
  journal      = {Pattern Recognition},
  month        = {4},
  pages        = {112327},
  shortjournal = {Pattern Recognition},
  title        = {Efficacy of varying sensing features for enhanced performance of deep-learning-informed multidimensional force platform},
  volume       = {172},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). TP-LReID: Lifelong person re-identification using text prompts. <em>PR</em>, <em>172</em>, 112326. (<a href='https://doi.org/10.1016/j.patcog.2025.112326'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Lifelong person re-identification (LReID) aims to develop a single model that is capable of continuously learning from new domain (present) while retaining knowledge from previously encountered ones (past) and generalizing to unseen domains (future). However, distribution shifts across these domains pose a significant challenge in maintaining performance across past, present, and future domains, that is, causing the catastrophic forgetting on previously seen domains and limited generalization to unseen ones. To address the above issues, we propose to guide consistent feature extraction to bridge distribution shifts using text prompts designed to remain invariant across domains. First, identity-consistent text prompts capturing high-level image semantics are extracted and aligned with image features throughout the lifelong learning pipeline. Moreover, to enhance generalization to unseen domains, we introduce an adversarial training that text features are contrastively aligned with both original and future-style image features, the latter generated by applying gradient-based perturbations in the feature space. Compared with 21 representative models on 11 benchmark datasets, our proposed model, trained without access to historical data, achieves performance comparable to the model trained using a joint training approach, and it performs well on all of the past, present, and future domains. We further explored the forgetting of the first historical domain and the generalization to all unseen domains under all 24 orders, and the results confirmed the superiority of our model. Codes will be released if this paper is accepted.},
  archive      = {J_PR},
  author       = {Zhaoshuo Liu and Zhiwei Guo and Chaolu Feng and Wei Li and Kun Yu and Jun Hu and Jinzhu Yang},
  doi          = {10.1016/j.patcog.2025.112326},
  journal      = {Pattern Recognition},
  month        = {4},
  pages        = {112326},
  shortjournal = {Pattern Recognition},
  title        = {TP-LReID: Lifelong person re-identification using text prompts},
  volume       = {172},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Zoom-shot: Fast, efficient and unsupervised zero-shot knowledge transfer from CLIP to vision encoders. <em>PR</em>, <em>172</em>, 112323. (<a href='https://doi.org/10.1016/j.patcog.2025.112323'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Foundation models like CLIP demonstrate exceptional capabilities over a broad domain of knowledge, such as with zero-shot classification; however, they also require significant computational resources, narrowing their real-world utility. Recent studies have shown that mapping features from pre-trained vision encoders into CLIP’s latent space can transfer some of CLIP’s abilities to smaller vision encoders, offering a promising alternative. Yet, the performance of these vision encoders still falls short of CLIP’s native capabilities, particularly in low-data regimes. In this work, we argue that enhancing training data coverage/diversity significantly improves mapping efficacy. We achieve this using tailored loss functions rather than relying on data augmentation or increasing training samples. For instance, we exploit the inherent multimodal nature of CLIP’s latent space, by incorporating cycle-consistency loss as one of our loss functions. Moreover, the mapping is learned using entirely unlabelled and unpaired data, eliminating the need for manual labelling or data pairing in novel domains. From these findings, our resulting method (Zoom-shot) offers a viable path to flexible zero-shot models for resource-limited, data-scarce settings. We test Zoom-shot’s zero-shot performance across various pre-trained vision encoders on coarse- and fine-grained datasets and achieve superior performance compared to recent works. In our ablations, we find Zoom-shot allows for a trade-off between data and compute during training; allowing for a significant reduction in required training data. All code and models are available on GitHub.},
  archive      = {J_PR},
  author       = {Jordan Shipard and Arnold Wiliem and Kien Nguyen Thanh and Wei Xiang and Clinton Fookes},
  doi          = {10.1016/j.patcog.2025.112323},
  journal      = {Pattern Recognition},
  month        = {4},
  pages        = {112323},
  shortjournal = {Pattern Recognition},
  title        = {Zoom-shot: Fast, efficient and unsupervised zero-shot knowledge transfer from CLIP to vision encoders},
  volume       = {172},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Wavelet-guided diffusion enhancement network with directional learning for single-pixel imaging. <em>PR</em>, <em>172</em>, 112322. (<a href='https://doi.org/10.1016/j.patcog.2025.112322'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Single-pixel imaging provides significant advantages for non-visible wavelength detection and ultra-compressed sensing. However, accurate reconstruction from severely under-sampled measurements remains challenging. To tackle this, we propose a novel wavelet-guided diffusion enhancement network with directional learning for single-pixel imaging (WGDNet), which hierarchically reconstructs images through wavelet component-aware reinforcement. Specifically, we design a sampling-guided model to capture essential textures and produce an initial image decomposed into high- and low-frequency components. The low-frequency part is enhanced with adaptive diffusion to preserve structure, while the high-frequency part is directionally incorporated through a multi-frequency adaptive fusion attention (MAFA) mechanism to refine details. Building on this, we develop a residual spatial adaptive fusion (RSAF) module to effectively combine low-frequency structures and high-frequency details. Extensive experiments on five public datasets demonstrate that our method achieves superior performance in both structural preservation and detail recovery. Successful implementation in the imaging system validates the applicability in real scenarios.},
  archive      = {J_PR},
  author       = {Dawei Song and Qiurong Yan and Hui Wang and Jian Yang and Xiaolong Luo},
  doi          = {10.1016/j.patcog.2025.112322},
  journal      = {Pattern Recognition},
  month        = {4},
  pages        = {112322},
  shortjournal = {Pattern Recognition},
  title        = {Wavelet-guided diffusion enhancement network with directional learning for single-pixel imaging},
  volume       = {172},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Hierarchical community-based graph generation model for improving structural diversity. <em>PR</em>, <em>172</em>, 112320. (<a href='https://doi.org/10.1016/j.patcog.2025.112320'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Graph generation remains a challenging task due to the high dimensionality of graphs and the complex dependencies among their edges. Existing models often struggle to produce structurally diverse graphs. To address this limitation, we propose a novel generative framework specifically designed to capture structural diversity in graph generation. Our approach follows a sequential process: initially, a community detection algorithm partitions the input graph into distinct communities. Each community is then generated independently using deep generative models, while a dedicated module concurrently learns the interconnections between communities. To scale to graphs with a larger number of communities, we extend our approach into a hierarchical generative model. The proposed framework not only improves generation accuracy but also significantly reduces generation time for large-scale graphs. Moreover, it enables the application of prior methods that were previously incapable of handling such graphs. To highlight the shortcomings of existing approaches, we conduct experiments on a synthetic dataset comprising diverse graph structures. The results demonstrate substantial improvements in standard evaluation metrics as well as in the quality of the generated graphs.},
  archive      = {J_PR},
  author       = {Masoomeh Sadat Razavi and Abdolreza Mirzaei and Mehran Safayani},
  doi          = {10.1016/j.patcog.2025.112320},
  journal      = {Pattern Recognition},
  month        = {4},
  pages        = {112320},
  shortjournal = {Pattern Recognition},
  title        = {Hierarchical community-based graph generation model for improving structural diversity},
  volume       = {172},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). GLGF-CR: A gated local-global fusion approach for cloud removal in real-world remote sensing. <em>PR</em>, <em>172</em>, 112319. (<a href='https://doi.org/10.1016/j.patcog.2025.112319'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Optical satellite imagery is a critical data source for Earth observation in remote sensing. However, cloud cover often degrades image quality, hindering its application and analysis. Therefore, effective cloud removal from optical satellite images has become a prominent research direction. In real-world scenarios, thick clouds act as pure noise, completely obscuring underlying information, while thin clouds provide partially beneficial information that can be leveraged for reconstruction. Traditional cloud removal methods often fail to distinguish between these two types of noise, leading to suboptimal performance. To address this limitation, we propose a novel cloud removal model, GLGF-CR, which incorporates a Gated Local-Global Fusion module. This module is designed to effectively separate and process the distinct characteristics of thick and thin clouds. For thick clouds, which contain no recoverable information, the model focuses on robust reconstruction using complementary data sources. For thin clouds, the model extracts and utilizes the beneficial information embedded in the partially obscured regions, enabling more accurate and detailed reconstruction. Additionally, a Dual Cross-Attention mechanism is introduced to establish robust mappings between SAR and optical modalities, further improving fusion accuracy. To handle domain shifts between source and target domains, we incorporate a domain adaptation module, which enhances the model’s ability to generalize across diverse real-world scenarios. The proposed algorithm not only outperforms existing methods on the large-scale real-world dataset SEN12MS-CR but also demonstrates strong cross-domain transferability on the Henan flood dataset. By explicitly addressing the dual nature of cloud noise–pure noise in thick clouds and partially beneficial information in thin clouds–this work advances the field of beneficial noise learning, demonstrating how noise can be systematically analyzed and utilized to improve model performance in complex scenarios.},
  archive      = {J_PR},
  author       = {Ganchao Liu and Jiawei Qiu and Jincheng Huang and Yuan Yuan},
  doi          = {10.1016/j.patcog.2025.112319},
  journal      = {Pattern Recognition},
  month        = {4},
  pages        = {112319},
  shortjournal = {Pattern Recognition},
  title        = {GLGF-CR: A gated local-global fusion approach for cloud removal in real-world remote sensing},
  volume       = {172},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Semi-supervised feature selection with concept factorization and robust label learning. <em>PR</em>, <em>172</em>, 112317. (<a href='https://doi.org/10.1016/j.patcog.2025.112317'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Feature selection (FS) is essential for improving model performance in high-dimensional data by identifying the most relevant features. Concept Factorization (CF), building on Non-negative Matrix Factorization (NMF), is valued for revealing meaningful data structure and producing interpretable concept vectors. However, existing CF-based FS methods are typically unsupervised and do not leverage label information, leading to a bias toward high-variance features. This bias can result in the omission of low-variance features that may be highly discriminative, ultimately reducing the effectiveness of FS and compromising model performance, especially in tasks where subtle or rare patterns are important. To address these limitations, this paper proposes SCFLR, a novel semi-supervised FS method that combines CF with robust label learning. SCFLR establishes the CF framework based on the feature space by expressing each concept vector as a conic combination of the feature vectors, thereby leveraging both the underlying data structure and available label information to select a more informative and balanced set of features. To this end, SCFLR defines a linear regression-based loss function derived from the generated concept vectors to leverage information from labeled data. This loss function is further enhanced through a label learning framework based on the L 2 , 1 -norm to ensure a robust label approximation. SCFLR also utilizes the dual-graph regularization to maintain the local geometric structures in both feature and data spaces. In order to tackle the optimization problem of SCFLR, an efficient algorithm, with proof of its convergence, is introduced. Finally, the experimental validation of the SCFLR method on multiple datasets highlights its effectiveness and superior performance compared to other FS methods.},
  archive      = {J_PR},
  author       = {Razieh Sheikhpour and Farid Saberi-Movahed and Mahdi Jalili and Kamal Berahmand},
  doi          = {10.1016/j.patcog.2025.112317},
  journal      = {Pattern Recognition},
  month        = {4},
  pages        = {112317},
  shortjournal = {Pattern Recognition},
  title        = {Semi-supervised feature selection with concept factorization and robust label learning},
  volume       = {172},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Deep learning for DBT classification with saliency-guided 2D synthesis. <em>PR</em>, <em>172</em>, 112316. (<a href='https://doi.org/10.1016/j.patcog.2025.112316'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Digital Breast Tomosynthesis (DBT) is a key imaging modality for breast cancer detection, improving lesion visibility by reducing tissue overlap inherent in conventional mammography. In this work, we propose a novel deep learning framework that classifies DBT volumes as malignant or non-malignant, while simultaneously generating a synthetic 2D image to assist diagnostic interpretation. This image is derived from a 3D saliency map computed by the internal attention mechanisms of the model, which highlights and preserves the most diagnostically relevant regions from the original volume. A surface is defined in this saliency space, enabling sampling and projection into a 2D diagnostic representation. This projection offers a compact summary of the volumetric scan, assisting clinicians in diagnostic interpretation and potentially alleviating the cognitive workload. A standard convolutional neural network trained on these synthetic 2D images achieves classification performance comparable to models operating directly on full 3D volumes. We train and evaluate our method on the OPTIMAM dataset and assess generalization through external validation on the independent BCS-DBT dataset without retraining. Results show that the model performs robustly across different clinical sources and provides an interpretable, computationally efficient tool for DBT-based breast cancer diagnosis.},
  archive      = {J_PR},
  author       = {Marco Cantone and Ciro Russo and Federico~V.~L. Dell’Ascenza and Claudio Marrocco and Alessandro Bria},
  doi          = {10.1016/j.patcog.2025.112316},
  journal      = {Pattern Recognition},
  month        = {4},
  pages        = {112316},
  shortjournal = {Pattern Recognition},
  title        = {Deep learning for DBT classification with saliency-guided 2D synthesis},
  volume       = {172},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Noise-tolerant scheme and explicit regularizer for deep active learning with noisy oracles. <em>PR</em>, <em>172</em>, 112313. (<a href='https://doi.org/10.1016/j.patcog.2025.112313'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Exploring the query strategies based on deep learning shows promising results in terms of designing the criteria for active learning. However, the labels provided by the oracles might be noisy (inaccurate) due to similarities across several classes causing ambiguity, leading to unreliable results. To address this issue, we propose a noise-tolerant deep active learning method. Specifically, we design a consistency regularization for deep attention network as explicit regularizer, which is used to measure the uncertainty of examples. Besides, we develop the robust model for dealing with the noisy oracles , which first take the associations that make from embeddings of labeled data to those of unlabeled data and back, then we employ the association probability as a weighting fusion schema into angular margin based loss. Moreover, we design the submodular maximization function for reducing the redundancy of selected batch examples. Finally, the formulation is encapsulated into the multi-task framework that helps to adaptive learning towards more generalizable performance. Experimentally, we conduct extensive experiments on classification and segmentation tasks, and the results clearly demonstrate the superiority of the proposed method to the existing state-of-the-art deep active learning approaches.},
  archive      = {J_PR},
  author       = {Yanchao Li and Ziteng Xie and Hongwu Zhong and Guangwei Gao},
  doi          = {10.1016/j.patcog.2025.112313},
  journal      = {Pattern Recognition},
  month        = {4},
  pages        = {112313},
  shortjournal = {Pattern Recognition},
  title        = {Noise-tolerant scheme and explicit regularizer for deep active learning with noisy oracles},
  volume       = {172},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Linguistic query-guided mask generation for referring image segmentation. <em>PR</em>, <em>172</em>, 112306. (<a href='https://doi.org/10.1016/j.patcog.2025.112306'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Referring image segmentation aims to segment the image region of interest according to the given language expression, which is a typical multi-modal task. Existing methods either adopt the pixel classification-based or the learnable query-based framework for mask generation, both of which are insufficient to deal with various text-image pairs with a fix number of parametric prototypes. The motivation of this work is to propose an end-to-end framework built on transformer to perform Linguistic query-Guided mask generation, dubbed LGFormer. It views the linguistic features as query to generate a specialized prototype for arbitrary input image-text pair, thus generating more consistent segmentation results. Moreover, we design several cross-modal interaction modules (e.g. vision-language bidirectional attention module, VLBA) in both encoder and decoder to achieve better cross-modal alignment. Extensive experiments demonstrate that our LGFormer achieves a new state-of-the-art performance on ReferIt, RefCOCO+, and RefCOCOg by large margins. Code is available at https://github.com/mqchen1993/LGFormer .},
  archive      = {J_PR},
  author       = {Zhichao Wei and Xiaohao Chen and Mingqiang Chen and Hao Li and Zilong Dong and Siyu Zhu},
  doi          = {10.1016/j.patcog.2025.112306},
  journal      = {Pattern Recognition},
  month        = {4},
  pages        = {112306},
  shortjournal = {Pattern Recognition},
  title        = {Linguistic query-guided mask generation for referring image segmentation},
  volume       = {172},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). TransSTC: Transformer tracker meets efficient spatial-temporal cues. <em>PR</em>, <em>172</em>, 112303. (<a href='https://doi.org/10.1016/j.patcog.2025.112303'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, researchers have started developing trackers using the powerful global modeling capabilities of transformer networks. However, existing transformer trackers usually model all template spatial cues indiscriminately and ignore temporal cues of target state changes. This distracts the tracker’s attention and gradually fails to understand the target’s latest state. Therefore, we propose a new tracker called TransSTC, which explores the effective spatial cues in the template and temporal cues during tracking to improve the tracker’s performance. Specifically, we design the target-aware focused coding network to emphasize the efficient spatial cues in the templates, alleviating the impact of spatial cues with low associations of targets in templates on the tracker’s localization accuracy. Additionally, we employ the multi-temporal template update structure that accurately captures variations in the target’s appearance. Within this structure, the collected samples are assessed for target appearance similarity and environmental interference, followed by a three-level sample selection process to ensure the accurate template update. Finally, we introduce the motion constraint framework to dynamically adjust the classification results based on the target’s historical motion trajectory. Extensive experimental results on seven tracking benchmarks demonstrate that TransSTC achieves competitive tracking performance.},
  archive      = {J_PR},
  author       = {Hong Zhang and Wanli Xing and Yifan Yang and Hanyang Liu and Ding Yuan},
  doi          = {10.1016/j.patcog.2025.112303},
  journal      = {Pattern Recognition},
  month        = {4},
  pages        = {112303},
  shortjournal = {Pattern Recognition},
  title        = {TransSTC: Transformer tracker meets efficient spatial-temporal cues},
  volume       = {172},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). MGFNet: Multi-granularity medical pattern fusion network for patient risk prediction. <em>PR</em>, <em>172</em>, 112302. (<a href='https://doi.org/10.1016/j.patcog.2025.112302'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The goal of patient risk prediction tasks is to predict a patient’s future disease or mortality risk based on his/her historical electronic health record (EHR). Most prior works focus on learning patient evolution patterns from longitudinal EHR data, while ignoring the differences in temporal granularity in medical data, resulting in insufficient information exploitation. To address these limitations, we propose the M ulti- G ranularity Medical Pattern F usion Net work (MGFNet) for patient risk prediction based on temporal data. It learns the evolutionary patterns of medical data at different temporal granularities (both at the vital sign-level and visit-level), and introduces a gated filtering function and a contrastive learning strategy for multi-granularity fusion, which captures fused information from different temporal granularities and supervises each other to obtain a more effective information representation. In addition, for patients with variable visit lengths, we introduce a soft curriculum learning method to learn these patterns by assigning different weights to medical samples to improve prediction accuracy. The final experimental results demonstrate that MGFNet effectively improves the performance of risk prediction compared with state-of-the-art approaches.},
  archive      = {J_PR},
  author       = {Lin Cheng and Yuliang Shi and Xiaojing Yu and Xinyu Li and Xinjun Wang and Zhongmin Yan and Zhiyong Chen},
  doi          = {10.1016/j.patcog.2025.112302},
  journal      = {Pattern Recognition},
  month        = {4},
  pages        = {112302},
  shortjournal = {Pattern Recognition},
  title        = {MGFNet: Multi-granularity medical pattern fusion network for patient risk prediction},
  volume       = {172},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Learning multi-scale spatial-frequency features for image denoising. <em>PR</em>, <em>172</em>, 112300. (<a href='https://doi.org/10.1016/j.patcog.2025.112300'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent advancements in multi-scale architectures have demonstrated exceptional performance in image denoising tasks. However, existing architectures mainly depends on a fixed single-input single-output Unet architecture, ignoring the multi-scale representations of pixel level. In addition, previous methods treat the frequency domain uniformly, ignoring the different characteristics of high-frequency and low-frequency noise. In this paper, we propose a novel multi-scale adaptive dual-domain network (MADNet) for image denoising. We use image pyramid inputs to restore noise-free results from low-resolution images. In order to realize the interaction of high-frequency and low-frequency information, we design an adaptive spatial-frequency learning unit (ASFU), where a learnable mask is used to separate the information into high-frequency and low-frequency components. In the skip connections, we design a global feature fusion block to enhance the features at different scales. Extensive experiments on both synthetic and real noisy image datasets verify the effectiveness of MADNet compared with current state-of-the-art denoising approaches.},
  archive      = {J_PR},
  author       = {Xu Zhao and Chen Zhao and Xiantao Hu and Hongliang Zhang and Ying Tai and Jian Yang},
  doi          = {10.1016/j.patcog.2025.112300},
  journal      = {Pattern Recognition},
  month        = {4},
  pages        = {112300},
  shortjournal = {Pattern Recognition},
  title        = {Learning multi-scale spatial-frequency features for image denoising},
  volume       = {172},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Deep intrinsic image decomposition via physics-aware neural networks. <em>PR</em>, <em>172</em>, 112299. (<a href='https://doi.org/10.1016/j.patcog.2025.112299'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Intrinsic image decomposition (IID) aims to separate an observed image into its underlying reflectance and shading components. This task is challenging due to the complex interplay of lighting, surface geometry, and material reflectance in real-world scenes. To address these challenges, this paper proposes a physics-aware deep neural network with a single-encoder, double-decoder architecture. The encoder incorporates an explicit alternating process inspired by a physics-guided model, enabling iterative decoupling of image features into reflectance and shading. Two asymmetric decoders are designed to reconstruct reflectance and shading maps based on their distinct properties. In addition, we introduce a shading loss function leverages spatial distributions of texture and structure. Unlike standard total variation (TV) losses, it employs a texture-likelihood-weighted TV norm, where weights are derived via a patch-matching scheme to distinguish isotropic textures from anisotropic image edges. This design enhances the model’s ability to suppress texture while preserving structure. Experimental results on three datasets (MIT, MPI-Sintel, and IIW) show the effectiveness of our method: on MIT and MPI-Sintel, it reduces the mean-squared-errors of both reflectance and shading by over 40 % compared to existing works, and on IIW, it achieves a superior WHDR score of 13.2, outperforming all existing methods.},
  archive      = {J_PR},
  author       = {Yan Huang and Kangjie Liu and Tengyue Chen and Yong Xu and Hui Ji},
  doi          = {10.1016/j.patcog.2025.112299},
  journal      = {Pattern Recognition},
  month        = {4},
  pages        = {112299},
  shortjournal = {Pattern Recognition},
  title        = {Deep intrinsic image decomposition via physics-aware neural networks},
  volume       = {172},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Domain adapter for visual object tracking based on hyperspectral video. <em>PR</em>, <em>172</em>, 112296. (<a href='https://doi.org/10.1016/j.patcog.2025.112296'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Object tracking based on hyperspectral video attracts increasing attention due to the rich material and motion information in the hyperspectral videos. The prevailing hyperspectral methods adapt pretrained RGB-based object tracking networks for hyperspectral tasks by converting the hyperspectral images into false-color images and fine-tuning the whole network on hyperspectral datasets, which achieves impressive results in challenging scenarios. However, the performance of hyperspectral trackers is limited by the spectral information loss during the transformation, and fine-tuning the entire pretrained network is inefficient for practical applications. To address the issues, a new hyperspectral object tracking method based on domain adaption, hyperspectral adapter for tracking (HyA-T), is proposed in this work. The hyperspectral adapter for the self-attention (HAS) and the hyperspectral adapter for the multilayer perceptron (HAM) are proposed to generate the adaption information and to transfer the multi-head self-attention (MSA) module and the multilayer perceptron (MLP) in pretrained network for the hyperspectral object tracking task by augmenting the spectral information in the original hyperspectral images into the calculation of the MSA and MLP. Additionally, the hyperspectral enhancement of input (HEI) is proposed to augment the original spectral information into the input of the tracking network. The proposed methods extract spectral information directly from the hyperspectral images, which reduce the negative impact of the spectral information loss caused by the transformation. Moreover, only the parameters in the proposed methods are fine-tuned, which is more efficient than the existing methods. Extensive experiments were conducted on four datasets with various spectral bands, verifying the effectiveness of the proposed methods. The HyA-T achieves state-of-the-art performance on all the datasets.},
  archive      = {J_PR},
  author       = {Long Gao and Yunhe Zhang and Langkun Chen and Yan Jiang and Gang He and Weiying Xie and Yunsong Li},
  doi          = {10.1016/j.patcog.2025.112296},
  journal      = {Pattern Recognition},
  month        = {4},
  pages        = {112296},
  shortjournal = {Pattern Recognition},
  title        = {Domain adapter for visual object tracking based on hyperspectral video},
  volume       = {172},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Few-shot image generation via information transfer from the built geodesic surface. <em>PR</em>, <em>172</em>, 112293. (<a href='https://doi.org/10.1016/j.patcog.2025.112293'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Generative models trained with limited data often struggle with poor fidelity and diversity. While adapting large pre-trained models is a common solution, such an approach requires significant resources and suitable source domains, which are often unavailable. To address these limitations, we propose Information Transfer from the Built Geodesic Surface (ITBGS), a framework that generates high-quality images from scratch. The core of ITBGS is our Feature Augmentation on Geodesic Surface (FAGS) module, which constructs a Geodesic surface to create a diverse pseudo-source domain from the initial samples. By transferring structural information from the augmented domain to guide the generator’s training, our method completely removes the need for pre-trained models. To refine the output, a supporting Interpolation and Regularization (I&R) module is also introduced to enhance the smoothness and perceptual quality of generated images. Extensive experiments demonstrate that ITBGS achieves state-of-the-art or comparable performance on various few-shot datasets, successfully balancing image fidelity and diversity.},
  archive      = {J_PR},
  author       = {Yuexing Han and Liheng Ruan and Bing Wang},
  doi          = {10.1016/j.patcog.2025.112293},
  journal      = {Pattern Recognition},
  month        = {4},
  pages        = {112293},
  shortjournal = {Pattern Recognition},
  title        = {Few-shot image generation via information transfer from the built geodesic surface},
  volume       = {172},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). FADMB: Fully attention-based dual memory bank network for weakly supervised video anomaly detection. <em>PR</em>, <em>172</em>, 112288. (<a href='https://doi.org/10.1016/j.patcog.2025.112288'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Video anomaly detection is crucial for analyzing surveillance videos and plays a significant role in maintaining public safety. Recent advances in weakly supervised methods, utilizing video-level labels, have improved performance based on techniques like multi-instance learning and temporal modeling. Furthermore, memory banks demonstrate great potential in unsupervised anomaly detection, prompting their integration into weakly supervised setups. However, these methods depend on the Top- k selection mechanism to update the prototypes within memory banks, which has limitations such as overlooking valuable prototypes, leading to a biased updating process, and requiring hyperparameters. To tackle these challenges, we introduce a novel video anomaly detection model, FADMB ( F ully A ttention-based D ual M emory B ank network), which replaces the Top- k selection mechanism with an innovative attention-based prototype updating paradigm to obtain a more comprehensive and robust memory bank. Additionally, we design a Hybrid Encoder that encodes local and global temporal information to produce superior video representations. Extensive experiments demonstrate the superiority of FADMB, achieving 85.79 % AUC on UCF-Crime dataset and 83.29 % AP on XD-Violence dataset.},
  archive      = {J_PR},
  author       = {Zhiming Luo and Shuheng Huang and Kun Yang and Jianzhe Gao and Shaozi Li},
  doi          = {10.1016/j.patcog.2025.112288},
  journal      = {Pattern Recognition},
  month        = {4},
  pages        = {112288},
  shortjournal = {Pattern Recognition},
  title        = {FADMB: Fully attention-based dual memory bank network for weakly supervised video anomaly detection},
  volume       = {172},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Enhancing cloud security with authentication for securing sensitive information. <em>PR</em>, <em>171</em>, 112345. (<a href='https://doi.org/10.1016/j.patcog.2025.112345'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In cloud computing, data encryption is an essential security measure that ensures sensitive data is converted into an unreadable format, which can only be accessed with the decryption keys. Though it provides security, the data is accessed by unauthorized access, breaches, and cyber-attacks due to improper security measures. Hence, this study introduces a novel Reinforcement camellia with a multifactor authentication Framework (RCMAF) for securing the data. The prime contribution of this study is securing the cloud-sensitive information, and the key management is done in the reinforcement-dense layer by Amazon Web Services (AWS). Also, the multifactor authentication is executed during the decryption process by processing the login strategy, one-time password (OTP), security questions, and permission to access. The model's performance is measured in terms of encryption time, decryption time, computation time, throughput, and confidential rate to analyze the model's performance, and it is compared with the conventional techniques in the Python environment. The results demonstrate that the developed method provides robust security.},
  archive      = {J_PR},
  author       = {Kavuri K.S.V.A. Satheesh and T. Krishna Sree},
  doi          = {10.1016/j.patcog.2025.112345},
  journal      = {Pattern Recognition},
  month        = {3},
  pages        = {112345},
  shortjournal = {Pattern Recognition},
  title        = {Enhancing cloud security with authentication for securing sensitive information},
  volume       = {171},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Zero-shot skeleton-based action recognition with dual visual-text alignment. <em>PR</em>, <em>171</em>, 112342. (<a href='https://doi.org/10.1016/j.patcog.2025.112342'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Zero-shot action recognition, which addresses the issue of scalability and generalization in action recognition and allows the models to adapt to new and unseen actions dynamically, is an important research topic in computer vision communities. The key to zero-shot action recognition lies in aligning visual features with semantic vectors representing action categories. Most existing methods either directly project visual features onto the semantic space of text category or learn a shared embedding space between the two modalities. However, a direct projection cannot accurately align the two modalities, and learning robust and discriminative embedding space between visual and text representations is often difficult. To address these issues, we introduce Dual Visual-Text Alignment (DVTA) for skeleton-based zero-shot action recognition. The DVTA consists of two alignment modules-Direct Alignment (DA) and Augmented Alignment (AA)-along with a designed Semantic Description Enhancement (SDE). The DA module maps the skeleton features to the semantic space through a specially designed visual projector, followed by the SDE, which is based on cross-attention to enhance the connection between skeleton and text, thereby reducing the gap between modalities. The AA module further strengthens the learning of the embedding space by utilizing deep metric learning to learn the similarity between skeleton and text. Our approach achieves state-of-the-art performances on several popular zero-shot skeleton-based action recognition benchmarks. The code is available at: https://github.com/jidongkuang/DVTA .},
  archive      = {J_PR},
  author       = {Jidong Kuang and Hongsong Wang and Chaolei Han and Yang Zhang and Jie Gui},
  doi          = {10.1016/j.patcog.2025.112342},
  journal      = {Pattern Recognition},
  month        = {3},
  pages        = {112342},
  shortjournal = {Pattern Recognition},
  title        = {Zero-shot skeleton-based action recognition with dual visual-text alignment},
  volume       = {171},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). FRCAE: Feature regularization meta-learning with channel-wise attention expansion. <em>PR</em>, <em>171</em>, 112334. (<a href='https://doi.org/10.1016/j.patcog.2025.112334'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Meta-learning aims to learn universal feature representations from a limited number of samples, enabling models to rapidly adapt and generalize when faced with new tasks. Enhancing the capacity to learn these universal features is paramount for improving the performance of meta-learning models. To achieve this goal, we propose a Feature Regularization Meta-Learning with Channel-Wise Attention Expansion (FRCAE). Firstly, we leverage channel-wise attention expansion mechanisms to enhance the representation capacity of image information, enabling models to capture richer image features. Secondly, by minimizing the information loss between the support set and query set outputs, we ensure that the query set obtains consistent feature representations with the support set. Through this approach, meta-learning models can effectively learn general feature representations, thereby enhancing their generalization capabilities on new tasks. Extensive experimentation and analysis demonstrate that our approach enhances the performance of different meta-learning models on tasks like cross-domain few-shot classification, few-shot fine-grained classification, and few-shot classification. Code is available at: https://github.com/LengZhixiong/FRCAE .},
  archive      = {J_PR},
  author       = {Maofa Wang and Zhixiong Leng and Pingping Hu and Jingjing Huang and Binrui Wang and Kun Zhou and Guanyu Hu and Bingchen Yan and Lang Wu and Yanlin Xu and Quan Wan and Zhiguo Zhou},
  doi          = {10.1016/j.patcog.2025.112334},
  journal      = {Pattern Recognition},
  month        = {3},
  pages        = {112334},
  shortjournal = {Pattern Recognition},
  title        = {FRCAE: Feature regularization meta-learning with channel-wise attention expansion},
  volume       = {171},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). TranSpike: Pixel-wise frequency reconstruction and spike interaction for remote photoplethysmography. <em>PR</em>, <em>171</em>, 112329. (<a href='https://doi.org/10.1016/j.patcog.2025.112329'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Facial vision-based contactless physiological signal measurement is an important intelligent application. Currently, there are a large number of fully supervised deep learning frameworks targeting this field to remotely extract biosignals by detecting facial skin optical imaging changes caused by cardiac activity. However, these solutions suffer from training data paucity, label inconsistent, and misalignment. Although many self-supervised methods have been proposed, they generally fall into the trap of capturing irrelevant periodic features and spatial semantic drifts since the very sensitive and weak pulse-induced skin color variations. To address the above issues, we design a novel self-supervised video Transformer framework for remote physiological measurements, named TranSpike. First, we abandon the popular unlabeled and blind video frame resampling, and construct positive and negative samples in pixel-wise frequency reconstruction of facial reflections. This strategy simulates pseudo heart rate labels and avoids the spike attenuation caused by existing Nyquist–Shannon and temporal random samplings. Second, we improve the vision Transformer backbone and optimize the related model to consider long-term temporal physiological cues beyond spatial context and spatiotemporal perception. Third, we propose an innovative contrastive paradigm and corresponding architecture that inspire the interaction of biological signals, pulse rates, and power spectra to recover rhythmic features. Experimental results demonstrate that our scheme outperforms representative methods when targeting mainstream datasets. Importantly, our model not only competes with existing self-supervised techniques, but also shows remarkable consistency with state-of-the-art full-supervised monitoring architectures in zero-annotation.},
  archive      = {J_PR},
  author       = {Hang Shao and Lei Luo and Jianjun Qian and Chuanfei Hu and Shuo Chen and Jian Yang},
  doi          = {10.1016/j.patcog.2025.112329},
  journal      = {Pattern Recognition},
  month        = {3},
  pages        = {112329},
  shortjournal = {Pattern Recognition},
  title        = {TranSpike: Pixel-wise frequency reconstruction and spike interaction for remote photoplethysmography},
  volume       = {171},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). KANBalance: Kolmogorov–Arnold network mitigates class imbalance. <em>PR</em>, <em>171</em>, 112325. (<a href='https://doi.org/10.1016/j.patcog.2025.112325'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Addressing class imbalance is critical in machine learning, especially in high-stakes biomedical domains. This paper introduces KANBalance, an integrative approach that combines newly emerged Kolmogorov–Arnold Networks (KANs) with Focal Loss to tackle skewed class distributions. By substituting fixed neural activations with trainable univariate spline expansions, KANs deliver localized transformations that elevate minority-class patterns. The Focal Loss component further amplifies these challenging samples, collectively reshaping decision boundaries in favor of underrepresented instances. KAN’s ability to “zoom in” on narrow feature intervals complements Focal Loss’s capacity to emphasize hard examples, forming a synergistic framework that addresses each technique’s inherent limitations. We validate our method on two medical imaging benchmarks—pediatric chest X-rays for pneumonia detection and brain MRIs for tumor identification, both exhibiting pronounced imbalance. Results demonstrate superior accuracy, F1 scores, and AUC compared to widely used sampling baselines (e.g., SMOTE, ADASYN), and ablation studies confirm the synergy between spline-based adaptations and a minority-focused loss. Our findings reveal that coupling imbalance-aware objectives with adaptive network representations can significantly improve minority detection in complex healthcare contexts.},
  archive      = {J_PR},
  author       = {Jaber Qezelbash-Chamak and Karen Hicklin and Minhee Kim},
  doi          = {10.1016/j.patcog.2025.112325},
  journal      = {Pattern Recognition},
  month        = {3},
  pages        = {112325},
  shortjournal = {Pattern Recognition},
  title        = {KANBalance: Kolmogorov–Arnold network mitigates class imbalance},
  volume       = {171},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). RVSA-3D: Voxel-based fully sparse attention 3D object detection for rail transit obstacle perception. <em>PR</em>, <em>171</em>, 112324. (<a href='https://doi.org/10.1016/j.patcog.2025.112324'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Detecting distant, sparse obstacles poses a significant challenge in large-scale 3D object detection for rail transit. Therefore, we propose RVSA-3D, a voxel-based fully sparse attention architecture for obstacle perception. Key components include: The submanifold pooling attention (SPA) module, mitigating central feature loss in sparse backbones by combining submanifold sparse convolution principles with attention for finer-grained down-sampling. The multi-scale information fusion (MIF) module, optimizing cross-scale feature alignment and interaction during fusion. The high-confidence sparse attention (HSA) module, generating dynamic, high-confidence class-aware queries and using hierarchical cross-attention to adaptively aggregate multi-scale context, yielding high-quality features blending global and local details. RVSA-3D achieves state-of-the-art performance on KITTI and Rail3D datasets, demonstrating an excellent accuracy-efficiency balance suitable for rail transit applications.},
  archive      = {J_PR},
  author       = {Lirong Lian and Yong Qin and Zhiwei Cao and Yang Gao and Wei Li and Xiaoqing Cheng},
  doi          = {10.1016/j.patcog.2025.112324},
  journal      = {Pattern Recognition},
  month        = {3},
  pages        = {112324},
  shortjournal = {Pattern Recognition},
  title        = {RVSA-3D: Voxel-based fully sparse attention 3D object detection for rail transit obstacle perception},
  volume       = {171},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Clustering-guided contrastive prototype learning: Towards semi-supervised medical image segmentation. <em>PR</em>, <em>171</em>, 112321. (<a href='https://doi.org/10.1016/j.patcog.2025.112321'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Semi-supervised learning (SSL) has been widely exploited for medical image segmentation tasks because it allows training models over scarce labeled data and abundant unlabeled data, with comparable performance to supervised learning. However, existing SSL methods struggle with class imbalance and unreliable pseudo-label problems, causing error accumulation and performance drops. To address these issues, we propose a C lustering-guided contrastive P rototype L earning (CPL) to generate segmentation masks through pixel-to-prototype comparison. Specifically, CPL leverages cluster queries in the mask Transformer encoder to learn prototypes, which are class-aware latent representations of images. To solve the class imbalance problem, we introduce contrast loss between the prototypes learned by the teacher and the student model. We further formalize the segmentation task as a pixel-to-cluster center assignment problem; the updated cluster centers actively guide the consistency between the mask and per-pixel classification in the decoder, thereby improving the quality of pseudo labels. Our method is a plug-in and can be integrated into any SSL model. We conducted comparative and ablation studies with six state-of-the-art SSL models on the ACDC, LA and LiTs medical image datasets. The experimental results show performance improvement over all baseline models and the effectiveness of our prototype learning and contrastive learning. Code is available at https://github.com/lzh2264/CPL .},
  archive      = {J_PR},
  author       = {Zihe Lv and Zhengda Wu and Jinghua Zhu},
  doi          = {10.1016/j.patcog.2025.112321},
  journal      = {Pattern Recognition},
  month        = {3},
  pages        = {112321},
  shortjournal = {Pattern Recognition},
  title        = {Clustering-guided contrastive prototype learning: Towards semi-supervised medical image segmentation},
  volume       = {171},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). CPSL: A semi-supervised framework with class prototype-based modeling for combating noisy labels. <em>PR</em>, <em>171</em>, 112318. (<a href='https://doi.org/10.1016/j.patcog.2025.112318'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The effectiveness of deep learning models is primarily attributed to the development of datasets free from disturbances. However, these datasets contain noisy annotations due to mistakes in manual labeling. The mainstream method, sample selection, can mitigate negative impact, but shows considerable fluctuations. Moreover, semi-supervised training gradually degrades due to error accumulation caused by imprecise selection. In the paper, we propose a novel learning framework for learning with noisy labels in image classification tasks, called Class Prototype-based Modeling (CPM) for Semi-supervised Learning (CPSL). We employ cluster analysis on class-wise features to construct CPM, thereby combining prototypical contrastive loss to divide clean and noisy samples. Next, we introduce knowledge distillation into two semi-supervised modules, in order to supervise the trained student network by the teacher one. The teacher network provides smoothing and pseudo labels for the FixMatch module, but assists in guessing labels for the MixMatch module. Additionally, a contrastive loss only for the labeled set, combined with class-wise feature-level prototypes provided by CPM and predicted features of samples, minimizes the memorization risk. Experimental results on corrupted datasets demonstrate the effectiveness of our approach, particularly in challenging datasets, such as too-large categories and real-world scenarios. Specifically, our method achieves 1.8 %, 3.0 %, and 2.1 % improvements on synthetic CIFAR-10, CIFAR-100, and Tiny-ImageNet datasets, and obtains 86.4 % and 75.1 % accuracies for real-world datasets, i.e., Animal10N and Clothing1M. Code is released at https://github.com/QiangqiangXia/CPSL .},
  archive      = {J_PR},
  author       = {Qiangqiang Xia and Feifei Lee and Lin Xie and Shuai Yang and Qing Bao and Qiu Chen},
  doi          = {10.1016/j.patcog.2025.112318},
  journal      = {Pattern Recognition},
  month        = {3},
  pages        = {112318},
  shortjournal = {Pattern Recognition},
  title        = {CPSL: A semi-supervised framework with class prototype-based modeling for combating noisy labels},
  volume       = {171},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Enhancing predictive performance on long-tail trajectories via clustering and specialized decoders. <em>PR</em>, <em>171</em>, 112315. (<a href='https://doi.org/10.1016/j.patcog.2025.112315'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Accurate forecasting of traffic participants’ future trajectories is crucial for the advancement of autonomous driving systems. Constructing robust models for such tasks requires access to comprehensive datasets that include a variety of diverse cases. Current naturalistic trajectory prediction datasets are often imbalanced, featuring a large number of easier examples and a deficiency of more challenging instances. This long-tail distribution poses a significant challenge, resulting in inadequate model performance on the rare, yet safety-critical, parts of the data. To address this issue, we have proposed a framework that utilizes an embedding-based clustering technique and a distribution-sensitive decoder module to generate precise predictions for tail samples. In addition, the proposed framework includes a trajectory clustering module to refine predictions and improve the model’s capacity to generate multiple plausible future trajectories. Experimental results show that our framework outperforms the state-of-the-art long tail prediction method on tail samples by 19.5 % on the Average Displacement error (ADE) and 25.5 % on the Final Displacement error (FDE). Additionally, our approach attains state-of-the-art performance in terms of ADE metric on the ETH/UCY datasets, while only slightly trailing Y-Net in terms of the FDE metric. We further conduct ablation studies to highlight the efficacy of each of the proposed innovations. Source codes are available at this GitHub repository .},
  archive      = {J_PR},
  author       = {G. Ganeshaaraj and Tharindu Fernando and Sridha Sridharan and Clinton Fookes},
  doi          = {10.1016/j.patcog.2025.112315},
  journal      = {Pattern Recognition},
  month        = {3},
  pages        = {112315},
  shortjournal = {Pattern Recognition},
  title        = {Enhancing predictive performance on long-tail trajectories via clustering and specialized decoders},
  volume       = {171},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Arbitrary style transfer via cube and cube root network and warping constraint. <em>PR</em>, <em>171</em>, 112314. (<a href='https://doi.org/10.1016/j.patcog.2025.112314'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Arbitrary style transfer can create beautiful works of art. At present, there are style transfer models that focus on the global image and style transfer models that focus on the details of the image. However, global stylization can lead to insufficient style display, and detailed stylization can result in a lack of information in the content. In addition, simple warping of the image can affect the model’s understanding of content and style. To solve the above problems, we propose a new fusion module, a cube and cube root network and a warping constraint loss. Inspired by the fact that a value is first cubed and then taken its cube root, it remains unchanged, we integrate three fusion methods, including the module focusing on the global stylization and the module focusing on the stylization of details, which can more deeply balance the content and style parts. The warped content image is injected into the style transfer model, resulting in the warped content loss, the warped style loss, and the warped MSE loss. The warping constraint during training facilitate the model’s understanding of image content and style. Comparative experiments and ablation experiments demonstrate that the model produces better stylization effects under the cube and cube root network and the warping constraint loss. The code will be open source at https://github.com/axyuhh/CCRNet .},
  archive      = {J_PR},
  author       = {Xiaoming Yu and Jie Tian and Zhenhua Hu},
  doi          = {10.1016/j.patcog.2025.112314},
  journal      = {Pattern Recognition},
  month        = {3},
  pages        = {112314},
  shortjournal = {Pattern Recognition},
  title        = {Arbitrary style transfer via cube and cube root network and warping constraint},
  volume       = {171},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Data imputation by pursuing better classification: A supervised kernel-based method. <em>PR</em>, <em>171</em>, 112312. (<a href='https://doi.org/10.1016/j.patcog.2025.112312'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Data imputation, the process of filling in missing feature elements for incomplete datasets, plays a crucial role in data-driven learning. A fundamental belief is that data imputation is helpful for learning performance, and it follows that the pursuit of better classification can guide the data imputation process. While some works consider using label information to assist in this task, their simplistic utilization of labels lacks flexibility and may rely on strict assumptions. In this paper, we propose a new framework that effectively leverages supervision information to complete missing data in a manner conducive to classification. Specifically, this framework operates in two stages. Firstly, it leverages labels to supervise the optimization of similarity relationships among data, represented by the kernel matrix, with the goal of enhancing classification accuracy. To mitigate overfitting that may occur during this process, a perturbation variable is introduced to improve the robustness of the framework. Secondly, the learned kernel matrix serves as additional supervision information to guide data imputation through regression, utilizing the block coordinate descent method. The superiority of the proposed method is evaluated on four real-world datasets by comparing it with state-of-the-art imputation methods. Remarkably, our algorithm significantly outperforms other methods when the data is missing more than 60 % of the features.},
  archive      = {J_PR},
  author       = {Ruikai Yang and Fan He and Mingzhen He and Kaijie Wang and Xiaolin Huang},
  doi          = {10.1016/j.patcog.2025.112312},
  journal      = {Pattern Recognition},
  month        = {3},
  pages        = {112312},
  shortjournal = {Pattern Recognition},
  title        = {Data imputation by pursuing better classification: A supervised kernel-based method},
  volume       = {171},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Unsupervised cross-domain semantic segmentation on multi-modality ovarian tumor ultrasound data. <em>PR</em>, <em>171</em>, 112311. (<a href='https://doi.org/10.1016/j.patcog.2025.112311'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Ovarian cancer is one of the most harmful gynecological diseases. Early detection of ovarian tumors, facilitated by computer-aided techniques, is key to significantly reducing mortality rates. Amidst advancements in medical diagnostics, ultrasound imaging has emerged as a ubiquitous tool in clinical environments. Nevertheless, current methods mainly focus on single-modality ultrasound segmentation or recognition of ovarian tumors, overlooking the representation potential of multi-modality imaging. To address this problem, we propose a Multi-Modality Ovarian Tumor Ultrasound (MMOTU) image dataset containing 1469 2d-ultrasound images and 170 contrast-enhanced ultrasonography (CEUS) images with pixel-wise and global-wise annotations. We focus on an unsupervised cross-domain semantic segmentation task to explore the model’s adaptation potential from 2d-ultrasound and CEUS modalities. For this task, we propose a feature alignment-based architecture named Dual-Scheme Domain-Selected Network (DS 2 Net). Specifically, we embed adversarial learning to conduct feature-level alignment and propose the Domain-Distinct Selected Module (DDSM) and Domain-Universal Selected Module (DUSM) to represent the distinct and universal features in source-style and target-style. Extensive experiments and analysis on the MMOTU image dataset underscore the remarkable performance of DS 2 Net in fostering bidirectional cross-domain adaptation between 2d-ultrasound and CEUS images, thereby enhancing segmentation accuracy and pushing the boundaries of ovarian tumor detection technologies. Our proposed dataset and code are available at https://github.com/cv516Buaa/MMOTU_DS2Net .},
  archive      = {J_PR},
  author       = {Shuchang Lyu and Qi Zhao and Wenpei Bai and Linghan Cai and Guangliang Cheng and Guangxia Cui and Min Yang and Lijiang Chen and Huiyu Zhou},
  doi          = {10.1016/j.patcog.2025.112311},
  journal      = {Pattern Recognition},
  month        = {3},
  pages        = {112311},
  shortjournal = {Pattern Recognition},
  title        = {Unsupervised cross-domain semantic segmentation on multi-modality ovarian tumor ultrasound data},
  volume       = {171},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Semi-supervised medical image segmentation via pseudo-labeling refinement and dual-adaptive adjustment schemes. <em>PR</em>, <em>171</em>, 112310. (<a href='https://doi.org/10.1016/j.patcog.2025.112310'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Segmentation of tissues, organs and lesions from 2D/3D medical images is crucial in clinical analysis, diagnosis and treatment. Fully-supervised segmentation methods are costly, since labeling extensive medical images is both laborious and time-consuming. Semi-supervised segmentation, which learns from both the labeled and unlabeled data, is a promising method to alleviate the requirement of annotations. However, most existing semi-supervised methods face challenges in filtering out pseudo-label noise, extracting useful information from hard-to-segment samples, and balancing the weights of supervised and unsupervised losses. In this work, we propose a novel pseudo-labeling-based semi-supervised framework for medical image segmentation tasks, comprising two training phases. The first process utilizes a small iteration to encourage the learning of reliable points in predictions, achieving the pseudo-labeling refinement. The second phase employs pseudo-labels to compute a novel loss based on the harmonic mean, which enhances focus on hard samples. Additionally, we introduce a new min-max strategy for adaptively adjusting the weights between supervised and unsupervised losses, improving the robustness of the segmentation model. Experimental results on four public multimodal medical datasets demonstrate that our proposed method outperforms other state-of-the-art semi-supervised segmentation methods.},
  archive      = {J_PR},
  author       = {Bin Zheng and Wenyi Zhao and Wentao Liu and Ziyang He and Chuanbo Qin and Huihua Yang},
  doi          = {10.1016/j.patcog.2025.112310},
  journal      = {Pattern Recognition},
  month        = {3},
  pages        = {112310},
  shortjournal = {Pattern Recognition},
  title        = {Semi-supervised medical image segmentation via pseudo-labeling refinement and dual-adaptive adjustment schemes},
  volume       = {171},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Adaptive feature boosting and distribution refinement for graph clustering. <em>PR</em>, <em>171</em>, 112309. (<a href='https://doi.org/10.1016/j.patcog.2025.112309'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep graph clustering is a common technique for the analysis of large-scale unlabeled graph data, which is fundamental yet challenging. Recently, the combination of Auto-Encoder and Graph Neural Networks in graph clustering methods has gained widespread attention. However, we observe that the weakly relevant features and unreliable target distribution weaken the discriminative representation of graph learning, which limits the graph clustering performance. To tackle this issue, we propose a novel method termed Adaptive feature Boosting and distribution Refinement (ABR) for graph clustering. Specifically, we adaptively enhance the model’s capability to capture crucial graph structure and node attributes in each learning iteration through a local-global feature boosting approach. Moreover, we design a dynamic distribution refinement strategy that measures the importance of various soft assignment distributions to generate a robust target distribution. Extensive experiments conducted on five benchmark datasets have demonstrated that our proposed ABR achieves an average improvement of 3.63 % on four metrics compared to the suboptimal graph clustering method.},
  archive      = {J_PR},
  author       = {Jingxin Liu and Xiangyan Tang and Renda Han and Wenxuan Tu and Ruili Wang},
  doi          = {10.1016/j.patcog.2025.112309},
  journal      = {Pattern Recognition},
  month        = {3},
  pages        = {112309},
  shortjournal = {Pattern Recognition},
  title        = {Adaptive feature boosting and distribution refinement for graph clustering},
  volume       = {171},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Dual-frequency awareness network for lightweight super resolution. <em>PR</em>, <em>171</em>, 112308. (<a href='https://doi.org/10.1016/j.patcog.2025.112308'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, convolutional neural networks (CNN) and transformer have achieved remarkable success in single-image super-resolution (SISR) tasks. However, these methods paid less attention to high-frequency and low-frequency information in SISR. Moreover, these methods often suffer from high computational and memory requirements. In this paper, we propose a dual-frequency awareness network (DFAN) for lightweight super resolution. DFAN utilizes a dual-branch structure that combines transformer and CNN. Particularly, our proposed DFAN consists of a dual-frequency fusion unit (DFFU) and a gated edge enhancement mechanism (GEEM). The DFFU integrates a low frequency attention block (LFA) and a high frequency integration block (HFI). Spatial and high-frequency information are enhanced by each other, and channel and low-frequency information are enhanced by each other. GEEM employs a feedforward neural network and a gating mechanism to enhance the edges of images. Experimental results reveal that our proposed DFAN can achieve the better super resolution results with a less number of parameters.},
  archive      = {J_PR},
  author       = {Yongsheng Dong and Hongjie Zhou and Ruijuan Zheng and Xuelong Li},
  doi          = {10.1016/j.patcog.2025.112308},
  journal      = {Pattern Recognition},
  month        = {3},
  pages        = {112308},
  shortjournal = {Pattern Recognition},
  title        = {Dual-frequency awareness network for lightweight super resolution},
  volume       = {171},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). DiffChar: A fast conditional diffusion model for air-writing chinese character generation. <em>PR</em>, <em>171</em>, 112307. (<a href='https://doi.org/10.1016/j.patcog.2025.112307'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Air-writing is a specific form of online handwriting. Recently, air-writing recognition has attracted increasing attention. Due to the large number of Chinese character categories, collecting diverse high-quality Chinese air-writing samples remains a costly challenge, constraining research progress. In this paper, we propose DiffChar, a generator-based method capable of rapidly generating high-quality air-writing samples. We regard air-writing samples not only as a time-series but also as a graph structure, thereby exploiting their dual characteristics. To better model the structure of characters, we propose a glyph-oriented graph-based U-shaped network, called GG-UNet. The GG-UNet models the structure of air-writing data from a graphical perspective. Simultaneously, we propose a position-dependent conditional embedding method, called PCE. It enhances the details of the generated sample by embedding position-specific conditional information at different locations in the input noise sequence. The generator of DiffChar is based on the design of GG-UNet and PCE. Through experiments, we demonstrate that the proposed modifications significantly improve the quality of Chinese character air-writing generation. Furthermore, through extended experiments, we demonstrate that DiffChar seamlessly integrates with a style-encoding network to synthesize stylized samples. Finally, we present GIAHCC-UCAS2024, a new air-writing dataset generated using DiffChar. This dataset encompasses 3811 Chinese characters, comprising 368,688 instances.},
  archive      = {J_PR},
  author       = {Weixi Zhao and Meiqi Wu and Weiqiang Wang},
  doi          = {10.1016/j.patcog.2025.112307},
  journal      = {Pattern Recognition},
  month        = {3},
  pages        = {112307},
  shortjournal = {Pattern Recognition},
  title        = {DiffChar: A fast conditional diffusion model for air-writing chinese character generation},
  volume       = {171},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Deep portfolio selection with contrastively aligned cross-modal attention. <em>PR</em>, <em>171</em>, 112305. (<a href='https://doi.org/10.1016/j.patcog.2025.112305'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Portfolio choice is one of the most important topics in finance. Ample studies apply deep learning methods to it. However, most of the proposed models may suffer from the following main challenges: (i) The scarcity of historical data in financial markets, renders deep learning methods much less robust. (ii) Market-based time-series instances such as price data may share high similarities most of the time, making it challenging to extract discriminative representations. In this paper, we tackle these challenges within a coherent deep-learning model: C ontrastively A ligned C r oss-mod a l ( C ross)- A ttention Mode l ( Caracal ) for portfolio selection. By introducing multi-modal data, we contrastively extract and fuse cross-modal information and attend to both data-driven and real-world spatio-temporal relations with respect to an objective loss that considers the entire joint distribution of asset returns. Specifically, we propose an Inter-Modal Contrastive Fusion Module to build connections among features of highly correlated multi-modal pairs. This allows the model to find and align temporal-related text-numeric pairs, and then exchange cross-modal information and fuse cross-modal features using the attention mechanism. We further design an Inner-Modal Contrastive Learning Module to guide the feature enhancement by simultaneously learning coarse-grained and fine-grained temporal feature representations from the similarity information between instances in each modality. Extensive experiments on three real-world datasets show that our proposed model generates superior investment performance in comparison to other state-of-the-art models. We also conduct rich ablation experiments to justify the effectiveness of each module.},
  archive      = {J_PR},
  author       = {Yupeng Fang and Huichou Huang and Ruirui Liu and Chaoyu Chen and Haoxian Liu and Qingyao Wu},
  doi          = {10.1016/j.patcog.2025.112305},
  journal      = {Pattern Recognition},
  month        = {3},
  pages        = {112305},
  shortjournal = {Pattern Recognition},
  title        = {Deep portfolio selection with contrastively aligned cross-modal attention},
  volume       = {171},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). GC-GS: Gradient control gaussian splatting with various image degradation. <em>PR</em>, <em>171</em>, 112304. (<a href='https://doi.org/10.1016/j.patcog.2025.112304'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent studies on 3D Gaussian Splatting demonstrate promising applications in novel view synthesis with impressive results. However, achieving high-fidelity rendering becomes challenging due to images degradation commonly encountered in real-world scenarios. To address this issue, this paper proposes a plug-and-play Gaussian Splatting restoration pipeline, named GC-GS (Gradient Control Gaussian Splatting), which is applicable to various types of degradation, such as blur, low-resolution, low-light, or underwater scenarios. Specifically, the pre-trained 2D restoration models are employed to directly restore the degraded images. Various types of input image degradation problems can be transformed into the view inconsistency. Moreover, a gradient control strategy is introduced to mitigate the view inconsistency. Such a strategy follows the unanimity rule to execute the densification. During densification, the accumulated average of the view-space positional gradients is updated. Gaussians will clone or split only if the updated average view-space positional gradients from all of the restored images are consistent with the gaussian densification threshold. In addition, gradient control strategy employs gradient weighted optimization to further mitigate the impact of inconsistent regions. The proposed method effectively integrates the capabilities of the 2D restoration model into the task of novel view synthesis under various input image degradation. Without the need for complex network design or training methods, our method can adaptively mitigate the impact of these inconsistencies on gaussian densification and optimization solely by evaluating and weighting gradient information. Experiments demonstrate that GC-GS achieves superior rendering quality compared to previous state-of-the-art 3D restoration methods specific to single tasks on real datasets, while maintaining real-time rendering capabilities. Our project page is available at https://github.com/plbbl/code.git .},
  archive      = {J_PR},
  author       = {Qida Cao and Jiajun Ding and Qingyuan Tang and Tianning Zhao and Xiaoling Gu and Jianping Fan and Zhou Yu},
  doi          = {10.1016/j.patcog.2025.112304},
  journal      = {Pattern Recognition},
  month        = {3},
  pages        = {112304},
  shortjournal = {Pattern Recognition},
  title        = {GC-GS: Gradient control gaussian splatting with various image degradation},
  volume       = {171},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Cascaded neural memory ODEs for predicting fluence maps in rectal cancer IMRT. <em>PR</em>, <em>171</em>, 112301. (<a href='https://doi.org/10.1016/j.patcog.2025.112301'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Rectal cancer necessitates effective treatment strategies, with radiation therapy (RT) being crucial. Manual generation of intensity-modulated radiation therapy (IMRT) plans is time-consuming and expertise-dependent. This paper introduces an innovative approach for automatic IMRT planning, featuring three key elements: a singularity coding method, a cascaded model with neural memory Ordinary Differential Equation (nmODE), and a new evaluation criterion. The coding method reduces input data dimensions, representing 3D spatial information through 2D images and lowering computational cost. The cascaded model, with Dose Prediction and Fluence Map Prediction sub-models, integrates nmODE blocks to enhance nonlinearity. Ablation studies highlight the effectiveness of the cascaded structure and nmODE. Collaborative training strategies and a dual encoder in the Fluence Map Prediction Model (FPM) facilitate end-to-end learning. The new evaluation criterion calculates the error in the tumor target region. Experiments on in-house rectal cancer datasets show superior accuracy and efficiency, providing a new method for automated IMRT planning. Our source code is available at https://github.com/XiangjieTan25/nmODE-FluencePrediction .},
  archive      = {J_PR},
  author       = {Xiangjie Tan and Ying Song and Qiang Wang and Chengrong Yu and Zhang Yi and Junjie Hu},
  doi          = {10.1016/j.patcog.2025.112301},
  journal      = {Pattern Recognition},
  month        = {3},
  pages        = {112301},
  shortjournal = {Pattern Recognition},
  title        = {Cascaded neural memory ODEs for predicting fluence maps in rectal cancer IMRT},
  volume       = {171},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Selective-relaxed contrastive learning for hyperspectral image classification with noisy labels. <em>PR</em>, <em>171</em>, 112298. (<a href='https://doi.org/10.1016/j.patcog.2025.112298'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, hyperspectral image classification (HIC) has achieved remarkable success in remote sensing. Existing methods, however, depend critically on accurate annotations and, under complex spectral signatures and environmental noise, label errors inevitably degrade spectral–spatial representations and impair model generalization. Some existing methods address label noise by discarding noisy samples, but this sacrifices informative data and reduces the diversity of positive–negative pairs required for contrastive learning, thereby undermining feature discriminability. To address these challenges, we propose selective-relaxed contrastive learning (SRCL), a unified framework for robust spectral–spatial representation learning under noisy labels. Specifically, we first employ a Gaussian mixture model (GMM) on per-sample loss values to partition the dataset into clean and noisy subsets. Currently, we perform spectral-spatial embedding graph learning to capture structural information and apply label propagation to correct erroneous annotations. Subsequently, we integrate a selective-relaxed contrastive module to aggregate low-confidence noisy samples into a super-class, increasing their affinity with the correct category and mitigating noise-induced errors. Meanwhile, high-confidence clean samples are employed in a supervised manner to generate reliable positive and negative pairs for robust representation learning. Extensive experiments on several benchmark datasets with varying noise levels demonstrate that SRCL substantially outperforms state-of-the-art approaches by effectively reutilizing noisy samples and significantly improving model effectiveness.},
  archive      = {J_PR},
  author       = {Jie Wang and Ziang Niu and Zheng Wang and Liaoyuan Tang and Bo Yan and Rong Wang and Feiping Nie},
  doi          = {10.1016/j.patcog.2025.112298},
  journal      = {Pattern Recognition},
  month        = {3},
  pages        = {112298},
  shortjournal = {Pattern Recognition},
  title        = {Selective-relaxed contrastive learning for hyperspectral image classification with noisy labels},
  volume       = {171},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Adaptive momentum weight averaging reduces initialization noise. <em>PR</em>, <em>171</em>, 112297. (<a href='https://doi.org/10.1016/j.patcog.2025.112297'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper investigates the training process of Crowd Counting Networks, which is often disrupted by noise. First, the training is sensitive to noisy initialization, making it difficult to evaluate the effectiveness of a novel model. Second, the learning curve exhibits significant fluctuations due to inherent noise in gradients and loss values, increasing the risk of overfitting to the validation set while degrading performance on the test set. To address these two issues, we propose Adaptive Momentum Weight Averaging (AMWA) to smoothen the loss surface and stabilize the training process. The network is updated based on weight averaging with an adaptive momentum that is dynamically determined by the validation error and the learning process variations. Then, our theoretical analysis shows that the proposed method decreases the variance of the network parameters during training, and improves the robustness to initialization. In experiments, we observe that the AMWA generalizes better to the test set on a wide variety of architectures and tasks: STEERER, VGG, ResNet, Dilated Network (CSRNet), and WideResNet on crowd counting. We further evaluate the proposed method on other tasks including image aesthetic assessment, blind image quality analysis, and image classification.},
  archive      = {J_PR},
  author       = {Jia Wan and Ziquan Liu and Junyu Gao and Xia Wu and Antoni B. Chan},
  doi          = {10.1016/j.patcog.2025.112297},
  journal      = {Pattern Recognition},
  month        = {3},
  pages        = {112297},
  shortjournal = {Pattern Recognition},
  title        = {Adaptive momentum weight averaging reduces initialization noise},
  volume       = {171},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). RGBT tracking via supervised mutual guiding. <em>PR</em>, <em>171</em>, 112295. (<a href='https://doi.org/10.1016/j.patcog.2025.112295'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {RGB and thermal infrared (TIR) data possess distinct visual properties, and leveraging their complementary advantages enables robust visual tracking in various weather and lighting conditions. Existing methods often employ mutual guiding mechanisms to enhance the representation of each modality. However, they typically optimize the mutual guidance models with tracking tasks as the primary objective, making it difficult to avoid unreliable guidance from low-quality modalities in complex environments. To address this issue, we propose an end-to-end supervised mutual guiding framework, which explicitly supervises the propagation of complementary information between modalities, for RGBT tracking. In particular, we perform reliable mutual guiding between RGB and thermal features through introducing the supervised modality reliability estimation, whose training labels are generated by comparing the tracking accuracy of different modalities. To mitigate the impact of noisy labels, we introduce a classification-based weight estimation approach that utilizes joint predictions from multiple classifiers. In addition, we optimize the tasks of reliability estimation, mutual guiding and RGBT tracking within a multi-task learning framework to leverage the interrelation among these tasks. We evaluate our tracker on five benchmark datasets, and the results demonstrate its superior performance compared to state-of-the-art methods. The experimental data and source code will be made publicly available at: https://github.com/mmic-lcl/Datasets-and-benchmark-code .},
  archive      = {J_PR},
  author       = {Lei Liu and Chenglong Li and Jin Tang and Changhe Li},
  doi          = {10.1016/j.patcog.2025.112295},
  journal      = {Pattern Recognition},
  month        = {3},
  pages        = {112295},
  shortjournal = {Pattern Recognition},
  title        = {RGBT tracking via supervised mutual guiding},
  volume       = {171},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Pest manager: A systematic framework for precise pest monitoring in invisible grain pile storage environments. <em>PR</em>, <em>171</em>, 112294. (<a href='https://doi.org/10.1016/j.patcog.2025.112294'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Pest infestations pose a significant threat to both the quality and quantity of stored grain, leading to substantial economic losses. Accurate and timely monitoring is essential for mitigation. However, traditional methods rely heavily on manual sampling, which is time-consuming, labor-intensive, and often inaccurate. Moreover, existing intelligent approaches primarily focus on the visible grain surface, failing to detect pest activity deep within the grain piles. To overcome these limitations, a systematic framework Pest Manager is proposed for precise pest counting and species identification in invisible grain pile storage environments. The framework consists of three key components: an improved grain probe trap ( PestMoni ), an infrared-based pest drop dataset ( PestSet ), and a multi-task Transformer-based architecture ( PestFormer ). Specifically, PestMoni utilizes two infrared emitter-photodiode pairs in an asymmetric orthogonal layout integrated into a customized circuit to detect and record pests as they fall. PestSet serves as a comprehensive dataset that supports robust pest counting and species classification across five representative stored-grain pest species. PestFormer is designed to analyze the characteristic waveforms of falling pests and incorporates a Conditional Modification Module (CMM) to mitigate device-to-device variability and standardize data processing. Extensive experiments were conducted to evaluate the reliability of PestMoni, the availability of PestSet, and the effectiveness of PestFormer. Notably, PestFormer achieved state-of-the-art performance, with 99.2 % accuracy in pest counting and 86.9 % accuracy in species identification, demonstrating its significant potential for pest management in grain storage facilities.},
  archive      = {J_PR},
  author       = {Chuanyang Ma and Jiangtao Li and Xingqun Qi and Muyi Sun and Huiling Zhou},
  doi          = {10.1016/j.patcog.2025.112294},
  journal      = {Pattern Recognition},
  month        = {3},
  pages        = {112294},
  shortjournal = {Pattern Recognition},
  title        = {Pest manager: A systematic framework for precise pest monitoring in invisible grain pile storage environments},
  volume       = {171},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Leverage cross-domain variations for generalizable person ReID representation learning. <em>PR</em>, <em>171</em>, 112292. (<a href='https://doi.org/10.1016/j.patcog.2025.112292'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Generalizable person ReID has significant practical value in challenging the fragile i.i.d. assumption by learning a domain-generalizable person representation applicable to out-of-distribution test samples. Existing methods explore feature disentanglement to learn a compact generic feature space by eliminating domain-specific knowledge. Such methods not only sacrifice discrimination in target domains but also limit the model’s robustness against per-identity appearance variations across views, which is an inherent characteristic of ReID. In this work, we formulate Cross-Domain Variations Mining (CDVM) to simultaneously explore explicit domain-specific knowledge while advancing generalizable representation learning. Our key insight is that cross-domain style variations need to be explicitly modelled to represent per-identity cross-view appearance changes. This approach retains the model’s robustness against cross-view style variations that can reflect the specific characteristics of different domains whilst maximizing the learning of a globally generalizable (invariant) representation. To this end, we propose utilizing cross-domain consensus to learn a domain-agnostic generic prototype. Subsequently, this prototype is refined by incorporating cross-domain style variations, thereby achieving cross-view feature augmentation. Additionally, we further enhance the discriminative power of the augmented representation by formulating an identity attribute constraint to impose attention on the importance of individual attributes, while maintaining overall consistency across all pedestrians. Extensive experiments validate that the proposed CDVM model outperforms existing state-of-the-art methods by significant margins.},
  archive      = {J_PR},
  author       = {Qilei Li and Shitong Sun and Weitong Cai and Shaogang Gong},
  doi          = {10.1016/j.patcog.2025.112292},
  journal      = {Pattern Recognition},
  month        = {3},
  pages        = {112292},
  shortjournal = {Pattern Recognition},
  title        = {Leverage cross-domain variations for generalizable person ReID representation learning},
  volume       = {171},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). A carving hierarchical information integration network for medical image segmentation. <em>PR</em>, <em>171</em>, 112291. (<a href='https://doi.org/10.1016/j.patcog.2025.112291'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Semantic segmentation techniques are widely applied in various image analysis tasks. However, compared with natural images, medical image segmentation presents greater challenges. For instance, lesions often vary significantly in morphology, size, and structure, and are frequently accompanied by low contrast and blurred boundaries. To simultaneously preserve fine tissue structures when handling large-scale lesions and ensure the coherence of the divergent structures of vessels, tumors, and other organs while accurately segmenting adjacent cells, this paper proposes the concept of “Global Capture and Local Carving”. It introduces a model that integrates a hierarchical information fusion strategy, named CarveNet. CarveNet incorporates a carving mechanism at three levels: downsampling, feature transmission, and bottleneck processing. Structural Carving Pooling Module underpins the downsampling carving, deeply optimizing the information structure and morphology at different levels to maximize detail retention and minimize downsampling loss. Multi-window Carving ViT is employed for transmission carving, enhancing global information modeling while refining local feature representation. The bottleneck carving integrates a long-distance recurrent communication mechanism with grid-like spatial random shuffling to strengthen the robustness and diversity of feature extraction. Experiments conducted on eight medical image datasets demonstrate that CarveNet consistently delivers outstanding performance across all tasks, surpassing the second-best method in Dice coefficient by 1.136 %. This fully validates its effectiveness in terms of multi-lesion adaptability, accuracy, and generalization capability. The code is available at https://github.com/YF-W/CarveNet .},
  archive      = {J_PR},
  author       = {Yutong Zhang and Yuefei Wang and Yuxuan Wan and Qinyu Zhao and Liangyan Zhao and Binxiong Li and Li Zhang and Zhixuan Chen},
  doi          = {10.1016/j.patcog.2025.112291},
  journal      = {Pattern Recognition},
  month        = {3},
  pages        = {112291},
  shortjournal = {Pattern Recognition},
  title        = {A carving hierarchical information integration network for medical image segmentation},
  volume       = {171},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Source-free domain adaptation using prompt learning for medical image segmentation. <em>PR</em>, <em>171</em>, 112290. (<a href='https://doi.org/10.1016/j.patcog.2025.112290'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The domain discrepancy in medical images acquired under different situations renders a major hurdle in deploying pre-trained medical image segmentation models for clinical use. Since it is less possible to distribute training data with the pre-trained model due to the huge data size and privacy concerns, source-free unsupervised domain adaptation (SFDA) has recently been increasingly studied based on either pseudo labels or prior knowledge. However, when the domain discrepancy is large, the image features and probability maps used by pseudo label-based SFDA and the consistent prior assumption and prior prediction network used by prior-guided SFDA may become less reliable. In this paper, we propose a Pro mpt learning based SFDA ( ProSFDA ) method for medical image segmentation, aiming to improve the quality of domain adaption by explicitly minimizing the domain discrepancy. The workflow of ProSFDA consists of a Prompt Learning Stage (PLS) and a Feature Alignment Stage (FAS). Specifically, in PLS, we estimate source-domain images via adding a domain-aware prompt to target-domain images. We then optimize the prompt by minimizing the statistic alignment loss, and thereby prompt the source model to generate reliable predictions on the altered target-domain images. In FAS, we also align the features of altered target-domain images and their styles-augmented counterparts to optimize the source model, and hence push the model to extract compact features. We have evaluated our ProSFDA on two multi-domain medical image segmentation benchmarks. Our results indicate that the proposed ProSFDA outperforms other SFDA methods substantially and is even comparable to UDA methods. Code is available at https://github.com/ShishuaiHu/ProSFDA .},
  archive      = {J_PR},
  author       = {Shishuai Hu and Zehui Liao and Yong Xia},
  doi          = {10.1016/j.patcog.2025.112290},
  journal      = {Pattern Recognition},
  month        = {3},
  pages        = {112290},
  shortjournal = {Pattern Recognition},
  title        = {Source-free domain adaptation using prompt learning for medical image segmentation},
  volume       = {171},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Adaptive feature selection-based feature reconstruction network for few-shot learning. <em>PR</em>, <em>171</em>, 112289. (<a href='https://doi.org/10.1016/j.patcog.2025.112289'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Few-shot learning (FSL) aims to accurately classify samples of different categories using extremely limited training data. In this work, we thoroughly analyze the fact that existing FSL methods ignore the differences, even significant differences, in feature representations extracted by different base backbones from each input image, which in turn affect classification performance. Therefore, a novel automatic feature selection (AFS) module is designed which has the capability to consistently obtain high-quality feature representations from each input image across different datasets and integrate quantized local and global features extracted from different base backbones using adaptive weights. Furthermore, the designed AFS module has the capability to effectively highlight the target feature information, suppress the influence of background noise, and improve the quality of feature representations. Then a novel AFS-based feature reconstruction (AFS-FR) network is proposed for performing different FSL tasks. Extensive experiments conducted on five benchmark datasets (i.e., CUB-200-2011, Stanford Dog, Mini-ImageNet, Tiered-ImageNet, and Aircraft) demonstrate the effectiveness and superiority of the proposed AFS-FR method over state-of-the-art approaches. Especially in the Tiered-ImageNet dataset, the classification accuracies of the proposed AFS-FR method under the 5-way 1-shot and 5-way 5-shot experimental settings are 86.30 ± 0.13 and 94.84 ± 0.06 respectively, which achieve about 4 % and 5 % improvement than the best performance indicators in the comparison methods respectively.},
  archive      = {J_PR},
  author       = {Jie Ren and Yaohui An and Tao Lei and Junpo Yang and Wenyue Zhang and Zicheng Pan and Yi Liao and Yongsheng Gao and Changming Sun and Weichuan Zhang},
  doi          = {10.1016/j.patcog.2025.112289},
  journal      = {Pattern Recognition},
  month        = {3},
  pages        = {112289},
  shortjournal = {Pattern Recognition},
  title        = {Adaptive feature selection-based feature reconstruction network for few-shot learning},
  volume       = {171},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Spatial-frequency cross-shift learning perceptive transformer features in decoupled hashing. <em>PR</em>, <em>171</em>, 112287. (<a href='https://doi.org/10.1016/j.patcog.2025.112287'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the rapid development of multimedia technology, the number of images has grown significantly, making the search for similar images an urgent necessity in everyday life. Hash image retrieval has gradually dominated the field of image retrieval due to its advantages of computational efficiency and high accuracy. However, existing image retrieval algorithms, whether based on convolutional neural networks (CNN) or Vision Transformers (ViT), primarily focus on enhancing the extraction of target category features in the spatial domain, neglecting the role of features in other domains, which to some extent limits retrieval accuracy. This paper proposes a Spatial-Frequency Cross-Shift Learning Perceptive Transformer Features in Decoupled Hashing (SFPTH), for image retrieval. First, based on the spatial location distribution of image features and the target category features present in the channels, we designed the Spatial-Spectral Dual-Domain Shift Perception (SSD) module within deep semantic feature layers. This design not only focus on features of the same category across different locations, but also the embedded frequency domain can also learn category information brought by target features enhanced through spatial shifts, thereby strengthening the output in deep features. This provides rich semantic information for discrete mapping, reducing the probability of misclassification. Secondly, we designed a bit decoupling loss to promote the independence of bits, which reduces the similarity of discrete values in model output by penalizing the correlation between labels, ensuring that the binarized output of the hash codes is more discriminative. We conducted extensive experiments on three public image retrieval datasets: MS-COCO, ImageNet, and CIFAR10, achieving retrieval performance of 93.33 % , 94.92 % , and 94.87 % , respectively.},
  archive      = {J_PR},
  author       = {Jing Zhang and Shuli Cheng and Liejun Wang},
  doi          = {10.1016/j.patcog.2025.112287},
  journal      = {Pattern Recognition},
  month        = {3},
  pages        = {112287},
  shortjournal = {Pattern Recognition},
  title        = {Spatial-frequency cross-shift learning perceptive transformer features in decoupled hashing},
  volume       = {171},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Globally localized alignment with category shifts for multisource domain adaptation. <em>PR</em>, <em>171</em>, 112286. (<a href='https://doi.org/10.1016/j.patcog.2025.112286'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In multisource domain adaptation (MDA), more information can be used to fully describe the target tasks, which performs well in image classification. However, domain shifts among several source and target domains greatly limit the MDA algorithms’ implementation. Although various transfer learning approaches have been proposed, they mostly concentrate on domain level in two scenarios: single source domain or multiple source domains, ignoring the importance of category shifts in both cases. To overcome these issues, we propose a globally localized alignment with category shifts (GLACS) method for image classification. Specifically, we first construct a global cross-domain correlation matrix, which mines the maximum correlations among data samples. While learning the greatest correlations across domains for feature representation purpose, we also focus on the local category shift alignment by identifying the feature representations that are distinguished from each other. Samples from the source or target domain that belong to the same class are draw closer together, while samples of different classes are simultaneously pushed apart. Hence, we alleviate the domain shifts and category shifts at the domain and class levels. Furthermore, we utilize an adaptive weighting strategy on the classifier to produce more reliable results. Comprehensive experiments on six visualization benchmark datasets demonstrate that our method can achieve significant performance.},
  archive      = {J_PR},
  author       = {Ming Zhao and Wanming Huang and Yuwu Lu and Zhihui Lai and Xuelong Li},
  doi          = {10.1016/j.patcog.2025.112286},
  journal      = {Pattern Recognition},
  month        = {3},
  pages        = {112286},
  shortjournal = {Pattern Recognition},
  title        = {Globally localized alignment with category shifts for multisource domain adaptation},
  volume       = {171},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). A degree-corrected stochastic block model for community discovery in signed networks with heterogeneous degree distributions. <em>PR</em>, <em>171</em>, 112285. (<a href='https://doi.org/10.1016/j.patcog.2025.112285'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Community discovery in signed networks has garnered considerable attention as a key topic in network science. However, existing methods often struggle with heterogeneous degree distributions and face scalability issues due to high computational complexity-particularly when the number of communities is unknown. To overcome these challenges, we propose the degree-corrected signed stochastic block model (DCS2BM), a probabilistic framework designed to accurately detect community structures in signed networks, with varying degree distributions. We integrate the minimum message length criterion with a component-wise expectation-maximization algorithm to develop a scalable approach that performs parameter estimation and model selection. Experimental results on both synthetic and real-world datasets demonstrate that DCS2BM outperforms state-of-the-art methods in both accuracy and efficiency. On synthetic networks with heterogeneous degrees, our model achieves an average NMI (normalized mutual information) of 0.99-exceeding the best baseline by 4.2 %. Furthermore, for large-scale networks with up to 40,000 nodes, DCS2BM maintains consistently high performance ( N M I > 0.99 ) while reducing computation time by 87.4 % compared to traditional approaches.},
  archive      = {J_PR},
  author       = {Zhejian Yang and Yang Li and Bo Yu and Jifeng Hu and Hechang Chen},
  doi          = {10.1016/j.patcog.2025.112285},
  journal      = {Pattern Recognition},
  month        = {3},
  pages        = {112285},
  shortjournal = {Pattern Recognition},
  title        = {A degree-corrected stochastic block model for community discovery in signed networks with heterogeneous degree distributions},
  volume       = {171},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Recurrent progressive fusion-based learning for multi-source remote sensing image classification. <em>PR</em>, <em>171</em>, 112284. (<a href='https://doi.org/10.1016/j.patcog.2025.112284'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the rapid development of earth observation technology, the joint classification of panchromatic (PAN) and multispectral (MS) images has gained significant research value. However, despite the acquisition of large amounts of data, the few-shot problem often arises due to insufficient labeled data, and the function space of the network degrades the generalization performance. In this paper, we propose a recurrent progressive few-shot network (RPF-Net) for the classification of dual-source remote sensing images. It mainly consists of two parts: solving for the optimized fusion direction and adaptive feature-trusted decision-level fusion. In the first part, considering the problem of insufficient function space constraints under few-shot conditions, we propose representative-reinforcement learning, which performs the next fusion step by analyzing the state of the current moment and selecting the optimal action. This recurrent progressive propagation process dynamically adjusts the fusion features, guiding them toward the optimal fusion direction within a larger function space under few-shot conditions. In the second part, considering that the importance of different source features in multiple fusions is different, we focus on uncertainty theory and perform focused decision-level fusion by analyzing the characteristics of different source features. This network can dynamically adjust the fusion direction and fusion method of features, solving the problem of too large function space under few-shot conditions. The results on multiple datasets have verified the effectiveness and stability of the proposed algorithm. Our code is available at: https://github.com/cominclip/RPF-Net .},
  archive      = {J_PR},
  author       = {Xinchen Zhang and Hao Zhu and Xiaotong Li and Biao Hou and Wenhao Zhao and Xiaoyu Yi and Wenping Ma and Licheng Jiao},
  doi          = {10.1016/j.patcog.2025.112284},
  journal      = {Pattern Recognition},
  month        = {3},
  pages        = {112284},
  shortjournal = {Pattern Recognition},
  title        = {Recurrent progressive fusion-based learning for multi-source remote sensing image classification},
  volume       = {171},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Fusion of window controls to recognize and analyze erasable patterns over data streams. <em>PR</em>, <em>171</em>, 112283. (<a href='https://doi.org/10.1016/j.patcog.2025.112283'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Erasable pattern mining uncovers interesting knowledge on patterns that produce low gain, facilitating the development of planning strategies in financial management to minimize loss. However, handling rapidly accumulating data poses challenges to the efficiency and relevance of results. Prior works have implemented various windows, such as sliding and damped window mechanisms, to effectively process large data streams. In this paper, we propose a novel window fusion-based erasable pattern recognizing and analyzing approach, which fuses the damped and sliding windows to efficiently analyze refined and relevant erasable patterns in stream environments. DSEPML divides stream data into batches of pre-defined volume and considers the significance of each batch differently according to generation time. Performance evaluation on real and synthetic datasets indicates that the proposed method is around 1.5 times up to more than an order of magnitude faster compared to state-of-the-art algorithms without compromising memory usage, and it scales well with increasing data size. Moreover, statistical tests regarding the algorithm results demonstrate the significance of the discovered patterns compared to the prior research.},
  archive      = {J_PR},
  author       = {Doyoon Kim and Hyeonmo Kim and Seungwan Park and Doyoung Kim and Hanju Kim and Myungha Cho and Unil Yun},
  doi          = {10.1016/j.patcog.2025.112283},
  journal      = {Pattern Recognition},
  month        = {3},
  pages        = {112283},
  shortjournal = {Pattern Recognition},
  title        = {Fusion of window controls to recognize and analyze erasable patterns over data streams},
  volume       = {171},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Advection-diffusion spatiotemporal recurrent network for regional wind speed prediction. <em>PR</em>, <em>171</em>, 112282. (<a href='https://doi.org/10.1016/j.patcog.2025.112282'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Regional wind speed prediction is an important spatiotemporal prediction task for optimizing wind power utilization. Existing spatiotemporal recurrent networks update the information of the memory cell by relying on gated mechanisms that capture only pointwise information flow, thereby limiting their ability to capture spatial information transport within the memory cell. To address this limitation and improve the accuracy of regional wind speed prediction, we propose a novel advection-diffusion spatiotemporal recurrent network, termed ADNet, which integrates the advection-diffusion equation into a spatiotemporal recurrent network to guide the information updating process. Specifically, we introduce a new advection-diffusion LSTM (AD-LSTM) block with advection and diffusion modules to capture the spatial information transport within the memory cell. To effectively integrate the advection and diffusion modules into the information updating process of the memory cell, we have designed a new multi-scale channel attention (MSCA) unit. This unit leverages both global and local information across the channel dimension of the memory cell to generate attention coefficients that adaptively emphasize advection and diffusion. To evaluate ADNet’s predictive performance, we conduct extensive experiments on three real-world wind speed datasets. The results demonstrate that ADNet significantly outperforms baseline methods. Our code is available at https://github.com/ShidongC/ADNet .},
  archive      = {J_PR},
  author       = {Shidong Chen and Baoquan Zhang and Dong Liu and Xutao Li and Yunming Ye and Kenghong Lin and Rui Ye},
  doi          = {10.1016/j.patcog.2025.112282},
  journal      = {Pattern Recognition},
  month        = {3},
  pages        = {112282},
  shortjournal = {Pattern Recognition},
  title        = {Advection-diffusion spatiotemporal recurrent network for regional wind speed prediction},
  volume       = {171},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Enhancing federated learning through exploring filter-aware relationships and personalizing local structures. <em>PR</em>, <em>171</em>, 112281. (<a href='https://doi.org/10.1016/j.patcog.2025.112281'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Federated learning (FL) is a promising distributed paradigm, eliminating the need for data sharing but facing challenges from data heterogeneity. Personalized parameter generation through a hypernetwork proves effective, yet existing methods fail to personalize local model structures. This leads to redundant parameters struggling to adapt to diverse data distributions. To address these limitations, we propose FedOFA , utilizing personalized orthogonal filter attention for parameter recalibration. The core is the Two-stream Filter-aware Attention (TFA) module, meticulously designed to extract personalized filter-aware attention maps, incorporating Intra-Filter Attention (IntraFA) and Inter-Filter Attention (InterFA) streams. These streams enhance representation capability and explore optimal implicit structures for local models. Orthogonal regularization minimizes redundancy by averting inter-correlation between filters. Furthermore, we introduce an Attention-Guided Pruning Strategy (AGPS) for communication efficiency. AGPS selectively retains crucial neurons while masking redundant ones, reducing communication costs without performance sacrifice. Importantly, FedOFA operates on the server side, incurring no additional computational cost on the client, making it advantageous in communication-constrained scenarios. Extensive experiments validate superior performance over state-of-the-art approaches 1},
  archive      = {J_PR},
  author       = {Ziyuan Yang and Zerui Shao and Hui Yu and Huijie Huangfu and Andrew Beng Jin Teoh and Xiaoxiao Li and Hongming Shan and Yi Zhang},
  doi          = {10.1016/j.patcog.2025.112281},
  journal      = {Pattern Recognition},
  month        = {3},
  pages        = {112281},
  shortjournal = {Pattern Recognition},
  title        = {Enhancing federated learning through exploring filter-aware relationships and personalizing local structures},
  volume       = {171},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Unsupervised heterogeneous group streaming feature selection. <em>PR</em>, <em>171</em>, 112280. (<a href='https://doi.org/10.1016/j.patcog.2025.112280'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Feature selection aims to select the optimal feature subsets from the dataset and has been widely applied in many fields and systems. However, data are not always static, and most of them are unlabeled. Besides, features may be heterogeneous and generated dynamically in practical applications. Therefore, online streaming feature selection was proposed that assumes the features are generated one by one or group by group on the fly while the number of instances remains fixed. This paper focuses on a new practical issue of online unsupervised streaming feature selection where the features are heterogeneous and dynamically generated in groups. Difficulties come from three aspects: the lack of label information, the uncertainty about the feature space, and the dynamic generation of heterogeneous streaming features. To solve this issue, we propose a new online Unsupervised Heterogeneous Group Streaming Feature Selection method named UHGSFS. To handle the problem of heterogeneous streaming features without the feature type information, UHGSFS applies MIC (Maximal Information Coefficient) to evaluate feature relationships without assuming data distribution in advance. To address the challenge of unlabeled information, UHGSFS clusters streaming features by the density based on the Gaussian kernel function and minimizes redundancy by selecting representative features. Extensive experiments were conducted on 13 benchmark datasets, with comprehensive comparisons against state-of-the-art supervised and unsupervised streaming feature selection methods. The experimental results demonstrate that our proposed method achieves comparable or even superior performance relative to supervised streaming feature selection methods.},
  archive      = {J_PR},
  author       = {Peng Zhou and Qianzhen Chen and Lei Sang and Shu Zhao and Xindong Wu},
  doi          = {10.1016/j.patcog.2025.112280},
  journal      = {Pattern Recognition},
  month        = {3},
  pages        = {112280},
  shortjournal = {Pattern Recognition},
  title        = {Unsupervised heterogeneous group streaming feature selection},
  volume       = {171},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). SSRDepth: Combining semantic segmentation and relative depth for monocular metric depth estimation. <em>PR</em>, <em>171</em>, 112279. (<a href='https://doi.org/10.1016/j.patcog.2025.112279'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Monocular depth estimation is a pivotal task in computer vision, aimed to predict a dense depth map from a single image. Existing methods can achieve satisfactory performance through carefully designed network architectures. However, they often ignore the semantic information of scene structures, leading to poor performance in the semantic boundary areas of the predicted depth maps. To address this issue, we propose SSRDepth, a monocular depth estimation method that leverages semantic segmentation to improve metric depth prediction. Specifically, we decompose metric depth into segmentation scale and semantic-aware relative depth, which are predicted separately through Segmentation Proposal Scale (SPS) module and an Adaptive Relative Depth (ARD) module. The SPS module predicts the segmentation scale under the constraints of semantic segmentation and scale alignment, endowing the model with semantic awareness and maintaining sharp semantic boundaries. Meanwhile, the ARD module adaptively predicts semantic-aware relative depth within a normalized depth space through a hybrid regression approach. Furthermore, to effectively capture structural details in semantic segments, we propose the Segmentation Query Guided Aggregation (SQGA) module, which utilizes a novel feature interaction approach to enhance the aggregation of multi-scale features, guided by masks produced through a Segmentation Mask Generator (SMG). Extensive experiments show that our method achieves competitive results on the indoor, outdoor, and unseen datasets.},
  archive      = {J_PR},
  author       = {Yuhang Zou and Xiaoming Chen and Zhangyan Zhao and Chenghua Zhang and Haipeng Liu},
  doi          = {10.1016/j.patcog.2025.112279},
  journal      = {Pattern Recognition},
  month        = {3},
  pages        = {112279},
  shortjournal = {Pattern Recognition},
  title        = {SSRDepth: Combining semantic segmentation and relative depth for monocular metric depth estimation},
  volume       = {171},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Reasoning elicitation and multi-granularity contrastive learning for text-rich image understanding in large vision-language models. <em>PR</em>, <em>171</em>, 112278. (<a href='https://doi.org/10.1016/j.patcog.2025.112278'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Understanding text-rich images plays a pivotal role in multimodal reasoning, as real-world scenes often contain abundant text, such as logo images, advertisement images, electronic invoices, PDF scanned images, etc. While current large vision-language models (LVLMs) have shown promise, many primarily focus on improving input resolution, yet lack dedicated mechanisms for reasoning and rely on autoregressive objectives for implicit alignment between textual and visual elements. This lack of precise alignment and reasoning capability highlights a key gap: existing models are still limited in their ability to effectively integrate textual semantics with visual structure, which impairs performance in tasks such as layout interpretation, contextual understanding of embedded text, and multimodal reasoning in real-world scenarios. Therefore, we developed a Reasoning Elicitation and Multi-Granularity Contrastive Learning (REMGCL) method for text-rich image understanding in large vision-language models. The reasoning elicitation module first integrates text information and detailed image descriptions to generate instruction-following rationale data in a cost-efficient manner. Then, the general instruction-following data and the generated rationale data are integrated to fine-tune the model, enhancing its reasoning capabilities. The Multi-Granularity Contrastive Learning (MGCL) loss aims to align visual and semantic features at both coarse-grained and fine-grained levels. At the coarse-grained level, we align the image features with words filtered out by a label filtering algorithm. At the fine-grained level, we align the word embeddings and patch embeddings corresponding to the text areas in the image. The MGCL captures hierarchical representations and improves the ability of the model to comprehend complex visual-textual relationships. Extensive experiments on benchmark text-rich image understanding datasets demonstrate the efficacy of the REMGCL, showcasing significant performance improvements in reasoning tasks.},
  archive      = {J_PR},
  author       = {Jiazhi Xia and Xiangyu Zhu and Bingchuan Jiang and Shichao Kan},
  doi          = {10.1016/j.patcog.2025.112278},
  journal      = {Pattern Recognition},
  month        = {3},
  pages        = {112278},
  shortjournal = {Pattern Recognition},
  title        = {Reasoning elicitation and multi-granularity contrastive learning for text-rich image understanding in large vision-language models},
  volume       = {171},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Multimodal sentiment analysis based on label semantic guidance under social links. <em>PR</em>, <em>171</em>, 112277. (<a href='https://doi.org/10.1016/j.patcog.2025.112277'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The proliferation of social media platforms has led to an explosion of multimodal data that encapsulates rich emotional content. Effectively integrating heterogeneous modalities to predict sentiment polarity remains a critical challenge. Existing approaches often underexploit sentiment prior knowledge and largely ignore the impact of social links on emotional trends, resulting in suboptimal performance. To address these limitations, we propose a novel multimodal sentiment analysis framework, i.e., Label Semantic Guidance under Social Links (LSGSL). LSGSL enhances sentiment reasoning by jointly modeling visual-textual features and the social relationships between users. Specifically, it encodes social links as a graph structure to facilitate sentiment-aware interactions across modalities, and introduces a novel use of sentiment labels-not merely as classification targets, but as semantic embeddings that guide the fusion and reasoning processes. Furthermore, LSGSL adopts a multi-task learning paradigm that jointly optimizes three objectives: image-text contrastive loss, sentiment-guided semantic similarity loss, and sentiment polarity classification loss. Extensive experiments on three widely-used benchmark datasets demonstrate that LSGSL consistently outperforms state-of-the-art methods, offering new insights into the role of social context and semantic label guidance in multimodal sentiment analysis.},
  archive      = {J_PR},
  author       = {Yun Liu and Xiaoming Zhang and Bo Zhang and Guofeng He and Ke Zhou and Zhoujun Li},
  doi          = {10.1016/j.patcog.2025.112277},
  journal      = {Pattern Recognition},
  month        = {3},
  pages        = {112277},
  shortjournal = {Pattern Recognition},
  title        = {Multimodal sentiment analysis based on label semantic guidance under social links},
  volume       = {171},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). TIPS: Two-level prompt selection for more stability-plasticity balance in continual learning. <em>PR</em>, <em>171</em>, 112276. (<a href='https://doi.org/10.1016/j.patcog.2025.112276'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent advances in prompt-based continual learning have demonstrated remarkable performance in resisting catastrophic forgetting. However, the effectiveness of these methods heavily depends on prompt selection strategy. Moreover, most existing methods overlook the model plasticity since they focus on solving the model’s stability issues, leading to a sharp decline in performance for new classes in long task sequences of incremental learning. To address these limitations, we propose a novel prompt-based continual learning method called TIPS, which mainly consists of two modules: (1) a novel two-level prompt selection strategy combined with a set of adaptive weights for sparse joint tuning, aiming to improve the accuracy of prompt selection; (2) a semantic knowledge distillation module that enhances the generalization ability to new classes by creating a language token and utilizing semantic information of class names. We validated TIPS on 4 datasets across three incremental task scenarios. TIPS surpasses or matches SOTA in all scenario settings, maintaining stable prompt selection accuracy throughout multiple incremental learning sessions. Notably, TIPS outperformed the current state-of-the-art by 2.03 %, 4.78 %, 1.18 %, and 5.59 % on CIFAR, ImageNet-R, CUB-200, and DomainNet. Our code locates at: https://github.com/gogo-l/Tips .},
  archive      = {J_PR},
  author       = {Zhikun Feng and Liang Peng and Kang Dang and Mian Zhou and Ping Kuang and MingYu Wu and Liu Yu and Jionglong Su},
  doi          = {10.1016/j.patcog.2025.112276},
  journal      = {Pattern Recognition},
  month        = {3},
  pages        = {112276},
  shortjournal = {Pattern Recognition},
  title        = {TIPS: Two-level prompt selection for more stability-plasticity balance in continual learning},
  volume       = {171},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). One framework to rule them all: Unifying multimodal tasks with LLM neural-tuning. <em>PR</em>, <em>171</em>, 112275. (<a href='https://doi.org/10.1016/j.patcog.2025.112275'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Large-scale models have exhibited remarkable capabilities across diverse domains, including automated medical services and intelligent customer support. However, as most large models are trained on single-modality corpora, enabling them to effectively process and understand multimodal signals remains a significant challenge. Current research often focuses on designing task-specific or scenario-specific tuning strategies, which limits the scalability and versatility. To address this limitation, we propose a unified framework that concurrently handles multiple tasks and modalities. In this framework, all modalities and tasks are represented as unified tokens and trained using a single, consistent approach. To enable efficient multitask processing, we introduce a novel tuning strategy termed neural tuning, inspired by the concept of sparse distributed representation in the human brain, where only specific subsets of neurons are activated for each task. Furthermore, to advance research in multimodal and multitask learning, we present a new benchmark, MMUD, which includes samples annotated with multiple task labels spanning reasoning segmentation, referring segmentation, image captioning, and text-to-image generation. By applying neural tuning to pretrained large models on the MMUD benchmark, we demonstrate the ability to handle multiple tasks simultaneously in a streamlined and efficient manner. All codes and datasets will be released at https://github.com/kiva12138/NeuralTuning .},
  archive      = {J_PR},
  author       = {Hao Sun and Yu Song and Jiaqing Liu and Jihong Hu and Yen-Wei Chen and Lanfen Lin},
  doi          = {10.1016/j.patcog.2025.112275},
  journal      = {Pattern Recognition},
  month        = {3},
  pages        = {112275},
  shortjournal = {Pattern Recognition},
  title        = {One framework to rule them all: Unifying multimodal tasks with LLM neural-tuning},
  volume       = {171},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Predictive summarization framework for resource-constrained device surveillance videos. <em>PR</em>, <em>171</em>, 112274. (<a href='https://doi.org/10.1016/j.patcog.2025.112274'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The high occurrence of crime in various cities worldwide has detrimental effects on both the victims and the communities they belong to. Although deep learning techniques are acknowledged for their effectiveness in predicting future events by analyzing past behaviors, those approaches have resulted in poor prediction accuracy for video surveillance data. To tackle this issue, a new framework called the Horned Lizard ZfNet Summarization Framework (HLZSF) was introduced in this research. The primary processes, like filtering, key frame extraction, crime events tracking, and crime prediction with classification, were performed. To filter the noise features continuously, the mathematical steps of the filtering process were processed in the hidden layer with the min-max scalar function. Moreover, the crime events tracking function is executed by processing the food-hunting behaviour of the horned lizard. In addition, the feature selection from the video frame is performed with the horned lizard skin colour changing as the best solution. The Python environment is adopted for this study to validate the video surveillance database. Here, the incorporation of the horned lizard's best solution, which is skin changing behaviour based on a specific object in the deep network, has been employed to earn the finest feature selection and prediction outcome. The accuracy attained by the novel HLZSF is 97.87 %, and the recorded F-score is 97.88 %, precision 98.01 %, and recall 97.8 %, which is the finest outcome compared to alternative models.},
  archive      = {J_PR},
  author       = {Tabiya Manzoor Beigh and Dr. V. Prasanna Venkatesan and J. Arumugam},
  doi          = {10.1016/j.patcog.2025.112274},
  journal      = {Pattern Recognition},
  month        = {3},
  pages        = {112274},
  shortjournal = {Pattern Recognition},
  title        = {Predictive summarization framework for resource-constrained device surveillance videos},
  volume       = {171},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Splitting criteria for ordinal decision trees: An experimental study. <em>PR</em>, <em>171</em>, 112273. (<a href='https://doi.org/10.1016/j.patcog.2025.112273'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Ordinal Classification (OC) addresses those classification tasks where the labels exhibit a natural order. Unlike nominal classification, which treats all classes as mutually exclusive and unordered, OC takes the ordinal relationship into account, producing more accurate and relevant results. This is particularly critical in applications where the magnitude of classification errors has significant consequences. Despite this, OC problems are often tackled using nominal methods, leading to suboptimal solutions. Although decision trees are among the most popular classification approaches, ordinal tree-based approaches have received less attention when compared to other classifiers. This work provides a comprehensive survey of ordinal splitting criteria, standardising the notations used in the literature to enhance clarity and consistency. Three ordinal splitting criteria, Ordinal Gini (OGini), Weighted Information Gain, and Ranking Impurity, are compared to the nominal counterparts of the first two (Gini and information gain), by incorporating them into a decision tree classifier. An extensive repository considering 45 publicly available OC datasets is presented, supporting the first experimental comparison of ordinal and nominal splitting criteria using well-known OC evaluation metrics. The results have been statistically analysed, highlighting that OGini stands out as the best ordinal splitting criterion to date, reducing the mean absolute error achieved by Gini by more than 3.02 % . To promote reproducibility, all source code developed, a detailed guide for reproducing the results, the 45 OC datasets, and the individual results for all the evaluated methodologies are provided.},
  archive      = {J_PR},
  author       = {Rafael Ayllón-Gavilán and Francisco José Martínez-Estudillo and David Guijo-Rubio and César Hervás-Martínez and Pedro Antonio Gutiérrez},
  doi          = {10.1016/j.patcog.2025.112273},
  journal      = {Pattern Recognition},
  month        = {3},
  pages        = {112273},
  shortjournal = {Pattern Recognition},
  title        = {Splitting criteria for ordinal decision trees: An experimental study},
  volume       = {171},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). DLPP-UL: Dose-level parameter prompted unpaired learning for low-dose CT image restoration. <em>PR</em>, <em>171</em>, 112272. (<a href='https://doi.org/10.1016/j.patcog.2025.112272'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Unpaired deep-learning (UPDL) methods have been widely studied for X-ray computed tomography (CT) imaging, and have achieved promising performances for low-dose CT (LDCT) image restoration with specific dose-level parameter setting. However, CT scans with different dose-level parameter settings (i.e., different combinations of tube currents/mAs and/or exposure views) would result in significant heterogeneity in CT images. Directly training the existing UPDL models with heterogeneous datasets would result in poor performance. To address this issue, we propose a dose-level parameter prompted unpaired learning (DLPP-UL) method, which can be effectively trained with heterogeneous datasets for LDCT image restoration. Specifically, we construct a dose-level parameter prompted (DLPP) generator and discriminator for a cycle-consistent adversarial network (CycleGAN), which can efficiently embed dose-level parameter prompting features. In addition, we share the dose-level parameter prompting features among the generators and discriminator, and design a dose-level distinguishable (DLD) training strategy for stable training the DLPP-UL model. Both simulated and real datasets with multiple dose-level parameter settings are adopted to evaluate the effectiveness of the proposed DLPP-UL method. Experimental results demonstrate that the proposed DLPP-UL model can achieve promising performance in terms of both quantitative and qualitative measurements, making it highly attractive for practical LDCT image restoration.},
  archive      = {J_PR},
  author       = {Yaoduo Zhang and Wenyu Zhang and Gaofeng Chen and Pengfei Wang and Danyang Li and Jianhua Ma and Ji He},
  doi          = {10.1016/j.patcog.2025.112272},
  journal      = {Pattern Recognition},
  month        = {3},
  pages        = {112272},
  shortjournal = {Pattern Recognition},
  title        = {DLPP-UL: Dose-level parameter prompted unpaired learning for low-dose CT image restoration},
  volume       = {171},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Recurrent submatrix feature transfer collaborative filtering via nuclear norm regularization. <em>PR</em>, <em>171</em>, 112271. (<a href='https://doi.org/10.1016/j.patcog.2025.112271'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Existing collaborative filtering algorithms often struggle with data sparsity and cold-start problems. To address these challenges, we propose a novel regularization method, named Recurrent Submatrix Feature Transfer Collaborative Filtering via Nuclear Norm Regularization (ReSFTNNR). Our approach first partitions the rating matrix into two submatrices with divergent sparsity levels. We then introduce a feature transfer method that leverages nuclear norm regularization to propagate user preferences across submatrices, thereby enhancing prediction accuracy for the sparse submatrix. Furthermore, the preference structure of the sparse submatrix is transferred to the dense submatrix to improve overall prediction accuracy. We develop a fixed-point method to solve the optimization model, which scales efficiently to large problems. Although the proposed objective function is non-convex, theoretical analysis demonstrates its convergence. Evaluations on synthetic datasets and real-world datasets such as MovieLens-1M, Douban-Book, and Amazon-CD datasets show that ReSFTNNR significantly outperforms state-of-the-art methods in recommendation accuracy.},
  archive      = {J_PR},
  author       = {Jing Wang and Cheng Yao and Hao Lin},
  doi          = {10.1016/j.patcog.2025.112271},
  journal      = {Pattern Recognition},
  month        = {3},
  pages        = {112271},
  shortjournal = {Pattern Recognition},
  title        = {Recurrent submatrix feature transfer collaborative filtering via nuclear norm regularization},
  volume       = {171},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Learning feature-enhanced multi-scale network for SE(3)-equivariant motion prediction. <em>PR</em>, <em>171</em>, 112270. (<a href='https://doi.org/10.1016/j.patcog.2025.112270'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Modeling SE(3)-equivariant motion remains a challenging task, primarily due to the absence of highly expressive features and the inherent constraints of the equivariant framework. In this paper, we introduce a novel SE(3)-equivariant model that addresses these limitations through feature enhancement, multi-scale message extraction and learnable frame transform. We further introduce a directional motion updating mechanism to capture dynamic trajectories, alongside a memory integration module that incorporates historical feature to enrich current representations. Experimental results show that these designs significantly improve the performance in SE(3)-equivariant motion prediction tasks for multi-body systems. Our model achieves state-of-the-art results across multiple benchmarks, including the Mocap human motion dataset and the MD17 and MD22 molecular dynamics datasets.},
  archive      = {J_PR},
  author       = {Nanzhe Huang and Suo Zhao and Dong Li and Ruixuan Yu},
  doi          = {10.1016/j.patcog.2025.112270},
  journal      = {Pattern Recognition},
  month        = {3},
  pages        = {112270},
  shortjournal = {Pattern Recognition},
  title        = {Learning feature-enhanced multi-scale network for SE(3)-equivariant motion prediction},
  volume       = {171},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Token-based dynamic bit-width assignment for ViT quantization. <em>PR</em>, <em>171</em>, 112269. (<a href='https://doi.org/10.1016/j.patcog.2025.112269'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Post-training quantization (PTQ) is an effective compression technique for vision transformers (ViTs). It lowers bit-widths of weights and/or activations efficiently by calibrating quantization parameters, such as a quantization interval, using a small number of training samples. We have observed that 1) dynamic ranges are different significantly across tokens, and 2) the ranges could differ depending on input instances, even for the same token. To address the scale variations in tokens, we introduce a token-based dynamic bit-width assignment method for quantization of ViTs, that adjusts the bit-width of each token dynamically adaptive to input instances. To this end, we exploit a quantization error for each token as an indicator for the bit-width adjustment. Instead of computing the quantization error for every token at runtime, which is computationally demanding, we propose to approximate the quantization error using a quantization interval. We also present a shifting quantizer specific to post-GELU activations that show an asymmetric distribution between positive and negative values. Experimental results demonstrate the effectiveness of our approach, boosting the performance of a vanilla token-wise quantization scheme drastically, with marginal computational costs.},
  archive      = {J_PR},
  author       = {Dohyung Kim and Jaehyeon Moon and Junghyup Lee and Geon Lee and Jeimin Jeon and Bumsub Ham},
  doi          = {10.1016/j.patcog.2025.112269},
  journal      = {Pattern Recognition},
  month        = {3},
  pages        = {112269},
  shortjournal = {Pattern Recognition},
  title        = {Token-based dynamic bit-width assignment for ViT quantization},
  volume       = {171},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). CM-CCL: Collaborative multi-scale concept-cognitive learning for knowledge discovery. <em>PR</em>, <em>171</em>, 112268. (<a href='https://doi.org/10.1016/j.patcog.2025.112268'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Concept-cognitive learning (CCL) provides an effective method for representing knowledge in data, with the use of concepts as knowledge carriers being its most significant characteristic. However, existing CCL models neglect the utilization of multi-scale information, resulting in insufficient representation capabilities of the learned concepts. Therefore, this paper proposes a novel multi-scale concept-cognitive learning model to address this issue. Firstly, a rational multi-scale data construction method is provided based on the characteristics of CCL. Then, a multi-scale feature selection method is introduced, which considers both the inter-scale correlations and intra-scale class distances. On this basis, progressive concepts are learned by integrating similar granular concepts at each scale to explicitly represent the knowledge in the data. Furthermore, a mechanism for the collaboration among progressive concepts at different scales is proposed to complete the classification task. Finally, a series of experiments are conducted to validate the effectiveness of the proposed CM-CCL model.},
  archive      = {J_PR},
  author       = {Weihua Xu and Jinbo Wang and Qinghua Zhang},
  doi          = {10.1016/j.patcog.2025.112268},
  journal      = {Pattern Recognition},
  month        = {3},
  pages        = {112268},
  shortjournal = {Pattern Recognition},
  title        = {CM-CCL: Collaborative multi-scale concept-cognitive learning for knowledge discovery},
  volume       = {171},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Improving uncertainty estimation in deep neural networks by modeling class and instance uncertainty. <em>PR</em>, <em>171</em>, 112267. (<a href='https://doi.org/10.1016/j.patcog.2025.112267'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the pursuit of enhancing the trustworthiness of deep learning models, there has been a growing interest in improving their uncertainty estimation for more reliable decision-making. Recent methodologies have concentrated on refining uncertainty estimation particularly in the context of identifying out-of-distribution input samples. This work introduces and explores two distinct categories of uncertainty: class uncertainty and instance uncertainty . The former highlights uncertainty stemming from inter-class resemblances, while the latter addresses analogous uncertainty within individual instances. We propose a novel framework, Adaptive Similarity Labeling (ASL), that captures both class-level and instance-level uncertainty by adaptively assigning soft labels based on semantic similarity and instance difficulty. ASL mitigates feature collapse and improves uncertainty estimation without adding inference-time overhead, and can be used regardless of the uncertainty metric used. The proposed approach is architecture-agnostic and can be combined with recent state-of-the-art architectures for uncertainty estimation. Experiments conducted on several datasets demonstrate the effectiveness of ASL in improving uncertainty estimation and out-of-distribution detection across diverse tasks.},
  archive      = {J_PR},
  author       = {Dimitrios Spanos and Nikolaos Passalis and Anastasios Tefas},
  doi          = {10.1016/j.patcog.2025.112267},
  journal      = {Pattern Recognition},
  month        = {3},
  pages        = {112267},
  shortjournal = {Pattern Recognition},
  title        = {Improving uncertainty estimation in deep neural networks by modeling class and instance uncertainty},
  volume       = {171},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). MMIFprompt: Efficient modality-aware prompting for multimodal medical image fusion. <em>PR</em>, <em>171</em>, 112266. (<a href='https://doi.org/10.1016/j.patcog.2025.112266'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multimodal Medical Image Fusion (MMIF) integrates complementary information from different imaging modalities to provide a more comprehensive and accurate understanding of a patient’s pathological condition. However, most existing methods rely on complex architectures, making it challenging to achieve a balance between performance and efficiency, especially with limited medical datasets. In this work, we propose MMIFprompt, a prompt-based fine-tuning approach that guides a Vision Foundation Model (VFM) using minimal trainable parameters for optimal fusion results. Our approach incorporates a novel Mixture of Expert Selection (MES) mechanism that adapts features through content-guided prompt generation using a modality expert dictionary, effectively adapting the pretrained VFM to MMIF while mitigating cross-modality interference. To further enhance knowledge transfer, we design two types of prompts: Bidirectional Common Prompts (BCP) to extract modality-shared information and Unidirectional Modality-Specific Prompts (UMSP) to preserve modality-specific features. Without full model fine-tuning, MMIFprompt achieves superior fusion quality with only 7.1M trainable parameters-just 8.7 % of the backbone’s parameters. Notably, it offers strong generalization across diverse medical image fusion tasks and can be seamlessly integrated into various VFMs as a plug-and-play module, maintaining excellent fusion performance. Our code is available at https://github.com/weilisi1125/MMIFprompt .},
  archive      = {J_PR},
  author       = {Lisi Wei and Xihang Hu and Kai Diao and Na Ta and Yunjie Tu and Xiaoli Zhang},
  doi          = {10.1016/j.patcog.2025.112266},
  journal      = {Pattern Recognition},
  month        = {3},
  pages        = {112266},
  shortjournal = {Pattern Recognition},
  title        = {MMIFprompt: Efficient modality-aware prompting for multimodal medical image fusion},
  volume       = {171},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Skea-topo: A skeleton-aware loss function for topologically accurate boundary segmentation. <em>PR</em>, <em>171</em>, 112265. (<a href='https://doi.org/10.1016/j.patcog.2025.112265'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Topological consistency is crucial in boundary segmentation tasks for reticular images, such as those in neuron electron microscopy, aerial road imaging, and material microscopic imaging. Misalignments in segmentation results frequently bring about significant topological errors, which can exert a more detrimental impact on subsequent analysis compared to pixel-level inaccuracies. Existing methods generally struggle to effectively identify and resolve topological errors, especially in complex reticular structures and irregularly shaped objects. In this study, Skea-Topo, a novel skeleton-based loss function is proposed to boost topological accuracy, so as to address these challenges. Skea-Topo comprises two key components: 1) Skeleton-Aware Weighted Loss (Skeaw), which leverages a pre-weighted graph to prioritize error-prone regions and refines weighting strategies using object skeletons for improved handling of irregular objects; 2) Boundary Rectified Term (BoRT), which identifies topological critical pixels during training and applies additional weighting to these pixels. We evaluate Skea-Topo on five diverse datasets, including SNEMI3D, IRON, MASS. ROAD, DRIVE, and the newly introduced Al-La MicroData from the materials domain. Comprehensive experiments demonstrate that Skea-Topo outperforms state-of-the-art methods, achieving a remarkable improvement of up to 7 points in the Variation of Information (VI) metric, reducing the VI score from 76.67 to 69.02. Both objective and subjective evaluations confirm the superior performance of Skea-Topo across all datasets, highlighting its effectiveness in overcoming the limitations of existing approaches. Code is available at https://github.com/clovermini/Skea_topo .},
  archive      = {J_PR},
  author       = {Chuni Liu and Boyuan Ma and Yujie Xie and Xiaojuan Ban and Haiyou Huang and Hao Wang and Weihua Xue and Ke Xu},
  doi          = {10.1016/j.patcog.2025.112265},
  journal      = {Pattern Recognition},
  month        = {3},
  pages        = {112265},
  shortjournal = {Pattern Recognition},
  title        = {Skea-topo: A skeleton-aware loss function for topologically accurate boundary segmentation},
  volume       = {171},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Adversarial spectral perturbation for single-domain generalized object detection. <em>PR</em>, <em>171</em>, 112264. (<a href='https://doi.org/10.1016/j.patcog.2025.112264'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper focuses on Single-Domain Generalized Object Detection (Single-DGOD), aiming to develop an object detector trained on a single source domain that generalizes well to multiple unseen target domains. However, learning a robust detector that relies solely on single-domain data is challenging due to limited data diversity. To address this issue, we propose an Adversarial Spectral Perturbation (AdvSP) approach to improve Single-DGOD generalization. Specifically, we first conduct a comparative analysis of the Fourier spectra of multiple images, revealing that image color, texture, and semantic content are characterized by three distinct Fourier spectral components. On this basis, we develop a spectral perturbation module (SPM) containing three learnable perturbation factors to jointly disrupt the Fourier spectra, simulating diverse variations in image attributes. Furthermore, we introduce an adversarial learning objective to optimize the SPM, and the learned SPM is then employed to construct adversarial images for detector training. Visual analysis of style features demonstrates that AdvSP greatly diversifies source images, while encouraging results on three benchmark datasets further confirm the effectiveness of AdvSP. Moreover, AdvSP features a plug-and-play design and can be easily integrated into different detection frameworks.},
  archive      = {J_PR},
  author       = {Xiangsheng Wang and Kangcheng Bin and Xiaojian Wang and Pengcheng Wan and Ting Hu and Ping Zhong},
  doi          = {10.1016/j.patcog.2025.112264},
  journal      = {Pattern Recognition},
  month        = {3},
  pages        = {112264},
  shortjournal = {Pattern Recognition},
  title        = {Adversarial spectral perturbation for single-domain generalized object detection},
  volume       = {171},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Multi-modal graph-aware pre-training for disease classification and cross-modal retrieval in chest radiology. <em>PR</em>, <em>171</em>, 112263. (<a href='https://doi.org/10.1016/j.patcog.2025.112263'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent advances in medical foundation models have demonstrated significant potential for chest X-ray (CXR) analysis, particularly through vision-language pre-training. However, critical challenges remain, including the scarcity of well-annotated multi-modal datasets and the difficulty in establishing precise anatomical region-to-text alignment due to unstructured clinical reports. To address these limitations, we propose a novel graph-based foundation model for CXR, named Graph-Aware Vision-Language Pre-training (GAVLP), which introduces two key innovations. First, our structured data generation module leverages the large language model (LLM) to transform sparse disease labels into richly annotated, structured semantic descriptions, effectively bridging the gap between limited clinical annotations and the need for comprehensive training supervision. Second, our Region-wise Graph Learning (RGL) framework constructs a global-to-regional graph topology to model hierarchical relationships between CXR regions and their clinical semantics. By treating region-specific features as relational nodes within a graph convolutional network (GCN), we achieve fine-grained alignment of visual and textual representations while preserving anatomical consistency. Extensive experiments on standard medical benchmarks demonstrate that GAVLP significantly outperforms state-of-the-arts methods in key downstream tasks, including zero-shot classification and cross-modal retrieval. Our framework advances multi-modal representation learning by unifying visual and non-visual features in a graph-structured space, offering enhanced interpretability and clinical applicability.},
  archive      = {J_PR},
  author       = {Ruibin Chen and Weidong Min and Qing Han and Jiahao Li and Longfei Li and Haifan Wu},
  doi          = {10.1016/j.patcog.2025.112263},
  journal      = {Pattern Recognition},
  month        = {3},
  pages        = {112263},
  shortjournal = {Pattern Recognition},
  title        = {Multi-modal graph-aware pre-training for disease classification and cross-modal retrieval in chest radiology},
  volume       = {171},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Attention-assisted multilevel fusion framework for generalized iris presentation attack detection. <em>PR</em>, <em>171</em>, 112262. (<a href='https://doi.org/10.1016/j.patcog.2025.112262'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Iris presentation attack detection (IPAD) is crucial for the security of iris recognition (IR) systems, especially as application scenarios and privacy concerns increase. However, most existing IPAD methods still face two major challenges: 1) effectively detecting both global attacks and local attacks; and 2) performance degradation due to cross-domain variations. Additionally, the model size and memory requirements must be considered for real-world deployment. To address these challenges, we propose an Attention-assisted Multilevel Fusion framework for IPAD, named AMF-IPAD. The proposed framework first employs an iris mask-guided attention module to separately extract ocular features for detecting global attacks and iris features for detecting local attacks. We then introduce an attention-assisted multilevel fusion strategy that systematically integrates complementary information across the image, feature, and score levels for robust iris liveness classification inference. Finally, a weighted joint loss function, combining pixel-wise supervision and self-distillation, is used to train the model effectively. The experimental results show that the proposed method achieves state-of-the-art performance on four publicly available datasets ( e.g. , LivDet-Iris 2017) under various protocols, demonstrating its superior generalization in cross-domain scenarios, while being able to effectively detect diverse attack types.},
  archive      = {J_PR},
  author       = {Caiyong Wang and Lin Li and Fukang Guo and Haiyu Wang and Zhaofeng He and Zhenan Sun},
  doi          = {10.1016/j.patcog.2025.112262},
  journal      = {Pattern Recognition},
  month        = {3},
  pages        = {112262},
  shortjournal = {Pattern Recognition},
  title        = {Attention-assisted multilevel fusion framework for generalized iris presentation attack detection},
  volume       = {171},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Reasoning step by step via a neural-symbolic geometry problem solver. <em>PR</em>, <em>171</em>, 112261. (<a href='https://doi.org/10.1016/j.patcog.2025.112261'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Geometry problem solving (GPS) is a challenging task that requires comprehensive multimodal geometry representation and reasoning capabilities. Existing approaches still have limitations, primarily due to their insufficient understanding of multimodal geometry context and their heavy reliance on costly solution programs for interpretability. Therefore, we propose a Neural-Symbolic Geometry Problem Solver (NS-GPS), which performs step-by-step reasoning to generate the intermediate solution programs with the answers as guidance. Drawing upon the dual process theory in cognitive science, NS-GPS integrates a progressive fusion module for robust multimodal semantic understanding of the geometry diagram and text problem, as well as a neural-symbolic reasoning module for step-wise sub-program generation and execution. Additionally, the intermediate executed results of the sub-programs provide training feedback by comparing with the answer to promote solution program generation. Extensive experiments on four datasets demonstrate that NS-GPS achieves competitive performance to the state-of-the-arts, while achieving interpretability with less cost.},
  archive      = {J_PR},
  author       = {Yaxian Wang and Bifan Wei and Yinghong Ma and Xudong Jiang and Henghui Ding and Zhongmin Cai and Jun Liu},
  doi          = {10.1016/j.patcog.2025.112261},
  journal      = {Pattern Recognition},
  month        = {3},
  pages        = {112261},
  shortjournal = {Pattern Recognition},
  title        = {Reasoning step by step via a neural-symbolic geometry problem solver},
  volume       = {171},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Temporal-invariant video contrastive learning: A novel perspective from brain knowledge. <em>PR</em>, <em>171</em>, 112256. (<a href='https://doi.org/10.1016/j.patcog.2025.112256'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Contrastive learning, as a powerful self-supervised learning method, has made significant strides for images. However, previous studies for videos concentrate on practical performance improvements and yield remarkable results, while still lacking more in-depth theoretical exploration behind these advancements. Especially in light of the additional temporal dimension in videos compared to images, the importance of developing a novel perspective to rethink introducing new knowledge for dynamic-video is highlighted, which further supports the improvement of the learned representations. Therefore, this paper aims to explore a novel intellectual perspective, which injects new insights into video contrastive learning to further promote technical progress. For that, we delve into brain knowledge to redefine the essential temporal information (called temporal-invariant information) and temporal-redundancy in videos, subsequently exploring the construction of temporal views inspired by the attentional modulation mechanism as Temporal-invariant Video Contrastive Learning (TiVCL). Specifically, four temporal-invariant strategies are proposed to disturb temporal-redundancy, corresponding to distinct cognitive characteristics in human top-down attention mechanism (i.e., attention competition, perceptual constancy, motion association, and contour integration), providing preliminary validation for the heuristic value of single cognitive characteristics. Furthermore, we synergistically integrate these four strategies into an integration scheme, demonstrating the crucial role of multi-cognitive characteristics collaboration in extracting temporal-invariant information in video contrastive learning. Empirical experiments on multiple video benchmarks are performed to validate the optimal parameters of the temporal views generated through both individual strategies and the integration scheme in the proposed TiVCL. Moreover, excellent results across different video tasks also demonstrate the superior performance of TiVCL over advanced video methods. This study attempts to draw a more expansive picture for motivating more promising brain-inspired works for videos.},
  archive      = {J_PR},
  author       = {Wei Lin and Changxing Jing and Xinghao Ding and Huanqiang Zeng},
  doi          = {10.1016/j.patcog.2025.112256},
  journal      = {Pattern Recognition},
  month        = {3},
  pages        = {112256},
  shortjournal = {Pattern Recognition},
  title        = {Temporal-invariant video contrastive learning: A novel perspective from brain knowledge},
  volume       = {171},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Unpaired overwater image defogging using inverted dark channel prior-guided cycle-consistent generative adversarial network. <em>PR</em>, <em>171</em>, 112255. (<a href='https://doi.org/10.1016/j.patcog.2025.112255'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Existing image defogging methods have made significant advancements. However, these approaches are primarily optimized for land scenes, resulting in suboptimal performance when applied to overwater images due to the distinct characteristics of overwater scenes. In this paper, we propose an inverted dark channel Prior-Guided Cycle-consistent Generative Adversarial Network (PG-CycleGAN) for overwater image defogging. Specifically, an inverted dark channel prior map is designed to suppress the sky and highlight objects over the water. Building on this prior map, we develop a prior encoder to extract object-related features. Additionally, we propose a Prior-Guided Residual Block (PGRB) and a Prior-Guided TriUpsample (PGTU) module, which effectively integrate the extracted prior features for both feature encoding and upsampling. This integrated approach enhances the network’s ability to accurately restore overwater objects, leading to improved defogging performance. Furthermore, we develop a prior map-guided GAN loss and a prior map-guided cycle-consistency loss, which guide the network to recover objects with greater fidelity while minimizing unnecessary restoration of the sky region. Through extensive experimental comparisons, our method demonstrates superior performance over existing state-of-the-art approaches in terms of qualitative analysis, quantitative metrics, and improvements in object detection.},
  archive      = {J_PR},
  author       = {Yaozong Mo and Chaofeng Li and Tuxin Guan and Qiuping Jiang and Wenqi Ren and Wenwu Wang and Xiao-Jun Wu},
  doi          = {10.1016/j.patcog.2025.112255},
  journal      = {Pattern Recognition},
  month        = {3},
  pages        = {112255},
  shortjournal = {Pattern Recognition},
  title        = {Unpaired overwater image defogging using inverted dark channel prior-guided cycle-consistent generative adversarial network},
  volume       = {171},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). MA-neck: Mutual attention-based feature enhancement for lightweight object detection. <em>PR</em>, <em>171</em>, 112254. (<a href='https://doi.org/10.1016/j.patcog.2025.112254'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Lightweight networks are essential for real-time object detection. However, most existing models reduce the complexity primarily by decreasing the network’s depth, which hinders the extraction of rich semantic information and results in insufficient accuracy when detecting diverse objects. To address this limitation, this paper proposes an effective mutual attention-based neck (MA-Neck) to leverage the inherent correlations among features from different network stages. The MA-Neck integrates a co-attention mechanism to enhance state-of-the-art lightweight networks, primarily focusing on learning discriminative information between semantic and spatial feature representations. Then, a self-attention enhancement (SAE) module is developed to strengthen the network’s ability to learn foreground representations. Furthermore, we propose a mutual graph channel attention (MGCA) module that aggregates discriminative features and facilitates efficient and effective stages for capturing global correlations. Extensive experiments are implemented on various benchmark datasets to evaluate the proposed method. The experimental results demonstrate the effectiveness and superiority of the proposed method over state-of-the-art alternatives.},
  archive      = {J_PR},
  author       = {Dongxu Cheng and Hao Li and Zifang Zhou and Yan Yang},
  doi          = {10.1016/j.patcog.2025.112254},
  journal      = {Pattern Recognition},
  month        = {3},
  pages        = {112254},
  shortjournal = {Pattern Recognition},
  title        = {MA-neck: Mutual attention-based feature enhancement for lightweight object detection},
  volume       = {171},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Self cycle strategy for unpaired visible-to-infrared image translation. <em>PR</em>, <em>171</em>, 112253. (<a href='https://doi.org/10.1016/j.patcog.2025.112253'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Image-to-Image (I2I) translation is an important approach for addressing modality differences and enriching data. However, it overlooks the reexamination of generated images. This paper proposes a generative adversarial network-based image translation algorithm, SelfCycle, that functions a one-sided unpaired image translation method. SelfCycle leverages the implicit discriminative power of a generative network to reexamine generated images, thus narrowing the gap between these images and target-domain images. For the visible-to-infrared image translation task, we propose the IRSelfCycle algorithm, which includes a convolutional fusion visual state space model capable of obtaining global features without requiring excessive computational resources while retaining local detail information. Experiments on two I2I datasets demonstrate the advantages of the SelfCycle framework, and experiments on two visible-infrared image datasets illustrate the advantages of IRSelfCycle, which achieves state-of-the-art performance. Our source code will be available at https://github.com/447425299/SelfCycle .},
  archive      = {J_PR},
  author       = {Decao Ma and Juan Su and Bing Li and Yong Xian and Shaopeng Li and Yao Ding},
  doi          = {10.1016/j.patcog.2025.112253},
  journal      = {Pattern Recognition},
  month        = {3},
  pages        = {112253},
  shortjournal = {Pattern Recognition},
  title        = {Self cycle strategy for unpaired visible-to-infrared image translation},
  volume       = {171},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). A recurrent-attention mechanism for medical image segmentation. <em>PR</em>, <em>171</em>, 112252. (<a href='https://doi.org/10.1016/j.patcog.2025.112252'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Accurate medical image segmentation is crucial for computer-aided clinical diagnosis, treatment planning, and intervention. Deep learning advancements have led to the widespread use of Self-Attention (SA) based expert models in tasks such as X-ray, CT, and MRI segmentation due to the ability to model global dependencies. However, the design of SA does not specifically cater to detailed feature modeling, leading to mis-segmentation, especially with tiny or morphologically irregular organs or lesions. Additionally, diverse medical imaging modalities and diseases require expert knowledge to construct different architectures, limiting generalizability. Therefore, in this study, we first elucidated and demonstrated the critical role of feature interaction capability in enhancing the fine segmentation performance of expert models in medical image segmentation tasks. Then, we innovatively designed a general and robust attention mechanism named Recurrent-Attention (RecA). RecA builds a pipeline that continuously maintains and passes the processing feature flow within the model, covering the global dependency modeling capability from a single-layer feature map to the whole model, enhancing the feature interaction ability. Finally, Performance evaluations on two widely used 2D and three 3D public datasets validated RecA's effectiveness. The results showed that RecA can seamlessly replace SA in existing expert models, significantly improving performance without increasing parameter size or computational complexity. RecA exhibits outstanding robustness, independent of expert models and input image modalities. The introduction of RecA offers a new and efficient solution for detailed feature modeling in medical image segmentation, with the potential to become a foundational attention mechanism in future segmentation models.},
  archive      = {J_PR},
  author       = {Guorun Li and Lei Liu and Yuefeng Du and Peng Liu and Xiaoyu Li and Taiguo Qi and Zhi Qiao and Du Chen and Zhenghe Song},
  doi          = {10.1016/j.patcog.2025.112252},
  journal      = {Pattern Recognition},
  month        = {3},
  pages        = {112252},
  shortjournal = {Pattern Recognition},
  title        = {A recurrent-attention mechanism for medical image segmentation},
  volume       = {171},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Bayesian classifier calibration based on synthesized samples for zero-shot chinese character recognition. <em>PR</em>, <em>171</em>, 112251. (<a href='https://doi.org/10.1016/j.patcog.2025.112251'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Zero-shot character recognition aims to recognize unseen characters that have never appeared in training, utilizing auxiliary information like radical descriptions or printed templates. Zero-shot recognition is important for Chinese characters because of the huge number of character classes and the inability to pre-collect training samples for all classes. For zero-shot Chinese character recognition (ZSCCR), previous methods learn a cross-modal alignment between the character samples and the auxiliary information, and then employ it to recognize unseen characters. These methods ignore two issues: domain shift implying that the alignment learned from seen characters deviates on unseen characters, and anisotropic Gaussian distribution implying that the feature distribution of each unseen character exhibits highly variable variance across different directions. To alleviate these issues, we propose to synthesize unseen character samples to build a Gaussian density-based Bayesian classifier in the feature space, and calibrate the classifier to fit into the true feature distributions of unseen characters. To assist the Bayesian classifier calibration, a cross-modal prototype classifier and a conditional generator are first trained on seen characters. The parameters of unseen classes in the Bayesian classifier are estimated on the features of synthesized data and calibrated by interpolation, smoothing, and regularization. The calibration helps reduce the bias of estimated parameters to mitigate the domain shift. Experiments on both a handwritten Chinese character dataset and a natural scene character dataset show the effectiveness and superiority of our proposed method. The code of this method has been released at https://github.com/bad-meets-joke/BCCSS},
  archive      = {J_PR},
  author       = {Xiang Ao and Xiao-Hui Li and Xu-Yao Zhang and Cheng-Lin Liu},
  doi          = {10.1016/j.patcog.2025.112251},
  journal      = {Pattern Recognition},
  month        = {3},
  pages        = {112251},
  shortjournal = {Pattern Recognition},
  title        = {Bayesian classifier calibration based on synthesized samples for zero-shot chinese character recognition},
  volume       = {171},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Targeted attack via adversarial patch outside bounding box. <em>PR</em>, <em>171</em>, 112244. (<a href='https://doi.org/10.1016/j.patcog.2025.112244'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, untargeted physical adversarial attacks against object detectors in the field of autonomous driving have been widely studied. Compared with untargeted attacks, targeted attacks offer more precise control over the detector’s output and pose a greater threat, yet they have rarely been studied. Meanwhile, adversarial patches located outside bounding box offer greater stealthiness and lower deployment difficulty. However, achieving targeted physical attacks outside bounding box is a non-trivial problem. To address this problem, this paper proposes a novel targeted attack framework by leveraging a multi-task collaborative parallel optimization strategy. Specifically, the framework decomposes the attack process into two subtasks optimized in parallel: (1) Eliminating perception of the object. This subtask aims to help the object evade detection. We propose a loss function that combines global semantic representations with local intermediate layer features to achieve this. (2) Reconstructing an erroneous perception of the object. This subtask aims to force the model to output a preset bounding box and class label. We propose a loss function that establishes an incorrect spatial association between the adversarial patch and the original object to achieve this. The proposed method finally tested against three state-of-the-art cnn based object detectors: Yolov8, Yolov9, and Yolov11. Results show that our adversarial patch achieves up to 78.1 % targeted attack success rate, outperforming the compared methods by as much as 77.7 %. More importantly, we verified the effectiveness of our attack in the physical world. The patch remains effective across different devices, viewing angles, and distances. We hope that our proposed method can serve as a baseline for targeted physical attacks conducted outside the object bounding box. Our code is available at: https://github.com/mayfly227/TAOB .},
  archive      = {J_PR},
  author       = {Kang Deng and Qixiang Chen and Yu Zhang and Zhi Lin and Shenjian Gong and Zhenyu Liang and Anjie Peng and Xing Yang and Defu Lian},
  doi          = {10.1016/j.patcog.2025.112244},
  journal      = {Pattern Recognition},
  month        = {3},
  pages        = {112244},
  shortjournal = {Pattern Recognition},
  title        = {Targeted attack via adversarial patch outside bounding box},
  volume       = {171},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). GBK-DPC: Density peak clustering based on granular ball with K-nearest neighbor. <em>PR</em>, <em>171</em>, 112243. (<a href='https://doi.org/10.1016/j.patcog.2025.112243'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Density Peak Clustering (DPC) is a density-based clustering algorithm that selects cluster centers based on local density and relative distance. However, existing Granular Ball (GB)-based DPC methods often rely on threshold-based, one-pass GB construction, which limits the quality of generated GBs, particularly in boundary regions. Moreover, GB density is typically estimated solely from internal compactness, which fails to reflect its global representativeness. These limitations cause suboptimal center identification. To address this, we propose a Density Peak Clustering based on Granular Ball with K-Nearest Neighbor (GBK-DPC), which introduces an adaptive optimization mechanism to divide large-radius GBs, enhancing boundary resolution. It further defines GB density using a dual-factor estimation that incorporates both internal compactness and external neighborhood density. A dynamic allocation strategy is introduced to allocate non-central points only when sufficient neighboring GB information is available, thereby improving fault tolerance. Experiments on 23 datasets demonstrate that GBK-DPC achieves excellent clustering performance.},
  archive      = {J_PR},
  author       = {Yongting Ni and Jin Qian and Enhong Chen and Shaowei Yan and Jiangang Ye},
  doi          = {10.1016/j.patcog.2025.112243},
  journal      = {Pattern Recognition},
  month        = {3},
  pages        = {112243},
  shortjournal = {Pattern Recognition},
  title        = {GBK-DPC: Density peak clustering based on granular ball with K-nearest neighbor},
  volume       = {171},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). CORE-CLIP: Smart collaborative reasoning driven by CLIP for human-object interaction detection. <em>PR</em>, <em>171</em>, 112242. (<a href='https://doi.org/10.1016/j.patcog.2025.112242'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Human-object interaction (HOI) detection has attracted more and more attention due to its wide potential applications. Recently, Contrastive Language-Image Pre-training (CLIP) achieves promising results in 2D/3D zero-shot and few-shot learning. To overcome current heavy reliance on the large collection of annotated HOI data and often failure to recognize such unseen HOI relationships in training datasets in existing methods, we propose a smart collaborative reasoning framework named CORE-CLIP based on semantic enhancement driven by the openness and robustness of CLIP. Specifically, the text guided dual fusion attention module (TDA) captures precise interaction patterns by progressively integrating CLIP text embeddings, CLIP visual features, and global contextual features. The semantically enhanced explicit interaction classification module (SEC) is initialized by leveraging semantic information of objects, actions, and interactions generated through CLIP text embeddings, to ensure alignment between visual features and linguistic semantics. Extensive experiments demonstrate that CORE-CLIP has state-of-the-art performance on two benchmark datasets. In particular, compared to GEN-VLKT, the proposed CORE-CLIP improves 1.36 mAP on V-COCO and 1.01 mAP on HICO-DET. Zero-shot experimental results outperform state-of-the-art methods GEN-VLKT by a signifigant margin of 34.4 % in UO type setting and HOICLIP by 50.9 % in UO type setting, respectively. The inference speed of CORE-CLIP surprisingly attains 38.3 FPS while its total parameters are only 52.9M.},
  archive      = {J_PR},
  author       = {Yuequan Yang and Haojun Zhang and Zhiqiang Cao and Jiarui Hong and Junzhi Yu and Xu Wang},
  doi          = {10.1016/j.patcog.2025.112242},
  journal      = {Pattern Recognition},
  month        = {3},
  pages        = {112242},
  shortjournal = {Pattern Recognition},
  title        = {CORE-CLIP: Smart collaborative reasoning driven by CLIP for human-object interaction detection},
  volume       = {171},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Robust locality regularized non-negative matrix factorization with structure preservation for image classification. <em>PR</em>, <em>171</em>, 112241. (<a href='https://doi.org/10.1016/j.patcog.2025.112241'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Despite the empirical success of non-negative matrix factorization (NMF) methods based on graph embedding, there are still some deficiencies, such as: 1) They are sensitive to affinity matrix due to encoding inherent structure by a self-defined graph cannot perfectly accommodate different underlying distributions of various datasets; 2) There is no comprehensive consideration of global and local structures; 3) The structural consistency of the feature space, the representation space and the original space are not persisted. Hence, we propose a novel model, i.e. robust locality regularized non-negative matrix factorization with structure preservation (RLNMF-SP), to tackle the above issues. Firstly, the low-rank attribute is applied to suppress the negative effects of noise and occlusion on the model, while obtaining a characterization of the global structure of the sample. Secondly, the Euclidean distance is employed to automatically assign appropriate neighborhoods to each data point, which reach to explicitly grasp the geometric topology without human intervention. Finally, graph embedding and positional constraint criteria are adopted to achieve structure preservation. Specifically, the learned similarity matrix is adopted to maintain the neighbor relationship invariant in the representation space, and hold the compactness within classes and separability between classes in the feature space. Numerous experiments have shown that this model exhibits excellent performance on interference-free, noise-containing, occlusion-containing, and mixed noise and occlusion datasets, and its recognition rate is on average 1–4 % higher than that of other models.},
  archive      = {J_PR},
  author       = {Minghua Wan and Ying Zhao and Jun Yin and Chengli Sun and Guowei Yang},
  doi          = {10.1016/j.patcog.2025.112241},
  journal      = {Pattern Recognition},
  month        = {3},
  pages        = {112241},
  shortjournal = {Pattern Recognition},
  title        = {Robust locality regularized non-negative matrix factorization with structure preservation for image classification},
  volume       = {171},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Cross-modal generalizable visual-language models via inter-modal bidirectional supervision for enhanced pathology image recognition. <em>PR</em>, <em>171</em>, 112240. (<a href='https://doi.org/10.1016/j.patcog.2025.112240'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Microscopic-level digital pathology is the gold standard for cancer diagnosis, staging, and postoperative prognosis. In pathology image recognition, clinical descriptions—such as cell morphology, mitotic activity, and nuclear characteristics—play a crucial role in diagnosis alongside image features. Recent advancements in vision-language models (VLMs) have improved pathology image analysis by incorporating cue words to refine feature extraction. Integrating physicians' cues has further enhanced interpretability, fostering greater trust among clinicians and patients. However, most of current VLM methodologies depend on the assumption of perfect alignment between features across the pathology image and cue word modalities, compressing multimodal features into a single vector. This approach weakens predictive capabilities and robustness, leading to inaccuracies when inputs are misaligned. As a solution, this work introduces a novel VLM that utilizes paired multimodal data to learn unified discrete representations, enabling effective cross-modal generalization for downstream tasks. The model employs Information Co-Compression (ICC) and Cross-Exponential Moving Average (CEMA) to capture fine-grained features within paired data, significantly improving cross model features retrieval ability. Experimental results on a bladder cancer pathology image database demonstrate a prediction accuracy of 90.51 % and an F1-score of 92.5 %. The model also generalizes well to a stomach adenocarcinoma dataset, achieving 92.68 % accuracy and surpassing state-of-the-art (SOTA) models by 6.28 %. Additionally, testing on a natural image dataset yielded an accuracy of 83.83 %, outperforming baseline model by 13.52 %. These results highlight the model’s superior predictive performance and its potential to enhance trust and decision-making in real-world medical applications.},
  archive      = {J_PR},
  author       = {Lingxuan Hou and Yan Zhuang and Yuhua Xie and Hangyi Kan and Zhiwei Huang and Jiangli Lin},
  doi          = {10.1016/j.patcog.2025.112240},
  journal      = {Pattern Recognition},
  month        = {3},
  pages        = {112240},
  shortjournal = {Pattern Recognition},
  title        = {Cross-modal generalizable visual-language models via inter-modal bidirectional supervision for enhanced pathology image recognition},
  volume       = {171},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). STGFormer: Spatio-temporal GraphFormer for 3D human pose estimation in video. <em>PR</em>, <em>171</em>, 112239. (<a href='https://doi.org/10.1016/j.patcog.2025.112239'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Current methods for video-based 3D human pose estimation have achieved significant progress. However, they still face pressing challenges, such as the underutilization of spatio-temporal body structure features in transformers and the inadequate granularity of spatio-temporal interaction modeling in graph convolutional networks, which leads to pervasive depth ambiguity in monocular 3D human pose estimation. To address these limitations, this paper presents the Spatio-Temporal GraphFormer framework (STGFormer) for 3D human pose estimation in videos. First, we propose a Spatio-Temporal criss-cross Graph (STG) attention mechanism that better leverages inherent human body graph priors across sequential data and captures long-range spatio-temporal dependencies more effectively. Next, we introduce a dual-path Modulated Hop-wise Regular GCN (MHR-GCN) to independently process temporal and spatial dimensions, preserving both rich temporal-dynamic features and high-dimensional spatial representations. Furthermore, the module leverages modulation to optimize parameter efficiency and incorporates spatio-temporal hop-wise skip connections to capture higher-order information. Finally, we demonstrate that our method achieves state-of-the-art performance on the Human3.6M and MPI-INF-3DHP datasets.},
  archive      = {J_PR},
  author       = {Yang Liu and Zhiyong Zhang},
  doi          = {10.1016/j.patcog.2025.112239},
  journal      = {Pattern Recognition},
  month        = {3},
  pages        = {112239},
  shortjournal = {Pattern Recognition},
  title        = {STGFormer: Spatio-temporal GraphFormer for 3D human pose estimation in video},
  volume       = {171},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). ADA framework for unsupervised domain adaptation person re-identification. <em>PR</em>, <em>171</em>, 112238. (<a href='https://doi.org/10.1016/j.patcog.2025.112238'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Domain shift remains a critical barrier for generalizing person re-identification (ReID) models across datasets. To address this challenge, we present a sparse self-Attention augmented Domain Adaptation (ADA) framework that learns domain-invariant identity features through three key innovations: (1) Sandwich Attention Primitive (SAP), a novel computational unit designed to boost primitive-level domain adaptation. (2) Sparse self-Attention Augmented Bottleneck block (SAAB block), a hierarchical block integrating SAP to enhance adaptation at the architecture level. (3) Scalable Design, if necessary, SAAB block can be flexibly cascaded to construct task-specific ADA framework. Experiments on three benchmarks validate ADA’s superiority: (1) Achieves state-of-the-art performance across domains (e.g., 16.5 % mAP gain on CUHK03 → Market-1501). (2) Demonstrates consistent generalizability and adaptability.},
  archive      = {J_PR},
  author       = {Wei Zhang and Peijun Ye and Dihu Chen and Tao Su},
  doi          = {10.1016/j.patcog.2025.112238},
  journal      = {Pattern Recognition},
  month        = {3},
  pages        = {112238},
  shortjournal = {Pattern Recognition},
  title        = {ADA framework for unsupervised domain adaptation person re-identification},
  volume       = {171},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Macro-expression-guided micro-expression recognition: A motion similarity perspective. <em>PR</em>, <em>171</em>, 112237. (<a href='https://doi.org/10.1016/j.patcog.2025.112237'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Micro-expression recognition (MER) is a challenging task due to the subtle and short-lived facial muscle movements involved. Macro-expressions, in contrast, are more evident and easy to recognize. Yet, both expressions share similar facial muscles to express the same emotions. We exploit this observation to propose a novel Macro-expression guidance network (MAG) that uses motion similarity to aid MER. The MAG has four key components: 1) motion vectorization, which transforms facial motion into a vector tensor representation to capture motion dynamics; 2) nonlinear amplification, which enhances the intensity of micro-expression features to make them more salient; 3) macro-micro matching, which aligns macro- and micro-expressions with the highest motion similarity to achieve a one-to-one mapping between the two modalities; and 4) guidance mechanism, which enables macro-expressions to guide the extraction of micro-expression features using convolutional operations. We perform extensive experiments on 7 datasets under 3 benchmarks and demonstrate that MAG outperforms state-of-the-art methods for MER.},
  archive      = {J_PR},
  author       = {Chenquan Gan and Junhao Xiao and Qingyi Zhu and Ye Zhu},
  doi          = {10.1016/j.patcog.2025.112237},
  journal      = {Pattern Recognition},
  month        = {3},
  pages        = {112237},
  shortjournal = {Pattern Recognition},
  title        = {Macro-expression-guided micro-expression recognition: A motion similarity perspective},
  volume       = {171},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Precision at scale: Domain-specific datasets on-demand. <em>PR</em>, <em>171</em>, 112236. (<a href='https://doi.org/10.1016/j.patcog.2025.112236'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent self-supervised learning methods rely on massive general-domain datasets for robust model pretraining. However, these datasets may lack specificity required in specialized domains. Collecting large, supervised datasets to compensate for this limitation is also cumbersome. This raises a key question: Can automatically crafted domain-specific datasets serve as efficient and effective SSL pretrainers, performing comparable to—or even surpassing—much larger state-of-the-art general-domain datasets? To address this challenge, we propose Precision at Scale (PaS) , a novel modular pipeline for automatic creation of domain-specific datasets on-demand. PaS leverages Large Language Models (LLMs) and Vision-Language Models (VLMs) through three distinct phases: Concept Generation, where LLMs identify relevant domain concepts; Image Collection, utilizing VLMs and Generative models to gather appropriate images; Data Curation, ensuring quality and relevance by eliminating unrelated or redundant images. We conduct extensive experiments across three complex domains — food, insects, and birds — proving that PaS datasets compete and often surpass existing domain-specific datasets in diversity, scale, and effectiveness as pretrainers. Models pretrained on PaS datasets outperform those trained on large-scale general-domain datasets (ImageNet-1K) by up to 21 % and surpass same-scale domain-specific datasets by 6.7 % across classification tasks. Notably, despite being an order of magnitude smaller, PaS datasets outperform ImageNet-21K pretraining, with improvements of 3.3 % in fine-tuning and 9.5 % in few-shot learning, and showing superior performance on specialized dense tasks. Furthermore, by efficiently fine-tuning pretrained VLMs like CLIP and SigLIP using low-rank methods, we achieve performance gains (+4.2 % over CLIP) in specialized domains with minimal overhead, demonstrating the versatility of PaS datasets.},
  archive      = {J_PR},
  author       = {Jesús M. Rodríguez-de-Vera and Imanol G. Estepa and Ignacio Sarasúa and Bhalaji Nagarajan and Petia Radeva},
  doi          = {10.1016/j.patcog.2025.112236},
  journal      = {Pattern Recognition},
  month        = {3},
  pages        = {112236},
  shortjournal = {Pattern Recognition},
  title        = {Precision at scale: Domain-specific datasets on-demand},
  volume       = {171},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Frequency-enhanced contextual conversion network for esophageal lesion segmentation. <em>PR</em>, <em>171</em>, 112235. (<a href='https://doi.org/10.1016/j.patcog.2025.112235'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Esophageal cancer (EC) is a prevalent disease with a high fatality rate, making early detection in clinical practice challenging. Similar to identifying colon polyps, doctors often utilize an endoscope to observe EC lesions. However, segmenting early esophageal cancer is difficult, due to the flat shape, similar color to the surrounding mucosa, and the interference of esophageal inflammation. Moreover, this field lacks a comprehensive benchmark for depth exploration. To address these challenges, we construct a new benchmark and propose a novel Frequency-enhanced Contextual Conversion Network (FCC-Net) for esophageal lesion segmentation, which effectively captures multi-level context information and leverages the frequency domain to identify fine-grained lesion boundaries. Specifically, we propose a Frequency-enhanced Context Module (FCM) to merge high-level features, facilitating the capture of global context information and offering essential cues from the frequency domain for accurate lesion boundary identification. Additionally, we present a Scale-induced Cross-level Fusion (SCF) module that fully integrates features from the adjacent levels. This enables the learning of multi-scale features through different convolution kernels, effectively handling scale variations. Furthermore, a Contextual Conversion Module (CCM) is presented to incorporate the global context information and boundary-related cues to enhance EC lesion segmentation performance. Experimental results across multiple EC datasets demonstrate that our model outperforms other state-of-the-art medical segmentation methods. The benchmark will be released at https://github.com/taozh2017/FCC-Net .},
  archive      = {J_PR},
  author       = {Ziqi Tang and Xiaotong Niu and Long Rong and Yizhe Zhang and Yawei Bi and Nan Ru and Longsong Li and Ningli Chai and Tao Zhou},
  doi          = {10.1016/j.patcog.2025.112235},
  journal      = {Pattern Recognition},
  month        = {3},
  pages        = {112235},
  shortjournal = {Pattern Recognition},
  title        = {Frequency-enhanced contextual conversion network for esophageal lesion segmentation},
  volume       = {171},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Structured knowledge-inspired two-stage knowledge alignment framework for alzheimer’s disease diagnosis. <em>PR</em>, <em>171</em>, 112234. (<a href='https://doi.org/10.1016/j.patcog.2025.112234'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Early diagnosis is crucial for effectively managing and treating Alzheimer’s Disease (AD), yet current diagnostic methods encounter challenges when dealing with multi-modal data, particularly feature fusion and model interpretability. Traditional AD diagnostic techniques often rely on single biomarkers or basic data fusion methods, which fail to fully capture the complex interactions between different types of data, limiting diagnostic accuracy and reducing physicians’ confidence in the models. We propose SKITS-KAF , an innovative S tructured K nowledge- I nspired T wo- S tage K nowledge A lignment F ramework, designed to address these challenges by enriching feature expression capabilities and enhances diagnostic stability and interpretability. In the SKIA phase, employing structured knowledge to achieve high-dimensional semantic alignment through modality-specific feature extraction techniques. The DLSA phase further refines feature consistency using Generalized Canonical Correlation Analysis (GCCA), which captures complex relationships between data types and enhances the model’s interpretability, making the results more reliable. The framework is modular and flexible, enabling it to adapt to various clinical scenarios by swapping feature extraction models or classifiers as needed. Experimental results show that SKITS-KAF outperforms existing advanced methods in classifying AD, Mild Cognitive Impairment (MCI), and Cognitively Normal (CN) individuals, achieving a higher classification of 88.08 % accuracy. These results demonstrate its effectiveness for early AD diagnosis and its potential value in clinical applications.},
  archive      = {J_PR},
  author       = {Fulin Zheng and Hong Wang and Tianyu Liu and Feiyan Feng and Kai Wu and Cheng Liang and for the Alzheimer’s Disease Neuroimaging Initiative and Australian Imaging Biomarkers and Lifestyle flagship study of ageing},
  doi          = {10.1016/j.patcog.2025.112234},
  journal      = {Pattern Recognition},
  month        = {3},
  pages        = {112234},
  shortjournal = {Pattern Recognition},
  title        = {Structured knowledge-inspired two-stage knowledge alignment framework for alzheimer’s disease diagnosis},
  volume       = {171},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Central point link learning guided sparse dynamic diagonal embedding for feature selection. <em>PR</em>, <em>171</em>, 112233. (<a href='https://doi.org/10.1016/j.patcog.2025.112233'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In order to obtain more accurate pseudo-labels, more and more unsupervised feature selection (UFS) algorithms consider the intrinsic structure of data. However, data in real life not only has its inherent structure, but also has external connections between data. To address these issues, this article proposes central point link learning guided sparse dynamic diagonal embedding for feature selection (CPLDE). First, the diagonal graph is dynamically constructed in the subspace, and the sparsity of the diagonal graph is ensured by l 2,1 -norm. The graph constructed through sparse diagonal dynamics preserves a more accurate intrinsic distribution of data. Secondly, CPLDE constructs the central point link graph in the data space, preserving the distance information between each data and the central points. By using the similarity of distance information, the external connection between data can be obtained. Then, the critical knowledge of the pseudo-label obtained by the central point link learning is used to guide the sparse diagonal graph embedded in the pseudo-label learning process. As a result, the relevance between the intrinsic distribution of data and pseudo-labels, as well as the relevance between the extrinsic relationship of data and pseudo-labels, are enhanced. In addition, the central point link graph and sparse diagonal embedding graph form a dual graph structure, while retaining the manifold structure of the data. Finally, a new FS framework is constructed by combining the idea of dual graph learning and maximizing the between-class distance with trace ratio. To better embed the impact of the central points on data in FS and reduce losses, an improved update optimization method is proposed, which can obtain a better feature transformation matrix. Through comprehensive experiments on 8 datasets, it is verified that the FS performance of CPLDE is superior to 7 typical UFS algorithms.},
  archive      = {J_PR},
  author       = {Ronghua Shang and Jiarui Kong and Weitong Zhang and Yangyang Li},
  doi          = {10.1016/j.patcog.2025.112233},
  journal      = {Pattern Recognition},
  month        = {3},
  pages        = {112233},
  shortjournal = {Pattern Recognition},
  title        = {Central point link learning guided sparse dynamic diagonal embedding for feature selection},
  volume       = {171},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Underwater image enhancement by diffusion model with customized CLIP-classifier. <em>PR</em>, <em>171</em>, 112232. (<a href='https://doi.org/10.1016/j.patcog.2025.112232'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Underwater Image Enhancement (UIE) aims to improve the visual quality of underwater images. Unlike conventional enhancement tasks, underwater images face the challenge of lacking real reference images. Existing works select well-enhanced images from various approaches as references to train enhancement models, but their performance is inherently limited by the synthetic reference images. To address this, we propose CLIP-UIE, a novel multimodal framework that leverages Contrastive Language-Image Pretraining (CLIP) to incorporate prior knowledge from the in-air natural domain to counteract the adverse impact of synthetic reference images. For this purpose, we first generate a large-scale synthetic underwater dataset based on three representative synthesis methods to activate an image-to-image diffusion model. Then, we incorporate the prior knowledge from the in-air natural domain with CLIP to train an explicit CLIP-Classifier. Subsequently, we integrate this CLIP-Classifier with UIE benchmark datasets to jointly fine-tune the pre-trained diffusion model, steering the enhancement process toward the in-air natural domain and surpassing the constraints of the synthetic reference domain. Additionally, we observe that during fine-tuning, the image-to-image diffusion model and the CLIP-Classifier primarily focus on high-frequency intermediate variables. Therefore, we propose a partial fine-tuning strategy targeting these variables that is 10 times faster than conventional strategies. Extensive experiments demonstrate that our method exhibits a more natural appearance. The source code and pre-trained models are available on the project homepage: https://oucvisiongroup.github.io/CLIP-UIE.html/ .},
  archive      = {J_PR},
  author       = {Shuaixin Liu and Kunqian Li and Yilin Ding and Qi Qi},
  doi          = {10.1016/j.patcog.2025.112232},
  journal      = {Pattern Recognition},
  month        = {3},
  pages        = {112232},
  shortjournal = {Pattern Recognition},
  title        = {Underwater image enhancement by diffusion model with customized CLIP-classifier},
  volume       = {171},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). GraphMamba: Graph-driven spatial order-aware mamba for medical image segmentation. <em>PR</em>, <em>171</em>, 112231. (<a href='https://doi.org/10.1016/j.patcog.2025.112231'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Existing Mamba-based medical image segmentation methods faces two persistent shortcomings: their dependence on inflexible predefined scanning strategies and their limited capacity for spatial perception when handling anatomies with complex geometries. To address these challenges, we introduce GraphMamba, a novel graph-driven spatial order-aware Mamba framework. Our approach is rooted in a crucial insight: achieving robust spatial understanding requires learning structural relationships that maintain consistency regardless of input patch orders. GraphMamba implements this principle through two pivotal technical advancements. First, graph-driven topology-aware adaptive scanning module dynamically generates content-aware scanning orders by constructing and refining dynamic graph, thereby replacing conventional predefined orders with topology-guided traversal strategies. Second, spatial consistency learning establishes geometric invariance across varying scanning orders through carefully designed regularization constraints, enabling both geometry-coherence and order-invariant representation learning. Through rigorous evaluation on multiple clinical segmentation tasks, including myocardium and myocarditis lesion segmentation, liver and liver tumor segmentation, and abdominal multi-organ segmentation tasks, GraphMamba demonstrates substantial improvements in both segmentation accuracy and robustness compared to existing methods, confirming its effectiveness for medical image analysis.},
  archive      = {J_PR},
  author       = {Chengjin Yu and Hao Zhang and Cailing Pu and Sangyin Lv and Jing Yu and Xiaorui Wu and Dongsheng Ruan and Hanyu Xuan and Yuanting Yan},
  doi          = {10.1016/j.patcog.2025.112231},
  journal      = {Pattern Recognition},
  month        = {3},
  pages        = {112231},
  shortjournal = {Pattern Recognition},
  title        = {GraphMamba: Graph-driven spatial order-aware mamba for medical image segmentation},
  volume       = {171},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Learning from not-all-negative N-tuples and unlabeled data. <em>PR</em>, <em>171</em>, 112230. (<a href='https://doi.org/10.1016/j.patcog.2025.112230'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Supervised classification has achieved significant success across various domains; however, the increasing cost of labeled data has become a major bottleneck as data scales up. In response, weakly-supervised learning has emerged as a promising approach, aiming to leverage complex and diverse sources of weak supervision. However, existing weakly-supervised algorithms struggle when dealing with ordered high-dimensional data, particularly in the context of pairwise learning. To address this challenge, this paper investigates the limitations of existing algorithms that focus on learning from pairs containing at least one positive class, and extends them to N-tuples where at least one positive class is present. This extension aims to adapt the method to more generalized and complex scenarios encountered in real-world applications. Specifically, we introduce a novel weakly-supervised learning framework for both not-all-negative N-tuples and unlabeled data, offering a powerful strategy for training a binary classifier. In this setting, the dataset contains N-tuples with at least one positive class and unlabeled data points. To provide guarantees for the trained classifier, we establish an estimation error bound to prove its consistency. Finally, through rigorous experimental settings, we demonstrate that our proposed method effectively overcomes the challenges posed by not-all-negative N-tuples and unlabeled data, making it applicable to real-world scenarios.},
  archive      = {J_PR},
  author       = {Shuying Huang and Junpeng Li and Changchun Hua and Yana Yang},
  doi          = {10.1016/j.patcog.2025.112230},
  journal      = {Pattern Recognition},
  month        = {3},
  pages        = {112230},
  shortjournal = {Pattern Recognition},
  title        = {Learning from not-all-negative N-tuples and unlabeled data},
  volume       = {171},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Complementary bidirectional fusion for multi-view graph clustering. <em>PR</em>, <em>171</em>, 112229. (<a href='https://doi.org/10.1016/j.patcog.2025.112229'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-view graph methods emphasize integrating the information from different views to obtain promising performance. To achieve this goal, we introduce a novel model that performs C omplementary B idirectional F usion with C ross-view G raph F ilter(CBF-CGF). Specifically, CBF-CGF employs graph filters to capture high-order neighborhood information and learns a consistent graph embedding through an adaptive bidirectional fusion mechanism between similarity matrices and their spectral embeddings. This consistency graph embedding preserves both the global connectivity patterns captured by the similarity matrix and the local geometric structure inherited from the spectral embedding. Notably, adaptive weights are introduced to the indicator matrix to mine characteristics of different clusters. To tackle the optimization challenges, we propose an efficient iterative algorithm and provide a detailed analysis of its convergence and complexity. Extensive experiments validate that our method can yield comparable performance to state-of-the-art methods on nine real-world datasets, particularly on the COIL20 dataset where all evaluation metrics surpass 96.43 %.},
  archive      = {J_PR},
  author       = {Yanjin Tan and Danyang Wu and Xiaojun Yang and Cen Chen and Hong Man and Jin Xu},
  doi          = {10.1016/j.patcog.2025.112229},
  journal      = {Pattern Recognition},
  month        = {3},
  pages        = {112229},
  shortjournal = {Pattern Recognition},
  title        = {Complementary bidirectional fusion for multi-view graph clustering},
  volume       = {171},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). DAHI: A fast and efficient density aided hyper inference technique for large scene object detection. <em>PR</em>, <em>171</em>, 112228. (<a href='https://doi.org/10.1016/j.patcog.2025.112228'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Detecting small objects in large-scale scenes remains a fundamental challenge in object detection, primarily due to scale variation, occlusion, and limited resolution. In order to contribute in this research topic, we propose Density Aided Hyper Inference (DAHI), a lightweight and detector-agnostic framework that enhances detection performance through a structured, three-stage inference process. DAHI combines: (i) Region Density Estimation (RDE), which identifies areas likely to contain overlooked objects; (ii) Density-Aided Crop Selection (DACS), which efficiently selects high-density, low-overlap regions for re-inference; and (iii) Crop Margin Aware Non-Maximum Suppression (CMA-NMS), which merges detections from full-image and region-based inferences while mitigating boundary-related errors. DAHI requires no retraining and integrates seamlessly with standard object detectors. Experiments on several aerial and driving detection benchmarks demonstrate improved detection quality and runtime efficiency compared to existing multi-inference approaches, while introducing reduced computational overhead. These results support the use of DAHI as an effective and practical enhancement for small object detection in complex visual scenes.},
  archive      = {J_PR},
  author       = {Jonay Suárez–Ramírez and Daniel Santana–Cedrés and Nelson Monzón},
  doi          = {10.1016/j.patcog.2025.112228},
  journal      = {Pattern Recognition},
  month        = {3},
  pages        = {112228},
  shortjournal = {Pattern Recognition},
  title        = {DAHI: A fast and efficient density aided hyper inference technique for large scene object detection},
  volume       = {171},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Image harmonization in complex degradation scenes. <em>PR</em>, <em>171</em>, 112227. (<a href='https://doi.org/10.1016/j.patcog.2025.112227'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Image harmonization aims to synthesize photo-realistic images given composite images constructed by combining background and foreground source images. Most existing methods assume that the only difference between the two sources is illumination. However, in degradation scenarios, the source images inevitably exhibit complex distribution inconsistencies, including noise and resolution variations. Previous works have struggled to handle such inconsistencies despite their impressive performance. In this paper, we propose an image Harmonization Conditional Diffusion Model (H-CDM) to address the challenge in complex degradation scenes. Our model eliminates the intricate distribution inconsistencies by iteratively transforming the distributions from one source to another in a tractable manner. To achieve this, we first introduce a degradation-aware network with a degradation map estimation module (DEM). Given the degraded composite image, the degradation map can be generated by this estimation module and serves as a prior in our model. Further, we design a multi-scale masked deep supervision (MDS) loss to enhance the generation of photo-realistic harmonized images with fewer sampling steps. To verify the effectiveness of our H-CDM, we compare our H-CDM with other comparison methods on the newly constructed D-iHarmony4 dataset, where we achieve promising performance across the three metrics, mean squared error (MSE), peak signal-to-noise ratio (PSNR), and foreground MSE (fMSE). To evaluate the robustness of our H-CDM, we also conduct a mean opinion score (MOS) test. The results demonstrate that our H-CDM produces more favorable and photo-realistic images. Our H-CDM can be applied in image editing to generate user-friendly composite images, especially when one of the source images is degraded due to being captured under non-ideal photography conditions. Our code and dataset can be accessed at https://github.com/guanguanboy/HCDM .},
  archive      = {J_PR},
  author       = {Guanlin Li and Bin Zhao and Xuelong Li},
  doi          = {10.1016/j.patcog.2025.112227},
  journal      = {Pattern Recognition},
  month        = {3},
  pages        = {112227},
  shortjournal = {Pattern Recognition},
  title        = {Image harmonization in complex degradation scenes},
  volume       = {171},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). A general outlier filtering method for feature matching via local motion consistency based markov network. <em>PR</em>, <em>171</em>, 112226. (<a href='https://doi.org/10.1016/j.patcog.2025.112226'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Feature matching is a fundamental yet challenging task in computer vision. To address the critical problem of outlier removal, we propose a novel framework: the Local Motion Consistency based Markov network (LMCM). Our method models the problem on a pairwise Markov network, where each potential match is a binary node. The core of LMCM is a principled, dual-level consistency model. First, a unary potential evaluates the reliability of each individual match based on its local neighborhood consensus. Second, a pairwise potential enforces geometric compatibility between adjacent matches. This two-tiered approach effectively captures rich topological constraints while maintaining computational efficiency. We formulate the final inference as an Unconstrained Binary Quadratic Programming (UBQP) problem and solve it with a fast, gradient-based discrete optimization algorithm. Our method is general, efficient, and requires no training, making it a versatile filtering module applicable to any set of putative correspondences, regardless of their origin from handcrafted or learned feature descriptors. Extensive experiments demonstrate state-of-the-art performance on diverse tasks, including a 6.4 % accuracy gain in fundamental matrix estimation on challenging wide-baseline datasets, all while maintaining a low, linearithmic time complexity.},
  archive      = {J_PR},
  author       = {Fan Fan and Songchu Deng and Yong Ma and Jun Huang},
  doi          = {10.1016/j.patcog.2025.112226},
  journal      = {Pattern Recognition},
  month        = {3},
  pages        = {112226},
  shortjournal = {Pattern Recognition},
  title        = {A general outlier filtering method for feature matching via local motion consistency based markov network},
  volume       = {171},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). ConsistencyTrack: A robust multi-object tracker with a generation strategy of consistency model. <em>PR</em>, <em>171</em>, 112225. (<a href='https://doi.org/10.1016/j.patcog.2025.112225'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-object tracking (MOT) is a critical technology in computer vision, designed to detect multiple targets in video sequences and assign each target a unique ID per frame. Existed MOT methods excel at accurately tracking multiple objects in real-time across various scenarios. However, these methods still face challenges such as poor noise resistance and frequent ID switches. In this research, we propose a novel ConsistencyTrack, joint detection and tracking (JDT) framework that formulates detection and association as a denoising diffusion process on perturbed bounding boxes. This progressive denoising strategy significantly improves the model’s noise resistance. During the training phase, paired object boxes within two adjacent frames are diffused from ground-truth boxes to a random distribution, and then the model learns to detect and track by reversing this process. In inference, the model refines randomly generated boxes into detection and tracking results through minimal denoising steps. ConsistencyTrack also introduces an innovative target association strategy to address target occlusion. Experiments on the MOT16, MOT17 and DanceTrack datasets demonstrate that ConsistencyTrack outperforms other compared methods, especially better than DiffusionTrack in inference speed and other performance metrics. Our code is available at https://github.com/Tankowa/ConsistencyTrack .},
  archive      = {J_PR},
  author       = {Lifan Jiang and Zhihui Wang and Siqi Yin and Guangxiao Ma and Peng Zhang and Boxi Wu},
  doi          = {10.1016/j.patcog.2025.112225},
  journal      = {Pattern Recognition},
  month        = {3},
  pages        = {112225},
  shortjournal = {Pattern Recognition},
  title        = {ConsistencyTrack: A robust multi-object tracker with a generation strategy of consistency model},
  volume       = {171},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). IDEA: Image description enhanced CLIP-adapter for image classification. <em>PR</em>, <em>171</em>, 112224. (<a href='https://doi.org/10.1016/j.patcog.2025.112224'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {CLIP (Contrastive Language-Image Pre-training) has attained great success in pattern recognition and computer vision. Transferring CLIP to downstream tasks (e.g., zero- or few-shot classification) is a hot topic in multimodal learning. However, current studies focus on single-modality adaptation and fail to capture target-relevant fine-grained features. In this paper, we propose an Image Description Enhanced CLIP-Adapter (IDEA), a multimodal adapter that effectively boosts CLIP’s performance on few-shot classification tasks. This adapter leverages the textual descriptions in the training set to enhance the model’s ability to capture fine-grained features. Meanwhile, IDEA is a training-free method for CLIP, and it can be comparable to or even exceeds state-of-the-art models on multiple tasks. Furthermore, we introduce Trainable-IDEA (T-IDEA), which extends IDEA by adding two lightweight learnable components (i.e., a projector and a learnable latent space), further enhancing the model’s performance and achieving SOTA results on 11 datasets. As one important contribution, we employ the LLaMA model and design a comprehensive pipeline to generate textual descriptions for images of 11 datasets, resulting in a total of 1,637,795 image-text pairs, named “IMD-11”. Our code and data are released at https://github.com/FourierAI/IDEA .},
  archive      = {J_PR},
  author       = {Zhipeng Ye and Feng Jiang and Qiufeng Wang and Kaizhu Huang and Jiaqi Huang},
  doi          = {10.1016/j.patcog.2025.112224},
  journal      = {Pattern Recognition},
  month        = {3},
  pages        = {112224},
  shortjournal = {Pattern Recognition},
  title        = {IDEA: Image description enhanced CLIP-adapter for image classification},
  volume       = {171},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). MG-mono: A lightweight multi-granularity method for self-supervised monocular depth estimation. <em>PR</em>, <em>171</em>, 112223. (<a href='https://doi.org/10.1016/j.patcog.2025.112223'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Self-supervised monocular depth estimation (MDE) has garnered significant attention in various fields, particularly for real-time applications such as autonomous driving and robotic navigation, where computational resources are limited. In these contexts, MDE must achieve a balance between lightweight design and high accuracy. Nonetheless, the existing lightweight MDE methods usually sacrifice performance for efficiency. To address this challenge, we propose MG-Mono, a Multi-Granularity method for lightweight self-supervised MDE. Specifically, we propose a Multi-Granularity Information Fusion (MGIF) module within the encoder to comprehensively capture and integrate image features at pixel, local and global granularities. In our MGIF, the global dependency of pixels is modeled by Fast Fourier Transform (FFT), which can avoid the quadratic complexity and improve the efficiency of our method. Furthermore, we introduce a Feature-weighted Consistency Loss to extract favorable semantic priors from a pre-trained semantic segmentation model to guide the feature generation of our method. This strategy enhances the feature representation without increasing inference time, leading to improved depth estimation accuracy. At last, we also propose an efficient Neighborhood-Weighted Cooperative (NWC) prediction head in the decoder to refine the depth map by leveraging local contextual depth information. Experiments on KITTI, KITTI with Improved Ground Truth and Make3D datasets demonstrate that MG-Mono achieves state-of-the-art performance while maintaining a low parameter count and high inference speed. Code is available at https://github.com/PENGFly2022/MGMono.git .},
  archive      = {J_PR},
  author       = {Pengfei Wang and Shuhan Liu and Qiang Li and Yugen Yi and Jianzhong Wang},
  doi          = {10.1016/j.patcog.2025.112223},
  journal      = {Pattern Recognition},
  month        = {3},
  pages        = {112223},
  shortjournal = {Pattern Recognition},
  title        = {MG-mono: A lightweight multi-granularity method for self-supervised monocular depth estimation},
  volume       = {171},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). TSBA: A two-stage poison-only backdoor attack on visual object tracking. <em>PR</em>, <em>171</em>, 112222. (<a href='https://doi.org/10.1016/j.patcog.2025.112222'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Training high-performance Visual Object Tracking (VOT) models often relies on third-party resources, making these models vulnerable to backdoor attacks. In such attacks, attackers can implant backdoors by poisoning the training dataset and manipulating the model training process. Existing backdoor attack methods for VOT assume that the attacker has complete control over the model training process, or the designed attacks are untargeted, which limits the practicality and effectiveness of these methods. To address this issue, we propose a Two-Stage Poison-Only Backdoor Attack (TSBA). Specifically, in a poison-only scenario, TSBA employs a two-stage poisoning strategy to attach triggers to both the object region and the selected background region in video frames, while using contrastive loss and total variation loss to optimize the triggers, enhancing the effectiveness and stealthiness of the attack. Extensive experiments under various settings show that our backdoor attack significantly degrades the performance of trackers based on Siamese networks, Transformer, and temporal information, outperforming existing attack methods. Moreover, we validate the robustness of our attack against several potential backdoor defenses.},
  archive      = {J_PR},
  author       = {Yilang Zhang and Yanjun Pu and Jingzheng Li and Shuxin Zhao and Bo Lang},
  doi          = {10.1016/j.patcog.2025.112222},
  journal      = {Pattern Recognition},
  month        = {3},
  pages        = {112222},
  shortjournal = {Pattern Recognition},
  title        = {TSBA: A two-stage poison-only backdoor attack on visual object tracking},
  volume       = {171},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Cross-domain aggregation frequency fusion transformer for lightweight image super-resolution. <em>PR</em>, <em>171</em>, 112221. (<a href='https://doi.org/10.1016/j.patcog.2025.112221'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, significant progress has been made in single image super-resolution (SISR) tasks with methods based on CNN. However, the computational and memory costs of large CNN models limit their applicability on embedded or mobile devices, and the utilization of global contextual information is constrained. Vision Transformers have shown improved results in super-resolution tasks by effectively capturing global information through self-attention mechanisms. Nevertheless, existing ViT methods have certain limitations, such as the restriction to a single dimension aggregation in spatial or channel domains, increased computational complexity as the input image size grows, and neglect of the requirements for texture patterns. To address these issues, this paper proposes a lightweight image super-resolution method called Cross-domain Aggregation Frequency Fusion Transformer (CAFT). Specifically, CAFT consists of a Spatial Domain Cross-Exploration Attention (SCEA) module for deep interaction in the spatial domain across axes (height and width), a Channel Domain Transposed Attention (CDTA) module for interaction across feature channels, and a Frequency-guided Fusion Block (FGFB) that leverages frequency domain processing to extract fine-grained image detail features. The combination of these modules achieves global correlations, texture feature extraction, and information propagation. In the experiments, we evaluate the CAFT method on multiple lightweight SISR datasets and compare it against existing SISR methods. Our method exhibits improvements of 0.40 dB, 0.47 dB, and 0.17 dB on Manga109 compared to DiVANet, SPAN, and SRConvNet-L, respectively. The results demonstrate that our approach provides excellent super-resolution performance and better detail restoration while maintaining low computational complexity and parameter count. The source code is available at https://github.com/Prtick/CAFT .},
  archive      = {J_PR},
  author       = {Mang Hu and Jiansi Ren and Min Hu and Ziyang Li},
  doi          = {10.1016/j.patcog.2025.112221},
  journal      = {Pattern Recognition},
  month        = {3},
  pages        = {112221},
  shortjournal = {Pattern Recognition},
  title        = {Cross-domain aggregation frequency fusion transformer for lightweight image super-resolution},
  volume       = {171},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). One-step late fusion multiple kernel clustering via tensorized adaptive multi-scale partition fusion. <em>PR</em>, <em>171</em>, 112220. (<a href='https://doi.org/10.1016/j.patcog.2025.112220'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent years, multiple kernel clustering (MKC) has extensive applications in data analysis field. Due to its characteristic of low computational complexity, Late Fusion Multiple Kernel Clustering (LFMKC) stands out among existing MKC algorithm methods. However, LFMKC is still confronted with some problems. Firstly, LFMKC only uses single-scale partition to obtain the clustering results, failing to learn about the information in the kernel matrices thoroughly. Secondly, LFMKC cannot utilize the higher-order correlations since the base partitions are fixed. Besides, LFMKC need an extra k -means step to yield the result. To overcome these limitations, we design a new method named One-step Late fusion Multiple kernel clustering via Tensorized adaptive Multi-scale partition Fusion (OLMTMF), which integrates multi-scale fusion, high-order tensor information and spectral rotation (SR) into a unified framework. To be more precise, we first design an adaptive algorithm to integrate multi-scale partition instead of single partition, fully exploring the information in different kernel matrices. Next, OLMTMF builds a tensor with different fused partitions, constrained by Tensor Nuclear Norm (TNN), to discover the higher-order correlations among these fused partitions. Finally, OLMTMF utilizes SR to directly obtain clustering results, further improving the clustering performance. We also develop an alternative procedure with theoretical convergence guarantee to optimize the objective function of OLMTMF. Extensive experiments indicate that OLMTMF achieves excellent clustering performance on different datasets with low computational complexity. The source code can be downloaded from: https://github.com/luxinrui018/OLMTMF .},
  archive      = {J_PR},
  author       = {Xinrui Lu},
  doi          = {10.1016/j.patcog.2025.112220},
  journal      = {Pattern Recognition},
  month        = {3},
  pages        = {112220},
  shortjournal = {Pattern Recognition},
  title        = {One-step late fusion multiple kernel clustering via tensorized adaptive multi-scale partition fusion},
  volume       = {171},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). A novel score function for conformal prediction in rule-based binary classification. <em>PR</em>, <em>171</em>, 112219. (<a href='https://doi.org/10.1016/j.patcog.2025.112219'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Computer scientists consider an artificial intelligence system safe and trustworthy if it fulfills four pillars: robustness, transparency, fairness, and privacy. We propose a fifth fundamental aspect: conformal guarantee, that is, the probabilistic assurance that the system will behave as expected. We introduce CONFIDERAI (Conformal Interpretable by Design Explainable and Reliable Artificial Intelligence), a new score function for binary rule-based classifiers depending on both rules’ performance and geometry; the latter includes both the position of points within rule boundaries and rule overlaps, these being quantified via geometrical rule similarity. Furthermore, we address the problem of individuating regions in the feature space in which conformal guarantees are satisfied, by defining the concept of conformal critical set (CCS). The overall method is tested with promising results, comparable in efficiency to traditional scores, on ten datasets of real-world interest, such as domain name server tunneling detection and cardiovascular disease prediction. Moreover, newly generated rules from CCS resulted into an improved precision and reduced error on a target class, thus avoiding prediction failures in safety-critical contexts.},
  archive      = {J_PR},
  author       = {Sara Narteni and Alberto Carlevaro and Fabrizio Dabbene and Marco Muselli and Maurizio Mongelli},
  doi          = {10.1016/j.patcog.2025.112219},
  journal      = {Pattern Recognition},
  month        = {3},
  pages        = {112219},
  shortjournal = {Pattern Recognition},
  title        = {A novel score function for conformal prediction in rule-based binary classification},
  volume       = {171},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). YOLO-FCE: A feature and clustering enhanced object detection model for species classification. <em>PR</em>, <em>171</em>, 112218. (<a href='https://doi.org/10.1016/j.patcog.2025.112218'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Australia harbours a rich and unique diversity of wildlife, constituting a vital component of the nation’s ecological heritage. Accurate species identification in expansive and remote natural environments remains a significant challenge. In this study, we propose YOLO-Feature and Clustering Enhanced (YOLO-FCE), an improved model based on the YOLOv9 architecture. We conducted a series of cluster-distance-based analyses to evaluate and enhance the model’s feature extraction capabilities. The proposed model was trained and tested on a dataset containing 50 Australian animal species, with 700 images per species, resulting in a total of 35,000 images. YOLO-FCE achieved a mean Average Precision (mAP50:95) of 87.5 % and a precision of 98.2 %. On a separate validation set of previously unseen images, it attained a recognition accuracy of 91.29 % with an average confidence score of 0.801. Compared with baseline models including YOLOv9, YOLOv11, and Faster R-CNN evaluated on the same dataset, YOLO-FCE demonstrated robust performance.},
  archive      = {J_PR},
  author       = {Qianqian Zhang and Khandakar Ahmed and Muhammad Imad Khan and Hua Wang and Youyang Qu},
  doi          = {10.1016/j.patcog.2025.112218},
  journal      = {Pattern Recognition},
  month        = {3},
  pages        = {112218},
  shortjournal = {Pattern Recognition},
  title        = {YOLO-FCE: A feature and clustering enhanced object detection model for species classification},
  volume       = {171},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). DiffClick: Click-differentiated enhancement network for interactive segmentation. <em>PR</em>, <em>171</em>, 112217. (<a href='https://doi.org/10.1016/j.patcog.2025.112217'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Click-based interactive segmentation aims to achieve precise segmentation using minimal positive and negative clicks. Existing methods often overlook the differences between positive and negative clicks. They have different objectives, and the number of negative clicks is far less than the number of positive ones. This leads to inadequate background refinement in the absence of negative clicks. In response, we propose DiffClick , a novel framework that processes positive and negative clicks in different enhancement modules. Starting from the initial segmentation results derived from the first click, both our Foreground Enhancement Module and Background Enhancement Module utilize a weight fusion module to augment features based on the type of guidance received. The Foreground Enhancement Module refines foreground features guided by positive clicks. Similarly, the Background Enhancement Module processes negative clicks and improves background segmentation by incorporating weight maps of background regions. Additionally, this module includes a non-target prototype to provide supplementary background guidance, ensuring effective segmentation even when negative clicks are lacking. Extensive experiments show that DiffClick beats most existing methods, especially when negative clicks are absent.},
  archive      = {J_PR},
  author       = {Siqi Song and Siyue Yu and Huiyu Zhou and Xiaowei Huang and Limin Yu and Jimin Xiao},
  doi          = {10.1016/j.patcog.2025.112217},
  journal      = {Pattern Recognition},
  month        = {3},
  pages        = {112217},
  shortjournal = {Pattern Recognition},
  title        = {DiffClick: Click-differentiated enhancement network for interactive segmentation},
  volume       = {171},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Research progress on the application of deep learning in fingerprint recognition. <em>PR</em>, <em>171</em>, 112216. (<a href='https://doi.org/10.1016/j.patcog.2025.112216'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Latent fingerprints are an important basis for identifying suspects and occupy a pivotal position in forensic science. In recent years, deep learning has become well-known for its powerful learning ability, and its application in fingerprint recognition has gradually attracted people's attention. Compared with traditional fingerprint recognition technology, deep learning-based fingerprint recognition technology has outstanding advantages such as intelligence, high efficiency, and strong adaptability. This review summarizes in detail the research progress of deep learning in fingerprint recognition applications from two aspects: deep learning models and fingerprint recognition processes. In terms of deep learning models, the research progress of models such as deep belief networks, convolutional neural networks, autoencoders, and generative adversarial networks is highlighted. In terms of the recognition process, the application of deep learning in fingerprint image preprocessing, fingerprint feature extraction, and fingerprint image matching is emphasized. Finally, based on the current development trends of deep learning and fingerprint recognition, this paper provides prospects for the development of deep learning in the field of fingerprints.},
  archive      = {J_PR},
  author       = {An Peng and Rui Huang},
  doi          = {10.1016/j.patcog.2025.112216},
  journal      = {Pattern Recognition},
  month        = {3},
  pages        = {112216},
  shortjournal = {Pattern Recognition},
  title        = {Research progress on the application of deep learning in fingerprint recognition},
  volume       = {171},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Are deep learning models robust to partial object occlusion in visual recognition tasks?. <em>PR</em>, <em>171</em>, 112215. (<a href='https://doi.org/10.1016/j.patcog.2025.112215'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Image classification models, including convolutional neural networks (CNNs), perform well on a variety of classification tasks but struggle under conditions of partial occlusion of relevant objects. Methods to improve performance under occlusion, including data augmentation, part-based clustering, and more inherently robust architectures, including Vision Transformer (ViT) models, have, to some extent, been evaluated on their ability to classify objects under partial occlusion. However, evaluations of these methods have largely relied on images containing artificial occlusion, since they are inexpensive to generate and label. Additionally, these methods are compared to early, now outdated models, and rarely to each other. We contribute the Image Recognition Under Occlusion (IRUO) dataset, based on the OVIS dataset in [1]. IRUO utilizes real-world and artificially occluded images to test and benchmark leading methods’ robustness to partial occlusion in visual recognition tasks. In addition, we contribute the design and results of a human study using images from IRUO evaluating human classification performance on multiple levels and types of occlusion. We find that ViT-based models show higher recognition accuracy than modern CNN-based models, which are more accurate than earlier CNN-based models, but that ViT models are still modestly below human accuracy. We also find that diffuse occlusion, in which relevant objects are seen through“holes” in occluders such as fences and leaves, can greatly reduce the accuracy of deep recognition models as compared to humans, especially CNNs.},
  archive      = {J_PR},
  author       = {Kaleb Kassaw and Francesco Luzi and Leslie M. Collins and Jordan M. Malof},
  doi          = {10.1016/j.patcog.2025.112215},
  journal      = {Pattern Recognition},
  month        = {3},
  pages        = {112215},
  shortjournal = {Pattern Recognition},
  title        = {Are deep learning models robust to partial object occlusion in visual recognition tasks?},
  volume       = {171},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). An adaptive multi-graph fusion for tumor grading in pathology images. <em>PR</em>, <em>171</em>, 112214. (<a href='https://doi.org/10.1016/j.patcog.2025.112214'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cancer grading is crucial for patient care, but integrating diverse knowledge of cancer pathogenesis in deep-learning models remains challenging. Traditional graph neural networks (GNN) often overlook the need to connect and balance the information across different levels of granularity, which is essential in histopathology for accurately capturing cellular morphology, tissue architecture, and spatial relationships. To address these challenges and provide a more holistic view of the patient's condition, we propose an Adaptive Multi-Graph Fusion-based Attentive Graph Neural Network (AMGF-GNN), which includes (1) three distinct graphs constructed and processed based on community information, feature similarity, and a combination of both to learn cell-to-cell interactions from three different views, enabling to distinguish the influence of each node in every single graph, (2) Adaptive Attentive Multi-Graph Fusion module which adaptively fuses the embeddings from all three paths, ensuring that the most relevant features from each graph are prioritized, and learns to balance the contributions of the community and feature-based information, addressing the uncertainty of their relative importance for the final grading task, (3) a dual-level loss optimization incorporates intra-graph and inter-graph similarity measures, ensuring consistency and robustness in the learned embeddings. The proposed method was experimentally validated and compared with nine other state-of-the-art (SOTA) models using the glioma TCGA and invasive ductal carcinoma (IDC) datasets. It achieved a superior accuracy of 89.68 % for the binary grading of the glioma TCGA dataset, outperforming other competing models and demonstrating the effectiveness of AMGF-GNN.},
  archive      = {J_PR},
  author       = {Islam Alzoubi and Bowen Xin and Rolf Bjerkvig and Jian Wang and Xiuying Wang},
  doi          = {10.1016/j.patcog.2025.112214},
  journal      = {Pattern Recognition},
  month        = {3},
  pages        = {112214},
  shortjournal = {Pattern Recognition},
  title        = {An adaptive multi-graph fusion for tumor grading in pathology images},
  volume       = {171},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Slowly expanding neural network for class incremental learning. <em>PR</em>, <em>171</em>, 112213. (<a href='https://doi.org/10.1016/j.patcog.2025.112213'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Currently deep learning models often rapidly forget the knowledge of old classes when they are continually updated to learn knowledge of new classes. To alleviate such catastrophic forgetting issue, the state-of-the-art approach often freezes the learned feature extractor to preserve the old knowledge and introduces additional model components when learning new knowledge each time. The forgetting issue can be effectively handled by such approach at the price of rapid model expansion. In this paper, we propose a novel continual learning framework, called SEIL, with a much Slower model Expansion rate and better Incremental Learning performance than its counterparts. Specifically, instead of introducing an entire feature extractor, the majority of network parameters are shared during continual learning, and only the remaining lightweight modules are expanded for each new task. An out-of-the-distribution (OOD) technique is also applied to two auxiliary classifiers for more class-balanced predictions. The proposed method achieves new state-of-the-art results on multiple benchmark datasets under the standard continual learning settings.},
  archive      = {J_PR},
  author       = {Zhengjin Xu and Xuyang Li and Xiaobin Chang and Wei-Shi Zheng and Ruixuan Wang},
  doi          = {10.1016/j.patcog.2025.112213},
  journal      = {Pattern Recognition},
  month        = {3},
  pages        = {112213},
  shortjournal = {Pattern Recognition},
  title        = {Slowly expanding neural network for class incremental learning},
  volume       = {171},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Spectral analysis on 3D orthogonal moments for effective terrain matching. <em>PR</em>, <em>171</em>, 112212. (<a href='https://doi.org/10.1016/j.patcog.2025.112212'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Aiming to improve the terrain matching performance, the spectral analysis on the typical 3D orthogonal moments (OMs) is accomplished for the first time. The typical 3D OMs include Zernike moments (ZM), orthogonal Fourier-Mellin moments (OFMM), fractional-order Jacobi-Fourier moments (FJFM), exponent-Fourier moments (EFM), generic polar complex exponent transform (GPCET), and Bessel-Fourier moments (BFM). The general expression of the typical 3D OMs in both the spatial and the frequency domains is derived firstly. Then, the terrain spectrum is analyzed by the Fourier central slicing method. Both the spherical harmonic and the radial spectra of the typical 3D OMs are investigated further. Based on the spectral analysis on the typical 3D OMs, the terrain matching algorithms are designed accordingly. Numerical experiments indicate that the matching performance of the algorithms coincides with the spectral analysis results of the typical 3D OMs. These new findings are the foundation for designing an effective terrain matching algorithm.},
  archive      = {J_PR},
  author       = {Junjie Zhou and Quan Hu and Kedong Wang and Jinling Wang},
  doi          = {10.1016/j.patcog.2025.112212},
  journal      = {Pattern Recognition},
  month        = {3},
  pages        = {112212},
  shortjournal = {Pattern Recognition},
  title        = {Spectral analysis on 3D orthogonal moments for effective terrain matching},
  volume       = {171},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Encoder-decoder nonnegative matrix factorization with β-divergence for data clustering. <em>PR</em>, <em>171</em>, 112211. (<a href='https://doi.org/10.1016/j.patcog.2025.112211'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Nonnegative Matrix Factorization (NMF), as a group representation learning model, produces part-based representation with interpretable features that can be applied to various problems, such as data clustering. The findings indicate that the NMF model with β divergence ( β -NMF) performs excellently in clustering different data types and noise assumptions. However, existing NMF-based data clustering methods are defined within a latent decoder model, lacking a verification mechanism. Recently, self-representation techniques have been applied to a wide range of tasks, empowering models to autonomously learn and verify representations that faithfully reflect the intricacies and nuances inherent in their input data. This paper proposes a self-representation factorization model for data clustering that incorporates local information into its learning process. The Regularized Encoder-Decoder NMF model based on β divergence ( β -REDNMF) integrates encoder and decoder factorizations into a β cost function that mutually verify and refine each other, resulting in the formation of more distinct clusters. To incorporate the local information into the method, we add a graph regularization to the model. The β -REDNMF, owing to its autoencoder-like architecture and utilization of local information, produces more informative word embeddings with generalization abilities that apply to various data types. We present an efficient and effective optimization algorithm based on multiplicative update rules to solve the proposed unified model. The experimental results on the ten well-known datasets show that the proposed β -REDNMF model outperforms other state-of-the-art data clustering methods.},
  archive      = {J_PR},
  author       = {Sayvan Soleymanbaigi and Amjad Seyedi and Fardin Akhlaghian Tab and Fatemeh Daneshfar},
  doi          = {10.1016/j.patcog.2025.112211},
  journal      = {Pattern Recognition},
  month        = {3},
  pages        = {112211},
  shortjournal = {Pattern Recognition},
  title        = {Encoder-decoder nonnegative matrix factorization with β-divergence for data clustering},
  volume       = {171},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). A general aggregation federated learning intervention algorithm based on do-calculus. <em>PR</em>, <em>171</em>, 112210. (<a href='https://doi.org/10.1016/j.patcog.2025.112210'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article explores federated long-tail learning (Fed-LT), where clients hold private, heterogeneous data that collectively form a global long-tail distribution. We propose two methods: (a) Client Re-weighted Prior Analyzer (CRePA), which balances the global model’s performance on tail and non-tail categories and enhances performance on tail categories while maintaining it on non-tail categories. (b) Federated Long-Tail Causal Intervention Model (FedLT-CI) computes clients’ causal effects on the global model’s performance in the tail and enhances the interpretability of Fed-LT. Extensive experiments on the CIFAR-10-LT and CIFAR-100-LT datasets demonstrate the following: (1) CRePA outperforms other baselines, achieving state-of-the-art (SOTA) performance. In scenarios with high heterogeneity and severe long-tail distributions, CRePA improves tail performance by 6.3 % and 5 % compared to CReFF and FedGrab, respectively. (2) FedLT-CI, by intervening during the aggregation process in federated learning (FL), effectively enhances the tail performance of baselines while maintaining stable non-tail performance. For instance, on CIFAR-10-LT under a severe imbalance setting ( α = 0.1 , I F G = 100 ), applying the intervention strategy to the FedAvg, FedGrab, and CRePA models improves tail performance by 4.5 %, 2.1 %, and 1.9 %.},
  archive      = {J_PR},
  author       = {Zhenyuan Huang and Wenzhong Tang and Hui Zhang and Haijun Yang},
  doi          = {10.1016/j.patcog.2025.112210},
  journal      = {Pattern Recognition},
  month        = {3},
  pages        = {112210},
  shortjournal = {Pattern Recognition},
  title        = {A general aggregation federated learning intervention algorithm based on do-calculus},
  volume       = {171},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). DIA: Deriving linguistic information from auxiliary languages for remote sensing image captioning. <em>PR</em>, <em>171</em>, 112209. (<a href='https://doi.org/10.1016/j.patcog.2025.112209'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Remote sensing image captioning (RSIC) is a cross-modal task aimed at describing scene categories, object classes, and their spatial relationships in remote sensing images using natural language. Existing methods typically focus on training models in single language, neglecting the linguistic-enhancing information derived from syntactic structure differences and diverse expressions of the same objects and scenes. This information, present in cross-linguistic annotated data, can significantly enhance language perception and enrich training data. To verify the effectiveness of this information, we propose an auxiliary language-enhanced network called DIA, which leverages linguistic information from auxiliary languages to improve the quality and fluency of target language generation. DIA consists of shared visual feature extractor, target language generator, and auxiliary language generator. The shared visual feature extractor integrates the Linguistic-Irrelevant Feature Enrichment (LiFE) module, while a Linguistic Bridge connects the target and auxiliary language generators. The LiFE module employs linguistic-irrelevant feature extraction and multi-view attention to extract precise visual features, enriching the representations while minimizing language bias. Multi-view attention balances deep semantic expressions and linguistic-irrelevant features. The Linguistic Bridge establishes interactive pathway between the target language generator (ALG) and the auxiliary language generator (TLG), enabling the TLG to learn from the ALG’s language modeling capabilities. This interaction allows the TLG to handle complex visual features, improving language generation performance. Extensive experiments demonstrate that our model achieves significant performance improvements on the UCM, Sydney, RSICD, and NWPU datasets. Specifically, on the UCM dataset, BLEU-4 is improved by 5.06 %, and CIDEr is improved by 16.86 %.},
  archive      = {J_PR},
  author       = {Tao Yang and Qing Zhou and Qi Wang},
  doi          = {10.1016/j.patcog.2025.112209},
  journal      = {Pattern Recognition},
  month        = {3},
  pages        = {112209},
  shortjournal = {Pattern Recognition},
  title        = {DIA: Deriving linguistic information from auxiliary languages for remote sensing image captioning},
  volume       = {171},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Sliced wasserstein graph kernel for measuring global topological similarity of brain functional networks. <em>PR</em>, <em>171</em>, 112208. (<a href='https://doi.org/10.1016/j.patcog.2025.112208'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Brain functional network describes and quantifies the information transport between brain regions and has been widely applied to the diagnosis tasks of brain diseases. The Wasserstein distance is gradually used to investigate the optimal transport problems in brain functional networks. However, most of the existing brain network analysis methods based on Wasserstein distance focus on the optimal transport between specific topologies (e.g., tree and pyramid structure) and neglect the optimal transport in global topologies of brain functional networks. To tackle this problem, we propose a new distance called graph sliced Wasserstein (GSW) distance to measure the optimal transport cost in global topologies of brain functional networks. Based on GSW distance, we further propose a new graph kernel called sliced Wasserstein graph (SWG) kernel to measure the global topological similarity of brain functional networks. We prove that our proposed GSW distance is a distance metric and SWG kernel is positive definite. To evaluate the effectiveness of the proposed method, we perform the classification experiments on functional magnetic resonance imaging data of brain diseases from ADHD-200 dataset, ABIDE dataset and ADNI dataset. The experimental results show that our proposed GSW distance can be utilized to analyze the statistical differences of brain functional networks and identify important brain regions. The SWG kernel can significantly improve classification accuracy compared to state-of-the-art graph kernels and graph neural networks for classifying brain diseases.},
  archive      = {J_PR},
  author       = {Kai Ma and Qi Zhu and Xuyun Wen and Xibei Yang and Daoqiang Zhang},
  doi          = {10.1016/j.patcog.2025.112208},
  journal      = {Pattern Recognition},
  month        = {3},
  pages        = {112208},
  shortjournal = {Pattern Recognition},
  title        = {Sliced wasserstein graph kernel for measuring global topological similarity of brain functional networks},
  volume       = {171},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Bridging attention fusion-based multi-view graph neural networks for spatial gene expression prediction. <em>PR</em>, <em>171</em>, 112207. (<a href='https://doi.org/10.1016/j.patcog.2025.112207'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Spatial transcriptomics is an emerging technology that aligns gene expression profiles with histology images. Recently, methods based on multi-view information fusion have been proposed to infer gene expression profiles from histology images. However, they tend to face two key challenges: the acquisition of view-specific and view-common information, and the utilization of shared information across multiple views in achieving information fusion. To tackle these challenges, we propose Bridging Attention Fusion-based Multi-view Graph Neural Networks (BAF-MGNN) which incorporate graph processing module and a bridge module, to capture the specific information and shared information from different feature spaces. Specifically, the graph processing module includes visual graph processing and spatial graph processing block, aiming to acquire view-specific information. To facilitate the alignment and fusion of visual and spatial embedding, we introduce a bridge module, which considers the shared information as a bridge connecting different views. Subsequently, the fused embedding is utilized for the spatial gene expression prediction. Extensive experiments on human breast cancer datasets show that BAF-MGNN achieves superior performance compared with state-of-the-art methods. The ablation experiment also showcases the essential role of each module. In summary, BAF-MGNN can achieve multi-view information fusion by leveraging shared information and enable the spatial gene expression prediction from histology images.},
  archive      = {J_PR},
  author       = {Weicheng Sun and Ping Zhang and Jinsheng Xu and Weihan Zhang and Yongbin Zeng and Li Li},
  doi          = {10.1016/j.patcog.2025.112207},
  journal      = {Pattern Recognition},
  month        = {3},
  pages        = {112207},
  shortjournal = {Pattern Recognition},
  title        = {Bridging attention fusion-based multi-view graph neural networks for spatial gene expression prediction},
  volume       = {171},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Image dehazing via RGB-FIR multimodal fusion and collaborative learning. <em>PR</em>, <em>171</em>, 112206. (<a href='https://doi.org/10.1016/j.patcog.2025.112206'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The mainstream deep learning-based image dehazing methods that rely solely on RGB information experience a sharp decline in performance when confronted with dense fog. To this aim, this paper proposes an RGB-FIR multimodal fusion-transfer network, along with a collaborative optimization strategy tailored for image dehazing. The developed network comprises two key modules: a multi-level feature fusion module and a multi-scale image transfer module. Specifically, the former employs a cross-modal multi-scale based on cross-pooling attention model to explore the complementary information cross scales and levels of RGB-FIR multimodal images. The latter characterizes the mapping relationship between the fused multimodal features and the ground-truth image. To further boost the optimization and facilitate the dehazing performance, a collaborative optimization strategy is elaborated to harmonies the merits of both generative adversarial loss, feature matching loss, perceptual loss, and fidelity loss. To validate the effectiveness of our proposed dehazing algorithm, we have collected 11,736 pairs of foggy and foggy-free images using a binocular RGB-FIR camera. Both subjective and objective experiments demonstrate that our RGB-FIR multimodal dehazing algorithm outperforms existing state-of-the-art (SOTA) image dehazing methods in terms of restoring details, textures, and color information from foggy images.},
  archive      = {J_PR},
  author       = {Ruolin Du and Han Wang and Wenjie Liu and Guangcheng Wang and Kui Jiang and Hanseok Ko},
  doi          = {10.1016/j.patcog.2025.112206},
  journal      = {Pattern Recognition},
  month        = {3},
  pages        = {112206},
  shortjournal = {Pattern Recognition},
  title        = {Image dehazing via RGB-FIR multimodal fusion and collaborative learning},
  volume       = {171},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Experimental evaluation of szemerédi’s regularity lemma in graph-based clustering. <em>PR</em>, <em>171</em>, 112205. (<a href='https://doi.org/10.1016/j.patcog.2025.112205'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {One major problem of graph-based clustering lies in the large computation load resulted by the similarity graph. Several previous works have shown that Szemerédi’s regularity lemma can be useful in relieving this problem. Based on this lemma, we partition the original graph to obtain a reduced graph, which inherits the major structure of the original graph with a much smaller cardinality. By performing clustering on the reduced graph and mapping data labels back to the original graph, the computation load can be reduced significantly. In further works we found that the parameters of this method have significant influences on the clustering results, and this issue hasn’t been dealt with in previous works. In this paper we present a thorough investigation of the influences of the parameters on clustering results, in experiments with four representative algorithms and a large number of real datasets. As a result, we find out the appropriate ranges of parameters to improve both clustering accuracy and computation efficiency significantly. We also show that regularity partitioning outperforms ordinary k-means-based partitioning, demonstrating the advantage of the regularity lemma in building the reduced graph. Furthermore, experimental results show that relatively old algorithms can be enhanced based on this lemma to outperform recent state-of-the-art ones. This work goes a step further in extending the application of the regularity lemma from pure theoretical to practical realms.},
  archive      = {J_PR},
  author       = {Jian Hou and Juntao Ge and Huaqiang Yuan and Marcello Pelillo},
  doi          = {10.1016/j.patcog.2025.112205},
  journal      = {Pattern Recognition},
  month        = {3},
  pages        = {112205},
  shortjournal = {Pattern Recognition},
  title        = {Experimental evaluation of szemerédi’s regularity lemma in graph-based clustering},
  volume       = {171},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Locally adaptive one-class classifier fusion with dynamic ℓp-norm constraints for robust anomaly detection. <em>PR</em>, <em>171</em>, 112204. (<a href='https://doi.org/10.1016/j.patcog.2025.112204'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper presents a novel approach to one-class classifier fusion using locally adaptive learning with dynamic ℓ p-norm constraints. Our framework dynamically adjusts fusion weights based on local data characteristics, addressing key challenges in ensemble-based anomaly detection. By incorporating an interior-point optimization technique, our method significantly improves computational efficiency over traditional Frank-Wolfe approaches, achieving up to 19× speed gains in complex scenarios. We evaluate the framework on UCI benchmark datasets and robotics-related temporal sequence datasets, demonstrating superior performance across diverse anomaly types. Statistical validation via Skillings-Mack tests confirms significant advantages over existing methods, consistently achieving top rankings in both pure and non-pure learning scenarios. The framework’s ability to adapt to local data patterns while remaining computationally efficient makes it particularly valuable for real-time anomaly detection applications.},
  archive      = {J_PR},
  author       = {Sepehr Nourmohammadi and Arda Sarp Yenicesu and Shervin Rahimzadeh Arashloo and Ozgur S. Oguz},
  doi          = {10.1016/j.patcog.2025.112204},
  journal      = {Pattern Recognition},
  month        = {3},
  pages        = {112204},
  shortjournal = {Pattern Recognition},
  title        = {Locally adaptive one-class classifier fusion with dynamic ℓp-norm constraints for robust anomaly detection},
  volume       = {171},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). DecoupleNet: Domain-specific task decoupling network for low-light image enhancement. <em>PR</em>, <em>171</em>, 112203. (<a href='https://doi.org/10.1016/j.patcog.2025.112203'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Mapping noisy, low-light RAW images to well-exposed sRGB images is both a promising and challenging task. Traditional Image Signal Processing (ISP) pipelines exhibit suboptimal performance in extreme low-light environments. Existing deep learning-based approaches, including both single-stage and multi-stage methods, have shown great potential in enhancing RAW low-light images. Single-stage models usually struggle with domain ambiguity. Conversely, multi-stage models tend to neglect domain-specific challenges due to their reliance on similar modules across various domains, which may lead to suboptimal performance. To address these limitations, we propose a domain-specific task decoupling network (DecoupleNet) designed to deeply decouple the entangled task into two subtasks across the two domains. Specifically, we introduce a channel-normalized denoising block for effective noise suppression in the RAW domain, as well as a color correction transformer block for precise color correction in the sRGB domain. Furthermore, we design a spatial frequency block in both domains to capture fine details and textures, highlighting the often underutilized role of frequency information. Extensive experiments demonstrate that our approach achieves competitive performance, surpassing state-of-the-art methods on specific metrics across the SID and MCR datasets. Specifically, 0.12 PSNR improvement on Sony dataset, 3.03 % PSNR improvement on MCR dataset and a 0.094 reduction in LPIPS on the Fuji dataset. The code is available at: https://github.com/drafly/decouplenet .},
  archive      = {J_PR},
  author       = {Peiliang Huang and Xianmin Chen and Xiaoxu Feng and Qiangqiang Wang and Dingwen Zhang and Longfei Han and Junwei Han},
  doi          = {10.1016/j.patcog.2025.112203},
  journal      = {Pattern Recognition},
  month        = {3},
  pages        = {112203},
  shortjournal = {Pattern Recognition},
  title        = {DecoupleNet: Domain-specific task decoupling network for low-light image enhancement},
  volume       = {171},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). An effective two-stage auto-weighted multi-view consensus clustering via bipartite graph. <em>PR</em>, <em>171</em>, 112202. (<a href='https://doi.org/10.1016/j.patcog.2025.112202'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Although present various multi-view clustering algorithms have gained significant achievements, there are still three drawbacks in most algorithms. First, they mainly concern of single-view or multi-view ensemble independently, which ignore the consistent joint learning between single view and consensus view; Second, most algorithms mainly adopt single-stage fusion strategy, such as graph-based or subspace-based, which neglect considering multi-stage fusion strategy; Third, most of them did not consider different roles played by different views. Therefore, we propose an effective two-stage auto-weighted multi-view consensus clustering via bipartite graph (TWMVC-BG) in this paper. Specifically, at the early-stage, different views can be transferred to multiple view groups in any combination, which can capture multi-granularity information in three levels (w.r.t. anchors, neighbors and features) by bipartite graph via anchors. A set of diversity bipartite graphs can be obtained from multiple view groups. At the late-stage, the graph-based concept is adopted to generate the intra-group consensus graph S ( i ) for diversity bipartite graph in i -th view group and the adaptive weight learning strategy is used with non-parameter form. Finally, the global consensus graph U is constructed by multiple intra-group consensus representation S ( i ) . Based on this, we can obtain final clustering result on the convergence consensus graph U . Experiments show that TWMVC-BG algorithm outperform state-of-the-art comparison methods on seven kinds of multi-view datasets. On partial datasets, our method can reach 10 %-18 % improvement than the best comparison algorithm.},
  archive      = {J_PR},
  author       = {Zhenni Jiang and Hui Li and Xiyu Liu and Wenke Zang},
  doi          = {10.1016/j.patcog.2025.112202},
  journal      = {Pattern Recognition},
  month        = {3},
  pages        = {112202},
  shortjournal = {Pattern Recognition},
  title        = {An effective two-stage auto-weighted multi-view consensus clustering via bipartite graph},
  volume       = {171},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Image compression using optimal transport mapping based on ranking visual saliency. <em>PR</em>, <em>171</em>, 112201. (<a href='https://doi.org/10.1016/j.patcog.2025.112201'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Due to the growth of multimedia technology and increased popularity of virtual reality (VR) and augmented reality (AR) applications, the need for efficient image transmission and storage is also proliferating. We propose an image compression method based on image region importance recognition and optimal transport (OT). As not all regions in an image is equally important, we can identify critical regions in the image for high-fidelity compression and use low-fidelity compression for less critical regions. In compression, critical regions are enlarged while less important regions are reduced in size. In this process we use OT to assign higher importance to the critical regions. In uncompression, inverse OT is used to restore the image. Our method has the advantages that, first, it is applicable to various compression frameworks, including both traditional and deep learning-based compression frameworks, and second, it is more flexible in adjusting the foreground and background of the image, and the ratio of the foreground to the background can be freely adjusted according to the actual needs of application. Experimental results show that our method can improve the quality of restored image while achieving a high compression rate. Comparisons under varying experimental conditions show that our method outperforms existing techniques at the same level of compression rate by improving reconstruction fidelity for key regions by 4.5–7.0 % (95 % confidence interval).},
  archive      = {J_PR},
  author       = {Zihang Li and Dongsheng An and Xianfeng Gu and Xiaoyin Xu and Min Zhang},
  doi          = {10.1016/j.patcog.2025.112201},
  journal      = {Pattern Recognition},
  month        = {3},
  pages        = {112201},
  shortjournal = {Pattern Recognition},
  title        = {Image compression using optimal transport mapping based on ranking visual saliency},
  volume       = {171},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Multi-branch perturbation learning with constraint simulation for semi-supervised semantic segmentation. <em>PR</em>, <em>171</em>, 112200. (<a href='https://doi.org/10.1016/j.patcog.2025.112200'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Current semi-supervised semantic segmentation (SSS) methods improve generalization via weak-to-strong pseudo-supervision with image perturbations. However, many methods are limited by employing a single perturbation mode and a specific weak-to-strong learning strategy, restricting exploration of the perturbation space and hindering performance in fine-grained segmentation. While diverse perturbations are intuitively beneficial, simply combining them can lead to inefficient optimization and instability. In this paper, we propose a multi-branch strong perturbation constraint learning framework for SSS. Our framework introduces a novel multi-branch perturbation learning (MSPL) strategy, employing multiple parallel branches with diverse strong augmentations to expand the perturbation space and capture complex semantic variations. We further design a novel constraint simulation loss (CSSL), based on a hierarchical consistency learning structure (weak-to-strong and strong-to-strong), which enforces strong-to-strong consistency between different perturbation branches. CSSL mitigates instability and enhances robustness to perturbation-induced noise, enabling the network to better generalize and achieve more accurate segmentation, especially for fine object boundaries. Extensive evaluations on benchmark datasets (PASCAL VOC 2012, Cityscapes, COCO) demonstrate that our method achieves state-of-the-art performance. Ablation studies further validate the effectiveness of our proposed MSPL and CSSL components.},
  archive      = {J_PR},
  author       = {Ruyu Liu and Feng Xiao and Jianhua Zhang and Xiufeng Liu and Xu Cheng and Shengyong Chen and Bo Sun and Houxiang Zhang},
  doi          = {10.1016/j.patcog.2025.112200},
  journal      = {Pattern Recognition},
  month        = {3},
  pages        = {112200},
  shortjournal = {Pattern Recognition},
  title        = {Multi-branch perturbation learning with constraint simulation for semi-supervised semantic segmentation},
  volume       = {171},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). MoCoPCI: Inter-frame motion correlation guided efficient 3D point cloud interpolation. <em>PR</em>, <em>171</em>, 112199. (<a href='https://doi.org/10.1016/j.patcog.2025.112199'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {LiDAR point cloud streams provide precise 3D mapping and detailed spatial information but limited temporal resolution due to hardware constraints. Multi-frame point cloud interpolation, which synthesizes intermediate frames between consecutive point clouds, presents a promising solution. However, previous methods treat point cloud sequence interpolation as an iterative process, resulting in prolonged inference times and making it challenging to maintain the temporal consistency of the predicted point cloud sequence. To address these issues, we propose MoCoPCI, a novel one-shot sequence interpolation framework designed for faster and more accurate nonlinear predictions. The core of our approach is a motion correlation-based bidirectional flow prediction, which ensures temporal consistency in the predicted point cloud sequence while significantly improving efficiency. Additionally, we introduce a cross-attention mechanism based on extrapolation and injection to capture multi-scale motion information from consecutive frames. Meanwhile, a point-wise compensation mechanism refines the warped points, collectively enhancing the network’s ability to model nonlinear and non-rigid motion. Extensive experiments on benchmark outdoor datasets demonstrate that MoCoPCI achieves state-of-the-art performance, significantly reducing the Chamfer distance by an impressive 24.35 %, while attaining a remarkable 2-fold speedup in inference time. The code is available at https://github.com/icdm-adteam/MoCoPCI .},
  archive      = {J_PR},
  author       = {Pengfei Yang and Feng Wu and Minyang Liu and Ting Zhong and Fan Zhou},
  doi          = {10.1016/j.patcog.2025.112199},
  journal      = {Pattern Recognition},
  month        = {3},
  pages        = {112199},
  shortjournal = {Pattern Recognition},
  title        = {MoCoPCI: Inter-frame motion correlation guided efficient 3D point cloud interpolation},
  volume       = {171},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Self-supervised learning video anomaly detection based on time interval prediction and noise classification. <em>PR</em>, <em>171</em>, 112198. (<a href='https://doi.org/10.1016/j.patcog.2025.112198'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Video Anomaly Detection (VAD) aims to automatically identify anomalous events in videos that significantly deviate from normal behavioral patterns. Self-supervised learning motivates models to learn effective features from unlabeled data by designing proxy tasks. However, existing approaches often rely on coarse-grained modeling, focusing mainly on global sequence order or holistic scene structures, which may limit their ability to capture subtle motion changes or localized anomalies. Therefore, this paper proposes a self-supervised learning framework combined with fine-grained spatio-temporal proxy tasks to extract key features more accurately. For the temporal branch, we design a time interval prediction task: given a fixed middle frame and randomly sampled frames from both sides, the model predicts their temporal intervals relative to the center frame, thereby modeling the dynamic patterns of behavior. To enhance temporal modeling capabilities, we introduce a multi-head self-attention mechanism to capture inter-frame dependencies in the input sequence. The spatial branch employs a noise classification task inspired by diffusion models, where varying levels of noise are added to image patches, and the model predicts the corresponding noise levels. This encourages learning of local appearance features and patch-level sensitivity to perturbations. Our method is trained in an end-to-end manner and does not rely on pre-trained models. Experiments on three benchmark datasets demonstrate stable performance: the method achieves AUC scores of 98.6 % on UCSD Ped2, 91.7 % on CUHK Avenue, and 83.7 % on ShanghaiTech. These results suggest that the proposed approach can generalize well across different scenes, perspectives, and types of anomalous behavior.},
  archive      = {J_PR},
  author       = {Yishuo Liu and Chuanxu Wang and Qingyang Yang and Lanxiao Li and Binghui Wang},
  doi          = {10.1016/j.patcog.2025.112198},
  journal      = {Pattern Recognition},
  month        = {3},
  pages        = {112198},
  shortjournal = {Pattern Recognition},
  title        = {Self-supervised learning video anomaly detection based on time interval prediction and noise classification},
  volume       = {171},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). GrEp: Graph-based epithelial cell classification refinement in histopathology H&E images. <em>PR</em>, <em>171</em>, 112197. (<a href='https://doi.org/10.1016/j.patcog.2025.112197'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The automatic cell segmentation and classification from whole slide images plays an important role in digital pathology, unlocking new opportunities for biomarker discovery. Despite extensive research, this task faces persistent challenges such as the differentiation of epithelial cells into normal and malignant. Many existing models lack reporting of epithelial subtyping, and when available, their performance is often suboptimal. This work benchmarks state-of-the-art methods to highlight this limitation and introduces GrEp, a geometric deep learning strategy that considers the broader epithelium tissue architecture to infer cell-level classification rather than relying exclusively on nuclei morphology. The proposed graph-based workflow significantly outperformed state-of-the-art nuclei classification models in colorectal cancer and generalized effectively to two unseen tissue types, endometrium and pancreas, proving the robustness of the geometry-based model. Given its speed and accuracy, we believe GrEp to be a valuable method to refine epithelial cell classification for downstream analyses in clinical and research settings.},
  archive      = {J_PR},
  author       = {Ana Leni Frei and Javier Garcia-Baroja and Tilman Rau and Christina Neppl and Alessandro Lugli and Wiebke Solass and Martin Wartenberg and Andreas Fischer and Inti Zlobec},
  doi          = {10.1016/j.patcog.2025.112197},
  journal      = {Pattern Recognition},
  month        = {3},
  pages        = {112197},
  shortjournal = {Pattern Recognition},
  title        = {GrEp: Graph-based epithelial cell classification refinement in histopathology H&E images},
  volume       = {171},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Light field collaborative perception for visual object tracking. <em>PR</em>, <em>171</em>, 112196. (<a href='https://doi.org/10.1016/j.patcog.2025.112196'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Applying powerful appearance models to locate moving targets is gaining popularity. However, distracting elements in complex scenes often lead to deviations in target perception. Compared with appearance cues, explicit 4D spatial-angular cues provided by light field is more beneficial for addressing this problem, but they are far from being fully explored in existing visual trackers. In this paper, we propose a collaboration graph-based light field collaborative perception network (LFCPNet) that decomposes the boundary perception to mine light field spatial-angular cues. Furthermore, we propose an adaptive propagation method to deploy appearance and spatial-angular cues in tracking. By assessing the adaptive score from the propagated tracking status, the tracker can effectively combine multi-modal cues from light field to distinguish similar objects. Our tracking method is evaluated using R8TRACK, a light field tracking dataset that we collect. The experiments show that our method outperforms the state-of-the-art methods and significantly improves the tracking performance.},
  archive      = {J_PR},
  author       = {Mianzhao Wang and Fan Shi and Xu Cheng and Meng Zhao},
  doi          = {10.1016/j.patcog.2025.112196},
  journal      = {Pattern Recognition},
  month        = {3},
  pages        = {112196},
  shortjournal = {Pattern Recognition},
  title        = {Light field collaborative perception for visual object tracking},
  volume       = {171},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Fast multi-view clustering via tensor hyperbolic tangent-p norm minimization. <em>PR</em>, <em>171</em>, 112195. (<a href='https://doi.org/10.1016/j.patcog.2025.112195'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Tensor-based multi-view clustering methods have gained significant attention due to their ability to directly capture high-order information, often outperforming matrix-based approaches. However, these methods face challenges in efficiently processing large-scale datasets due to their high computational complexity. Moreover, most existing tensor-based approaches rely on the tensor nuclear norm (TNN) to approximate the tensor rank function. However, TNN penalizes larger singular values, which are essential for preserving critical structural information, thus constraining the extraction of multi-view information. To address these challenges, we propose a novel fast multi-view clustering method via tensor hyperbolic tangent- p norm minimization. First, we incorporate an efficient anchor selection strategy and construct tensors from anchor-based representations, significantly reducing the computational burden of tensor-based approaches for large-scale datasets. Second, we introduce the tensor hyperbolic tangent- p norm (THT p N), a more robust and accurate approximation of the tensor rank function, enabling improved extraction of multi-view consistency and complementarity. Extensive experiments on eight real-world datasets show that our proposed model not only surpasses tensor-based methods in clustering performance but also outperforms matrix-based methods in computational efficiency, establishing a new benchmark for fast multi-view clustering. Code is available at https://github.com/usualheart/FTHMC .},
  archive      = {J_PR},
  author       = {Yongbo Yu and Zhoumin Lu and Jingjing Xue and Rong Wang and Zongcheng Miao and Feiping Nie},
  doi          = {10.1016/j.patcog.2025.112195},
  journal      = {Pattern Recognition},
  month        = {3},
  pages        = {112195},
  shortjournal = {Pattern Recognition},
  title        = {Fast multi-view clustering via tensor hyperbolic tangent-p norm minimization},
  volume       = {171},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Token pyramid pooling-driven style adapter learning with dual-view balanced loss for imbalanced diabetic retinopathy grading. <em>PR</em>, <em>171</em>, 112194. (<a href='https://doi.org/10.1016/j.patcog.2025.112194'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Precise diabetic retinopathy (DR) grading is essential for developing personalized and effective treatment plans. Although deep neural networks (DNNs) have achieved promising DR grading results, constructing a precise and trustworthy DR grading model remains challenging due to limited high-quality medical image data, high computational costs, and imbalanced data distributions. To tackle these challenges, we explore the transferability of feature representations from pre-trained vision foundation models (VFMs) to fundus images through adapter learning, aiming to build an efficient imbalanced DR grading model. Unlike classical full-tuning, which fine-tunes all pre-trained parameters of VFMs, adapter learning achieves competitive performance by adding negligible fine-tuned parameter number. Motivated by the above analysis, we develop a Token Pyramid Pooling-Driven Style Adapter Learning (TPDSAL) to better capture task-specific feature representations from VFMs, which fully exploits pathological distribution prior of DR and the inherent fundus imaging characteristics. Besides, we propose a novel dual-view balanced loss (DVB) to improve imbalanced DR grading performance and trustworthiness, which explores the potential of training class frequencies in sample-wise predicted logit space and sample-wise loss value space simultaneously. Extensive experiments on four public fundus image datasets manifest the superiority of our TPDSAL with DVB over competitive transfer tuning and loss methods in terms of imbalanced grading performance and trustworthiness. Further analysis suggests that clinical prior knowledge utilization is beneficial for adapter learning in capturing task-specific feature representations from VFMs.},
  archive      = {J_PR},
  author       = {Jilu Zhao and Xiaoqing Zhang and Jiawei Zhang and Hanxi Sun and Qiushi Nie and Zunjie Xiao and Linxia Xiao and Fengyun Zhang and Yan Hu and Jiang Liu},
  doi          = {10.1016/j.patcog.2025.112194},
  journal      = {Pattern Recognition},
  month        = {3},
  pages        = {112194},
  shortjournal = {Pattern Recognition},
  title        = {Token pyramid pooling-driven style adapter learning with dual-view balanced loss for imbalanced diabetic retinopathy grading},
  volume       = {171},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Video-based human pose estimation via feature decoupling and multi-hypothesis calibration. <em>PR</em>, <em>171</em>, 112193. (<a href='https://doi.org/10.1016/j.patcog.2025.112193'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Video-based human pose estimation is a challenging task in computer vision. Most existing methods directly employ neural networks to extract mixed high-dimensional spatiotemporal features and estimate a deterministic pose solution for each frame. However, such methods heavily rely on the original intricate features that entangle redundant information, and neglect pose ambiguity incurred by degradations such as occlusions and blur. In this paper, we present a novel framework which addresses the above issues from two aspects. (i) We propose a Hierarchical Representation Decoupling module, along with a theoretical mutual information objective, to explicitly decouple the original spatiotemporal features into multiple non-overlapping lower-dimensional components and remove redundancy. (ii) We further introduce a Context-Aware Multi-hypothesis Inference module, reasoning a set of potential hypotheses for each frame based on different spatiotemporal semantics to compensate for the pose ambiguity. Extensive experiments demonstrate that our approach outperforms state-of-the-art methods on three benchmarks, PoseTrack2017, PoseTrack2018, and PoseTrack21.},
  archive      = {J_PR},
  author       = {Runyang Feng and Tze Ho Elden Tse and Haoming Chen and Hyung Jin Chang and Haifeng Zhong and Yixing Gao},
  doi          = {10.1016/j.patcog.2025.112193},
  journal      = {Pattern Recognition},
  month        = {3},
  pages        = {112193},
  shortjournal = {Pattern Recognition},
  title        = {Video-based human pose estimation via feature decoupling and multi-hypothesis calibration},
  volume       = {171},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Dual-hierarchical knowledge distillation for video captioning. <em>PR</em>, <em>171</em>, 112192. (<a href='https://doi.org/10.1016/j.patcog.2025.112192'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Video captioning remains a challenging task due to the tradeoff between accuracy and computational efficiency. We propose CapDistill , a dual hierarchical distillation framework that transfers semantic knowledge from a powerful teacher model to a lightweight student model. CapDistill captures object-level and action-level semantics from captions and transfers multilevel knowledge including object features, action features, and word-level predictions through a hierarchical strategy. To reduce the impact of noisy annotations, we introduce a caption quality grading mechanism that assigns quality-based weights to training captions. Experiments on MSR-VTT and MSVD demonstrate that CapDistill achieves state-of-the-art accuracy while significantly reducing inference cost. Code is available at: https://github.com/ccc000-png/SFTCap .},
  archive      = {J_PR},
  author       = {HuiLan Luo and SiQi Wan and Xia Cai and ChanJuan Wang and HongKun Chen},
  doi          = {10.1016/j.patcog.2025.112192},
  journal      = {Pattern Recognition},
  month        = {3},
  pages        = {112192},
  shortjournal = {Pattern Recognition},
  title        = {Dual-hierarchical knowledge distillation for video captioning},
  volume       = {171},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). DocAligner: Automating the annotation of photographed documents through real-virtual alignment. <em>PR</em>, <em>171</em>, 112191. (<a href='https://doi.org/10.1016/j.patcog.2025.112191'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, data-driven approaches have dominated the field of Document Artificial Intelligence (DAI), making high-quality, large-scale annotated data increasingly crucial. Consequently, numerous automated annotation methods have been developed for annotating scanned and digital-born documents. However, these techniques prove ineffective in new scenarios involving photographed documents. Combined with the time-consuming and labor-intensive nature of manual annotation, these limitations have significantly hindered the development of DAI in photographed contexts. To address this, we propose DocAligner, a novel method that automates the annotation of photographed documents through real-virtual alignment. DocAligner establishes dense correspondences between real photographed documents and their virtual clean counterparts, transferring pre-existing annotations from the virtual to the real domain. DocAligner incorporates several innovative features to account for the characteristics of document images. Firstly, it utilizes a non-rigid pre-alignment to address non-rigid deformations in document pairs. Secondly, it performs multi-scale alignment based on multi-scale features to handle large displacements and ensure high accuracy. Additionally, DocAligner employs a GRU-based recurrent module to enhance the output flows at high resolution space, obtaining better fine-grained alignment. We construct a synthetic dataset for the supervised learning of DocAligner, and then adopt a self-supervised learning approach based on real-world data to improve the robustness in real-world scenarios. Extensive experiments demonstrate the effectiveness of DocAligner-annotated data for five photographed DAI tasks, along with a notable improvement in efficiency compared to manual annotation. Codes and datasets are available at https://github.com/ZZZHANG-jx/DocAligner .},
  archive      = {J_PR},
  author       = {Jiaxin Zhang and Peirong Zhang and Huiyi Cheng and Xinhong Chen and Haowei Xu and Kai Ding and Lianwen Jin},
  doi          = {10.1016/j.patcog.2025.112191},
  journal      = {Pattern Recognition},
  month        = {3},
  pages        = {112191},
  shortjournal = {Pattern Recognition},
  title        = {DocAligner: Automating the annotation of photographed documents through real-virtual alignment},
  volume       = {171},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Robust tube localization for mars sample return: Lightweight YOLO-segmentation with angle-guided PnP. <em>PR</em>, <em>171</em>, 112190. (<a href='https://doi.org/10.1016/j.patcog.2025.112190'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {One considered approach in the planned Mars Sample Return (MSR) campaign involves accurately identifying and retrieving sample tubes from the Martian surface. This paper presents an innovative approach that utilises lightweight computer’vision techniques to enhance the efficiency and accuracy of the Sample Transfer Arm (STA) aboard the MSR lander. Our methodology employs the YOLOv8 deep learning model for image segmentation, and centroid detection of tubes in the challenging dusty Martian environment. These detected masks and centroids provide the foundation for constructing an outlined representation of the tubes, which is critical for precise spatial orientation. We exploit the knowledge of the object geometry to find key points and match them using their relative positions with respect to the geometry. Subsequently, a Perspective-n-Point (PnP) algorithm with RANSAC utilizes this outline and pre-computed 3D coordinates to ascertain the tube’s pose. This enables the STA’s camera-equipped gripper to locate and retrieve the samples accurately. This process is meticulously tailored for the constrained computational resources available on Martian missions, addressing limitations in processing speed and lack of parallelization capabilities. Extensive simulations under Martian-like conditions demonstrate the robustness and reliability of our approach, which would be a necessary technology to enable a backup tube retrieval concept for a MSR campaign using a robotic arm by ensuring precise and efficient sample collection. This method can achieve sub-degree and sub-centimeter accuracy with a single image.},
  archive      = {J_PR},
  author       = {Daniel Posada and Tu-Hoa Pham and Nikos Mavrakis and Philip Bailey},
  doi          = {10.1016/j.patcog.2025.112190},
  journal      = {Pattern Recognition},
  month        = {3},
  pages        = {112190},
  shortjournal = {Pattern Recognition},
  title        = {Robust tube localization for mars sample return: Lightweight YOLO-segmentation with angle-guided PnP},
  volume       = {171},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). A coarse-fine matching method as the visual guidance of uniform allowances for robot grinding. <em>PR</em>, <em>171</em>, 112189. (<a href='https://doi.org/10.1016/j.patcog.2025.112189'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Matching, as a critical component in the visual guidance of robotic abrasive belt grinding and polishing, enables the establishing a material allowance distribution model between the scanned data and design model of the aero blade workpiece. However, conventional matching methods yield models with poor consistency, which exacerbates the instability of the robotic grinding and polishing system. This paper constructs an objective function that minimizes the component distance under local Frenet frame. It introduces the constraint on uniform allowances about machining conditions. A space location for subsequent processing is found in solving the multi-objective optimization function. To speed up the iterative process, the method utilizes a cosine similarity vector and incorporates the double features of torsion and curvature to assist the similarity function in making judgments. The paper also examines the principle behind setting the similarity threshold. However, the initial position relationship must be sufficiently close for the target point cloud to reach the desired position during iterations. Otherwise, the matching calculation will converge to the local minimum. To address this issue, the paper proposes the objective constraint of minimizing the density entropy difference on the partitions after cylinder parametrization of the point cloud. It determines the initial spatial positional relationship within the controllable error. Through comparison with both state-of-the-art and classical matching algorithms, the proposed method demonstrates superior performance in efficiency, precision, convergence, and consistency of allowance distribution.},
  archive      = {J_PR},
  author       = {Jingyu Sun and Yadong Gong and Mingjun Liu and Xianli Zhao and Jibin Zhao},
  doi          = {10.1016/j.patcog.2025.112189},
  journal      = {Pattern Recognition},
  month        = {3},
  pages        = {112189},
  shortjournal = {Pattern Recognition},
  title        = {A coarse-fine matching method as the visual guidance of uniform allowances for robot grinding},
  volume       = {171},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). SFIR: Optimizing spatial and frequency domains for image restoration. <em>PR</em>, <em>171</em>, 112188. (<a href='https://doi.org/10.1016/j.patcog.2025.112188'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Image restoration (IR) plays a pivotal role in image processing, significantly influencing subsequent computer vision tasks. Recent advancements have highlighted the efficacy of deep neural networks in enhancing image restoration. Yet, these methods often underutilize critical spatial and frequency domain information, thereby constraining their performance. Specifically, Transformer-based techniques, despite bolstering global feature associations in the spatial domain, incur high computational costs and yield marginal performance gains. Addressing these limitations, we introduce SFIR, a novel convolutional network model designed for comprehensive image degradation correction. SFIR incorporates an encoder-decoder architecture, with its core innovation rooted in two modules: the Multi-scale Spatial Enhancement (MSE) module and the Frequency Amplitude Modulation (FAM) module. The MSE module enhances the model’s ability to harness spatial domain information. It achieves this by combining multi-scale feature fusion with sophisticated local window-based non-local attention mechanisms. In addition, MSE also includes an adaptive detail enhancement block, which further optimizes the model’s ability to handle complex degradations by refining detail features and smoothing features. On the other hand, the FAM module effectively addresses the challenges of substantial amplitude map variances across various channels. It uses dynamic weighting modulation to improve image clarity and restore texture details. Collectively, these modules enhance SFIR’s efficiency in leveraging both spatial and frequency domain insights, leading to notable restoration quality improvements. Experimental validation underscores SFIR’s superior performance in deraining, dehazing, and particularly deblurring, while also showcasing its advantages in parameter efficiency and computational cost. Besides, we also released the source code at https://github.com/ClimBin/SFIR .},
  archive      = {J_PR},
  author       = {Yubin Gu and Yuan Meng and Siting Chen and Jiayi Ji and Xiaoshuai Sun and Weijian Ruan and Rongrong Ji},
  doi          = {10.1016/j.patcog.2025.112188},
  journal      = {Pattern Recognition},
  month        = {3},
  pages        = {112188},
  shortjournal = {Pattern Recognition},
  title        = {SFIR: Optimizing spatial and frequency domains for image restoration},
  volume       = {171},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). GridCLIP: One-stage object detection by grid-level CLIP representation learning. <em>PR</em>, <em>171</em>, 112187. (<a href='https://doi.org/10.1016/j.patcog.2025.112187'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {CLIP provides a shared image-text representation space with rich and diverse vocabulary, enabling object detection in undersampled and unseen categories. Recent CLIP-based object detection works show two-stage detectors typically outperform one-stage designs, but with significantly higher computational costs. A fundamental limitation of a two-stage detector is region-level alignment (distillation), which requires hundreds of image encoder forward passes from both the detector and CLIP in each image. In this work, we propose GridCLIP, a one-stage detector that requires only a single image encoder inference per input image, achieving up to 43 × faster training and 5 × faster inference compared to its two-stage counterpart ViLD, while substantially narrowing the accuracy gap. GridCLIP introduces a dual alignment strategy to learn fine-grained, grid-level representations: (1) grid-level alignment: learning grid-level features aligned with CLIP text encoder using annotated category labels, and (2) image-level alignment: aggregating grid-level features into an image-level representation aligned with the CLIP image encoder, which allows GridCLIP to learn grid-level representations of a broad range of categories, especially undersampled and unseen categories. Experiments on the LVIS benchmark show that GridCLIP achieves competitive results, with strong generalization to COCO and VOC, demonstrating its efficiency and effectiveness as a CLIP-based detector.},
  archive      = {J_PR},
  author       = {Jiayi Lin and Shitong Sun and Shaogang Gong},
  doi          = {10.1016/j.patcog.2025.112187},
  journal      = {Pattern Recognition},
  month        = {3},
  pages        = {112187},
  shortjournal = {Pattern Recognition},
  title        = {GridCLIP: One-stage object detection by grid-level CLIP representation learning},
  volume       = {171},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). LogicMix: Sample mixing data augmentation for multi-label image classification with partial labels. <em>PR</em>, <em>171</em>, 112186. (<a href='https://doi.org/10.1016/j.patcog.2025.112186'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-label image classification datasets are often partially labeled where many labels are missing, posing a significant challenge to training accurate deep classifiers. Most existing approaches assume the missing labels as negatives and/or exploit image and category relationships to regularize training. Orthogonally, this paper studies blending samples in such incomplete datasets as new samples, extending the training data magnitude to increase generalization. First, the proposed LogicMix mixes multiple partially labeled samples to produce new samples, where their unknown labels are naturally mixed by OR’s logical equivalences, without replacement with constants. Subsequently, a Decouple Partial-Asymmetric Loss is proposed to assign separate label-focusing policies to original and new samples, addressing the learning imbalance from the different positive-negative label imbalances between original and augmented samples. Finally, we propose a complete learning framework called 2WayAug-PL. LogicMix and conventional data augmentation collaborate to extend the diversity of new samples in both the sample-sample relation and human prior knowledge, while pseudo-labeling compensates for the lack of labels to provide more supervision signals. 27 partially labeled dataset scenarios derived from three benchmarking datasets with various learning difficulties are utilized for comprehensive experiments. LogicMix has shown remarkable effectiveness and generality in improving mAP against compared sample-mixing data augmentation methods. In particular, 2WayAug-PL achieves state-of-the-art average mAP of 84.3 %, 50.1 %, and 93.8 % on MS-COCO, VG-200, and Pascal VOC 2007, respectively. It further pushes the previous best performance achieved by different frameworks by 0.6 % (CFT), 0.6 % (CFT), and 0.1 % (SR). Moreover, 2WayAug-PL significantly outperforms all compared frameworks, as shown by statistical tests. Code is available at: https://github.com/maxium0526/logic_mix .},
  archive      = {J_PR},
  author       = {Chak Fong Chong and Jielong Guo and Xu Yang and Wei Ke and Pedro Henriques Abreu and Yapeng Wang and Sio-Kei Im},
  doi          = {10.1016/j.patcog.2025.112186},
  journal      = {Pattern Recognition},
  month        = {3},
  pages        = {112186},
  shortjournal = {Pattern Recognition},
  title        = {LogicMix: Sample mixing data augmentation for multi-label image classification with partial labels},
  volume       = {171},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). MPR-net: Medicinal plant recognition network with dual-branch attention fusion. <em>PR</em>, <em>171</em>, 112185. (<a href='https://doi.org/10.1016/j.patcog.2025.112185'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The recognition of medicinal plant images possesses substantial practical significance. However, there are very few methods for recognizing medicinal plants worldwide. Additionally, existing public datasets for medicinal plants are limited in both size and diversity, encompassing only a small number of species, which constrains the development of high-precision recognition models. To address these challenges, we have constructed a comprehensive medicinal plant image benchmark, named the Karst Landform Herbs Dataset, for medicinal plant recognition research and also proposed a novel Dual-Branch Attention Fusion-based Medicinal Plant Recognition Network, called MPR-net, for Medicinal Plant image recognition. Our constructed new Karst Landform Herbs Dataset contains 56,650 images representing 120 species of medicinal plants. To guarantee the effectiveness on natural images with complex background, the proposed MPR-net designs a dual-branch fused attention module for discriminative feature extraction, thereby enhancing its suitability for medicinal plant recognition tasks. Through a series of tests and evaluations conducted on the Karst Landform Herbs Dataset, as well as other publicly available medicinal plant datasets, experimental results demonstrate that the proposed method achieves outstanding performance in medicinal plant recognition, significantly enhancing recognition accuracy and model robustness, and maintains lightweight. Our code is available at https://github.com/lingf5877/MPR-Net , dataset is available at https://data.mendeley.com/drafts/pskvtbsmzw .},
  archive      = {J_PR},
  author       = {Zhanyan Tang and Yusen Fu and Mu Li and Huiling Liang and Yibing Tang and Jie Wen},
  doi          = {10.1016/j.patcog.2025.112185},
  journal      = {Pattern Recognition},
  month        = {3},
  pages        = {112185},
  shortjournal = {Pattern Recognition},
  title        = {MPR-net: Medicinal plant recognition network with dual-branch attention fusion},
  volume       = {171},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Medical image segmentation using dual-decoder mutual teaching with a mean teacher framework. <em>PR</em>, <em>171</em>, 112184. (<a href='https://doi.org/10.1016/j.patcog.2025.112184'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Accurate segmentation of medical images is essential for many clinical applications and is now typically achieved by training deep learning models on large annotated datasets. However, acquiring sufficient labeled images remains challenging, as pixel-level manual annotations are highly time-consuming. To substantially reduce the manual effort, we developed a novel semi-supervised segmentation method, termed dual-decoder mutual teaching (DDMT), which incorporates a smoothed exponential moving average (sEMA) scheme and a shape consistency constraint (SCC) scheme into the classical mean teacher (MT) framework. The sEMA scheme enhances the stability of the student and teacher models during training, while the SCC scheme ensures consistent learning of shape characteristics across the two different decoders within each model. With these two innovative components, DDMT achieves promising segmentation performance when trained on limited labeled images and abundant unlabeled images. Experiments on public datasets for left atrium, pancreas, and optic disc segmentation demonstrated that DDMT consistently outperforms several state-of-the-art semi-supervised learning (SSL) methods (e.g., MT, UAMT, DTC, and MCNet) across varying proportions of labeled images. The source code is publicly available at https://github.com/wmuLei/ddmt.},
  archive      = {J_PR},
  author       = {Juan Zhang and Gaoqiang Jiang and Zhongwen Li and Bihan Tian and Shuchen Yu and Qingxiang Yu and Jie Zhou and Hao Chen and Jiantao Pu and Quanyong Yi and Lei Wang},
  doi          = {10.1016/j.patcog.2025.112184},
  journal      = {Pattern Recognition},
  month        = {3},
  pages        = {112184},
  shortjournal = {Pattern Recognition},
  title        = {Medical image segmentation using dual-decoder mutual teaching with a mean teacher framework},
  volume       = {171},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). A core knowledge reasoning architecture for scene graph. <em>PR</em>, <em>171</em>, 112183. (<a href='https://doi.org/10.1016/j.patcog.2025.112183'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {How to comprehensively understand scene graphs to extract rich object representations and relationship predictions has been a formidable challenge. However, the intrinsic issues of incomplete data and long-tail relationship categories in scene graphs have severely constrained advancements. Hence, we propose an architecture based on Core knOwledge Reasoning (CORE) to alleviate them. It consists of a core entity reasoning framework and a core relation reasoning framework, mining main knowledge from the visual and commonsense data respectively. The latent core insights of the data and models are extensively excavated, which greatly alleviates the problem of incomplete data and guides the generation of more abundant and meaningful predictions. In addition, we introduce a Bayesian inference network to better balance the trade-offs between these two types of knowledge. It harmonizes visual knowledge and commonsense knowledge through an optimal matching strategy with a view to improving the long-tail distribution of data. Ultimately, we have generated the most enriched scene graph of semantic knowledge through guidance in this architecture. Extensive experiments on the Visual Genome dataset and Open Image V6 dataset have demonstrated the exceptional performance of CORE in extracting core knowledge.},
  archive      = {J_PR},
  author       = {Na Tian and Youjia Shao and Xiangfu Ding and Li Wang and Wencang Zhao},
  doi          = {10.1016/j.patcog.2025.112183},
  journal      = {Pattern Recognition},
  month        = {3},
  pages        = {112183},
  shortjournal = {Pattern Recognition},
  title        = {A core knowledge reasoning architecture for scene graph},
  volume       = {171},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Domain aware post training quantization for vision transformers in deployment. <em>PR</em>, <em>171</em>, 112182. (<a href='https://doi.org/10.1016/j.patcog.2025.112182'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Despite the increasing popularity of Vision Transformers (ViTs) on vision tasks, their deployment on edge devices presents two main challenges: performance degradation due to the necessary model compression amidst computational constraints, and accuracy drop stemming from domain shift effects. Although existing post-training quantization (PTQ) methods can reduce computational load for ViTs, they often fail under extreme low-bit conditions and domain shift scenarios. To address the two challenges, this paper introduces a novel D omain A ware Post-training Quant ization ( DAQuant ) approach that simultaneously tackles extreme model compression and domain adaptation for ViTs in deployment. Compared with large language models (LLMs), the activations in ViTs account for a much larger portion of memory in inference, making the ViT quantization more difficult. We empirically found that simply adopting the equivalent smoothing strategy in LLM quantization would likely fail in ViTs, as it still results in a lot of inferior outliers. Consequently, the proposed DAQuant sophisticated developed a distribution-aware smoothing method with Learnable Activation Clipping (LAC) to mitigate the effects of outliers. Additionally, we propose an effective domain alignment strategy to improve the model’s generalizability, which preserves model’s optimization on source domain while enhancing generalization ability on the target domain. DAQuant demonstrates superior performance in both quantization error and generalization capacity, outperforming existing quantization methods significantly in real-device deployment scenarios.},
  archive      = {J_PR},
  author       = {Li Wang and Chao Zeng and Miao Zhang and Jianlong Wu and Liqiang Nie},
  doi          = {10.1016/j.patcog.2025.112182},
  journal      = {Pattern Recognition},
  month        = {3},
  pages        = {112182},
  shortjournal = {Pattern Recognition},
  title        = {Domain aware post training quantization for vision transformers in deployment},
  volume       = {171},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). A general dual-view framework for instance weighted naive bayes. <em>PR</em>, <em>171</em>, 112181. (<a href='https://doi.org/10.1016/j.patcog.2025.112181'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Instance weighting is an effective and flexible method to alleviate the attribute conditional independence assumption in naive Bayes (NB). However, existing instance weighting methods mainly focus on how to learn a specific weight for each instance, ignoring the limitation of the original view. In this study, we argue that real-world applications are rather complicated, and it is sub-optimal to learn instance weights only using the original view. Based on this premise, we propose a novel general framework called dual-view instance weighted naive Bayes (DIWNB). In DIWNB, we first construct multiple K-nearest neighbor (KNN) classifiers and select those with the lowest error rate to classify each training instance in turn to build the generated view. Next, we learn a specific weight for each training instance, and build an instance weighted NB model in each view. Finally, we weightedly fuse the class-membership probabilities of dual views to predict the class label for each test instance. To construct the generated view, we design a hard label approach and a soft label approach, and thus two different versions are created, which we denote as DIWNB H and DIWNB S , respectively. Experimental results on 60 benchmark and 2 real-world datasets demonstrate the effectiveness of DIWNB.},
  archive      = {J_PR},
  author       = {Huan Zhang and Kexin Meng and Pei Lv and Shuo He and Mingliang Xu},
  doi          = {10.1016/j.patcog.2025.112181},
  journal      = {Pattern Recognition},
  month        = {3},
  pages        = {112181},
  shortjournal = {Pattern Recognition},
  title        = {A general dual-view framework for instance weighted naive bayes},
  volume       = {171},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Deep learnable spectral decomposition of 3D baby faces. <em>PR</em>, <em>171</em>, 112180. (<a href='https://doi.org/10.1016/j.patcog.2025.112180'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we introduce a novel, deep 3D morphable model for meshes with common triangulation. Specifically, we apply it to reconstruct baby faces. The proposed algorithm is simple, adaptable, and specifically targeted to perform well on small datasets. We combine Graph-Laplacian based spectral decomposition with a learnable, transformer-like component. The decomposition matrices are applied as skip-connections, providing our architecture with a prior that encodes both local and global information of the underlying mesh structure. The learnable component does not make any domain-specific assumptions and can override the prior, if necessary. This flexibility also allows our model to perform well on larger datasets. We further modify the decomposition matrices to create deeper versions of this architecture and introduce a data augmentation strategy: flipping and rotations are applied to the deviations from the mean, rather than directly to the samples. In our experiments, we compare the reconstruction error of the proposed architecture against the state of the art, examine the effect of data augmentation across a small baby face dataset and a larger adult dataset and inspect our model’s capabilities to generate new samples from the encoded distribution. We show that our method outperforms current baby face models, as well as state of the art 3D morphable models, especially on the raw data. Additionally, we demonstrate that the proposed data augmentation substantially improves existing models.},
  archive      = {J_PR},
  author       = {Michael Zappe and Antonia Alomar and Marius George Linguraru and Gemma Piella and Federico M. Sukno},
  doi          = {10.1016/j.patcog.2025.112180},
  journal      = {Pattern Recognition},
  month        = {3},
  pages        = {112180},
  shortjournal = {Pattern Recognition},
  title        = {Deep learnable spectral decomposition of 3D baby faces},
  volume       = {171},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). SegMIC: A universal model for medical image segmentation through in-context learning. <em>PR</em>, <em>171</em>, 112179. (<a href='https://doi.org/10.1016/j.patcog.2025.112179'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Medical images encompass a wide array of modalities and anatomical structures, requiring numerous segmentation tasks. However, current methods tend to be highly specialized, struggling to generalize to Out-of-Distribution tasks without retraining. Developing a universal segmentation model is thus a valuable yet challenging goal. In this work, we introduce SegMIC, a novel learning paradigm that addresses these challenges through In-Context Learning. The primary aim of SegMIC is to perform segmentation across arbitrary anatomies by harnessing contextual information derived from a single reference image and its corresponding annotation. Building on this concept, SegMIC employs a tailored joint mask on the image-annotation pairs of both the query and the reference, compelling the model to perform tasks conditioned on visible contextual annotation patches. Thus the inference process is streamlined, requiring only a single reference pair to specify the desired task. To train our SegMIC, we compiled an unprecedented large-scale segmentation benchmark (UniMedDB), comprising totaling 49k images and 82k annotations spanning over 14 modalities and dozens of anatomical structures. Comprehensive experiments demonstrate that SegMIC adeptly utilizes context to handle diverse segmentation tasks across various modalities and anatomies, achieving superior Dice and IoU scores of 0.919 and 0.870 on in-distribution tasks while outperforming the best competitors on out-of-distribution tasks by margins of 0.111 in Dice and 0.146 in IoU. Our code and datasets are available at https://github.com/JWZhao-uestc/SegMIC .},
  archive      = {J_PR},
  author       = {Jianwei Zhao and Fan Yang and Xin Li and Zhicheng Jiao and Qiang Zhai and Xiaomeng Li and De Wu and Huazhu Fu and Hong Cheng},
  doi          = {10.1016/j.patcog.2025.112179},
  journal      = {Pattern Recognition},
  month        = {3},
  pages        = {112179},
  shortjournal = {Pattern Recognition},
  title        = {SegMIC: A universal model for medical image segmentation through in-context learning},
  volume       = {171},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). GraphMSR: A graph foundation model-based approach for MRI image super-resolution with multimodal semantic integration. <em>PR</em>, <em>171</em>, 112178. (<a href='https://doi.org/10.1016/j.patcog.2025.112178'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {High-resolution medical imaging is essential for accurate disease diagnosis and treatment planning, yet obtaining such images often involves prohibitive hardware costs and extended acquisition times. To address this challenge, recent advances in artificial intelligence have introduced graph foundation models, which have demonstrated remarkable capabilities in capturing complex relationships and integrating multimodal information across various domains. Recognizing the potential of these models in medical imaging, particularly their ability to naturally represent anatomical structures and their relationships while incorporating clinical knowledge, we introduce GraphMSR, a novel graph foundation model that revolutionizes medical image super-resolution. Our framework leverages the representational power of graph neural networks to model both local anatomical details and global structural relationships, while introducing innovations in graph construction, semantic integration, and attention mechanisms specifically designed for medical imaging tasks. Through extensive validation on large-scale medical imaging datasets, including brain MRI scans, our approach demonstrates superior performance in both quantitative metrics and visual assessments, achieving significant improvements in structural preservation and detail enhancement compared to state-of-the-art methods.},
  archive      = {J_PR},
  author       = {Zhiquan Qin and Zihao He and Yan Zhang and Yunhang Shen and Ke Li},
  doi          = {10.1016/j.patcog.2025.112178},
  journal      = {Pattern Recognition},
  month        = {3},
  pages        = {112178},
  shortjournal = {Pattern Recognition},
  title        = {GraphMSR: A graph foundation model-based approach for MRI image super-resolution with multimodal semantic integration},
  volume       = {171},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Temporal consistent multi-view perception for robust embodied manipulation. <em>PR</em>, <em>171</em>, 112177. (<a href='https://doi.org/10.1016/j.patcog.2025.112177'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Perception plays a critical role in developing effective policies for multi-task embodied manipulation, in which visual comprehension and task interpretation are essential. Existing methods typically rely on multi-view 2D representations for visual perception, aiming to build computation-friendly perception modules through imitation learning from extensive collections of high-quality robot trajectories. However, these approaches face significant challenges when expert demonstrations are limited or tasks are highly complex, resulting in inefficiencies. To address these limitations, we propose T emporal Consistent M ulti- V iew P erception (TMVP), a sample-efficient two-stage framework for robot manipulation that integrates temporal information into multi-view representations. Specifically, TMVP employs contrastive learning to extract meaningful, task-relevant features from visual inputs, enhancing temporal consistency and alignment with task instructions. This results in visual representations that are temporally coherent and grounded in task trajectories, enabling the model to better comprehend and execute complex manipulation tasks from diverse perspectives. Experiments conducted on RLBench demonstrate that TMVP outperforms baseline models across a wide range of tasks, achieving superior multi-task performance and few-shot training efficiency. These results highlight the potential of TMVP as an efficient and effective solution for embodied manipulation.},
  archive      = {J_PR},
  author       = {Haoyuan Chen and Rushuai Yang and Junjie Zhang and Xiaoyu Wen and Yi Chen and Dengxiu Yu and Chenjia Bai and Zhen Wang},
  doi          = {10.1016/j.patcog.2025.112177},
  journal      = {Pattern Recognition},
  month        = {3},
  pages        = {112177},
  shortjournal = {Pattern Recognition},
  title        = {Temporal consistent multi-view perception for robust embodied manipulation},
  volume       = {171},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Adaptive proximal regularization for image smoothing. <em>PR</em>, <em>171</em>, 112176. (<a href='https://doi.org/10.1016/j.patcog.2025.112176'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Image filters based on global optimization have garnered significant attention recently. However, existing optimization models struggle to effectively accommodate the sparsity requirements on the regularization terms. In this paper, we propose to build the regularization term upon the proximal p -norm, which is shown to be able to achieve sparser results. Furthermore, we propose a novel weighting scheme, which adapts to not only the balancing parameter, but also the bandwidth and the sparsity parameters in the proximal p -norm. By adopting different weights, our model provides a general framework for both edge- and structure-preserving image smoothing. Furthermore, in order to overcome the blocky artifacts in two-directional gradient regularization (4-neighborhood), we propose to consider the pixel differences in two more directions (8-neighborhood). Finally, to solve the proposed optimization model with the adaptive non-convex proximal regularization, we propose an efficient solution based on the alternating direction method of multipliers, generalized shrinkage, and the Fourier domain optimization. We have conducted experiments on a variety of applications, including image smoothing, HDR tone mapping, compression artifact removal, texture removal, and depth map upsampling. Both quantitative and qualitative results indicate the superiority of the proposed method. Moreover, our method is highly efficient, it is able to process 720P color images at interactive rates on a modern GPU.},
  archive      = {J_PR},
  author       = {Yang Yang and Shunli Ji and Lanling Zeng and Keyang Cheng},
  doi          = {10.1016/j.patcog.2025.112176},
  journal      = {Pattern Recognition},
  month        = {3},
  pages        = {112176},
  shortjournal = {Pattern Recognition},
  title        = {Adaptive proximal regularization for image smoothing},
  volume       = {171},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). A feature selection method based on clonal selection with beneficial noise. <em>PR</em>, <em>171</em>, 112175. (<a href='https://doi.org/10.1016/j.patcog.2025.112175'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Feature selection for high-dimensional data is a fundamental challenge in pattern recognition, particularly within resource-constrained Internet of Things (IoT) environments. Existing feature selection algorithms often present a trade-off between computational efficiency and selection accuracy. The clonal selection algorithm, while offering potential for effective global search, is constrained by its reliance on fixed-intensity mutation operators. This limitation renders the algorithm susceptible to premature convergence to local optima. Inspired by the phenomenon wherein beneficial noise promotes optimization processes across various domains, this paper proposes a feature selection method based on Clonal Selection with Beneficial Noise, named CSBN, for the feature selection of high-dimensional data. CSBN maps the discrete feature selection problem into a continuous space using real-valued encoding. Subsequently, it introduces a dynamic noise strategy guided by antibody affinity. Specifically, low-affinity antibodies are subjected to stronger, directional noise to enhance global exploration and escape from local optima. Conversely, high-affinity antibodies undergo subtle noise to facilitate a more refined local search. Experimental results on 24 public benchmark datasets spanning various dimensions demonstrate that CSBN achieves an average feature dimensionality reduction rate of 99.52 %. It improves the average classification accuracy from 73.56 % (with all features) to 92.91 % (with the selected subset). Compared to classical feature selection methods, CSBN improves average classification accuracy by 11.05 % while maintaining comparable computational overhead. Compared to hybrid methods, CSBN improves classification accuracy by 9.15 % and feature reduction rate by 13.74 % at lower computational cost.},
  archive      = {J_PR},
  author       = {Wenshan Li and Chenyi Huang and Ao Liu and Yilin Zhang and Beibei Li and Junjiang He and Wenbo Fang and Hongxia Wang},
  doi          = {10.1016/j.patcog.2025.112175},
  journal      = {Pattern Recognition},
  month        = {3},
  pages        = {112175},
  shortjournal = {Pattern Recognition},
  title        = {A feature selection method based on clonal selection with beneficial noise},
  volume       = {171},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Practical privacy-preserving federated learning based on multiparty homomorphic encryption for large-scale models. <em>PR</em>, <em>171</em>, 112174. (<a href='https://doi.org/10.1016/j.patcog.2025.112174'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Federated Learning (FL) is a distributed machine learning framework that enables multiple clients to collaboratively train a global model without sharing their private data. However, the privacy of clients’ data remains at risk during the aggregation process, as sensitive information can be inferred from the uploaded gradients. Traditional Privacy-Preserving Federated Learning (PPFL) techniques often compromise either efficiency or accuracy. To address this challenge, this paper introduces a novel PPFL scheme that integrates homomorphic encryption (HE) with a mask generation technique to enhance both security and performance. Specifically, the mask generation technique employs a homomorphic pseudorandom generator (HPRG) to generate a pseudorandom sequence that serves as a mask vector to conceal d -dimensional local gradients. Subsequently, HE is utilized to encrypt the HPRG seed, ensuring that only the sum of mask vectors can be reconstructed on the server side. While existing HE-based approaches require encrypting and decrypting each dimension of the d -dimensional gradients individually (i.e., d encryption/decryption operations), our approach performs HE only once on a single seed (i.e., one encryption/decryption operation). Thus, the computation and communication overhead in our scheme does not scale with the gradient’s dimension. The scheme also provides a security proof demonstrating semantic security for both local gradients and aggregated results, with high tolerance for collusion and dropout (both up to N − 2 ). Experiments demonstrate that our scheme improves computation efficiency by 5 × ∼ 10 × and communication efficiency by 3 × ∼ 7 × compared to state-of-the-art (SOTA) HE-based schemes for a model size of 4 × 10 5 . Moreover, as the model size d increases, these advantages become more pronounced, making our scheme a practical solution for privacy-preserving federated learning, especially for large-scale models.},
  archive      = {J_PR},
  author       = {Xian Qin and Xue Yang and Xiaohu Tang},
  doi          = {10.1016/j.patcog.2025.112174},
  journal      = {Pattern Recognition},
  month        = {3},
  pages        = {112174},
  shortjournal = {Pattern Recognition},
  title        = {Practical privacy-preserving federated learning based on multiparty homomorphic encryption for large-scale models},
  volume       = {171},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Causal unsupervised semantic segmentation. <em>PR</em>, <em>171</em>, 112173. (<a href='https://doi.org/10.1016/j.patcog.2025.112173'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Unsupervised semantic segmentation aims to achieve high-quality semantic grouping without human annotations. With the advent of self-supervised learning, various frameworks utilize pre-trained features for the unsupervised prediction. However, a significant challenge in this unsupervised setup is determining the appropriate level of granularity required for segmenting concepts. To address it, we propose a novel framework, CAusal Unsupervised Semantic sEgmentation (CAUSE), which leverages insights from causal inference. Specifically, we bridge an intervention-oriented approach to define two-step unsupervised prediction: (i) constructing a discretized concept clusterbook as a mediator, representing concept prototypes, (ii) concept-wise self-supervised learning for pixel-level grouping using an explicit link from the mediator. Through extensive experiments, we corroborate the effectiveness of CAUSE and achieve state-of-the-art in unsupervised semantic segmentation.},
  archive      = {J_PR},
  author       = {Junho Kim and Byung-Kwan Lee and Yong Man Ro},
  doi          = {10.1016/j.patcog.2025.112173},
  journal      = {Pattern Recognition},
  month        = {3},
  pages        = {112173},
  shortjournal = {Pattern Recognition},
  title        = {Causal unsupervised semantic segmentation},
  volume       = {171},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). ArtGlyphDiffuser: Text-driven artistic glyph generation via style-to-CLIP projection and multi-level controlled diffusion. <em>PR</em>, <em>171</em>, 112172. (<a href='https://doi.org/10.1016/j.patcog.2025.112172'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Automatic Artistic Glyph Image Generation is a challenging task that involves transferring the style of a reference image onto a source image while preserving its original content. Although existing methods, such as font generation and font style transfer, have demonstrated promising results, their reliance on image-based content poses limitations. The style information embedded within these images can affect generated performance and lead to practical inconveniences. Therefore, this paper first proposes a one-shot text-driven artistic glyph image generation method named ArtGlyphDiffuser, which is a fine-tuning approach based on a pre-trained large model – Stable Diffusion (SD). To achieve the fusion of cross-modal information from text and images, we introduce the Style-to-CLIP Projection module, which maps the reference image onto the latent space of SD. Furthermore, we propose a Multi-Level Controlled block that seamlessly integrates information from various scales into the denoising process of the U-Net network, enhancing the extraction and generation of complex strokes, glyphs, colors, and textures. To prevent the model from solely focusing on pixel-level features, we incorporate a Coarse-Grained Context-Consistent Loss without prior knowledge. This approach encourages the model to prioritize high-level features and effectively complements the diffusion loss. Lastly, we propose the Randomly Masked Style strategy to accelerate the model’s training while maintaining its ability to generate diverse glyphs. Extensive experiments demonstrate that our approach can generate natural artistic glyph images with exquisite details and achieve state-of-the-art performance. The code and datasets will be published at https://github.com/Luxb0124/ArtGlyphDiffuser .},
  archive      = {J_PR},
  author       = {Xiongbo Lu and Yaxiong Chen and Yi Rong and Shengwu Xiong},
  doi          = {10.1016/j.patcog.2025.112172},
  journal      = {Pattern Recognition},
  month        = {3},
  pages        = {112172},
  shortjournal = {Pattern Recognition},
  title        = {ArtGlyphDiffuser: Text-driven artistic glyph generation via style-to-CLIP projection and multi-level controlled diffusion},
  volume       = {171},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Improving domain generalization via enhanced style transfer incorporating PCA. <em>PR</em>, <em>171</em>, 112171. (<a href='https://doi.org/10.1016/j.patcog.2025.112171'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Research has demonstrated that Convolutional Neural Networks (CNNs) have a tendency to base their predictions on image styles rather than image content, resulting in models that are susceptible to reduced performance when confronted with unseen domains. Domain generalization aims to address this problem by training models that can effectively generalize to previously unseen domains by leveraging information from multiple source domains. In the context of domain generalization, recent advances have advocated for the integration of style transfer techniques into the training pipeline, to synthesize samples with diverse styles. However, these methods often face limitations in achieving sufficient diversity in generated styles while maintaining semantic consistency with the source domain. This paper proposes a novel approach to overcome these limitations by combining style transfer with principal component analysis (PCA). First, PCA is applied to extract the principal components of style information. Then, perturbations along these components create new style features, which are used with a style transfer technique to generate multiple re-stylized images while preserving semantic consistency with the content images. These diverse yet content-consistent images are integrated into the training process of a classification model, improving its capacity for domain generalization. Experimental results demonstrate that our method achieves superior performance compared to state-of-the-art techniques on several cross-domain benchmarks.},
  archive      = {J_PR},
  author       = {Bing Hu and Chong-zhi Gao and Liangzhao Yu},
  doi          = {10.1016/j.patcog.2025.112171},
  journal      = {Pattern Recognition},
  month        = {3},
  pages        = {112171},
  shortjournal = {Pattern Recognition},
  title        = {Improving domain generalization via enhanced style transfer incorporating PCA},
  volume       = {171},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). RETTA: Retrieval-enhanced test-time adaptation for zero-shot video captioning. <em>PR</em>, <em>171</em>, 112170. (<a href='https://doi.org/10.1016/j.patcog.2025.112170'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Despite the significant progress of fully-supervised video captioning, zero-shot methods remain much less explored. In this paper, we propose a novel zero-shot video captioning framework named R etrieval- E nhanced T est- T ime A daptation (RETTA), which takes advantage of existing pre-trained large-scale vision and language models to directly generate captions with test-time adaptation. Specifically, we bridge video and text using four key models: a general video-text retrieval model XCLIP, a general image-text matching model CLIP, a text alignment model AnglE, and a text generation model GPT-2, due to their source-code availability. The main challenge is how to enable the text generation model to be sufficiently aware of the content in a given video so as to generate corresponding captions. To address this problem, we propose using learnable tokens as a communication medium among these four frozen models GPT-2, XCLIP, CLIP, and AnglE. Different from the conventional way that trains these tokens with training data, we propose to learn these tokens with soft targets of the inference data under several carefully crafted loss functions, which enable the tokens to absorb video information catered for GPT-2. This adaptation requires only a few iterations ( e.g. , 16) and does not require ground truth data. Extensive experimental on MSR-VTT, MSVD, and VATEX, show absolute 5.1 % ∼ 32.4 % improvements in CIDEr scores compared to several state-of-the-art zero-shot video captioning methods.},
  archive      = {J_PR},
  author       = {Yunchuan Ma and Laiyun Qing and Guorong Li and Yuankai Qi and Amin Beheshti and Quan Z. Sheng and Qingming Huang},
  doi          = {10.1016/j.patcog.2025.112170},
  journal      = {Pattern Recognition},
  month        = {3},
  pages        = {112170},
  shortjournal = {Pattern Recognition},
  title        = {RETTA: Retrieval-enhanced test-time adaptation for zero-shot video captioning},
  volume       = {171},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Softmatch distance: A novel distance for weakly-supervised trend change detection in bi-temporal images. <em>PR</em>, <em>171</em>, 112169. (<a href='https://doi.org/10.1016/j.patcog.2025.112169'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Change detection (CD) aims to identify changes in foregrounds, i.e., objects of interest. As two common sub-tasks, general CD (GCD) focuses on providing binary changes (change or no change), while semantic CD (SCD) further distinguishes categories of involved objects. However, binary changes in GCD are often not practical enough, while annotating semantic labels for SCD is very expensive. Therefore, researchers propose a novel solution that intuitively divides changes into three trends (“appear”, “disappear” and “transform”), named trend CD (TCD) in this paper. It offers more change details than GCD, while requiring less manual annotation cost than SCD. However, compared to training a GCD model, training a TCD model with supervised learning still requires trend labels annotated across three trends, which remains costly and time-consuming. To address this, we propose a weakly-supervised TCD method that requires only binary labels (change or no change) for training, enabling trend predictions (“appear,” “disappear,” “transform,” and no change) to be obtained. This approach not only leverages existing abundant datasets with binary labels for TCD but also significantly saves the annotation costs associated with trend labels. Specifically, a softmatch distance is developed to construct a weakly-supervised TCD branch within a simple GCD model. In this way, the model that is able to predict both binary and trending changes can be trained only using GCD labels. Furthermore, an approach is presented to successfully separate the foreground and background of images, which also is crucial for the weakly-supervised TCD task. The experiment results on four public data sets are highly encouraging, which demonstrates the effectiveness of our proposed model. Our source codes are available at https://github.com/TangXu-Group/SoftMatch .},
  archive      = {J_PR},
  author       = {Yuqun Yang and Xu Tang and Xiangrong Zhang and Changzhe Jiao and Jingjing Ma and Licheng Jiao},
  doi          = {10.1016/j.patcog.2025.112169},
  journal      = {Pattern Recognition},
  month        = {3},
  pages        = {112169},
  shortjournal = {Pattern Recognition},
  title        = {Softmatch distance: A novel distance for weakly-supervised trend change detection in bi-temporal images},
  volume       = {171},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). FEQIN: A feature-enhanced query interaction network for breast ultrasound segmentation. <em>PR</em>, <em>171</em>, 112168. (<a href='https://doi.org/10.1016/j.patcog.2025.112168'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Accurate segmentation of breast ultrasound images is crucial for improving the diagnostic accuracy of breast cancer. However, this critical task is challenged by the substantial anatomical similarity between lesions and surrounding tissues, as well as the speckle noise inherent in ultrasound imaging. To overcome these limitations, we propose a novel Feature-Enhanced Query Interaction Network (FEQIN), which achieves precise segmentation through three main technical innovations. First, we utilize multi-scale feature extraction with enhanced representation capabilities for small tumor characteristics. Second, we design a Feature Enhancement Module (FEM) that preserves critical tumor characteristics during feature propagation through adaptive channel recalibration. Third, we incorporate a multi-scale deformable attention mechanism that enhances information capture efficiency while accelerating inference speed. Finally, our Query Interaction Module (QIM) establishes spatial-semantic correlations that significantly improve lesion localization sensitivity. Extensive evaluations across three clinical datasets demonstrate the superior performance of FEQIN, particularly its effectiveness in handling real-world scenarios with varying image quality.},
  archive      = {J_PR},
  author       = {Xiling Luo and Le Ou-Yang},
  doi          = {10.1016/j.patcog.2025.112168},
  journal      = {Pattern Recognition},
  month        = {3},
  pages        = {112168},
  shortjournal = {Pattern Recognition},
  title        = {FEQIN: A feature-enhanced query interaction network for breast ultrasound segmentation},
  volume       = {171},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Reconsidering the interplay between behaviors: A cross-attentive behavior-aware GCN-based recommendation. <em>PR</em>, <em>171</em>, 112167. (<a href='https://doi.org/10.1016/j.patcog.2025.112167'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-behavior recommender systems aim to offer users with items by considering various behaviors, thus received a great deal of attention in recent years. Current studies mainly employ graph convolutional networks to learn users’ preferences. However, they typically consider the impact of behaviors from the items to users, while neglect those from users to items. Moreover, the interplay between adjacent behaviors is still not fully explored. To tackle these issues, we propose a Cross-Attentive Behavior-aware Graph Convolutional Network (CAB-GCN) for recommendation. Specifically, we propose a bidirectional weight aggregation network, which simultaneously considers the impact of behaviors from both sides of the item and user. Then, we present an improved behavior chain to adaptively order the weighted behaviors. Finally, we devise a cross-attention mechanism equipped with feature-level sliding window, which is capable of considering the interplay between adjacent behaviors. Extensive experiments on three datasets demonstrate that CAB-GCN significantly outperforms state-of-the-art methods, achieving average improvements of 40.21 %, 65.43 %, and 10.26 % on each dataset. The datasets and codes are available on GitHub at https://github.com/ZZY-GraphMiningLab/CAB-GCN .},
  archive      = {J_PR},
  author       = {Dezheng Meng and Jinyu Zhang and Chao Li and Zhongying Zhao},
  doi          = {10.1016/j.patcog.2025.112167},
  journal      = {Pattern Recognition},
  month        = {3},
  pages        = {112167},
  shortjournal = {Pattern Recognition},
  title        = {Reconsidering the interplay between behaviors: A cross-attentive behavior-aware GCN-based recommendation},
  volume       = {171},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Incomplete multi-view clustering with cross-view generation via pre-trained transformer. <em>PR</em>, <em>171</em>, 112166. (<a href='https://doi.org/10.1016/j.patcog.2025.112166'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Addressing missing views in incomplete multi-view clustering is a significant challenge in real applications. Numerous view recovery methods have been proposed to address this problem. However, most existing methods fail to effectively capture cross-view relationships between instances across different views. Additionally, many of these methods assume the availability of a certain percentage of complete multi-view data. In response to these limitations, we introduce a novel cross-view generative model based on Transformer. This model is designed to infer missing views and generate a comprehensive semantic representation, effectively addressing the limitations of existing methods. Our approach integrates the Transformer as an intermediary between the encoder and decoder to impute latent representations for unavailable instances. To capture meaningful cross-view relationships, we employ masked multi-head self-attention based on the Transformer architecture, allowing the model to dynamically learn associations across views. Additionally, we introduce a generative pre-training process that does not require complete data, allowing the generator to recover missing views. Furthermore, a self-supervised fine-tuning process is implemented to exploit intra-view similarity, thereby enhancing the overall quality of the generated data. Extensive experiments and analyses on several incomplete benchmark datasets demonstrate the effectiveness and efficiency of our novel method compared with several state-of-the-art methods.},
  archive      = {J_PR},
  author       = {Hang Gao and Cheng Liu and Hongming Sun and Gaoyang Li and Ying Li and You Zhou and Wei Du},
  doi          = {10.1016/j.patcog.2025.112166},
  journal      = {Pattern Recognition},
  month        = {3},
  pages        = {112166},
  shortjournal = {Pattern Recognition},
  title        = {Incomplete multi-view clustering with cross-view generation via pre-trained transformer},
  volume       = {171},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Multiple temporal scale aggregate network for temporal action segmentation. <em>PR</em>, <em>171</em>, 112165. (<a href='https://doi.org/10.1016/j.patcog.2025.112165'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Temporal action segmentation is essential for understanding long-form videos, yet accurately segmenting actions remains challenging due to complex temporal dynamics. To address this, existing models often adopt a multi-stage refinement strategy. In this framework, a preliminary prediction is first generated from a hierarchical Encoder by progressively integrating the original video features, which is then refined through multiple cascading Decoders. However, they typically focus on designing sophisticated networks to uncover complex temporal dependencies that are only present in the final output of the Encoder, ignoring the fact that various scales of temporal dependencies are already captured due to the hierarchical structure. To investigate whether segmentation quality can benefit from leveraging this unexplored information, we propose a U-Net style model, termed Multiple Temporal Scale Aggregate Network (MTSAN). Specially, unlike existing models that successively refine predictions using only the final-layer features, we extract intermediate features throughout the hierarchical process to preserve rich multi-scale temporal contexts. The Decoders are then designed to refine predictions at different temporal granularities, corresponding to the features from the same-level Encoder. To effectively exchange information across all temporal granularities, we further propose a Cross Temporal Fusion Network (CTFN) to adaptively fuse complementary information across different temporal fusion scales. Our experimental results demonstrate that explicitly utilizing multi-scale temporal information contained in Encoder is an effective approach for further improving the performance of multi-stage refinement strategy.},
  archive      = {J_PR},
  author       = {Zhichao Zheng and Ying Zhou and Yi Chen and Yanhui Gu and Junsheng Zhou and Zheyan Ji},
  doi          = {10.1016/j.patcog.2025.112165},
  journal      = {Pattern Recognition},
  month        = {3},
  pages        = {112165},
  shortjournal = {Pattern Recognition},
  title        = {Multiple temporal scale aggregate network for temporal action segmentation},
  volume       = {171},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). OpenCIL: Benchmarking out-of-distribution detection in class incremental learning. <em>PR</em>, <em>171</em>, 112163. (<a href='https://doi.org/10.1016/j.patcog.2025.112163'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Class incremental learning (CIL) aims to learn a model that can not only incrementally accommodate new classes, but also maintain the learned knowledge of old classes. Out-of-distribution (OOD) detection in CIL is to retain this incremental learning ability, while being able to reject unknown samples that are drawn from different distributions of the learned classes. This capability is crucial to the safety of deploying CIL models in open worlds. However, despite remarkable advancements in the respective CIL and OOD detection, there lacks a systematic and large-scale benchmark to assess the capability of advanced CIL models in detecting OOD samples. To fill this gap, in this study we design a comprehensive empirical study to establish such a benchmark, named OpenCIL , offering a unified protocol for enabling CIL models with different OOD detectors using two principled OOD detection frameworks. One key observation we find through our comprehensive evaluation is that the CIL models can be severely biased towards the OOD samples and newly added classes when they are exposed to open environments. Motivated by this, we further propose a novel approach for OOD detection in CIL, namely Bi-directional Energy Regularization ( BER ), which is specially designed to mitigate these two biases in different CIL models by having energy regularization on both old and new classes. Extensive experiments show that BER can substantially improve the OOD detection capability across a range of CIL models, achieving state-of-the-art performance on the OpenCIL benchmark. All codes and datasets are open-source at https://github.com/mala-lab/OpenCIL .},
  archive      = {J_PR},
  author       = {Wenjun Miao and Guansong Pang and Trong-Tung Nguyen and Ruohuan Fang and Jin Zheng and Xiao Bai},
  doi          = {10.1016/j.patcog.2025.112163},
  journal      = {Pattern Recognition},
  month        = {3},
  pages        = {112163},
  shortjournal = {Pattern Recognition},
  title        = {OpenCIL: Benchmarking out-of-distribution detection in class incremental learning},
  volume       = {171},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Mining representative tokens via transformer-based multi-modal interaction for RGB-T tracking. <em>PR</em>, <em>171</em>, 112162. (<a href='https://doi.org/10.1016/j.patcog.2025.112162'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {RGB-T tracking leverages the complementarity of visible and thermal modalities for robust performance in challenging environments. However, previous RGB-T trackers are vulnerable to irrelevant backgrounds and ignore the modality gap. To address the above issues, we propose MRTTrack, a Transformer-based RGB-T tracking framework consisting of a multi-modal separate-then-collaborative (MSC) module and a cross-modal discrepancy constraint (CDC). Specifically, the MSC is designed to mitigate irrelevant background interference and operates in two stages: target-oriented token selection and multi-modal token interaction. By recursively aggregating attention maps across layers, the target-oriented token selection produces an index mask for representative tokens, which is then used to guide multi-modal token interaction via mask-based attention. Additionally, CDC enforces consistency across modalities on non-representative tokens, thereby alleviating performance degradation caused by modality gap. Comprehensive evaluations on LasHeR, RGBT210, RGBT234, and VTUAV benchmarks demonstrate strong goal-reaching performance and notable robustness improvements of our method. The code is available at https://github.com/gao5yy/MRTTrack .},
  archive      = {J_PR},
  author       = {Pujian Lai and Dong Gao and Shilei Wang and Gong Cheng},
  doi          = {10.1016/j.patcog.2025.112162},
  journal      = {Pattern Recognition},
  month        = {3},
  pages        = {112162},
  shortjournal = {Pattern Recognition},
  title        = {Mining representative tokens via transformer-based multi-modal interaction for RGB-T tracking},
  volume       = {171},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Corrigendum to “FaceChain-MMID: Generating highly identity-consistent realistic portraits via dividing & merging multi-modal representations” [Pattern recognition 168 (2025) 111858]. <em>PR</em>, <em>171</em>, 112161. (<a href='https://doi.org/10.1016/j.patcog.2025.112161'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_PR},
  author       = {Chao Xu and Fei Wang and Cheng Yu and Baigui Sun and Jian Zhao},
  doi          = {10.1016/j.patcog.2025.112161},
  journal      = {Pattern Recognition},
  month        = {3},
  pages        = {112161},
  shortjournal = {Pattern Recognition},
  title        = {Corrigendum to “FaceChain-MMID: Generating highly identity-consistent realistic portraits via dividing & merging multi-modal representations” [Pattern recognition 168 (2025) 111858]},
  volume       = {171},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Corrigendum to “MA-FSAR: Multimodal adaptation of CLIP for few-shot action recognition” [Pattern recognition 169 (2026) 111902]. <em>PR</em>, <em>171</em>, 112160. (<a href='https://doi.org/10.1016/j.patcog.2025.112160'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_PR},
  author       = {Jiazheng Xing and Jian Zhao and Chao Xu and Mengmeng Wang and Guang Dai and Yong Liu and Jingdong Wang and Xuelong Li},
  doi          = {10.1016/j.patcog.2025.112160},
  journal      = {Pattern Recognition},
  month        = {3},
  pages        = {112160},
  shortjournal = {Pattern Recognition},
  title        = {Corrigendum to “MA-FSAR: Multimodal adaptation of CLIP for few-shot action recognition” [Pattern recognition 169 (2026) 111902]},
  volume       = {171},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). From indoor to outdoor: Unsupervised domain adaptive gait recognition. <em>PR</em>, <em>171</em>, 112159. (<a href='https://doi.org/10.1016/j.patcog.2025.112159'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Gait recognition is an important pattern recognition task and has significant applications for public security, which has progressed rapidly with the development of deep learning. However, existing learning-based gait recognition methods mainly focus on a single domain, especially the constrained laboratory environment. In this work, we study a new problem of unsupervised domain adaptive gait recognition (UDA-GR), that learns a gait identifier with supervised labels from the indoor scenes (source domain) and is applied to the outdoor wild scenes (target domain). For this purpose, we develop an uncertainty estimation and regularization-based UDA-GR framework. Specifically, we investigate the characteristics of gaits in the indoor and outdoor scenes, for estimating the sample uncertainty, which is used in the unsupervised pseudo label based fine-tuning on the target domain to alleviate the noises of the pseudo labels. We also propose a negative learning strategy to learn view-independent features in the supervised pre-training on the source domain. Finally, we establish a new benchmark for this new yet practical problem, experimental results on which show the effectiveness of the proposed method. Our work is available in https://github.com/kkw98/UDAGR .},
  archive      = {J_PR},
  author       = {Likai Wang and Wei Feng and Ruize Han and Xiangqun Zhang and Yanjie Wei and Song Wang},
  doi          = {10.1016/j.patcog.2025.112159},
  journal      = {Pattern Recognition},
  month        = {3},
  pages        = {112159},
  shortjournal = {Pattern Recognition},
  title        = {From indoor to outdoor: Unsupervised domain adaptive gait recognition},
  volume       = {171},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Hyper adversarial tuning for boosting adversarial robustness of pretrained large vision transformers. <em>PR</em>, <em>171</em>, 112158. (<a href='https://doi.org/10.1016/j.patcog.2025.112158'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Large vision Transformers (ViTs) have achieved competitive performance in various computer vision tasks based on large-scale pre-training. However, large ViTs still remain vulnerable to adversarial examples, emphasizing the necessity of enhancing their adversarial robustness. While adversarial training is an effective defense for deep convolutional models, it often faces scalability issues with large ViTs due to high computational costs. Recent approaches propose robust fine-tuning methods, such as adversarial tuning of low-rank adaptation (LoRA) in ViT, however, they still struggle to match the accuracy of full parameter adversarial fine-tuning. An effective synergy of various defense mechanisms offers a promising approach to enhancing the robustness of ViT, yet this paradigm remains largely underexplored. To address this, we propose hyper adversarial tuning (HyperAT), a meta learning approach, which captures shared defensive knowledge among different methods to improve model robustness efficiently and effectively simultaneously. Specifically, adversarial tuning of each defense method is formulated as a learning task, and a HyperNetwork generates LoRA specific to this defense. Then, a random sampling and tuning strategy is proposed to extract and facilitate the defensive knowledge transfer between different defenses. Finally, diverse LoRAs are merged adaptively to further enhance the adversarial robustness. Experiments on various datasets and model architectures demonstrate that HyperAT significantly enhances the adversarial robustness of pretrained large vision models without excessive computational overhead, establishing a new state-of-the-art benchmark.},
  archive      = {J_PR},
  author       = {Kangtao Lv and Wenyan Fan and Huangsen Cao and Kainan Tu and Yihuai Xu and Zhimeng Zhang and Yang Li and Xin Ding and Yongwei Wang},
  doi          = {10.1016/j.patcog.2025.112158},
  journal      = {Pattern Recognition},
  month        = {3},
  pages        = {112158},
  shortjournal = {Pattern Recognition},
  title        = {Hyper adversarial tuning for boosting adversarial robustness of pretrained large vision transformers},
  volume       = {171},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Modalities collaboration and granularities interaction for fine–grained sketch-based image retrieval. <em>PR</em>, <em>171</em>, 112157. (<a href='https://doi.org/10.1016/j.patcog.2025.112157'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Fine-Grained Sketch-Based Image Retrieval (FG-SBIR) is a challenging task due to the substantial modality gap and subtle inter-class variations. Existing methods ignore the complementarity information between modalities and the rich context across granularities. To address these limitations, we propose a Modalities Collaboration and Granularities Interaction (MCGI) framework. Specifically, we develop a Cross-Modality Information Compensation (CMIC) module to alleviate the modality gap, which integrates complementary information from sketches and photos and propagates knowledge to single-modality features. Furthermore, we develop a Cross-Granularity Prototype Interaction (CGPI) module to learn discriminative fine-grained representations, which comprises Multi-Granularity Prototype Learning (MGPL), Cross-Granularity Information Interaction (CGII), and Multi-Granularity Prototype-aware Contrastive loss (MGPC). MGPL aims to acquire discriminative multi-granularity features. CGII is developed to capture rich contextual information. MGPC further aligns the feature distributions of sketches and photos. Extensive experiments on four public datasets demonstrate the superiority of our MCGI, with Rank-1 accuracies of 78.6 % on QML-Chair-V2, 44.5 % on QML-Shoe-V2, 96.0 % on Clothes-V1, and 91.5 % on Sketch Re-ID, outperforming existing state-of-the-art methods. The code has been released at https://github.com/nengdong96/MCGI .},
  archive      = {J_PR},
  author       = {Yafei Zhang and Junchao Ge and Jiaman Ding and Neng Dong and Shaojie Qiao and Zhengtao Yu and Huafeng Li},
  doi          = {10.1016/j.patcog.2025.112157},
  journal      = {Pattern Recognition},
  month        = {3},
  pages        = {112157},
  shortjournal = {Pattern Recognition},
  title        = {Modalities collaboration and granularities interaction for fine–grained sketch-based image retrieval},
  volume       = {171},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). OV-GT3D: A generalizable open-vocabulary two-stage 3D detector with dual path distillation. <em>PR</em>, <em>171</em>, 112156. (<a href='https://doi.org/10.1016/j.patcog.2025.112156'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we propose a new network architecture for open-vocabulary 3D object detection. Conventional methods train a class-agnostic 3D object detector by classifying foreground and background predictions, followed by object-level distillation to align object representation with CLIP embedding. However, the split for foreground and background makes the detector overfit on training categories as it penalizes unannotated but correct predictions, and the object-level distillation also lacks semantic information since the class-agnostic detector only focuses on geometric information. To this end, we propose the Generalizable Open-vocabulary Two-stage 3D Detector with Dual Path Distillation (OV-GT3D). This framework effectively detects objects from novel categories and performs open-vocabulary classification with enriched semantics. Specifically, we design a localization-guided proposal generator and a semantic-based ROI discriminator for the Generalizable Two-stage 3D Detector (GT3D). Leveraging the two-stage architecture, we introduce Dual Path Distillation (DPD), which enhances object representations through point-level semantic distillation that aligns 3D point clouds with pixel-aligned CLIP features and point-to-object aggregation that propagates fine-grained semantics from points to object proposals. This makes representations semantic-aware and well-aligned with CLIP embeddings. Comprehensive evaluations on the ScanNet and SUN-RGBD datasets demonstrate that OV-GT3D outperforms prior methods, achieving state-of-the-art performance across a wide array of benchmarks.},
  archive      = {J_PR},
  author       = {Zhihao Sun and Xiuwei Xu and Bin Fan and Jiwen Lu and Hongmin Liu},
  doi          = {10.1016/j.patcog.2025.112156},
  journal      = {Pattern Recognition},
  month        = {3},
  pages        = {112156},
  shortjournal = {Pattern Recognition},
  title        = {OV-GT3D: A generalizable open-vocabulary two-stage 3D detector with dual path distillation},
  volume       = {171},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Mitigating label noise impact: A plug-and-play RECIST-based iterative label refinement teacher-student learning paradigm for medical image segmentation. <em>PR</em>, <em>171</em>, 112155. (<a href='https://doi.org/10.1016/j.patcog.2025.112155'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, deep learning methods, particularly convolutional neural networks (CNNs), have achieved remarkable results in the field of medical image segmentation. Most existing medical image segmentation approaches rely on supervised deep learning strategies, utilizing manually annotated labels for training. Currently, numerous researchers are continually advancing the state-of-the-art in various medical image segmentation fields by proposing novel network architectures. However, we have found that, in addition to network architecture, label noise significantly limits segmentation performance. To address the issue of label noise, we propose a plug-and-play teacher-student learning strategy that iteratively redefines training labels, known as RECIST-Based Iterative Label Refinement (RILR). RILR introduces three strategies to refine training labels based on model prediction results: the RECIST-based consistent replacement strategy to prevent model overfitting, the RECIST-based conservative label refinement strategy to reduce false positives, and the RECIST-based radial label refinement strategy to minimize false negatives. Additionally, we present a new refinement loss function for training networks using the refined labels. We conducted experiments using multiple mainstream segmentation networks across three different modality datasets. The experimental results demonstrate that the proposed plug-and-play RILR method effectively reduces the impact of label noise on segmentation outcomes and enhances the performance of the segmentation networks.},
  archive      = {J_PR},
  author       = {Zhaoshuo Diao and Yan Zhang and Ye Yuan and Qian Wang and Ying Zhang and Yue Gao},
  doi          = {10.1016/j.patcog.2025.112155},
  journal      = {Pattern Recognition},
  month        = {3},
  pages        = {112155},
  shortjournal = {Pattern Recognition},
  title        = {Mitigating label noise impact: A plug-and-play RECIST-based iterative label refinement teacher-student learning paradigm for medical image segmentation},
  volume       = {171},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). TWFNet: Introducing transitional weather conditions for autonomous driving with a spatio-temporal forecasting network. <em>PR</em>, <em>171</em>, 112154. (<a href='https://doi.org/10.1016/j.patcog.2025.112154'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Autonomous vehicle technology (AVT) relies on achieving exceptional safety and reliability, especially under challenging weather conditions such as extreme rain, fog, and clouds. In nature, we observe transitional weather states (e.g., sunny to rainy, rainy to sunny) characterized by dynamic shifts between extreme weather conditions. These dynamic shifts present significant challenges, such as sudden changes in visibility, impairing the efficiency and safety of AVT. Existing research works in AVT have primarily focused on performing scene perception and path planning in clear and distinct weather conditions. However, there is a significant gap in addressing the difficulties posed by transitional weather conditions. Moreover, these studies often overlook the critical aspects of weather forecasting, particularly in transitional weather scenarios. Predicting weather changes is crucial to ensure consistent performance and accurate scene perception in dynamic environmental conditions. Hence, we propose a novel framework called Transitional Weather Forecasting Network (TWFNet), an end-to-end spatiotemporal architecture comprising two main modules, (i) a transformer network with encoder and decoder layers to process CNN-extracted spatial features from image sequences, generating temporal features for the predicted weather transition sequence and (ii) GRU layers with self-attention for classifying the generated temporal features into different transition weather states. These components are trained jointly, providing a robust solution for forecasting transitional weather patterns. Further, we introduce an adverse intermediate weather forecasting driving dataset (AIWFD6) consisting of six transitional weather states such as sunny to foggy, foggy to sunny, sunny to rainy, rainy to sunny, cloudy to rainy, and rainy to cloudy generated using a variational autoencoder that leverages data interpolation to produce transitions. We evaluate the performance of our proposed TWFNet on the introduced AIWFD6 dataset along with a large-scale publicly available simulated continuous weather dataset called SHIFT. Our proposed TWFNet achieves a forecasting performance of 97.46 % and 74.08 % on the AIWFD6 and SHIFT datasets, respectively. These results demonstrate the efficiency of our approach in comparison with other temporal sequence modes for effectively forecasting transitional weather states across different observation sequence lengths.},
  archive      = {J_PR},
  author       = {Kondapally Madhavi and K. Naveen Kumar and C. Gayathri},
  doi          = {10.1016/j.patcog.2025.112154},
  journal      = {Pattern Recognition},
  month        = {3},
  pages        = {112154},
  shortjournal = {Pattern Recognition},
  title        = {TWFNet: Introducing transitional weather conditions for autonomous driving with a spatio-temporal forecasting network},
  volume       = {171},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Boosting illuminant estimation in deep color constancy through brightness robustness enhancement. <em>PR</em>, <em>171</em>, 112153. (<a href='https://doi.org/10.1016/j.patcog.2025.112153'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep Neural Network-driven Color Constancy (DNNCC) models have advanced significantly in estimating illuminant chromaticity for color correction. However, their susceptibility to brightness variations–a critical factor in color constancy–remains unexplored. Our empirical analysis reveals that state-of-the-art DNNCC models exhibit notable sensitivity to brightness perturbations despite their chromaticity-focused design, potentially compromising performance in real-world scenarios with inherent illumination variability. From the insights of our analysis, we propose a Brightness Robustness Enhancement strategy (BRE) for DNNCC models. BRE employs adaptive step-size adversarial brightness augmentation to identify high-risk brightness variations and generate augmented training samples. Subsequently, BRE develops a brightness-robustness-aware model optimization strategy that integrates adversarial brightness training and brightness contrastive loss, significantly bolstering the brightness robustness of DNNCC models. As a parameter-agnostic plug-in, BRE can be seamlessly integrated into existing DNNCC models, without incurring additional overhead during the testing phase. Experiments on ColorChecker and Cube+ datasets demonstrate that BRE consistently enhances illuminant estimation performance and reduces estimation error by an average of 4.69 % across seven mainstream DNNCC models, highlighting the importance of brightness robustness in advancing color constancy.},
  archive      = {J_PR},
  author       = {Mengda Xie and Chengzhi Zhong and Yiling He and Zhan Qin and Meie Fang},
  doi          = {10.1016/j.patcog.2025.112153},
  journal      = {Pattern Recognition},
  month        = {3},
  pages        = {112153},
  shortjournal = {Pattern Recognition},
  title        = {Boosting illuminant estimation in deep color constancy through brightness robustness enhancement},
  volume       = {171},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). A segmentation knowledge-based global-local attention network for tumor classification in breast ultrasound images. <em>PR</em>, <em>171</em>, 112152. (<a href='https://doi.org/10.1016/j.patcog.2025.112152'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Computer-aided diagnosis (CAD) technology has become an integral part of early breast cancer diagnosis in medical ultrasound imaging. Nonetheless, the accurate segmentation and classification of breast tumor images continue to pose significant challenges due to the uneven intensity distribution, indistinct boundaries, and the irregular shapes of tumors. A primary reason is that many CAD methods fail to capitalize on the interrelation between tumor segmentation and classification. Furthermore, most existing methods focus on classifying global images or tumor regions of interest (ROIs), often disregarding the interplay between global and local features. This study proposes a segmentation knowledge based global-local attention classification network (SGLA-Net). First, the segment anything model (SAM) is utilized to obtain high-quality segmentation masks from a limited number of annotated samples. Then, global and local feature representations are derived from enhanced images obtained through the segmentation mask. Moreover, a global-local feature interaction (GLFI) block is designed to adaptively integrate global and local information for classification. The proposed method achieved segmentation Dice coefficients of 81.239 % and 80.516 % on the internal and external datasets, respectively. In terms of classification, the method obtained Area Under the Curve (AUC) values of 0.9532 and 0.8521 on the internal and external datasets, surpassing five state-of-the-art breast ultrasound classification methods.},
  archive      = {J_PR},
  author       = {Tao Jiang and Ying Li and Yifang Li and Wenyu Xing and Ming Yu and Feng Xie and Dean Ta},
  doi          = {10.1016/j.patcog.2025.112152},
  journal      = {Pattern Recognition},
  month        = {3},
  pages        = {112152},
  shortjournal = {Pattern Recognition},
  title        = {A segmentation knowledge-based global-local attention network for tumor classification in breast ultrasound images},
  volume       = {171},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Safeguarding the target: Enhancing multi-source domain adaptation through domain reorganization. <em>PR</em>, <em>171</em>, 112151. (<a href='https://doi.org/10.1016/j.patcog.2025.112151'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-Source Domain Adaptation (MSDA) aims to transfer knowledge from multiple labeled source domains to enhance the performance of a task in an unlabeled target domain. Existing methods primarily focus on two aspects to leverage information from the source domains: (1) jointly unifying all domains, and (2) separately aligning the target with the source domains. However, these methods do not effectively utilize diverse domain information that could further improve task performance, despite their emphasis on mitigating domain gaps. In this paper, we propose a novel perspective on Multi-Source Domain Adaptation (MSDA), which involves rationally reorganizing domains to ‘Safeguard’ the Target Domain (StTD) from the decision boundaries of the task. The reorganization procedure consists of two steps: (1) constructing an enclosure with source domains and (2) anchoring the target domain at the center of the enclosure. To avoid causing semantic information loss during the domain feature reorganization, the procedure is conducted in a projected subspace that enhances task-related features. Our proposed StTD approach has demonstrated state-of-the-art results across four MSDA datasets, highlighting its effectiveness.},
  archive      = {J_PR},
  author       = {Yuwei He and Guiguang Ding and Yuchen Guo and Fan Yang and Tao He},
  doi          = {10.1016/j.patcog.2025.112151},
  journal      = {Pattern Recognition},
  month        = {3},
  pages        = {112151},
  shortjournal = {Pattern Recognition},
  title        = {Safeguarding the target: Enhancing multi-source domain adaptation through domain reorganization},
  volume       = {171},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). LungConVT-net: A visual transformer network with blended features for pneumonia detection. <em>PR</em>, <em>171</em>, 112150. (<a href='https://doi.org/10.1016/j.patcog.2025.112150'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Respiratory ailments, especially pneumonia, demand advanced diagnostic tools for timely and more accurate detection. Addressing this critical need, we introduce LungConVT-Net, an innovative architecture that blend the strengths of Vision Transformer (ViT) and Convolutional Neural Networks (CNN) to delineate among three crucial lung conditions viz., Viral Pneumonia, Bacterial Pneumonia, and COVID-19, as well as normal lung manifestations. The proposed model leverages depthwise separable convolutions, optimizing computational efficiency without sacrificing spatial filtering. Additionally, we integrate the proposed Dynamic Hierarchical Multi-Head Attention Convolution (DH-MHAC) and Adaptive Multi-Granular Multi-Head Attention (AMG-MHA) modules. These modules bridge the self-attention mechanisms with convolutions and utilize non-overlapping patches, culminating in enhanced feature extraction, respectively. A strategically incorporated Multi-Layer Perceptron (MLP) block within the AMG-MHA refines the model’s prowess in understanding intricate data patterns. The Gradient Connection Enhancers (GCE) capture both long-range and short-range feature dependencies, addressing potential challenges in gradient descent and promoting training stability. Experimental evaluations, spanning from bi-class to complex quad-class combinations, reveal the model’s performance compared to the state-of-the-art models. The results unveil AUC scores consistently surpassing 99 % in most bi-class scenarios and show strong performance in complex multi-class settings, with AUC scores exceeding 99 % for Pneumonia, COVID-19, and Normal categories. Moreover, in the quad-class combination, our model achieves an AUC score of 98.19 %, highlighting LungConVT-Net’s effectiveness in advancing respiratory disease diagnostics. The complete implementation code is publicly available: Codebase github .},
  archive      = {J_PR},
  author       = {Asifuzzaman Lasker and Mridul Ghosh and Md Obaidullah Sk and Teresa Goncalves and Chandan Chakraborty and Kaushik Roy},
  doi          = {10.1016/j.patcog.2025.112150},
  journal      = {Pattern Recognition},
  month        = {3},
  pages        = {112150},
  shortjournal = {Pattern Recognition},
  title        = {LungConVT-net: A visual transformer network with blended features for pneumonia detection},
  volume       = {171},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Modeling and clustering of heterogeneous multivariate categorical sequences. <em>PR</em>, <em>171</em>, 112149. (<a href='https://doi.org/10.1016/j.patcog.2025.112149'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Clustering algorithms for quantitative data have been explored in literature extensively. However, many real-life applications involve qualitative data. The range of clustering procedures available in this framework is very limited. Recently, categorical sequences attracted the attention of researchers and several promising methods were developed for univariate sequences. However, observations are often utilized in the form of multivariate categorical sequences. Currently existing methods either impose restrictive and often unrealistic assumptions or reformulate multivariate sequences as univariate ones with a higher number of states, which is often computationally prohibitive even in low-dimensional cases. The contribution of this paper is twofold. First, we explore the concept of inter-sequence transitions and develop a method for modeling their probabilities. Second, we extend the proposed model to the case of heterogeneity, which allows handling challenging real-life scenarios. As we demonstrate through the series of simulation studies, the developed mixture shows good modeling and clustering performance. The applications of the methodology to stylometry and British Household Panel Survey data produce interesting and meaningful results.},
  archive      = {J_PR},
  author       = {Yingying Zhang and Volodymyr Melnykov},
  doi          = {10.1016/j.patcog.2025.112149},
  journal      = {Pattern Recognition},
  month        = {3},
  pages        = {112149},
  shortjournal = {Pattern Recognition},
  title        = {Modeling and clustering of heterogeneous multivariate categorical sequences},
  volume       = {171},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Pixel2Noise: A lightweight self-supervised denoising for single image zero-shot recognition. <em>PR</em>, <em>171</em>, 112148. (<a href='https://doi.org/10.1016/j.patcog.2025.112148'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Image denoising technologies are pivotal in providing high-fidelity input images for autonomous driving, cognitive robotics, drone swarm networks, Virtual Reality (VR), Augmented Reality (AR), and other advanced applications. However, resource-constrained terminal devices cannot sustainably support high-complexity and high-fidelity image denoising. To address this problem, this paper proposed a lightweight zero-shot pixel-level image denoising approach. First, we constructed the lightweight Regional Feature Enhanced Denoising Network (RFED-Net) that greatly reduces the network parameters by focusing on high-noise feature regions. To enable RFED-Net to learn the true values of vertex pixels, we proposed a Vertex-to-Edge pyramid mapping method (VE). Then, the Slope-to-Vertex consistency monitoring strategy (SV) is designed to construct the slope filter. Multilevel noisy images are constructed with slope filters and noise estimation to improve image consistency. Finally, numerous experiments on the Mcmaster18, PolyU, and other datasets prove that the Peak Signal-to-Noise Ratio (PSNR) of our method improves by 2.96 % with 28.57 % faster processing compared to lightweight benchmark methods. With the same denoising quality, the image processing speed of our method is improved by 51.22 % compared to other highly complex methods.},
  archive      = {J_PR},
  author       = {Minghai Jiao and Yixian Liu and Jing Wang and Yuhuai Peng},
  doi          = {10.1016/j.patcog.2025.112148},
  journal      = {Pattern Recognition},
  month        = {3},
  pages        = {112148},
  shortjournal = {Pattern Recognition},
  title        = {Pixel2Noise: A lightweight self-supervised denoising for single image zero-shot recognition},
  volume       = {171},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Disparity guidance and spatial-angular interaction for single-view-based light field synthesis. <em>PR</em>, <em>171</em>, 112147. (<a href='https://doi.org/10.1016/j.patcog.2025.112147'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Local light field synthesis aims to synthesize a 4D light field from a single-view image, and it has a vast range of applications, such as VR, refocusing and depth estimation. This task, however, is very challenging due to the difficulty in understanding the scene geometry (e.g.disparity map) from a single-view image. Existing methods tackle this task either by warping the estimated scene geometry, or by super-resolution technique. Nevertheless, they may fail due to the inaccurate estimation of scene geometry without the supervision of ground truth and insufficient usage of angular and spatial information of light fields. To address these, we propose a Disparity-Guided Light Field generation (DGLF) paradigm. Specifically, DGLF parallelly generates target light fields and disparity maps which are exploited to guide the generation process to be aware of scene geometry. Moreover, we devise an interactive discriminator that aids in synthesizing a more perceptually realistic light field by capturing the interaction of spatial and angular information. Lastly, based on DGLF and interactive discriminator, we develop a disparity-guided adversarial generative network (DG-GAN) for light field synthesis. We theoretically analyze the generalization performance of DGLF. Extensive experiments on five light field datasets demonstrate the effectiveness of DG-GAN. Our code link: https://github.com/YifYang993/DG-GAN.git},
  archive      = {J_PR},
  author       = {Yifan Yang and Zhen Qiu and Shuhai Zhang and Mingkui Tan},
  doi          = {10.1016/j.patcog.2025.112147},
  journal      = {Pattern Recognition},
  month        = {3},
  pages        = {112147},
  shortjournal = {Pattern Recognition},
  title        = {Disparity guidance and spatial-angular interaction for single-view-based light field synthesis},
  volume       = {171},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). SemiSketch: An ancient mural sketch extraction network based on reference prior and gradient frequency compensation. <em>PR</em>, <em>171</em>, 112146. (<a href='https://doi.org/10.1016/j.patcog.2025.112146'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Sketches hold considerable research value for archaeologists as they convey the ancient culture, artistic techniques, and social contexts of murals. However, the widespread presence of deteriorated artifacts and the scarcity of artifact databases make it difficult to train a sketch extraction model. This study develops SemiSketch, a novel semi-supervised framework that extracts clean, coherent sketches from ancient murals. By leveraging a dual-branch learning paradigm, it effectively mitigates challenges such as deterioration artifacts, noise, and data scarcity. SemiSketch innovatively uses a pixel-level reference mechanism as an intermediary “pivot” between deteriorated murals and clean sketches. It decomposes the training process into two branches: an unsupervised branch that transforms murals into sketches, and a supervised branch that refines noiseless line styles through pixel-level correspondences. We introduce a shared CNN-hybrid Vision Transformer generator to integrate the two branches, combining CNN-based transpose self-attention and axial attention to capture local and global information, thereby enhancing the extraction of key lines in murals. Additionally, a gradient frequency compensation module is employed to effectively mitigate noise caused by deterioration artifacts, resulting in more complete and cleaner sketches. Empirical evaluations are conducted on various styles of datasets, including the Fengguo Temple Buddhist frescoes, Dunhuang murals, and Indian murals. Extensive experiments show that SemiSketch substantially outperforms a wide range of baselines and effectively extracts clear and coherent sketches. We release the source code at https://github.com/Alice77bai/SemiSketch .},
  archive      = {J_PR},
  author       = {Zhe Yu and Jun Wang and Shuyi Qu and Qunxi Zhang and Yirong Ma and Shenglin Peng and Jinye Peng},
  doi          = {10.1016/j.patcog.2025.112146},
  journal      = {Pattern Recognition},
  month        = {3},
  pages        = {112146},
  shortjournal = {Pattern Recognition},
  title        = {SemiSketch: An ancient mural sketch extraction network based on reference prior and gradient frequency compensation},
  volume       = {171},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). HTVR: Hierarchical text-to-video retrieval based on relative similarity. <em>PR</em>, <em>171</em>, 112145. (<a href='https://doi.org/10.1016/j.patcog.2025.112145'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the rapid growth of video and the demand for similarity search, text-video retrieval has garnered significant attention. Existing methods usually extract features by unified encoders and compute pair-wise similarity. However, most methods employ symmetric cross-entropy loss for contrastive learning, crudely reducing the distance between positives and expanding the distance between negatives. This may ignore some effective information between negatives and lead to learning untrustworthy representations. Besides, by relying solely on constructed feature similarity, these methods often fail to capture the shared semantic information between negatives. To mitigate these problems, we propose a novel Hierarchical Text-to-Video Retrieval based on Relative Similarity, named HTVR. The developed HTVR introduces a relative similarity matrix to replace the traditional contrastive loss. Specifically, to build the relative similarity matrix, we leverage textual descriptions to construct the virtual semantic label features. Then, we can measure the relative similarity between instances based on virtual semantic label features, providing more detailed supervision for cross-modal matching. Moreover, we perform multi-level alignment by aggregating tokens to handle fine-grained semantic concepts with diverse granularity and flexible combinations. The relative similarity is further utilized for intra-modal constraints, preserving shared semantic concepts. Finally, extensive experiments on three benchmark datasets (including MSRVTT, MSVD, and DiDeMo) illustrate that our HTVR can achieve superior performances, demonstrating the efficacy of the proposed method. The source code of this work will be available at: https://github.com/junmaZ/HTVR .},
  archive      = {J_PR},
  author       = {Donglin Zhang and Zhiwen Wang and Zhikai Hu and Xiao-Jun Wu},
  doi          = {10.1016/j.patcog.2025.112145},
  journal      = {Pattern Recognition},
  month        = {3},
  pages        = {112145},
  shortjournal = {Pattern Recognition},
  title        = {HTVR: Hierarchical text-to-video retrieval based on relative similarity},
  volume       = {171},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Long-term pre-training for temporal action detection with transformers. <em>PR</em>, <em>171</em>, 112144. (<a href='https://doi.org/10.1016/j.patcog.2025.112144'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Temporal action detection (TAD) is challenging, yet fundamental for real-world video applications. Recently, DETR-based models for TAD have been prevailing thanks to their unique benefits. However, transformers demand a huge dataset, and unfortunately data scarcity in TAD causes a severe degeneration. In this paper, we identify two crucial problems from data scarcity: attention collapse and imbalanced performance. To this end, we propose a new pre-training strategy, Long-Term Pre-training (LTP), tailored for transformers. LTP has two main components: 1) class-wise synthesis, 2) long-term pretext tasks. Firstly, we synthesize long-form video features by merging video snippets of a target class and non-target classes. They are analogous to untrimmed data used in TAD, despite being created from trimmed data. In addition, we devise two types of long-term pretext tasks to learn long-term dependency. They impose long-term conditions such as finding second-to-fourth or short-duration actions. Our extensive experiments show state-of-the-art performances in DETR-based methods on ActivityNet-v1.3 and THUMOS14 by a large margin. Moreover, we demonstrate that LTP significantly relieves the data scarcity issues in TAD.},
  archive      = {J_PR},
  author       = {Jihwan Kim and Miso Lee and Jae-Pil Heo},
  doi          = {10.1016/j.patcog.2025.112144},
  journal      = {Pattern Recognition},
  month        = {3},
  pages        = {112144},
  shortjournal = {Pattern Recognition},
  title        = {Long-term pre-training for temporal action detection with transformers},
  volume       = {171},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Hierarchical feature distillation model via dual-stage projections and graph embedding label propagation for emotion recognition. <em>PR</em>, <em>171</em>, 112143. (<a href='https://doi.org/10.1016/j.patcog.2025.112143'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In multi-source domain adaptation, challenges include negative transfer caused by feature coupling and the inefficiency of pseudo-label generation. This paper develops a multi-source domain adaptive framework for EEG-based recognition (MSGELP), which integrates a two-stage projection matrix decoupling mechanism and graph-embedded label propagation. The method employs a dynamic source selection mechanism that adaptively selects the top- K most similar source domains based on similarity evaluation across target-source domain pairs, while eliminating latent sources of negative transfer. At the feature decoupling level, a learnable two-stage projection matrix, including a global projection matrix and an alignment projection matrix, is designed to explicitly separate cross-domain knowledge: the global projection matrix extracts common feature spanning multiple domains, while the alignment projection matrix captures domain-specific feature of source-target pairs, preserving discriminative information while avoiding feature entanglement. Furthermore, by constructing a similarity graph of source-target domain pairs and iteratively propagating labels, graph embedding techniques, along with iterative updates to the projection matrices, achieve continuous cross-domain knowledge distillation, effectively improving pseudo-label generation accuracy. Finally, rigorous testing of the cross-subject leave-one-subject-out cross-validation strategy on the SEED-IV and SEED-V datasets achieved classification accuracies of 68.70 % and 63.09 %, respectively. Experimental results indicate that the MSGELP effectively learns a shared subspace, mitigates the negative transfer problem, and outperforms state-of-the-art methods. The code is available at https://github.com/czihan1022/MSGELP/ .},
  archive      = {J_PR},
  author       = {Chao Ren and Jinbo Chen and Rui Li and Yijiang Chen and Tianzhi Wang and Weihao Zheng and Xiaowei Zhang and Bin Hu},
  doi          = {10.1016/j.patcog.2025.112143},
  journal      = {Pattern Recognition},
  month        = {3},
  pages        = {112143},
  shortjournal = {Pattern Recognition},
  title        = {Hierarchical feature distillation model via dual-stage projections and graph embedding label propagation for emotion recognition},
  volume       = {171},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Enhanced human lower-limb motion recognition using flexible sensor array and relative position image. <em>PR</em>, <em>171</em>, 112142. (<a href='https://doi.org/10.1016/j.patcog.2025.112142'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Accurate lower-limb motion recognition is crucial for fields such as sports rehabilitation, smart healthcare, and intelligent monitoring, with wearable sensors being widely applied as a key technological approach. However, current mainstream lower-limb motion recognition methods mainly rely on single-node wearable sensors and improvements in classification models, leading to significant deficiencies in terms of data dimensionality and feature extraction capabilities. To address these issues, this paper proposes a motion recognition method that integrates a flexible sensor array and relative position images. First, a motion capture system using a flexible array sensor was developed, capable of collecting muscle activity data from 16 channels during lower-limb movements, providing rich data support for lower-limb motion recognition. Secondly, a novel relative position image generation method is proposed, which converts array data into grayscale images containing both time and spatial location information, thereby enhancing the feature expression ability of the raw data. Finally, a new network named MCRANet is designed, incorporating multi-scale cascaded residual attention module and cross-feature interaction module, significantly enhancing the feature extraction capability. Experimental results show that the proposed method performs excellently, achieving an accuracy of 97.88 % for lower-limb motion recognition, significantly outperforming existing advanced methods.},
  archive      = {J_PR},
  author       = {Chao Lian and Wayne Jason Li and Yafeng Kang and Wenjing Li and Dongyu Zhou and Zhikun Zhan and Meng Chen and Jiao Suo and Yuliang Zhao},
  doi          = {10.1016/j.patcog.2025.112142},
  journal      = {Pattern Recognition},
  month        = {3},
  pages        = {112142},
  shortjournal = {Pattern Recognition},
  title        = {Enhanced human lower-limb motion recognition using flexible sensor array and relative position image},
  volume       = {171},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). DiffusionEngine: Diffusion model is scalable data engine for object detection. <em>PR</em>, <em>171</em>, 112141. (<a href='https://doi.org/10.1016/j.patcog.2025.112141'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Data is the cornerstone of deep learning. This paper reveals that the recently-developed Diffusion Model is a scalable data engine for object detection. Existing methods for scaling up detection-oriented data often require manual collection or generative models to obtain target images, followed by data augmentation and labeling to produce training pairs, which are costly, complex, or lacking diversity. To address these issues, we present DiffusionEngine (DE), a data scaling-up engine that provides high-quality detection-oriented training pairs in a single stage. DE consists of a pre-trained diffusion model and an effective Detection-Adapter , contributing to generating scalable, diverse and generalizable detection data in a plug-and-play manner. Detection-Adapter is learned to align the implicit semantic and location knowledge in off-the-shelf diffusion models with detection-aware signals to make better bounding-box predictions. Additionally, we contribute two datasets, i.e. , COCO-DE and VOC-DE , to scale up existing detection benchmarks for facilitating follow-up research. Extensive experiments demonstrate that data scaling-up via DE can achieve significant improvements in diverse scenarios, such as various detection algorithms, self-supervised pre-training, data-sparse, label-scarce, cross-domain, and semi-supervised learning. For example, when using DE with a DINO-based adapter to scaling-up data, mAP is improved by 3.1 % on COCO, 7.6 % on VOC and 11.5 % on Clipart.},
  archive      = {J_PR},
  author       = {Manlin Zhang and Jie Wu and Yuxi Ren and Jiahong Yang and Ming Li and Andy J. Ma},
  doi          = {10.1016/j.patcog.2025.112141},
  journal      = {Pattern Recognition},
  month        = {3},
  pages        = {112141},
  shortjournal = {Pattern Recognition},
  title        = {DiffusionEngine: Diffusion model is scalable data engine for object detection},
  volume       = {171},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Efficient greedy optimization method for k-means. <em>PR</em>, <em>171</em>, 112140. (<a href='https://doi.org/10.1016/j.patcog.2025.112140'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {k -means is a widely used and classical clustering algorithm, with numerous studies dedicated to enhancing its performance. In this paper, we first review previous approaches to improving k -means, and highlight the significance of greedy strategies. Inspired by this, we propose a novel greedy-based optimization algorithm for k -means, referred to as greedy local k -means (GLKM). GLKM initially treats each sample as an individual cluster and employs a greedy merging strategy to iteratively merge pairs of clusters, based on minimizing incremental k -means loss. The k -nearest neighbors graph is utilized to focus on local structure and improve efficiency. A red-black tree is employed to directly retrieve merged cluster pairs. GLKM yields deterministic clustering results and does not require random initialization or multiple runs to obtain optimal clustering results. In addition, it avoids producing empty clusters. Experiments on synthetic and benchmark datasets demonstrate the effectiveness and superiority of GLKM. Its ability to handle various data types and scales makes it a reliable choice for a wide range of real-world scenarios.},
  archive      = {J_PR},
  author       = {Yuan Yuan and Lin Zhao and Shenfei Pei and Feiping Nie},
  doi          = {10.1016/j.patcog.2025.112140},
  journal      = {Pattern Recognition},
  month        = {3},
  pages        = {112140},
  shortjournal = {Pattern Recognition},
  title        = {Efficient greedy optimization method for k-means},
  volume       = {171},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). CCPNet: Joining the pooling transformer and target context for medical image segmentation. <em>PR</em>, <em>171</em>, 112139. (<a href='https://doi.org/10.1016/j.patcog.2025.112139'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The automated and precise segmentation of medical images is vital for the clinical diagnosis and treatment planning. Recent segmentation networks utilize the self-attention that calculates a global similarity matrix to capture the long-range dependency, which breaks the local limitation in the convolution. However, the quadratic complexity is unbearable when integrating the global context of high-resolution features. Besides, existing researches prefer to propose powerful encoders but neglect to design ingenious decoders. Following the encoder structure to design decoder is difficult to generate precise masks, owing to both difference in functionality. In this paper, we propose a segmentation network CCPNet to avoid mentioned drawbacks. Specifically, we redesign the encoding unit to efficiently blend details and semantics in each scale, and optimize its structure from the standpoint of maximizing the gradient combination. Moreover, we design the decoder that utilizes prior intermediate probability to formulate target contexts and refine decoding features through the spatial reduction cross-attention. Overall, it forms cascade adjustments to enhance the inter-category consistency and the intra-category difference. Our extensive evaluations on three public benchmarks and one self-built dataset, reveal the superiority of our CCPNet in terms of accuracy and complexity. On ACT-1K, our approach obtains best accuracy(76.38 % on mIoU and 85.04 % on mDice) compared to existing typical methods. In applications, we originally establish a standard to quantitatively analyze the tibial dyschondroplasia based on the segmented masks of self-built dataset. The code will be available at https://github.com/LATIESUS/CCPNet .},
  archive      = {J_PR},
  author       = {Yakun Yang and Xiangping Liu and Hongcheng Xue and Chungang Feng and Hao Qu and Longhe Wang and Lin Li},
  doi          = {10.1016/j.patcog.2025.112139},
  journal      = {Pattern Recognition},
  month        = {3},
  pages        = {112139},
  shortjournal = {Pattern Recognition},
  title        = {CCPNet: Joining the pooling transformer and target context for medical image segmentation},
  volume       = {171},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Partial label learning with semi-supervised clustering disambiguation. <em>PR</em>, <em>171</em>, 112138. (<a href='https://doi.org/10.1016/j.patcog.2025.112138'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In partial label learning, the most challenging issue is that the ground-truth labels of training samples are concealed within their respective candidate label sets. Generally, this problem is addressed by disambiguating the candidate label set via manipulating either feature or label space. In this paper, we propose a novel partial label learning algorithm named PLOD that implements the disambiguation process as a semi-supervised clustering problem, enabling the natural utilization of both the structural information in the feature space and weakly supervised labeling information. Specifically, PLOD aims to infer the ground-truth label for each training sample using a tailor-made constrained k -means algorithm. PLOD initiates the cluster center for each class by averaging instances whose candidate label set includes that class. Subsequently, instances are assigned to the nearest class within their candidate label set. The cluster centers are then updated by averaging the instances of each class, followed by a repeated class assignment procedure. These two steps are iteratively performed until convergence. After that, a one-versus-one decomposition is employed to solve the resulting problem, treating the assigned class for each training sample as ground-truth. Finally, PLOD aggregates the related predictive outputs of the one-versus-one decomposition for each class to derive the final prediction. Experimental results on both artificial and real-world partial label data sets demonstrate the superior performance of our proposed PLOD algorithm. The code is publicly available at https://github.com/liujunying-ai/PLOD .},
  archive      = {J_PR},
  author       = {Jun-Ying Liu and Jian-Ping Sun and Ya-Hong Zhao and Bin-Bin Jia and Min-Ling Zhang},
  doi          = {10.1016/j.patcog.2025.112138},
  journal      = {Pattern Recognition},
  month        = {3},
  pages        = {112138},
  shortjournal = {Pattern Recognition},
  title        = {Partial label learning with semi-supervised clustering disambiguation},
  volume       = {171},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). CLTR: Continual learning time-varying regularization for robust classification of noisy label images. <em>PR</em>, <em>171</em>, 112137. (<a href='https://doi.org/10.1016/j.patcog.2025.112137'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {On datasets with noisy labels, the deep neural network will overfit the noisy labels, which will weaken the generalization of the model. Related studies show that deep neural networks have the characteristic of memorizing clean data first and then noisy data. Regularization and small loss algorithms exploit the memory characteristics of the network to improve its noise immunity. However, they tend to ignore some clean samples or require additional hyperparameters, and lack control over the direction of network parameter updates. In this paper, we propose a continual learning time-varying regularization (CLTR) based robust classification method for noisy label images. Specifically, learning with noisy label is considered as a continual learning process. By introducing the parameter processing method in catastrophic forgetting, the training parameters of the network are decomposed into clean and noisy parameters. In CLTR, the update direction of clean and noisy parameters is controlled by regularization terms with time-varying coefficients. The time-varying coefficient comes from the initial prediction of the network, which truly reflects the dynamic influence of noisy labels on the network and avoids the setting of hyperparameters. It greatly reduces the complexity of network training and improves its applicability in practice. Extensive experiments on synthetic and real-world benchmarks confirm the superior performance of the proposed method. The code will be released at https://github.com/vpsg-research/CLTR .},
  archive      = {J_PR},
  author       = {Yanhong Li and Zhiqing Guo and Liejun Wang},
  doi          = {10.1016/j.patcog.2025.112137},
  journal      = {Pattern Recognition},
  month        = {3},
  pages        = {112137},
  shortjournal = {Pattern Recognition},
  title        = {CLTR: Continual learning time-varying regularization for robust classification of noisy label images},
  volume       = {171},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). FRIES: Framework for inconsistency estimation of saliency metrics. <em>PR</em>, <em>171</em>, 112136. (<a href='https://doi.org/10.1016/j.patcog.2025.112136'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Saliency maps are widely used as a post-hoc approach to explain the decision-making process of Deep Learning (DL) based image classification models, but evaluating their fidelity remains a complex problem. While saliency metrics have been introduced to evaluate the fidelity of saliency maps, existing saliency metrics, such as perturbation-based saliency metrics, have been previously reported to demonstrate statistical inconsistency. Although inconsistencies have been noted in different works, there exists no mechanism for estimating the same, i.e., Inconsistency Estimation (IE). Our primary objective is to address this limitation, and therefore, we propose a framework to estimate the inconsistency of saliency metrics for any given DL model. The framework enables building IE models for estimating the inconsistency by employing a set of perturbation types and schemes. The framework’s modular architecture provides flexibility across (i) perturbation types (Inpainting, Uniform, and Gaussian blur), (ii) perturbation schemes (pixel-wise and patch-wise), (iii) learning mechanisms (Convolutional Neural Networks and Vision Transformers) and (iv) IE modeling techniques (bagging and boosting). Extensive experimental results are shown on three well-known DL architectures (Inception-V3, Xception, and ResNet-50) on three different public datasets, including the Imagenette, Oxford-IIIT Pets Dataset, and PASCAL VOC 2007, along with results on ViTs for Oxford-IIIT Pets Dataset, and PASCAL VOC 2007. With a comprehensive evaluation of seven different perturbation types that include two inpainting, two Gaussian blur (with kernel widths of 0.9 and 1.5), and three uniform perturbations, our work shows the effectiveness of the proposed approach in estimating inconsistency. Statistically founded tests such as repeated cross-validation and the Permutation Test further validate the idea of the proposed framework for estimating the inconsistency of saliency metrics across unseen perturbations, making it useful in real-world scenarios.},
  archive      = {J_PR},
  author       = {Revoti Prasad Bora and Philipp Terhörst and Raymond Veldhuis and Raghavendra Ramachandra and Kiran Raja},
  doi          = {10.1016/j.patcog.2025.112136},
  journal      = {Pattern Recognition},
  month        = {3},
  pages        = {112136},
  shortjournal = {Pattern Recognition},
  title        = {FRIES: Framework for inconsistency estimation of saliency metrics},
  volume       = {171},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Mitigating fusion bias for RGB-D salient object detection. <em>PR</em>, <em>171</em>, 112135. (<a href='https://doi.org/10.1016/j.patcog.2025.112135'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The prevalent two-stream structure for RGB-D salient object detection (SOD) encounters the biased fusion problem, where the fusion process exhibits a strong inclination towards one modality, due to substantial modality gaps and the adaptability of deep learning technologies. This would yield insufficient cross-modal information exploitation, thus resulting in inaccurate localization or incomplete segmentation of salient objects. To address such issues, in this paper, we reconsider the two imperative stages of RGB-D SOD, namely object localization and object segmentation, and propose a novel Bias Constraint Network (BCNet) to mitigate fusion bias for better saliency prediction. In the object localization stage, our new Common Information Mining (CIM) module explores shared semantic information in both RGB and depth data to reduce semantic bias and accurately locate objects. In the object segmentation stage, a Regional Confidence Generation (RCG) and a Fusion and Decoupling module (FDM) work together to select more reliable information from each modality and ensure effective integration of unimodal data into fused features. This minimizes reliance on a single modality, like RGB data, enabling BCNet to effectively use information from each modality for complete salient object segmentation. These three components create a nested structure to address biased fusion challenges, ultimately boosting RGB-D SOD performance. Experimental results across challenging datasets demonstrate the superiority of our model over some state-of-the arts.},
  archive      = {J_PR},
  author       = {Yang Yang and Nianchang Huang and Qiang Zhang and Jungong Han},
  doi          = {10.1016/j.patcog.2025.112135},
  journal      = {Pattern Recognition},
  month        = {3},
  pages        = {112135},
  shortjournal = {Pattern Recognition},
  title        = {Mitigating fusion bias for RGB-D salient object detection},
  volume       = {171},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). A novel broad learning network with residual technique. <em>PR</em>, <em>171</em>, 112134. (<a href='https://doi.org/10.1016/j.patcog.2025.112134'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As a newly proposed neural network, broad learning system (BLS) has superior performance and frugal time consumption. However, the degradation problem is unavoidable for BLS, since the parameter randomness leads to indetermination of the linear independence of the new nodes and the original nodes. Regrettably, there is no work until now to solve the degradation problem of broad networks. In order to address it, this paper proposes a new incremental mechanism to construct a sequence of residual operators with norm convergence for BLS, and this new learning system is called the broad residual learning system (BRLS). It is proved that he norm convergence of the residual operators can prevent network performance deterioration with the progress of network learning to effectively mitigate the degradation problem. Also, the basic BRLS and three incremental BRLS networks (adding enhancement nodes, feature nodes, and input data) are introduced to satisfy the various scenarios requirements. By the analysis and comparison of time complexity between BRLS and BLS, one can show the novel BRLS incremental mechanism has great advantages of fast computing speed and frugal computing memory. In addition, the convergence theorem and universal approximation property of BRLS are proved to ensure the feasibility and effectiveness. Validation experiments are performed on publicly available datasets to demonstrate effectiveness of the proposed methods.},
  archive      = {J_PR},
  author       = {Han Su and Zhongyan Li and Wanquan Liu and Jiankai Chen},
  doi          = {10.1016/j.patcog.2025.112134},
  journal      = {Pattern Recognition},
  month        = {3},
  pages        = {112134},
  shortjournal = {Pattern Recognition},
  title        = {A novel broad learning network with residual technique},
  volume       = {171},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Single image defocus deblurring via multimodal-guided diffusion and depth-aware fusion. <em>PR</em>, <em>171</em>, 112133. (<a href='https://doi.org/10.1016/j.patcog.2025.112133'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Defocus blur is inherently related to depth of field, yet its complex spatial variations pose significant challenges for existing methods. Two primary limitations persist: (1) insufficient semantic perception, introducing artifacts in degraded regions, and (2) the inability to explicitly infer depth cues, leading to over-sharpening in focused areas and inadequate restoration in defocused regions. To address these issues, we propose MDDF-SIDD, a novel two-stage framework that combines multimodal-guided diffusion and depth-aware fusion for single-image defocus deblurring. In the first stage, we construct a multimodal representation by integrating fidelity-aware image features with blur-adaptive prompt features. This representation guides a pretrained text-to-image diffusion model to achieve high perceptual quality with precise structural and semantic alignment. In the second stage, we exploit depth priors to generate a depth-adaptive weight map, distinguishing focused regions from defocused ones. This enables a Laplacian pyramid-based fusion strategy, which adaptively blends sharp details from the original image with the refined estimation from the first stage, ensuring both local detail preservation and global consistency. Extensive experiments demonstrate that MDDF-SIDD outperforms state-of-the-art methods in both quantitative metrics and perceptual fidelity, setting a new benchmark for defocus deblurring.},
  archive      = {J_PR},
  author       = {Xiaopan Li and Shiqian Wu and Qile Zhu and Shoulie Xie and Sos Agaian},
  doi          = {10.1016/j.patcog.2025.112133},
  journal      = {Pattern Recognition},
  month        = {3},
  pages        = {112133},
  shortjournal = {Pattern Recognition},
  title        = {Single image defocus deblurring via multimodal-guided diffusion and depth-aware fusion},
  volume       = {171},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Kernel-aware dynamic convolution for dense prediction. <em>PR</em>, <em>171</em>, 112131. (<a href='https://doi.org/10.1016/j.patcog.2025.112131'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent dynamic convolution methods present the dynamism over learnable weights, which have been proved to increase representation ability. However, their dynamic designs can only target a specific problem. In this paper, we propose a new convolution called Kernel-aware Dynamic Convolution (KDConv), which extends the dynamism over applied kernels to explicitly address both the common visual spatial geometric transformation and scale variation problems using an unified way. Specifically, KDConv adaptively determines the most suitable kernel from the filter pool in each spatial location through a kernel-aware module. In the filter pool, we set multiple shared filters with different size and offset parameters. The kernel-aware module builds the dependency between these kernels and generates the index map for selection. Our design of kernel-aware dynamic size and offset is specifically advantageous for dense prediction including object detection and instance segmentation. Extensive experiments show that KDConv achieves largely performance improvement over the state-of-the-art dynamic convolution methods.},
  archive      = {J_PR},
  author       = {Gaoge Han and Mingjiang Liang and Jinglei Tang and Yongkang Cheng and Shaoli Huang and Wei Liu},
  doi          = {10.1016/j.patcog.2025.112131},
  journal      = {Pattern Recognition},
  month        = {3},
  pages        = {112131},
  shortjournal = {Pattern Recognition},
  title        = {Kernel-aware dynamic convolution for dense prediction},
  volume       = {171},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Sarcasm detection enhanced by multi-modal topics using denoising diffusion probabilistic models. <em>PR</em>, <em>171</em>, 112130. (<a href='https://doi.org/10.1016/j.patcog.2025.112130'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-modal sarcasm detection is a challenging research issue in the understanding of communication with multiple modalities, as different kinds of interactions between these modalities are needed to be captured. Although topics often play an important role in sarcasm understanding, limited efforts have been devoted to analyzing topical information for multi-modal sarcasm detection. Therefore, in this paper, we propose a topic-based input feature enhancement method with two-stage training process, which uses multi-modal topics to help sarcasm detection. Specifically, in the first stage of training, an unsupervised topic modeling approach named MultiTopClus is proposed. This approach leverages a multi-modal latent topic space to represent topics and is pre-trained with unlabeled video data. Then, in the second training stage, the MultiTopClus model is frozen, and a diffusion model is applied to effectively reconstruct a multi-modal feature representation which is a combination of two kinds of input features: topical features generated by the topic model, and original features used in an existing multi-modal sarcasm detection model. Finally, the reconstructed feature representation of each modality is fed to the existing multi-modal sarcasm detection network to make sarcasm predictions. Systematic experiments are carried out on twelve existing multi-modal sarcasm detection models and two multi-modal sarcasm datasets (a video-based dataset and a tweet-based dataset). The experimental results reveal a notable improvement in overall performance across twelve baseline sarcasm detection models. These findings highlight the effectiveness of our proposed approach.},
  archive      = {J_PR},
  author       = {Xiaoqiang Zhang and Guangyao Li and Xiaomeng Li and Buwen Liang and Ying Chen},
  doi          = {10.1016/j.patcog.2025.112130},
  journal      = {Pattern Recognition},
  month        = {3},
  pages        = {112130},
  shortjournal = {Pattern Recognition},
  title        = {Sarcasm detection enhanced by multi-modal topics using denoising diffusion probabilistic models},
  volume       = {171},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). EIFA-KD: Explicit and implicit feature augmentation with knowledge distillation for long-tailed visual data classification. <em>PR</em>, <em>171</em>, 112129. (<a href='https://doi.org/10.1016/j.patcog.2025.112129'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To effectively tackle long-tailed visual data classification, we propose a new classification method based on explicit and implicit feature augmentation framework with knowledge distillation called EIFA-KD. The explicit augmentation divides the contextual semantic features into class-relevant and class-irrelevant features, and synthesizes the virtual features by aggregating these two features from different samples. This can enhance the diversity of the tail classes and alleviate class imbalance. The implicit augmentation models the augmented range of a tail class sample feature as a mixture of multiple Gaussian distributions, and adaptively transfers distribution information of multiple support-classes to samples of tail classes so as to enlarge tail sample range and further enrich tail samples. In addition, we design a multi-sub-branch network based on knowledge distillation to generate high-quality features by aggregating the multiple sub-branches. Experimental results show that the proposed model outperforms several state-of-the-art models on four widely used benchmarks.},
  archive      = {J_PR},
  author       = {Xiyan Deng and Xiaoli Wang and Yifan Sun and Xusheng Zhao and Siju Tian and Minqi Li and Yuping Wang},
  doi          = {10.1016/j.patcog.2025.112129},
  journal      = {Pattern Recognition},
  month        = {3},
  pages        = {112129},
  shortjournal = {Pattern Recognition},
  title        = {EIFA-KD: Explicit and implicit feature augmentation with knowledge distillation for long-tailed visual data classification},
  volume       = {171},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Gauging-β: Border-aware hierarchical clustering based on density and proximity. <em>PR</em>, <em>171</em>, 112128. (<a href='https://doi.org/10.1016/j.patcog.2025.112128'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Data clustering plays a crucial role in scientific discovery and various real-world applications. However, many clustering algorithms encounter challenges that compromise their effectiveness and the accuracy of data grouping. This paper addresses three key challenges faced by most algorithms: (1) parameter setting, (2) data convexity, and (3) data separation. The proposed algorithm leverages density-based methods to identify and remove border points, effectively separating data sets. A hierarchical, single-linkage-based algorithm is then applied to the remaining points to generate the main clusters. Finally, the border points are reintegrated into the formed clusters. Experimental results demonstrate that the algorithm is capable of handling both convex and non-convex, as well as well-separated and poorly-separated, data sets. The impact of parameter settings on clustering outcomes is thoroughly investigated. Additionally, further experiments on real-world data sets reveal that the consistency of clustering results with classification labels strongly depends on an appropriate measure of sample similarity.},
  archive      = {J_PR},
  author       = {Jinli Yao and Yong Zeng},
  doi          = {10.1016/j.patcog.2025.112128},
  journal      = {Pattern Recognition},
  month        = {3},
  pages        = {112128},
  shortjournal = {Pattern Recognition},
  title        = {Gauging-β: Border-aware hierarchical clustering based on density and proximity},
  volume       = {171},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Active contour model combining frequency domain information for noisy vessel image segmentation. <em>PR</em>, <em>171</em>, 112126. (<a href='https://doi.org/10.1016/j.patcog.2025.112126'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Noise remains a persistent challenge in medical image segmentation, with existing mathematical and computer science-based algorithms still leaving substantial room for improvement. This paper proposes an active contour model combining frequency domain information (ACFDI) to enhance segmentation accuracy and efficiency for noisy vessel images. The innovation lies in leveraging Fourier transform-derived frequency domain information: low-frequency region information serves as the initial contour, eliminating the need for traditional initial contour selection and addressing sensitivity issues, while high-frequency region information is incorporated as weight coefficients in the energy functional to accelerate functional evolution and improve segmentation precision. Comparative experiments with other level set and deep learning models on noisy vessel images validate the ACFDI model’s superiority and stability.},
  archive      = {J_PR},
  author       = {Xin Jiang and Chong Feng and Tianyi Han and Boying Wu and Yunyun Yang},
  doi          = {10.1016/j.patcog.2025.112126},
  journal      = {Pattern Recognition},
  month        = {3},
  pages        = {112126},
  shortjournal = {Pattern Recognition},
  title        = {Active contour model combining frequency domain information for noisy vessel image segmentation},
  volume       = {171},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). THTFormer: Topology-adaptive hypergraph transformer network for skeleton-based action recognition. <em>PR</em>, <em>171</em>, 112125. (<a href='https://doi.org/10.1016/j.patcog.2025.112125'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Skeleton-based action recognition has broad promising applications in the field of rehabilitation training. Accurate action recognition ensures that patients can perform rehabilitation training action accurately, thus improving the rehabilitation efficacy and reducing the risk of secondary injuries. Previous works have successfully applied graph convolution to skeleton-based action recognition, achieving excellent performance with the prior knowledge of human structures. However, graph convolutional network struggle to capture dynamic high-order correlations in human action due to it only reliance on fixed topological structure and express the adjacent joints relationship, limiting it ability to model complex and action-dependent joint correlations. To break through the above limitations, we innovatively propose a Topology-adaptive Hypergraph Transformer Network (THTFormer). THTFormer consists of a Topology-adaptive Hypergraph Convolutional module (THC) and a Hypergraph Self-Attention module (HSA). THC integrates human body topology prior knowledge to capture high-order joint correlations and adaptively adjust for different inputs. HSA is used to obtain global dependencies between joints. Besides, we design a Contrastive Learning Branch (CLB) based on intra-class and inter-class calibration to optimize local action representations and improve action recognition for similar human topology structures. Extensive experiments demonstrate that THTFormer achieves current open source state-of-the-art performances on three benchmark datasets (NTU RGB+D 60, NTU RGB+D 120 and NW-UCLA).},
  archive      = {J_PR},
  author       = {Nan Ma and Genbao Xu and Yiheng Han and Beining Sun},
  doi          = {10.1016/j.patcog.2025.112125},
  journal      = {Pattern Recognition},
  month        = {3},
  pages        = {112125},
  shortjournal = {Pattern Recognition},
  title        = {THTFormer: Topology-adaptive hypergraph transformer network for skeleton-based action recognition},
  volume       = {171},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Interpretable multi-view clustering via anchor graph-based tensor decomposition with convergence guarantees. <em>PR</em>, <em>171</em>, 112124. (<a href='https://doi.org/10.1016/j.patcog.2025.112124'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-view data capture characteristics of objects from multiple perspectives, facilitating the extraction of valuable information. Non-negative matrix factorization (NMF) is widely used for multi-view clustering (MVC), it is typically applied to each view independently. Although uniformity for each view is enforced, the hidden spatial structure remains unexploited. To address this limitation, we propose an interpretable multi-view clustering method, IMVATC, embedded within a unified learning framework. IMVATC employs a novel density peak clustering (NDPC) strategy to select anchor points cross-view and adaptively learns anchor graphs for each view. Decomposing the anchor tensor formed by stacking anchor graphs produces consistent clustering results without the need for post-processing. A tensor low-rank constraint and cross-view local manifold mining further preserve the intrinsic structure of the data. We develop an efficient algorithm and rigorously prove its convergence based on the KKT condition. Experimental results across multiple real-world datasets demonstrate that IMVATC outperforms current leading multi-view clustering methods.},
  archive      = {J_PR},
  author       = {Jingyu Wang and Tingquan Deng and Ming Yang},
  doi          = {10.1016/j.patcog.2025.112124},
  journal      = {Pattern Recognition},
  month        = {3},
  pages        = {112124},
  shortjournal = {Pattern Recognition},
  title        = {Interpretable multi-view clustering via anchor graph-based tensor decomposition with convergence guarantees},
  volume       = {171},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). A systematic review of interpretability and explainability for speech emotion features in automatic speech emotion recognition. <em>PR</em>, <em>171</em>, 112122. (<a href='https://doi.org/10.1016/j.patcog.2025.112122'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Speech Emotion Recognition (SER) is a method of identifying emotional states from the human voice. Automatic SER (ASER) is a research domain where Machine Learning (ML) is used to extract and analyze speech features to predict emotional states. Using ML in a sensitive area like SER requires transparency and reliability of the models. For instance, ASER is crucial to understanding the underlying decision-making in real-world applications such as mental health monitoring systems. Researchers, therefore, have focused attention on advancing the interpretability and explainability of ASER models. Interpretability maximizes human understanding of complex processes by providing meaningful insights. Explainability presents the interpretable insights in a clear and human-understandable manner. Some standard interpretability methods include feature importance, feature selection methods, and attention models. Explainability methods include SHapley Additive exPlanations (SHAP), visualizations using embedding plots, saliency maps, etc., and feature importance analysis. The current systematic review explores the different interpretability and explainability methods for speech emotion features. The current review paper aims to identify the progress in the area, identify potential research gaps, and motivate future research.},
  archive      = {J_PR},
  author       = {Hiruni Maleesa Jayasinghe and Kok Wai Wong and Anupiya Nugaliyadde},
  doi          = {10.1016/j.patcog.2025.112122},
  journal      = {Pattern Recognition},
  month        = {3},
  pages        = {112122},
  shortjournal = {Pattern Recognition},
  title        = {A systematic review of interpretability and explainability for speech emotion features in automatic speech emotion recognition},
  volume       = {171},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Joint low-rank and sparse components extraction for cross-domain recognition. <em>PR</em>, <em>171</em>, 112121. (<a href='https://doi.org/10.1016/j.patcog.2025.112121'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Feature extraction provides a convenient way for data analysis. However, the applicability of conventional methods in real scenarios is limited by their sensitivity to the distribution of data. This paper considers an observation that the data from different domains have both strong correlation and uniqueness. Therefore, the common and individual components of each domain should be extracted for data analysis. From this perspective, we propose a novel joint low-rank and sparse components extraction (JLSC) method which combines aspects of both feature extraction and transfer learning, using three different matrices to extract different feature components. Specifically, the information-rich component is extracted by introducing the PCA-like matrix, and the common and individual components are learned by extracting the low-rank feature and column-sparsity feature, respectively. We developed an efficient augmented Lagrange multiplier-based optimization algorithm, which integrates projected matrix rank and sparsity constraints. Our experiments on different datasets show that JLSC significantly outperforms state-of-the-art methods.},
  archive      = {J_PR},
  author       = {Zhixiang Zeng and Weijun Sun and Xiaozhao Fang and Guoxu Zhou and Shengli Xie},
  doi          = {10.1016/j.patcog.2025.112121},
  journal      = {Pattern Recognition},
  month        = {3},
  pages        = {112121},
  shortjournal = {Pattern Recognition},
  title        = {Joint low-rank and sparse components extraction for cross-domain recognition},
  volume       = {171},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Capsule networks do not need to model everything. <em>PR</em>, <em>171</em>, 112119. (<a href='https://doi.org/10.1016/j.patcog.2025.112119'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Capsule networks are biologically inspired neural networks that group neurons into vectors called capsules, each explicitly representing an object or one of its parts. The routing mechanism connects capsules in consecutive layers, forming a hierarchical structure between parts and objects, also known as a parse tree. Capsule networks often attempt to model all elements in an image, requiring large network sizes to handle complexities such as intricate backgrounds or irrelevant objects. However, this comprehensive modeling leads to increased parameter counts and computational inefficiencies. Our goal is to enable capsule networks to focus only on the object of interest, reducing the number of parse trees. We accomplish this with REM (Routing Entropy Minimization), a technique that minimizes the entropy of the parse tree-like structure. REM drives the model parameters distribution towards low entropy configurations through a pruning mechanism, significantly reducing the generation of intra-class parse trees. This empowers capsules to learn more stable and succinct representations with fewer parameters and negligible performance loss.},
  archive      = {J_PR},
  author       = {Riccardo Renzulli and Enzo Tartaglione and Marco Grangetto},
  doi          = {10.1016/j.patcog.2025.112119},
  journal      = {Pattern Recognition},
  month        = {3},
  pages        = {112119},
  shortjournal = {Pattern Recognition},
  title        = {Capsule networks do not need to model everything},
  volume       = {171},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). IFA: Illumination-aware feature aggregation model for salient object detection. <em>PR</em>, <em>171</em>, 112118. (<a href='https://doi.org/10.1016/j.patcog.2025.112118'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Significant progress has been made in salient object detection. Most methods extract information directly on RGB features result in a loss of semantic information. When others introduce spatial information by using depth, difficult-to-access depth and the imbalance of depth have brought serious challenge. In this paper, we introduce illumination map based on Retinex theory inspired by low-light enhancement to SOD. Easy-to-obtain illumination can not only supplement semantic information, but also one can avoid the problem of unbalanced object depth. We design the dual-scale progressive fusion module, which can aggregate the illumination by proposed cross-scale information fusion. We embed it to design an illumination-aware model, where we also build a tailored multi-scale feature connection prediction network to obtain salient maps. Through experiments completed on five datasets, our method has demonstrated excellent detection ability and provides a novel solution for SOD,especially in underwater and low-light datasets our method still achieves outstanding results.},
  archive      = {J_PR},
  author       = {Miao Li and Hongyun Zhang and Kecan Cai and Witold Pedrycz and Duoqian Miao and Ying Gao},
  doi          = {10.1016/j.patcog.2025.112118},
  journal      = {Pattern Recognition},
  month        = {3},
  pages        = {112118},
  shortjournal = {Pattern Recognition},
  title        = {IFA: Illumination-aware feature aggregation model for salient object detection},
  volume       = {171},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). SuperCM: Improving semi-supervised learning and domain adaptation through differentiable clustering. <em>PR</em>, <em>171</em>, 112117. (<a href='https://doi.org/10.1016/j.patcog.2025.112117'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Semi-Supervised Learning (SSL) and Unsupervised Domain Adaptation (UDA) enhance the model performance by exploiting information from labeled and unlabeled data. The clustering assumption has proven advantageous for learning with limited supervision and states that data points belonging to the same cluster in a high-dimensional space should be assigned to the same category. Recent works have utilized different training mechanisms to implicitly enforce this assumption for the SSL and UDA. In this work, we take a different approach by explicitly involving a differentiable clustering module which is extended to leverage the supervised data to compute its centroids. We demonstrate the effectiveness of our straightforward end-to-end training strategy for SSL and UDA over extensive experiments and highlight its benefits, especially in low supervision regimes, both as a standalone model and as a regularizer for existing approaches.},
  archive      = {J_PR},
  author       = {Durgesh Singh and Ahcène Boubekki and Robert Jenssen and Michael Kampffmeyer},
  doi          = {10.1016/j.patcog.2025.112117},
  journal      = {Pattern Recognition},
  month        = {3},
  pages        = {112117},
  shortjournal = {Pattern Recognition},
  title        = {SuperCM: Improving semi-supervised learning and domain adaptation through differentiable clustering},
  volume       = {171},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). L2MLP: A novel MLP-based locality learning method for point cloud analysis. <em>PR</em>, <em>171</em>, 112115. (<a href='https://doi.org/10.1016/j.patcog.2025.112115'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As the crucial measurement for surrounding 3D environment, point cloud has received widespread attention, and encouraged various deep learning-based methods. To provide advanced perception performance, feature presentation of locality of 3D point cloud has been continuously explored. However, the existing methods capture neighborhood structure inadequately since the inter-correlation among neighbor points are ignored. In this paper, we highlight a novel MLP-based method for L ocality L earning in point cloud processing termed as L2MLP. Specifically, we first analyze the representative locality learning strategies in existing point cloud networks, which motivates our new architecture comprised of two blocks. One block applies MLP to the feature channels of each point (termed as channel-interaction block), and another block applies MLP across points within the locality (termed as point-interaction block), which emphasize the interaction on both channels and points. Compared with the conventional strategies, our L2MLP emphasizes summarizing the relevance among all points in the locality achieving accurate description and retain high time effectiveness due to the concise MLP structure. To comprehensively validate the gain for 3D perception, we seamlessly integrate the L2MLP module into multiple typical point cloud networks and perform extensive evaluations across various tasks, including point cloud classification, segmentation, and registration. The remarkably enhanced performance demonstrates the effectiveness of the proposed method convincingly.},
  archive      = {J_PR},
  author       = {Zhiyuan Zhang and Zhihui Li and Panhe Hu and Junpeng Shi},
  doi          = {10.1016/j.patcog.2025.112115},
  journal      = {Pattern Recognition},
  month        = {3},
  pages        = {112115},
  shortjournal = {Pattern Recognition},
  title        = {L2MLP: A novel MLP-based locality learning method for point cloud analysis},
  volume       = {171},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Multilevel cross-domain relational network for drug repositioning. <em>PR</em>, <em>171</em>, 112114. (<a href='https://doi.org/10.1016/j.patcog.2025.112114'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Existing drug repositioning methods typically rely on Drug–Disease Association Network and Similarity Networks, with entities being embedded through different encoders. However, this approach fails to fully exploit the potential deep relationships between drugs and diseases. To this end, a drug repositioning framework MCDR has been proposed in this paper, which integrates the relevant information of drugs and diseases into a unified hierarchical framework-the Multilevel Cross-Domain Relational Network. To encode the direct associations between drugs and diseases, a pre-training drug–drug interaction (DDI) prediction model is used to evaluate the potential interactions between drugs and construct the Drug Collaboration Network, while a similarity-based strategy is adopted to construct the Disease Collaboration Network. Finally, a Cross-Domain Integrated Encoder has been designed to effectively integrate the multilevel cross-domain relational network. Through 10-fold cross-validation on four public datasets, superior performance has been demonstrated by MCDR, especially in De Novo scenario, with a 42 % improvement in AUPR, and its real-world applicability has been evaluated through case studies.},
  archive      = {J_PR},
  author       = {Dongjiang Niu and Xiaofeng Wang and Zengqian Deng and Bowen Tang and Zhen Li},
  doi          = {10.1016/j.patcog.2025.112114},
  journal      = {Pattern Recognition},
  month        = {3},
  pages        = {112114},
  shortjournal = {Pattern Recognition},
  title        = {Multilevel cross-domain relational network for drug repositioning},
  volume       = {171},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Multi-level graph self-supervised learning for multi-modal medical corpus construction. <em>PR</em>, <em>171</em>, 112113. (<a href='https://doi.org/10.1016/j.patcog.2025.112113'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-modal medical corpus, as a novel tool for computer-aided medical diagnosis and learning research, hold significant value in exploring pathogenic mechanisms. However, in the field of neuroscience, brain imaging often lacks semantic labels, making the construction of multi-modal medical corpus challenging. Functional magnetic resonance imaging (fMRI), a commonly used brain imaging, can be employed to study functional brain networks and perform classification to obtain labels. Moreover, due to limited data samples, developing graph foundation model for brain network classification is important. We propose a multi-level graph self-supervised learning (MLGSL) method, which performs multi-level pretraining tasks to uncover disease-related correlation patterns. This graph foundation model is then used to classify brain networks to obtain semantic labels. Additionally, by integrating the potential biomarkers with relevant textual matched through large language model, a multi-modal medical corpus can be constructed. Specifically, MLGSL first conducts a functional brain network link prediction task at the individual level, and a population-level link prediction task on the population-associated network. In the encoder part of pretraining task, the proposed multi-channel enhanced attention graph convolution strengthens the attention mechanism via connection strength, while integrating visible node representations learned from different perspectives. Subsequently, MLGSL performs category prediction of functional brain networks through fine-tuning. Experiments demonstrate that MLGSL achieves optimal brain network classification performance, thereby enabling the acquisition of accurate semantic labels. By combining brain imaging data with key textual information, a multi-modal medical corpus can be constructed, providing important value for interpretation of pathological information and clinical diagnosis of diseases.},
  archive      = {J_PR},
  author       = {Yuping Lin and Jingxi Feng and Xudong Chen and Rundong Xue and Jue Jiang and Zhiqiang Tian and Juan Wang},
  doi          = {10.1016/j.patcog.2025.112113},
  journal      = {Pattern Recognition},
  month        = {3},
  pages        = {112113},
  shortjournal = {Pattern Recognition},
  title        = {Multi-level graph self-supervised learning for multi-modal medical corpus construction},
  volume       = {171},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Multi resolution based local adaptive guided filter for fast large scale image denoising. <em>PR</em>, <em>171</em>, 112110. (<a href='https://doi.org/10.1016/j.patcog.2025.112110'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, the image acquired by camera devices usually has millions of pixels, but most traditional denoising methods mainly focus on improving the quality of denoising results and not able to remove noise from high resolution image in a reasonable time. For an efficient and fast large scale RGB image denoising method, we propose multi-resolution based adaptive Guided Filter in this work. First we analyse the performance of Guided Filter in denoising task and indicate that using the noisy image to interpolate its guidance image may improve the PSNR of denoising result under certain conditions. Then we propose the estimation scheme for local noise intensity in RGB image and obtain ancillary image through weighted sum across the color channels. The overall denoising process is conducted under multiple resolutions and consists of two stages. The ancillary image is denoised in stage one and used as guidance to remove noise in RGB image in stage two. Under each resolution, in ancillary image denoising, the developed method uses the interpolation between noisy image and up-sampling result of lower resolution denoised image as the guidance to perform Guided Filter, while in RGB image denoising, the interpolation is performed on the image to be filtered instead of its guidance. Numerical experiments on mainstream denoising datasets show that the proposed method is a hundred times faster than NLH and even faster than deep learning based methods which performed on GPU, while the PSNR of overall denoising results is only slightly lower.},
  archive      = {J_PR},
  author       = {Yuanmin Wang and Jinsong Leng},
  doi          = {10.1016/j.patcog.2025.112110},
  journal      = {Pattern Recognition},
  month        = {3},
  pages        = {112110},
  shortjournal = {Pattern Recognition},
  title        = {Multi resolution based local adaptive guided filter for fast large scale image denoising},
  volume       = {171},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Robust unsupervised visual tracking via image-to-video identity knowledge transferring. <em>PR</em>, <em>171</em>, 112109. (<a href='https://doi.org/10.1016/j.patcog.2025.112109'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Unsupervised Visual Tracking (UVT), categorized as identity-agnostic task, facilitates visual tracking in open-world scenarios without the need for extensive bounding box annotations. Existing methods, however, are limited by their sensitivity to initial target template, relying on costly auxiliary information. In this paper, we explore the potential of readily available identity knowledge in improving the consistency of attention for target templates with ambiguous meanings. We introduce A utonomous U nsupervise D I dentity T racking (AUDI-T), a novel approach that autonomously clarifies target identity, reducing reliance on target template initialization. AUDI-T employs a two-stage unsupervised learning framework that transfers identity knowledge from still images to key video frames, and subsequently to neighboring frames, ensuring a consistent identity across all frames. Extensive experiments on seven benchmark datasets demonstrate that AUDI-T significantly surpasses state-of-the-art UVT methods, achieving more than 10 % improvement in precision on the large-scale TrackingNet dataset. Moreover, our framework is adaptable to various backbones and training strategies, underscoring its wide applicability in visual tracking.},
  archive      = {J_PR},
  author       = {Bin Kang and Zongyu Wang and Dong Liang and Tianyu Ding and Songlin Du},
  doi          = {10.1016/j.patcog.2025.112109},
  journal      = {Pattern Recognition},
  month        = {3},
  pages        = {112109},
  shortjournal = {Pattern Recognition},
  title        = {Robust unsupervised visual tracking via image-to-video identity knowledge transferring},
  volume       = {171},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Recognition of near-duplicate periodic patterns by continuous metrics with approximation guarantees. <em>PR</em>, <em>171</em>, 112108. (<a href='https://doi.org/10.1016/j.patcog.2025.112108'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper rigorously solves the challenging problem of recognizing periodic patterns under rigid motion in Euclidean geometry. The 3-dimensional case is practically important for justifying the novelty of solid crystalline materials (periodic crystals) and for patenting medical drugs in a solid tablet form. Past descriptors based on finite subsets fail when a unit cell of a periodic pattern discontinuously changes under almost any perturbation of atoms, which is inevitable due to noise and atomic vibrations. The major problem is not only to find complete invariants (descriptors with no false negatives and no false positives for all periodic patterns) but to design efficient algorithms for distance metrics on these invariants that should continuously behave under noise. The proposed continuous metrics solve this problem in any Euclidean dimension and are algorithmically approximated with small error factors in times that are explicitly bounded in the size and complexity of a given pattern. The proved Lipschitz continuity allows us to confirm all near-duplicates filtered by simpler invariants in major databases of experimental and simulated crystals. This practical detection of noisy duplicates will stop the artificial generation of ‘new’ materials from slight perturbations of known crystals. Several such duplicates are under investigation by five journals for data integrity.},
  archive      = {J_PR},
  author       = {Olga D. Anosova and Daniel E. Widdowson and Vitaliy A. Kurlin},
  doi          = {10.1016/j.patcog.2025.112108},
  journal      = {Pattern Recognition},
  month        = {3},
  pages        = {112108},
  shortjournal = {Pattern Recognition},
  title        = {Recognition of near-duplicate periodic patterns by continuous metrics with approximation guarantees},
  volume       = {171},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Coarse-to-fine crack cue for robust crack detection. <em>PR</em>, <em>171</em>, 112107. (<a href='https://doi.org/10.1016/j.patcog.2025.112107'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Crack detection is an important task in computer vision. Despite impressive in-dataset performance, deep learning-based methods still struggle in generalizing to unseen domains. The thin structure property of cracks is usually overlooked by previous methods. In this work, we introduce CrackCue, a novel method for robust crack detection based on coarse-to-fine crack cue generation. The core concept lies on leveraging the thin structure property to generate a robust crack cue, guiding the crack detection. Specifically, we first employ a simple max-pooling and upsampling operation on the crack image. This results in a coarse crack-free background, based on which a fine crack-free background can be obtained via a reconstruction network. The difference between the original image and fine crack-free background provides a fine crack cue. This fine cue embeds robust crack prior information which is unaffected by complex backgrounds, shadow, and varied lighting. As a plug-and-play method, we incorporate the proposed CrackCue into three advanced crack detection networks. Extensive experimental results demonstrate that the proposed CrackCue significantly improves the generalization ability and robustness of the baseline methods. The source code will be publicly available.},
  archive      = {J_PR},
  author       = {Zelong Liu and Yuliang Gu and Zhichao Sun and Huachao Zhu and Xin Xiao and Bo Du and Laurent Najman and Yongchao Xu},
  doi          = {10.1016/j.patcog.2025.112107},
  journal      = {Pattern Recognition},
  month        = {3},
  pages        = {112107},
  shortjournal = {Pattern Recognition},
  title        = {Coarse-to-fine crack cue for robust crack detection},
  volume       = {171},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Artwork protection against unauthorized neural style transfer and aesthetic color distance metric. <em>PR</em>, <em>171</em>, 112105. (<a href='https://doi.org/10.1016/j.patcog.2025.112105'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Neural style transfer (NST) generates new images by combining the style of one image with the content of another. However, unauthorized NST can exploit artwork, raising concerns about artists’ rights and motivating the development of proactive protection methods. We propose Locally Adaptive Adversarial Color Attack (LAACA), enabling artists to conveniently protect their work from unauthorized NST by pre-processing the artwork image before public release, providing content-independent protection regardless of which content image it may later be combined with. LAACA introduces adaptive perturbations that significantly degrade NST quality while maintaining the visual integrity of the original image. We also develope LAACAv2, which resists the current SOTA adversarial perturbation removal method — SDEdit-based adversarial purification. Additionally, we introduce the Aesthetic Color Distance Metric (ACDM) to better evaluate color-sensitive tasks like NST. Extensive experiments across various NST techniques demonstrate our methods outperform baselines in structural similarity, color preservation, and perceptual quality. User studies with both general users and art experts confirm the practical applicability of our approach, addressing the social trust crisis in the art community while advancing adversarial machine learning at the intersection of art, technology, and intellectual property rights.},
  archive      = {J_PR},
  author       = {Zhongliang Guo and Yifei Qian and Shuai Zhao and Junhao Dong and Yanli Li and Ognjen Arandjelović and Lei Fang and Chun Pong Lau},
  doi          = {10.1016/j.patcog.2025.112105},
  journal      = {Pattern Recognition},
  month        = {3},
  pages        = {112105},
  shortjournal = {Pattern Recognition},
  title        = {Artwork protection against unauthorized neural style transfer and aesthetic color distance metric},
  volume       = {171},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Neural recommendation by user-item-user and item-user-item relation modeling. <em>PR</em>, <em>171</em>, 112104. (<a href='https://doi.org/10.1016/j.patcog.2025.112104'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Relational recommendation leverages multiple connections: user-item, item-item, and user-user. Existing works are gradually shifting from direct interaction and static representation to indirect interaction and dynamic representation. In this paper, we propose a neural recommendation method with dynamic embeddings for both users and items by deep user-item-user and item-user-item attribute relation modeling (DeepRelRec). The dynamic user embedding considers the target user-item pair by constructing user groups that have rated the item. The dynamic item embedding also considers the pair by constructing item groups that have been interacted with by the user. The two kinds of groups are identified by the attribute relations. Finally, we train the two dynamic embeddings above jointly to obtain the prediction. Experiments are undertaken on three real-world recommendation datasets in comparison with eleven baselines. Results show that DeepRelRec has improved Recall, Precision, mean reciprocal rank (MRR), and normalized discounted cumulative gain (NDCG) by at least 5.19 %, 0.58 %, 12.94 %, and 2.73 %, respectively.},
  archive      = {J_PR},
  author       = {Yuan-Yuan Xu and Hui Xiao and Heng-Ru Zhang and Dan-Dong Wang and Kun Wang and Fan Min},
  doi          = {10.1016/j.patcog.2025.112104},
  journal      = {Pattern Recognition},
  month        = {3},
  pages        = {112104},
  shortjournal = {Pattern Recognition},
  title        = {Neural recommendation by user-item-user and item-user-item relation modeling},
  volume       = {171},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). LESOD: Lightweight and efficient network for RGB-D salient object detection. <em>PR</em>, <em>171</em>, 112103. (<a href='https://doi.org/10.1016/j.patcog.2025.112103'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, some RGB-Depth salient object detection (SOD) methods pursuing superior detection performance, often come with a large number of parameters and high computational costs. This restricts their application in devices with limited computational resources. While existing lightweight RGB-D SOD networks possess fewer parameters, they tend to exhibit relatively modest detection performance. To address this issue, we are focused on designing a lightweight RGB-D SOD network that maintains high detection performance while minimizing the number of parameters. To this end, we propose a lightweight and efficient network for RGB-D SOD, termed LESOD. Considering the significance of cross-modal information interaction in RGB-D SOD, we introduce the cross-modal integration module (CIM) to fuse semantic relationships between RGB and depth images. Furthermore, to effectively reduce the interference from the background and accurately decode distinct saliency maps from the multi-dimensional fused feature maps, we integrate high-level semantic information with low-level detail features and construct the multi-level feature enhancement module (MFEM). The LESOD has 2.9M parameters, a computational complexity of 3.3 GFLOPs, an inference speed of 101.2 FPS on GPU, and 16.4 FPS on CPU. Experimental results on 7 datasets show that LESOD obtains superior detection performance compared to 15 state-of-the-art RGB-D SOD methods. The codes and results can be accessed at https://github.com/mingyu6346/LESOD .},
  archive      = {J_PR},
  author       = {Mingyu Zhong and Jing Sun and Fasheng Wang and Fuming Sun},
  doi          = {10.1016/j.patcog.2025.112103},
  journal      = {Pattern Recognition},
  month        = {3},
  pages        = {112103},
  shortjournal = {Pattern Recognition},
  title        = {LESOD: Lightweight and efficient network for RGB-D salient object detection},
  volume       = {171},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). High–performance grasp pose detection via point cloud serialization attention. <em>PR</em>, <em>171</em>, 112099. (<a href='https://doi.org/10.1016/j.patcog.2025.112099'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Detecting 6 degrees of freedom (6-DoF) grasp poses based on computer vision technology is an effective solution for automatically grasping arbitrary objects. Current 6-DoF grasp pose detection methods generally directly utilize existing point cloud feature extraction backbones. The focus of current approaches is primarily on designing more effective grasp pose parameter detection heads or tricks to predict 6-DoF grasp poses from the extracted features, often overlooking the critical role of the backbone network in achieving high-precision grasp pose detection. To address this, we propose an end-to-end point cloud serialized attention-based 6-DoF grasp pose detection (PSAGrasp) algorithm, which leverages attention mechanism to optimize the point cloud feature extraction process, thereby further improving the accuracy of grasp pose detection. Specifically, we serialize the unordered point cloud using space-filling curve and design a multi-encoder and -decoder cascaded serialized point cloud feature extraction backbone network based on the Transformer architecture to effectively extract point cloud features. On this basis, we propose a grasp pose detection head specifically designed for 6-DoF grasp pose detection. Furthermore, to ensure that the predicted grasp poses conform to real-world physical principle, we integrate a forced constraint based on the antipodal grasping rule into the predicted poses. The experimental results demonstrate that the detection accuracy of our algorithm has significantly improved on the GraspNet-1Billion benchmark, achieving the state-of-the-art performance. In addition, our algorithm achieved an 87.88 % grasp success rate on unseen objects in real-world Franka robot experiments. Our code is available at: https://github.com/upc-ghy/PSAGrasp .},
  archive      = {J_PR},
  author       = {Haiyuan Gui and Shanchen Pang and Xiao He and Teng Wang and Sibo Qiao and Luqi Wang and Shihang Yu},
  doi          = {10.1016/j.patcog.2025.112099},
  journal      = {Pattern Recognition},
  month        = {3},
  pages        = {112099},
  shortjournal = {Pattern Recognition},
  title        = {High–performance grasp pose detection via point cloud serialization attention},
  volume       = {171},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). AttDNet: An attribute-driven deep learning network for visual emotion analysis. <em>PR</em>, <em>171</em>, 112098. (<a href='https://doi.org/10.1016/j.patcog.2025.112098'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the rapid development of multimedia technology, more and more people tend to express their views through images on the Internet. Therefore, visual emotion analysis (VEA) has attracted significant attention from scholars all over the world. However, emotion is a highly subjective concept that can be influenced by various attributes of the image, such as color, brightness, faces, objects, scenes and so on. These attributes are associated with the process of human cognitive emotion and they are present not only in the global features, but also in the local features. However, most existing methods rely solely on global features for classification. Some methods focus on shallow local features, but the obtained information is not comprehensive enough. In order to comprehensively extract the emotion-related attribute information in images, we propose an Attribute-Driven Deep Learning Network (AttDNet). This network comprises an attribute branch with multiple modules and a facial branch that can obtain facial attribute. The modules in the attribute branch focus on various attribute information in the image. Among them, CSCAM module can extract color information and key objects in the image, LEconv module can extract context information of objects in images, and MLPooling module can extract scene information in images. In addition, to bring samples of the same emotional category closer together, we use CenterLoss to constraint the model. The experimental results demonstrate that the proposed method achieves state-of-the-art accuracy on public datasets.},
  archive      = {J_PR},
  author       = {Xiumei Wang and Yanzong Cheng and Peitao Cheng and Xi Zhang},
  doi          = {10.1016/j.patcog.2025.112098},
  journal      = {Pattern Recognition},
  month        = {3},
  pages        = {112098},
  shortjournal = {Pattern Recognition},
  title        = {AttDNet: An attribute-driven deep learning network for visual emotion analysis},
  volume       = {171},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Probabilistic smooth attention for deep multiple instance learning in medical imaging. <em>PR</em>, <em>171</em>, 112097. (<a href='https://doi.org/10.1016/j.patcog.2025.112097'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Multiple Instance Learning (MIL) paradigm is attracting plenty of attention in medical imaging classification, where labeled data is scarce. MIL methods cast medical images as bags of instances (e.g. patches in whole slide images, or slices in CT scans), and only bag labels are required for training. Deep MIL approaches have obtained promising results by aggregating instance-level representations via an attention mechanism to compute the bag-level prediction. These methods typically capture both local interactions among adjacent instances and global, long-range dependencies through various mechanisms. However, they treat attention values deterministically, potentially overlooking uncertainty in the contribution of individual instances. In this work we propose a novel probabilistic framework that estimates a probability distribution over the attention values, and accounts for both global and local interactions. In a comprehensive evaluation involving eleven state-of-the-art baselines and three medical datasets, we show that our approach achieves top predictive performance in different metrics. Moreover, the probabilistic treatment of the attention provides uncertainty maps that are interpretable in terms of illness localization.},
  archive      = {J_PR},
  author       = {Francisco M. Castro-Macías and Pablo Morales-Álvarez and Yunan Wu and Rafael Molina and Aggelos K. Katsaggelos},
  doi          = {10.1016/j.patcog.2025.112097},
  journal      = {Pattern Recognition},
  month        = {3},
  pages        = {112097},
  shortjournal = {Pattern Recognition},
  title        = {Probabilistic smooth attention for deep multiple instance learning in medical imaging},
  volume       = {171},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Learning multi-agent communication via graph contrastive learning. <em>PR</em>, <em>171</em>, 112093. (<a href='https://doi.org/10.1016/j.patcog.2025.112093'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, communication learning has emerged as a successful tool for tackling complex tasks in multi-agent reinforcement learning (MARL). Many MARL methods use graph neural networks (GNNs) to build a communication learning framework where agents and communication channels can be represented as nodes and edges in a graph. However, most GNN-based MARL methods simply aggregate features of neighboring agents to obtain message representations, which may not extract enough useful information. To tackle this problem, this paper investigates how to extract expressive information from neighboring agents to obtain high-quality message representations. Inspired by the recent success of contrastive learning methods, in this paper, we propose a m ulti- a gent communication protocol via g raph contrastiv e learning (MAGE), which utilizes contrast objectives to learn optimal message representations, considering feature and topological level. At the feature level, we corrupt agent features by adding more noise to insignificant neighboring agent features, to encourage the agent to recognize significant information. At the topological level, we adaptively remove edges by assigning larger removal probabilities to insignificant edges to highlight significant communication structures. Experiments across diverse benchmarks confirm that MAGE outperforms existing GNN-based MARL methods.},
  archive      = {J_PR},
  author       = {Wei Du and Zhengfan Chen and Shifei Ding and Chenglong Zhang and Wei Guo and Yuqing Sun and Guoxian Yu and Lizhen Cui},
  doi          = {10.1016/j.patcog.2025.112093},
  journal      = {Pattern Recognition},
  month        = {3},
  pages        = {112093},
  shortjournal = {Pattern Recognition},
  title        = {Learning multi-agent communication via graph contrastive learning},
  volume       = {171},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). TransVFC: A transformable video feature compression framework for machines. <em>PR</em>, <em>171</em>, 112091. (<a href='https://doi.org/10.1016/j.patcog.2025.112091'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Currently, an increasing number of video transmissions are focusing primarily on downstream machine vision tasks rather than on human vision. While the widely deployed human visual system (HVS)-oriented video coding standards such as H.265/HEVC and H.264/AVC are efficient, they are not the optimal approaches for video coding for machines (VCM) scenarios, leading to unnecessary bitrate expenditures. Academic and technical explorations within the VCM domain have led to the development of several strategies; however, conspicuous limitations remain in their adaptability to multitask scenarios. To address this challenge, we propose a Transformable Video Feature Compression (TransVFC) framework. It offers a compress-then-transfer solution and includes a video feature codec and feature space transform (FST) modules. In particular, the temporal redundancy of video features is squeezed by the codec through a scheme-based inter-prediction module. Then, the codec implements perception-guided conditional coding to minimize spatial redundancy and help the reconstructed features align with the downstream machine perception process. Subsequently, the reconstructed features are transferred to new feature spaces for diverse downstream tasks by the FST modules. To accommodate a new downstream task, only one lightweight FST module needs to be trained, avoiding the need to retrain and redeploy the upstream codec and downstream task networks. Experiments show that TransVFC achieves high rate-task performance for diverse tasks at different granularities. We expect our work to provide valuable insights for video feature compression in multitask scenarios. The codes are available at https://github.com/Ws-Syx/TransVFC .},
  archive      = {J_PR},
  author       = {Yuxiao Sun and Yao Zhao and Meiqin Liu and Chao Yao and Huihui Bai and Chunyu Lin and Weisi Lin},
  doi          = {10.1016/j.patcog.2025.112091},
  journal      = {Pattern Recognition},
  month        = {3},
  pages        = {112091},
  shortjournal = {Pattern Recognition},
  title        = {TransVFC: A transformable video feature compression framework for machines},
  volume       = {171},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). LoRA-based continual learning with constraints on critical parameter changes. <em>PR</em>, <em>171</em>, 112086. (<a href='https://doi.org/10.1016/j.patcog.2025.112086'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {LoRA-based continual learning represents a promising avenue for leveraging pre-trained models in downstream continual learning tasks. Recent studies have shown that orthogonal LoRA tuning effectively mitigates forgetting. However, this work unveils that under orthogonal LoRA tuning, the critical parameters for pre-tasks still change notably after learning post-tasks. To address this problem, we directly propose freezing the most critical parameter matrices in the Vision Transformer (ViT) for pre-tasks before learning post-tasks. In addition, building on orthogonal LoRA tuning, we propose orthogonal LoRA composition (LoRAC) based on QR decomposition, which may further enhance the plasticity of our method. Elaborate ablation studies and extensive comparisons demonstrate the effectiveness of our proposed method. Our results indicate that our method achieves state-of-the-art (SOTA) performance on several well-known continual learning benchmarks. For instance, on the Split CIFAR-100 dataset, our method shows a 6.35 % improvement in accuracy and a 3.24 % reduction in forgetting compared to previous methods. Our code is available at https://github.com/learninginvision/LoRAC-IPC .},
  archive      = {J_PR},
  author       = {Shimou Ling and Liang Zhang and Jiangwei Zhao and Lili Pan and Hongliang Li},
  doi          = {10.1016/j.patcog.2025.112086},
  journal      = {Pattern Recognition},
  month        = {3},
  pages        = {112086},
  shortjournal = {Pattern Recognition},
  title        = {LoRA-based continual learning with constraints on critical parameter changes},
  volume       = {171},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). A unified gradient-flow-based GAN framework with diffusion condition guided. <em>PR</em>, <em>171</em>, 112083. (<a href='https://doi.org/10.1016/j.patcog.2025.112083'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper presents a novel Diffusion Condition Guided Unified Gradient-Flow-Based GAN Framework model. Recently, researchers have proposed many deep generative models, including generative adversarial networks (GAN) and denoising diffusion models. Different from the common GAN model with empirical success but less theoretical explanation, gradient-flow-based GAN models offer better theoretical explanations, but they exhibit worse empirical results. To address this problem, we propose the Diffusion Condition Guided U nified G radient Flow Based GAN model, termed UGGAN, which incorporates a diffusion process as the condition with various gradient-flow-based GAN models. Crucially, we provide a theoretical foundation for formulating the UGGAN, ensuring it maintains the same convergence as the original gradient-flow-based GAN. Furthermore, we propose self-supervised conditions that utilize the accumulated gradient flow F m as the condition for the gradient-flow-based GAN generator, significantly influencing the diversity of synthesized samples. Additionally, we proposed that the diffusion condition can work as an early stop regularization for the gradient-flow-based model. Finally, we demonstrate the algorithms for the UGGAN, which contains various types of gradient-flow-based GAN models. Thanks to the diffusion condition, UGGAN surpasses the limitations of the current gradient-flow-based GAN and can generate diverse and high-fidelity samples. In our extensive experiments on various benchmark datasets for image generation, the UGGAN method has shown significant improvement in both the quality and diversity of the synthesized samples compared to the original gradient-flow-based GAN. Additionally, our UGGAN has produced competitive synthesis samples when compared to state-of-the-art generative models.},
  archive      = {J_PR},
  author       = {Chang Wan and Yanwei Fu and Minglu Li and Jungang Lou and Liyuan Chen and Zhonglong Zheng},
  doi          = {10.1016/j.patcog.2025.112083},
  journal      = {Pattern Recognition},
  month        = {3},
  pages        = {112083},
  shortjournal = {Pattern Recognition},
  title        = {A unified gradient-flow-based GAN framework with diffusion condition guided},
  volume       = {171},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). All-in-one adverse weather removal via dual state space-based diffusion model with degradation-aware guidance. <em>PR</em>, <em>171</em>, 112081. (<a href='https://doi.org/10.1016/j.patcog.2025.112081'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In real-world scenarios, imaging devices are inevitably affected by adverse weather conditions, which degrade the performance of downstream vision tasks. Although diffusion models are effective in removing adverse weather effects, their restoration performance is hindered by limitations in the noise estimation network and insufficient utilization of degradation-specific information. Recently, State Space Models (SSMs) have outperformed Transformers in both performance and efficiency. However, their scanning strategies fail to ensure spatial continuity and overlook cross-channel interactions. In this paper, we propose a new all-in-one adverse weather removal framework, named Weather-DiffMamba. Specifically, we design a Degradation-Aware Mamba to extract degradation-aware features, which are converted into prompts to guide the restoration process. Additionally, we propose a spatial and channel-based scanning strategy with 10 scanning directions to enhance the ability to capture long-range dependencies and cross-channel interactions. Extensive experiments validate the superior performance of our method over multiple state-of-the-art (SOTA) methods in all-in-one adverse weather removal, as well as its effectiveness in real-world scenarios.},
  archive      = {J_PR},
  author       = {Dirui Xie and Xiaofang Hu and Yue Zhou and Shukai Duan},
  doi          = {10.1016/j.patcog.2025.112081},
  journal      = {Pattern Recognition},
  month        = {3},
  pages        = {112081},
  shortjournal = {Pattern Recognition},
  title        = {All-in-one adverse weather removal via dual state space-based diffusion model with degradation-aware guidance},
  volume       = {171},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Weakly supervised crowd counting with joint CNN and transformer network. <em>PR</em>, <em>171</em>, 112077. (<a href='https://doi.org/10.1016/j.patcog.2025.112077'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Currently, fully supervised methods based on density map estimation are the primary research directions for crowd counting. However, such methods require the location-level annotations of persons in an image, which is time-consuming and laborious. Therefore, a weakly supervised method relying only on count-level annotations is urgently required. As a CNN is not suitable for modelling the global context and interactions between image patches, crowd counting with weakly supervised learning via a CNN generally does not provide good performance. A weakly supervised model using Transformer was sequentially proposed to model the global context and learn the contrast features. However, the Transformer directly partitions crowd images into a series of tokens, which may not be a good choice because each pedestrian is an independent individual, and the number of parameters of the network is very large. Hence, in this study, we propose a weakly supervised crowd counting method using Joint CNN and Transformer Network (JCTNet). JCTNet consists of three parts: a CNN feature extraction module (CFM), a Transformer feature extraction module (TFM), and a counting regression module (CRM). In particular, the CFM extracts crowd semantic information features and then sends their patch partitions to the TFM to model the global context. The CRM is used to predict the number of people. Extensive experiments and visualisations demonstrate that JCTNet can effectively focus on crowded regions and obtain superior performance in weakly supervised counting on five mainstream datasets, i.e., ShanghaiTech Parts A and B, UCF-QNRF, UCF-CC-50, and NWPU-Crowd. The number of parameters of the model can be reduced by approximately 67–73 % compared with that of a pure Transformer network. We also attempted to explain the phenomenon in which a model constrained only by count-level annotations can focus on crowded regions. We believe that our study can promote further research on weakly supervised crowd counting as well as any object counting. The code is available at https://github.com/wfs123456/JCTnet .},
  archive      = {J_PR},
  author       = {Fusen Wang and Kai Liu and Ning Wei and Nong Sang and Xiaofeng Xia and Jun Sang},
  doi          = {10.1016/j.patcog.2025.112077},
  journal      = {Pattern Recognition},
  month        = {3},
  pages        = {112077},
  shortjournal = {Pattern Recognition},
  title        = {Weakly supervised crowd counting with joint CNN and transformer network},
  volume       = {171},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Unified image restoration and enhancement: Degradation calibrated cycle reconstruction diffusion model. <em>PR</em>, <em>171</em>, 112073. (<a href='https://doi.org/10.1016/j.patcog.2025.112073'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Image restoration and enhancement are pivotal for numerous computer vision applications; however, efficiently unifying these tasks remains a significant challenge. Inspired by the iterative refinement capabilities of diffusion models, we propose CycleRDM, a novel framework designed to unify restoration and enhancement tasks while achieving high-quality mapping. Specifically, CycleRDM first learns the mapping relationships among the degraded domain, the rough normal domain, and the normal domain through a two-stage diffusion inference process. We subsequently transfer the final calibration process to the wavelet low-frequency domain via the discrete wavelet transform, performing fine-grained calibration from a frequency domain perspective by leveraging task-specific frequency spaces. To improve the restoration quality, we design a feature gain module for the decomposed wavelet high-frequency domain to eliminate redundant features. Additionally, we employ multimodal textual prompts and the Fourier transform to drive stable denoising and reduce randomness during the inference process. After extensive validation, CycleRDM can be effectively generalized to a wide range of image restoration and enhancement tasks while requiring only a small number of training samples to be significantly superior on various benchmarks of reconstruction quality and perceptual quality. The source code is available at https://github.com/hejh8/CycleRDM .},
  archive      = {J_PR},
  author       = {Minglong Xue and Jinhong He and Shivakumara Palaiahnakote and Mingliang Zhou},
  doi          = {10.1016/j.patcog.2025.112073},
  journal      = {Pattern Recognition},
  month        = {3},
  pages        = {112073},
  shortjournal = {Pattern Recognition},
  title        = {Unified image restoration and enhancement: Degradation calibrated cycle reconstruction diffusion model},
  volume       = {171},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). RSPH: Robust self-paced hashing for cross-modal retrieval. <em>PR</em>, <em>171</em>, 112072. (<a href='https://doi.org/10.1016/j.patcog.2025.112072'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, hashing technology has attracted considerable attention in cross-modal retrieval due to its high retrieval efficiency and low storage overhead. Therefore, many cross-modal hashing (CMH) methods have been proposed and performed well. However, most existing CMH methods treat all samples equally without discrimination, which may lead to suboptimal local minima. Besides, most existing approaches usually leverage modality-specific information when learning hashing functions, which cannot effectively capture both intra-and inter-modality correlations in multimodal data. To mitigate these problems, a novel Robust Self-Paced hashing (RSPH) approach is developed for cross-modal retrieval. Concretely, in the proposed RSPH, the multimodal data are progressively integrated into the hashing learning process from easy to hard, helping to avoid poor local minima. Besides, the ℓ 2 , 1 -norm is employed in the proposed method to further reduce the impact of outliers and noise. To fully exploit multimodal information, this method introduces a collaboration projection scheme that captures both modality-specific properties and cross-modal correlations, resulting in more effective hash functions. Experimental results on three benchmark datasets validate our RSPH can achieve state-of-the-art performance, demonstrating its efficacy.},
  archive      = {J_PR},
  author       = {Donglin Zhang and Zhikai Hu and Xiao-Jun Wu and Josef Kittler},
  doi          = {10.1016/j.patcog.2025.112072},
  journal      = {Pattern Recognition},
  month        = {3},
  pages        = {112072},
  shortjournal = {Pattern Recognition},
  title        = {RSPH: Robust self-paced hashing for cross-modal retrieval},
  volume       = {171},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). 3D portrait stylization with adaptive semantic editing based on GAN latent codes. <em>PR</em>, <em>171</em>, 112069. (<a href='https://doi.org/10.1016/j.patcog.2025.112069'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Stylized 3D portrait generation faces two key challenges: preserving facial identity during artistic transformations and enabling precise semantic control over edited outputs. While existing methods based on 3D GANs can adapt to new styles through fine-tuning, they often suffer from geometric distortions and excessive texture oversmoothing, particularly in cartoon styles requiring exaggerated features. To address these limitations, we propose a novel 3D portrait stylization framework that decomposes the task into three independent subproblems: controlled generation, geometric stylization, and texture stylization. By leveraging GAN latent codes as conditional inputs, our approach simultaneously achieves multi-style generation and attribute manipulation without compromising geometric or textural fidelity. Extensive experiments on paired multi-style face datasets, constructed from benchmark methods, demonstrate the consistent superiority of our method consistently outperforms state-of-the-art methods in terms of visual quality and editing accuracy.},
  archive      = {J_PR},
  author       = {Yantao Song and Xiangchong Jia and Jieru Jia and Yudong Liang and Xinyan Liang},
  doi          = {10.1016/j.patcog.2025.112069},
  journal      = {Pattern Recognition},
  month        = {3},
  pages        = {112069},
  shortjournal = {Pattern Recognition},
  title        = {3D portrait stylization with adaptive semantic editing based on GAN latent codes},
  volume       = {171},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Bayes-optimal minimax probability machines. <em>PR</em>, <em>171</em>, 112068. (<a href='https://doi.org/10.1016/j.patcog.2025.112068'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Minimax Probability Machine (MPM) model, a robust machine learning method, has been proposed to address the stochastic nature of data distributions by minimizing the upper bound on classification error probabilities. However, the worst-case scenario assumption in traditional MPM models may not be ideal for datasets that more closely follow a multivariate Gaussian distribution. This paper seeks to bridge this gap by introducing Gaussian adaptations to two MPM variants: the Robust Maximum Margin Classifier (RMMC) and the Cobb–Douglas Learning Machine (CD-LeMa), resulting in three novel methodologies that function as optimal Bayes classifiers. An extensive empirical comparison of various MPM variants, considering both robust and Gaussian cases, provides valuable insights into their predictive capabilities. The proposed Gaussian versions of the MPM models achieve the best performance in 71% of cases, with the RMMC variant emerging as the top approach (an average rank of 2.32 among 10 classifiers, with 1 being the top-ranked method on a given dataset). The Gaussian variants excel primarily in medium-sized datasets (1000 samples or more) and, for the MEMPM, RMMC, and CD-LeMa methods, perform better than their robust counterparts in 90% of cases.},
  archive      = {J_PR},
  author       = {Sebastián Maldonado and Julio López and Miguel Carrasco and Paul Bosch},
  doi          = {10.1016/j.patcog.2025.112068},
  journal      = {Pattern Recognition},
  month        = {3},
  pages        = {112068},
  shortjournal = {Pattern Recognition},
  title        = {Bayes-optimal minimax probability machines},
  volume       = {171},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Twin learning for domain agnostic time series analysis: A regime-switch approach. <em>PR</em>, <em>171</em>, 112065. (<a href='https://doi.org/10.1016/j.patcog.2025.112065'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Correlations among variables in complex ecosystems such as weather systems and financial markets result in large amounts of dynamic and co-evolving time sequences. The benefits of discovering and predicting intricate patterns (aka regimes) in time sequences are multifold, including better understanding of the ecosystem dynamics, optimizing model selection, and improving interpretability of results. Despite recent advancements, existing methods primarily emphasize predictive accuracy, which might overshadow the need to comprehend the structural dynamics within the series. Additionally, these methods often encounter one or more of the following limitations: (1) difficulty in identifying regimes within domain-dependent segmentations; (2) inability to integrate nonlinear relationships across time series; (3) lack of an effective method to encapsulate the temporal behaviors. To tackle these challenges, we introduce a twin learning regime-switch model to simultaneously learn domain-agnostic segmentation and regime switch in a principled way. Specifically, we devise a kernel-based method that determines the duration of regime and captures dynamic switches through potent representations, accounting for the non-linear interactions between series. With this model, it is feasible to automatically achieve the two subtasks of identifying the optimal regimes and determining the most suitable segmentation. Experimental results on synthetic and real-world datasets indicate that our method is capable of revealing the structures that underpin the behavior of co-evolving ecosystems, which display different dynamics. These structures can be leveraged to better define regimes with superior predictive capabilities compared to widely used traditional models and state-of-the-art neural network models.},
  archive      = {J_PR},
  author       = {Kunpeng Xu and Lifei Chen and Shengrui Wang},
  doi          = {10.1016/j.patcog.2025.112065},
  journal      = {Pattern Recognition},
  month        = {3},
  pages        = {112065},
  shortjournal = {Pattern Recognition},
  title        = {Twin learning for domain agnostic time series analysis: A regime-switch approach},
  volume       = {171},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Person image generation via regional style rectification. <em>PR</em>, <em>171</em>, 112062. (<a href='https://doi.org/10.1016/j.patcog.2025.112062'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Person image generation is a challenging task and has been used in many people centered applications. Current person image generation methods mainly focus on transferring the texture features of the source image to the target pose, but ignore the constraint effect of the source image on the generated results. In this paper, we propose a regional style rectification network for person image generation, which aims to use the style information of source images to improve the textures of the generated person images. Within the proposed model, we design a regional style compensation module to rectify the shape-independent style information of the generated image to the source by producing a regional style residual map. Furthermore, we propose a regional style consistency loss to strengthen the style rectification, which realizes the direct supervision of source images over target image generation, ignoring the shape differences between the source and target images. Besides, we use a dual-attention-based texture transformation module which exploits the correlations between source and target images to better preserve the texture features of source images. The experiment results for pose transfer and attribute editing demonstrate the effectiveness of the proposed model.},
  archive      = {J_PR},
  author       = {Guiyu Xia and Yun Liu and Zhedong Jin and Yubao Sun},
  doi          = {10.1016/j.patcog.2025.112062},
  journal      = {Pattern Recognition},
  month        = {3},
  pages        = {112062},
  shortjournal = {Pattern Recognition},
  title        = {Person image generation via regional style rectification},
  volume       = {171},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Information-coupled MRI acceleration via multi-modal mapping and progressive masking. <em>PR</em>, <em>171</em>, 112061. (<a href='https://doi.org/10.1016/j.patcog.2025.112061'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Due to the well-balanced interpretability and performance, Deep Unrolling Network (DUN) has emerged as a compelling paradigm in multi-modal MRI reconstruction. Nonetheless, current methodologies remain hampered by the challenge of information segregation, in which features from different modalities are learned in isolation. In addition, the prefetched masks remain disjoint from iterative model updates. To address these limitations, we introduce two innovative mechanisms. First, two auxiliary transformations are elaborated that explicitly capture intermodal dependencies: the shallow transformation consolidates spatial intricacies, while the high-dimensional counterpart delves into deep feature extraction through enriched channel representations. Second, based on a meticulous analysis of the non-uniform frequency distribution in the low- and high-frequency components of the mask, we design a new unrolling paradigm that leverages a progressive masking scheme, integrating a dilation–contraction mechanism to dynamically regulate learnable frequency regions during the forward propagation across successive stages. Comprehensive experiments, spanning diverse sampling patterns, acceleration rates, generalization tests, and ablation studies, demonstrate that our approach, without reliance on complex architectures, consistently surpasses state-of-the-art methods in both visual fidelity and quantitative metrics. The accompanying code is provided and will be publicly available.},
  archive      = {J_PR},
  author       = {Jiawei Jiang and Jiacheng Chen and Honghui Xu and Yuhang He and Jianwei Zheng},
  doi          = {10.1016/j.patcog.2025.112061},
  journal      = {Pattern Recognition},
  month        = {3},
  pages        = {112061},
  shortjournal = {Pattern Recognition},
  title        = {Information-coupled MRI acceleration via multi-modal mapping and progressive masking},
  volume       = {171},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). ABM: An automatic body measurement framework via body deformation and topology-aware B-spline approximation. <em>PR</em>, <em>171</em>, 112060. (<a href='https://doi.org/10.1016/j.patcog.2025.112060'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Accurate 3D human body measurement constitutes a critical foundation for applications ranging from virtual try-on to anthropometric analysis. Current methods are mainly divided into two categories: landmark-based detection and template-based deformation. While landmark-driven methods demonstrate efficiency, their measurement accuracy heavily relies on precise landmark localization that proves vulnerable to point cloud noise and missing data. Conversely, template-based approaches require computationally intensive iterative optimization to align parametric models with target scans, exhibiting similar sensitivity to data imperfections. To overcome these limitations, we propose ABM (Automatic Body Measurement) - an automatic framework integrating template-driven landmark detection with topology-aware B-spline approximation. ABM consists of three core components: (1) A deformation-based landmark detection module that establishes anatomical correspondence through non-rigid template registration, (2) A contour reconstruction pipeline employing concave envelope analysis to organize disordered point clouds into ordered measurement loops, and (3) A neural B-spline approximation module that automatically generates topology-compliant control points through learned geometric priors. Notably, our neural B-spline formulation enables continuous surface characterization while maintaining anthropometric validity, overcoming the discrete measurement limitations of conventional spline fitting. Our method achieves outstanding results on unordered point clouds of “A” pose and “T” pose, eliminating the dependence on parameterized human bodies.},
  archive      = {J_PR},
  author       = {Xin Ning and Limin Jiang and Liping Zhang and Tingran Wang and Yuhao Wang and Weijun Li and Pengjiang Qian},
  doi          = {10.1016/j.patcog.2025.112060},
  journal      = {Pattern Recognition},
  month        = {3},
  pages        = {112060},
  shortjournal = {Pattern Recognition},
  title        = {ABM: An automatic body measurement framework via body deformation and topology-aware B-spline approximation},
  volume       = {171},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). OBEPA: Object-embedding predictive alignment for semi-supervised object detection. <em>PR</em>, <em>171</em>, 112057. (<a href='https://doi.org/10.1016/j.patcog.2025.112057'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the rapid development of Semi-Supervised Object Detection (SSOD), the performance of object detectors has been greatly improved. Despite the promising results, the existing SSOD methods mainly focus on selecting optimal pseudo-labels or alleviating the negative impact of noisy pseudo-labels, and few works have taken attention to explore the effectiveness of embedding alignment technology for SSOD. In this paper, we propose the OBject-Embedding Predictive Alignment (OBEPA) for SSOD by introducing embedding predictor and object-embedding contrastive learning. The embedding predictor is performed by adding a small convolutional network in the student branch before embedding alignment, which can reduce the discrepancy in embeddings between the teacher and student branches. Object-embedding contrastive learning is applied by proposing a balanced hierarchical embedding clustering and an object-aware InfoNCE loss to enhance the discriminability of the embedding at the object level. Specifically, in the student branch and the teacher branch, we respectively group all pixel-level representations within one image into multiple clusters. The positive contrastive samples of each pixel embedding in the student branch come from the corresponding embedding in the teacher branch, while the negative contrastive samples are sampled from other embedding clusters. Evaluation of the proposed method on two SSOD benchmarks, including MS-COCO and Pascal VOC demonstrates its superiority against the previous state-of-the-art approaches.},
  archive      = {J_PR},
  author       = {Zunran Wang and Zhonghua Li and Zhijian Chen and Zhao Cao},
  doi          = {10.1016/j.patcog.2025.112057},
  journal      = {Pattern Recognition},
  month        = {3},
  pages        = {112057},
  shortjournal = {Pattern Recognition},
  title        = {OBEPA: Object-embedding predictive alignment for semi-supervised object detection},
  volume       = {171},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Time-series analysis of cellular shapes using transported velocity fields. <em>PR</em>, <em>171</em>, 112056. (<a href='https://doi.org/10.1016/j.patcog.2025.112056'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper presents a generative statistical model for analyzing time series of planar shapes. Using elastic shape analysis, we separate object kinematics (rigid motions and speed variability) from morphological evolution, representing the latter through transported velocity fields (TVFs). A principal component analysis (PCA) based dimensionality reduction of the TVF representation provides a finite-dimensional Euclidean framework, enabling traditional time-series analysis. We then fit a vector auto-regressive (VAR) model to the TVF-PCA time series, capturing the statistical dynamics of shape evolution. To characterize morphological changes, we use VAR model parameters for model comparison, synthesis, and sequence classification. Leveraging these parameters, along with machine learning classifiers, we achieve high classification accuracy. Extensive experiments on cell motility data validate our approach, demonstrating its effectiveness in modeling and classifying migrating cells based on morphological evolution—marking a novel contribution to the field.},
  archive      = {J_PR},
  author       = {Ximu Deng and Rituparna Sarkar and Elisabeth Labruyere and Jean-Christophe Olivo-Marin and Anuj Srivastava},
  doi          = {10.1016/j.patcog.2025.112056},
  journal      = {Pattern Recognition},
  month        = {3},
  pages        = {112056},
  shortjournal = {Pattern Recognition},
  title        = {Time-series analysis of cellular shapes using transported velocity fields},
  volume       = {171},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Visual and text prompts guided interpretable network for universal low-dose CT MAR. <em>PR</em>, <em>171</em>, 112052. (<a href='https://doi.org/10.1016/j.patcog.2025.112052'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Existing deep learning-based (DL-based) methods to tackle the task of simultaneous low-dose computed tomography (LDCT) reconstruction and metal artifact reduction (MAR), i.e. LDMAR, have achieved significant successes, but they still face the following limitations: ( i ) for multiple dose levels, previous one-by-one training strategy consumes storage cost, limiting their clinical applications. Universal models which train one model for multiple dose levels can save storage cost, but they lack specific prompt information, leading to suboptimal reconstructions; ( ii ) most model architectures are black-box, and ignore multi-scale information, limiting their interpretability and representation ability. To address these issues, we propose the so-called PMDeepST, a visual and text prompts guided multi-scale deep sparsifying transform network. Specifically, we design a multi-scale deep sparsifying transform architecture whose backbone network inherits interpretability of deep sparse representation model, and integrate a fusion module to exploit cross-scale complementarity. Most importantly, for adaptively shrinking representation coefficients on each scale, we propose a visual and text prompts guided shrinkage (ProS) subnetwork. ProS incorporates both visual and text prompts to guide training of the model, assisting to learn discriminant information between different noise levels. Extensive experiments demonstrate that the proposed universal method can allow a single model to accommodate various CT dose settings, and outperforms the state-of-the-art LDMAR methods.},
  archive      = {J_PR},
  author       = {Baoshun Shi and Bing Chen and Shaolei Zhang and Ke Jiang and Shuo Liu},
  doi          = {10.1016/j.patcog.2025.112052},
  journal      = {Pattern Recognition},
  month        = {3},
  pages        = {112052},
  shortjournal = {Pattern Recognition},
  title        = {Visual and text prompts guided interpretable network for universal low-dose CT MAR},
  volume       = {171},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). High-order aligned deep complementary and view-specific similarity graphs for unsupervised multi-view feature selection. <em>PR</em>, <em>171</em>, 112047. (<a href='https://doi.org/10.1016/j.patcog.2025.112047'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Current unsupervised multi-view feature selection methods exhibit critical limitations in capturing high-level semantics and modeling high-order consistency across views. These methods rely on low-level features derived from raw data or shallow nonnegative matrix factorization, often resulting in suboptimal data structure extraction. Moreover, conventional methods linearly align data views, ignoring high-order consistency. Although certain approaches employ low-rank tensor techniques to capture high-order consistency, they treat all data views as equally important and fail to fully exploit the complementary information of multi-view data. To address these issues, this paper proposes DCVSG (High-order Aligned D eep C omplementary and V iew-specific S imilarity G raphs for Unsupervised Multi-view Feature Selection). First, DCVSG employs deep semi-nonnegative matrix factorization to hierarchically extract view-specific semantic features, which are subsequently fused into deep complementary representations through intact latent space learning. Second, the model concurrently learns local similarity structures within the deep complementary view (represented by the deep complementary representations) and individual data views. To achieve high-order alignment between the deep complementary and data views, a view-weighted tensor nuclear norm, which dynamically assigns weights to different views, is further introduced. Extensive experiments conducted on seven benchmark datasets demonstrate the superiority of the proposed DCVSG. The code is available at: https://gitee.com/jshncu/feature-selection/tree/master/DCVSG .},
  archive      = {J_PR},
  author       = {Jian-Sheng Wu and Jia-Tao Yu and Jun-Yun Wu and Weidong Min and Wei-Shi Zheng},
  doi          = {10.1016/j.patcog.2025.112047},
  journal      = {Pattern Recognition},
  month        = {3},
  pages        = {112047},
  shortjournal = {Pattern Recognition},
  title        = {High-order aligned deep complementary and view-specific similarity graphs for unsupervised multi-view feature selection},
  volume       = {171},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Adaptive kernel subspace clustering with discrete group structure constraint. <em>PR</em>, <em>171</em>, 112045. (<a href='https://doi.org/10.1016/j.patcog.2025.112045'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Subspace clustering (SC) has emerged as a prominent research topic in computer vision and data mining. Traditional approaches, such as sparse subspace clustering (SSC) and low-rank representation (LRR), typically tackle the SC problem through a two-step process involving affinity matrix construction followed by spectral clustering, which often results in suboptimal clustering performance. To address these limitations, we propose a novel one-step SC method, termed DGCAKL, which directly learns the group indicators under discrete group structure constraints while adaptively learning the kernel mapping matrix to capture the nonlinear structure of high-dimensional data. These two learning processes are integrated into a mutually reinforcing optimization framework. Specifically, we employ the squared Schatten p -norm ( 0 < p ≤ 2 ) to impose non-convex low-rank constraints on the nonlinearly mapped data, effectively approximating the rank of data matrix in the kernel feature space. To compute the discrete group indicators, we circumvent the requirement for handling integer programming, enhancing both the efficiency and scalability of the proposed algorithm. Furthermore, we develop an efficient iterative optimization algorithm based on the alternating direction method of multipliers (ADMM), combined with iteratively reweighted minimization, and provide the convergence guarantee of the algorithm under mild conditions. Extensive experiments on both synthetic and real-world datasets demonstrate that the proposed method effectively handles nonlinear data structures and significantly improves clustering accuracy.},
  archive      = {J_PR},
  author       = {Wenyu Hu and Shaoting Peng and Tinghua Wang and Gaohang Yu and Jialin Hua},
  doi          = {10.1016/j.patcog.2025.112045},
  journal      = {Pattern Recognition},
  month        = {3},
  pages        = {112045},
  shortjournal = {Pattern Recognition},
  title        = {Adaptive kernel subspace clustering with discrete group structure constraint},
  volume       = {171},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Towards redundancy-free sub-networks in continual learning. <em>PR</em>, <em>171</em>, 112020. (<a href='https://doi.org/10.1016/j.patcog.2025.112020'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Catastrophic Forgetting (CF) is a prominent issue in continual learning. Parameter isolation addresses this challenge by masking a sub-network for each task to mitigate interference with old tasks. However, these sub-networks are constructed relying on weight magnitude, which does not necessarily correspond to the importance of weights, resulting in maintaining unimportant weights and constructing redundant sub-networks. To overcome this limitation, inspired by information bottleneck, which removes redundancy between adjacent network layers, we propose Information Bottleneck Masked sub-network (IBM) to eliminate redundancy within sub-networks. Specifically, IBM accumulates valuable information into essential weights to construct redundancy-free sub-networks, not only effectively mitigating CF by freezing the sub-networks but also facilitating new tasks training through the transfer of valuable knowledge. Additionally, IBM decomposes hidden representations to automate the construction process and make it flexible. Extensive experiments demonstrate that IBM consistently outperforms state-of-the-art methods. Notably, IBM surpasses the state-of-the-art parameter isolation method with a 70% reduction in the number of parameters within sub-networks and an 80% decrease in training time. The source code is available at https://github.com/zackschen/IBM-Net},
  archive      = {J_PR},
  author       = {Cheng Chen and Lianli Gao and Pengpeng Zeng and Mingsheng Cao and Heng Tao Shen},
  doi          = {10.1016/j.patcog.2025.112020},
  journal      = {Pattern Recognition},
  month        = {3},
  pages        = {112020},
  shortjournal = {Pattern Recognition},
  title        = {Towards redundancy-free sub-networks in continual learning},
  volume       = {171},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). One-pass online learning from data streams with unpredictable feature evolution. <em>PR</em>, <em>171</em>, 112003. (<a href='https://doi.org/10.1016/j.patcog.2025.112003'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, the field of online learning has gained significant attention, particularly in addressing the challenges posed by feature evolution in data streams, namely the disappearance of old features and the emergence of new ones. Many existing methods assume that feature evolution follows a predictable pattern, such as feature increment and specific retention of old features. However, in real-world scenarios, feature evolution is often unpredictable due to the uncertainty of feature disappearance and emergence. To address this challenge, this paper proposes a novel method called OLUFE for online learning from data streams with unpredictable feature evolution. OLUFE is a one-pass learning method where each instance appears only once, adhering to the evolving nature of data streams. It handles the dynamic evolution of the feature space with a kernel-based online learning method. Additionally, a projection-based support vector budget strategy is introduced to enhance the computational efficiency of the model. Theoretical analysis demonstrates that OLUFE achieves a sublinear regret O ( T ) under mild conditions. Extensive experiments on both synthetic and real-world datasets validate the effectiveness of OLUFE in handling data streams with unpredictable feature evolution.},
  archive      = {J_PR},
  author       = {Peng Zhang and Hongpeng Yin and Han Zhou},
  doi          = {10.1016/j.patcog.2025.112003},
  journal      = {Pattern Recognition},
  month        = {3},
  pages        = {112003},
  shortjournal = {Pattern Recognition},
  title        = {One-pass online learning from data streams with unpredictable feature evolution},
  volume       = {171},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Image splicing localization method driven by device difference feature guidance. <em>PR</em>, <em>171</em>, 112002. (<a href='https://doi.org/10.1016/j.patcog.2025.112002'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The increasing popularity of image editing tools poses a significant threat to the originality and authenticity of image content. The most common form of attack on image content is splicing forgery, which involves combining multiple donor images from different imaging devices to create a composite spliced image. In this study, we propose an image splicing region localization method based on device difference guidance. Its core idea lies in taking the physical characteristics of imaging devices as forensic clues, and determining the tampered image regions through effective extraction and analysis of these characteristics. The proposed method is a two-stage multi-scale network model named device difference feature guidance network (DDFG-Net), including the device difference guidance stage and the abnormal region localization stage. In the device difference guidance stage, a dual-stream Vision-Transformer network is adopted for camera source identification to extract the physical characteristics of different imaging devices. Immediately afterwards, the inconsistent features in the image are obtained by using the physical characteristics of the device, and the semantic information is decoupled from the inconsistent features introduced by splicing forgery to avoid the interference of semantic information on the forensic features. In the abnormal region location stage, we proposed the Channel-Wise up-sample and Spatial-Wise down-sample(CWU-SWD) and Cross-Scale Concatenator(CSC). The CWU-SWD module is used to reduce the loss of feature information when the scale changes and ensure the integrity of the information, thereby enabling the network to extract richer subtle features introduced by forgery. The CSC module is used for dynamic selection and fusion of multi-scale features, enabling the network to adapt to tampered areas of different scales. A large number of experimental comparisons on four different datasets have proved the excellent performance of DDFG-Net. Cross-dataset experiments and robustness tests further demonstrated the significant generalization and robustness of DDFG-Net. Furthermore, the ablation experiment further confirmed the rationality and effectiveness of each module in DDFG-Net.},
  archive      = {J_PR},
  author       = {Xiaofeng Wang and Yihang Wang and Ningning Bai and Ruidong Han and Jianpeng Hou and Tongtong Xu and Xi Huang and Shanmin Pang},
  doi          = {10.1016/j.patcog.2025.112002},
  journal      = {Pattern Recognition},
  month        = {3},
  pages        = {112002},
  shortjournal = {Pattern Recognition},
  title        = {Image splicing localization method driven by device difference feature guidance},
  volume       = {171},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). FacialTalk: Audio-driven high-fidelity facial portrait generation using 3D facial prior. <em>PR</em>, <em>171</em>, 111994. (<a href='https://doi.org/10.1016/j.patcog.2025.111994'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {NeRF-based audio-driven facial portrait generation methods often struggle with insufficient 3D facial information, making it difficult to achieve high-fidelity and comprehensive facial geometry reconstruction. We introduce FacialTalk, which encodes raw facial landmarks losslessly as conditional features and combines them with audio conditions to map the labels into a unified feature space. By incorporating landmark spatial position encoding, FacialTalk effectively decouples the distribution of the lips and face in the conditional space. FacialTalk begins by using 3D facial landmarks and audio as driving conditions to control lip synchronization and dynamic facial expressions. A Landmark Spatial-Aware Encoding is then applied within NeRF, assigning attention vectors to the lip and expression regions to minimize interference. Finally, Adaptive Torso Perception Encoding integrates head pose deformations and mapped keypoint coordinates to enable natural head and torso movements. Extensive experiments and user studies have shown that FacialTalk has achieved a PNSR of 37.1876 dB, which performs well in audio synchronization and in producing high-quality visual facial portrait videos.},
  archive      = {J_PR},
  author       = {Daowu Yang and Ying Liu and Qiyun Yang and Ruihui Li},
  doi          = {10.1016/j.patcog.2025.111994},
  journal      = {Pattern Recognition},
  month        = {3},
  pages        = {111994},
  shortjournal = {Pattern Recognition},
  title        = {FacialTalk: Audio-driven high-fidelity facial portrait generation using 3D facial prior},
  volume       = {171},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). M2Beats 2.0: When motion meets beats in short-form videos twice. <em>PR</em>, <em>171</em>, 111967. (<a href='https://doi.org/10.1016/j.patcog.2025.111967'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, short-form videos have gained popularity and the editing of these videos, particularly when motion is synchronized with music, is highly favored due to its beat-matching effect. However, detecting motion rhythm poses a significant challenge as it is influenced by multifaceted factors that make it difficult to define using explicit rules. Traditional methods, which attempt to define motion rhythm through predefined criteria, frequently produce unsatisfactory results. Conversely, learning-based methods can extract motion rhythm without relying on explicit rules but necessitate high-quality datasets. Regrettably, motion rhythm is closely tied to human subjective perception and has not been accurately annotated: existing datasets either simply substitute music rhythm for motion rhythm, which are not equivalent, or leverage the consistency between music and motion to annotate motion rhythm while this process does not incorporate human feedback. Moreover, limbs play a crucial role in conveying motion rhythm; however, existing methods primarily depend on generic graph convolution or attention mechanisms to extract features only at the whole-body level and are insufficient for capturing the detailed movements of the limbs, which causes poor performance in limb-dominant movements. To address these challenges, we propose a new dataset and a novel method: ( i ) Dataset: We construct a motion rhythm dataset, AIST-M2B 2.0, integrating the human feedback mechanism into the automated annotation pipeline to improve the quality of the dataset; additionally, we employ soft labels rather than binary labels to achieve more efficient training. ( ii ) Method: We propose M2BNet 2.0, a novel network tailored for rhythm, which includes a hierarchical graph convolution module that captures the rhythm expression at the whole-body, inter-limb, and intra-limb levels; furthermore, we design a strategy for enhancing the motion rhythm in short-form videos. Compared to the latest work, our approach improves the F-score and PR-AUC metrics by 6.6% and 9.8% on the AIST-M2B dataset, and by 9.8% and 7.4% on the AIST-M2B 2.0 dataset, respectively. The dataset and code are available at https://zyc-cver.github.io/M2Beats-2.0/ .},
  archive      = {J_PR},
  author       = {Yongchang Zhang and Ao Lv and Shuai He and Haiyang Zhang and Anlong Ming},
  doi          = {10.1016/j.patcog.2025.111967},
  journal      = {Pattern Recognition},
  month        = {3},
  pages        = {111967},
  shortjournal = {Pattern Recognition},
  title        = {M2Beats 2.0: When motion meets beats in short-form videos twice},
  volume       = {171},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Improved bimodal segmentation of multi-light images based on feature fusion. <em>PR</em>, <em>171</em>, 111947. (<a href='https://doi.org/10.1016/j.patcog.2025.111947'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In different modalities, observations of the same object often exhibit significantly distinct characteristics, such as, polarized images in geoscience. Multimodal semantic segmentation techniques are particularly important in these fields. However, discussions on multimodal image fusion technologies and the corresponding semantic segmentation methods in the geological domain remain limited. Moreover, when performing geological cross-modal fusion, existing fusion networks face challenges, such as low accuracy and high computational costs. To address these issues, we designed a dual-channel feature fusion segmentation network (DcFFSNet) by integrating the advantageous feature fusion module (AFFM). AFFM extracts complementary channel attention vectors, refines feature maps based on the unique characteristics of each modality, enhances the spatial attention weighting of salient feature regions, and combines these features through weighted addition. The DcFFSNet utilizes MobileNetV3 as its encoding backbone, incorporating atrous spatial pyramid pooling (ASPP) and five AFFMs, with DeepLabV3+ serving as the decoder. Comparative experiments demonstrated that DcFFSNet effectively resolves the issue of significant discrepancies in prediction results for different types of polarized rock images in geological research, achieving an impressive performance with MIoU of 73.43%, while maintaining a relatively low computational cost and model size.},
  archive      = {J_PR},
  author       = {Wei Zeng and Yiru Wang and Caihua Chen and Chaoyu Yao and Dongyu Zheng and Sheng Wang},
  doi          = {10.1016/j.patcog.2025.111947},
  journal      = {Pattern Recognition},
  month        = {3},
  pages        = {111947},
  shortjournal = {Pattern Recognition},
  title        = {Improved bimodal segmentation of multi-light images based on feature fusion},
  volume       = {171},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). AutoScaler: Self scale alignment for handwritten mathematical expression recognition. <em>PR</em>, <em>171</em>, 111872. (<a href='https://doi.org/10.1016/j.patcog.2025.111872'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Handwritten mathematical expression recognition (HMER) remains a challenging task in computer vision due to the complex spatial structures and diverse handwriting styles of mathematical expressions. While current approaches focus heavily on decoder modeling, the critical influence of input scale variations on recognition accuracy has been largely overlooked. This paper introduces AutoScaler, a novel scale-adaptive framework that addresses the issue of scale misalignment between input images and decoders for the first time in HMER. Our method comprises two novel components: (1) Stochastic Scale Training, a training strategy that enhances the model’s robustness to scale variations by enabling it to perceive mathematical expressions across diverse scales; (2) Optimal Scale Estimation, an inference technique that identifies the most suitable scale for each input image, ensuring proper alignment of visual features with decoder. Additionally, we propose Cross-scale Joint Approximation, which leverages complementary information between optimal and sub-optimal scales to further improve recognition performance. Extensive experiments on real-world datasets show our method achieves state-of-the-art results for both single-line and multi-line expressions, underscoring its effectiveness in advancing HMER. The code and model are available at https://github.com/SCUT-DLVCLab/AutoScaler .},
  archive      = {J_PR},
  author       = {Wentao Yang and Jiaxin Zhang and Lianwen Jin},
  doi          = {10.1016/j.patcog.2025.111872},
  journal      = {Pattern Recognition},
  month        = {3},
  pages        = {111872},
  shortjournal = {Pattern Recognition},
  title        = {AutoScaler: Self scale alignment for handwritten mathematical expression recognition},
  volume       = {171},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). LiGAPU: A LiDAR point cloud upsampling network for multiple complex scenes. <em>PR</em>, <em>170</em>, 112132. (<a href='https://doi.org/10.1016/j.patcog.2025.112132'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Point cloud upsampling is essential for restoring fine details in point clouds. While most existing methods focus on single-object point clouds with regular geometric shapes, there is limited research on upsampling LiDAR point clouds from complex scenes, which often exhibit intricate structures, non-uniform density, and significant noise. To fill this gap, we propose LiGAPU, a novel Geometry-Aware LiDAR point cloud upsampling network designed to enhance the modeling of spatial structures in these complex scenes. The core of LiGAPU lies in two key modules: a Dual-stream Gating mechanism-based convolutional (DGConv) module that robustly captures both scalar and vector features while modeling their global dependencies, and a GCS module for precise point-to-point distance prediction. By combining 3D Gaussian feature interpolation with Channel-Spatial attention mechanisms, the GCS module effectively models fine-grained local relationships to improve the positional accuracy of upsampled points. We conduct experiments on public benchmarks as well as our self-collected datasets. Experimental results demonstrate that LiGAPU improves upsampling accuracy across all datasets while preserving structural consistency, and exhibits strong robustness and generalization in complex scenes. The code is available at https://github.com/Sailkiki/LiGAPU .},
  archive      = {J_PR},
  author       = {Bolin Fu and Mingzhe Sui and Huajian Li and Fei Yang and Hang Yao and Tengfang Deng and Xing Zhang},
  doi          = {10.1016/j.patcog.2025.112132},
  journal      = {Pattern Recognition},
  month        = {2},
  pages        = {112132},
  shortjournal = {Pattern Recognition},
  title        = {LiGAPU: A LiDAR point cloud upsampling network for multiple complex scenes},
  volume       = {170},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). A dynamic context-aware aggregation strategy for small object detection. <em>PR</em>, <em>170</em>, 112127. (<a href='https://doi.org/10.1016/j.patcog.2025.112127'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Mutual occlusion and tiny object sizes severely hinder the extraction of semantic and structural information, which poses a significant obstacle for small object detection. In this study, a dynamic context-aware aggregation strategy (DCAS) is proposed to enhance the ability of small object detectors to identify occluded or tiny objects. The main approaches developed include the dynamic adjustment of the number of pooling layers in the spatial pyramid pooling fast module based on the proportion of tiny objects in the target dataset, the integration of lower-level features into feature pyramid networks, and the elimination of the highest-level of information processing modules to remove redundant semantic features for small object detection. The proposed model effectively captures contextual and structural information for occluded or tiny objects, which makes it adaptable to diverse scenarios. Comparison experiments on the VisDrone, BDD100K, and TT100K datasets demonstrated that DCAS-equipped models outperformed 11 popular or up-to-date small object detectors. Ablation studies using YOLOv5 and YOLOv8 further validated the effectiveness of DCAS. All findings consistently demonstrated that DCAS achieved 1.0 ∼ 7.2 % higher detection accuracy compared with baseline models. The source code and related datasets are available at https://github.com/crafly/DCAS.git .},
  archive      = {J_PR},
  author       = {Minhu Yang and Hexiang Bai and Jianlong Hu and Deyu Li},
  doi          = {10.1016/j.patcog.2025.112127},
  journal      = {Pattern Recognition},
  month        = {2},
  pages        = {112127},
  shortjournal = {Pattern Recognition},
  title        = {A dynamic context-aware aggregation strategy for small object detection},
  volume       = {170},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Physics–Environment interaction network for dense crowd behavior recognition. <em>PR</em>, <em>170</em>, 112123. (<a href='https://doi.org/10.1016/j.patcog.2025.112123'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Large-scale crowd behavior analysis is critical for public safety. However, intelligent systems face three major challenges: inter-individual occlusion, behavioral pattern variability, and complexity of behavioral evolution. To address these challenges, we propose the Physics-Environment Interaction Network (PEIN), which utilizes physical properties to model the movement characteristics of groups. Specifically, our method consists of two streams. The first stream, the physics-informed crowd property stream, leverages the similarity between dense crowd motion and fluid dynamics. It uses the Navier–Stokes (N–S) equation to model crowd motion and characterize key crowd attributes such as collectiveness, conflict, uniformity, and stability. Each term of the N–S equation is modeled using operators and neural networks guided by these properties, enabling crowd-level motion representation without relying on individual tracking. The second stream, the environment perception stream, addresses scene-level variability by capturing global spatiotemporal information from input video frames. A 3D network is used to enhance robustness by incorporating temporal context beyond instantaneous motion. A dual cross-attention mechanism fuses both streams’ features into a unified representation for behavior recognition. By incorporating physical laws as constraints, we design a physics-informed loss function combined with a crowd behavior loss function to optimize the model. Considering the diversity of dense crowds in real-life scenarios, we introduce the Mixed Crowd Behavior Dataset (MCBD) for crowd behavior recognition across various settings, encompassing both structured and unstructured crowds. Our proposed method achieves the most desirable results on the public dataset DCFD and the new MCBD.},
  archive      = {J_PR},
  author       = {Jiaqi Yu and Yanshan Zhou and Renjie Pan and Pingrui Lai and Hua Yang},
  doi          = {10.1016/j.patcog.2025.112123},
  journal      = {Pattern Recognition},
  month        = {2},
  pages        = {112123},
  shortjournal = {Pattern Recognition},
  title        = {Physics–Environment interaction network for dense crowd behavior recognition},
  volume       = {170},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Mamba-stereo: Mamba regularization for stereo matching. <em>PR</em>, <em>170</em>, 112120. (<a href='https://doi.org/10.1016/j.patcog.2025.112120'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Stereo matching networks are essential for applications such as autonomous driving, robotic navigation, and augmented reality (AR). Traditional cost volumes, built using image features, often fail to capture global geometric information, leading to detail loss, edge blurring, and errors in textureless regions. To address these issues, we introduce a novel cost volume enhancement module based on the Mamba mechanism, termed Mamba Regularization. Mamba Regularization flattens the high-dimensional cost volume into one-dimensional features, utilizing the Spatial Mamba Block to capture long-range dependencies within whole-volume features at every scale. Additionally, we design the Spatial Residual Convolution (SRC) module to compensate for spatial information loss during flattening. Experimental results on the SceneFlow and KITTI benchmarks demonstrate that Mamba Regularization can serve as a plug-and-play module, significantly enhancing the performance of various stereo matching networks. While it introduces a slight increase in inference time, it achieves a favorable trade-off between accuracy and computational efficiency. Moreover, the proposed module exhibits strong cross-dataset generalization and maintains high inference efficiency, further validating its effectiveness in real-world applications. Code is available at https://github.com/S1aoXuan/Mamba-Regularization .},
  archive      = {J_PR},
  author       = {Shaoxuan Suo and Jinxin Liu and Huihui Miao},
  doi          = {10.1016/j.patcog.2025.112120},
  journal      = {Pattern Recognition},
  month        = {2},
  pages        = {112120},
  shortjournal = {Pattern Recognition},
  title        = {Mamba-stereo: Mamba regularization for stereo matching},
  volume       = {170},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Colonic polyp segmentation based on transformer-convolutional neural networks fusion. <em>PR</em>, <em>170</em>, 112116. (<a href='https://doi.org/10.1016/j.patcog.2025.112116'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Colon polyp segmentation plays a vital role in medical image analysis. Despite promising advances, existing methods still struggle with the accurate delineation of small polyps exhibiting fuzzy boundaries. To address this challenge, we propose a novel hybrid network, Fusion-Transformer-HardNetMSEG (Fu-TransHNet). Fu-TransHNet formulates three views through Transformer branch, CNN branch, and the fusion module of both branches. These three views can extract global and local features, minimize information loss, and improve representational capacity. To further optimize segmentation performance, a multi-view collaborative learning strategy is employed to adaptively weight each view based on its contribution. Extensive experiments conducted on five public benchmark datasets demonstrate that Fu-TransHNet consistently outperforms existing methods, achieving performance gains ranging from 0.32 % to 36.56 %. The source code will be available at https://github.com/ChenxiLuo-code1/Fu-TransHNet .},
  archive      = {J_PR},
  author       = {Chenxi Luo and Yuanyuan Wang and Zhaohong Deng and Qiongdan Lou and Zhuangzhuang Zhao and Yuxi Ge and Shudong Hu},
  doi          = {10.1016/j.patcog.2025.112116},
  journal      = {Pattern Recognition},
  month        = {2},
  pages        = {112116},
  shortjournal = {Pattern Recognition},
  title        = {Colonic polyp segmentation based on transformer-convolutional neural networks fusion},
  volume       = {170},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). A dual-stage focus measure for vector-valued images in shape from focus. <em>PR</em>, <em>170</em>, 112112. (<a href='https://doi.org/10.1016/j.patcog.2025.112112'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In shape-from-focus, the focus measure operator (FM) determines the accuracy of the depth map of a reconstructed object. Typically, before applying FM, vector-valued (color) images are converted into gray-scale for simplicity. However, this conversion prevents the FM from incorporating focus information from all color channels, leading to inaccuracies in the depth map. To overcome this limitation, we propose a dual-stage focus measure. The first stage involves converting the vector-valued image volume into a scalar-valued image volume using a unique method that facilitates the application of FM. This conversion is achieved through vector operations that compute a scaled norm from the difference vectors, with a scaling factor adjusted according to the variations in these norms. In the second stage, the focus measure is applied using the Directional Ring Difference Filter (DRDF) in different directions to obtain directional focus volumes, which are then aggregated using a weighted approach to compute a unified focus volume from which the depth map is ultimately extracted. Our extensive evaluation, conducted on over 1200 scenes comprising more than 17,000 images, demonstrated the superiority of our method. It outperformed 18 renowned techniques, achieving around 6.7 percent lower RMSE compared to the second best method on the HCI14 dataset. The official implementation of our method is publicly available and can be found at: https://github.com/khurramashfaq/dual-stage-fm-sff .},
  archive      = {J_PR},
  author       = {Khurram Ashfaq and Muhammad Tariq Mahmood},
  doi          = {10.1016/j.patcog.2025.112112},
  journal      = {Pattern Recognition},
  month        = {2},
  pages        = {112112},
  shortjournal = {Pattern Recognition},
  title        = {A dual-stage focus measure for vector-valued images in shape from focus},
  volume       = {170},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Training-free subject-enhanced attention guidance for compositional text-to-image generation. <em>PR</em>, <em>170</em>, 112111. (<a href='https://doi.org/10.1016/j.patcog.2025.112111'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Existing subject-driven text-to-image generation models suffer from tedious fine-tuning steps and struggle to maintain both text-image alignment and subject fidelity. For generating compositional subjects, it often encounters problems such as object missing and attribute mixing, where some subjects in the input prompt are not generated or their attributes are incorrectly combined. To address these limitations, we propose a subject-driven generation framework and introduce training-free guidance to intervene in the generative process during inference time. This approach strengthens the attention map, allowing for precise attribute binding and feature injection for each subject. Notably, our method exhibits exceptional zero-shot generation ability, especially in the challenging task of compositional generation. Furthermore, we propose a novel GroundingScore metric to thoroughly assess subject alignment. The obtained quantitative results serve as compelling evidence showcasing the effectiveness of our proposed method.},
  archive      = {J_PR},
  author       = {Shengyuan Liu and Bo Wang and Ye Ma and Te Yang and Quan Chen and Di Dong},
  doi          = {10.1016/j.patcog.2025.112111},
  journal      = {Pattern Recognition},
  month        = {2},
  pages        = {112111},
  shortjournal = {Pattern Recognition},
  title        = {Training-free subject-enhanced attention guidance for compositional text-to-image generation},
  volume       = {170},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). A memory-tree driven network for multi-view fusion anomaly detection. <em>PR</em>, <em>170</em>, 112106. (<a href='https://doi.org/10.1016/j.patcog.2025.112106'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The unsupervised anomaly detection method has been widely discussed due to its advantages of eliminating manual labeling. However, the instability of reconstruction ability limits the reconstruction quality and detection quality of the reconstruction-based anomaly detection method. The existing methods use the same convolution block structure to compare information at two levels of image and feature spaces, which limits the use of image feature space in anomaly reconstruction. We propose a memory tree driven network for multi-view fusion anomaly detection, a novel anomaly detection network with an integrated fusion scheme. To prevent abnormal information from flowing into the image feature space, we design a memory feature fusion module called Memory Tree to set up anomaly information access control. Subsequently, we conduct a memory matching procedure to integrate memory information with strong correlation into the decoder network. Furthermore, we integrate multi-view image features to learn the image perception range. We further improve the discriminator’s architecture and impart the image generation process to boost the discriminator’s performance.},
  archive      = {J_PR},
  author       = {Xu Liu and Chunlei Wu and Huan Zhang and Leiquan Wang},
  doi          = {10.1016/j.patcog.2025.112106},
  journal      = {Pattern Recognition},
  month        = {2},
  pages        = {112106},
  shortjournal = {Pattern Recognition},
  title        = {A memory-tree driven network for multi-view fusion anomaly detection},
  volume       = {170},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Fully test-time rPPG estimation via synthetic signal-guided feature learning. <em>PR</em>, <em>170</em>, 112102. (<a href='https://doi.org/10.1016/j.patcog.2025.112102'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Many remote photoplethysmography (rPPG) estimation models have achieved promising performance in the training domain but often fail to accurately estimate physiological signals or heart rates (HR) in the target domains. Domain generalization (DG) or domain adaptation (DA) techniques are therefore adopted during the offline training stage to adapt the model to either unobserved or observed target domains by utilizing all available source domain data. However, in rPPG estimation problems, the adapted model usually encounters challenges in estimating target data with significant domain variation, such as different video capturing settings, individuals with different HR ranges, or imbalanced HR distributions. In contrast, Test-Time Adaptation (TTA) enables the model to adaptively estimate rPPG signals in various unseen domains by online adapting to unlabeled target data without referring to any source data. In this paper, we first establish a new TTA-rPPG benchmark that encompasses various domain information and HR distributions to simulate the challenges encountered in real-world rPPG estimation. Next, we propose a novel synthetic signal-guided rPPG estimation framework to address the forgetting issue during the TTA stage and to enhance the adaptation capability of the pre-trained rPPG model. To this end, we develop a synthetic signal-guided feature learning method by synthesizing rPPG signals as pseudo ground truths to guide a conditional generator in generating latent rPPG features. In addition, we design an effective spectral-based entropy minimization technique to encourage the rPPG model to learn new target domain information. Both the generated rPPG features and synthesized rPPG signals prevent the rPPG model from overfitting to target data and forgetting previously acquired knowledge, while also broadly covering various heart rate (HR) distributions. Our extensive experiments on the TTA-rPPG benchmark show that the proposed method achieves superior performance and outperforms previous DG and DA methods across most protocols of the benchmark.},
  archive      = {J_PR},
  author       = {Pei-Kai Huang and Tzu-Hsien Chen and Ya-Ting Chan and Kuan-Wen Chen and Shih-Yu Yang and Yen-Chun Chou and Chiou-Ting Hsu},
  doi          = {10.1016/j.patcog.2025.112102},
  journal      = {Pattern Recognition},
  month        = {2},
  pages        = {112102},
  shortjournal = {Pattern Recognition},
  title        = {Fully test-time rPPG estimation via synthetic signal-guided feature learning},
  volume       = {170},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). MSBP-net: A multi-scale boundary prediction network for automated polyp segmentation. <em>PR</em>, <em>170</em>, 112101. (<a href='https://doi.org/10.1016/j.patcog.2025.112101'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Accurate polyp segmentation is a critical step in the early diagnosis of colorectal cancer. Predicting the polyp boundaries with powerful camouflage properties is an intricate challenge in automatic segmentation tasks. Here, we propose a multi-scale boundary prediction network (MSBP-Net) for effective polyp segmentation with low complexity. The MSBP-Net consists of a pre-trained pyramid vision transformer and a novel lightweight decoder which contains three modified receptive field blocks (mRFBs), three boundary prediction modules (BPMs) and a shallow filtering module (SFM). Firstly, the mRFBs are developed to suppress redundant information and irrelevant background. Secondly, the BMPs are built based on reverse attention and multi-scale criss-cross attention to efficiently explore boundaries and fuse multi-scale information for recovering coarse-grained polyp masks. Then, the SFM is proposed based on Laplacian edge operators, which mines boundary cues in the shallow layers as a supplement to the coarse-grained masks and finally gains fine-grained segmentation masks. We conduct extensive comparative experiments on five public datasets. The ablation results prove the effectiveness of the proposed modules, achieving a highest dice similarity coefficient of 0.940. Compared to existing state-of-the-art methods, the MSBP-Net performs close to the optimal methods on the five test sets, while reducing the complexity by at least 20.2 %. The lightweight design of the MSBP-Net achieves a speed of 41 frames per second on a 3070 GPU (8 GB memory). Therefore, the MSBP-Net is a promising polyp segmentation method with robust performance and low complexity, and has the potential for real-time segmentation tasks.},
  archive      = {J_PR},
  author       = {Xing-Liang Pan and Ju-Rong Ding and Xia Li and Shuo Liu and Jie Wang and Bo Hua and Guo-Zhi Tang and Chang-Hua Zhong},
  doi          = {10.1016/j.patcog.2025.112101},
  journal      = {Pattern Recognition},
  month        = {2},
  pages        = {112101},
  shortjournal = {Pattern Recognition},
  title        = {MSBP-net: A multi-scale boundary prediction network for automated polyp segmentation},
  volume       = {170},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Conditional cross-domain prompt optimization for domain adaptation. <em>PR</em>, <em>170</em>, 112100. (<a href='https://doi.org/10.1016/j.patcog.2025.112100'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-source Unsupervised Domain Adaptation (MUDA) aims to transfer knowledge from multiple labeled source domains to an unlabeled target domain. Existing methods typically extract cross-domain shared representations by enforcing inter-domain invariance constraints on single visual modality feature distribution, to ensure the discriminability of the target joint distribution. However, in multi-source distribution scenarios, the coupling of domain and semantic information within the feature space limits the capacity of single modality feature transformation mechanisms to maintain the invariance of feature semantic information during training. To address this issue, we propose a Conditional Cross-domain Prompt Optimization (CCPO) method, which leverages pre-trained vision-language models to introduce textual modality, transforming the shared representation learning problem under multi-source feature distribution into a cross-domain prompt optimization task. Specifically, we design a latent space separation module from the perspective of data generation to decouple domain-specific and semantic features in the latent space, which mitigates the interference of domain variables on feature semantics, and we employ class-aware entropy weighting to adapt to label shift scenarios in the target domain. On this basis, we develop a set of conditional cross-domain prompts, which use the decoupled domain and semantic visual features as input conditional tokens in the prompt optimization process, ensuring cross-domain adaptability of the learned prompts. Extensive experiments demonstrate that CCPO achieves state-of-the-art performance on most MUDA tasks, substantiating its efficacy and generalization capabilities.},
  archive      = {J_PR},
  author       = {Bo Zhou and Long Liu and Chenyue Fan and Zhipeng Zhao},
  doi          = {10.1016/j.patcog.2025.112100},
  journal      = {Pattern Recognition},
  month        = {2},
  pages        = {112100},
  shortjournal = {Pattern Recognition},
  title        = {Conditional cross-domain prompt optimization for domain adaptation},
  volume       = {170},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). ReBaR: Reference-based reasoning for robust pose estimation from monocular images. <em>PR</em>, <em>170</em>, 112096. (<a href='https://doi.org/10.1016/j.patcog.2025.112096'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper introduces a novel method, ReBaR ( Re ference- Ba sed R easoning for Robust Human Pose and Shape Estimation), designed to estimate human body shape and pose from single-view images. ReBaR effectively addresses the challenges of occlusions and depth ambiguity by learning reference features for part regression reasoning. Our approach starts by extracting features from both body and part regions using an attention-guided mechanism. Subsequently, these features are used to encode additional part-body dependencies for individual part regression, with part features serving as queries and the body feature as a reference. This reference-based reasoning allows our network to infer the spatial relationships of occluded parts with the body, utilizing visible parts and body reference information. ReBaR outperforms contemporary methods on three benchmark datasets and still maintains competitive advantages among recent new approaches. Demonstrating significant improvement in handling depth ambiguity and occlusion. These results strongly support the effectiveness of our reference-based framework for estimating human body shape and pose from single-view images.},
  archive      = {J_PR},
  author       = {Yongkang Cheng and Mingjiang Liang and Jifeng Ning and Gaoge Han and Wei Liu and Shaoli Huang},
  doi          = {10.1016/j.patcog.2025.112096},
  journal      = {Pattern Recognition},
  month        = {2},
  pages        = {112096},
  shortjournal = {Pattern Recognition},
  title        = {ReBaR: Reference-based reasoning for robust pose estimation from monocular images},
  volume       = {170},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Prompt-affinity multi-modal class centroids for unsupervised domain adaption. <em>PR</em>, <em>170</em>, 112095. (<a href='https://doi.org/10.1016/j.patcog.2025.112095'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, the advancements in large vision-language models (VLMs) like CLIP have sparked a renewed interest in leveraging the prompt learning mechanism to preserve semantic consistency between source and target domains in unsupervised domain adaption (UDA). While these approaches show promising results, they encounter fundamental limitations when quantifying the similarity between source and target domain data, primarily stemming from the redundant and modality-missing class centroids. To address these limitations, we propose P rompt-affinity M ulti-modal C lass C entroids for UDA (termed as PMCC). Firstly, we fuse the text class centroids (directly generated from the text encoder of CLIP with manual prompts for each class) and image class centroids (generated from the image encoder of CLIP for each class based on source domain images) to yield the multi-modal class centroids. Secondly, we conduct the cross-attention operation between each source or target domain image and these multi-modal class centroids. In this way, these class centroids that contain rich semantic information of each class will serve as a bridge to effectively measure the semantic similarity between different domains. Finally, we design a logit bias head and employ a multi-modal prompt learning mechanism to accurately predict the true class of each image for both source and target domains. We conduct extensive experiments on 4 popular UDA datasets including Office-31, Office-Home, VisDA-2017, and DomainNet. The experimental results validate our PMCC achieves higher performance with lower model complexity than the state-of-the-art (SOTA) UDA methods. The code of this project is available at GitHub: https://github.com/246dxw/PMCC .},
  archive      = {J_PR},
  author       = {Xingwei Deng and Yangtao Wang and Yanzhao Xie and Xiaocui Li and Maobin Tang and Meie Fang and Wensheng Zhang},
  doi          = {10.1016/j.patcog.2025.112095},
  journal      = {Pattern Recognition},
  month        = {2},
  pages        = {112095},
  shortjournal = {Pattern Recognition},
  title        = {Prompt-affinity multi-modal class centroids for unsupervised domain adaption},
  volume       = {170},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Aligned music notation and lyrics transcription. <em>PR</em>, <em>170</em>, 112094. (<a href='https://doi.org/10.1016/j.patcog.2025.112094'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The digitization of vocal music scores presents unique challenges that go beyond traditional Optical Music Recognition and Optical Character Recognition, as it necessitates preserving the critical alignment between music notation and lyrics. This alignment is essential for proper interpretation and processing in practical applications. This paper introduces and formalizes, for the first time, the Aligned Music Notation and Lyrics Transcription (AMNLT) challenge, which addresses the complete transcription of vocal scores by jointly considering music symbols, lyrics, and their synchronization. We analyze different approaches to address this challenge, ranging from traditional divide-and-conquer methods that handle music and lyrics separately, to novel end-to-end solutions including direct transcription, unfolding mechanisms, and language modeling. To evaluate these methods, we introduce four datasets of Gregorian chants, comprising both real and synthetic sources, along with custom metrics specifically designed to assess both transcription and alignment accuracy. Our experimental results demonstrate that end-to-end approaches generally outperform heuristic methods in the alignment challenge, with language models showing particular promise in scenarios where sufficient training data is available. This work establishes the first comprehensive framework for AMNLT, providing both theoretical foundations and practical solutions for preserving and digitizing vocal music heritage.},
  archive      = {J_PR},
  author       = {Eliseo Fuentes-Martínez and Antonio Ríos-Vila and Juan C. Martinez-Sevilla and David Rizo and Jorge Calvo-Zaragoza},
  doi          = {10.1016/j.patcog.2025.112094},
  journal      = {Pattern Recognition},
  month        = {2},
  pages        = {112094},
  shortjournal = {Pattern Recognition},
  title        = {Aligned music notation and lyrics transcription},
  volume       = {170},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Separate to generalization: Two-branch feature separation framework for generalized underwater image restoration. <em>PR</em>, <em>170</em>, 112092. (<a href='https://doi.org/10.1016/j.patcog.2025.112092'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Despite the impressive achievements of recent underwater image restoration (UIR) methods, they still struggle to perform well in unseen underwater environments. The potential reason could be the entanglement between generalized (scene) and biased (attenuation) image information, which makes it difficult to separate the generalized one for UIR. To this end, we propose a flexible Two-Branch Feature Separation (TBFS) framework to enable the generalized UIR. TBFS designs a novel two-branch feature extractor that can effectively separate the scene and attenuation information. Specifically, a scene-specific branch captures the environment-generalized features with the guidance of underwater scene discrimination, and a attenuation-biased branch is encouraged to identify the environment-biased features by performing underwater environment classification. Besides, a multi-level orthogonality constraint is imposed to fully leverage the knowledge derived from two branches for better entangled information separation. Finally, TBFS reconstructs input images to ensure the physical properties of restoration results. Experimental results on both full-reference and non-reference underwater benchmarks quantitatively and qualitatively demonstrate the effectiveness of our method.},
  archive      = {J_PR},
  author       = {Jiahao Qi and Xingyue Liu and Chen Chen and Kangchen Bin and Ping Zhong},
  doi          = {10.1016/j.patcog.2025.112092},
  journal      = {Pattern Recognition},
  month        = {2},
  pages        = {112092},
  shortjournal = {Pattern Recognition},
  title        = {Separate to generalization: Two-branch feature separation framework for generalized underwater image restoration},
  volume       = {170},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). DPL: Dual-prior learning for weakly-supervised semantic segmentation in driving scenes. <em>PR</em>, <em>170</em>, 112090. (<a href='https://doi.org/10.1016/j.patcog.2025.112090'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Weakly supervised semantic segmentation (WSSS) with image-level labels has demonstrated significant potential but encounters substantial challenges in complex driving scenes. These challenges arise from the unique characteristics of urban environments, such as overlapping objects, significant scale variations, and complex spatial relationships. Recent advances in foundation models have shown promise, with CLIP enabling semantic understanding without pixel-level annotations and SAM demonstrating strong boundary perception capabilities. However, existing methods that utilize these models often fail to effectively integrate their complementary strengths for driving scenes understanding. To address these limitations, we propose Dual-Prior Learning (DPL), a novel framework that effectively integrates CLIP’s semantic guidance and SAM’s boundary perception as complementary priors. Our framework introduces three key components: 1) a Semantic Text-aware Multi-scale Perception (STMP) module that integrates CLIP text embeddings for effective multi-scale feature learning, 2) a dual-region learning mechanism that adaptively optimizes consistent and inconsistent regions through dynamic weight balancing, and 3) a SAM boundary supervision scheme that provides precise guidance through iterative weight adjustment. Extensive experiments on Cityscapes, CamVid, and WildDash2 validation sets demonstrate that our method achieves state-of-the-art performance, outperforming previous best results by significant margins of 19.7 %, 25.0 %, and 17.0 % mIoU, respectively, highlighting its substantial advances in driving scenes understanding. The code is available at https://github.com/shijueganzhi/DPL .},
  archive      = {J_PR},
  author       = {Yongqiang Li and Chuanping Hu and Kai Ren and Hao Xi and Jinhao Fan},
  doi          = {10.1016/j.patcog.2025.112090},
  journal      = {Pattern Recognition},
  month        = {2},
  pages        = {112090},
  shortjournal = {Pattern Recognition},
  title        = {DPL: Dual-prior learning for weakly-supervised semantic segmentation in driving scenes},
  volume       = {170},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Incremental rotation averaging revisited. <em>PR</em>, <em>170</em>, 112089. (<a href='https://doi.org/10.1016/j.patcog.2025.112089'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In order to further advance the accuracy and robustness of the incremental parameter estimation-based rotation averaging methods, in this paper, a new member of the Incremental Rotation Averaging (IRA) family is introduced, which is termed as IRAv4. As its most significant feature, a task-specific connected dominating set is extracted in IRAv4 to serve as a more reliable and accurate reference for rotation local-to-global alignment. This alignment reference is incrementally constructed, together with the absolute rotations of the vertices belong to it simultaneously estimated. Comprehensive evaluations are performed on both synthetic and real-world datasets, by which the effectiveness of both the reference construction method and the entire rotation averaging pipeline proposed in this paper is demonstrated.},
  archive      = {J_PR},
  author       = {Xiang Gao and Hainan Cui and Yangdong Liu and Shuhan Shen},
  doi          = {10.1016/j.patcog.2025.112089},
  journal      = {Pattern Recognition},
  month        = {2},
  pages        = {112089},
  shortjournal = {Pattern Recognition},
  title        = {Incremental rotation averaging revisited},
  volume       = {170},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Rethinking 6-DoF grasp detection: A flexible framework for high-quality grasping. <em>PR</em>, <em>170</em>, 112088. (<a href='https://doi.org/10.1016/j.patcog.2025.112088'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Robotic grasping is a fundamental skill for complex tasks and is essential to intelligent behavior. While most prior methods for general 6-DoF grasping focus on extracting scene-level semantic or geometric information, they often neglect adaptability to diverse downstream applications, such as target-oriented grasping. To address this, we rethink 6-DoF grasp detection from a grasp-centric perspective and propose FlexLoG , a versatile framework designed to unify scene-level and target-oriented grasping within a single system. Our framework integrates two core components: the Flexible Guidance Module and the Local Grasp model. The Flexible Guidance Module accommodates both global guidance (e.g., grasp heatmaps) and local guidance (e.g., visual-language grounding), ensuring robust grasp generation across varied task requirements. The Local Grasp model focuses on object-agnostic regional points, enabling precise, localized grasp predictions. Experiments demonstrate that FlexLoG surpasses existing methods, achieving a new state-of-the-art on the GraspNet-1Billion dataset. To validate the practical applicability, we reconstruct the dataset in a simulated digital twin environment, verifying the consistency of performance between simulated and benchmark results. Real-world robotic experiments across diverse scenarios further confirm the framework’s efficacy, achieving a success rate exceeding 90 %.},
  archive      = {J_PR},
  author       = {Pengwei Xie and Siang Chen and Wei Tang and Kaiqin Yang and Guijin Wang},
  doi          = {10.1016/j.patcog.2025.112088},
  journal      = {Pattern Recognition},
  month        = {2},
  pages        = {112088},
  shortjournal = {Pattern Recognition},
  title        = {Rethinking 6-DoF grasp detection: A flexible framework for high-quality grasping},
  volume       = {170},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). USGS: Enhancing sparse view synthesis with unseen viewpoint regularization in 3D gaussian splatting. <em>PR</em>, <em>170</em>, 112087. (<a href='https://doi.org/10.1016/j.patcog.2025.112087'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Neural Radiance Fields (NeRF) faces the challenge of slow inference and loss of 3D scene information in sparse input views. This paper proposes a regularization framework based on 3D Gaussian Splatting (3DGS), called USGS, which enables rapid inference under sparse input views while maintaining ultra-high rendering quality. Our method does not require any pre-trained models or deep prior knowledge. By employing regularization methods for unseen view Gaussians, the model can better learn 3D spatial information under sparse input views and effectively remove Gaussian ellipsoids with excessively high opacity near the camera. Furthermore, in order to prevent the 3DGS model from prematurely pruning 3D Gaussians that are crucial for depth and geometric information early in the training process, we proposed an annealing pruning strategy to prevent the loss of scene information caused by premature removal 3D Gaussians. Extensive experiments on the LLFF, DTU, Mip-NeRF 360, and LF datasets demonstrate that the rendering quality of USGS surpasses state-of-the-art (SOTA) methods while retaining the capability for real-time rendering.},
  archive      = {J_PR},
  author       = {Yao Zhang and Jiangshu Wei and Yuchao Wang and Jiajun Liu},
  doi          = {10.1016/j.patcog.2025.112087},
  journal      = {Pattern Recognition},
  month        = {2},
  pages        = {112087},
  shortjournal = {Pattern Recognition},
  title        = {USGS: Enhancing sparse view synthesis with unseen viewpoint regularization in 3D gaussian splatting},
  volume       = {170},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). WCFE-net: Weight constraint and flick enforcement for improving performance of binary neural networks. <em>PR</em>, <em>170</em>, 112085. (<a href='https://doi.org/10.1016/j.patcog.2025.112085'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Binary Neural Network (BNN) has attracted great attention as it can be efficiently deployed into resource-constrained devices. There still exists a large accuracy gap between BNNs and their full-precision counterparts. Two of the important reasons can be attributed to the difficult weight updating and frequent weight flipping during BNNs’ training. In this paper, one important observation is that solely considering one of the problems is insufficient due to they simultaneously exist and may mutually convert. Moreover, we found that the learning rate decay is helpful to elevate the accuracy of BNN and will dynamically change the clusters of weights that are difficult to update and frequently flip. Inspired by the observations, for the first time, the paper proposed a simple yet effective way to simultaneously tackle the difficult weight updating and frequent weight flipping problems, namely WCFE-Net, in which the w eight c onstraint and f lick e nforcement mechanisms are elaborated with special consideration of learning rate decay. Experiments are conducted on CIFAR-10 and ImageNet datasets to verify the effectiveness of the proposed WCFE-Net on the basis of classical models. The results show that the proposed WCFE-Net can not only elevate the accuracy but also ensure the training stability of BNNs and outperforms the state-of-the-art methods. The code is available at: https://github.com/Wang-Shuhuai/WCFE-Net .},
  archive      = {J_PR},
  author       = {Songwei Pei and Xiangshun Zhao and Mingyu Yuan},
  doi          = {10.1016/j.patcog.2025.112085},
  journal      = {Pattern Recognition},
  month        = {2},
  pages        = {112085},
  shortjournal = {Pattern Recognition},
  title        = {WCFE-net: Weight constraint and flick enforcement for improving performance of binary neural networks},
  volume       = {170},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). TPFS: A three-phase heuristic feature selection algorithm for large-scale sample datasets. <em>PR</em>, <em>170</em>, 112084. (<a href='https://doi.org/10.1016/j.patcog.2025.112084'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Feature selection is a critical technique in machine learning, widely applied across diverse real-world scenarios. However, practical datasets often comprise vast numbers of samples, posing significant challenges for memory-constrained systems. Thus, developing an efficient algorithm for selecting relevant features from large-scale sample datasets under such constraints is essential. Existing feature selection methods frequently face limitations in both scalability and effectiveness in these contexts. This article proposes a novel three-phase heuristic feature selection algorithm, termed TPFS, designed to address the challenges of large-scale sample datasets. The first phase, neighborhood compression, identifies and retains the most representative samples by delimiting adaptive neighborhoods, effectively reducing the dataset’s size. The second and third phases, hypergraph initialization and whale optimization, construct a hypergraph to model higher-order feature relationships and integrate a dual-mode design and a dynamic probability-driven ternary classifier system within the optimization process. These procedures replace the random initialization of solution populations and the optimization of single classifier, enhancing the quality of the selected feature subsets. Extensive experiments demonstrate that TPFS consistently outperforms state-of-the-art algorithms, particularly in processing large-scale sample datasets effectively.},
  archive      = {J_PR},
  author       = {Haoran Su and Jinkun Chen},
  doi          = {10.1016/j.patcog.2025.112084},
  journal      = {Pattern Recognition},
  month        = {2},
  pages        = {112084},
  shortjournal = {Pattern Recognition},
  title        = {TPFS: A three-phase heuristic feature selection algorithm for large-scale sample datasets},
  volume       = {170},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). A hybrid dual-augmentation constraint framework for single-source domain generalization in medical image segmentation. <em>PR</em>, <em>170</em>, 112082. (<a href='https://doi.org/10.1016/j.patcog.2025.112082'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Single-source domain generalization in medical image segmentation exhibits promising prospects for clinical applications, yet suffers from domain shifts. Learning domain-invariant representations is crucial for resisting domain shifts, which current augmentation and domain-invariant feature learning strive for implicitly or explicitly. Despite their common successes, sufficient and explicit representation learning incorporating properties of medical images still deserves exploration. Motivated by observations of limited stylized variation, small inter-class difference, and similar anatomical structure in medical images, a hybrid Dual-augmentation Constraint Framework (DCON) is proposed for effective domain-invariant representation learning. Our dual-view asymmetric augmentation utilizes diverse image-level and feature-level perturbations, including a global-local stylized augmentation with controllability. Bilevel contrastive learning performs two constraints: object-focused consistency between dual views and cross-individual constraint across images simultaneously to capture invariant semantic features. DCON outperforms existing advanced methods on three datasets for single-source domain generalization. Our source code is available at https://github.com/wrf-nj/DCON .},
  archive      = {J_PR},
  author       = {Ruofan Wang and Jintao Guo and Jian Zhang and Lei Qi and Qian Yu and Yinghuan Shi},
  doi          = {10.1016/j.patcog.2025.112082},
  journal      = {Pattern Recognition},
  month        = {2},
  pages        = {112082},
  shortjournal = {Pattern Recognition},
  title        = {A hybrid dual-augmentation constraint framework for single-source domain generalization in medical image segmentation},
  volume       = {170},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). GKC-net: Gated KAN with channel-position attention mechanism for image deraining. <em>PR</em>, <em>170</em>, 112080. (<a href='https://doi.org/10.1016/j.patcog.2025.112080'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Image deraining presents a significant challenge within the field of image processing, and recent advancements indicate that Transformer-based models exhibit superior performance compared to conventional CNNs. However, the self-attention mechanism in Transformers suffers from quadratic computational complexity, resulting in high inference time and their inability to simultaneously capture both precise global context and fine local details, which are essential for accurately restoring degraded image content. In response to these challenges, we propose a novel architecture called GKC-Net, which integrates a gated KAN module with a Channel-Position Attention Mechanism (CPAM). Significantly, GKC-Net achieves linear growth in computational complexity while preserving global context modeling capability, thereby completely eliminating the inherent quadratic complexity of Transformers. This enables high-resolution image restoration tasks to maintain competitive performance while significantly reducing computational overhead.Specifically, we propose the Gated KAN module, which combines the KAN network with a gating mechanism. KAN enhances computational efficiency and scalability via group-wise parameter sharing and variance-preserving initialization. The gating mechanism dynamically regulates information flow based on input features, mitigating over-smoothing and preserving critical details, thereby boosting model expressiveness while reducing computational cost. Additionally, we introduce the CPAM, which emphasizes both channel and positional information, thereby enhancing detail capture in deraining and achieving superior performance in rain removal tasks. Our proposed GKC-Net achieves state-of-the-art performance across several image restoration tasks, including image deraining, image dehazing, and image motion deblurring. This further highlights the superiority of our approach in diverse restoration scenarios and its robust adaptability to various image restoration challenges.},
  archive      = {J_PR},
  author       = {Mengsi Gong and Jilin Yu and Zhiwen Wang and Senlin Chi},
  doi          = {10.1016/j.patcog.2025.112080},
  journal      = {Pattern Recognition},
  month        = {2},
  pages        = {112080},
  shortjournal = {Pattern Recognition},
  title        = {GKC-net: Gated KAN with channel-position attention mechanism for image deraining},
  volume       = {170},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Dual-level aggregation network for video-based visible-infrared group re-identification. <em>PR</em>, <em>170</em>, 112079. (<a href='https://doi.org/10.1016/j.patcog.2025.112079'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Visible-infrared group re-identification (V-I G-ReID) aims to identify a group of people across disparate camera systems with varying imaging modalities. While existing methods primarily focus on image-based data, their performance still falls short. Given the proven benefits of video data in ReID, there’s a significant opportunity to enhance accuracy by exploring video-to-video matching techniques. In this paper, we propose the Dual-level Aggregation Network for video-based V-I G-ReID, where the Dual-level Aggregation Module first aggregates person-level features into group-level features and ultimately into tracklet features. Additionally, an Asymptotic Modality Learning strategy is developed to mitigate modality discrepancies by gradually guiding two modalities to merge into a unified modality. To facilitate research on video-based V-I G-ReID, we create a new large-scale dataset VVIG that moves closer to practical application, which contains 1,030,916 images and 27,709 tracklets of 427 groups. Extensive experiments on VVIG demonstrate the effectiveness of our proposed method. The code is available at https://github.com/WhollyOat/DAN .},
  archive      = {J_PR},
  author       = {Jianghao Xiong and Xiaohua Xie and Jian-Huang Lai},
  doi          = {10.1016/j.patcog.2025.112079},
  journal      = {Pattern Recognition},
  month        = {2},
  pages        = {112079},
  shortjournal = {Pattern Recognition},
  title        = {Dual-level aggregation network for video-based visible-infrared group re-identification},
  volume       = {170},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Multi-hop graph structural modeling for cancer-related circRNA-miRNA interaction prediction. <em>PR</em>, <em>170</em>, 112078. (<a href='https://doi.org/10.1016/j.patcog.2025.112078'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A substantial body of research indicates that circRNA can act as a sponge to absorb miRNA, thereby regulating the development of cancers. Existing circRNA-miRNA interactions (CMIs) prediction models mainly focus on single features and local structures of molecules, making it difficult to fully describe the overall properties of molecules and overlooking the multi-hierarchical associations between them. To address these challenges, we propose a computational model named GraCMI based on multi-hop graph structural modeling, which predicts CMIs by integrating structural and attribute information of molecules. GraCMI learns the representation of molecules in multi-level neighborhoods through constructing heterogeneous networks and performing high- and low-order matrix factorization. GraCMI captures both the intrinsic properties and global structures of molecules, extracting and fusing multi-source features, improving prediction accuracy. In the case studies, 7 out of the top 10 CMI pairs predicted using GraCMI on a real cancer-related dataset were confirmed. Additionally, GraCMI demonstrates a competitive advantage on two other classic datasets. Overall, the experimental results show that GraCMI can effectively predict CMIs, which is expected to provide new insights into future miRNA-mediated circRNA regulation of cancer development.},
  archive      = {J_PR},
  author       = {Mengmeng Wei and Lei Wang and Xiaorui Su and Bowei Zhao and Zhuhong You},
  doi          = {10.1016/j.patcog.2025.112078},
  journal      = {Pattern Recognition},
  month        = {2},
  pages        = {112078},
  shortjournal = {Pattern Recognition},
  title        = {Multi-hop graph structural modeling for cancer-related circRNA-miRNA interaction prediction},
  volume       = {170},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Improving adversarial transferability and imperceptibility with loss landscape and diffusion model. <em>PR</em>, <em>170</em>, 112076. (<a href='https://doi.org/10.1016/j.patcog.2025.112076'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep neural networks (DNNs) are vulnerable to adversarial examples, which introduce imperceptible perturbations on benign samples to mislead the prediction of DNNs. Transferability is a key property of adversarial examples, which enables adversarial examples crafted for one network to deceive other networks with high probability. However, the adversarial perturbations introduced by transferable attacks are perceptible to human observers. Although there are unrestricted attacks which can achieve good visual imperceptibility, the adversarial transferability of these attacks remains relatively low. In this paper, we propose to improve adversarial transferability and imperceptibility of adversarial examples via flat loss landscape and diffusion models. Specifically, we utilize denoising diffusion implicit model (DDIM) inversion operation to map the input image back to the diffusion latent space. Then we add perturbations on the diffusion latent space to achieve successful attacks on the surrogate model and flat input loss landscape, resulting in high adversarial transferability and imperceptible perturbations to human observers. Extensive experiments demonstrate that our proposed method enhances adversarial transferability while preserving the imperceptibility of the generated adversarial examples.},
  archive      = {J_PR},
  author       = {Jiayang Liu and Weiming Zhang and Han Fang and Wenbo Zhou and Ee-Chien Chang and Siew-Kei Lam},
  doi          = {10.1016/j.patcog.2025.112076},
  journal      = {Pattern Recognition},
  month        = {2},
  pages        = {112076},
  shortjournal = {Pattern Recognition},
  title        = {Improving adversarial transferability and imperceptibility with loss landscape and diffusion model},
  volume       = {170},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Enhancing outdoor vision: Binocular desnowing with dual-stream temporal transformer. <em>PR</em>, <em>170</em>, 112075. (<a href='https://doi.org/10.1016/j.patcog.2025.112075'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Video desnowing, aimed at removing snowflakes and enhancing the quality of videos, is a crucial yet intricate task essential for improving the effectiveness of outdoor vision systems. Compared to rain and haze, the inherent opacity and diverse morphology of snowflakes result in more pronounced background occlusions, thereby challenging the efficacy of current desnowing techniques, particularly those focusing solely on images or videos captured from a monocular perspective. To address these challenges, this paper proposes a Dual-Stream Temporal Transformer (DSTT) to advance snow removal and visual enhancement by leveraging comprehensive information from stereo views and spatial-temporal cues. More specifically, it incorporates a Dual-Stream Weight-shared Transformer (DSWT) module to exploit spatial information from different views. This module employs a hierarchical weight-sharing strategy to extract fused spatial features across different views from low-level to high-level layers. Subsequently, the Dual-Stream ConvLSTM (DS-CLSTM) module is introduced to capture temporal correlations across streaming frames. By combining temporal-spatial cues and complementary details from diverse views, videos can be effectively restored while preserving the original content’s details. In addition, two binocular snowy datasets – SnowKITTI2012 and SnowKITTI 2015 – are presented, providing a valuable resource for evaluating the binocular desnowing task. Comprehensive experiments evaluated on both synthetic and real-world snowy datasets demonstrate that our proposed method outperforms the state-of-the-art baselines.},
  archive      = {J_PR},
  author       = {En Yu and Jie Lu and Kaihao Zhang and Guangquan Zhang},
  doi          = {10.1016/j.patcog.2025.112075},
  journal      = {Pattern Recognition},
  month        = {2},
  pages        = {112075},
  shortjournal = {Pattern Recognition},
  title        = {Enhancing outdoor vision: Binocular desnowing with dual-stream temporal transformer},
  volume       = {170},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Mixture of coarse and fine-grained prompt tuning for vision-language model. <em>PR</em>, <em>170</em>, 112074. (<a href='https://doi.org/10.1016/j.patcog.2025.112074'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Visual Language Models (VLMs) exhibit impressive performance across various tasks but often suffer from degradation of prior knowledge when transferred to downstream tasks with limited computational samples. Prompt tuning methods emerge as an effective solution to mitigate this issue. However, most existing approaches solely rely on coarse-grained text prompt or fine-grained text prompt, which may limit the discriminative and generalization capabilities of VLMs. To address these limitations, we propose Mixture of Coarse and Fine-grained Prompt Tuning (MCFPT) , a novel method that integrates both coarse and fine-grained prompts to enhance the performance of VLMs. Inspired by the Mixture-of-Experts (MoE) mechanism, MCFPT incorporates a Mixed Fusion Module (MFM) to fuse and select coarse domain-shared text feature and fine-grained category-discriminative text feature to get the mixed feature. Additionally, a Dynamic Refinement Adapter (DRA) is introduced to adjust category distributions, ensuring consistency between refined and mixed text features. These components collectively improve the generalization and discriminative power of VLMs. Extensive experiments across four scenarios-base-to-new, few-shot classification, domain generalization, and cross-domain classification-demonstrate that MCFPT achieves exceptional performance compared to state-of-the-art methods, with significant improvements in HM scores across multiple datasets. Our findings highlight MCFPT as a robust approach for improving the adaptability and efficiency of Visual Language Models in diverse application domains.},
  archive      = {J_PR},
  author       = {Yansheng Gao and Zixi Zhu and Shengsheng Wang},
  doi          = {10.1016/j.patcog.2025.112074},
  journal      = {Pattern Recognition},
  month        = {2},
  pages        = {112074},
  shortjournal = {Pattern Recognition},
  title        = {Mixture of coarse and fine-grained prompt tuning for vision-language model},
  volume       = {170},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). High-fidelity 3D reconstruction via unified NeRF-mesh optimization with geometric and color consistency. <em>PR</em>, <em>170</em>, 112071. (<a href='https://doi.org/10.1016/j.patcog.2025.112071'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {NeRF-based methods excel in generating realistic volumetric representations but often struggle with creating high-quality meshes, which are crucial for applications requiring explicit surface models. Existing approaches train NeRF and mesh generation networks separately, which limits their ability to learn jointly and leads to spatial and visual misalignment. To address the problem, we propose a novel unified optimization method that collaboratively learns NeRF and colored mesh representations for enhancing 3D reconstruction from monocular RGB images. Our approach employs Triplane features to generate Signed Distance Fields guided by density inputs, which are processed through a differentiable iso-surface extraction module. To ensure consistency between the NeRF and Mesh representations, we introduce a novel cross-photometric loss comparing rasterized outputs with the volumetric rendered images for unified learning. Additionally, we utilize a face-aware chamfer distance and a color loss based on barycentric interpolation, enabling uniform point sampling and improved vertex- and face-level alignment. This enhances geometric accuracy and ensures precise color transfer from source images to the mesh. Our model is trained using 3D ground-truth data and 2D multi-view images, enabling a unified optimization pipeline. Comprehensive experiments demonstrate that our approach outperforms existing approaches in colorized mesh generation, demonstrating its effectiveness and potential for practical applications. On the Google Scanned Objects dataset, our method outperforms the previous state-of-the-art by achieving a 26.33% higher PSNR (24.76 vs. 19.60), a 43.56% lower LPIPS (0.092 vs. 0.163), and a 79.25% lower chamfer distance (0.011 vs. 0.053), demonstrating superior visual and geometric fidelity.},
  archive      = {J_PR},
  author       = {Rama Bastola Neupane and Kan Li and Zhuqing Mao},
  doi          = {10.1016/j.patcog.2025.112071},
  journal      = {Pattern Recognition},
  month        = {2},
  pages        = {112071},
  shortjournal = {Pattern Recognition},
  title        = {High-fidelity 3D reconstruction via unified NeRF-mesh optimization with geometric and color consistency},
  volume       = {170},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). MDGP-forest: A novel deep forest for multi-class imbalanced learning based on multi-class disassembly and feature construction enhanced by genetic programming. <em>PR</em>, <em>170</em>, 112070. (<a href='https://doi.org/10.1016/j.patcog.2025.112070'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Class imbalance is a significant challenge in the field of machine learning. Due to factors such as quantity differences and feature overlap among classes, the imbalance problem for multiclass classification is more difficult than that for binary one, which leads to the existing research primarily focusing on the binary classification scenario. This study proposes a novel deep forest algorithm with the aid of Genetic Programming (GP), MDGP-Forest, for the multiclass imbalance problem. MDGP-Forest utilizes Multi-class Disassembly and undersampling based on instance hardness between layers to obtain multiple binary classification datasets, each corresponding to a GP population for feature construction. The improved fitness function of GP assesses the incremental importance of the constructed features for enhanced vectors, introducing higher-order information into subsequent layers to improve predicted performance. Each GP population generates a set of new features that improve the separability of classes, empowering MDGP-Forest with the capability to address the challenge of overlapping features among multiple classes. We thoroughly evaluate the classification performance of MDGP-Forest on 35 datasets. The experimental results demonstrate that MDGP-Forest significantly outperforms existing methods in addressing multiclass imbalance problems, exhibiting superior predictive performance.},
  archive      = {J_PR},
  author       = {Zhikai Lin and Yong Xu and Kunhong Liu and Liyan Chen},
  doi          = {10.1016/j.patcog.2025.112070},
  journal      = {Pattern Recognition},
  month        = {2},
  pages        = {112070},
  shortjournal = {Pattern Recognition},
  title        = {MDGP-forest: A novel deep forest for multi-class imbalanced learning based on multi-class disassembly and feature construction enhanced by genetic programming},
  volume       = {170},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Coupled tensor train decomposition in federated learning. <em>PR</em>, <em>170</em>, 112067. (<a href='https://doi.org/10.1016/j.patcog.2025.112067'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Tensor decomposition (TD) has been recently revisited in the federated learning (FL) context as a means of unsupervised feature extraction that has the inherent ability to share features common to the agents and preserve the privacy of those that are agent-specific, through appropriate coupling. The canonical polyadic decomposition (CPD) model is almost invariably adopted in the federated TD literature. Instead, this paper proposes a coupled tensor train (CTT) FL framework, which, compared with CPD, offers improved feature extraction capabilities and enjoys increased accuracy and stability. Moreover, the estimation of the model ranks is easier. The proposed CTT approach is instantiated for two fundamental network structures: master–slave and decentralized. Simulation results on synthetic and real datasets demonstrate that the proposed method attains the objectives of an FL environment while outperforming existing alternatives and with practically no loss in learning performance incurred from its distributed character.},
  archive      = {J_PR},
  author       = {Xiangtao Zhang and Eleftherios Kofidis and Ruituo Wu and Ce Zhu and Le Zhang and Yipeng Liu},
  doi          = {10.1016/j.patcog.2025.112067},
  journal      = {Pattern Recognition},
  month        = {2},
  pages        = {112067},
  shortjournal = {Pattern Recognition},
  title        = {Coupled tensor train decomposition in federated learning},
  volume       = {170},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Subspace-orbit randomized algorithms for low rank approximations of third-order tensors in t-product format. <em>PR</em>, <em>170</em>, 112066. (<a href='https://doi.org/10.1016/j.patcog.2025.112066'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper focuses on computing low rank approximations of third-order tensors in the t-product format using random techniques. Given a truncated term K , we derive randomized algorithms for approximating the K -term t-SVD, which is called as the subspace-orbit randomized t-SVD (sort-SVD). Additionally, we conduct an analysis of the deterministic and probabilistic error bounds of the proposed algorithm, subject to certain assumptions. We integrate the present algorithm with the power method to enhance the accuracy of the approximate the K -term t-SVD. Furthermore, we demonstrate the effectiveness of our algorithms through numerous numerical examples. Lastly, the proposed algorithms are employed to compress data tensors from various image databases.},
  archive      = {J_PR},
  author       = {Xuezhong Wang and Kai Wang and Changxin Mo},
  doi          = {10.1016/j.patcog.2025.112066},
  journal      = {Pattern Recognition},
  month        = {2},
  pages        = {112066},
  shortjournal = {Pattern Recognition},
  title        = {Subspace-orbit randomized algorithms for low rank approximations of third-order tensors in t-product format},
  volume       = {170},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Class-incremental learning network for real-time anomaly recognition in surveillance environments. <em>PR</em>, <em>170</em>, 112064. (<a href='https://doi.org/10.1016/j.patcog.2025.112064'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The rise in crime rates has become a significant cause of property and life losses, necessitating the development of intelligent video surveillance systems for enhanced monitoring in law enforcement, transportation, and environmental contexts. However, the accurate identification of abnormal activities in real-time video surveillance systems remains a challenging task. Existing surveillance systems struggle with the vast amount of video streaming, making manual 24/7 monitoring impractical and error-prone. Traditional anomaly detection methods often process the entire dataset’s feature set, which can be limiting in complex scenarios, leading to incorrect predictions, especially with challenging patterns or inter-class similarities. Therefore, this paper addresses the limitations of automatic video anomaly recognition systems by developing a vision transformer-based class-incremental learning network (CILAR-Net). The CILAR-Net leverages a vision transformer to extract spatiotemporal features from surveillance video frames, followed by a GRU network for anomaly recognition. The incremental learning approach enables the model to adapt to new classes without retraining. The CILAR-Net is validated on challenging anomaly recognition datasets, including UCF-Crime, LAD-2000, and RWF-2000, showcasing a state-of-the-art performance. Comparative analysis with existing methods demonstrates the effectiveness of CILAR-Net, which achieves an accuracy of 53.03%, 79.07%, and 93.46%, with improvements of 2.03%, 9.67%, and 0.20% from state-of-the-art methods on the UCF-Crime, LAD-2000, and RWF-2000 datasets, respectively. These results highlight the practical advantage and robustness of our method in enhancing anomaly recognition performance across diverse datasets. This article addresses significant research gaps in anomaly recognition by providing a robust and efficient solution for real-world surveillance applications.},
  archive      = {J_PR},
  author       = {Adnan Hussain and Waseem Ullah and Noman Khan and Zulfiqar Ahmad Khan and Hikmat Yar and Sung Wook Baik},
  doi          = {10.1016/j.patcog.2025.112064},
  journal      = {Pattern Recognition},
  month        = {2},
  pages        = {112064},
  shortjournal = {Pattern Recognition},
  title        = {Class-incremental learning network for real-time anomaly recognition in surveillance environments},
  volume       = {170},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Learning relationship-guided vision-language transformer for facial attribute recognition. <em>PR</em>, <em>170</em>, 112063. (<a href='https://doi.org/10.1016/j.patcog.2025.112063'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Facial Attribute Recognition (FAR) remains a challenging task due to the poor quality of visual images. Existing FAR methods generally adopt the Convolutional Neural Network (CNN) to learn deep features of facial images, and attribute relationships are either obtained by a fixed clustering algorithm or manually grouped. In this paper, we propose a novel Relationship-Guided Vision-Language Transformer, termed RVLT, for FAR. The RVLT method can automatically learn attribute relationships from the linguistic modality, which is used to guide image feature extraction. Specifically, each vision-language Transformer encoder adopts an Image-Text Cross-Attention (ITCA), which is composed of an Image-to-Text Adjustment Attention (ITAA) and a Text-to-Image Guidance Attention (TIGA). The ITAA is used to adjust the text tokens from the linguistic modality to adapt to the visual information, and the TIGA employs the adjusted text tokens as the prior knowledge to guide the distribution of the image embeddings from the visual modality. Moreover, we employ a Token Selection Mechanism (TSM) to reduce the interference factors in the image background, so that the model can pay more attention to the regions related to the face. In addition, an Image-Text Alignment (ITA) loss is used to further align the tokens of the visual and linguistic modalities, and a Text-Aware Classification (TAC) loss is leveraged to ensure the correctness of the attribute relationships learned from the raw text as much as possible. Different from directly using the image embeddings for feature extraction, our method can leverage the attribute relationships automatically learned from the raw text to effectively highlight image features relevant to facial attributes. Experimental results demonstrate that the proposed RVLT achieves 87.34% and 91.80% accuracy on LFWA and CelebA, respectively. In the case of limited labeled data, RVLT outperforms the second-best FAR method by 1.76% and 0.25% accuracy under only 5% LFWA and 0.5% CelebA training data, respectively.},
  archive      = {J_PR},
  author       = {Si Chen and Mingxuan Lei and Da-Han Wang and Xu-Yao Zhang and Yan Yan and Shunzhi Zhu},
  doi          = {10.1016/j.patcog.2025.112063},
  journal      = {Pattern Recognition},
  month        = {2},
  pages        = {112063},
  shortjournal = {Pattern Recognition},
  title        = {Learning relationship-guided vision-language transformer for facial attribute recognition},
  volume       = {170},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Domain adaptive video summarization using generalized transformer. <em>PR</em>, <em>170</em>, 112059. (<a href='https://doi.org/10.1016/j.patcog.2025.112059'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Video summarization aims to extract salient segments from videos to construct concise and comprehensive synopses. Despite significant advancements, the diversity of video content and the constraint of limited training data pose challenges when applying trained models to new scenarios, often resulting in the domain shift problem. To address this challenge, we propose a domain adaptation framework tailored to video summarization from two aspects: (a) enhancing the generalization ability and (b) improving the adaptive ability of video summarization models. Specifically, we design a simple yet effective regularized feature encoder based on Transformer, where an averaging operation on attention weights serves as a form of regularization. This method mitigates overfitting to domain-specific cues and encourages the learning of more generalizable representations across diverse domains. Furthermore, we introduce a novel discrepancy reduction loss that aligns the distribution of inter-frame feature similarities and inter-frame prediction similarities, combined with a confidence weighting strategy, to adapt the regularized encoder to target domains and mitigate domain shift. Extensive experiments on multiple benchmark datasets demonstrate the effectiveness of our method. Our method achieves state-of-the-art performance under various settings on TVSum and SumMe, and obtains the best results on the transfer setting of Mr.HiSum.},
  archive      = {J_PR},
  author       = {Ziyi Wang and Yubo Zhu and Xinxiao Wu},
  doi          = {10.1016/j.patcog.2025.112059},
  journal      = {Pattern Recognition},
  month        = {2},
  pages        = {112059},
  shortjournal = {Pattern Recognition},
  title        = {Domain adaptive video summarization using generalized transformer},
  volume       = {170},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Text-guided camouflaged object detection. <em>PR</em>, <em>170</em>, 112058. (<a href='https://doi.org/10.1016/j.patcog.2025.112058'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Camouflaged object detection (COD) aims to identify and segment objects hidden in the background due to their high similarity in colour or texture. Recent efforts have investigated the utilization of foundation models to provide extra supervision for solving this task, such as object masks predicted by Segment Anything Models and labelled camouflaged object images generated by Stable Diffusion Models. In this work, instead of visual supervision, we endeavour to utilize Multimodal Large Language Models (MLLM) to provide textual information for COD. Specifically, we propose the Text-Guided Camouflaged Object Detection (TG-COD) framework, which consists of two main stages: extracting pseudo-textual description from MLLM and integrating textual information in the COD process. Our framework leverages the knowledge of MLLM and guides the detection process with textual information. Comprehensive experiments demonstrate that our method achieves state-of-the-art (SOTA) performance and also exhibits strong generalization capability in a few-shot setting.},
  archive      = {J_PR},
  author       = {Zefeng Chen and Yunqi Xue and Zhijiang Li and Philip Torr and Jindong Gu},
  doi          = {10.1016/j.patcog.2025.112058},
  journal      = {Pattern Recognition},
  month        = {2},
  pages        = {112058},
  shortjournal = {Pattern Recognition},
  title        = {Text-guided camouflaged object detection},
  volume       = {170},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Multi-head graph contrastive learning with hop augmentation for node classification. <em>PR</em>, <em>170</em>, 112055. (<a href='https://doi.org/10.1016/j.patcog.2025.112055'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Graph contrastive learning (GCL) extracts informative representations from graphs with limited or no labels. For node classification, augmented views of a node are treated as positive pairs with high similarity. While current augmentation methods often disrupt semantic and structural properties, we propose a hop augmentation technique based on the homophily assumption. This method uses multi-hop neighborhood information to create multiple views from the network’s multi-head outputs, enhancing GCL by iteratively expanding node neighborhoods. It captures comprehensive graph information in sub-feature spaces without structural distortion, leveraging the insight that connected nodes share labels and features. Experiments on diverse benchmarks demonstrate superior node classification performance compared to state-of-the-art GCL methods, even with minimal annotations. The code will be available at https://github.com/MinhZou/MHGCL-HA .},
  archive      = {J_PR},
  author       = {Minhao Zou and Yutong Wang and Xiaofeng Meng and Zhongxue Gan and Chun Guan and Siyang Leng},
  doi          = {10.1016/j.patcog.2025.112055},
  journal      = {Pattern Recognition},
  month        = {2},
  pages        = {112055},
  shortjournal = {Pattern Recognition},
  title        = {Multi-head graph contrastive learning with hop augmentation for node classification},
  volume       = {170},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). GS-net: Point cloud sampling with graph neural networks. <em>PR</em>, <em>170</em>, 112054. (<a href='https://doi.org/10.1016/j.patcog.2025.112054'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Point clouds are critical for applications like autonomous driving and robotics but face challenges from their massive scale and unstructured nature. To address this, we propose GS-Net, a novel G raph Neural Network(GNN)-based S ampling Net work. By leveraging graph-based message passing and jumping knowledge connections, GS-Net effectively captures and fuses multi-scale local geometric structures. Unlike conventional methods neglecting task-specific sampling requirements, GS-Net directly learns task-oriented sampling criteria from downstream models, ensuring the sampled point sets preserve essential geometric features. Extensive experiments on classification, retrieval, and registration tasks across diverse datasets—including ModelNet40, ScanObjectNN, and our newly introduced SemanticKITTI-cls—demonstrate GS-Net’s superior performance compared to existing sampling methods. Remarkably, GS-Net achieves 62.32% classification accuracy on ModelNet40 using only 8 sampled points. Ablation studies confirm GS-Net’s robustness and identify current limitations, providing insights for future work. The source code is available at: https://github.com/chenxl124578/GS-Net .},
  archive      = {J_PR},
  author       = {Xiaolei Chen and Jie Chen and Shoumeng Qiu and Xiangyang Xue and Jian Pu},
  doi          = {10.1016/j.patcog.2025.112054},
  journal      = {Pattern Recognition},
  month        = {2},
  pages        = {112054},
  shortjournal = {Pattern Recognition},
  title        = {GS-net: Point cloud sampling with graph neural networks},
  volume       = {170},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). TGFormer: Towards temporal graph transformer with auto-correlation mechanism. <em>PR</em>, <em>170</em>, 112053. (<a href='https://doi.org/10.1016/j.patcog.2025.112053'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The growing interest in Temporal Graph Neural Networks (TGNNs) stems from their ability to model complex dynamics and deliver superior performance. However, TGNNs encounter fundamental challenges in capturing long-term dependencies and identifying periodic patterns. To address these limitations, we propose TGFormer, a novel Transformer architecture specifically designed for temporal graphs. Our model redefines temporal graph learning by establishing a trajectory framework that aligns with time series analysis principles. This approach allows TGFormer to derive node representations through systematic analysis of historical interactions, enabling granular examination of node relationships across sequential timestamps. Building upon stochastic process theory, we develop an auto-correlation mechanism that systematically uncovers periodic dependencies in node interactions. This innovation empowers TGFormer to perform dependency discovery and representation aggregation at sub-interaction levels, demonstrating superior efficiency and accuracy compared to conventional attention mechanisms. Experimental validation across six public benchmarks confirms the effectiveness of our approach, with TGFormer at most achieving 9.35% precision improvement compared to state-of-the-art approaches.},
  archive      = {J_PR},
  author       = {Hongjiang Chen and Pengfei Jiao and Ming Du and Xuan Guo and Zhidong Zhao and Di Jin and Xiao Liu},
  doi          = {10.1016/j.patcog.2025.112053},
  journal      = {Pattern Recognition},
  month        = {2},
  pages        = {112053},
  shortjournal = {Pattern Recognition},
  title        = {TGFormer: Towards temporal graph transformer with auto-correlation mechanism},
  volume       = {170},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Verifiably robust conformal prediction for probabilistic guarantees under adversarial attacks. <em>PR</em>, <em>170</em>, 112051. (<a href='https://doi.org/10.1016/j.patcog.2025.112051'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Conformal Prediction (CP) is a popular uncertainty quantification method that provides distribution-free, statistically valid prediction sets, assuming that training and test data are exchangeable. In such a case, CP’s prediction sets are guaranteed to cover the (unknown) true test output with a user-specified probability. Nevertheless, this guarantee is violated when the data is subjected to adversarial attacks, which often result in a significant loss of coverage. Recently, several approaches have been put forward to recover CP guarantees in this setting. These approaches leverage variations of randomised smoothing to produce conservative sets which account for the effect of the adversarial perturbations. They are, however, limited in that they only support ℓ 2 -bounded perturbations and classification tasks. This paper introduces VRCP (Verifiably Robust Conformal Prediction) , a new framework that leverages recent neural network verification methods to recover coverage guarantees under adversarial attacks. We also demonstrate how VRCP can be used to mitigate poisoning attacks. Our VRCP method is the first to support perturbations bounded by arbitrary norms including ℓ 1 , ℓ 2 , and ℓ ∞ , as well as regression tasks. We evaluate and compare our approach on image classification tasks (CIFAR10, CIFAR100, and TinyImageNet) and regression tasks for deep reinforcement learning environments. In every case, VRCP achieves above nominal coverage and yields significantly more efficient and informative prediction regions than the SotA.},
  archive      = {J_PR},
  author       = {Linus Jeary and Tom Kuipers and Mehran Hosseini and Nicola Paoletti},
  doi          = {10.1016/j.patcog.2025.112051},
  journal      = {Pattern Recognition},
  month        = {2},
  pages        = {112051},
  shortjournal = {Pattern Recognition},
  title        = {Verifiably robust conformal prediction for probabilistic guarantees under adversarial attacks},
  volume       = {170},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). A multi-scale gate network for high-quality image deblurring. <em>PR</em>, <em>170</em>, 112050. (<a href='https://doi.org/10.1016/j.patcog.2025.112050'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Image deblurring (ID) aims to recover the latent sharp image from its blurred counterpart. Although numerous convolutional neural network (CNN)-based and Transformer-based methods have been developed for ID, most of them handle blurry images at pixel-level and without explicitly introducing structural information. According to our experiments, the patch-averaging operation, which involves average pooling followed by a broadcasting operation, can effectively capture structural information. And we observed that the structural information of blurred images obtained through patch-averaging is highly similar to that of their corresponding clear ground truth (GT) images. Additionally, recent MLP-based networks with gate mechanisms have shown promise with simpler and more efficient architectures. Inspired by these findings, we propose a multi-scale gate network (MSGN) to accurately capture structural information at different scales. This approach aids in restoring both edges and texture details. Specifically, we propose a multi-scale gate block (MGB) to capture and integrate pixel-level features and patch-level structure information by summarizing structural information of each patch into a block with the same size and location via patch-averaging operation. In addition, we develop a structural feed-forward network (SFFN), which further integrates each pixel with the structure information of its corresponding patch. Finally, we present a structural fusion strategy (SFS), in which the MGB and SFFN are used again to fuse the output of encoder with the output of decoder. Extensive experiments compared with existing state-of-the-art ID methods demonstrate that our proposed MSGN can achieve higher quality deblurring results in terms of both subjective vision and quantitative metrics.},
  archive      = {J_PR},
  author       = {Shu Tang and Yufu Lin and Xinbo Gao and Shuli Yang and Jiaxu Leng and Zengdan Pan and Hao Tian},
  doi          = {10.1016/j.patcog.2025.112050},
  journal      = {Pattern Recognition},
  month        = {2},
  pages        = {112050},
  shortjournal = {Pattern Recognition},
  title        = {A multi-scale gate network for high-quality image deblurring},
  volume       = {170},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). MICCAI 2023 STS challenge: A retrospective study of semi-supervised approaches for teeth segmentation. <em>PR</em>, <em>170</em>, 112049. (<a href='https://doi.org/10.1016/j.patcog.2025.112049'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Computer-aided diagnosis greatly enhances personalized treatment planning and diagnostic efficiency by providing accurate dental anatomy through teeth segmentation. However, it still constrained by the scarcity of high-quality annotated dental datasets. To address this issue, this paper presents a dataset combining both 2D panoramic X-rays with over 6500 images and 3D CBCT with over 580 volumes (88,500+ slices) to support the Semi-supervised Teeth Segmentation (STS) Challenge, which includes partially meticulous annotations and covers all age groups. Moreover, multi-phase semi-supervised teeth segmentation algorithms and high-confidence pseudo-labels refinement strategies were proposed by competitors during this challenge. Algorithms were verified on this proposed dataset and good segmentation performance were achieved, over 93+ and 80+ Dice score were obtained for top three 2D and 3D participants, demonstrating the high quality of this proposed dataset. This paper also summarizes the diverse methods employed by the top-ranking teams in the MICCAI 2023 STS Challenge. Our dataset is publicly accessible through Zenodo ( https://zenodo.org/records/10597292 ), and the participants’ code is hosted on GitHub ( https://github.com/ricoleehduu/STS-Challenge ).},
  archive      = {J_PR},
  author       = {Yaqi Wang and Yifan Zhang and Xiaodiao Chen and Shuai Wang and Dahong Qian and Fan Ye and Feng Xu and Hongyuan Zhang and Ruilong Dan and Qianni Zhang and Xingru Huang and Zhao Huang and Jun Liu and Zhiwen Zheng and Chengyu Wu and Yunxiang Li and Zhi Li and Zhean Ma and Weiwei Cui and Shan Luo and Qun Jin},
  doi          = {10.1016/j.patcog.2025.112049},
  journal      = {Pattern Recognition},
  month        = {2},
  pages        = {112049},
  shortjournal = {Pattern Recognition},
  title        = {MICCAI 2023 STS challenge: A retrospective study of semi-supervised approaches for teeth segmentation},
  volume       = {170},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Cross-alignment for efficient visual object tracking. <em>PR</em>, <em>170</em>, 112048. (<a href='https://doi.org/10.1016/j.patcog.2025.112048'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In visual object tracking, the disparity in performance between lightweight and heavyweight trackers presents a significant challenge, particularly for applications on edge devices. We introduce the Cross-Alignment Tracker (CAT), a novel dual-branch framework designed to bridge the performance gap by integrating both lightweight and heavyweight tracking components. During training, CAT processes inputs through two parallel branches, each consisting of a backbone network and a tracking head—one lightweight and the other heavyweight. This design facilitates simultaneous learning and output alignment across branches, thereby enhancing the performance of the lightweight tracker in a coordinated manner. A key aspect of our approach is the adaptation and alignment of outputs from both branches, ensuring that the accuracy and robustness of the heavyweight tracker can effectively guide and improve the learning of its lightweight counterpart. During online tracking, CAT leverages cross-combinations between the two branches to simultaneously generate four distinct one-stream trackers, each with varying levels of computational complexity and parameter size. Empirical evaluations, notably on the GOT-10k benchmark, reveal that CAT-Tiny surpasses existing real-time trackers by 4.8%, approaching the prowess of larger, high-performance models. Remarkably, a single training session yields four distinct model sizes, each tailored for varied tracking demands, showcasing the method’s unparalleled efficiency and scalability in advancing real-time object-tracking technology.},
  archive      = {J_PR},
  author       = {Shilei Wang and Mingjiang Liang and Shaoli Huang and Jifeng Ning and Gong Cheng},
  doi          = {10.1016/j.patcog.2025.112048},
  journal      = {Pattern Recognition},
  month        = {2},
  pages        = {112048},
  shortjournal = {Pattern Recognition},
  title        = {Cross-alignment for efficient visual object tracking},
  volume       = {170},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Graph neural networks with flow conservation constraints for real-time origin–destination matrix completion. <em>PR</em>, <em>170</em>, 112046. (<a href='https://doi.org/10.1016/j.patcog.2025.112046'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Accurate real-time estimation of Origin–Destination (OD) matrices is crucial for effective transportation network management. However, online recording systems often suffer from incomplete data, due to the limited observation intervals that fail to capture long-duration travel units within shorter time frames. This real-time incompleteness severely impacts applications requiring complete OD demand information, as critical flow data is either missing or under-represented. In this study, we propose GNN-ODFill, a novel graph neural network-based framework designed to address this challenge. GNN-ODFill estimates the true OD matrix in real-time by leveraging both incomplete historical data and real-time origin flow information. The model consists of three key components: (1) a pre-imputation module using Graph Attention Networks (GAT) for initial estimation, (2) a historical accumulation module that captures temporal dependencies via Graph Convolutional Networks (GCNs) and Gated Recurrent Units (GRUs), and (3) an OD allocation module that refines the estimates using real-time flow data, ensuring that the sum of origin–destination pairs from each station matches the complete observed origin flow. While previous methods often overlook the impact of real-time incompleteness, GNN-ODFill explicitly addresses this issue by ensuring that the estimated OD matrix is both spatially and temporally coherent. Furthermore, the framework highlights an underexplored gap in existing models: the limited use of flow conservation. Experimental results demonstrate the superior performance of GNN-ODFill in terms of both accuracy compared to existing methods. Additionally, the real-time complete OD matrix estimated by GNN-ODFill significantly improves the accuracy of future OD demand predictions.},
  archive      = {J_PR},
  author       = {Zhongyue Zhang and Jing Yang and Xiaoxu Chen and Yuankai Wu},
  doi          = {10.1016/j.patcog.2025.112046},
  journal      = {Pattern Recognition},
  month        = {2},
  pages        = {112046},
  shortjournal = {Pattern Recognition},
  title        = {Graph neural networks with flow conservation constraints for real-time origin–destination matrix completion},
  volume       = {170},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Uncertainty-aware efficient subgraph isomorphism using graph topology. <em>PR</em>, <em>170</em>, 112044. (<a href='https://doi.org/10.1016/j.patcog.2025.112044'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Subgraph isomorphism, also known as subgraph matching, is typically regarded as an NP-complete problem. This complexity is further compounded in practical applications where edge weights are real-valued and may be affected by measurement noise and potential missing data. Such graph matching routinely arises in applications such as image matching and map matching. Most subgraph matching methods fail to perform node-to-node matching under presence of such corruptions. We propose a method for identifying the node correspondence between a subgraph and a full graph in the inexact case without node labels in two steps - (a) extract the minimal unique topology preserving subset from the subgraph and find its feasible matching in the full graph, and (b) implement a consensus-based algorithm to expand the matched node set by pairing unique paths based on boundary commutativity. To demonstrate the accuracy and sub-linear time complexity of the proposed method, two simulation studies are performed on the Erdos–Renyi random graphs and spatial graphs. Additionally, two case studies are performed on the image-based affine covariant features dataset and KITTI stereo dataset respectively. Going beyond the existing subgraph matching approaches, the proposed method is shown to have realistically sub-linear computational efficiency, robustness to random measurement noise, and good statistical properties. Our method is also readily applicable to the exact matching case without loss of generality.},
  archive      = {J_PR},
  author       = {Arpan Kusari and Wenbo Sun},
  doi          = {10.1016/j.patcog.2025.112044},
  journal      = {Pattern Recognition},
  month        = {2},
  pages        = {112044},
  shortjournal = {Pattern Recognition},
  title        = {Uncertainty-aware efficient subgraph isomorphism using graph topology},
  volume       = {170},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). HSENet: Hierarchical semantic-enriched network for multi-modal image fusion. <em>PR</em>, <em>170</em>, 112043. (<a href='https://doi.org/10.1016/j.patcog.2025.112043'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we propose HSENet, a hierarchical semantic-enriched network capable of generating high-quality fused images with robust global semantic consistency and excellent local detail representation. The core innovation of HSENet lies in its hierarchical enrichment of semantic information through semantic gathering, distribution, and injection. Specifically, the network begins by balancing global information exchange via multi-scale feature aggregation and redistribution while dynamically bridging fusion and segmentation tasks. Following this, a progressive semantic dense injection strategy is introduced, employing dense connections to first inject global semantics into highly consistent infrared features and then propagate the semantic-infrared hybrid features to visible features. This approach effectively enhances semantic representation while minimizing high-frequency information loss. Furthermore, HSENet includes two types of feature fusion modules, to leverage cross-modal attention for more comprehensive feature fusion and utilize semantic features as a third input to further enhance the semantic representation for image fusion. These modules achieve robust and flexible feature fusion in complex scenarios by dynamically balancing global semantic consistency and fine-grained local detail representation. Our approach excels in visual perception tasks while fully preserving the texture features from the source modalities. The comparison experiments of image fusion and semantic segmentation demonstrate the superiority of HSENet in visual quality and semantic preservation. The code is available at https://github.com/Lxyklmyt/HSENet .},
  archive      = {J_PR},
  author       = {Xinyu Liu and Rui Ming and Songlin Du and Lianghua He and Haibo Luo and Guobao Xiao},
  doi          = {10.1016/j.patcog.2025.112043},
  journal      = {Pattern Recognition},
  month        = {2},
  pages        = {112043},
  shortjournal = {Pattern Recognition},
  title        = {HSENet: Hierarchical semantic-enriched network for multi-modal image fusion},
  volume       = {170},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Unveiling hidden vulnerabilities in digital human generation via adversarial attacks. <em>PR</em>, <em>170</em>, 112042. (<a href='https://doi.org/10.1016/j.patcog.2025.112042'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Expressive human pose and shape estimation (EHPS) is crucial for digital human generation, especially in applications like live streaming. While existing research primarily focuses on reducing estimation errors, it largely neglects robustness and security aspects, leaving these systems vulnerable to adversarial attacks. To address this significant challenge, we propose the Tangible Attack (TBA) , a novel framework designed to generate adversarial examples capable of effectively compromising any digital human generation model. Our approach introduces a Dual Heterogeneous Noise Generator (DHNG) , which leverages Variational Autoencoders (VAE) and ControlNet to produce diverse, targeted noise tailored to the original image features. Additionally, we design a custom adversarial loss function to optimize the noise, ensuring both high controllability and potent disruption. By iteratively refining the adversarial sample through multi-gradient signals from both the noise and the state-of-the-art EHPS model, TBA substantially improves the effectiveness of adversarial attacks. Extensive experiments demonstrate TBA’s superiority, achieving a remarkable 41.0% increase in estimation error, with an average improvement of approximately 17.0%. These findings expose significant security vulnerabilities in current EHPS models and highlight the need for stronger defenses in digital human generation systems.},
  archive      = {J_PR},
  author       = {Zhiying Li and Yeying Jin and Fan Shen and Zhi Liu and Weibin Chen and Pengju Zhang and Xiaomei Zhang and Boyu Chen and Michael Shen and Kejian Wu and Zhaoxin Fan and Jin Dong},
  doi          = {10.1016/j.patcog.2025.112042},
  journal      = {Pattern Recognition},
  month        = {2},
  pages        = {112042},
  shortjournal = {Pattern Recognition},
  title        = {Unveiling hidden vulnerabilities in digital human generation via adversarial attacks},
  volume       = {170},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Point2pix-zero: Point-driven refined diffusion for multi-object image editing. <em>PR</em>, <em>170</em>, 112041. (<a href='https://doi.org/10.1016/j.patcog.2025.112041'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Semantic image editing methods employing large-scale diffusion models have made significant strides in precise and controlled image editing with text prompts as guidance. However, these models struggle to handle complex images containing hard-described objects and/or multiple objects. In this work, we introduce a novel inference-time multi-object image editing strategy, Point2pix-Zero, editing a single object with the simple guidance of clicked points and the text of target objects. We employ an interactive methodology, point-discovery, as text-free guidance to identify the semantic information of intended edited objects and generate text prompts automatically. Instead of exploiting internal cross-attention maps of diffusion models as a guide, we inject external attention maps to rectify the visual-and-semantic pairing mismatches in cross-attention maps during the denoising process. Extensive empirical evaluations demonstrate the effectiveness of our proposed inference-time method in ensuring precise editing while maintaining image fidelity. Our method showcases superior performance in single- and multi-object image editing, positioning it as a new state-of-the-art.},
  archive      = {J_PR},
  author       = {Siyuan Wang and Yuyao Yan and Xi Yang and Rui Zhang and Qiufeng Wang and Guangliang Cheng and Kaizhu Huang},
  doi          = {10.1016/j.patcog.2025.112041},
  journal      = {Pattern Recognition},
  month        = {2},
  pages        = {112041},
  shortjournal = {Pattern Recognition},
  title        = {Point2pix-zero: Point-driven refined diffusion for multi-object image editing},
  volume       = {170},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). SAR image change detection via generalized extreme value (GEV) modeling. <em>PR</em>, <em>170</em>, 112040. (<a href='https://doi.org/10.1016/j.patcog.2025.112040'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The rapid growth of high-resolution synthetic aperture radar (SAR) images has created new challenges for change detection methods. High-resolution SAR images often exhibit extremely heterogeneous terrain, resulting in severe long-tailed distributions in the image histogram. Traditional change detection methods based on hypothesis testing theory rely on Gamma distributions, which struggle to accurately model the complex scenes in high-resolution images. Recently the Generalized Extreme Value (GEV) distribution is proven effective in describing the long-tail phenomenon. In this paper, we introduce GEV model into the hypothesis test theory and propose a GEV-based SAR change detection method. First, there may exist two or more heterogeneous components in a certain scene in high-resolution SAR images. We oversegment the image into homogeneous local regions using a superpixel algorithm and model each region with the GEV distribution. Based on this distribution, we then derive the GEV-based likelihood-ratio test (LRT) statistics to measure the similarity of two superpixels for unsupervised change detection. Finally, by analyzing the asymptotic behavior of the GEV-based LRT, we apply a threshold to obtain the change maps (CMs). To evaluate the performance of our approach, we conduct Monte Carlo experiments using empirical data to investigate the goodness-of-fit performance and asymptotic behavior of the LRT. Our method demonstrates superior performance compared to state-of-the-art approaches, achieving the highest overall accuracy in both studied areas.},
  archive      = {J_PR},
  author       = {Fan Zhang and Sijin Zheng and Fei Ma and Qiang Yin and Yongsheng Zhou},
  doi          = {10.1016/j.patcog.2025.112040},
  journal      = {Pattern Recognition},
  month        = {2},
  pages        = {112040},
  shortjournal = {Pattern Recognition},
  title        = {SAR image change detection via generalized extreme value (GEV) modeling},
  volume       = {170},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Dual-regularized mixed spatial attention network for breast MRI classification: Dataset and methodology. <em>PR</em>, <em>170</em>, 112039. (<a href='https://doi.org/10.1016/j.patcog.2025.112039'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Magnetic resonance imaging (MRI) is an effective tool for early detection of breast cancer. To assist physicians in enhancing diagnostic effectiveness and efficiency, a substantial number of machine learning-based methods for breast cancer MRI classification have emerged in recent years. However, existing studies face challenges at both the data and methodology levels. On the data aspect, many of these studies are based on small, private-owned datasets, limiting the practical impact and hindering further research. In addition, while combining Contrast-Enhanced T1-weighted MRI images (CE-T1WI) with their subtraction counterparts (Sub-T1WI) at different post-injection periods is a common clinical practice, there remains a gap in research utilizing such data. From a methodological perspective, most methods apply this mechanism directly and neglect the specific characteristics of the data, resulting in limited performance enhancement. To address these issues, we release a CE-Sub T1WI dataset containing 3,964 MRI images of different modalities and post-injection periods from 991 cases to facilitate research in this field. Additionally, we propose a novel dual-regularized mixed spatial attention network (DRMSA) in this work that incorporates the multi-scale information from both local images and comparison across different post-injection periods. Furthermore, inspired by the attention concentration and dissipation pattern of physicians in MRI analysis, we introduce two novel regularizers to enhance the quality of attention maps. Comprehensive studies against six widely recognized classification baselines and six additional networks that employ attention modules are assessed to compare DRMSA’s performance, highlighting its strengths in classification accuracy and interpretability in breast MRI analysis. The dataset and source code of this work are publicly available 2 .},
  archive      = {J_PR},
  author       = {Zongmo Huang and Ying Jiang and Guowu Yang and Yuting Wang and Wenjing Yang},
  doi          = {10.1016/j.patcog.2025.112039},
  journal      = {Pattern Recognition},
  month        = {2},
  pages        = {112039},
  shortjournal = {Pattern Recognition},
  title        = {Dual-regularized mixed spatial attention network for breast MRI classification: Dataset and methodology},
  volume       = {170},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Factorization model with total variation regularizer for image reconstruction and subgradient algorithm. <em>PR</em>, <em>170</em>, 112038. (<a href='https://doi.org/10.1016/j.patcog.2025.112038'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper concerns the reconstruction of images in which the pixels of images are missing and the observations are corrupted by noise. By leveraging the approximate low-rank and gradient smoothing prior information of images, we propose a factorization model with the total variation (TV) and a weakly convex surrogate of column ℓ 2 , 0 -norm regularizers. This model avoids the computation cost of SVDs required by those models of full matrix variables, and moreover, the TV regularizer accounts for the edge structure of the target image, and the weakly convex surrogate of column ℓ 2 , 0 -norm of factor matrices considers the rough upper estimation for the true rank. For the proposed nonconvex and nonsmooth model, we develop an efficient subgradient algorithm, and prove that any cluster point of its iterate sequence is a stationary point and the cost value sequence converges to a critical value. Numerical experiments are conducted on color images and hyperspectral images with pixel missing and observation that is corrupted by Gaussian or impulse noise. Numerical comparisons with seven state-of-art methods for color image reconstruction and one deep leaning method for hyperspectral image restoration validate the efficiency of the proposed method.},
  archive      = {J_PR},
  author       = {Bujin Li and Shaohua Pan and Yitian Qian},
  doi          = {10.1016/j.patcog.2025.112038},
  journal      = {Pattern Recognition},
  month        = {2},
  pages        = {112038},
  shortjournal = {Pattern Recognition},
  title        = {Factorization model with total variation regularizer for image reconstruction and subgradient algorithm},
  volume       = {170},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). High-frequency structure transformer for magnetic resonance image super-resolution. <em>PR</em>, <em>170</em>, 112037. (<a href='https://doi.org/10.1016/j.patcog.2025.112037'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Magnetic Resonance (MR) imaging is essential in clinical diagnostics due to its ability to capture detailed soft tissue structures. However, acquiring high-resolution MR images is expensive and often leads to reduced signal-to-noise ratios. To address this, MR image super-resolution aims to generate high-resolution images from low-resolution inputs. While deep neural networks have been widely applied for MR image super-resolution, they struggle to effectively utilize structural information critical for accurate reconstruction. This paper introduces a novel Transformer-based framework for super-resolving T2-weighted MR image which is a critical MR imaging modality. This framework excels in leveraging both intra-modality and inter-modality dependencies to enhance the structural information. The innovative component of our proposed architecture is termed as High-frequency Structure Transformer (HFST) which operates on the gradients of input images, leveraging the high-frequency structure prior. It also employs high-resolution T1-weighted images which is a more efficient MR imaging modality to provide substantial inter-modality structure priors for the processing of low-resolution T2-weighted images. HFST is featured by parallel intra-modality and inter-modality context exploration and window-based self-attention modules. Notably, both intra-head and inter-head correlations are incorporated to build up the self-attention modules, amplifying the relation extraction capacity. Rigorous evaluations on three benchmarks including IXI, BraTS2018, and fastMRI reveal that our method sets a new state of the art in MR image super-resolution. Especially, our method increases the PSNR metric by up to 1.28 dB under the 4 × super-resolution setting. Our codes are available at https://github.com/dummerchen/HFST .},
  archive      = {J_PR},
  author       = {Chaowei Fang and Bolin Fu and De Cheng and Lechao Cheng and Dingwen Zhang},
  doi          = {10.1016/j.patcog.2025.112037},
  journal      = {Pattern Recognition},
  month        = {2},
  pages        = {112037},
  shortjournal = {Pattern Recognition},
  title        = {High-frequency structure transformer for magnetic resonance image super-resolution},
  volume       = {170},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). ST-KeyS: Self-supervised transformer for keyword spotting in historical handwritten documents. <em>PR</em>, <em>170</em>, 112036. (<a href='https://doi.org/10.1016/j.patcog.2025.112036'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Keyword spotting (KWS) in historical documents is an important tool for the initial exploration of digitized collections. Nowadays, the most efficient KWS methods rely on machine learning techniques, which typically require a large amount of annotated training data. However, in the case of historical manuscripts, there is a lack of annotated corpora for training. To handle the data scarcity issue, we investigate the merits of self-supervised learning to extract useful representations of the input data without relying on human annotations and then use these representations in the downstream task. We propose ST-KeyS, a masked auto-encoder model based on vision transformers where the pretraining stage is based on the mask-and-predict paradigm without the need for labeled data. In the fine-tuning stage, the pre-trained encoder is integrated into a fine-tuned Siamese neural network model to improve feature embedding from the input images. We further improve the image representation using pyramidal histogram of characters (PHOC) embedding to create and exploit an intermediate representation of images based on text attributes. The proposed approach outperforms state-of-the-art methods trained on the same datasets in an exhaustive experimental evaluation of five widely used benchmark datasets (Botany, Alvermann Konzilsprotokolle, George Washington, Esposalles, and RIMES).},
  archive      = {J_PR},
  author       = {Sana Khamekhem Jemni and Sourour Ammar and Mohamed Ali Souibgui and Yousri Kessentini and Abbas Cheddad},
  doi          = {10.1016/j.patcog.2025.112036},
  journal      = {Pattern Recognition},
  month        = {2},
  pages        = {112036},
  shortjournal = {Pattern Recognition},
  title        = {ST-KeyS: Self-supervised transformer for keyword spotting in historical handwritten documents},
  volume       = {170},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Hybrid training of deep neural networks with multiple output layers for tabular data classification. <em>PR</em>, <em>170</em>, 112035. (<a href='https://doi.org/10.1016/j.patcog.2025.112035'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the rapidly evolving landscape of deep learning, the ability to balance performance and computational efficiency is indispensable. Layer-wise training, which involves independently training each hidden layer of a neural network with its private output layer, offers a promising avenue by enabling the construction of a single network that can leverage an ensemble of output layers during prediction. This approach has been successfully employed in state-of-the-art models like ensemble deep multilayer perceptron (edMLP) and ensemble deep random vector functional link (edRVFL), pushing the boundaries of their base models, MLP trained by backpropagation (BP) and RVFL trained using a closed-form solution (CFS). However, edRVFL often underperforms edMLP in accuracy, while edMLP incurs significantly higher computational cost. To this end, we introduce two novel hybrid training approaches that integrate BP and CFS, aiming to balance the trade-offs. Extensive experiments on diverse classification datasets reveal that one of the proposed models, ensemble deep adaptive sampling (edAS), achieves statistically significant improvements in classification accuracy over state-of-the-art models, including edRVFL, edMLP, and self-normalizing neural network (SNN), while being less computationally expensive. Furthermore, the second proposed model, MO-MLP, demonstrates statistically significant superiority over competing models while requiring less than one-third of the computation time needed by models that incorporate BP in a layer-wise manner. The source code for all proposed models is available on GitHub. 1},
  archive      = {J_PR},
  author       = {Mohamed Hamdy and Abdulaziz Al-Ali and Ponnuthurai N. Suganthan and Hussein Aly},
  doi          = {10.1016/j.patcog.2025.112035},
  journal      = {Pattern Recognition},
  month        = {2},
  pages        = {112035},
  shortjournal = {Pattern Recognition},
  title        = {Hybrid training of deep neural networks with multiple output layers for tabular data classification},
  volume       = {170},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). EC-SLAM: Effectively constrained neural RGB-D SLAM with TSDF hash encoding and joint optimization. <em>PR</em>, <em>170</em>, 112034. (<a href='https://doi.org/10.1016/j.patcog.2025.112034'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We introduce EC-SLAM, a real-time dense RGB-D Simultaneous Localization and Mapping (SLAM) system leveraging Neural Radiance Fields (NeRF). While recent NeRF-based SLAM systems have shown promising results, they have yet to exploit NeRF’s potential to estimate system state fully. EC-SLAM overcomes this limitation by using a Truncated Signed Distance Fields (TSDF) opacity function with sharp inductive bias to strengthen constraints in sparse parametric encodings, which reduces the number of model parameters and enhances accuracy. Additionally, our system employs a highly constrained global joint optimization approach coupled with a feature-based, uniform sampling algorithm, enabling efficient fusion between TSDF and sparse parametric encodings. This approach reinforces constraints on keyframes most relevant to the current frame, mitigates the influence of random sampling, and effectively utilizes NeRF’s implicit loop closure capability. Extensive evaluations and ablations on the Replica, ScanNet, and TUM datasets demonstrate state-of-the-art performance, achieving precise tracking and reconstruction while maintaining real-time operation at up to 21 FPS. The source code is available at https://github.com/Lightingooo/EC-SLAM .},
  archive      = {J_PR},
  author       = {Guanghao Li and Qi Chen and Yuxiang Yan and Jian Pu},
  doi          = {10.1016/j.patcog.2025.112034},
  journal      = {Pattern Recognition},
  month        = {2},
  pages        = {112034},
  shortjournal = {Pattern Recognition},
  title        = {EC-SLAM: Effectively constrained neural RGB-D SLAM with TSDF hash encoding and joint optimization},
  volume       = {170},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Context-semantic quality awareness network for fine-grained visual categorization. <em>PR</em>, <em>170</em>, 112033. (<a href='https://doi.org/10.1016/j.patcog.2025.112033'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Exploring and mining subtle yet distinctive features between sub-categories with similar appearances is crucial for fine-grained visual categorization (FGVC). However, the existing FGVC methods cannot mine discriminative features from low-quality samples, leading to a significant decline in performance. To address this issue, we propose a weakly supervised Context-Semantic Quality Awareness Network (CSQA-Net) for FGVC. Specifically, to assess and enhance the quality of multi-granularity visual representations, we propose the Multi-level Semantic Quality Evaluation (MSQE) module, composed of the Quality Probing (QP) classifier. To alleviate the scale confusion problems and accurately identify the local distinctive regions, the part navigator is developed. Moreover, the Multi-part and Multi-scale Cross-Attention (MMCA) module is designed to model the spatial contextual relationship between rich part descriptors and global semantics, thus capturing more discriminative details within the object. Finally, the context-aware features from MMCA and semantically enhanced features from MSQE are fed into the corresponding QP classifiers to evaluate the quality in real time, further boosting the discriminability. Comprehensive experiments on four popular and highly competitive datasets demonstrate the superiority of the proposed CSQA-Net in comparison with the state-of-the-art methods. Code is available at https://github.com/zmisiter/CSQA-Net .},
  archive      = {J_PR},
  author       = {Qin Xu and Sitong Li and Jiahui Wang and Bo Jiang and Bin Luo and Jinhui Tang},
  doi          = {10.1016/j.patcog.2025.112033},
  journal      = {Pattern Recognition},
  month        = {2},
  pages        = {112033},
  shortjournal = {Pattern Recognition},
  title        = {Context-semantic quality awareness network for fine-grained visual categorization},
  volume       = {170},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Feature selection based on rough diversity entropy. <em>PR</em>, <em>170</em>, 112032. (<a href='https://doi.org/10.1016/j.patcog.2025.112032'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Information entropy, as a powerful tool for measuring the uncertainty of information, is widely used in many fields such as communication, data compression, data mining and bioinformatics. However, the classical information entropy has two shortcomings, that is, information entropy cannot accurately measure the uncertainty of knowledge in some cases and the joint probability in information entropy is usually difficult to calculate for high-dimensional data. Additionally, uncertainty measure is the foundation of feature selection in granular computing. Inaccurate measures may lead to poor performance of feature selection methods. To address these issues, we propose a novel uncertainty measure called rough diversity entropy based on rough set theory. Rough diversity entropy can more accurately measure the uncertainty of knowledge compared with the classical information entropy. In this article, rough diversity entropy and its variants are first defined, and their related properties are studied. Next, a heuristic feature selection method based on the defined measures is put forward, and the corresponding algorithm is also designed. Finally, a series of experiments are executed to validate the effectiveness and rationality of the proposed method. The analysis results show that our proposed method has good performance compared with eight existing feature selection methods. Moreover, the proposed method improves the average accuracy of 15 datasets by 6.53% under four classifiers, and achieves an average feature reduction rate of up to 99.81%. We believe that the proposed method is an effective feature selection approach for classification learning.},
  archive      = {J_PR},
  author       = {Xiongtao Zou and Jianhua Dai},
  doi          = {10.1016/j.patcog.2025.112032},
  journal      = {Pattern Recognition},
  month        = {2},
  pages        = {112032},
  shortjournal = {Pattern Recognition},
  title        = {Feature selection based on rough diversity entropy},
  volume       = {170},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Mamba-YOLO: Multi-level adaptive rectangular convolution for document layout analysis. <em>PR</em>, <em>170</em>, 112031. (<a href='https://doi.org/10.1016/j.patcog.2025.112031'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Document Layout Analysis (DLA) is a crucial component in the field of document understanding and processing. However, the majority of existing public DLA datasets are predominantly in coarse-grained category and written in the English language, which limits their applicability in the diverse context of Chinese documents. To address this limitation, we introduce PeKi, a large-scale dataset for fine-grained Chinese document layout analysis that encompasses multiple domains. PeKi comprises a comprehensive collection of documents from five distinct fields, available in both scanned and digitally native formats. We have meticulously annotated all fine-grained titles and ensured that figure captions, table captions, and formula numbers are included within the respective scopes of figures, tables and formulas. Building on this foundation, we propose a novel approach to document layout analysis, termed DocMY. This method innovatively integrates the Mamba module into the YOLOv9 framework, for the first time, with a multi-level adaptive rectangular convolution. This can effectively perceive different layout elements with varying aspect ratios and understand structured continuous elements in the document. Experiments on the proposed benchmarks demonstrate that DocMY obtains competitive results. DocMY achieves a mAP of 94.8% on the PeKi dataset while reducing model parameters by 15.3% compared to previous methods. Our code and dataset are available at https://github.com/WenkMa/DocMY .},
  archive      = {J_PR},
  author       = {Wenkang Ma and Mingzhe Cao and Jinyue Ma and Zhenyang Dong and Chaozhi Yang and Zongmin Li},
  doi          = {10.1016/j.patcog.2025.112031},
  journal      = {Pattern Recognition},
  month        = {2},
  pages        = {112031},
  shortjournal = {Pattern Recognition},
  title        = {Mamba-YOLO: Multi-level adaptive rectangular convolution for document layout analysis},
  volume       = {170},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Unbalanced episode meta-learning with bi-sparse contrastive network for hyperspectral target detection. <em>PR</em>, <em>170</em>, 112030. (<a href='https://doi.org/10.1016/j.patcog.2025.112030'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep learning (DL) has been extensively applied to hyperspectral image target detection (HTD) with notable success. However, many existing DL-based methods focus on expanding the training samples to capture richer information, resulting in high computational costs and overfitting risks. Additionally, challenges such as complex data distributions and limited model transferability remain significant obstacles. To address these issues, we propose an unbalanced episode meta-learning with Bi-sparse contrastive network (UEML) for HTD. In contrast to directly modeling the target dataset, our approach leverages meta-learning to pre-train the model on a categorical dataset rich in label information, resulting in a universal detection model. Specifically, an unbalanced episode training paradigm is proposed for meta-task construction, which simulates the category-imbalance scenarios inherent to HTD by adaptively adjusting the support set, enabling the acquisition of content-agnostic yet task-relevant transferable meta-knowledge. Additionally, elastic sparsity constraints are imposed on the feature extraction process across both spatial and spectral dimensions, enhancing the model’s generalization and discriminative capabilities. During the fine-tuning phase, we employ a pseudo-sample generation strategy based on segmented sampling and spatial–spectral hybrid augmentation to construct the training set, allowing for more accurate and comprehensive sample extraction from complex background regions. This strategy effectively mitigates underfitting caused by insufficient information. Furthermore, contrastive learning is incorporated to address complexities arising by multi-class background characteristics in the pseudo-binary classification task, improving the stability of the detection model. Our proposed algorithm demonstrates rapid target detection capabilities, and experiments on six public datasets indicate that it performs significantly better than existing state-of-the-art methods. Code is available at: https://github.com/QYo-Liu/UEML .},
  archive      = {J_PR},
  author       = {Quanyong Liu and Yang Xu and Zebin Wu and Jiangtao Peng and Zhihui Wei},
  doi          = {10.1016/j.patcog.2025.112030},
  journal      = {Pattern Recognition},
  month        = {2},
  pages        = {112030},
  shortjournal = {Pattern Recognition},
  title        = {Unbalanced episode meta-learning with bi-sparse contrastive network for hyperspectral target detection},
  volume       = {170},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Robust oblique projection and weighted NMF for hyperspectral unmixing. <em>PR</em>, <em>170</em>, 112029. (<a href='https://doi.org/10.1016/j.patcog.2025.112029'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Hyperspectral unmixing (HU) is a crucial method for interpreting remotely sensed hyperspectral images (HSIs), with the aim of splitting the image into pure spectral components (endmembers) and their abundance fractions in every pixel of the scene. However, the effectiveness of this procedure is hindered by the presence of noise and anomalies. These kind of disruptions mainly arise from real-world factors such as atmospheric effects and endmember variability. To address this challenge, a novel approach called Graph-Regularized Oblique Projection Weighted NMF (GOP-WNMF) is introduced, which is grounded in a more precise separation of signal and noise subspaces, aiming to enhance the accuracy and robustness of the analysis. GOP-WNMF achieves this by constructing an oblique projector that projects each pixel onto the signal subspace, i.e., the space formed by signatures of endmembers, and parallel to the noise subspace. This approach effectively suppresses noise while preserving crucial spectral information. Furthermore, our new oblique NMF framework includes a unique residual-based weighting approach to detect and remove anomalies in pixels and spectral bands simultaneously. In addition to this, another weighting matrix is proposed by establishing a bipartite graph connecting endmembers and pixels to promote smoothness and sparsity in the resulting abundance maps. GOP-WNMF also enhances abundance map estimation accuracy by mitigating the negative effects of pixel outliers through the utilization of Laplacian eigenmaps technique to maintain the manifold structure of data. The effectiveness of GOP-WNMF is evaluated through comprehensive testing on synthetic and real HSIs, and its superiority is demonstrated over multiple state-of-the-art approaches. The source code is also available at https://github.com/yasinhashemi/GOP-WNMF .},
  archive      = {J_PR},
  author       = {Yasin Hashemi-Nazari and Azita Tajaddini and Farid Saberi-Movahed and Fernando Alonso-Fernandez and Prayag Tiwari},
  doi          = {10.1016/j.patcog.2025.112029},
  journal      = {Pattern Recognition},
  month        = {2},
  pages        = {112029},
  shortjournal = {Pattern Recognition},
  title        = {Robust oblique projection and weighted NMF for hyperspectral unmixing},
  volume       = {170},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Improve noise tolerance of robust feature selection via block-sparse projection learning. <em>PR</em>, <em>170</em>, 112028. (<a href='https://doi.org/10.1016/j.patcog.2025.112028'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Feature selection (FS) is crucial in robust representation learning by identifying informative features within high-dimensional data. However, the presence of noise can lead to the misidentification of features and distort data representation. Existing robust feature selection algorithms typically discard noisy features, leading to the loss of potentially beneficial information. Additionally, these methods are often hindered by the difficulty of tuning sparse regularization parameters, further affecting generalization. To address this, we propose a method for improving the noise tolerance of robust feature selection (NTRFS), which actively identifies and exploits beneficial noise during the selection process to enhance robustness. Specifically, NTRFS incorporates block-sparse projection learning with ℓ 2 , 1 -norm minimization, enhancing the model’s robustness to noise while preserving key features by leveraging the informative aspects of noise. Furthermore, by integrating anomaly estimation with adaptive weighting, NTRFS utilizes noise-tolerant information to promote the discovery of class prototypes, adjusting the weight of each feature based on its informative saliency. Additionally, the proposed ℓ 2 , 0 -norm constrained block-sparse projection learning module enhances discriminative power by exploiting local geometric relationships in data manifolds, without requiring regularization parameter tuning. Finally, to tackle the non-convex trace ratio and NP-hard block sparsity problems, we propose an efficient iterative optimization algorithm with guaranteed convergence. Experimental results on several real-world datasets show that NTRFS enhances robustness and improves classification performance by leveraging noise, outperforming advanced robust FS methods.},
  archive      = {J_PR},
  author       = {Jie Wang and Zheng Wang and Yu Guo and Rong Wang and Fei Wang and Feiping Nie},
  doi          = {10.1016/j.patcog.2025.112028},
  journal      = {Pattern Recognition},
  month        = {2},
  pages        = {112028},
  shortjournal = {Pattern Recognition},
  title        = {Improve noise tolerance of robust feature selection via block-sparse projection learning},
  volume       = {170},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Deep matrix factorization with adaptive weights for multi-view clustering. <em>PR</em>, <em>170</em>, 112027. (<a href='https://doi.org/10.1016/j.patcog.2025.112027'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, deep matrix factorization has been established as a powerful model for unsupervised tasks, achieving promising results, especially for multi-view clustering. However, existing methods often lack effective feature selection mechanisms and rely on empirical hyperparameter selection. To address these issues, we introduce a novel Deep Matrix Factorization with Adaptive Weights for Multi-View Clustering (DMFAW). Our method simultaneously incorporates feature selection and generates local partitions, enhancing clustering results. The feature weights are driven by a single, control-theory-inspired parameter that is updated dynamically, which improves stability and speeds convergence. A late fusion approach is then proposed to align the weighted local partitions with the consensus partition. Finally, the optimization problem is solved via an alternating optimization algorithm with theoretically guaranteed convergence. Extensive experiments on benchmark datasets highlight that DMFAW outperforms state-of-the-art methods in terms of clustering performance.},
  archive      = {J_PR},
  author       = {Yasser Khalafaoui and Basarab Matei and Martino Lovisetto and Nistor Grozavu},
  doi          = {10.1016/j.patcog.2025.112027},
  journal      = {Pattern Recognition},
  month        = {2},
  pages        = {112027},
  shortjournal = {Pattern Recognition},
  title        = {Deep matrix factorization with adaptive weights for multi-view clustering},
  volume       = {170},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Robust facial expression recognition by simultaneously addressing hard and mislabeled samples. <em>PR</em>, <em>170</em>, 112026. (<a href='https://doi.org/10.1016/j.patcog.2025.112026'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Facial Expression Recognition (FER) in the wild is a challenging task, especially when training data contains numerous mislabeled samples and hard samples. Typically, FER models either overfit to the mislabeled samples or underfit to the hard samples, resulting in degraded performance. However, most existing methods fail to address both issues simultaneously. To overcome this limitation, this paper introduces a novel FER method called Noise-Hard robust Graph (NHG), which dynamically supervises the updating of the adjacency matrix in the Graph Convolutional Networks, striking a balance between suppressing the impacts of noisy labels and encouraging learning from hard samples. First, we map high-dimensional facial expression features onto low-dimensional manifolds to initialize the topological relationships between the samples, thus measuring the hard sample relationships more accurately. Second, we design a Label Consistency Mask (LCM) strategy to retain potential connections for hard sample learning. LCM could also potentially preserve correct connections while noisy labels exist, supporting noise-robust learning. Third, based on differences in trends of ℓ 2 -norm variation between mislabeled samples and hard samples, the ℓ 2 -norm regularization (L2R) suppresses the learning of mislabeled samples while preserving the learning potential of hard samples and suppressing the propagation of their features within the graph. Experimental results demonstrate that our method achieves competitive performance compared to state-of-the-art methods in scenarios with noisy labels and hard samples.},
  archive      = {J_PR},
  author       = {Yuandong Min and Ruyi Xu and Jingying Chen and Yanfeng Ji and Xiaodi Liu},
  doi          = {10.1016/j.patcog.2025.112026},
  journal      = {Pattern Recognition},
  month        = {2},
  pages        = {112026},
  shortjournal = {Pattern Recognition},
  title        = {Robust facial expression recognition by simultaneously addressing hard and mislabeled samples},
  volume       = {170},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Point geometrical coulomb force: An explicit and robust embedding for point cloud analysis. <em>PR</em>, <em>170</em>, 112025. (<a href='https://doi.org/10.1016/j.patcog.2025.112025'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, most existing point cloud frameworks tend to utilize max pooling aggregation functions to aggregate local point cloud features. However, when handling data containing local high-frequency noise such as local drop, addition, and jitter, this mechanism leads to high-frequency noise that spreads from local to global and causes severe performance degradation. To address this issue, we creatively extend the concepts from the physical field, namely electrostatic field and Coulomb force into geometric processing. To be specific, we treat the entire point cloud placed in an electrostatic field and each point as a probe charge and then equip this field with a set of source charges according to the structure of the cloud. We endow these two types of charges with different electric quantities, which could encode informative geometrical structural information. By analogously computing the Coulomb force between the probe charge and its corresponding source charge, we finally propose an explicit embedding called Point Geometric Coulomb Force (PGCF) for each point. Due to the deep use of the structural information of the point cloud and the fact that the electrostatic field of each source charge could not be affected by the variations of the probe charges, the PGCF has been proven to provide richer geometric information while being robust to local noises. Using the PGCF combined with point coordinates as inputs can significantly improve the performances of existing 3D point cloud feature extraction frameworks, including point convolution, graph convolution, and point transformer, without additional parameters or computational overhead, thus not affecting their inference speed. Experimental results show that integrating the PGCF into existing works brings more desirable results in a wide range of 3D point cloud analysis tasks, including classification, part segmentation, and semantic segmentation.},
  archive      = {J_PR},
  author       = {Haojun Xu and Ling Hu and Qinsong Li and Shengjun Liu and Dong-ming Yan and Xinru Liu},
  doi          = {10.1016/j.patcog.2025.112025},
  journal      = {Pattern Recognition},
  month        = {2},
  pages        = {112025},
  shortjournal = {Pattern Recognition},
  title        = {Point geometrical coulomb force: An explicit and robust embedding for point cloud analysis},
  volume       = {170},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). IDEAL: Independent domain embedding augmentation learning. <em>PR</em>, <em>170</em>, 112024. (<a href='https://doi.org/10.1016/j.patcog.2025.112024'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep metric learning is fundamental to open-set pattern recognition and has become a focal point of research in recent years. Significant efforts have been devoted to designing sampling, mining, and weighting strategies within algorithmic-level deep metric learning (DML) loss objectives. However, less attention has been paid to input-level but essential data transformations. In this paper, we develop a novel mechanism, independent domain embedding augmentation learning (IDEAL) method. It can simultaneously learn multiple independent embedding spaces for multiple domains generated by predefined data transformations. Our IDEAL is orthogonal to existing DML techniques and can be seamlessly combined with one DML approach for enhanced performance. Empirical results on visual retrieval tasks demonstrate the superiority of the proposed method. For instance, IDEAL significantly improves the performance of both Multi-Similarity (MS) Loss and Hypergraph-Induced Semantic Tuplet (HIST) loss. Specifically, it boosts the Recall @ 1 from 84.5% → 87.1% for MS Loss on Cars-196 and from 65.8% → 69.5% on CUB-200. Similarly, for HIST loss, IDEAL improves the performance on Cars-196 from 87.4% → 90.3%, on CUB-200 from 69.7% to 72.3%. It significantly outperforms methods using basic network architectures (e.g., ResNet-50, BN-Inception), such as XBM and Intra-Batch. The source code of our proposed method is available at https://github.com/emdata-ailab/Ideal-learning .},
  archive      = {J_PR},
  author       = {Zelin Yang and Lin Xu and Shiyang Yan and Haixia Bi and Fan Li},
  doi          = {10.1016/j.patcog.2025.112024},
  journal      = {Pattern Recognition},
  month        = {2},
  pages        = {112024},
  shortjournal = {Pattern Recognition},
  title        = {IDEAL: Independent domain embedding augmentation learning},
  volume       = {170},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). AMGSN: Adaptive mask-guide supervised network for debiased facial expression recognition. <em>PR</em>, <em>170</em>, 112023. (<a href='https://doi.org/10.1016/j.patcog.2025.112023'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Facial expression recognition plays a crucial role in understanding human emotions and behavior. However, existing models often exhibit biases and imbalance towards diverse expression classes. To address this problem, we propose an Adaptive Mask-Guide Supervised Network (AMGSN) to enhance the uniform performance of the facial expression recognition models. We propose an adaptive mask guidance mechanism to mitigate bias and ensure uniform performance across different expression classes. AMGSN focuses on learning the ability to distinguish facial features with under-expressed expressions by dynamically generating masks during pre-training. Specifically, we employ an asymmetric encoder–decoder architecture, where the encoder encodes only the unmasked visible regions, while the lightweight decoder reconstructs the original image using latent representations and mask markers. By utilizing dynamically generated masks and focusing on informative regions, these models effectively reduce the interference of confounding factors, thus enhancing the discriminative power of the learned representation. In the pre-training stage, we introduce the Attention-Based Mask Generator (ABMG) to identify salient regions of expressions. Additionally, we advance the Mask Ratio Update Strategy (MRUS), which utilizes image reconstruction loss, to adjust the mask ratio for each image during pre-training. In the finetune stage, debiased center loss and contrastive loss are introduced to optimize the network to ensure the overall performance of expression recognition. Extensive experimental results on several standard datasets demonstrate that the proposed AMGSN significantly improves both balance and accuracy compared to state-of-the-art methods. For example, AMGSN reached 89.34% on RAF-DB, and 62.83% on AffectNet, respectively, with a standard deviation of only 0.0746 and 0.0484. This demonstrates the effectiveness of our improvements 1 .},
  archive      = {J_PR},
  author       = {Tianlong Gu and Hao Li and Xuan Feng and Yiqin Luo},
  doi          = {10.1016/j.patcog.2025.112023},
  journal      = {Pattern Recognition},
  month        = {2},
  pages        = {112023},
  shortjournal = {Pattern Recognition},
  title        = {AMGSN: Adaptive mask-guide supervised network for debiased facial expression recognition},
  volume       = {170},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). A survey on image compressive sensing: From classical theory to the latest explicable deep learning. <em>PR</em>, <em>170</em>, 112022. (<a href='https://doi.org/10.1016/j.patcog.2025.112022'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep learning has achieved significant advancements in both low-level and high-level computer vision tasks, which can also drive the development of an essential research field of Image Compressive Sensing (ICS) today and in the future. Nowadays model-inspired ICS reconstruction methods have gained considerable attention from researchers, resulting in numerous new developments. However, existing literature lacks a comprehensive summary of these advancements. To revitalize the field of ICS, it is crucial to summarize them to provide valuable insights for various other fields and practical applications. Thus, this article first looks back on foundational theories of ICS, including signal sparse representation, sampling and reconstruction. Next, we summarize different types of measurement matrices for sampling, which include learnable/non-learnable measurement matrix, uniform/non-uniform measurement matrix. Then, we provide a detailed review of ICS reconstruction, covering traditional optimization reconstruction methods, inexplicable reconstruction methods and explainable reconstruction methods as well as Transformer-based reconstruction methods, which will help readers quickly grasp the history of ICS development. We also evaluate several representative ICS reconstruction methods on publicly available datasets, comparing their performance and computational complexities to highlight their strengths and weaknesses. Finally, we conclude this paper and their future opportunities and challenges are prospected. All related materials can be found at https://github.com/mdcnn/CS-Survey .},
  archive      = {J_PR},
  author       = {Lijun Zhao and Yufeng Zhang and Xinlu Wang and Jinjing Zhang and Huihui Bai and Anhong Wang},
  doi          = {10.1016/j.patcog.2025.112022},
  journal      = {Pattern Recognition},
  month        = {2},
  pages        = {112022},
  shortjournal = {Pattern Recognition},
  title        = {A survey on image compressive sensing: From classical theory to the latest explicable deep learning},
  volume       = {170},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Enhancing robustness and efficiency of least square twin SVM via granular computing. <em>PR</em>, <em>170</em>, 112021. (<a href='https://doi.org/10.1016/j.patcog.2025.112021'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the domain of machine learning, least square twin support vector machine (LSTSVM) stands out as one of the state-of-the-art classification model. However, LSTSVM is not without its limitations. It exhibits sensitivity to noise and outliers, fails to adequately incorporate the structural risk minimization (SRM) principle, and often demonstrates instability under resampling scenarios. Moreover, its computational complexity and reliance on matrix inversions hinder the efficient processing of large datasets. As a remedy to the aforementioned challenges, we propose the robust granular ball LSTSVM (GBLSTSVM). GBLSTSVM is trained using granular balls instead of original data points. The core of a granular ball is found at its center, where it encapsulates all the pertinent information of the data points within the ball of specified radius. To improve scalability and efficiency, we further introduce the large-scale GBLSTSVM (LS-GBLSTSVM), which incorporates the SRM principle through regularization terms. Experiments are performed on UCI, KEEL, and NDC benchmark dataset demonstrate that both the proposed GBLSTSVM and LS-GBLSTSVM models consistently outperform the baseline models. The source code of the proposed GBLSTSVM model is available at https://github.com/mtanveer1/GBLSTSVM .},
  archive      = {J_PR},
  author       = {M. Tanveer and R.K. Sharma and A. Quadir and M. Sajid},
  doi          = {10.1016/j.patcog.2025.112021},
  journal      = {Pattern Recognition},
  month        = {2},
  pages        = {112021},
  shortjournal = {Pattern Recognition},
  title        = {Enhancing robustness and efficiency of least square twin SVM via granular computing},
  volume       = {170},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). AFPN: Alignment feature pyramid network for real-time semantic segmentation. <em>PR</em>, <em>170</em>, 112019. (<a href='https://doi.org/10.1016/j.patcog.2025.112019'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The structures of two pathways and the feature pyramid network (FPN) have achieved advanced performance in semantic segmentation. These two types of structures adopt different approaches to fuse low-level (shallow layer) spatial information and high-level (deep layer) semantic information. However, the segmentation results still lack local details due to the loss of information caused by simply fusing low-level feature details directly with multi-level deep features. To alleviate this problem, we propose an alignment feature pyramid network (AFPN) for real-time semantic segmentation. It can efficiently utilize both the low-level spatial information and high-level semantic information. Specifically, our AFPN consists of two components: the pooling enhancement attention block (PEAB) and the dual pooling alignment block (DPAB). The PEAB can effectively extract global information by using an aggregation pooling operation. The DPAB performs two types of pooling operations along the channel and spatial dimensions, reducing the differences between multi-scale feature maps. Extensive experiments show that AFPN achieves a better balance between accuracy and speed. On the Cityscapes, CamVid, and ADE20K datasets, AFPN achieves 78.75%, 79.24%, and 39.56% mIoU and the speed meets the real-time requirement. Our code can be available at the https://github.com/chongchongmao/AFPN .},
  archive      = {J_PR},
  author       = {Yongsheng Dong and Chongchong Mao and Lintao Zheng and Qingtao Wu and Mingchuan Zhang and Xuelong Li},
  doi          = {10.1016/j.patcog.2025.112019},
  journal      = {Pattern Recognition},
  month        = {2},
  pages        = {112019},
  shortjournal = {Pattern Recognition},
  title        = {AFPN: Alignment feature pyramid network for real-time semantic segmentation},
  volume       = {170},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Disentangled representation learning with causal effect transmission in variational autoencoder. <em>PR</em>, <em>170</em>, 112018. (<a href='https://doi.org/10.1016/j.patcog.2025.112018'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Disentangled Representation Learning in variational autoencoder (VAE) has emerged as a strategy to identify and disentangle underlying factors from observable data to improve recognition capabilities such as images, speeches, and biological signals. Existing disentanglement methods are mostly based on the prior assumption that latent variables are mutually independent, which is inconsistent with reality and fails to transmit causal effects among causal nodes. To address the above issues, we introduce a novel disentanglement representation learning model with causal effect transmission, named DRL CET . The main ideas of DRL CET involve (1) mapping encoded latent exogenous variables to causal variables and updating the causal structure by a constructed nonlinear/linear structural causal model (SCM), (2) designing hierarchical feature loss from discriminator to replace pixel-level loss in variational autoencoder for efficiently extracting causal features, and (3) aggregating causal information from adjacent nodes by a graph attention network (GAT) with intervention for transmitting causal effects. Extensive theoretical analyses and empirical studies on synthetic and real datasets demonstrate the effectiveness, viability, and superiority of our DRL CET over the state-of-the-arts. Our code is publicly available at https://github.com/youdianlong/DRLCET.git .},
  archive      = {J_PR},
  author       = {Dianlong You and Zexuan Li and Jiawei Shen and Zhao Yu and Shunfu Jin and Xindong Wu},
  doi          = {10.1016/j.patcog.2025.112018},
  journal      = {Pattern Recognition},
  month        = {2},
  pages        = {112018},
  shortjournal = {Pattern Recognition},
  title        = {Disentangled representation learning with causal effect transmission in variational autoencoder},
  volume       = {170},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). AniFaceDiff: Animating stylized avatars via parametric conditioned diffusion models. <em>PR</em>, <em>170</em>, 112017. (<a href='https://doi.org/10.1016/j.patcog.2025.112017'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Animating stylized head avatars with dynamic poses and expressions has become an important focus in recent research due to its broad range of applications (e.g. VR/AR, film and animation, privacy protection). Previous research has made significant progress by training controllable generative models to animate the reference avatar using the target pose and expression. However, existing portrait animation methods are mostly trained using human faces, making them struggle to generalize to stylized avatar references such as cartoon, painting, and 3D-rendered avatars. Moreover, the mechanisms used to animate avatars – namely, to control the pose and expression of the reference – often inadvertently introduce unintended features – such as facial shape – from the target, while also causing a loss of intended features, like expression-related details. This paper proposes AniFaceDiff, a Stable Diffusion based method with a new conditioning module for animating stylized avatars. First, we propose a refined spatial conditioning approach by Facial Alignment to minimize identity mismatches, particularly between stylized avatars and human faces. Then, we introduce an Expression Adapter that incorporates additional cross-attention layers to address the potential loss of expression-related information. Extensive experiments demonstrate that our method achieves state-of-the-art performance, particularly in the most challenging out-of-domain stylized avatar animation, i.e., domains unseen during training. It delivers superior image quality, identity preservation, and expression accuracy. This work enhances the quality of virtual stylized avatar animation for constructive and responsible applications. To promote ethical use in virtual environments, we contribute to the advancement of face manipulation detection by evaluating state-of-the-art detectors, highlighting potential areas for improvement, and suggesting solutions.},
  archive      = {J_PR},
  author       = {Ken Chen and Sachith Seneviratne and Wei Wang and Dongting Hu and Sanjay Saha and Md. Tarek Hasan and Sanka Rasnayaka and Tamasha Malepathirana and Mingming Gong and Saman Halgamuge},
  doi          = {10.1016/j.patcog.2025.112017},
  journal      = {Pattern Recognition},
  month        = {2},
  pages        = {112017},
  shortjournal = {Pattern Recognition},
  title        = {AniFaceDiff: Animating stylized avatars via parametric conditioned diffusion models},
  volume       = {170},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). A video anomaly detection framework based on semantic consistency and multi-attribute feature complementarity. <em>PR</em>, <em>170</em>, 112016. (<a href='https://doi.org/10.1016/j.patcog.2025.112016'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-attribute features such as appearance, optical flow, and pose have been widely applied in the field of Video Anomaly Detection. However, existing studies often overlook the semantic consistency between these features, which limits the potential relationships among different attributes and hinders the effective utilization of this information in anomaly detection. To address this issue, we propose a video anomaly detection framework based on Semantic Consistency and Multi-Attribute Feature Complementarity (SC-MAFC). Specifically, we design a three-branch encoder–decoder network to separately encode and predict appearance, optical flow, and pose features. By comparing the consistency differences between appearance and other attribute features at normal and anomalous moments, we can use these differences as the basis for anomaly detection. To better capture these differences, we introduce a Spatial-Channel Feature Complementarity Module (SCFCM), allowing different attribute features to complement each other and helping the model more accurately understand the semantic consistency of normal events, thus improving its ability to recognize the semantic consistency in normal events. Additionally, to further enhance detection performance, we introduce a memory module to degrade the reconstruction quality of features at anomalous moments, by which large errors are generated in future frame predictions, making anomalies more pronounced and easier to detect. It was evaluated on three benchmark datasets: UCSD Ped2, Avenue, and ShanghaiTech. The method proved effective, achieving AUC scores of 99.4% on UCSD Ped2, 93.9% on Avenue, and 84.6% on ShanghaiTech, demonstrating its robustness in various scenarios. The code is publicly accessible at the following link: https://github.com/jzt-dongli/SC-MAFC .},
  archive      = {J_PR},
  author       = {Chuanxu Wang and Zitai Jiang and Haigang Deng and Chunjuan Yan},
  doi          = {10.1016/j.patcog.2025.112016},
  journal      = {Pattern Recognition},
  month        = {2},
  pages        = {112016},
  shortjournal = {Pattern Recognition},
  title        = {A video anomaly detection framework based on semantic consistency and multi-attribute feature complementarity},
  volume       = {170},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). MHAN: Multi-head hybrid attention network for facial expression recognition. <em>PR</em>, <em>170</em>, 112015. (<a href='https://doi.org/10.1016/j.patcog.2025.112015'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Integrating Facial Expression Recognition (FER) with deep learning techniques has significantly enhanced emotion analysis performance in the past decade. Convolutional neural networks (CNNs) and attention mechanisms facilitate the automatic extraction of complex features from facial expressions. However, current methods often face challenges in accurately capturing subtle variations in expressions, tend to be computationally intensive, and are susceptible to overfitting. To address these challenges, this paper proposes a lightweight FER model based on multi-head hybrid attention networks (MHAN). It designs two innovative modules: efficient local attention mixed feature network (ELA-MFN) and multi-head hybrid attention mechanism (MHAtt). The former integrates multi-scale convolutional kernels with the ELA attention mechanism to enhance feature representation while ensuring precise localization of critical areas, all within a lightweight framework. The latter utilizes multiple attention heads to generate attention maps and capture subtle distinctions in expressions. With only 4.27M parameters (94% reduction from POSTER’s 71.8M), MHAN effectively reduces computational resource requirements, and can be efficiently implemented for both fully supervised and semi-supervised learning tasks. And it employs a smooth label loss function solving overfitting issue. We have validated the effectiveness of MHAN over three public datasets RAF-DB, AffectNet, and FERPlus, including cross-dataset tests. The results show that MHAN outperforms state-of-the-art models in terms of accuracy and computational complexity, demonstrating improved robustness. MHAN can also recognize the expressions of non-traditional datasets like sculptures, validating its cross-domain generalization capabilities. The source code is available at https://github.com/hanyao666/MHAN .},
  archive      = {J_PR},
  author       = {Xiaofeng Wang and Tianbo Han and Songling Liu and Muhammad Shahroz Ajmal and Lu Chen and Yongqin Zhang and Yonghuai Liu},
  doi          = {10.1016/j.patcog.2025.112015},
  journal      = {Pattern Recognition},
  month        = {2},
  pages        = {112015},
  shortjournal = {Pattern Recognition},
  title        = {MHAN: Multi-head hybrid attention network for facial expression recognition},
  volume       = {170},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Adaptive feature selection based on fuzzy rough set fusion model with class variance. <em>PR</em>, <em>170</em>, 112014. (<a href='https://doi.org/10.1016/j.patcog.2025.112014'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Fuzzy rough sets are a powerful tool for efficiently selecting features from large-scale datasets with uncertainty. However, most existing models overlook class density differences within data and lack adaptive learning capabilities, which limits their ability to fit data and ultimately impacts feature selection effectiveness. To address this limitation, we propose an adaptive fuzzy-rough feature selection algorithm that assesses feature importance by constructing a novel fuzzy-rough set fusion model. Specifically, we propose a novel Gaussian-based fuzzy relation incorporating class variance, which effectively resolves the limitations of classical rough sets in processing data with substantial inter-class density disparities. Next, we construct a fuzzy weighted approximation matrix and a fuzzy decision matrix, and develop an Adaptive Fuzzy Rough Set Fusion Model with Class Variance (AFRSFCV), which comprehensively accounts for feature importance during model construction. Finally, we employ the gradient descent method to optimize feature weights, efficiently identifying the most valuable features. Experimental results demonstrate that the proposed AFRSFCV outperforms most existing state-of-the-art algorithms in feature selection.},
  archive      = {J_PR},
  author       = {Changzhong Wang and Yang Zhang and Shuang An and Tingquan Deng},
  doi          = {10.1016/j.patcog.2025.112014},
  journal      = {Pattern Recognition},
  month        = {2},
  pages        = {112014},
  shortjournal = {Pattern Recognition},
  title        = {Adaptive feature selection based on fuzzy rough set fusion model with class variance},
  volume       = {170},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). SpIRL: Spatially-aware image representation learning under the supervision of relative position descriptors. <em>PR</em>, <em>170</em>, 112013. (<a href='https://doi.org/10.1016/j.patcog.2025.112013'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Extracting good visual representations from image contents is essential for solving many computer vision problems (e.g. image retrieval, object detection, classification). In this context, state-of-the-art approaches are mainly based on learning a representation using a neural network optimized for a given task. The encoders optimized in this way can then be deployed as backbones for various downstream tasks. When the latter involves reasoning about spatial information from the image content (e.g. retrieve similar structured scenes or compare spatial configurations), this may be suboptimal since models like convolutional neural networks struggle to reason about the relative position of objects in images. Previous studies on building hand-crafted spatial representations, thanks to Relative Position Descriptors (RPD), showed they were powerful to discriminate spatial relations between crisp objects, but such spatial descriptors have rarely been integrated into deep neural networks. We propose in this article different strategies embedded in a common framework called SpIRL (SPatially-aware Image Representation Learning) to guide the optimization of encoders to make them learn more spatial information, under the supervision of an RPD and with the help of a novel dataset (44k images) that does not induce learning semantic information. By using these strategies, we aim to help encoders build more spatially-aware representations. Our experimental results showcase that encoders trained under the SpIRL framework can capture accurate information about the spatial configurations of objects in images on two selected downstream tasks and public datasets.},
  archive      = {J_PR},
  author       = {Logan Servant and Michaël Clément and Laurent Wendling and Camille Kurtz},
  doi          = {10.1016/j.patcog.2025.112013},
  journal      = {Pattern Recognition},
  month        = {2},
  pages        = {112013},
  shortjournal = {Pattern Recognition},
  title        = {SpIRL: Spatially-aware image representation learning under the supervision of relative position descriptors},
  volume       = {170},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). VCGPrompt: Visual concept graph-aware prompt learning for vision-language models. <em>PR</em>, <em>170</em>, 112012. (<a href='https://doi.org/10.1016/j.patcog.2025.112012'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Prompt learning enables efficient fine-tuning of visual-language models (VLMs) like CLIP, demonstrating strong transferability across varied downstream tasks. However, adapting VLMs to open-vocabulary tasks is challenging due to the requirement to recognize diverse unseen data, which can cause overfitting and hinder generalization. To address this, we propose Visual Concept Graph-Aware Prompt Learning (VCGPrompt), which constructs visual concept graphs and uses fine-grained text prompts to enrich the general world knowledge of the model. Additionally, we introduce the Visual Concept Graph Aggregation Module (VCGAM) to prioritize the most distinctive visual concepts of each category and guide the learning of relevant visual features, which enhances the capability to perceive the open world. Our method achieves consistent improvements across three diverse generalization settings, including base-to-new, cross-dataset, and domain generalization, with performance gains of up to 0.95%. These results demonstrate the robustness and broad applicability of our approach under various scenarios. Detailed ablation studies and analyses validate the necessity of fine-grained prompts in the open-vocabulary setting.},
  archive      = {J_PR},
  author       = {Mengjia Wang and Fang Liu and Licheng Jiao and Shuo Li and Lingling Li and Puhua Chen and Xu Liu and Wenping Ma},
  doi          = {10.1016/j.patcog.2025.112012},
  journal      = {Pattern Recognition},
  month        = {2},
  pages        = {112012},
  shortjournal = {Pattern Recognition},
  title        = {VCGPrompt: Visual concept graph-aware prompt learning for vision-language models},
  volume       = {170},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Improve ranking algorithms based on matrix factorization in rating systems. <em>PR</em>, <em>170</em>, 112011. (<a href='https://doi.org/10.1016/j.patcog.2025.112011'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The proliferation of the Internet has led to an increase in the usage of rating systems. Inspired by matrix factorization, we present two improved iterative ranking algorithms called L1-AVG-RMF and AA-RMF for rating systems. In the new algorithms, the missing ratings are estimated by matrix factorization before applying traditional ranking algorithms. Theoretical analysis illustrates that the proposed algorithms have a better accuracy and robustness. And it is also demonstrated by Experiments with synthetic and real data. Additionally, experimental results also show that L1-AVG-RMF has superior effectiveness and robustness compared to some other ranking algorithms. Our findings emphasize the potential benefits of applying matrix factorization to ranking algorithms.},
  archive      = {J_PR},
  author       = {Shuyan Chen and Shengli Zhang and Gengzhong Zheng},
  doi          = {10.1016/j.patcog.2025.112011},
  journal      = {Pattern Recognition},
  month        = {2},
  pages        = {112011},
  shortjournal = {Pattern Recognition},
  title        = {Improve ranking algorithms based on matrix factorization in rating systems},
  volume       = {170},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Bidirectional skip-frame prediction for video anomaly detection with intra-domain disparity-driven attention. <em>PR</em>, <em>170</em>, 112010. (<a href='https://doi.org/10.1016/j.patcog.2025.112010'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Unsupervised video anomaly detection (VAD) trains only normal events to detect anomalies by large discrepancies during testing. Expanding the discriminative boundary between normal and abnormal events is the common goal and challenge of unsupervised VAD. To address this problem, we propose a Bidirectional Skip-frame Prediction (BiSP) method, leveraging intra-domain disparities between different features. Specifically, the BiSP skips frames in the training phase to achieve the forward and backward prediction respectively, and in the testing phase, it utilizes bidirectional consecutive frames to co-predict the same intermediate frames. Furthermore, we design the variance channel attention and context spatial attention from the perspectives of movement patterns and object scales, respectively. This design maximizes of the disparity between normal and abnormal in the feature extraction and delivery with different dimensions. Extensive experiments from four benchmark datasets demonstrate the effectiveness of the proposed BiSP, which substantially outperforms state-of-the-art competing methods. Ours code is available at https://github.com/jLooo/BiSP .},
  archive      = {J_PR},
  author       = {Jiahao Lyu and Minghua Zhao and Jing Hu and Runtao Xi and Xuewen Huang and Shuangli Du and Cheng Shi and Tian Ma},
  doi          = {10.1016/j.patcog.2025.112010},
  journal      = {Pattern Recognition},
  month        = {2},
  pages        = {112010},
  shortjournal = {Pattern Recognition},
  title        = {Bidirectional skip-frame prediction for video anomaly detection with intra-domain disparity-driven attention},
  volume       = {170},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). LiteNeRFAvatar: A lightweight NeRF with local feature learning for dynamic human avatar. <em>PR</em>, <em>170</em>, 112008. (<a href='https://doi.org/10.1016/j.patcog.2025.112008'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Creating high-quality dynamic human avatars within acceptable costs remains challenging in computer vision and computer graphics. The neural radiance field (NeRF) has become a fundamental means of generating human avatars due to its success in novel view synthesis. However, the storage-intensive and time-consuming per-scene training due to the transformation and evaluation of massive sampling points constrains its practical applications. In this paper, we introduce a novel lightweight NeRF model, LiteNeRFAvatar, to overcome these limits. To avoid the high-cost backward transformation of the sampling points, LiteNeRFAvatar decomposes the appearance features of clothed humans into multiple local feature spaces and transforms them forward according to human movements. Each local feature space affects a limited local area and is represented by an explicit feature volume created by the tensor decomposition techniques to support fast access. The sampling points retrieve the features based on the relative positions to the local feature spaces. The densities and the colors are then regressed from the aggregated features using a tiny decoder. We also adopt an empty space skipping strategy to further reduce the number of sampling points. Experimental results demonstrate that our LiteNeRFAvatar achieves a satisfactory balance between synthesis quality, training time, rendering speed and parameter size compared to the existing NeRF-based methods. For the demo of our method, please refer to the link on: https://youtu.be/UYfreeHtIZY . The source code will be released after the paper is accepted.},
  archive      = {J_PR},
  author       = {Junjun Pan and Xiaoyu Li and Junxuan Bai and Ju Dai},
  doi          = {10.1016/j.patcog.2025.112008},
  journal      = {Pattern Recognition},
  month        = {2},
  pages        = {112008},
  shortjournal = {Pattern Recognition},
  title        = {LiteNeRFAvatar: A lightweight NeRF with local feature learning for dynamic human avatar},
  volume       = {170},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Caudo-diff: Diffusion calibrated pseudo labels in guided latent space for minimally supervised medical segmentation. <em>PR</em>, <em>170</em>, 112007. (<a href='https://doi.org/10.1016/j.patcog.2025.112007'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Accurate segmentation of medical images is essential for clinical diagnosis and treatment planning. However, deep learning-based segmentation models are data-intensive, requiring large, well-annotated datasets which is an often challenging and costly requirement in medical fields. To reduce the reliance on manual labeling, we propose the minimally supervision based on an exemplar, leveraging only a single labeled sample while making full use of the remaining unlabeled data. In this case, two challenges need to be addressed. First, a lack of sufficient prior information: relying solely on a single exemplar limits the model’s ability to capture complex semantics. Second, the unreliability of pseudo labels: noise and inaccuracies in these labels introduce bias, hindering segmentation performance. To overcome these challenges, we propose a new pseudo-labeling paradigm by diffusion calibration. Follow this paradigm, we introduce Caudo-Diff, a novel method for calibrating pseudo labels using a deterministic diffusion model in a guided latent space, aiming to supplement prior information and improve pseudo-labeling reliability. Initial pseudo labels and features extracted by the segmentation network guide the model to focus on meaningful semantic regions. The pseudo labels are then refined to reduce noise and errors, enhancing segmentation accuracy. Experimental results show that Caudo-Diff improves segmentation performance with minimal supervision, offering a practical solution to the challenge of annotation scarcity in medical image segmentation.},
  archive      = {J_PR},
  author       = {Baoqi Yu and Yong Liu},
  doi          = {10.1016/j.patcog.2025.112007},
  journal      = {Pattern Recognition},
  month        = {2},
  pages        = {112007},
  shortjournal = {Pattern Recognition},
  title        = {Caudo-diff: Diffusion calibrated pseudo labels in guided latent space for minimally supervised medical segmentation},
  volume       = {170},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Graph pre-trained framework with spatio-temporal importance masking and fine-grained optimizing for neural decoding. <em>PR</em>, <em>170</em>, 112006. (<a href='https://doi.org/10.1016/j.patcog.2025.112006'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Neural decoding has always been the cutting-edge neuroscience issue, significant progress has been made in neural decoding with the support of deep learning technology. However, these breakthroughs are based on large-scale fully annotated functional magnetic resonance imaging (fMRI) data, which greatly hinders its further applicability. Recently, foundation models have garnered considerable attention in the realm of natural language processing, computer vision, and multimodal data processing due to their ability to circumvent the need for extensive annotated datasets while achieving notable accuracy gains. Nevertheless, the formulation of effective foundation model approaches tailored for connectivity-based complex spatio-temporal brain networks remains an unresolved challenge. To address these issues, in this paper, we proposed a general Temporal-Aware Graph Self-supervised Contrastive learning framework (TAGSC) for fMRI-based neural decoding. Concretely, it includes three innovative improvements to enhance fMRI-based graph foundation models: (i) a spatio-temporal augmentation strategy considers spatial brain region synergy and temporal information continuity to generate brain spatio-temporal contrastive views; (ii) a temporal-aware feature extractor learns brain spatio-temporal representations, which fully takes into account the continuous consistency of brain state transitions and fetches brain spatio-temporal interaction information from local to global; (iii) a fine-grained consistency loss assists in contrastive optimization from both temporal and spatial perspectives. Extensive evaluation on publicly available fMRI datasets demonstrated the superior performance of the proposed TAGSC and revealed biomarkers related to different states of the brain. To the best of our knowledge, it is among the earliest attempts to employ a spatio-temporal pre-trained model for neural decoding.},
  archive      = {J_PR},
  author       = {Ziyu Li and Zhiyuan Zhu and Qing Li and Xia Wu},
  doi          = {10.1016/j.patcog.2025.112006},
  journal      = {Pattern Recognition},
  month        = {2},
  pages        = {112006},
  shortjournal = {Pattern Recognition},
  title        = {Graph pre-trained framework with spatio-temporal importance masking and fine-grained optimizing for neural decoding},
  volume       = {170},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Learning embedded label-specific features for partial multi-label learning. <em>PR</em>, <em>170</em>, 112005. (<a href='https://doi.org/10.1016/j.patcog.2025.112005'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Partial multi-label learning (PML) aims to learn from instances with weak supervision, where each instance is associated with a set of candidate labels, among which only a subset is valid. Most existing approaches rely on identical feature representations to distinguish all class labels, overlooking the inherent distinctiveness of different labels, which leads to suboptimal model performance. Although recent studies have attempted to address this limitation by tailoring label-specific features, critical shortcomings remain: (1) isolated processing of feature tailoring and label disambiguation fails to leverage their synergistic relationship, and (2) direct extraction of label-specific features from the original feature space tends to yield unreliable results due to inherent noise and disturbances. This paper proposes a unified PML framework that jointly performs label disambiguation, embedded label-specific feature learning, and model induction. Within this framework, identifying ground-truth labels and generating label-specific features mutually reinforce each other, leading to continuous refinement. By customizing features from a compact and noise-free embedded space, the framework further ensures robustness and reliability in learning. Specifically, low-rank and sparse decomposition is employed to separate ground-truth labels from noisy ones, while a linear embedding discriminant model simultaneously generates embedded label-specific features and induces the model. Moreover, we enhance the classifier’s accuracy by assuming that the input and output spaces share local geometric structures, encouraging similar instances to have similar label sets. Extensive experiments on sixty-six real-world and synthetic datasets demonstrate that the proposed approach significantly outperforms state-of-the-art methods.},
  archive      = {J_PR},
  author       = {Xiaohan Xu and Hao Wang and Jialu Yao and Zan Zhang},
  doi          = {10.1016/j.patcog.2025.112005},
  journal      = {Pattern Recognition},
  month        = {2},
  pages        = {112005},
  shortjournal = {Pattern Recognition},
  title        = {Learning embedded label-specific features for partial multi-label learning},
  volume       = {170},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). MemoryFusion: A novel architecture for infrared and visible image fusion based on memory unit. <em>PR</em>, <em>170</em>, 112004. (<a href='https://doi.org/10.1016/j.patcog.2025.112004'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Existing image fusion methods utilize elaborate encoders to sequentially extract shallow and deep features from the source images. However, most methods lack long-term dependence, i.e. shallow details are inevitably lost when the network encodes deep features. To this end, some methods employ skip connections or dense connections to directly assign shallow features into deeper layers, potentially introducing redundant information and increasing computational loads. To overcome these drawbacks and enhance the generalization ability for low-quality scenarios, a novel fusion architecture based on Gated Recurrent Unit (GRU) termed as MemoryFusion is proposed. First, the Input Extension Encoder (IEE) transfers the source image into a feature sequence. Then a Recurrent Fusion Encoder (RFE) containing Recurrent Memory Fusion Unit (RMFU) is designed to learn the intrinsic correlation between the multi-modality feature sequences and generate the fusion feature sequence. This memory fusion unit utilizes a special gating mechanism to incorporate historical information and current input, and then adaptively selects the valuable content and forgets the redundant information. More importantly, it effectively relieves the computational pressure. Finally, since the modality information is distributed at different sequence depths and varying illumination intensity, the Multi-hierarchical Aggregation Module (MHAM) is designed to obtain the corresponding weight sequence. The aggregated fusion feature is obtained by integrating the fusion feature sequence with the weight sequence. Extensive experiments demonstrate that MemoryFusion is superior to the state-of-the-art fusion methods on multiple datasets. Even on low-quality images, such as low-light or foggy conditions, our method also demonstrates exceptional fusion performance and scene fidelity.},
  archive      = {J_PR},
  author       = {Jiachen He and Xiaoqing Luo and Zhancheng Zhang and Xiao-jun Wu},
  doi          = {10.1016/j.patcog.2025.112004},
  journal      = {Pattern Recognition},
  month        = {2},
  pages        = {112004},
  shortjournal = {Pattern Recognition},
  title        = {MemoryFusion: A novel architecture for infrared and visible image fusion based on memory unit},
  volume       = {170},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). An end-to-end shadow removal framework with an intuitive interaction scheme. <em>PR</em>, <em>170</em>, 112001. (<a href='https://doi.org/10.1016/j.patcog.2025.112001'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Shadow removal plays a crucial role in enhancing image quality by restoring the color and texture details of the shadow regions, thereby improving the performance of downstream visual tasks. Although recent shadow removal algorithms have achieved impressive results on benchmark datasets, shadows in such datasets are typically centralized and captured in relatively straightforward scenes. In contrast, real-world shadows tend to exhibit complex and irregular patterns due to the random distribution of objects, causing global processing methods to produce false positives and missed corrections. To address these challenges, this paper presents an end-to-end shadow removal framework leveraging Human-Computer Interaction (HCI), allowing simple bounding boxes to annotate targeted shadows. Our approach employs a novel chunked processing training strategy, which decomposes global shadow removal into iterative local refinements. Additionally, a Split-Channel module and an Edge-Weighted loss are incorporated to maintain consistent color and smooth edge transitions during restoration. Furthermore, an HSI-based shadow detection algorithm is proposed to generate shadow masks, facilitating end-to-end shadow removal. Experimental results demonstrate that our approach outperforms state-of-the-art methods on ISTD and SRD datasets, and exhibits robust performance on real-world images, effectively reducing restoration errors.},
  archive      = {J_PR},
  author       = {Ding Yuan and Yuqian Meng and Hanyang Liu and Yachun Feng and Hong Zhang and Yifan Yang},
  doi          = {10.1016/j.patcog.2025.112001},
  journal      = {Pattern Recognition},
  month        = {2},
  pages        = {112001},
  shortjournal = {Pattern Recognition},
  title        = {An end-to-end shadow removal framework with an intuitive interaction scheme},
  volume       = {170},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Domain-invariant representation learning via SAM for blood cell classification. <em>PR</em>, <em>170</em>, 112000. (<a href='https://doi.org/10.1016/j.patcog.2025.112000'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Accurate classification of blood cells is of vital significance in the diagnosis of hematological disorders, facilitating timely treatments for patients. However, in real-world scenarios, domain shifts caused by the variability in laboratory procedures and settings often result in rapid deterioration in model generalization performance. To address this issue, we propose a novel domain-invariant representation learning via the Segment Anything Model (SAM) for blood cell classification, referred to as DoRL. The DoRL comprises two main components: a LoRA-based SAM (LoRA-SAM) and a cross-domain autoencoder (CAE). The key advantage of DoRL is the ability to extract domain-invariant representations from various blood cell datasets in an unsupervised manner. Specifically, we first leverage the large-scale foundation model SAM, fine-tuned with LoRA, to generate robust and transferable visual representations of blood cells. Furthermore, we introduce the CAE to learn domain-invariant representations from the image embeddings across different-domain datasets. The CAE mitigates the impact of image artifacts and other domain-specific variations, ensuring the learned representations more generalizable. To validate the effectiveness of domain-invariant representations, we employ five widely used machine learning classifiers to construct blood cell classification models. Experimental results on two public blood cell datasets and a private real-world dataset demonstrate that our proposed DoRL achieves a new state-of-the-art cross-domain performance, surpassing existing methods by a significant margin. The DoRL, with its novel integration of LoRA-SAM and cross-domain autoencoding, provides a robust and effective solution for enhancing the generalization capabilities of blood cell classification models, potentially improving patient care and outcomes. The source code can be available at https://github.com/AnoK3111/DoRL .},
  archive      = {J_PR},
  author       = {Yongcheng Li and Lingcong Cai and Ying Lu and Cheng Lin and Yupeng Zhang and Jingyan Jiang and Genan Dai and Bowen Zhang and Jingzhou Cao and Xiangzhong Zhang and Xiaomao Fan},
  doi          = {10.1016/j.patcog.2025.112000},
  journal      = {Pattern Recognition},
  month        = {2},
  pages        = {112000},
  shortjournal = {Pattern Recognition},
  title        = {Domain-invariant representation learning via SAM for blood cell classification},
  volume       = {170},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Beyond conformal predictors: Adaptive conformal inference with confidence predictors. <em>PR</em>, <em>170</em>, 111999. (<a href='https://doi.org/10.1016/j.patcog.2025.111999'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Adaptive Conformal Inference (ACI) provides finite-sample coverage guarantees, enhancing the prediction reliability under non-exchangeability. This study demonstrates that these desirable properties of ACI do not require the use of Conformal Predictors (CP). We show that the guarantees hold for the broader class of confidence predictors, defined by the requirement of producing nested prediction sets, a property we argue is essential for meaningful confidence statements. We empirically investigate the performance of Non-Conformal Confidence Predictors (NCCP) against CP when used with ACI on non-exchangeable data. In online settings, the NCCP offers significant computational advantages while maintaining a comparable predictive efficiency. In batch settings, inductive NCCP (INCCP) can outperform inductive CP (ICP) by utilising the full training dataset without requiring a separate calibration set, leading to improved efficiency, particularly when the data are limited. Although these initial results highlight NCCP as a theoretically sound and practically effective alternative to CP for uncertainty quantification with ACI in non-exchangeable scenarios, further empirical studies are warranted across diverse datasets and predictors.},
  archive      = {J_PR},
  author       = {Johan Hallberg Szabadváry and Tuwe Löfström},
  doi          = {10.1016/j.patcog.2025.111999},
  journal      = {Pattern Recognition},
  month        = {2},
  pages        = {111999},
  shortjournal = {Pattern Recognition},
  title        = {Beyond conformal predictors: Adaptive conformal inference with confidence predictors},
  volume       = {170},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Dynamic example network for class-agnostic object counting. <em>PR</em>, <em>170</em>, 111998. (<a href='https://doi.org/10.1016/j.patcog.2025.111998'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This work addresses the class-agnostic counting and localization task, a critical challenge in computer vision where the goal is to count and locate objects of any category in an image using a few annotated examples. The primary challenge arises from the limited information on appearance due to the lack of diverse examples, which hampers the model’s ability to generalize to varied object appearances. To tackle this issue, we propose a dynamic example network (DEN), consisting of a Location and Example Decoder module (LEDM) designed to incrementally expand the set of examples and refine predictions through multiple iterations. Additionally, our negative example mining strategy identifies informative negative examples across the entire dataset, further improving the model’s discriminative capacity. Extensive experiments on five datasets—FSC-147, FSCD-LVIS, CARPARK, UAVCC, and Visdrone—demonstrate the effectiveness of our approach, showing marked improvements over several state-of-the-art methods. The source code and trained models will be publicly accessible to facilitate further research and application in the field.},
  archive      = {J_PR},
  author       = {Xinyan Liu and Guorong Li and Yuankai Qi and Ziheng Yan and Weigang Zhang and Laiyun Qing and Qingming Huang},
  doi          = {10.1016/j.patcog.2025.111998},
  journal      = {Pattern Recognition},
  month        = {2},
  pages        = {111998},
  shortjournal = {Pattern Recognition},
  title        = {Dynamic example network for class-agnostic object counting},
  volume       = {170},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). UR2P-dehaze: Learning a simple image dehaze enhancer via unpaired rich physical prior. <em>PR</em>, <em>170</em>, 111997. (<a href='https://doi.org/10.1016/j.patcog.2025.111997'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Image dehazing techniques aim to enhance contrast and restore details, which are essential for preserving visual information and improving image processing accuracy. Existing methods may struggle to capture the physical characteristics of images fully and deeply, which could limit their ability to reveal image details. To overcome this limitation, we propose an unpaired image dehazing network, called the Simple Image Dehaze Enhancer via Unpaired Rich Physical Prior (UR2P-Dehaze). First, to accurately estimate the illumination, reflectance, and color information of the hazy image, we design a Shared Prior Estimator (SPE) that is iteratively trained to ensure the consistency of illumination and reflectance, generating clear, high-quality images. Additionally, a self-monitoring mechanism is introduced to eliminate undesirable features, providing reliable priors for image reconstruction. Next, we propose Dynamic Wavelet Separable Convolution (DWSC), which effectively integrates key features across both low and high frequencies, significantly enhancing the preservation of image details and ensuring global consistency. Finally, to effectively restore the color information of the image, we propose an Adaptive Color Corrector that addresses the problem of unclear colors. The PSNR, SSIM, LPIPS, FID and CIEDE2000 metrics on the benchmark dataset show that our method achieves state-of-the-art performance. It also contributes to the performance improvement of downstream tasks. The project code is available at https://github.com/Fan-pixel/UR2P-Dehaze .},
  archive      = {J_PR},
  author       = {Minglong Xue and Shuaibin Fan and Shivakumara Palaiahnakote and Mingliang Zhou},
  doi          = {10.1016/j.patcog.2025.111997},
  journal      = {Pattern Recognition},
  month        = {2},
  pages        = {111997},
  shortjournal = {Pattern Recognition},
  title        = {UR2P-dehaze: Learning a simple image dehaze enhancer via unpaired rich physical prior},
  volume       = {170},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). SceneLLM: Implicit language reasoning in LLM for dynamic scene graph generation. <em>PR</em>, <em>170</em>, 111992. (<a href='https://doi.org/10.1016/j.patcog.2025.111992'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Dynamic scenes contain intricate spatio-temporal information, crucial for mobile robots, UAVs, and autonomous driving systems to make informed decisions. Parsing these scenes into semantic triplets Subject-Predicate-Object for accurate Scene Graph Generation (SGG) is highly challenging due to the fluctuating spatio-temporal complexity. Inspired by the reasoning capabilities of Large Language Models (LLMs), we propose SceneLLM , a novel framework that leverages LLMs as powerful scene analyzers for dynamic SGG. Our framework introduces a Video-to-Language (V2L) mapping module that transforms video frames into linguistic signals (scene tokens), making the input more comprehensible for LLMs. To better encode spatial information, we devise a Spatial Information Aggregation (SIA) scheme, inspired by the structure of Chinese characters, which encodes spatial data into tokens. Using Optimal Transport (OT), we generate an implicit language signal from the frame-level token sequence that captures the video’s spatio-temporal information. To further improve the LLM’s ability to process this implicit linguistic input, we apply Low-Rank Adaptation (LoRA) to fine-tune the model. Finally, we use a transformer-based SGG predictor to decode the LLM’s reasoning and predict semantic triplets. Our method achieves state-of-the-art results on the Action Genome (AG) benchmark, and extensive experiments show the effectiveness of SceneLLM in understanding and generating accurate dynamic scene graphs.},
  archive      = {J_PR},
  author       = {Hang Zhang and Zhuoling Li and Jun Liu},
  doi          = {10.1016/j.patcog.2025.111992},
  journal      = {Pattern Recognition},
  month        = {2},
  pages        = {111992},
  shortjournal = {Pattern Recognition},
  title        = {SceneLLM: Implicit language reasoning in LLM for dynamic scene graph generation},
  volume       = {170},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Online multi-label streaming feature selection by affinity significance, affinity relevance and affinity redundancy. <em>PR</em>, <em>170</em>, 111990. (<a href='https://doi.org/10.1016/j.patcog.2025.111990'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-label streaming feature selection has applied to various fields to deal with the applications that features arrive dynamically. However, most exist multi-label streaming feature selection methods ignore that a feature tends to provide more classification information for part of labels, rather than equal information for all labels. This phenomenon results part of labels get more information from selected features, while other labels lack information. In order to address the issue, we propose a novel multi-label streaming feature selection method. Firstly, we come up with the concept of affinity between features and labels. Secondly, we propose the concepts of affinity significance, affinity relevance and affinity redundancy to evaluate streaming features in three dimensions. Thirdly, we propose a novel multi-label streaming feature selection method named OMFS-FA. OMFS-FA has three phases to retain affinity significant features, remove affinity irrelevant features and remove affinity redundant features respectively. Finally, experiments on performance analysis, statistic analysis, number of selected features and running time analysis are conducted, verifying that OMFS-FA significantly outperforms other eleven methods in terms of effectiveness and efficiency.},
  archive      = {J_PR},
  author       = {Jianhua Dai and Duo Xu and Chucai Zhang},
  doi          = {10.1016/j.patcog.2025.111990},
  journal      = {Pattern Recognition},
  month        = {2},
  pages        = {111990},
  shortjournal = {Pattern Recognition},
  title        = {Online multi-label streaming feature selection by affinity significance, affinity relevance and affinity redundancy},
  volume       = {170},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Do it yourself dynamic single image super resolution network via ODE. <em>PR</em>, <em>170</em>, 111987. (<a href='https://doi.org/10.1016/j.patcog.2025.111987'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Single Image Super Resolution (SISR) aims at characterizing fine-grain information given a low-resolution image. Recent progress shows that SISR can be viewed as a dynamic process that can be modeled using Ordinary Differential Equations (ODEs). As a result, ODE inspired neural network shows superior performance with limited number of parameters, as well as interpretability for network structure. However, the current ODE based approach restricts the neural network structure to a static single-branch residual network, while dynamic structures can adaptively adjust their parameters(or even structures) best suitable for each test image and lead to better SISR performance. To take advantage of ODE and dynamic network structures in both, we introduce the Implicit Runge–Kutta scheme to construct an ODE-inspired multi-branch residual module that serves as a basic module, which is helpful to capture information at different scales. Then, an attention module is applied on the weights of the Implicit Runge–Kutta scheme to obtain a new dynamic network module, which is equivalent to encourage different branch to jointly attend different positions to obtain the best performance. Experiments demonstrate that our approach outperforms state-of-the-art ODE-inspired methods with less or comparable number of parameters.},
  archive      = {J_PR},
  author       = {Xiao Zhang and Zhen Zhang and Wei Wei and Lei Zhang and Yanning Zhang},
  doi          = {10.1016/j.patcog.2025.111987},
  journal      = {Pattern Recognition},
  month        = {2},
  pages        = {111987},
  shortjournal = {Pattern Recognition},
  title        = {Do it yourself dynamic single image super resolution network via ODE},
  volume       = {170},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Supervised incremental feature selection using regularization vector for dynamic multi-scale interval valued datasets. <em>PR</em>, <em>170</em>, 111985. (<a href='https://doi.org/10.1016/j.patcog.2025.111985'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Feature selection is pivotal for enhancing machine learning and data mining models, where its accuracy directly affects model performance and applicability. Traditional methods often overlook the dynamic nature of data and the multi-scale aspect of high-dimensional datasets, leading to limitations in real-world applications. This paper introduces a novel incremental feature selection method using a regularization vector ( R V ) tailored for dynamic multi-scale interval valued fuzzy decision systems ( D - M I v F D ) . The paper first establishes the concepts of object affiliation relation and class, providing a theoretical basis for integrating replay and regularization. It then introduces the affiliation contradictory state ( A C S ) and R V , broadening the application of contradictory state ( C S ) in dynamic settings and enabling efficient feature selection. The integration of regularization and replay strategies is realized through four algorithms designed for different update patterns. Empirical results across various datasets show that the proposed method significantly outperforms multiple conventional techniques, highlighting its practical potential for real-world deployments.},
  archive      = {J_PR},
  author       = {Zihan Feng and Xiaoyan Zhang},
  doi          = {10.1016/j.patcog.2025.111985},
  journal      = {Pattern Recognition},
  month        = {2},
  pages        = {111985},
  shortjournal = {Pattern Recognition},
  title        = {Supervised incremental feature selection using regularization vector for dynamic multi-scale interval valued datasets},
  volume       = {170},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). C2SLM: A correlation-based clustering-assisted sparse learning model for electric vehicle market demand forecasting. <em>PR</em>, <em>170</em>, 111984. (<a href='https://doi.org/10.1016/j.patcog.2025.111984'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Battery electric vehicle (BEV) market demand forecasting is challenged by high-dimensional feature selection from a regression model. The Least Absolute Shrinkage and Selection Operator (Lasso) is a sparse learning model used for this purpose. However, due to the correlation structure of data, Lasso can become unstable in its feature selection. This instability is further amplified in econometric panel data because of unobserved heterogeneity. Motivated by this challenge, we propose a correlation-based clustering-assisted sparse learning model (C2SLM) based on Lasso for a BEV market demand forecasting problem in the U.S. state of Alabama. Three stages structure the C2SLM: (i) correlation-polarized feature generation, (ii) panel-based feature clustering, and (iii) clustering-assisted Lasso (caLasso). We specifically aggregate the data to capture heterogeneous market factors for the panel. We test the C2SLM against benchmark models with or without panel consideration. Our proposed model outperforms others and sheds significant insights into BEV policymaking. Additionally, our model performs consistently robustly on Gross Domestic Product (GDP) and S&P 500 stock trading market datasets.},
  archive      = {J_PR},
  author       = {Muting Ma and Mesut Yavuz and Matthew Hudnall and Qin Wang},
  doi          = {10.1016/j.patcog.2025.111984},
  journal      = {Pattern Recognition},
  month        = {2},
  pages        = {111984},
  shortjournal = {Pattern Recognition},
  title        = {C2SLM: A correlation-based clustering-assisted sparse learning model for electric vehicle market demand forecasting},
  volume       = {170},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Staircase sign method: Boosting adversarial attacks by mitigating gradient distortion. <em>PR</em>, <em>170</em>, 111983. (<a href='https://doi.org/10.1016/j.patcog.2025.111983'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Crafting adversarial examples for the transfer-based attack is challenging and remains a research hot spot. Currently, such attack methods are based on the hypothesis that the substitute model and the victim model learn similar decision boundaries, and they conventionally apply Sign Method (SM) to manipulate the gradient as the resultant perturbation. Although SM is efficient, it only extracts the sign of gradient units but ignores their value difference, which inevitably leads to a deviation. Therefore, we propose a novel Staircase Sign Method (S 2 M) to alleviate this issue, thus boosting attacks. Technically, our method heuristically divides the gradient sign into several segments according to the values of the gradient units, and then assigns each segment with a staircase weight for better crafting adversarial perturbation. As a result, our adversarial examples perform better in both white-box and black-box manner without being more visible. Since S 2 M just manipulates the resultant gradient, our method can be generally integrated into the family of FGSM algorithms, and the computational overhead is negligible. Extensive experiments on the ImageNet dataset demonstrate the effectiveness of our proposed methods, which significantly improve the transferability ( i.e. , on average, 5.8% for normally trained models and 12.8% for adversarially trained defenses). Our code is available in https://github.com/qilong-zhang/Staircase-sign-method .},
  archive      = {J_PR},
  author       = {Pengpeng Zeng and Shengming Yuan and Qilong Zhang and Huimin Deng and Hui Xu and Lianli Gao},
  doi          = {10.1016/j.patcog.2025.111983},
  journal      = {Pattern Recognition},
  month        = {2},
  pages        = {111983},
  shortjournal = {Pattern Recognition},
  title        = {Staircase sign method: Boosting adversarial attacks by mitigating gradient distortion},
  volume       = {170},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Dynamic selection of gaussian samples for object detection on drone images via shape sensing. <em>PR</em>, <em>170</em>, 111978. (<a href='https://doi.org/10.1016/j.patcog.2025.111978'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Label assignment (LA) strategy has been extensively studied as a fundamental issue in object detection. However, the drastic scale changes and wide variations in shape (aspect ratio) of objects in drone images result in a sharp performance drop for general LA strategies. To address the above problems, we propose an adaptive Gaussian sample selection strategy for multi-scale objects via shape sensing. Specifically, we first conduct Gaussian modeling for receptive field priors and ground-truth (gt) boxes, ensuring that the non-zero distance metric between any feature point and any ground truth on the whole image is obtained. Subsequently, we theoretically analyze and show that Kullback–Leibler Divergence (KLD) can measure distance according to the characteristics of the object. Taking advantage of this property, we utilize the statistical characteristics of the top-K highest KLD-based matching scores as the positive sample selection threshold for each gt, thereby assigning adequate high-quality samples to multi-scale objects. More importantly, we introduce an adaptive shape-aware strategy that adjusts the sample quantity according to the aspect ratio of objects, guiding the network to balanced learning for multi-scale objects with various shapes. Extensive experiments show that our dynamic shape-aware LA strategy is applicable to a variety of advanced detectors and achieves consistently improved performances on two major benchmarks (i.e., VisDrone and UAVDT), demonstrating the effectiveness of our approach.},
  archive      = {J_PR},
  author       = {Yixuan Li and Yulong Xu and Renwu Sun and Pengnian Wu and Meng Zhang},
  doi          = {10.1016/j.patcog.2025.111978},
  journal      = {Pattern Recognition},
  month        = {2},
  pages        = {111978},
  shortjournal = {Pattern Recognition},
  title        = {Dynamic selection of gaussian samples for object detection on drone images via shape sensing},
  volume       = {170},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Unaligned multi-view clustering via diversified anchor graph fusion. <em>PR</em>, <em>170</em>, 111977. (<a href='https://doi.org/10.1016/j.patcog.2025.111977'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Clear sample correspondence across views is a key presupposition of traditional multi-view clustering. However, in practical applications, uncertainties during the data collection process may lead to the violation of this presupposition, producing unaligned multi-view data. In this paper, to overcome the obstacle of multi-view fusion caused by unaligned samples and achieve efficient unaligned multi-view clustering, a novel Diversified Anchor Graph Fusion (DAGF) method is proposed. Specifically, view-specific bipartite graphs with diversified anchors are constructed to adapt to the characteristics of unaligned multi-view data. Then, with the devised sample alignment and anchor integration strategy, these bipartite graphs are fused to learn a joint bipartite graph with explicit cluster membership structure. The proposed DAGF method not only overcomes the adverse effects of unaligned samples on cross-view information fusion, but also preserves complementary view-specific clustering structure information, enabling efficient and effective clustering. Systematic experimental results on real-world datasets demonstrate the advantages of the DAGF method in both clustering performance and computational complexity. Code available: https://github.com/revolution6575/DAGF.git .},
  archive      = {J_PR},
  author       = {Hongyu Jiang and Hong Tao and Zhangqi Jiang and Chenping Hou},
  doi          = {10.1016/j.patcog.2025.111977},
  journal      = {Pattern Recognition},
  month        = {2},
  pages        = {111977},
  shortjournal = {Pattern Recognition},
  title        = {Unaligned multi-view clustering via diversified anchor graph fusion},
  volume       = {170},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Leveraging pre-trained models for kernel machines. <em>PR</em>, <em>170</em>, 111961. (<a href='https://doi.org/10.1016/j.patcog.2025.111961'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Pre-training techniques have successfully promoted the training of neural networks. Since neural networks and kernel machines share similar properties, such as both learning the problems by the non-linear projection on features and both being capable of handling complex tasks, the idea of pre-training may also help kernel machines achieve promising training speed. However, existing pre-training-based kernel machine solvers show limited improvements on efficiency when the hyper-parameter varies. To effectively reduce the training cost, we propose a novel method that can make efficient use of pre-trained models to infer kernel machine models with different hyper-parameters. Our pre-training-based method is built on top of theoretical foundations. The difference between the model inferred based on pre-training and the optimal model is theoretically bounded by a constant. Experimental results show that our method can save an order of magnitude of training time compared with the existing approach while producing competitive accuracy.},
  archive      = {J_PR},
  author       = {Yawen Chen and Zeyi Wen and Jian Chen and Jin Huang},
  doi          = {10.1016/j.patcog.2025.111961},
  journal      = {Pattern Recognition},
  month        = {2},
  pages        = {111961},
  shortjournal = {Pattern Recognition},
  title        = {Leveraging pre-trained models for kernel machines},
  volume       = {170},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). WANN-DPC: Density peaks finding clustering based on weighted adaptive nearest neighbors. <em>PR</em>, <em>170</em>, 111953. (<a href='https://doi.org/10.1016/j.patcog.2025.111953'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {DPC (Density Peak Clustering) algorithm and most of its variants are unable to identify the cluster centers of dense and sparse clusters simultaneously. In addition, the “Domino Effect” of DPC cannot be entirely avoided in its variants. Despite ANN-DPC (Adaptive Nearest Neighbor DPC) being able to detect cluster centers of dense and sparse clusters, its adaptive nearest neighbors of a point may introduce bias in the local density, cluster centers and clustering. To address these limitations of ANN-DPC, the WANN-DPC (Weighted Adaptive Nearest Neighbor DPC) algorithm is proposed. The key contributions of WANN-DPC are as follows: (1) A novel weighted local density of a point is defined by weighting its close and far neighbors, (2) a correction factor is proposed to detect cluster centers in turn, and (3) a two-step assignment strategy is presented utilizing nearest neighbor relationships and weighted membership degrees. Extensive experiments on benchmark datasets demonstrate the superiority of the WANN-DPC over its peers.},
  archive      = {J_PR},
  author       = {Juanying Xie and Huan Yan and Mingzhao Wang and Philip W. Grant and Witold Pedrycz},
  doi          = {10.1016/j.patcog.2025.111953},
  journal      = {Pattern Recognition},
  month        = {2},
  pages        = {111953},
  shortjournal = {Pattern Recognition},
  title        = {WANN-DPC: Density peaks finding clustering based on weighted adaptive nearest neighbors},
  volume       = {170},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Boundary feature alignment for semi-supervised medical image segmentation. <em>PR</em>, <em>170</em>, 111946. (<a href='https://doi.org/10.1016/j.patcog.2025.111946'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Methods based on consistency regularization and pseudo-labeling are prevalent in semi-supervised medical image segmentation, often involving perturbation-invariant training at the input or feature level and pseudo-label self-training. However, these methods primarily focus on the overall generalization performance of the model while lacking an explicit mechanism for modeling boundary features. This imbalance between global consistency and local boundaries results in suboptimal boundary segmentation. In this work, we propose a boundary feature alignment (BFA) method for semi-supervised medical image segmentation to address the challenge of accurately localizing boundaries. Our core idea is to encourage the model to learn universal boundary feature representations for both labeled and unlabeled images during the core stage of feature extraction. Specifically, we design 3D boundary extractors tailored to extract both label and pseudo-label boundaries, achieving stable and salient boundaries. By mixing label and pseudo-label boundaries, the encoder is encouraged to learn a universal boundary feature representation. This process enables the early embedding of boundary features and implicitly fosters alignment across different annotation states. We implemented BFA within the mean-teacher framework and conducted extensive experiments on semi-supervised medical image segmentation datasets, including Left Atrium (LA), Pancreas-CT, and ACDC. The experimental results demonstrate that this simple and effective mechanism is highly efficient. Compared to existing methods, our approach achieved state-of-the-art performance. The project will be released on https://github.com/hyg902/BFA-main .},
  archive      = {J_PR},
  author       = {Yigeng Huang and Suwen Li and Zheng Guo and Qiao Mei and Zhuo Han and Xiujuan Wang and Huanqin Wang},
  doi          = {10.1016/j.patcog.2025.111946},
  journal      = {Pattern Recognition},
  month        = {2},
  pages        = {111946},
  shortjournal = {Pattern Recognition},
  title        = {Boundary feature alignment for semi-supervised medical image segmentation},
  volume       = {170},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Nearest-neighbor class prototype prompt and simulated logits for continual learning. <em>PR</em>, <em>170</em>, 111933. (<a href='https://doi.org/10.1016/j.patcog.2025.111933'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Continual learning allows a single model to acquire knowledge from a sequence of tasks within a non-static data stream without succumbing to catastrophic forgetting. Vision transformers, pre-trained on extensive datasets, have recently made prompt-based methods viable as exemplar-free alternatives to methods reliant on rehearsal. Nonetheless, the majority of these methods employ a key–value query system for integrating pertinent prompts, which might result in the keys becoming stuck in local minima. To counter this, we suggest a straightforward nearest-neighbor class prototype search approach for deducing task labels, which improves the alignment with appropriate prompts. Additionally, we boost task label inference accuracy by embedding prompts within the query function itself, thereby enabling better feature extraction from the samples. To further minimize inter-task confusion in cross-task classification, we incorporate simulated logits into the classifier during training. These logits emulate strong responses from other tasks, aiding in the refinement of the classifier’s decision boundaries. Our method outperforms many existing prompt-based approaches, setting a new state-of-the-art record on three widely-used class-incremental learning datasets.},
  archive      = {J_PR},
  author       = {Yue Lu and Jie Tan and Shizhou Zhang and Yinghui Xing and Guoqiang Liang and Yanning Zhang},
  doi          = {10.1016/j.patcog.2025.111933},
  journal      = {Pattern Recognition},
  month        = {2},
  pages        = {111933},
  shortjournal = {Pattern Recognition},
  title        = {Nearest-neighbor class prototype prompt and simulated logits for continual learning},
  volume       = {170},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Instruction-guided fusion of multi-layer visual features in large vision-language models. <em>PR</em>, <em>170</em>, 111932. (<a href='https://doi.org/10.1016/j.patcog.2025.111932'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Large Vision-Language Models (LVLMs) have achieved remarkable success in a wide range of multimodal tasks by integrating pre-trained vision encoders with large language models. However, current LVLMs primarily rely on visual features extracted from the final layers of the vision encoder, overlooking the complementary information available in shallower layers. While recent works have explored the fusion of multi-layer visual features in LVLMs, they typically adopt task-agnostic fusion strategies and do not examine the dependencies of these features on different tasks. To address these gaps, we systematically investigate the individual contributions of hierarchical visual features in the context of LVLMs. Our findings reveal that the visual features from different layers exhibit complementary strengths, with their effectiveness varying across different tasks. Motivated by these insights, we introduce an instruction-guided vision aggregator that dynamically fuses multi-layer visual features based on task-specific instructions. Extensive evaluations demonstrate the superior performance of our method. We hope this work will provide valuable insights into the adaptive use of hierarchical visual features in LVLMs. Our code is available at: https://github.com/YiZheng-zy/IGVA .},
  archive      = {J_PR},
  author       = {Xu Li and Yi Zheng and Haotian Chen and Xiaolei Chen and Yuxuan Liang and Chenghang Lai and Bin Li and Xiangyang Xue},
  doi          = {10.1016/j.patcog.2025.111932},
  journal      = {Pattern Recognition},
  month        = {2},
  pages        = {111932},
  shortjournal = {Pattern Recognition},
  title        = {Instruction-guided fusion of multi-layer visual features in large vision-language models},
  volume       = {170},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). GasSeg: A lightweight real-time infrared gas segmentation network for edge devices. <em>PR</em>, <em>170</em>, 111931. (<a href='https://doi.org/10.1016/j.patcog.2025.111931'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Infrared gas segmentation (IGS) focuses on identifying gas regions within infrared images, playing a crucial role in gas leakage prevention, detection, and response. However, deploying IGS on edge devices introduces strict efficiency requirements, and the intricate shapes and weak visual features of gases pose significant challenges for accurate segmentation. To address these challenges, we propose GasSeg, a dual-branch network that leverages boundary and contextual cues to achieve real-time and precise IGS. Firstly, a Boundary-Aware Stem is introduced to enhance boundary sensitivity in shallow layers by leveraging fixed gradient operators, facilitating efficient feature extraction for gases with diverse shapes. Subsequently, a dual-branch architecture comprising a context branch and a boundary guidance branch is employed, where boundary features refine contextual representations to alleviate errors caused by blurred contours. Finally, a Contextual Attention Pyramid Pooling Module captures key information through context-aware multi-scale feature aggregation, ensuring robust gas recognition under subtle visual conditions. To advance IGS research and applications, we introduce a high-quality real-world IGS dataset comprising 6,426 images. Experimental results demonstrate that GasSeg outperforms state-of-the-art models in both accuracy and efficiency, achieving 90.68% mIoU and 95.02% mF1, with real-time inference speeds of 215 FPS on a GPU platform and 62 FPS on an edge platform. The dataset and code are publicly available at: https://github.com/FisherYuuri/GasSeg .},
  archive      = {J_PR},
  author       = {Huan Yu and Jin Wang and Jingru Yang and Kaixiang Huang and Yang Zhou and Fengtao Deng and Guodong Lu and Shengfeng He},
  doi          = {10.1016/j.patcog.2025.111931},
  journal      = {Pattern Recognition},
  month        = {2},
  pages        = {111931},
  shortjournal = {Pattern Recognition},
  title        = {GasSeg: A lightweight real-time infrared gas segmentation network for edge devices},
  volume       = {170},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Lighting dark images with linear attention and decoupled network. <em>PR</em>, <em>170</em>, 111930. (<a href='https://doi.org/10.1016/j.patcog.2025.111930'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Nighttime photography encounters escalating challenges in extremely low-light conditions, primarily attributable to the ultra-low signal-to-noise ratio. For real-world deployment, a practical solution must not only produce visually appealing results but also require minimal computation. However, most existing methods are either focused on improving restoration performance or employ lightweight models at the cost of quality. This paper proposes a lightweight network that outperforms existing state-of-the-art (SOTA) methods in low-light enhancement tasks while minimizing computation. The proposed network incorporates Siamese Self-Attention Block (SSAB) and Skip-Channel Attention (SCA) modules, which enhance the model’s capacity to aggregate global information and are well-suited for high-resolution images. Additionally, based on our analysis of the low-light image restoration process, we propose a Two-Stage Framework that achieves superior results. Our model can restore a UHD 4K resolution image with minimal computation while keeping SOTA restoration quality.},
  archive      = {J_PR},
  author       = {Jiazhang Zheng and Qiuping Liao and Lei Li and Cheng Li and Yangxing Liu},
  doi          = {10.1016/j.patcog.2025.111930},
  journal      = {Pattern Recognition},
  month        = {2},
  pages        = {111930},
  shortjournal = {Pattern Recognition},
  title        = {Lighting dark images with linear attention and decoupled network},
  volume       = {170},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Hypergraph-driven landmark detection foundation model on echocardiography for cardiac function quantification. <em>PR</em>, <em>170</em>, 111927. (<a href='https://doi.org/10.1016/j.patcog.2025.111927'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cardiac function quantification is crucial for the diagnosis and treatment of heart diseases. At present, cardiac function quantification mainly relies on segmentation based methods or regression-based methods. Segmentation based methods are affected by the accuracy of the segmentation results, while regression-based methods lack interpretability. In this paper, we propose a hypergraph-driven landmark recognition foundation model on echocardiogram for cardiac function quantification. We leverage an adaptive dynamic hypergraph (ADH) foundation backbone to capture long-range correlations with enhanced controllability and stability. Simultaneously, we introduce a bidirectional hypergraph spatio-temporal (BHST) decoder that uses hypergraphs to capture spatial relationships between landmarks, while incorporating both forward and backward propagation to capture temporal dependencies. The model was trained and evaluated on large scale datasets (more than 10,000 patients and including public datasets CAMUS and EchoNet-Dynamic, as well as a private dataset EchoStrain). Experimental results show that our method outperforms other deep learning methods for landmark detection and cardiac function quantification relevant indicators (LVEF and GLS). The code for this work is publicly available: Project Code},
  archive      = {J_PR},
  author       = {Suyu Dong and Delong Li and Yixin Sun and Xiaoxiao Hu and Yuming Zhao},
  doi          = {10.1016/j.patcog.2025.111927},
  journal      = {Pattern Recognition},
  month        = {2},
  pages        = {111927},
  shortjournal = {Pattern Recognition},
  title        = {Hypergraph-driven landmark detection foundation model on echocardiography for cardiac function quantification},
  volume       = {170},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Multi-task dynamic graph learning for brain disorder identification with functional MRI. <em>PR</em>, <em>170</em>, 111922. (<a href='https://doi.org/10.1016/j.patcog.2025.111922'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Dynamic functional connectivity (FC) analysis based on resting-state functional magnetic resonance imaging (rs-fMRI) is widely used for automated diagnosis of brain disorders. A large number of dynamic FC analysis methods rely on sliding window techniques to extract time-varying features of brain activity from localized time periods. However, these methods are sensitive to window parameters and individual differences, leading to significant variability in the extracted features and impacting the stability and accuracy of disease classification. Additionally, while dynamic graph learning holds promise in modeling time-varying brain networks, existing methods still encounter difficulties in effectively capturing spatio-temporal dynamic information. Therefore, in this paper we propose a multi-task dynamic graph learning framework (MT-DGL) to align FC trajectories and learn the spatio-temporal dynamic information for brain disease recognition. The MT-DGL mainly includes three parts: (1) SPD-valued FC trajectory alignment module for overcoming the model’s dependence on sliding window parameters and mitigating the impact of asynchrony in execution rates across individuals, (2) Mamba-based multi-scale dynamic graph learning module for extracting spatio-temporal dynamic features from fMRI time series, and (3) multi-scale fusion and multi-task learning strategy to enhance the model’s understanding of age-related brain FC changes and improve the effectiveness of brain disorder identification. Experimental results indicate that the proposed method exhibits excellent performance in several publicly available fMRI datasets. Specifically, on the largest site in the ABIDE dataset, the accuracy and area under the curve reached 73.9% and 74.9%, respectively.},
  archive      = {J_PR},
  author       = {Yunling Ma and Chaojun Zhang and Di Xiong and Han Zhang and Shihui Ying},
  doi          = {10.1016/j.patcog.2025.111922},
  journal      = {Pattern Recognition},
  month        = {2},
  pages        = {111922},
  shortjournal = {Pattern Recognition},
  title        = {Multi-task dynamic graph learning for brain disorder identification with functional MRI},
  volume       = {170},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Densely activated self-attention for semantic segmentation. <em>PR</em>, <em>170</em>, 111920. (<a href='https://doi.org/10.1016/j.patcog.2025.111920'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In semantic segmentation, pixels often share the same mask labels across a vast region. However, in recent prevalent transformer-based models, predictions frequently suffer from incompleteness or discontinuities. The dilemma is caused by sparse activation of vanilla self-attention during feature extraction. During the vanilla self-attention, each query over-focuses on a small number of relevant keys but neglects numerous keys sharing the same category with it, restricting the capture of universal feature representation for its category. Such sparsely activated self-attention will further cause unignorable feature differences among tokens sharing the same class in feature maps, introducing noise on final predictions. To reduce such differences, we propose the Densely Activated self-attention Module (DAM), a novel pluggable module designed to generate densely activated self-attention. Inserted after the encoder, it encourages each query to attend to a broader range of keys, obtaining more consistent features. Experimental results on three widely used benchmarks with six different baselines demonstrate that DAM consistently improves performance with a negligible increase in parameters and FLOPs. Our work provides a new perspective on the behavior of self-attention in semantic segmentation.},
  archive      = {J_PR},
  author       = {Liwen Xiao and Wenze Liu and Zhicheng Wang and Yiran Wang and Zhiyu Pan and Hao Lu and Zhiguo Cao},
  doi          = {10.1016/j.patcog.2025.111920},
  journal      = {Pattern Recognition},
  month        = {2},
  pages        = {111920},
  shortjournal = {Pattern Recognition},
  title        = {Densely activated self-attention for semantic segmentation},
  volume       = {170},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Bayesian forward regularization replacing ridge in online randomized neural network with multiple output layers. <em>PR</em>, <em>170</em>, 111886. (<a href='https://doi.org/10.1016/j.patcog.2025.111886'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Forward regularization (-F) with unsupervised knowledge was advocated to replace canonical Ridge regularization (-R) in online linear learners, as it achieved a lower relative regret boundary. However, we observe that -F cannot perform as expected in practice, even possibly losing to -R for online tasks. We identify two main causes for this: (1) inappropriate intervened regularization, and (2) non-i.i.d. nature and data distribution changes in online learning (OL), both of which result in unstable posterior distribution and optima offset of the learner. To improve these, we first introduce the adjustable forward regularization (- k F), a more general -F with controllable knowledge intervention. We also derive - k F’s incremental updates with variable learning rate, and study relative regret and boundary in OL. Inspired by the regret analysis, to curb unstable penalties, we further propose - k F-Bayes style with k synchronously self-adapted to revise the intractable tuning of - k F by considering parametric posterior distribution changes in non-i.i.d. online data streams. Additionally, we integrate the - k F and - k F-Bayes into a multi-layer ensemble deep random vector functional link (edRVFL) and present two practical algorithms for batch learning, avoiding past replay and catastrophic forgetting. In experiments, we conducted tests on numerical simulation, tabular, and image datasets, where - k F-Bayes surpassed traditional -R and -F, highlighting the efficacy of ready-to-work - k F-Bayes and the great potentials of edRVFL- k F-Bayes in OL and continual learning (CL) scenarios.},
  archive      = {J_PR},
  author       = {Junda Wang and Minghui Hu and Ning Li and Ponnuthurai Nagaratnam Suganthan},
  doi          = {10.1016/j.patcog.2025.111886},
  journal      = {Pattern Recognition},
  month        = {2},
  pages        = {111886},
  shortjournal = {Pattern Recognition},
  title        = {Bayesian forward regularization replacing ridge in online randomized neural network with multiple output layers},
  volume       = {170},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Domain incremental learning for object detection. <em>PR</em>, <em>170</em>, 111882. (<a href='https://doi.org/10.1016/j.patcog.2025.111882'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Incremental learning is an important research spot that can help the model continually adapt to new data without forgetting old knowledge. In the field of object detection, existing works mainly focus on new-class incremental learning. However, in many practical applications, the well pre-trained object detection systems often suffer from abrupt performance degradations when adapting to new data domain, i.e. , the drastic changes in object size, class distribution and image context. To this end, we propose a new-instance incremental learning task for object detection, called Domain Incremental Learning with Limited Budgets (DILLB). DILLB adopts the conventional incremental setting to pre-train the detection network on the source dataset and incrementally fine-tune the network on the target domain. Its objective is to maximize the performance of the detector on the target dataset, while avoiding catastrophic forgetting on the source one. Meanwhile, to increase the challenge of DILLB, we also place strict restrictions on the access to label information of both source and target datasets, helping DILLB get closer to the practical applications. In terms of baselines, we not only reconstruct common approaches of incremental learning and domain adaption, but also propose a novel teacher–student based method for domain incremental object detection, which demonstrates obvious merits in improving performance and alleviating catastrophic forgetting. In this paper, we also propose a comprehensive experimental setup for DILLB to promote its research in object detection. Our source code is available at https://github.com/Disguiser15/DILLB .},
  archive      = {J_PR},
  author       = {Gen Luo and Jiamu Sun and Lei Jin and Yiyi Zhou and Qiang Xu and Rongrong Fu and Xiaoshuai Sun and Rongrong Ji},
  doi          = {10.1016/j.patcog.2025.111882},
  journal      = {Pattern Recognition},
  month        = {2},
  pages        = {111882},
  shortjournal = {Pattern Recognition},
  title        = {Domain incremental learning for object detection},
  volume       = {170},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Structure anchor graph learning for multi-view clustering. <em>PR</em>, <em>170</em>, 111880. (<a href='https://doi.org/10.1016/j.patcog.2025.111880'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the growth of data and diverse data sources, clustering large-scale multi-view data has emerged as a prominent topic in the field of machine learning. Anchor graph is an efficient strategy to improve the scalability of graph based multi-view clustering methods because it can capture the essence of the entire dataset by utilizing only a small set of representative anchor points. However, most existing anchor graph based methods encounter at least one of the following two challenges: the first one is the separation of anchor selection from the anchor graph construction process, while the second one is the requirement of an additional clustering step to generate the indicator matrix. Both of the separated steps can potentially lead to suboptimal solutions. In this paper, we propose structure anchor graph learning for multi-view clustering (SAGL), which jointly addresses the two challenges within a unified learning framework. Specifically, instead of utilizing the fixed anchors selected during the pre-processing step, SAGL jointly learns the consensus anchors in the latent space, and constructs anchor graph by assigning larger similarity values to sample-anchor pairs with shorter distances. Meanwhile, by manipulating the connected components of the anchor graph with rank constraint, SAGL obtains the anchor graph with clear cluster structure that can directly reveal the indicator of samples without any post-processing step. As a result, it becomes a truly one-stage end-to-end learning problem. In addition, a simple yet effective transformation is introduced to convert vector-sum-from to matrix-multiplication-form with trace operation, which leads an efficient optimization algorithm. Extensive experiments on several real-world multi-view datasets demonstrate the effectiveness and efficiency of the proposed methods over other state-of-the-art MvC methods.},
  archive      = {J_PR},
  author       = {Wei Guo and Zhe Wang and Wei Shao},
  doi          = {10.1016/j.patcog.2025.111880},
  journal      = {Pattern Recognition},
  month        = {2},
  pages        = {111880},
  shortjournal = {Pattern Recognition},
  title        = {Structure anchor graph learning for multi-view clustering},
  volume       = {170},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Boosting zero-shot learning through neuro-symbolic integration. <em>PR</em>, <em>170</em>, 111869. (<a href='https://doi.org/10.1016/j.patcog.2025.111869'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Zero-shot learning (ZSL) aims to train deep neural networks to recognize objects from unseen classes, starting from a semantic description of the concepts. Neuro-symbolic (NeSy) integration refers to a class of techniques that incorporate symbolic knowledge representation and reasoning with the learning capabilities of deep neural networks. However, to date, few studies have explored how to leverage NeSy techniques to inject prior knowledge during the training process to boost ZSL capabilities. Here, we present Fuzzy Logic Prototypical Network (FLPN) that formulates the classification task as prototype matching in a visual-semantic embedding space, which is trained by optimizing a NeSy loss. Specifically, FLPN exploits the Logic Tensor Network (LTN) framework to incorporate background knowledge in the form of logical axioms by grounding a first-order logic language as differentiable operations between real tensors. This prior knowledge includes class hierarchies (classes and macroclasses) along with robust high-level inductive biases. The latter allow, for instance, to handle exceptions in class-level attributes and to enforce similarity between images of the same class, preventing premature overfitting to seen classes and improving overall performance. Both class-level and attribute-level prototypes through an attention mechanism specialized for either convolutional- or transformer-based backbones. FLPN achieves state-of-the-art performance on the GZSL benchmarks AWA2 and SUN, matching or exceeding the performance of competing algorithms with minimal computational overhead. The code is available at https://github.com/FrancescoManigrass/FLPN .},
  archive      = {J_PR},
  author       = {Francesco Manigrasso and Fabrizio Lamberti and Lia Morra},
  doi          = {10.1016/j.patcog.2025.111869},
  journal      = {Pattern Recognition},
  month        = {2},
  pages        = {111869},
  shortjournal = {Pattern Recognition},
  title        = {Boosting zero-shot learning through neuro-symbolic integration},
  volume       = {170},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Learning region-aware style-content feature transformations for face image beautification. <em>PR</em>, <em>170</em>, 111861. (<a href='https://doi.org/10.1016/j.patcog.2025.111861'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As a representative image-to-image translation task, facial makeup transfer is typically performed by applying intermediate feature normalization, conditioned on the style information extracted from a reference image. However, the relevant methods are typically limited in range of applicability, due to that the style information is independent of source images and lack of spatial details. To realize precise makeup transfer and further associate with face component editing, we propose a Semantic Region Style-content Feature Transformation approach, which is referred to as SRSFT. Specifically, we encode both reference and source images into region-wise feature vectors and maps, based on semantic segmentation masks. To address the misalignment in poses and expressions, region-wise spatial transformations are inferred to align the reference and source masks, and are then applied to explicitly warp the reference feature maps to the source face, without any extra supervision. The resulting feature maps are fused with the source ones and inserted into a generator for image synthesis. On the other hand, the reference and source feature vectors are also fused and used to determine the modulation parameters at multiple intermediate layers. SRSFT is able to achieve superior beautification performance in terms of seamlessness and fidelity.},
  archive      = {J_PR},
  author       = {Zhen Xu and Si Wu},
  doi          = {10.1016/j.patcog.2025.111861},
  journal      = {Pattern Recognition},
  month        = {2},
  pages        = {111861},
  shortjournal = {Pattern Recognition},
  title        = {Learning region-aware style-content feature transformations for face image beautification},
  volume       = {170},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). A brainwave verification system integrating passwords with EEG templates for online identification and authentication. <em>PR</em>, <em>169</em>, 112009. (<a href='https://doi.org/10.1016/j.patcog.2025.112009'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, the popularity of brainwave verification has been steadily increasing, with a plethora of paradigms and algorithmic research emerging in this field. However, most works suffer from issues such as low performance, lack of online systems, irrevocability, and are far from meeting practical demands. In response to these challenges, we propose a dual-factor brainwave verification system that integrates passwords with electroencephalogram (EEG) templates. Our approach involved conducting single-target experiments to select sequences, multi-target offline experiments to optimize parameters, and ultimately performed online experiments for validation. Using a 240 Hz refresh rate, we constructed a keyboard input for EEG signals using a 127-bit m-sequence code, and combined passwords with EEG signals to achieve online identification and authentication functionalities. We collected cross-day data from 30 participants, with a time interval of approximately six months. In the online system, the recognition accuracy was 99.2 % with a 4-digit password (4 s), 99.93 % with a 6-digit password (6 s), and 100 % with an 8-digit password (8 s). The equal error rates for online authentication were 1.79 % (4-digit password, 8 s), 1.03 % (6-digit password, 12 s), and 0.8 % (8-digit password, 16 s). The proposed system introduces dual-factor verification, significantly improving online performance in cross-day recognition and authentication. Simultaneously, it allows template replacement without the need for repeated registration, thus providing revocability and enhancing the system's resilience against attacks. These advantages position the proposed system favorably in the field of biometric recognition, contributing to the practical application of EEG-based identity verification systems.},
  archive      = {J_PR},
  author       = {Hongze Zhao and Yijun Wang and Xiaorong Gao},
  doi          = {10.1016/j.patcog.2025.112009},
  journal      = {Pattern Recognition},
  month        = {1},
  pages        = {112009},
  shortjournal = {Pattern Recognition},
  title        = {A brainwave verification system integrating passwords with EEG templates for online identification and authentication},
  volume       = {169},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). DB-SBR: A dual-backbone model for student behavior recognition. <em>PR</em>, <em>169</em>, 111996. (<a href='https://doi.org/10.1016/j.patcog.2025.111996'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Aiming at the challenge of student behavior recognition in such complex scenarios as smart classroom, we propose a Dual-Backbone Model for Student Behavior Recognition (DB-SBR). First, we innovatively propose a dual-backbone feature extraction scheme for the same input, which effectively improves the robustness and feature extraction capability of the model. Among them, we design the CSP Bottleneck with Partial Convolution (CSPC) module for the construction and optimization of the CSPC ConvNet backbone network, and construct the Simplified Swin Transformer (SST) backbone network through structural optimization, which makes the model achieve a better balance between efficiency and accuracy. In addition, we propose an Adaptive Weighted Attention Feature Fusion (AAFusion) module, which realizes more effective feature integration and further promotes the comprehensive performance of the model. Experimental results on the SCB-Dataset3 demonstrate that DB-SBR outperforms existing methods, achieving F1-score, mAP@0.5, and mAP@0.5:0.95 of 0.687, 0.729, and 0.544, respectively, with improvements of 2.5 %, 3.7 %, and 4.2 % over the baseline. DB-SBR provides a new solution for high-quality student behavior recognition in smart classrooms.},
  archive      = {J_PR},
  author       = {Lin Wang and Xiaolong Xu},
  doi          = {10.1016/j.patcog.2025.111996},
  journal      = {Pattern Recognition},
  month        = {1},
  pages        = {111996},
  shortjournal = {Pattern Recognition},
  title        = {DB-SBR: A dual-backbone model for student behavior recognition},
  volume       = {169},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Dual-masked contrastive learning based hypergraph foundation model for whole slide images. <em>PR</em>, <em>169</em>, 111995. (<a href='https://doi.org/10.1016/j.patcog.2025.111995'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The spatial and contextual information in whole slide images (WSIs) is crucial for improving the diagnostic accuracy of computer-aided diagnosis (CAD). Although existing hypergraph-based methods have shown their effectiveness in capturing complex contextual dependencies among patches, they still lack the generalizability across cancer types and diagnostic tasks. Foundation models can address this issue by utilizing the strategy of self-supervised pre-training on large-scale datasets, which allows knowledge transfer across different tissue types and diseases. However, existing hypergraph self-supervised approaches have yet to effectively integrate fine-grained local tissue features with global inter-region dependencies. To this end, we propose a Dual-Masked Contrastive Learning based Hypergraph Foundation Model for WSIs. For pre-training the HFM, a novel node-hyperedge dual-masked hypergraph modeling strategy is developed. It masks parts of the node and hyperedge features to generate two augmented hypergraphs, respectively, and then trains an HFM to reconstruct the masked features for capturing local relationships among nodes and hyperedges. To enhance the model's ability to learn global dependencies in WSIs, the contrastive constraints are imposed on the augmented hypergraphs. Moreover, a max-mask strategy mitigates overfitting by preventing over-reliance on dominant features. After pre-training, the proposed DMCL-HFM can be fine-tuned for various downstream tasks, such as cancer survival prediction and subtype classification. Extensive experiments on WSI datasets from different tissues and diseases validate the effectiveness of DMCL-HFM, which consistently outperforms existing graph- and hypergraph-based approaches, as well as hypergraph pre-training approaches.},
  archive      = {J_PR},
  author       = {Xueying Zhou and Saisai Ding and Wenhua Zhang and Juncheng Li and Jun Wang and Jiasheng Chen and Jun Shi},
  doi          = {10.1016/j.patcog.2025.111995},
  journal      = {Pattern Recognition},
  month        = {1},
  pages        = {111995},
  shortjournal = {Pattern Recognition},
  title        = {Dual-masked contrastive learning based hypergraph foundation model for whole slide images},
  volume       = {169},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Advancing federated domain generalization in ophthalmology: Vision enhancement and consistency assurance for multicenter fundus image segmentation. <em>PR</em>, <em>169</em>, 111993. (<a href='https://doi.org/10.1016/j.patcog.2025.111993'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Federated learning has transformed privacy-preserving medical image analysis, but the diversity of imaging equipment and conditions poses significant challenges in creating models that generalize effectively across domains. Current federated domain generalization (FedDG) methods often require partial information sharing, which may compromise privacy standards. To address this, we introduce the Federated Domain-Generalization Vision Enhancement and Consistency Assurance (FedDG-VECA) approach. This method enhances the generalization ability of federated learning by independently strengthening local node, integrating a Federated Vision Feature Extractor (FVFE) for global data capture and local fine-tuning, a Federated Vision Augmentation Strategy (FVAS) to simulate diverse image distributions, and a Federated Bootstrapped Consistency Assurance (FBCA) mechanism using a dual MLP network for stable, consistent model performance across varied data sources. Initial experiments confirm that FedDG-VECA significantly improves model generalization without compromising privacy, ensuring robust and consistent diagnostic capabilities across multiple institutions.},
  archive      = {J_PR},
  author       = {Yuxin Ye and Nian Liu and Yang Zhao and Xianxun Zhu and Jun Wang and Yan Liu},
  doi          = {10.1016/j.patcog.2025.111993},
  journal      = {Pattern Recognition},
  month        = {1},
  pages        = {111993},
  shortjournal = {Pattern Recognition},
  title        = {Advancing federated domain generalization in ophthalmology: Vision enhancement and consistency assurance for multicenter fundus image segmentation},
  volume       = {169},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). MGCM: Multi-modal graph convolutional mamba for cancer survival prediction. <em>PR</em>, <em>169</em>, 111991. (<a href='https://doi.org/10.1016/j.patcog.2025.111991'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, multi-modal learning that combines pathology images and transcriptomics has emerged as a promising paradigm for cancer survival prediction. However, existing methods fail to fully exploit the complex co-expressions between genes, resulting in inadequate learning of transcriptomics and compromised prediction accuracy. Moreover, most existing studies seek to enhance cross-modal learning through attention mechanisms, which often lead to quadratic complexity when processing high-dimensional pathology and transcriptomic data, hindering the comprehensive learning of cross-modal correlations. To tackle the above limitations, in this paper, we propose a novel Multi-modal Graph Convolutional Mamba (MGCM) framework for cancer survival prediction. Our framework constructs multi-modal graphs for pathology and transcriptomics to enhance intra-modal representations. Specifically, we organize transcriptomics into a co-expression network and leverage a multi-level graph convolutional network to fully capture expressions of individual genes and their co-expressions. Meanwhile, pathology images are also represented as graphs, where nodes correspond to image patches and edges reflect the spatial adjacency between patches. This pathology graph is learned in a sample-and-aggregate manner, which promotes the exploration of various characteristics in tumor cells and their microenvironment. To improve cross-modal interactions with computational efficiency, we introduce a progressive strategy, incorporating a Bi-Interactive Mamba block to model initial cross-modal correlations, followed by a Tri-Interactive Mamba block to prioritize critical information and filter out redundancy for further refinement. Extensive experiments on three cancer datasets from The Cancer Genome Atlas (TCGA) demonstrate the superiority of our compared to other state-of-the-art uni-modal and multi-modal survival prediction approaches. The code will be available at https://github.com/gluucose/MGCM .},
  archive      = {J_PR},
  author       = {Jiaqi Cui and Yilun Li and Dinggang Shen and Yan Wang},
  doi          = {10.1016/j.patcog.2025.111991},
  journal      = {Pattern Recognition},
  month        = {1},
  pages        = {111991},
  shortjournal = {Pattern Recognition},
  title        = {MGCM: Multi-modal graph convolutional mamba for cancer survival prediction},
  volume       = {169},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). An unlabelled data-driven sparse coding fusion framework for tumor classification. <em>PR</em>, <em>169</em>, 111989. (<a href='https://doi.org/10.1016/j.patcog.2025.111989'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep learning has shown promise in clinically assisted diagnosis but is often limited by low interpretability and the scarcity of labelled data. To address these challenges, we propose an Unlabelled Data-driven Sparse Coding Fusion (UD-SCF) model that avoids the reliance on deep neural networks by directly utilizing the abundance of unlabelled data. The model unifies feature representation and classification under an interpretable sparse coding framework, optimized iteratively via a front-end fusion strategy. UD-SCF offers flexibility for integrating upstream and downstream components; in this work, we adopt deep non-negative matrix factorization (NMF) for feature extraction and inverse-projection sparse representation -based classification (SRC). The entire framework is optimized using a Mixed Gauss-Seidel and Jacobian ADMM (M-ADMM) algorithm, with theoretical convergence and stability analysis provided. Experimental results on public gene expression tumor datasets show that UD-SCF achieves outstanding multi-class tumor classification performance, with an average accuracy of 98.89 %, sensitivity of 100 %, specificity of 97.78 %, missed diagnosis rate of 0.00 %, and misdiagnosis rate of 2.22 %.},
  archive      = {J_PR},
  author       = {Xiaohui Yang and Quanling Zhao and Wenming Wu and Limin Su},
  doi          = {10.1016/j.patcog.2025.111989},
  journal      = {Pattern Recognition},
  month        = {1},
  pages        = {111989},
  shortjournal = {Pattern Recognition},
  title        = {An unlabelled data-driven sparse coding fusion framework for tumor classification},
  volume       = {169},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). A graph transformer-based foundation model for brain functional connectivity network. <em>PR</em>, <em>169</em>, 111988. (<a href='https://doi.org/10.1016/j.patcog.2025.111988'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Although foundation models have advanced many medical imaging fields, their absence in neuroimage analysis limits progress in neuroscience and clinical practice. Brain functional connectivity (FC) analysis is central to understanding brain function and widely used in neuroscience. We propose a foundation model tailored for brain functional connectivity networks (FCN). Our graph transformer model integrates node and edge embeddings to extract FCN features and adapts flexibly to classification, regression, and clustering via task-specific adapters. We validate the model on fMRI data from 10,718 scans across multiple tasks: gender classification, mental disorder classification (distinguishing schizophrenia or autism from healthy population), brain age prediction, and depressive and anxiety disorder biotyping. Compared to 14 competing methods, our model consistently outperforms them. Moreover, it facilitates biomarker discovery by identifying task-specific FC patterns. In summary, we present a novel, versatile foundation model for FCN that advances neuroimaging research through scalable and interpretable analysis.},
  archive      = {J_PR},
  author       = {Yulong Wang and Vince D Calhoun and Godfrey D Pearlson and Peter Kochunov and Theo G.M. van Erp and Yuhui Du},
  doi          = {10.1016/j.patcog.2025.111988},
  journal      = {Pattern Recognition},
  month        = {1},
  pages        = {111988},
  shortjournal = {Pattern Recognition},
  title        = {A graph transformer-based foundation model for brain functional connectivity network},
  volume       = {169},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). CE-AH: A contrast-enhanced attention hierarchical network for alzheimer's disease diagnosis based on structural MRI. <em>PR</em>, <em>169</em>, 111986. (<a href='https://doi.org/10.1016/j.patcog.2025.111986'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Numerous deep learning-based methods utilizing structural magnetic resonance imaging (sMRI) have been developed for diagnosing Alzheimer's disease (AD). However, the majority of these methods overlook the localized nature of brain atrophy. Moreover, they often rely on a single scale, neglecting the global structural information. To address these issues concurrently, this study introduces a Contrast-Enhanced Attention Hierarchical (CE-AH) network for AD diagnosis. Initially, we employ pretraining through contrastive learning to bolster the network's feature extraction capabilities. Subsequently, we engineer a hierarchical model that integrates dual attention mechanisms to extract multi-scale discriminative features. To overcome the convergence challenges and training instability inherent in patch-based methods, we implement group normalization in place of batch normalization. The CE-AH model's performance is assessed on the ADNI dataset, yielding outstanding classification outcomes. Furthermore, experimental results indicate that the training process of our proposed model is remarkably stable.},
  archive      = {J_PR},
  author       = {Tianxiang Wang and Qun Dai and Han Lu},
  doi          = {10.1016/j.patcog.2025.111986},
  journal      = {Pattern Recognition},
  month        = {1},
  pages        = {111986},
  shortjournal = {Pattern Recognition},
  title        = {CE-AH: A contrast-enhanced attention hierarchical network for alzheimer's disease diagnosis based on structural MRI},
  volume       = {169},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Deep supervised anomaly detection for generalized face forgery detection. <em>PR</em>, <em>169</em>, 111976. (<a href='https://doi.org/10.1016/j.patcog.2025.111976'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Nowadays, face forgery poses a significant threat to societal security, making the development of effective countermeasures imperative. Though most existing methods adopt neural networks to automatically extract discriminative features for forgery detection and have achieved promising results, significant challenges remain. Namely, when detecting forgery faces generated by unseen forgery methods, the detection performance degrades significantly, indicating poor generalization capability. To address such limitation, a novel deep supervised anomaly detection for generalized face forgery detection (DAGFD) is proposed in this paper. Specifically, the artifact map detector optimized by triplet focal loss and metric-softmax loss is first used to locate the forgery regions and obtain artifact maps. Next, forgery detection is reformulated from the supervised anomaly detection perspective, and the artifact map score is calculated to detect forgery videos. Furthermore, mean square error (MSE) loss is used to minimize the artifact map score of real samples while increase the one of forgery samples to generalize well to unseen forgery methods. Also, circle loss is used for auxiliary classifier to learn more discriminative artifact features. Finally, the experimental results demonstrate that the proposed method’s detection accuracy is better than other state-of-the-art methods.},
  archive      = {J_PR},
  author       = {Fan Zhang and Ting Yang and Lin Cao and Kangning Du and Yanan Guo and Peiran Song and Chen Shao},
  doi          = {10.1016/j.patcog.2025.111976},
  journal      = {Pattern Recognition},
  month        = {1},
  pages        = {111976},
  shortjournal = {Pattern Recognition},
  title        = {Deep supervised anomaly detection for generalized face forgery detection},
  volume       = {169},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Refining pseudo-labels through iterative mix-up for weakly supervised semantic segmentation. <em>PR</em>, <em>169</em>, 111975. (<a href='https://doi.org/10.1016/j.patcog.2025.111975'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Weakly supervised semantic segmentation (WSSS) aims to provide accurate pixel-level annotation based on only weak guidance, primarily derived from image-level labels. Recent WSSS methods exploit pseudo-labels generated from improved class activation maps (CAMs) to train a fine-grained classification model for semantic segmentation. However, these pseudo-labels are unreliable because they tend to either miss parts of the objects or include irrelevant regions due to weak guidance from individual images. In this paper, we propose a simple yet effective iterative mix-up strategy, Pseudo-Label-based Mix (PL-Mix), that refines pseudo-labels iteratively, thereby further enhancing WSSS performance. During each iteration, we migrate object regions from pseudo-labels produced in previous steps and render them with new contexts in a mix-up fashion. Due to model consistency enforcement across varied backgrounds and new combinations of multiple objects from enriched image samples, these pseudo-labels progressively become more accurate and reliable. Further enhanced by a masking strategy and a CAM-based earth mover’s distance loss, we achieve state-of-the-art performance on the PASCAL VOC2012 and MS COCO2014 benchmark datasets.},
  archive      = {J_PR},
  author       = {Yifan Wang and Kunhao Yuan and Gerald Schaefer and Xiyao Liu and Linglin Jing and Kehua Guo and James Z. Wang and Hui Fang},
  doi          = {10.1016/j.patcog.2025.111975},
  journal      = {Pattern Recognition},
  month        = {1},
  pages        = {111975},
  shortjournal = {Pattern Recognition},
  title        = {Refining pseudo-labels through iterative mix-up for weakly supervised semantic segmentation},
  volume       = {169},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Hybrid quantum sparse coding and dynamic convolution capsule network for enhanced image classification. <em>PR</em>, <em>169</em>, 111974. (<a href='https://doi.org/10.1016/j.patcog.2025.111974'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Image classification is a crucial process in various applications, including vision tracking, facial recognition, and motion identification. Traditional methods face challenges such as high computational complexity, slow convergence, and inaccurate classification. To address these, this paper introduces a Hybrid Quantum Sparse Coding with Dynamic Convolution Capsule Network (HQSC-DCCN). This multi-layer framework enhances image feature classification by incorporating advanced techniques for both local and global feature representation. Global semantic patterns are captured using the Residual Pooled Vision Transformer (RPViT), which combines hybrid pooled residual multihead attention for adaptive feature extraction. Residual local convolutional blocks focus on extracting local pattern extraction, while QSC enhances feature compression and ensures efficient convergence by avoiding local minima. The encoded features are enhanced using a Dynamic Graph Convolutional Network (D-GCN). The final classification is performed using a Dynamic Convolution Capsule Network (DCCN), which preserves spatial hierarchies, leading to improved classification performance and computational efficiency. The proposed HQSC-DCCN model achieves superior performance with an average classification accuracy of 98.59 %, a recall of 98.80 %, an F1 score of 98.32 %, and a specificity of 97.76 % averaged across five benchmark datasets. This approach significantly improves feature representation and classification accuracy, making it well-suited for complex and noisy images in real-world scenarios.},
  archive      = {J_PR},
  author       = {Zhong Shu},
  doi          = {10.1016/j.patcog.2025.111974},
  journal      = {Pattern Recognition},
  month        = {1},
  pages        = {111974},
  shortjournal = {Pattern Recognition},
  title        = {Hybrid quantum sparse coding and dynamic convolution capsule network for enhanced image classification},
  volume       = {169},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Phys-EdiGAN: A privacy-preserving method for editing physiological signals in facial videos. <em>PR</em>, <em>169</em>, 111966. (<a href='https://doi.org/10.1016/j.patcog.2025.111966'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Remote Photoplethysmography (rPPG) technology can derive physiological signals such as heart rate by detecting subtle color changes in facial videos, thereby assessing people’s physiological and health conditions. However, since this technology can extract physiological data through ordinary cameras without the subject’s awareness, it poses a risk of misuse for monitoring and analyzing an individual’s physiological and psychological state, potentially leading to significant privacy breaches. To address this issue, this paper introduces Phys-EdiGAN, a facial video physiological signals editing method based on Conditional Generative Adversarial Networks (CGANs). This method enables customizable editing of physiological privacy information in facial videos, effectively preventing privacy leaks. Phys-EdiGAN introduces beneficial noise, allowing the model to learn decoupled representations between rPPG signal features and other features, thereby improving the editing of the rPPG signal in the video. We evaluated Phys-EdiGAN on several popular databases (e.g., UBFC-rPPG and VIPL-HR), and the results demonstrate that it allows precise customization of physiological privacy information while preserving high video quality and ensuring that facial identity, expressions, and movements remain consistent. This technology enhances user trust in facial video monitoring, prevents misuse of physiological data, and improves the overall security of video monitoring systems.},
  archive      = {J_PR},
  author       = {Xiaoguang Tu and Zhiyi Niu and Juhang Yin and Yanyan Zhang and Ming Yang and Lin Wei and Yu Wang and Zhaoxin Fan and Jian Zhao},
  doi          = {10.1016/j.patcog.2025.111966},
  journal      = {Pattern Recognition},
  month        = {1},
  pages        = {111966},
  shortjournal = {Pattern Recognition},
  title        = {Phys-EdiGAN: A privacy-preserving method for editing physiological signals in facial videos},
  volume       = {169},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). A taylor expansion-based texture and edge-preserving interpolation approach for arbitrary-scale image super-resolution. <em>PR</em>, <em>169</em>, 111965. (<a href='https://doi.org/10.1016/j.patcog.2025.111965'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The arbitrary-scale image super-resolution (ASISR) method based on implicit neural representation has exhibited remarkable performance. However, it struggles with texture and edge structure restoration, particularly at large upsample scales. This limitation arises from employing per-pixel L 1 loss exclusively and lacking constraints on image edge information within the upsample modules. To address this issue, we propose a novel texture and edge-preserving interpolation approach that leverages Taylor expansion within the upsample module for ASISR task and introduce a gradient loss, enabling better capture of subtle changes and edge structures. Leveraging the approximation capabilities of Taylor expansion, our approach improves the utilization of information of surrounding pixels for more accurate image restoration. In particular, we confine the space of the recovered images to the Bounded Variation space by the gradient loss, allowing functions to exhibit skip discontinuities (i.e., sharp edges of the image) while limiting the total variation. Then, we apply the Taylor expansion-based interpolation within this constrained space to achieve high-quality restored images. This approach maintains the overall smoothness of the images while effectively restoring the edges. Additionally, we introduce a Gradient-based Attention Block that can be seamlessly integrated into the standard Swin Transformer Layer to capture high-frequency information within features. Experimental results demonstrate that our interpolation approach significantly enhances the recovery of edge structure information and texture in SR images, outperforming previous state-of-the-art methods, especially on Urban100 and Manga109 datasets, which have complex line structures and texture details. Our code is available at https://github.com/boohit/Taylor-Expansion-based-EIUM .},
  archive      = {J_PR},
  author       = {Shibo Wang and Yuming Xing and Shengzhu Shi and Zhichang Guo},
  doi          = {10.1016/j.patcog.2025.111965},
  journal      = {Pattern Recognition},
  month        = {1},
  pages        = {111965},
  shortjournal = {Pattern Recognition},
  title        = {A taylor expansion-based texture and edge-preserving interpolation approach for arbitrary-scale image super-resolution},
  volume       = {169},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). HumanRecon: Neural reconstruction of dynamic human using geometric cues and physical priors. <em>PR</em>, <em>169</em>, 111964. (<a href='https://doi.org/10.1016/j.patcog.2025.111964'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent methods for dynamic human reconstruction have achieved promising reconstruction results. Most of these methods rely only on RGB color supervision without considering explicit geometric constraints or physical priors. This makes existing human reconstruction techniques more prone to overfitting to color and causes inherent geometric ambiguities, especially in sparse multi-view setups. Motivated by recent advances in monocular geometry prediction, we consider the geometric constraints of estimated depth and normals in learning neural implicit representation for dynamic human reconstruction. As a geometric regularization, this provides reliable yet explicit supervision information and improves reconstruction quality. We also exploit several beneficial physical priors, such as adding noise to the view direction and maximizing density on the human surface. These priors ensure the color rendered along rays is robust to view direction and reduce the inherent ambiguities of density estimated along rays. Experimental results demonstrate that depth and normal cues, predicted by human-specific monocular estimators, can provide effective supervision signals and render more accurate images. Finally, we also show that the proposed physical priors significantly reduce overfitting and improve the quality of novel view synthesis. Our code is available at: https://github.com/PRIS-CV/HumanRecon .},
  archive      = {J_PR},
  author       = {Junhui Yin and Wei Yin and Hao Chen and Xuqian Ren and Zhanyu Ma and Jun Guo and Yifan Liu},
  doi          = {10.1016/j.patcog.2025.111964},
  journal      = {Pattern Recognition},
  month        = {1},
  pages        = {111964},
  shortjournal = {Pattern Recognition},
  title        = {HumanRecon: Neural reconstruction of dynamic human using geometric cues and physical priors},
  volume       = {169},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Multimodal latent emotion recognition from micro-expression and physiological signal. <em>PR</em>, <em>169</em>, 111963. (<a href='https://doi.org/10.1016/j.patcog.2025.111963'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper aims at the benefits of incorporating multimodal data for improving latent emotion recognition accuracy, focusing on micro-expression (ME) and physiological signals (PS). In particular, we propose a novel multimodal learning framework that combines ME and PS information. This framework includes a novel 1D separable and mixable depthwise inception CNN, tailored to extract informative features from diverse physiological signals effectively. Additionally, we develop a standardised normal distribution weighted feature fusion methodology, which facilitates the reconstruction of feature maps from different frames within micro-expression videos. To achieve comprehensive multimodal learning, we introduce guided attention modules that assist recognising latent emotions from micro-expressions (including colour and depth information) and physiological signals. Our empirical results show that the proposed approach outperforms the benchmark method, with the weighted fusion method and guided attention modules both contributing to enhanced performance.},
  archive      = {J_PR},
  author       = {Liangfei Zhang and Yifei Qian and Ognjen Arandjelović and Tianyi Zhu and Hongjiang Xiao},
  doi          = {10.1016/j.patcog.2025.111963},
  journal      = {Pattern Recognition},
  month        = {1},
  pages        = {111963},
  shortjournal = {Pattern Recognition},
  title        = {Multimodal latent emotion recognition from micro-expression and physiological signal},
  volume       = {169},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Confident learning-based noise correction for crowdsourcing. <em>PR</em>, <em>169</em>, 111962. (<a href='https://doi.org/10.1016/j.patcog.2025.111962'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In crowdsourcing scenarios, each instance’s multiple noisy labels are collected from different crowd workers. Afterward, its integrated label is inferred through label integration. Because integrated labels always contain a certain level of noise, numerous noise correction algorithms have been proposed in recent years. However, almost all of them do not take into account the non-uniformity of label noise under the class-conditional noise (CCN) assumption. To our knowledge, confident learning, a framework of learning with noisy labels based on the CCN assumption, demonstrates robustness to non-uniform label noise. Therefore, this paper introduces confident learning into crowdsourcing scenarios and proposes a confident learning-based noise correction (CLNC) algorithm. In CLNC, we first concatenate each instance’s original attributes with its multiple noisy labels to construct an augmented attribute view and train a classifier on it. We then use this classifier to obtain a portion of predicted labels with high confidence. Subsequently, we use the original integrated labels and the predicted labels to filter instances, obtaining a clean set and a noisy set. Finally, we train two heterogeneous classifiers on the clean set to correct the instances in the noisy set. The experimental results on simulated and real-world crowdsourced datasets validate the effectiveness of our CLNC.},
  archive      = {J_PR},
  author       = {Bingrui Su and Liangxiao Jiang and Shanshan Si},
  doi          = {10.1016/j.patcog.2025.111962},
  journal      = {Pattern Recognition},
  month        = {1},
  pages        = {111962},
  shortjournal = {Pattern Recognition},
  title        = {Confident learning-based noise correction for crowdsourcing},
  volume       = {169},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). No blind alignment but generation: A different view of continuous sign language recognition based on diffusion. <em>PR</em>, <em>169</em>, 111960. (<a href='https://doi.org/10.1016/j.patcog.2025.111960'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Continuous sign language recognition (CSLR) tasks typically rely on cross-modal alignment. However, due to the weakly supervised nature of CSLR, manual alignment often fails to align sign frames with glosses accurately. In this paper, we present a novel perspective for CSLR by treating it as a video-to-text generation task. Utilizing a diffusion model, we bypass the error-prone weakly supervised alignment. By fully exploiting the cross-modal feature correspondence capability of the diffusion model and capitalizing on the semantic consistency between sign videos and textual glosses, the gloss features are obtained directly. Furthermore, we introduce a contrastive learning-based strategy to enhance feature representation during the denoising phase, offering a sophisticated alternative to the simplistic attention mechanisms traditionally employed and guiding the denoising process of the diffusion model. Comprehensive experiments conducted on four public continuous sign language recognition datasets highlight the effectiveness of our generative approach in CSLR, demonstrating that it outperforms state-of-the-art methods.},
  archive      = {J_PR},
  author       = {Xi Geng and Yunan Li and Zhuoqi Ma and Zixiang Lu and Qiguang Miao},
  doi          = {10.1016/j.patcog.2025.111960},
  journal      = {Pattern Recognition},
  month        = {1},
  pages        = {111960},
  shortjournal = {Pattern Recognition},
  title        = {No blind alignment but generation: A different view of continuous sign language recognition based on diffusion},
  volume       = {169},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Dynamic multi-modal hypergraph learning for semi-supervised multi-label image recognition. <em>PR</em>, <em>169</em>, 111959. (<a href='https://doi.org/10.1016/j.patcog.2025.111959'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, multi-label image recognition has emerged as a crucial task in computer vision, requiring simultaneous detection of multiple objects or attributes within images. Unlike single-label classification, this task demands explicit modeling of complex label correlations. Existing methods primarily focus on low-order pairwise relationships, failing to capture higher-order dependencies critical for real-world scene understanding. Additionally, the long-tail distribution of labels often causes models to prioritize frequent head labels while neglecting rare tail labels with limited training samples. To address these challenges, we propose a Dynamic Multi-modal Hypergraph Learning (DMHL) framework for semi-supervised multi-label recognition. DMHL constructs adaptive hypergraphs by fusing visual features, co-occurrence statistics, and textual embeddings. The framework dynamically refines these hypergraphs through three novel modules: HyperPrune, which prunes redundant hyperedges; HyperTransform, which generates dynamic hyperedges from node features; and HyperTune, which optimizes hypergraph weights via feature similarity alignment. These dynamic optimization modules enable DMHL to capture intricate high-order label correlations. Furthermore, DMHL employs hypergraph residual concatenation to enhance deep feature representations, which are leveraged for dynamic pseudo-label generation to alleviate label imbalance. Extensive experiments demonstrate DMHL achieves state-of-the-art results across four benchmarks: 86.0% mAP on MS-COCO (0.7% gain over prior SOTA), 96.3%/96.5% on Pascal VOC 2007/2012, and 64.0% on NUS-WIDE. Notably, in semi-supervised settings with 5% labeled data, DMHL surpasses previous methods by over 20% mAP on MS-COCO (70.5%), highlighting its effectiveness in capturing intricate label relationships and improving tail label recognition.},
  archive      = {J_PR},
  author       = {Chen Zhang and Cheng Xu and Yu Xie and Wenjie Mao and Bin Yu},
  doi          = {10.1016/j.patcog.2025.111959},
  journal      = {Pattern Recognition},
  month        = {1},
  pages        = {111959},
  shortjournal = {Pattern Recognition},
  title        = {Dynamic multi-modal hypergraph learning for semi-supervised multi-label image recognition},
  volume       = {169},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). DFINet: Dynamic feedback iterative network for infrared small target detection. <em>PR</em>, <em>169</em>, 111958. (<a href='https://doi.org/10.1016/j.patcog.2025.111958'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, deep learning-based methods have made impressive progress in infrared small target detection (IRSTD). However, the weak and variable nature of small targets constrains the feature extraction and scene adaptation of existing methods, leading to low data utilization and poor robustness. To address this issue, we innovatively introduce the feedback mechanism into IRSTD and propose the dynamic feedback iterative network (DFINet). The main motivation is to guide the model training and prediction utilizing the history prediction mask (HPMK) of previous rounds. On the one hand, in the training phase, DFINet can further mine the key features of real targets by training in multiple iterations with limited data; on the other hand, in the prediction phase, DFINet can correct the wrong results through feedback iterative to improve the model robustness. Specifically, we first propose the dynamic feedback feature fusion module (DFFFM), which dynamically interacts HPMK with feature maps through a hard attention mechanism to guide feature mining and error correction. Then, for better feature extraction, the cascaded hybrid pyramid pooling module (CHPP) is devised to capture both global and local information. Finally, we propose the dynamic semantic fusion module (DSFM), which innovatively utilizes feedback information to guide the fusion of high-level and low-level features for better feature representation in different scenarios. Extensive experimental results on publicly available datasets of NUDT-SIRST, IRSTD-1k, and SIRST Aug show that DFINet outperforms several state-of-the-art methods and achieves superior detection performance. Our code will be publicly available at https://github.com/uisdu/DFINet .},
  archive      = {J_PR},
  author       = {Jing Wu and Changhai Luo and Zhaobing Qiu and Liqiong Chen and Rixiang Ni and Yunxiang Li and Feng Huang and Jian Wu},
  doi          = {10.1016/j.patcog.2025.111958},
  journal      = {Pattern Recognition},
  month        = {1},
  pages        = {111958},
  shortjournal = {Pattern Recognition},
  title        = {DFINet: Dynamic feedback iterative network for infrared small target detection},
  volume       = {169},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Modality-specificity multi-aware evidence fusion algorithm using CFP and OCT for fundus diseases diagnosis. <em>PR</em>, <em>169</em>, 111957. (<a href='https://doi.org/10.1016/j.patcog.2025.111957'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Automatic diagnosis of fundus diseases is crucial for clinical decision support. Existing single-modality and single-aware methods often underutilize the rich information present in fundus images. To overcome this, we propose a novel modality-specific multi-aware evidence fusion (MSMAE) algorithm for accurate fundus disease diagnosis and grading. Our approach first identifies tissue regions by extracting keypoints from color fundus photographs (CFP) and segmenting supervoxels from optical coherence tomography (OCT) using a conditional random field supervoxel (CRF-SV) method. For each modality, tissue-aware, structure-aware, and global-aware features are extracted to comprehensively capture local and contextual information. An evidence fusion module integrates these multi-aware predictions within each modality, while an evidence knowledge fusion module leverages prior knowledge to reconcile discrepancies across modalities. Furthermore, a progressive transfer fusion curriculum learning strategy is introduced to guide the model from simple to complex grading tasks. Experimental results on benchmark datasets demonstrate that MSMAE consistently outperforms state-of-the-art methods in both disease diagnosis and grading. The codes are available at https://github.com/ecustyy/msmae .},
  archive      = {J_PR},
  author       = {Yang Yu and Hongqing Zhu and Tianwei Qian and Ning Chen and Bingcang Huang},
  doi          = {10.1016/j.patcog.2025.111957},
  journal      = {Pattern Recognition},
  month        = {1},
  pages        = {111957},
  shortjournal = {Pattern Recognition},
  title        = {Modality-specificity multi-aware evidence fusion algorithm using CFP and OCT for fundus diseases diagnosis},
  volume       = {169},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Transform domain tensor-based deep unrolling network for single-frame infrared small target detection. <em>PR</em>, <em>169</em>, 111956. (<a href='https://doi.org/10.1016/j.patcog.2025.111956'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Single-frame infrared small target (SIRST) detection is crucial in military and civilian applications. However, accurate SIRST detection is challenging due to the lack of target texture feature and low signal-to-noise ratio. One promising approach is to use constrained image models exploiting low-rank and sparse priors to separate the target and the background. However, a challenge in using preselected models is the mismatch between the model and the dataset. Deep learning models have achieved promising results, whereas the lack of interpretability in most of these methods renders them unsuitable for detecting dim and small target. In this paper, we propose a novel deep unrolling network based on adaptive tensor priors in the transform domain, namely TTDUNet. Specifically, the infrared image is constructed into patch tensors composed of low-rank background and sparse target components. We define a generalized tensor singular value decomposition based on arbitrary unitary transformations and utilize the transform domain tensor nuclear norm to enforce the low-rankness of the background tensor. Simultaneously, the sparsity of target component is exploited by a learnable sparse transformation. The detection problem is thus posed as an optimization problem solved by the alternating direction method of multipliers (ADMM). The iterative steps are unrolled into a deep neural network to adaptively learn tensor transform domain and associated hyperparameters. Additionally, we introduce a novel cross-layer feature fusion module and a dynamic tensor singular value threshold module to capture cross-dimensional dependencies and enhance tensor representation robustness. Experimental results on public datasets demonstrate TTDUNet's superiority over the state-of-the-art methods.},
  archive      = {J_PR},
  author       = {Bin Xiao and Yue Hu},
  doi          = {10.1016/j.patcog.2025.111956},
  journal      = {Pattern Recognition},
  month        = {1},
  pages        = {111956},
  shortjournal = {Pattern Recognition},
  title        = {Transform domain tensor-based deep unrolling network for single-frame infrared small target detection},
  volume       = {169},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Structural feature enhanced transformer for fine-grained image recognition. <em>PR</em>, <em>169</em>, 111955. (<a href='https://doi.org/10.1016/j.patcog.2025.111955'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Existing fine-grained image recognition (FGIR) models mainly rely on high-level semantic features to extract discriminative information, ignoring the potential role of the overall structural information of objects and the structural relationships between key parts. To address this issue, we propose the Structural Feature Enhancement Transformer (SFETrans). SFETrans consists of a visual transformer backbone network responsible for extracting complex semantic features. Additionally, it includes a structural modeling (SM) branch and an amplitude component exchange (ACE) module, both dedicated to enhancing the learning of structural features. The SM branch actively models the structural relationships between key parts of objects and extracts corresponding structural features, while the ACE module guides the model to learn structural information in the phase spectrum by introducing implicit constraints during training. By synergizing the backbone network and the two modules, SFETrans exhibits competitive performance on four benchmark datasets and outperforms other comparison methods in terms of computational efficiency.},
  archive      = {J_PR},
  author       = {Ying Yu and Wei Wei and Cairong Zhao and Jin Qian and Enhong Chen},
  doi          = {10.1016/j.patcog.2025.111955},
  journal      = {Pattern Recognition},
  month        = {1},
  pages        = {111955},
  shortjournal = {Pattern Recognition},
  title        = {Structural feature enhanced transformer for fine-grained image recognition},
  volume       = {169},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). CPG: Contrastive patch-graph learning for 3D point cloud. <em>PR</em>, <em>169</em>, 111954. (<a href='https://doi.org/10.1016/j.patcog.2025.111954'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Point cloud annotation is extremely laborious, which largely limits the performance of existing point cloud models. To address this issue, recent advancements in self-supervised learning have been extensively employed. However, due to the unstructured nature of point clouds, current methods struggle to simultaneously model both local and global features. In this paper, we propose a novel C ontrastive P atch- G raph ( CPG ) pre-training strategy for generic point cloud analysis. Our CPG strategy proposes to transform the point cloud into a structured representation, termed as the patch-graph. We further perform contrastive pretext tasks on both local patch nodes and global graphs, allowing to simultaneously enhance the point cloud encoder’s ability to capture both local and global characteristics. Importantly, our CPG learns task-specific multi-dimensional edge features to comprehensive model semantic relationships between the obtained local patches. Extensive experiments on multiple downstream point cloud analysis tasks (i.e., point cloud classification, segmentation, and object detection) demonstrate that our CPG effectively and consistently improves performance across various point cloud encoders, enabling them to surpass encoders trained with other state-of-the-art self-supervised methods.},
  archive      = {J_PR},
  author       = {Junjie Zhou and Yingde Song and Chinwai Chiu and Yongping Xiong and Yuxin Luo and Siyang Song},
  doi          = {10.1016/j.patcog.2025.111954},
  journal      = {Pattern Recognition},
  month        = {1},
  pages        = {111954},
  shortjournal = {Pattern Recognition},
  title        = {CPG: Contrastive patch-graph learning for 3D point cloud},
  volume       = {169},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Generalized ordered wasserstein distance for sequential data. <em>PR</em>, <em>169</em>, 111952. (<a href='https://doi.org/10.1016/j.patcog.2025.111952'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Many practical tasks often require computation of similarity or distance between two sequences. Existing distance measures for sequential data typically align the two sequences, using the differences between the elements in the two sequences. However, such alignments are often not generalized enough to accommodate complex relationships between two sequences. In this article, we propose a novel distance, called Generalized Ordered Wasserstein (GOW), which uses a weighted combination of basis functions to capture both linear and nonlinear relationships between the two sequences. We analyze several properties of the proposed distance and show that GOW generalizes some existing distances and more expressive. Particularly, GOW enables automatic and adaptive selection of the basis functions, which is really beneficial to practical applications. Extensive experiments on widely available public datasets validate effectiveness and show superior performance of GOW over several existing distances. Our code is available at https://github.com/TungDP/GOW .},
  archive      = {J_PR},
  author       = {Tung Doan and Hiep To and Hong Vuong and Muriel Visani and Atsuhiro Takasu and Khoat Than},
  doi          = {10.1016/j.patcog.2025.111952},
  journal      = {Pattern Recognition},
  month        = {1},
  pages        = {111952},
  shortjournal = {Pattern Recognition},
  title        = {Generalized ordered wasserstein distance for sequential data},
  volume       = {169},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Implicit alignment and query refinement for RGB-T semantic segmentation. <em>PR</em>, <em>169</em>, 111951. (<a href='https://doi.org/10.1016/j.patcog.2025.111951'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {RGB-Thermal (RGB-T) semantic segmentation aims to achieve comprehensive scene understanding by integrating thermal infrared images, thereby improving segmentation accuracy and robustness under adverse imaging conditions. Except for complicated calibration issue, most existing CNN-based methods struggle to capture global context. Introducing the Transformer with learnable queries could capture global context but still suffer from query ambiguity due to limited interpretability, hindering the improvement of performance. To address these challenges, we propose a novel implicit alignment and query refinement method, named IQSeg. For the calibration issue, our pipeline incorporates an implicit alignment stage that establishes intrinsic cross-modal correlations through deformable feature alignment, enhancing feature representation and achieving superior segmentation performance without relying on an explicit deformation field, as required by existing methods. Furthermore, to address the query ambiguity issue, we propose a two-stage framework that introduces class-aware or instance-aware query cues to refine segmentation results. Specifically, the query-based refinement stage iteratively enhances global information prediction under the guidance of the implicit alignment stage. Experimental results show that IQSeg achieves superior segmentation accuracy under the misalignment condition compared to state-of-the-art methods. The code is made publicly available at: https://github.com/LuCky-cv-14/IQSeg .},
  archive      = {J_PR},
  author       = {Chang Liu and Haizhuang Liu and Junbao Zhuo and Bochao Zou and Jiansheng Chen and Qianchuan Zhao and Huimin Ma},
  doi          = {10.1016/j.patcog.2025.111951},
  journal      = {Pattern Recognition},
  month        = {1},
  pages        = {111951},
  shortjournal = {Pattern Recognition},
  title        = {Implicit alignment and query refinement for RGB-T semantic segmentation},
  volume       = {169},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). GSM: Global semantic memory. <em>PR</em>, <em>169</em>, 111950. (<a href='https://doi.org/10.1016/j.patcog.2025.111950'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Unsupervised anomaly detection is to detect previously unseen rare samples without any prior knowledge about them. With the emergence of deep learning, many methods employ normal data reconstruction to train detection models, which is expected to yield relatively large errors when reconstructing anomalies. However, recent studies find that anomalies can be overgeneralized, resulting in reconstruction errors as small as normal samples. In this paper, we examine the anomaly overgeneralization problem and propose global semantic information learning. Normal and anomalous samples may share the same local feature such as textures, edges, and corners, but have separability at the global semantic level. To address this, we propose the global semantic memory with novel cascade architecture designed to capture global semantic information in the latent space and introduce a configurable sparsification and random forgetting mechanism. Our proposed method achieves state-of-the-art experimental results on different public benchmarks, without the introduction of any additional auxiliary loss.},
  archive      = {J_PR},
  author       = {Jiahao Li and Yiqiang Chen and Yunbing Xing and Yang Gu and Xiangyuan Lan},
  doi          = {10.1016/j.patcog.2025.111950},
  journal      = {Pattern Recognition},
  month        = {1},
  pages        = {111950},
  shortjournal = {Pattern Recognition},
  title        = {GSM: Global semantic memory},
  volume       = {169},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Few-shot object detection algorithm with active representative sample selection based on density peak clustering. <em>PR</em>, <em>169</em>, 111949. (<a href='https://doi.org/10.1016/j.patcog.2025.111949'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Few-Shot Object Detection (FSOD) algorithm holds great potential for various real-world applications. It aims to use a lot of data from base classes for training and then apply the knowledge to recognize targets in new classes. However, many existing FSOD algorithms employing a two-phase learning mode overlook the practical necessity of constructing the training set for the finetuning phase. Their experimental assumption neglects the construction process for the finetuning training set, leading to uncertainty and potentially negative impacts on algorithm performance, as well as overlooking potential information in existing data. For addressing the above issues, this paper presents an active representative sample selection mechanism for the FSOD algorithm. The proposed approach leverages the characteristics of the Density Peak Clustering framework. And it is incorporated between the base training phase and the finetuning phase. During this phase, representative samples are extracted using the DPC method and reweighted based on label information from their neighbors, thereby maintaining recognition capability for base classes and providing enhanced comparison information for detecting novel classes. The experimental results demonstrate the effectiveness of the proposed method. For the road disease dataset, our method on the Meta-DETR framework achieves a mAP(50) of 0.440 for novel objects in all shots scenario, compared to the baseline of 0.343. The improved CME achieves a mAP(50) of 0.344 in 10 shots scenario, compared to the baseline of 0.210. In the 30 shots scenario of the RDD2022 dataset, our method demonstrates a mAP(50) of 0.497, while the baseline method yields 0.420.},
  archive      = {J_PR},
  author       = {Zhou Liang and Jiangtao Ren},
  doi          = {10.1016/j.patcog.2025.111949},
  journal      = {Pattern Recognition},
  month        = {1},
  pages        = {111949},
  shortjournal = {Pattern Recognition},
  title        = {Few-shot object detection algorithm with active representative sample selection based on density peak clustering},
  volume       = {169},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). DDS-NAS: Dynamic data selection within neural architecture search via on-line hard example mining applied to image classification. <em>PR</em>, <em>169</em>, 111948. (<a href='https://doi.org/10.1016/j.patcog.2025.111948'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In order to address the scalability challenge within Neural Architecture Search (NAS), we speed up NAS training via dynamic hard example mining within a curriculum learning framework. By utilising an autoencoder that enforces an image similarity embedding in latent space, we construct an efficient kd -tree structure to order images by furthest neighbour dissimilarity in a low-dimensional embedding. From a given query image from our subsample dataset, we can identify the most dissimilar image within the global dataset in logarithmic time. Via curriculum learning, we then dynamically re-formulate an unbiased subsample dataset for NAS optimisation, upon which the current NAS solution architecture performs poorly. We show that our DDS-NAS framework speeds up gradient-based NAS strategies by up to 27 × without loss in performance. By maximising the contribution of each image sample during training, we reduce the duration of a NAS training cycle and the number of iterations required for convergence.},
  archive      = {J_PR},
  author       = {Matt Poyser and Toby P. Breckon},
  doi          = {10.1016/j.patcog.2025.111948},
  journal      = {Pattern Recognition},
  month        = {1},
  pages        = {111948},
  shortjournal = {Pattern Recognition},
  title        = {DDS-NAS: Dynamic data selection within neural architecture search via on-line hard example mining applied to image classification},
  volume       = {169},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Leveraging multi-class background description and token dictionary representation for hyperspectral anomaly detection. <em>PR</em>, <em>169</em>, 111945. (<a href='https://doi.org/10.1016/j.patcog.2025.111945'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Hyperspectral anomaly detection is aimed at distinguishing between background and anomalous regions in hyperspectral images, and plays a crucial role iSn various applications. However, the existing deep learning methods face challenges when dealing with complex background distributions and insufficient training data. In this article, we propose a novel multi-class background description transformer network (MBDTNet) to address the problems of imprecise background distribution learning and poor anomaly detection. Firstly, we propose an image-level end-to-end data augmentation method based on self-supervised training, which enhances the diversity and quantity of the training samples through adaptive clustering and spatial masking strategies. Secondly, based on the principles of low-rank representation, a sparse self-attention mechanism based on token dictionary representation is designed to help the model focus on key background features and guide the model in recognizing anomalies. Finally, a token dictionary learning mechanism for multi-class background description is established by combining Gaussian discriminant analysis with a conditional distance function, and intra-class and inter-class losses are designed to enhance the model’s ability to separate background and anomalies. Experiments on five benchmark datasets demonstrate the superiority and applicability of the proposed MBDTNet method, showing that it outperforms the current state-of-the-art hyperspectral anomaly detection methods.},
  archive      = {J_PR},
  author       = {Zhiwei Wang and Kun Tan and Xue Wang and Wen Zhang},
  doi          = {10.1016/j.patcog.2025.111945},
  journal      = {Pattern Recognition},
  month        = {1},
  pages        = {111945},
  shortjournal = {Pattern Recognition},
  title        = {Leveraging multi-class background description and token dictionary representation for hyperspectral anomaly detection},
  volume       = {169},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). DT-RSRGAN: An one-off domain translation generative model for real image super-resolution. <em>PR</em>, <em>169</em>, 111944. (<a href='https://doi.org/10.1016/j.patcog.2025.111944'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In single image super-resolution (SISR) tasks, there is inevitably a “domain gap” between synthetic and realistic datasets, which leads to performance drop accordingly. Domain translation (DT) based approaches have then emerged to narrow the discrepancy by converting data across source and target domains while maintaining semantic consistency. Currently, one-off and two-stage models constitute the main DT-SISR methods. However, due to the inability to incorporate prior knowledge of pre-trained SR networks, one-off methods often demonstrate inferior performance to the structurally complex two-stage models. To achieve both simplicity and performance gain, we propose an one-off DT-SISR model DT-RSRGAN for real-world SISR (RW-SISR). Our underlying principle is to recover LR observations via exploring vision transformer (ViT) based on self-attention (SA) mechanisms in adversarial generative models, aiming to fully explore knowledge of image internal correlation in the absence of external prior information. We then devise an image complexity (IC) loss in DT-RSRGAN, serving as a relaxed form of constraint in the absence of high-resolution (HR) training references for the one-off condition, thus suppressing artifacts that haunt GAN-based SR results. The aforementioned measures collectively facilitate the implementation of DT-RSRGAN in an one-off manner while achieving competitive performance compared to state-of-the-art (SOTA) DT-SISR solutions. Extensive experiments on multiple benchmarks validate the effectiveness and superiority of DT-RSRGAN towards RW-SISR issues.},
  archive      = {J_PR},
  author       = {Haiyu Zhang and Shaolin Su and Yu Zhu and Lingmei Zhang and Qingsen Yan and Jinqiu Sun and Yanning Zhang},
  doi          = {10.1016/j.patcog.2025.111944},
  journal      = {Pattern Recognition},
  month        = {1},
  pages        = {111944},
  shortjournal = {Pattern Recognition},
  title        = {DT-RSRGAN: An one-off domain translation generative model for real image super-resolution},
  volume       = {169},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Multi-graph graph matching for coronary artery semantic labeling in invasive coronary angiograms. <em>PR</em>, <em>169</em>, 111943. (<a href='https://doi.org/10.1016/j.patcog.2025.111943'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Coronary artery disease (CAD) stands as the leading cause of death worldwide, and invasive coronary angiography (ICA) remains the gold standard for assessing vascular anatomical information. However, deep learning-based methods encounter challenges in generating semantic labels for arterial segments, primarily due to the morphological similarity between arterial branches and varying anatomy of arterial system between different projection view angles and patients. To address this challenge, we model the vascular tree as a graph and propose a multi-graph matching (MGM) algorithm for coronary artery semantic labeling. The MGM algorithm assesses the similarity between arterials in multiple vascular tree graphs, considering the cycle consistency between each pair of graphs. As a result, the unannotated arterial segments are appropriately labeled by matching them with annotated segments. Through the incorporation of anatomical graph structure, radiomics features, and semantic mapping, the proposed MGM model achieves an impressive accuracy of 0.9471 for coronary artery semantic labeling using our multi-site dataset with 718 ICAs. With the semantic labeled arteries, an overall accuracy of 0.9155 was achieved for stenosis detection. The proposed MGM presents a novel tool for coronary artery analysis using multiple ICA-derived graphs, offering valuable insights into vascular health and pathology.},
  archive      = {J_PR},
  author       = {Chen Zhao and Zhihui Xu and Pukar Baral and Michel Esposito and Weihua Zhou},
  doi          = {10.1016/j.patcog.2025.111943},
  journal      = {Pattern Recognition},
  month        = {1},
  pages        = {111943},
  shortjournal = {Pattern Recognition},
  title        = {Multi-graph graph matching for coronary artery semantic labeling in invasive coronary angiograms},
  volume       = {169},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). ADSUNet: Accumulation-difference-based siamese U-net for inter-frame infrared dim and small target detection. <em>PR</em>, <em>169</em>, 111942. (<a href='https://doi.org/10.1016/j.patcog.2025.111942'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The detection for infrared dim and small target (IRDST) aims to accurately locate these targets in images and has been widely explored. The single-frame detection method with spatial information can detect IRDST with high signal-to-clutter ratio (SCR), while fails to detect IRDST with a low SCR. To resolve this problem, some traditional methods tend to incorporate spatio-temporal information for detecting IRDST. However, these methods need to adjust many hyperparameters to adapt to different complex backgrounds. Learning-based multi-frame methods are emerging and can overcome the defect but suffer from redundancy and inefficiency of information. In this work, IRDST detection method based on Accumulation-Difference-Based Siamese U-Net (ADSUNet) is proposed. In this method, the comprehensive performances for IRDST with low SCR improve through the integrated end-to-end network, which fuses the spatial significant accumulation characteristics and difference characteristics of gray levels between inter-frames. In details, firstly, multi-scale spatial salient features of IRDST in inter-frame images are extracted by Siamese network. Subsequently, the proposed accumulation-difference attention module is then utilised to adaptively fuse the inter-frame feature maps, thereby ensuring full and effective exploitation of the high-dimensional inter-frame spatio-temporal information of IRDST. Finally, a probability likelihood diagram of IRDST is obtained by a decoding network that restores the scale of feature map. Compared with other methods, ADSUNet achieves state-of-the-art (SOTA) performance in target detectability, false alarm suppressibility, real-time performance, with AUC values of 0.993 and 0.998 in two datasets (SCR < 3), FPS of 30.97, the parameter quantity of 1.498 M.},
  archive      = {J_PR},
  author       = {Liuwei Zhang and Yuyang Xi and Zhipeng Wang and Wang Zhang and Fanjiao Tan and Qingyu Hou},
  doi          = {10.1016/j.patcog.2025.111942},
  journal      = {Pattern Recognition},
  month        = {1},
  pages        = {111942},
  shortjournal = {Pattern Recognition},
  title        = {ADSUNet: Accumulation-difference-based siamese U-net for inter-frame infrared dim and small target detection},
  volume       = {169},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Reconstructing data representation for multi-label feature selection. <em>PR</em>, <em>169</em>, 111941. (<a href='https://doi.org/10.1016/j.patcog.2025.111941'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Feature selection is a mainstream paradigm for coping with high-dimensional multi-label data, because it has higher interpretability in dimensionality reduction techniques. Multi-label Feature Selection (MFS) learns the relationship between features and labels to evaluate the discriminative ability of each feature. However, many recent MFS methods ignore the imprecise high-dimensional feature space and the harsh binary label space in the raw multi-label data, and they directly extract the inherent correlations based on raw data to assist the selection process. To address the issues, we propose a novel method named reconstructing data representation for multi-label feature selection (RDMFS) in the paper. Firstly, we utilize the feature space itself to explore the dynamic feature relationship graph, and it is also imposed the constraints of low-rank representation. Secondly, the original binary label space is perturbed by the directional traction matrix, which can generate a label relaxation matrix. Finally, we reconstruct the feature space using the low-rank representation of dynamic feature graph, which can establish a mapping relationship with the label relaxation matrix. These described properties construct the objective function of RDMFS, which is solved by an optimization algorithm. Experiments on real datasets and comparisons with state-of-the-art methods clearly validate the effectiveness of RDMFS.},
  archive      = {J_PR},
  author       = {Yuling Fan and Peizhong Liu and Jinghua Liu},
  doi          = {10.1016/j.patcog.2025.111941},
  journal      = {Pattern Recognition},
  month        = {1},
  pages        = {111941},
  shortjournal = {Pattern Recognition},
  title        = {Reconstructing data representation for multi-label feature selection},
  volume       = {169},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Dual similarity enhanced hybrid orthogonal fusion for multimodal named entity recognition. <em>PR</em>, <em>169</em>, 111940. (<a href='https://doi.org/10.1016/j.patcog.2025.111940'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the rise of multimodal content on social media, Multimodal Named Entity Recognition (MNER) has become vital. Previous studies have improved text-based recognition by incorporating visual information but often struggle with semantic degradation during cross-modal learning and feature redundancy in fusion. To address these issues, we propose a Dual Similarity Enhanced Hybrid Orthogonal Fusion (DSE-HOF) network for MNER, using a semantic constraint strategy with self-attention and cross-modal attention, which effectively captures semantic relations within and between modalities, producing robust hybrid features. Specifically, we introduce a word-level similarity to filter relevant image regions and a modal-level similarity to control the integration of visual features. Extensive experiments on two widely-used MNER datasets demonstrate the effectiveness of our approach. Our model achieves average improvements of 1.398% and 1.681% in overall F1 scores compared to state-of-the-art baselines on the Twitter-2015 and Twitter-2017 datasets, respectively. Further ablation studies and qualitative analyses validate the contribution of each component in our proposed framework.},
  archive      = {J_PR},
  author       = {Chunmao Jiang and Yongpeng Wang and Baoping Xiong},
  doi          = {10.1016/j.patcog.2025.111940},
  journal      = {Pattern Recognition},
  month        = {1},
  pages        = {111940},
  shortjournal = {Pattern Recognition},
  title        = {Dual similarity enhanced hybrid orthogonal fusion for multimodal named entity recognition},
  volume       = {169},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). RFMSU: A multivariate symmetrical uncertainty-based random forest. <em>PR</em>, <em>169</em>, 111939. (<a href='https://doi.org/10.1016/j.patcog.2025.111939'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Decision Trees (DTs) have become very popular classifiers due to their good performance and, most of all, their interpretability. In addition, the machine learning community is also paying attention to Random Forests (RFs) since they defy the interpretability-accuracy tradeoff. Most RFs strategies are based on univariate measures, a fact that may limit the capability of identifying the interaction among more than two features. In order to overcome this problem many multivariate approaches have been proposed. However, most of them are based on finding linear or non-linear combinations of features. In this work, we propose a novel univariate RF strategy that builds DTs using the Multivariate Symmetrical Uncertainty (MSU) measure as splitting criterion. The proposal, referred to as RF M S U , was tested on high-dimensional datasets and compared to state-of-the-art univariate and multivariate DTs and RFs classifiers. Results suggest that RF M S U is capable of finding simpler rules than other RFs approaches while keeping a high predictive power equivalent to that of multivariate approaches. The DT strategies considered obtained simpler models than RF M S U , but at the expense of degrading the classifier. Thus, we can conclude that RF M S U is a RF-based classifier that achieves a good trade-off between the performance and the complexity of the model.},
  archive      = {J_PR},
  author       = {Miguel García-Torres and Francisco Saucedo and Federico Divina and Santiago Gómez-Guerrero},
  doi          = {10.1016/j.patcog.2025.111939},
  journal      = {Pattern Recognition},
  month        = {1},
  pages        = {111939},
  shortjournal = {Pattern Recognition},
  title        = {RFMSU: A multivariate symmetrical uncertainty-based random forest},
  volume       = {169},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Novel dimensionality reduction approaches for higher-order multidimensional data. <em>PR</em>, <em>169</em>, 111938. (<a href='https://doi.org/10.1016/j.patcog.2025.111938'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper investigates the extension of dimension reduction techniques to the multi-dimension case using the Einstein product. Our focus lies on graph-based methods, encompassing both linear and nonlinear approaches, within both supervised and unsupervised learning frameworks. Additionally, we explore variants such as repulsion graphs and kernel methods for linear approaches. Furthermore, we present two generalizations for each method, considering single and multiple weights. Our paper demonstrates the straightforward nature of these generalizations and provides theoretical insights. Numerical experiments are conducted, and results are compared with original methods, highlighting the efficiency of our proposed methods, particularly in handling high-dimensional data such as color images.},
  archive      = {J_PR},
  author       = {Alaeddine Zahir and Khalide Jbilou and Ahmed Ratnani},
  doi          = {10.1016/j.patcog.2025.111938},
  journal      = {Pattern Recognition},
  month        = {1},
  pages        = {111938},
  shortjournal = {Pattern Recognition},
  title        = {Novel dimensionality reduction approaches for higher-order multidimensional data},
  volume       = {169},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Cross-lingual font generation via patch-level style contrastive learning and relative position awareness. <em>PR</em>, <em>169</em>, 111937. (<a href='https://doi.org/10.1016/j.patcog.2025.111937'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Few-shot font generation is a widely applied technique that generates fonts in various styles using only a few reference characters. However, the applicability of the existing few-shot font generation models to cross-lingual font generation (i.e., generating fonts in some language system from another language system in the zero-shot scenario) is constrained. Cross-lingual font generation is critical for billboard design and multi-lingual road signs, especially in low-resource languages with limited font styles. A novel framework integrating patch-level style contrastive learning and relative position awareness is proposed to address this challenge. The patch-level module captures shared stylistic attributes across languages, including fine-grained stroke details. The relative position awareness module enhances alignment between local content features and spatial locations in character structures. Both modules are language-agnostic, extracting intrinsic character representations without auxiliary annotations, enabling zero-shot cross-lingual font generation. Experiments on cross-lingual and few-shot font generation using a self-collected 185-font dataset demonstrate superior performance over existing methods in generation quality and generalization. The codes of the proposed model are available from the link https://github.com/JinshanZeng/RPFont .},
  archive      = {J_PR},
  author       = {Jinshan Zeng and Yiyang Yuan and Xijia Wang and Yan Zhang and Yefei Wang},
  doi          = {10.1016/j.patcog.2025.111937},
  journal      = {Pattern Recognition},
  month        = {1},
  pages        = {111937},
  shortjournal = {Pattern Recognition},
  title        = {Cross-lingual font generation via patch-level style contrastive learning and relative position awareness},
  volume       = {169},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). CVACL-MA: Comprehensive variate analysis and collaborative learning with multi-adapter for multivariate time series forecasting. <em>PR</em>, <em>169</em>, 111936. (<a href='https://doi.org/10.1016/j.patcog.2025.111936'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multivariate time series forecasting aims to predict future values or trends by using the time dependence and potential patterns of multiple interrelated time series variates in historical data. Recently, multivariate time series forecasting methods have focused on learning correlations across variates, enabling more accurate prediction of future trends by delving into the connections between different variates. However, the underlying assumption in these methodologies is that existing multivariate time series variates exhibit positive correlations, thereby allowing for mutual interdependence. This assumption potentially ignores the in-depth examination of the correlation quality of multivariate time series variates. Unfortunately, in practice, not all variates exhibit a clear positive correlation. The relationships between time-series variates are often complex, involving positive, negative, and potential non-linear connections, and may even the exhibit lack of correlation. To address this challenge, we propose a comprehensive variate analysis and collaborative learning with multi-adapter (CVACL-MA) model to learn the similarities and disparities between variates. First a cross-time encoder is introduced in the proposed model. And aims to learn the global and local features of each variate in the time dimension. Next, considering the correlation level between multivariate variates, a multivariate similarity analysis module is proposed based on the cosine similarity calculation method. And it can divide the original multivariate time series variates into sets of non-positive correlation variates and sets of positive correlation variates. Then, the multi-adapter learning backbone is introduced to learn the disparity of non-positive correlation variates. Meanwhile, a collaborative learning of similar variates backbone is proposed to facilitate the cooperative learning of interdependencies within sets of positive correlation variates. Extensive experiments show that CVACL-MA significantly outperforms state-of-the-art models on nine real-world benchmark datasets, such as weather, ETT, electricity, solar-energy, and so on. The effectiveness of the proposed method is proved by in-depth research and analysis. We release the code at https://github.com/yejunjiePhD/CVACL-MA .},
  archive      = {J_PR},
  author       = {Junjie Ye and Chengli Zhou and Xiaojun Zhou and Yaqun Huang and Chunna Zhao},
  doi          = {10.1016/j.patcog.2025.111936},
  journal      = {Pattern Recognition},
  month        = {1},
  pages        = {111936},
  shortjournal = {Pattern Recognition},
  title        = {CVACL-MA: Comprehensive variate analysis and collaborative learning with multi-adapter for multivariate time series forecasting},
  volume       = {169},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Improving mutation pathogenicity prediction of metal-binding sites in proteins with a panoramic attention mechanism. <em>PR</em>, <em>169</em>, 111935. (<a href='https://doi.org/10.1016/j.patcog.2025.111935'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Accurately identifying the pathogenicity of mutations at protein-metal binding sites is crucial for uncovering the structural and functional complexity of metalloproteins, and the molecular mechanisms behind numerous diseases. In this study, we present a novel deep learning framework, CASTLE, for the accurate pathogenicity prediction of mutations at metal-binding sites through an effective depiction of the message-passing in local environment surrounding both the metals and metal-binding sites. Specifically, CASTLE weaves multiple attention-driven units to construct comprehensive panoramic message-passing paths, enabling the capture of intricate structural patterns associated with the metal-binding conformations. In addition, CASTLE seamlessly integrates structural information at both residue and atomic levels, and further deeply fuses structural and sequence representations to ensure the incorporation of more comprehensive information. We demonstrate that CASTLE significantly outperforms other state-of-the-art methods across all datasets we evaluated, showcasing its robustness and generalization abilities. Our interpretability analysis illustrates CASTLE’s capability in capturing meaningful representations from the metal coordination-dependent environments. Moreover, the model optimization reaffirms the advantages of our model building strategy, which can effectively capture distinct binding patterns for different metal types. Overall, CASTLE provides a powerful deep learning tool that may offer valuable insights into the study of metalloprotein-related disease mechanisms and drug design.},
  archive      = {J_PR},
  author       = {Yuan Zhang and Jiafeng Wu and Qiuye Zhao and Mingyuan Dong and Junsheng Deng and Xieping Gao and Kai Hu and Dapeng Xiong},
  doi          = {10.1016/j.patcog.2025.111935},
  journal      = {Pattern Recognition},
  month        = {1},
  pages        = {111935},
  shortjournal = {Pattern Recognition},
  title        = {Improving mutation pathogenicity prediction of metal-binding sites in proteins with a panoramic attention mechanism},
  volume       = {169},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). On the design fundamentals of diffusion models: A survey. <em>PR</em>, <em>169</em>, 111934. (<a href='https://doi.org/10.1016/j.patcog.2025.111934'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Diffusion models are learning pattern-learning systems to model and sample from data distributions with three functional components namely the forward process, the reverse process, and the sampling process. The components of diffusion models have gained significant attention with many design factors being considered in common practice. Existing reviews have primarily focused on higher-level solutions, covering less on the design fundamentals of components. This study seeks to address this gap by providing a comprehensive and coherent review of seminal designable factors within each functional component of diffusion models. This provides a finer-grained perspective of diffusion models, benefiting future studies in the analysis of individual components, the design factors for different purposes, and the implementation of diffusion models.},
  archive      = {J_PR},
  author       = {Ziyi Chang and George A. Koulieris and Hyung Jin Chang and Hubert P.H. Shum},
  doi          = {10.1016/j.patcog.2025.111934},
  journal      = {Pattern Recognition},
  month        = {1},
  pages        = {111934},
  shortjournal = {Pattern Recognition},
  title        = {On the design fundamentals of diffusion models: A survey},
  volume       = {169},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Neural spatial–temporal tensor representation for infrared small target detection. <em>PR</em>, <em>169</em>, 111929. (<a href='https://doi.org/10.1016/j.patcog.2025.111929'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Optimization-based approaches dominate infrared small target detection as they leverage infrared imagery’s intrinsic low-rankness and sparsity. While effective for single-frame images, they struggle with dynamic changes in multi-frame scenarios as traditional spatial–temporal representations often fail to adapt. To address these challenges, we introduce a Neural-represented Spatial–Temporal Tensor (NeurSTT) model. This framework employs nonlinear networks to enhance spatial–temporal feature correlations in background approximation, thereby supporting target detection in an unsupervised manner. Specifically, we employ neural layers to approximate sequential backgrounds within a low-rank informed deep scheme. A neural three-dimensional total variation is developed to refine background smoothness while reducing static target-like clusters in sequences. Traditional sparsity constraints are incorporated into the loss functions to preserve potential targets. By replacing complex solvers with a deep updating strategy, NeurSTT simplifies the optimization process in a domain-awareness way. Visual and numerical results across various datasets demonstrate that our method outperforms detection challenges. Notably, it has 16.6 × fewer parameters and averaged 22.44% higher in I o U compared to the suboptimal method on 256 × 256 sequences. The code is available at https://github.com/fengyiwu98/NeurSTT .},
  archive      = {J_PR},
  author       = {Fengyi Wu and Simin Liu and Haoan Wang and Bingjie Tao and Junhai Luo and Zhenming Peng},
  doi          = {10.1016/j.patcog.2025.111929},
  journal      = {Pattern Recognition},
  month        = {1},
  pages        = {111929},
  shortjournal = {Pattern Recognition},
  title        = {Neural spatial–temporal tensor representation for infrared small target detection},
  volume       = {169},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Generalizing across non-stationary series via learning dynamic causal factors. <em>PR</em>, <em>169</em>, 111928. (<a href='https://doi.org/10.1016/j.patcog.2025.111928'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Learning domain-invariant representations is a crucial task for achieving out-of-distribution generalization . Recent efforts have begun to incorporate causality into this process, aiming to identify and understand the causal factors relevant to various tasks. However, when confronted with non-stationary time series data, simply extending existing generalization methods may prove ineffective. This inadequacy stems from their failure to adequately model the underlying causal factors, exacerbated by temporal domain shifts in addition to source domain shifts. In this paper, we thoroughly examine the challenges posed by both source and temporal shifts through a causal lens in the context of generalizing non-stationary time series data. We introduce a novel model called the Dynamic Causal Sequential Variational Auto-Encoder (DCSVAE), designed specifically to learn dynamic causal factors. By effectively disentangling the representation of non-stationary time series data, our model distinguishes between dynamic causal, dynamic non-causal, and static non-causal factors, thereby facilitating temporal generalization. To enhance disentanglement, we introduce two constraints on latent variables based on mutual information. Theoretical guarantees rooted in information theory validate the feasibility of our approach. Our experiments, conducted on both synthetic and real datasets, demonstrate the superior performance of the proposed model in time series domain generalization tasks when compared to state-of-the-art benchmarks.},
  archive      = {J_PR},
  author       = {Weifeng Zhang and Yan Liu and Xovee Xu and Fan Zhou and Ting Zhong and Kunpeng Zhang},
  doi          = {10.1016/j.patcog.2025.111928},
  journal      = {Pattern Recognition},
  month        = {1},
  pages        = {111928},
  shortjournal = {Pattern Recognition},
  title        = {Generalizing across non-stationary series via learning dynamic causal factors},
  volume       = {169},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Hyper-BTS: Brain tumor segmentation based on hypergraph guidance. <em>PR</em>, <em>169</em>, 111926. (<a href='https://doi.org/10.1016/j.patcog.2025.111926'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The automatic brain tumor segmentation task involves extracting key information from multi-modal MRI images, which frequently exhibit complex and nonlinear spatial relationships. High-order relation modeling can effectively reveal more detailed and complex nonlinear dependencies, such as intricate tumor morphology, boundaries, and their relationships with surrounding tissues, which are essential for accurate tumor segmentation. In this paper, we propose a hypergraph-guided feature decoupling learning model that effectively enhances feature alignment in tumor lesion regions by deeply modeling high-order feature correlations, significantly improving segmentation accuracy. Specifically, we propose a multi-level semantic encoding module that effectively integrates multi-modal and multi-scale information to generate deeply fused feature representations with rich semantic information. We further introduce a novel hypergraph-guided disentangled representation module to model high-order semantic correlations both across and within modalities, thereby capturing lesion region features more comprehensively. To mitigate the interference of irrelevant features, we adopt a contrastive learning paradigm to enhance the correlation between features, ensuring that the learned representations are highly focused on the target tumor region. Finally, we design a region-consistency decoding module that utilizes a dual-branch structure to decode different semantic contexts of the lesion region, generating the final accurate segmentation result through feature aggregation. Extensive experiments on the BraTS 2019, 2020, and 2021 datasets demonstrate that the proposed model significantly outperforms existing state-of-the-art methods in segmentation performance, validating the substantial potential of high-order relation modeling in brain tumor segmentation.},
  archive      = {J_PR},
  author       = {Qianren Guo and Yuehang Wang and Yongji Zhang and Hong Qi and Yuhua Hu and Yu Jiang},
  doi          = {10.1016/j.patcog.2025.111926},
  journal      = {Pattern Recognition},
  month        = {1},
  pages        = {111926},
  shortjournal = {Pattern Recognition},
  title        = {Hyper-BTS: Brain tumor segmentation based on hypergraph guidance},
  volume       = {169},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Video and noise collaboratively guided semi-supervised diffusion model for video action detection. <em>PR</em>, <em>169</em>, 111925. (<a href='https://doi.org/10.1016/j.patcog.2025.111925'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Semi-supervised video action detection is one of the key technologies for achieving video understanding, effectively alleviating the limitations of obtaining large-scale action boundary annotations. However, the lack of labeled data makes existing semi-supervised methods vulnerable to background noise in unlabeled data, consequently compromising the robustness of action representations. In contrast, diffusion models demonstrate strong capability in learning robust data distribution representations through their iterative forward and reverse processes. Therefore, this paper approaches the training of the semi-supervised framework from a novel perspective of diffusion generation. Specifically, a video and noise collaboratively guided semi-supervised diffusion model for video action detection is proposed in this paper, referred to as Semi-Diff. First, during the training phase, Gaussian noise is added to the ground truth through multiple steps to transform it into pure noise. Subsequently, the model is trained to reverse the noise into action localization maps. In addition, this paper further proposes a Conditional Guidance Strategy and additionally incorporates Reconstruction loss to prevent feature confusion. This strategy utilizes video encoding features and noise to collaboratively guide the model, aiming to further enhance the quality of diffusion. During the inference phase, random noise is input into the diffusion model for denoising. It is noteworthy that the method proposed in this paper decouples the noise prediction process and the action localization process during the diffusion process, enabling competitive performance to be achieved with fewer iteration steps. Extensive experimental results on JHMDB-21 and UCF101-24 demonstrate that the proposed method can generate accurate action detection results and exhibits strong competitiveness.},
  archive      = {J_PR},
  author       = {Qiming Zhang and Zhengping Hu and Yulu Wang and Hehao Zhang and Jirui Di},
  doi          = {10.1016/j.patcog.2025.111925},
  journal      = {Pattern Recognition},
  month        = {1},
  pages        = {111925},
  shortjournal = {Pattern Recognition},
  title        = {Video and noise collaboratively guided semi-supervised diffusion model for video action detection},
  volume       = {169},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Rethinking contrastive learning in session-based recommendation. <em>PR</em>, <em>169</em>, 111924. (<a href='https://doi.org/10.1016/j.patcog.2025.111924'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Session-based recommendation aims to predict intents of anonymous users based on limited behaviors. With the ability in alleviating data sparsity, contrastive learning is prevailing in the task. However, we spot that existing contrastive learning based methods still suffer from three obstacles: (1) they overlook item-level sparsity and primarily focus on session-level sparsity; (2) they typically augment sessions using item IDs like crop, mask and reorder, failing to ensure the semantic consistency of augmented views; (3) they treat all positive-negative signals equally, without considering their varying utility. To this end, we propose a novel multi-modal adaptive contrastive learning framework called MACL for session-based recommendation. In MACL , a multi-modal augmentation is devised to generate semantically consistent views at both item and session levels by leveraging item multi-modal features. Besides, we present an adaptive contrastive loss that distinguishes varying contributions of positive-negative signals to improve self-supervised learning. Extensive experiments on three real-world datasets demonstrate the superiority of MACL over state-of-the-art methods.},
  archive      = {J_PR},
  author       = {Xiaokun Zhang and Bo Xu and Fenglong Ma and Zhizheng Wang and Liang Yang and Hongfei Lin},
  doi          = {10.1016/j.patcog.2025.111924},
  journal      = {Pattern Recognition},
  month        = {1},
  pages        = {111924},
  shortjournal = {Pattern Recognition},
  title        = {Rethinking contrastive learning in session-based recommendation},
  volume       = {169},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Trunk-branch contrastive network with multi-view deformable aggregation for multi-view action recognition. <em>PR</em>, <em>169</em>, 111923. (<a href='https://doi.org/10.1016/j.patcog.2025.111923'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-view action recognition aims to identify actions in a given multi-view scene. Traditional studies initially extracted refined features from each view, followed by implemented paired interaction and integration, but they potentially overlooked the critical local features in each view. When observing objects from multiple perspectives, individuals typically form a comprehensive impression and subsequently fill in specific details. Drawing inspiration from this cognitive process, we propose a novel trunk-branch contrastive network (TBCNet) for RGB-based multi-view action recognition. Distinctively, TBCNet first obtains fused features in the trunk block and then implicitly supplements vital details provided by the branch block via contrastive learning, generating a more informative and comprehensive action representation. Within this framework, we construct two core components: the multi-view deformable aggregation (MVDA) and the trunk-branch contrastive learning. MVDA employed in the trunk block effectively facilitates multi-view feature fusion and adaptive cross-view spatio-temporal correlation, where a global aggregation module (GAM) is utilized to emphasize significant spatial information and a composite relative position bias (CRPB) is designed to capture the intra- and cross-view relative positions. Moreover, a trunk-branch contrastive loss is constructed between aggregated features and refined details from each view. By incorporating two distinct weights for positive and negative samples, a weighted trunk-branch contrastive loss is proposed to extract valuable information and emphasize subtle inter-class differences. The effectiveness of TBCNet is verified by extensive experiments on four datasets including NTU-RGB+D 60, NTU-RGB+D 120, PKU-MMD, and N-UCLA dataset. Compared to other RGB-based methods, our approach achieves state-of-the-art performance in cross-subject and cross-setting protocols.},
  archive      = {J_PR},
  author       = {Yingyuan Yang and Guoyuan Liang and Can Wang and Xiaojun Wu},
  doi          = {10.1016/j.patcog.2025.111923},
  journal      = {Pattern Recognition},
  month        = {1},
  pages        = {111923},
  shortjournal = {Pattern Recognition},
  title        = {Trunk-branch contrastive network with multi-view deformable aggregation for multi-view action recognition},
  volume       = {169},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Hypergraph-based semantic and topological self-supervised learning for brain disease diagnosis. <em>PR</em>, <em>169</em>, 111921. (<a href='https://doi.org/10.1016/j.patcog.2025.111921'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Brain networks involve complex multi-view interactions, and accurately modeling these information is essential for improving brain disease diagnosis. Traditional graph-based methods often fail to capture the high-order topological and semantic features inherent in functional brain networks, which limits their diagnostic effectiveness. To address this limitation, we propose a HyperGraph-based Semantic and Topological self-supervised learning (HGST) method for brain disease diagnosis. HGST utilizes hypergraph structures to model high-order functional connections among brain regions, thereby capturing complex multi-region interactions. Through hypergraph self-supervised learning, HGST extracts latent topological and semantic features to generate high-order embeddings for downstream brain disease diagnosis. Specifically, HGST comprises two core components: a semantic-aware module based on hypergraph link prediction and a topology-aware module based on hyperedge structure similarity. The former learns high-order embeddings by restoring masked node features, while the latter captures local topological associations by measuring similarities among hyperedges involving multiple brain regions. Finally, we employ a pretrained shared hypergraph neural network as an encoder to extract high-order brain network embeddings for each subject, facilitating brain disease diagnosis. We validated HGST on two public brain disease datasets, ADHD and MDD, where it outperformed existing methods in diagnostic performance. Additionally, we visualized the high-order brain networks produced by hypergraph self-supervised learning and identified key disease-related regions, further supporting the effectiveness and interpretability of our method. The source code is available in https://github.com/iMoonLab/HGST .},
  archive      = {J_PR},
  author       = {Xiangmin Han and Mengqi Lei and Junchang Li},
  doi          = {10.1016/j.patcog.2025.111921},
  journal      = {Pattern Recognition},
  month        = {1},
  pages        = {111921},
  shortjournal = {Pattern Recognition},
  title        = {Hypergraph-based semantic and topological self-supervised learning for brain disease diagnosis},
  volume       = {169},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Stock investment selection via time series subpatterns and multi-relationship fusion. <em>PR</em>, <em>169</em>, 111919. (<a href='https://doi.org/10.1016/j.patcog.2025.111919'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Due to the non-stationary dynamics and complex interrelationships in the stock market, stock investment selection remains a critical and challenging task in the fintech field. We propose a novel stock investment selection framework based on time series subpatterns and multi-relationship fusion. First, we propose a novel stock time-series subpattern representation that effectively captures local temporal patterns, enabling precise characterization of stock-specific features. We further learn the temporal dependencies among subpatterns to reveal implicit pattern changes in each stock’s historical data. Second, unlike traditional methods that learn stock correlation coefficients and fuse them to obtain stock features, we leverage the Transformer Encoder’s capability to capture inherent relationships between data, enabling the extraction of implicit short-term relationship between subpatterns of different stocks and thus directly obtaining features influenced by short-term inter-stock relationship. Third, we fuse the short-term and long-term stock relationships to obtain a fused relationship that influences stock trend changes. This multi-relationship fusion reflects the complex synchronous variation patterns between stocks and facilitates information exchange between long-term and short-term relationships, thereby uncovering the latent connections between these two types of relationships. Finally, we combine the temporal dependencies of stock subpatterns with the fused relationship to predict stock trends. Experimental results on three datasets demonstrate that the proposed method outperforms the state-of-the-art methods.},
  archive      = {J_PR},
  author       = {Yandi Shi and Jing Chi and Suochao Yi and Caiming Zhang},
  doi          = {10.1016/j.patcog.2025.111919},
  journal      = {Pattern Recognition},
  month        = {1},
  pages        = {111919},
  shortjournal = {Pattern Recognition},
  title        = {Stock investment selection via time series subpatterns and multi-relationship fusion},
  volume       = {169},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Towards to real world vehicle privacy protection: A new dataset and benchmark. <em>PR</em>, <em>169</em>, 111918. (<a href='https://doi.org/10.1016/j.patcog.2025.111918'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The vehicle privacy protection plays a vital role in releasing or sharing of traffic videos. License plate, as the identifiable mark of a vehicle, contains the most sensitive information for a vehicle. Therefore, masking the license plates is a common way to protect the privacy of corresponding vehicles. However, in the real world scenarios, it is often hard to locate the small and shifting license plates, and therefore precise and cost-effective privacy protection is quite challenging. To address this problem in surveillance video, we fully explore all available spatio-temporal cues and design bidirectional Kalman filter model in the consecutive frames to locate missing license plates. To verify effectiveness of the proposed benchmark, we build a new License Plates Privacy-preserving Dataset (LPPD) collected from various scenes with diverse privacy and utility annotations. We demonstrate that our proposed method show very promising capability of privacy protection on the real world dataset without sacrificing its utility.},
  archive      = {J_PR},
  author       = {Jiayi Lin and Chengming Zou and Long Lan and Yong Luo and Yue Yu and Yaowei Wang and Wei Zeng and Yonghong Tian},
  doi          = {10.1016/j.patcog.2025.111918},
  journal      = {Pattern Recognition},
  month        = {1},
  pages        = {111918},
  shortjournal = {Pattern Recognition},
  title        = {Towards to real world vehicle privacy protection: A new dataset and benchmark},
  volume       = {169},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). MIMAR-OSA: Enhancing obstructive sleep apnea diagnosis through multimodal data integration and missing modality reconstruction. <em>PR</em>, <em>169</em>, 111917. (<a href='https://doi.org/10.1016/j.patcog.2025.111917'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Polysomnography (PSG) is the gold standard for diagnosing obstructive sleep apnea (OSA). However, it is limited by its reliance on sleep state, costly equipment, and complex operational requirements. Existing deep learning approaches for OSA detection often utilize limited modalities such as electrocardiograms and snoring audio, frequently overlooking the potential of facial image data and other multidimensional inputs. To address these limitations, we propose a data-driven diagnostic framework, MIMAR-OSA , which leverages three distinct modalities: basic physiological measurements, facial image data, and snoring audio collected by patients using portable devices. Our framework employs attention grids in the visual domain to identify facial key points from images at various angles, a graph attention network to emphasize and connect critical facial features across images, and semantic processing of patient’s basic physical signs information in the linguistic domain. For the audio domain, we utilize diffWave to reconstruct missing audio components. Comprehensive data integration and analysis are facilitated by a fusion transformer combined with a mixture of expert (MoE) models. Our framework was validated on a university hospital OSA dataset, achieving a 92.24% accuracy. Additionally, testing our MIMAR-OSA model on open-source multimodal learning benchmarks demonstrated its superiority in multi-classification tasks over other state-of-the-art models, confirming its broad applicability and extensibility across various medical settings.},
  archive      = {J_PR},
  author       = {Xihe Qiu and Yingchen Wei and Xiaoyu Tan and Weidi Xu and Haodong Wang and Jingru Ma and Jingjing Huang and Zhijun Fang},
  doi          = {10.1016/j.patcog.2025.111917},
  journal      = {Pattern Recognition},
  month        = {1},
  pages        = {111917},
  shortjournal = {Pattern Recognition},
  title        = {MIMAR-OSA: Enhancing obstructive sleep apnea diagnosis through multimodal data integration and missing modality reconstruction},
  volume       = {169},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Unveiling explainability in face anti-spoofing: Hybrid feature extraction with XAI-guided feature aggregation. <em>PR</em>, <em>169</em>, 111905. (<a href='https://doi.org/10.1016/j.patcog.2025.111905'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Face anti-spoofing (FAS) is a critical component of biometric security systems aimed at distinguishing genuine users from spoofed faces presented via printed photos, replayed videos, or 3D masks. While existing FAS models achieve high performance, they often suffer from limited generalization and interpretability, especially when faced with unseen attack scenarios. To address these challenges, we propose a novel, explainable face anti-spoofing framework that introduces two key innovations: XAIPooling and XAIDropout. These mechanisms leverage Shapley values to guide the pooling and dropout processes, ensuring that only the most significant features are retained during feature aggregation and regularization. Unlike traditional pooling and dropout, which operate without regard for feature importance, XAIPooling and XAIDropout prioritize critical features, leading to better generalization and robustness against cross-dataset attacks. The proposed model employs a multi-stream architecture that processes RGB and LBP-transformed images through independent convolutional pathways. Features from these two streams are fused to create a comprehensive feature representation. The model’s feature extraction pipeline includes convolutional layers, ReLU activations, batch normalization, XAIPooling, and XAIDropout, followed by a series of fully connected layers for final classification. By incorporating explainability into pooling and dropout operations, the proposed framework offers greater transparency in decision-making, enabling insight into the features most responsible for detecting spoofing attempts. Extensive evaluations are conducted on five widely-used benchmark datasets: MSU-MFSD, NUAA, LCC-FASD, ROSE-Youtu, and 3DMAD, covering various attack types, devices, and environmental conditions.},
  archive      = {J_PR},
  author       = {Ravi Pratap Singh and Ratnakar Dash and Ramesh Kumar Mohapatra},
  doi          = {10.1016/j.patcog.2025.111905},
  journal      = {Pattern Recognition},
  month        = {1},
  pages        = {111905},
  shortjournal = {Pattern Recognition},
  title        = {Unveiling explainability in face anti-spoofing: Hybrid feature extraction with XAI-guided feature aggregation},
  volume       = {169},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Constrained and directional ensemble attention for facial action unit detection. <em>PR</em>, <em>169</em>, 111904. (<a href='https://doi.org/10.1016/j.patcog.2025.111904'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Facial action unit (AU) detection is a challenging task, due to the subtlety of each AU in local area and the correlations among AUs in global face. In recent years, the prevailing attention mechanism has been introduced to AU detection. However, the inherent mechanism of self-attention weight distribution has been rarely explored. Besides, ensemble learning is an efficient technique, but gains little attention in AU detection. Considering the above limitations, we propose a local self-attention constraining (LSC) network, by regarding the self-attention distribution of each AU as a spatial distribution, and constraining it based on prior knowledge so as to capture AU-related local information. Moreover, to learn correlations among different AU regions, we propose a global dual-directional attention (GDA) network, which adaptively learns global attention map from both vertical and horizontal directions. Last but not least, the two networks from different views of capturing patterns are assembled to integrate both advantages. Extensive experiments on BP4D, DISFA, and GFT benchmarks demonstrate that our methods including local self-attention constraining, global dual-directional attention, and multi-view ensemble all significantly surpass state-of-the-art AU detection works.},
  archive      = {J_PR},
  author       = {Zhiwen Shao and Bikuan Chen and Yong Zhou and Xuehuai Shi and Canlin Li and Lizhuang Ma and Dit-Yan Yeung},
  doi          = {10.1016/j.patcog.2025.111904},
  journal      = {Pattern Recognition},
  month        = {1},
  pages        = {111904},
  shortjournal = {Pattern Recognition},
  title        = {Constrained and directional ensemble attention for facial action unit detection},
  volume       = {169},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). FCRNet: Learning non-linear correspondences variation via a graph-based feature embedding for false correspondence removal. <em>PR</em>, <em>169</em>, 111903. (<a href='https://doi.org/10.1016/j.patcog.2025.111903'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article addresses the critical challenge of false correspondence removal in image feature matching. While existing approaches predominantly rely on the assumption of motion consistency to distinguish correct correspondences, the non-consistent but correct correspondences are frequently disregarded, leading to poor performance in scenes with complex structures. This study proposes an innovative false correspondence removal network (FCRNet), which designs a learnable frequency response function h ( λ ) to better fit the motion of both consistent and non-consistent correct correspondences. Instead of extracting pixel-based 2D movements, we introduce a graph-based Motion-Fitting Residual Layer (MFRL) to extract high-dimensional motion-fitting residual features, effectively capturing the 3D geometrical variation of matching pairs. Furthermore, an adaptive loss function is adopted to balance and train using the two distinct types of correct correspondences. The principal contribution of FCRNet lies in leveraging a graph-based learning model to fit the non-consistent complex motion patterns of a minority correct correspondences, which are often overlooked by previous methods. Extensive experiments demonstrate that the proposed FCRNet outperforms existing algorithms in terms of precision for false correspondence removal and camera pose estimation. The code for this study is available at https://github.com/Livsdjo/FCRNet-Network-code .},
  archive      = {J_PR},
  author       = {Ruiyuan Li and Zhaolin Xiao and Meng Zhang and Haiyan Jin and Haonan Su},
  doi          = {10.1016/j.patcog.2025.111903},
  journal      = {Pattern Recognition},
  month        = {1},
  pages        = {111903},
  shortjournal = {Pattern Recognition},
  title        = {FCRNet: Learning non-linear correspondences variation via a graph-based feature embedding for false correspondence removal},
  volume       = {169},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). MA-FSAR: Multimodal adaptation of CLIP for few-shot action recognition. <em>PR</em>, <em>169</em>, 111902. (<a href='https://doi.org/10.1016/j.patcog.2025.111902'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Applying large-scale vision-language pre-trained models like CLIP to few-shot action recognition (FSAR) can significantly enhance both performance and efficiency. While several studies have recognized this advantage, most rely on full-parameter fine-tuning to adapt CLIP’s visual encoder to FSAR data, which not only incurs high computational costs but also overlooks the potential of the visual encoder to engage in temporal modeling and focus on targeted semantics directly. To tackle these issues, we introduce MA-FSAR, a framework that employs the Parameter-Efficient Fine-Tuning (PEFT) technique to enhance the CLIP visual encoder in terms of action-related temporal and semantic representations. Our solution involves a token-level Fine-grained Multimodal Adaptation mechanism: a Global Temporal Adaptation captures motion cues from video sequences, while a Local Multimodal Adaptation integrates text-guided semantics from the support set to emphasize action-critical features. Additionally, we propose a prototype-level text-guided construction module to further enrich the temporal and semantic characteristics of video prototypes. Extensive experiments demonstrate our superior performance in various tasks using minor trainable parameters.},
  archive      = {J_PR},
  author       = {Jiazheng Xing and Jian Zhao and Chao Xu and Mengmeng Wang and Guang Dai and Yong Liu and Jingdong Wang and Xuelong Li},
  doi          = {10.1016/j.patcog.2025.111902},
  journal      = {Pattern Recognition},
  month        = {1},
  pages        = {111902},
  shortjournal = {Pattern Recognition},
  title        = {MA-FSAR: Multimodal adaptation of CLIP for few-shot action recognition},
  volume       = {169},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). A cooperative hybrid breeding swarm intelligence algorithm for feature selection. <em>PR</em>, <em>169</em>, 111901. (<a href='https://doi.org/10.1016/j.patcog.2025.111901'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Feature selection (FS) is critical for reducing dimensionality and enhancing model accuracy but remains challenging in high-dimensional datasets. Metaheuristic algorithms, especially swarm intelligence, are often applied to optimization issues like FS, however, they struggle with the local optima and slow convergence in high-dimensional space. To address these issues, this paper proposes a cooperative hybrid breeding swarm intelligence algorithm (CHBSI), which simulates a hybrid breeding optimization algorithm based on heterosis theory, where the population is divided into three subpopulations for cooperative evolution. In each subpopulation, solutions are updated using a well-established particle swarm optimization algorithm, with Lévy flights integrated to enhance the global search capabilities and avoid the local optima. To further accelerate convergence during local optimization, a hyperbolic dynamic adjustment mechanism is introduced. Moreover, a classical ant colony optimization algorithm is incorporated to guide the global search via pheromone trails, thus improving exploration in complex search space. The best solutions from subpopulations are shared periodically, facilitating efficient information exchange. For binary FS, a regularized binary encoding is used to adapt the algorithm. The effectiveness of CHBSI is validated on 26 benchmark functions, and its binary version (bCHBSI) is tested on FS tasks across 18 UCI datasets. The experimental results show that both CHBSI and bCHBSI excel in convergence speed, fitness, classification accuracy, F1-score, the size of feature subset, and stability, significantly outperforming 10 advanced algorithms, including the S-shaped and V-shaped variants of CHBSI. CHBSI demonstrates scalability and robustness, maintaining superior performance across datasets with varying dimensionalities and complexities.},
  archive      = {J_PR},
  author       = {Mengqing Mei and Songsong Zhang and Zhiwei Ye and Mingwei Wang and Wen Zhou and Jia Yang and Jixin Zhang and Lingyu Yan and Jun Shen},
  doi          = {10.1016/j.patcog.2025.111901},
  journal      = {Pattern Recognition},
  month        = {1},
  pages        = {111901},
  shortjournal = {Pattern Recognition},
  title        = {A cooperative hybrid breeding swarm intelligence algorithm for feature selection},
  volume       = {169},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Cutting through the noise: Explaining residuals in multivariate time series with motif analysis. <em>PR</em>, <em>169</em>, 111900. (<a href='https://doi.org/10.1016/j.patcog.2025.111900'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Modeling real-world system dynamics is challenging due to non-periodic patterns, such as stimuli-dependent physiological responses in health, event-driven traffic in mobility, and news-triggered interactions in societal systems. In the absence of contextual data, state-of-the-art methods—including advanced deep learning architectures—struggle to model these irregular behaviors. Furthermore, their predictive focus often limits their utility for descriptive analysis, hampering knowledge acquisition. This work addresses these challenges by proposing a methodology to decompose multivariate time series residuals into statistically significant, meaningful events, effectively filtering noise. We extend the motif discovery task to identify irregular patterns satisfying key properties: non-triviality, statistical significance, multi-dimensionality, and actionability. Our approach introduces principles to mitigate biases, evaluate statistical significance, place robust hyperparameterization, explore relationships in multivariate residuals, and incorporate domain knowledge through specialized masks. Real-world case studies validate the proposed methodology, uncovering explainable patterns relevant to human activity recognition, energy consumption, and urban planning, accounting for up to 50% of irregular components and revealing hidden system behaviors.},
  archive      = {J_PR},
  author       = {Miguel G. Silva and Sara C. Madeira and Rui Henriques},
  doi          = {10.1016/j.patcog.2025.111900},
  journal      = {Pattern Recognition},
  month        = {1},
  pages        = {111900},
  shortjournal = {Pattern Recognition},
  title        = {Cutting through the noise: Explaining residuals in multivariate time series with motif analysis},
  volume       = {169},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Leveraging negative correlation for full-range self-attention in vision transformers. <em>PR</em>, <em>169</em>, 111899. (<a href='https://doi.org/10.1016/j.patcog.2025.111899'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Self-Attention (SA) mechanisms that employ the nonlinear Softmax function restrict attention weights to the range (0, 1). This inherent limitation of SA hinders its ability to recognize negative correlations among feature attributes. To overcome this limitation, we propose Full-Range Self-Attention (FSA) . Compared to SA, FSA operates more effectively by recognizing negative correlations, adjusting focus intensity, and requiring no extra parameters. We first utilize the projection values of K T onto Q in both directions as weights for positive and negative similarities. Then, we develop a Bidirectional Attention Weight Selection (BAWS) strategy that performs Softmax normalization in each of the two directions, providing the model with more insight into the negative interdependencies. Furthermore, the FSA employs an Attention Redistribution Connection (ARC) to dynamically adjust attention weights, effectively extending the possibility of attentional capture. We conduct comprehensive experiments on a variety of models for classification, segmentation, and detection tasks. Our FSA significantly improves the performance of these models across tasks. The code is available at here.},
  archive      = {J_PR},
  author       = {Wei Long and Ziyang Chen and Wenting Li and Yongjun Zhang and He Yao and Jiaxin Peng and Zhongwei Cui},
  doi          = {10.1016/j.patcog.2025.111899},
  journal      = {Pattern Recognition},
  month        = {1},
  pages        = {111899},
  shortjournal = {Pattern Recognition},
  title        = {Leveraging negative correlation for full-range self-attention in vision transformers},
  volume       = {169},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). TOFFNet: A texture orientation-based feature fusion network for contactless multimodal finger recognition. <em>PR</em>, <em>169</em>, 111898. (<a href='https://doi.org/10.1016/j.patcog.2025.111898'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As biometrics technology evolves, there is a growing demand for secure and accurate recognition systems, especially in security-sensitive or confidential applications. However, traditional unimodal recognition systems often suffer from factors such as variations in lighting, posture and occlusion, resulting in decreased accuracy. To address these limitations, multimodal recognition systems have emerged, combining multiple biometric modalities to enhance system robustness and accuracy. In this study, we propose a Texture Orientation-based Feature Fusion Network (TOFFNet), which leverages orientation information extracted from contactless finger texture features, effectively improving recognition system performance. This network utilizes the texture feature information from diverse modalities to achieve complementarity and fusion between them. A directional attention fusion module is developed to compute attention along the parallel or perpendicular axis of the finger’s long axis, facilitating texture orientation-based feature fusion. Moreover, a specialized sampling strategy for challenging samples is designed to enhance the performance of multimodal tasks. To prove the effectiveness of the proposed method, we employ THU-FVFP, a visible light finger textures and finger vein dataset containing contactless modalities that are suitable for sensitive environments. The results of extensive experiments show that our proposed method can fully utilize the information of the two modalities to improve recognition performance.},
  archive      = {J_PR},
  author       = {Siqi Wang and Zishuang Wang and Jiapeng Lin and Wenming Yang and Qingmin Liao},
  doi          = {10.1016/j.patcog.2025.111898},
  journal      = {Pattern Recognition},
  month        = {1},
  pages        = {111898},
  shortjournal = {Pattern Recognition},
  title        = {TOFFNet: A texture orientation-based feature fusion network for contactless multimodal finger recognition},
  volume       = {169},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Rethinking multi-pattern mining from a perspective of pattern prototype learning. <em>PR</em>, <em>169</em>, 111896. (<a href='https://doi.org/10.1016/j.patcog.2025.111896'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Visual pattern mining aims to discover discriminative and frequent visual elements in images. Earlier studies have adopted a two-step framework comprising representation learning followed by pattern mining . However, the reliance on clustering algorithms in the mining step introduces hypersensitivity to parameter selection, resulting in either redundant or missing visual patterns. Furthermore, the optimization based on two non-interactive steps prevents the framework from adapting itself to diverse data. In this work, we rethink the multi-pattern mining task from the perspective of prototype learning, and introduce an end-to-end Visual Pattern Prototypes Proposing (VP3) method with two losses: class-wise and pattern-wise contrastive loss. VP3 eliminates the reliance on clustering algorithms by treating prototypes as visual patterns, and integrates the mining results (pattern prototypes) into representation learning . Consequently, VP3 can achieve more robust results. Extensive experiments conducted on four benchmark datasets demonstrate the robustness and superior performance of VP3 over six state-of-the-art methods. Code is available at https://github.com/BeCarefulOfYournaoke/VP3 .},
  archive      = {J_PR},
  author       = {Guanghui Shi and Xuefeng Liang and Xiaoyu Lin},
  doi          = {10.1016/j.patcog.2025.111896},
  journal      = {Pattern Recognition},
  month        = {1},
  pages        = {111896},
  shortjournal = {Pattern Recognition},
  title        = {Rethinking multi-pattern mining from a perspective of pattern prototype learning},
  volume       = {169},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Fully PolSAR image reconstruction for enhanced land cover mapping. <em>PR</em>, <em>169</em>, 111895. (<a href='https://doi.org/10.1016/j.patcog.2025.111895'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Polarimetric Synthetic Aperture Radar (PolSAR) images provide richer ground object information than optical images due to their full-polarization capabilities. However, annotating PolSAR images is time-consuming, limiting the available research data and increasing the risk of overfitting. Addressing the scarcity of labeled data is crucial for land cover mapping. For this, we propose a PolSAR image reconstruction method called PolSAR to RGB (P2R), which expands the dataset by generating reconstructed RGB images from original PolSAR images. While the reconstructed images augment the dataset, they still contain lots of speckle noise, leading to lower land cover mapping performance. To alleviate this defect, we also design a Gate Denoised Module (GDM) consisting of two components: Image Denoising guided by Original Polarization (IDOP) and Feature Generation with Gated Unit (FGGU) module. Specifically, the IDOP module reduces the impact of noise on the feature map by learning similar characteristics between the original polarization and reconstructed RGB images. The FGGU module adaptively fuses the low-level feature map with the denoised feature map by a weighted gate, which helps remove the noise in images without affecting their details. Experiments on several publicly available PolSAR datasets show the effectiveness of our land cover mapping method, reaching 53.94% mIoU in AIR-PolSAR-Seg, 76.71% mIoU in PolSF-RS2, and 90.34% mIoU in PolSF-GF3.},
  archive      = {J_PR},
  author       = {Xu Sun and Junyu Gao and Yuan Yuan},
  doi          = {10.1016/j.patcog.2025.111895},
  journal      = {Pattern Recognition},
  month        = {1},
  pages        = {111895},
  shortjournal = {Pattern Recognition},
  title        = {Fully PolSAR image reconstruction for enhanced land cover mapping},
  volume       = {169},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Deformable feature alignment and refinement for moving infrared small target detection. <em>PR</em>, <em>169</em>, 111894. (<a href='https://doi.org/10.1016/j.patcog.2025.111894'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The detection of moving infrared small targets has been a challenging and prevalent research topic. The current state-of-the-art methods are mainly based on 3D CNN or ConvLSTM to aggregate information from adjacent frames to facilitate the detection of the current frame. However, these methods implicitly utilize motion information and fail to explicitly explore motion compensation, resulting in poor performance in the case of a video sequence including large motion. In this paper, we propose a Deformable Feature Alignment and Refinement (DFAR) method based on deformable convolution to explicitly use motion context in both the training and inference stages. Specifically, a Temporal Deformable Alignment (TDA) module based on the designed Dilated Convolution Attention Fusion (DCAF) block is developed to explicitly align the adjacent frames with the current frame at the feature level. Then, the feature refinement module adaptively fuses the aligned features and further aggregates useful spatio-temporal information by means of the proposed Attention-guided Deformable Fusion (AGDF) block. In addition, to improve the alignment of adjacent frames with the current frame, we extend the traditional loss function by introducing a new motion compensation loss. Extensive experimental results demonstrate the effectiveness of our proposed DFAR method. We will release our codes.},
  archive      = {J_PR},
  author       = {Dengyan Luo and Yanping Xiang and Hu Wang and Luping Ji and Shuai Li and Mao Ye},
  doi          = {10.1016/j.patcog.2025.111894},
  journal      = {Pattern Recognition},
  month        = {1},
  pages        = {111894},
  shortjournal = {Pattern Recognition},
  title        = {Deformable feature alignment and refinement for moving infrared small target detection},
  volume       = {169},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Generative attack in complex real-world scenarios. <em>PR</em>, <em>169</em>, 111893. (<a href='https://doi.org/10.1016/j.patcog.2025.111893'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Existing generative attacks primarily learn from single-object scenarios, and they might fail to handle the intricate spatial and semantic relationships between multiple objects, which are common in real-world scenarios with dense occlusions and other complexities. Addressing these limitations, we propose Generative Attack in Complex Real-world Scenarios (GACRS), a novel method designed to enhance the transferability of adversarial examples. Primarily, our analysis indicates that existing method for utilizing the CLIP text branch is limited, mainly due to its random sampling strategies, which introduces sampling bias and restricts it to the scenarios with only two categories within a single scene. Thus, we propose a multi-object clustering-based text sampling method tailored for the CLIP text branch, thereby enhancing the diversity and relevance of text features and providing more meaningful guidance for generator optimization. In addition, to the best of our knowledge, we are the first to apply curriculum learning to the training process of generative attacks. This operation involves a dynamic input sample selection strategy that adapts to different training phases, enabling the generator to transition from simpler tasks to more complex tasks, thereby improving the generalization capability of adversarial perturbations. Extensive experiments across within-domain, cross-domain, and cross-task scenarios show that GACRS consistently outperforms existing peer methods. Codes will be released at https://github.com/phyyyy/GACRS.},
  archive      = {J_PR},
  author       = {Hongyu Peng and Gong Cheng and Xuxiang Sun},
  doi          = {10.1016/j.patcog.2025.111893},
  journal      = {Pattern Recognition},
  month        = {1},
  pages        = {111893},
  shortjournal = {Pattern Recognition},
  title        = {Generative attack in complex real-world scenarios},
  volume       = {169},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). UC-PUAL: A universally consistent classifier of positive-unlabelled data. <em>PR</em>, <em>169</em>, 111892. (<a href='https://doi.org/10.1016/j.patcog.2025.111892'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Positive-unlabelled (PU) learning is a challenging task in pattern recognition, as there are only labelled-positive instances and unlabelled instances available for the training of a classifier. The task becomes even harder when the PU data show an underlying trifurcate pattern that positive instances roughly distribute on both sides of ground-truth negative instances. To address this issue, we propose a universally consistent PU classifier with asymmetric loss (UC-PUAL) on positive instances. We also propose two three-block algorithms for non-convex optimisation to enable UC-PUAL to obtain linear and kernel-induced non-linear decision boundaries, respectively. Theoretical and experimental results verify the superiority of UC-PUAL. The code for UC-PUAL is available at https://github.com/tkks22123/UC-PUAL .},
  archive      = {J_PR},
  author       = {Xiaoke Wang and Rui Zhu and Jing-Hao Xue},
  doi          = {10.1016/j.patcog.2025.111892},
  journal      = {Pattern Recognition},
  month        = {1},
  pages        = {111892},
  shortjournal = {Pattern Recognition},
  title        = {UC-PUAL: A universally consistent classifier of positive-unlabelled data},
  volume       = {169},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). High-quality coronary artery segmentation via fuzzy logic modeling coupled with dynamic graph convolutional network. <em>PR</em>, <em>169</em>, 111891. (<a href='https://doi.org/10.1016/j.patcog.2025.111891'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Coronary artery segmentation is a critical yet challenging task in diagnosing and evaluating coronary artery disease (CAD), as manual delineation is labor-intensive, time-consuming, and prone to inter-observer variability. Automated segmentation methods have been developed to mitigate these issues; however, existing approaches are often insufficiently accurate to meet clinical use standards. This is due to the intricate anatomical structures of coronary arteries (CAs), their sparsity, and the low contrast with adjacent tissues. To address these challenges, we propose a novel 3D deep network, FG-Net, designed to achieve high-quality coronary artery segmentation. Specifically, we introduce a novel boundary completeness enhancement (BCE) module for uncertainty reduction via fuzzy logic modeling, capable of enhancing the completeness of vessel boundaries and thus improving the network’s ability to determine CA boundaries under challenging circumstances. Furthermore, we present a trajectory continuity enhancement (TCE) module based on a dynamic graph convolutional network (GCN) that is capable of adaptively enhancing the continuity of vessel trajectories, thereby improving the network’s ability to deal with complicated artery structures spanning across diverse scales. Extensive experiments on two in-house datasets and three publicly available datasets consistently demonstrate that FG-Net achieves superior segmentation performance and generalization ability, outperforming multiple state-of-the-art algorithms across various metrics. Code is available at FG-Net .},
  archive      = {J_PR},
  author       = {Caixia Dong and Duwei Dai and Yang Li and Songhua Xu},
  doi          = {10.1016/j.patcog.2025.111891},
  journal      = {Pattern Recognition},
  month        = {1},
  pages        = {111891},
  shortjournal = {Pattern Recognition},
  title        = {High-quality coronary artery segmentation via fuzzy logic modeling coupled with dynamic graph convolutional network},
  volume       = {169},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Efficient ℓ2,1-norm graph for robust semi-supervised classification. <em>PR</em>, <em>169</em>, 111890. (<a href='https://doi.org/10.1016/j.patcog.2025.111890'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Representation-based methods for affinity graph construction have exhibited excellent performance in many semi-supervised learning applications. This paper aims to develop a novel representation-based affinity graph construction method to reveal the true intrinsic subspace structures of data. First, we propose to construct an efficient and robust ℓ 2 , 1 -norm graph (termed L21graph) by imposing the ℓ 2 , 1 -norm minimization regularization and the adaptive distance penalty over the representation coefficient matrix. We theoretically prove that the ℓ 2 , 1 -norm converges towards its nuclear norm and can serve as the convex relaxation of the nuclear norm. Moreover, as a mixed matrix norm of the ℓ 1 -norm and ℓ 2 -norm, the ℓ 2 , 1 -norm leverages the discriminative nature of the ℓ 1 -norm while can drive closed-form solution like the ℓ 2 -norm. Thus L21graph achieves improvement in both efficiency and effectiveness. Second, in case of noisy dataset, the pair-wise distances computed from the corrupted samples may fail to capture the true neighbor relationship of data, leading to performance degradation. To this end, we propose to recover clean low-rank components from noisy data (by robust principal component analysis) and learn the affinity graph in one framework, which is referred to as L21graph with denoised data (termed L21graph-DF). Extensive experiments on multiple public benchmark datasets verify that our proposed method outperforms the existing state-of-the-art affinity graph learning method for semi-supervised classification.},
  archive      = {J_PR},
  author       = {Xiaoguang Qiao and Caikou Chen and Weiye Wang and Qian Peng and Abdul Ghafar},
  doi          = {10.1016/j.patcog.2025.111890},
  journal      = {Pattern Recognition},
  month        = {1},
  pages        = {111890},
  shortjournal = {Pattern Recognition},
  title        = {Efficient ℓ2,1-norm graph for robust semi-supervised classification},
  volume       = {169},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Graph perceiver IO: A general architecture for graph-structured data. <em>PR</em>, <em>169</em>, 111889. (<a href='https://doi.org/10.1016/j.patcog.2025.111889'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multimodal machine learning has been widely studied for the development of general intelligence. Recently proposed Perceiver and its variant (Perceiver IO) have shown promising results in addressing diverse types of input modalities with a universal model architecture. However, they have mainly focused on image and text data modalities, and it is unclear whether this kind of universal architecture can also be effective for graph-structured datasets. As the graph data contains topological information, which is lacking in the image and text data, it is non-trivial to devise a universal architecture for graph and other data modalities altogether. In this study, we provide a Graph Perceiver IO (GPIO), a class of Perceiver IO models that addresses graph-structured datasets. We keep the main structure of the GPIO the same as with the Perceiver, which can already handle multiple data modalities, while focusing on how we can extend it to the graph domain. By leveraging positional encoding and output query smoothing, GPIO serves as a general architecture that handles graph-structured data as well as text and image data. Besides, we further propose GPIO+ for the multimodal few-shot classification that incorporates both images and graphs simultaneously. Through extensive experiments covering link prediction, graph classification, node classification, and multimodal text classification, we demonstrate that GPIO and GPIO+ outperform the representative graph neural network baseline models, while requiring lower computational complexity than them.},
  archive      = {J_PR},
  author       = {Seyun Bae and Hoyoon Byun and Changdae Oh and Yoon-Sik Cho and Kyungwoo Song},
  doi          = {10.1016/j.patcog.2025.111889},
  journal      = {Pattern Recognition},
  month        = {1},
  pages        = {111889},
  shortjournal = {Pattern Recognition},
  title        = {Graph perceiver IO: A general architecture for graph-structured data},
  volume       = {169},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Progressive frequency-decoupled network with neighbor-aware self-evolving convolution kernels for pansharpening. <em>PR</em>, <em>169</em>, 111888. (<a href='https://doi.org/10.1016/j.patcog.2025.111888'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Pansharpening uses the spatial details of the panchromatic image (PAN) to sharpen the multispectral image (MS), so as to obtain a higher-resolution multispectral image for subsequent vision tasks. Existing methods focus more on high-level semantic feature extraction, the diverse features cannot be effectively distinguished and fully utilized. In this paper, we propose a novel strategy to generate neighbor-aware self-evolving convolution kernels (NSConv) and introduce a frequency-decoupled block (FDBlock) for deep fusion. The proposed NSConv can dynamically optimize in multiple dimensions and enhance the ability to extract locally correlated features. Furthermore, the FDBlock realizes the interactive fusion of dual-domain features based on NSConv to learn the complementary representation, making the overall network have a more diverse representation and strong neighborhood feature extraction ability. We conduct extensive qualitative and quantitative experiments to validate the effectiveness of the proposed network and demonstrate its significant performance against other state-of-the-art methods.},
  archive      = {J_PR},
  author       = {Rui Miao and Fengguang Peng and Dong Dong and Hang Shi},
  doi          = {10.1016/j.patcog.2025.111888},
  journal      = {Pattern Recognition},
  month        = {1},
  pages        = {111888},
  shortjournal = {Pattern Recognition},
  title        = {Progressive frequency-decoupled network with neighbor-aware self-evolving convolution kernels for pansharpening},
  volume       = {169},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). A multi-model dynamic filtering of label noise for regression. <em>PR</em>, <em>169</em>, 111887. (<a href='https://doi.org/10.1016/j.patcog.2025.111887'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Training with the numerical noisy labels in regression would weaken the generalization performance of the model. Noise filtering is a popular technique for removing mislabeled samples to reduce noise. Existing filtering methods usually utilize fixed models; however, they are prone to falling into a vicious circle of noisy data and biased models, i.e., a chicken-and-egg dilemma in noise filtering. We propose a dynamic model recall filtering (DMRF) algorithm to address this issue from the perspectives of model selection and data preprocessing. Firstly, we propose a strong correlation model selection criterion to alleviate the biased model, guaranteed by a reduced error bound. Then, we employ warm-up filtering and a data recall strategy in noisy data preprocessing to balance the noise removal and overcleaning. Finally, we design a noise estimator by constructing an adaptive posterior distribution of the latent true label according to the maximum a posteriori rule. The experimental results on benchmark datasets as well as real-world age estimation and gaze estimation tasks show that our DMRF algorithm has a more accurate noise estimation and yields a better generalization performance than previous algorithms.},
  archive      = {J_PR},
  author       = {Gaoxia Jiang and Fan Lei and Wenjian Wang},
  doi          = {10.1016/j.patcog.2025.111887},
  journal      = {Pattern Recognition},
  month        = {1},
  pages        = {111887},
  shortjournal = {Pattern Recognition},
  title        = {A multi-model dynamic filtering of label noise for regression},
  volume       = {169},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Skeleton-prompt: A cross-dataset transfer learning approach for skeleton action recognition. <em>PR</em>, <em>169</em>, 111885. (<a href='https://doi.org/10.1016/j.patcog.2025.111885'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper presents Skeleton-Prompt, a novel tuning method designed to tackle cross-dataset transfer issues in skeleton action recognition models. Given the scarcity of large-scale 3D skeleton datasets and the variability in keypoint structures across datasets, existing methods often rely on training models from scratch, necessitating extensive labeled data and exhibiting high sensitivity to occlusion. Our approach aims to fine-tune pre-trained models to adapt to limited real-world skeleton data. We use 2D skeletons as inputs and leverage a large human motion dataset for 2D to 3D pose estimation to learn generalizable motion features. A lightweight prompt generator produces instance-level prompts, and we employ dynamic queries with cross-attention to refine the semantic information of the input data. Additionally, we introduce a joint-enhanced multi-stream fusion mechanism based on self-attention to improve robustness against incomplete skeletons. Skeleton-Prompt represents a significant advancement in efficient fine-tuning for skeleton action recognition, effectively addressing cross-dataset generalization challenges in a data-efficient and parameter-efficient manner.},
  archive      = {J_PR},
  author       = {Mingqi Lu and Xiaobo Lu and Jun Liu},
  doi          = {10.1016/j.patcog.2025.111885},
  journal      = {Pattern Recognition},
  month        = {1},
  pages        = {111885},
  shortjournal = {Pattern Recognition},
  title        = {Skeleton-prompt: A cross-dataset transfer learning approach for skeleton action recognition},
  volume       = {169},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Insights into imbalance-aware multilabel prototype generation mechanisms for k-nearest neighbor classification in noisy scenarios. <em>PR</em>, <em>169</em>, 111884. (<a href='https://doi.org/10.1016/j.patcog.2025.111884'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Prototype Generation (PG) techniques enhance the efficiency of the k -Nearest Neighbor ( k NN) classifier by condensing datasets through the use of specific rules. More precisely, these strategies work on the premise of merging the elements in the reference data collection to generate an alternative and more compact data assortment that substitutes the former one without remarkably affecting the recognition performance. Nevertheless, despite being widely studied in multiclass scenarios, PG is still underexplored in multilabel contexts, leading to limitations, notably in the handling of label imbalance and noise. In this regard, this work introduces a reduction framework that allows for multilabel PG methods to handle these challenges of label imbalance and noise. The proposed mechanisms comprise a selection strategy that exclusively preserves noise-free samples in the process, a mechanism to avoid severely imbalanced samples from being inadequately processed, and two new merging policies for the PG methods to generate novel samples. These enhancements are considered along with three established multilabel PG methods: Multilabel Reduction through Homogeneous Clustering, Multilabel Chen, and Multilabel Reduction through Space Partitioning. Evaluations are conducted using three k NN-based multilabel classifiers and 12 diverse datasets with different levels of label imbalance. We additionally study the performance with varying values of k under different label-noise scenarios. The results are assessed through statistical tests and indicate that our proposals outperform the original methods that disregard label imbalance, even in the presence of noise, thus validating these approaches and fostering further research in the field.},
  archive      = {J_PR},
  author       = {Jose J. Valero-Mas and Carlos Penarrubia and Francisco J. Castellanos and Antonio Javier Gallego and Jorge Calvo-Zaragoza},
  doi          = {10.1016/j.patcog.2025.111884},
  journal      = {Pattern Recognition},
  month        = {1},
  pages        = {111884},
  shortjournal = {Pattern Recognition},
  title        = {Insights into imbalance-aware multilabel prototype generation mechanisms for k-nearest neighbor classification in noisy scenarios},
  volume       = {169},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Visual perception-inspired 3D point cloud sampling. <em>PR</em>, <em>169</em>, 111883. (<a href='https://doi.org/10.1016/j.patcog.2025.111883'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Task-oriented sampling aims to predict the importance of points of a point cloud to better serve downstream tasks, which has attracted increasing attention in the fields of computer vision and visualization in recent years. However, existing methods cannot sufficiently leverage both global saliency and local saliency cues, resulting in suboptimal performance that requires further improvement. To tackle this challenge, we propose a novel 3D point cloud sampling method inspired by the human visual perception mechanism in this study, which can effectively extract important point cloud subsets from critical regions to better adapt to downstream tasks, thereby maintaining superior sampling performance. The proposed Visual Perception-inspired 3D Point Cloud Sampling (VPI-3DPS) method simulates the human visual system’s dynamic attention-shifting strategy by combining coarse-grained attention-driven sampling with fine-grained detail preservation. This allows our approach to adaptively capture both global context and local details within point cloud data, safeguarding downstream task performance. By leveraging Gated Recurrent Units (GRUs) for long-term dependency modeling and integrating Graph Convolutional Networks (GCNs) to capture local structures, VPI-3DPS obtains an integrated representation of regional correlation and detail awareness. Extensive experiments show that VPI-3DPS outperforms existing methods. Compared to the best-performing approaches, it achieves an average increase of 1.29% in classification accuracy, an average reduction of 13.20% in registration MRE, and an average decrease of 4.29% in Chamfer Distance for reconstruction.},
  archive      = {J_PR},
  author       = {Xu Wang and Yi Jin and Hui Yu and Yigang Cen and Yidong Li},
  doi          = {10.1016/j.patcog.2025.111883},
  journal      = {Pattern Recognition},
  month        = {1},
  pages        = {111883},
  shortjournal = {Pattern Recognition},
  title        = {Visual perception-inspired 3D point cloud sampling},
  volume       = {169},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Normalized cut co-clustering with out-of-sample extension. <em>PR</em>, <em>169</em>, 111881. (<a href='https://doi.org/10.1016/j.patcog.2025.111881'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Co-clustering is a critical data mining technology in various real-world applications, where anchor-based methods reveal the dual relationships between samples and anchors. Due to the information loss caused by relaxation and post-processing, classical anchor-based methods suffer from potential performance degradation. To overcome this disadvantage, we propose a Normalized Cut Co-Clustering ( NC 3 ) model, which assigns clusters for samples and anchors by alternatively updating the discrete label matrices. Different from traditional anchor-based co-clustering methods, our model solves the original discrete normalized cut problem on the bipartite graph directly. To address the discrete cut problem, an iterative coordinate ascent algorithm is presented, which can speed up the clustering process. Through optimization on the label matrices of samples and anchors, the clusters can be obtained without relaxation–discretization operation. Furthermore, the proposed NC 3 model can tackle the out-of-sample clustering issue based on labels of anchors. Through extensive experiments, we validate the effectiveness of our model, achieving competitive results compared to state-of-the-art approaches.},
  archive      = {J_PR},
  author       = {Jingyu Wang and Mingqing Liu and Feiping Nie and Xuelong Li},
  doi          = {10.1016/j.patcog.2025.111881},
  journal      = {Pattern Recognition},
  month        = {1},
  pages        = {111881},
  shortjournal = {Pattern Recognition},
  title        = {Normalized cut co-clustering with out-of-sample extension},
  volume       = {169},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Making clustering methods workable for shapes using the ordinary procrustes sum of squares. <em>PR</em>, <em>169</em>, 111878. (<a href='https://doi.org/10.1016/j.patcog.2025.111878'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the last few decades, a number of clustering methods for vectorial data have matured. Occasionally, researchers are interested in making them workable for data of other types, such as an object’s shape. The current difficulty in applying them to shape clustering is that clustering must be invariant to several similarity transformations of shapes. Therefore, we use the ordinary Procrustes sum of squares (OSS) in Procrustes analysis, which refers to analysis that is invariant to the similarity transformations between shapes. Thus, in this study, we aim to make mature methods workable in shape clustering. To achieve this, the essential points that we need to address are to rewrite the optimization problem associated with each method and to present a feasible method for computing the solution to the problem. As a result, we base the method on the OSS, its derivative, or a symmetric matrix concerning the OSS. Another aim of this study is to identify specific applications of shape clustering. We demonstrate the applications through experiments using datasets that contain skewed line drawings, American football formations, and baseball pitch trajectories. We examine the OSS-based methods by considering shape classification and determining the best method for implementing the OSS. As a result, regardless of the dataset, we demonstrate that the OSS in every method is more effective than other shape distances and that the convex clustering method incorporating the OSS is best performed in several ways.},
  archive      = {J_PR},
  author       = {Kazunori Iwata and Yuki Nontani and Kazushi Mimura},
  doi          = {10.1016/j.patcog.2025.111878},
  journal      = {Pattern Recognition},
  month        = {1},
  pages        = {111878},
  shortjournal = {Pattern Recognition},
  title        = {Making clustering methods workable for shapes using the ordinary procrustes sum of squares},
  volume       = {169},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Dual assignment of labels for end-to-end fully convolutional object detection. <em>PR</em>, <em>169</em>, 111877. (<a href='https://doi.org/10.1016/j.patcog.2025.111877'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Fully convolutional detectors discard the one-to-many assignment and adopt a one-to-one assigning strategy to achieve end-to-end detection but suffer from the slow convergence issue. In this paper, we revisit these two assignment methods and find that bringing one-to-many assignment back to end-to-end fully convolutional detectors helps with model convergence. Based on this observation, we propose D ual A ssignment of labels for end-to-end fully convolutional de TE ction (DATE). Our method constructs two branches with one-to-many and one-to-one assignment during training and speeds up the convergence of the branch trained with one-to-one assignment strategy by providing more supervision signals. DATE only uses the branch with the one-to-one matching strategy for model inference, which does not bring inference overhead. Experimental results show that Dual Assignment speeds up model convergence.Our code and models are publicly available at https://github.com/YiqunChen1999/date .},
  archive      = {J_PR},
  author       = {Yiqun Chen and Qiang Chen and Qinghao Hu and Jian Cheng},
  doi          = {10.1016/j.patcog.2025.111877},
  journal      = {Pattern Recognition},
  month        = {1},
  pages        = {111877},
  shortjournal = {Pattern Recognition},
  title        = {Dual assignment of labels for end-to-end fully convolutional object detection},
  volume       = {169},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). RIDE: Redensification-based intrinsic density estimation for knowledge graphs. <em>PR</em>, <em>169</em>, 111876. (<a href='https://doi.org/10.1016/j.patcog.2025.111876'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The rapid advancement of knowledge graphs has facilitated their widespread application across diverse tasks. However, the prohibitive costs associated with constructing knowledge graphs have resulted in pervasive incomplete issues, significantly constraining their overall utility and effectiveness. While considerable efforts have been made to enhance the completeness of knowledge graphs, there remains a lack of a universal quantitative framework for evaluating knowledge graph completeness. This study first delineates two primary sources of incompleteness in knowledge graphs. To measure the incompleteness arising from the insufficiency of facts, the concept of intrinsic density is introduced, which is defined as the completeness of the pattern encapsulated within the existing data of a knowledge graph. To operationalize this concept, we propose RIDE , a novel method that quantitatively measures intrinsic density by assessing a knowledge graph’s ability to regenerate missing facts based on the patterns embedded in its existing data. To the best of our knowledge, this is the first practical and general approach for quantitatively evaluating the completeness of knowledge graphs from an intrinsic perspective.},
  archive      = {J_PR},
  author       = {Wei-Kun Kong and Yiping Duan and Xiaoming Tao and Yueran Zu},
  doi          = {10.1016/j.patcog.2025.111876},
  journal      = {Pattern Recognition},
  month        = {1},
  pages        = {111876},
  shortjournal = {Pattern Recognition},
  title        = {RIDE: Redensification-based intrinsic density estimation for knowledge graphs},
  volume       = {169},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). AdaPrompt-IR: Adaptive learning to perceive degradation semantic and prompting for all-in-one image restoration. <em>PR</em>, <em>169</em>, 111875. (<a href='https://doi.org/10.1016/j.patcog.2025.111875'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Image restoration is a critical task aimed at recovering high-quality, clean images from their degraded counterparts. While deep learning-based methods have made significant strides in this area, they still struggle with generalizing across a wide range of degradation types and levels. This limitation restricts their practical applicability, as it often requires training separate models for each degradation type and knowing the specific degradation in advance to apply the appropriate model. In this work, we introduce AdaPrompt-IR, an adaptive all-in-one image restoration network that combines degradation semantic mining and prompt learning. Our approach leverages a degradation representation codebook to effectively decompose and capture various degradation types. Building upon this foundation, we propose a degradation activation module, integrated with a dual-branch architecture, which explicitly facilitates the restoration of missing degradation semantics. Additionally, we incorporate degradation-specific information as prompts to implicitly guide the restoration process. Extensive experiments demonstrate that our method achieves state-of-the-art performance across multiple image restoration tasks, including image denoising, dehazing, deraining, motion deblurring, and low-light image enhancement. Our code is available at https://github.com/hagerwang/AdaPromote-IR .},
  archive      = {J_PR},
  author       = {Wei Sun and Qianzhou Wang and Yaqi Wang and Zhiqiang Hou and Qingsen Yan and Yanning Zhang},
  doi          = {10.1016/j.patcog.2025.111875},
  journal      = {Pattern Recognition},
  month        = {1},
  pages        = {111875},
  shortjournal = {Pattern Recognition},
  title        = {AdaPrompt-IR: Adaptive learning to perceive degradation semantic and prompting for all-in-one image restoration},
  volume       = {169},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Multiview feature selection combining latent space and similarity structure learning. <em>PR</em>, <em>169</em>, 111874. (<a href='https://doi.org/10.1016/j.patcog.2025.111874'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Unsupervised multiview feature selection dependent on similar or clustering structures has dramatically progressed, but both ignore the mutually reinforcing relationship between structure learning. Firstly, the two methods of appeal ignore the possibility of shared joint learning, although they can learn coherent information. Secondly, they are inadequate for data structure learning and lack awareness of co-learning global and local structures. This paper proposes an unsupervised feature selection method (SCSF_FS) that integrates similarity and clustering structures learning to capture intrinsic information in data. Specifically, we use latent space learning to partition multiple data matrices into view-specific base matrices and clustering metrics. Structural learning is used to transform specific clustering metrics into coherent clustering structures. In addition, adaptive weights are used on each view’s similarity matrix to learn a consistent similarity structure, while laplacian graph learning is introduced to unify the similarity and clustering structures. Consequently, a unified framework is designed to unify multiview consistency learning, global structure, and local structure preservation. Moreover, an optimization iteration algorithm is designed to solve it. Comparison with eight algorithms shows the effectiveness of the proposed method.},
  archive      = {J_PR},
  author       = {Zhuowen Li and Hongmei Chen and Tengyu Yin and Zhong Yuan and Chuan Luo and Shi-Jinn Horng and Tianrui Li},
  doi          = {10.1016/j.patcog.2025.111874},
  journal      = {Pattern Recognition},
  month        = {1},
  pages        = {111874},
  shortjournal = {Pattern Recognition},
  title        = {Multiview feature selection combining latent space and similarity structure learning},
  volume       = {169},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Efficiently harmonizing information sharing for heterogeneous graph contrastive learning. <em>PR</em>, <em>169</em>, 111873. (<a href='https://doi.org/10.1016/j.patcog.2025.111873'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In contrastive learning, the metapath coupled mutual information maximization paradigm struggles to capture rich node context, due to challenge with global representation consistency. Moreover, it simplistically encodes semantic subgraphs in isolation, which not only overlooks potential interactions between different semantic structures but also leads to redundant node encoding. To tackle these challenges, we propose an Efficiently Harmo nizing Information Sharing for H eterogeneous G raph C ontrastive L earning (HarmoHGCL). Specifically, topology and attribute knowledge are decoupled to capture different relationships and node-specific information. Additionally, a semantic subgraph fusion strategy is proposed to capture the structural interactions between different semantics and employ them as anchor samples. Finally, the above learning modules enable efficient cross-view contrastive learning and harmonize information from different views by node attributes sharing and triple loss strategies. Experimental results show that HarmoHGCL outperforms state-of-the-art methods. The source codes can be accessed at GitHub. 1},
  archive      = {J_PR},
  author       = {Xiangkai Zhu and Chao Li and Yeyu Yan and Jinhu Fu and Zhongying Zhao and Qingtian Zeng},
  doi          = {10.1016/j.patcog.2025.111873},
  journal      = {Pattern Recognition},
  month        = {1},
  pages        = {111873},
  shortjournal = {Pattern Recognition},
  title        = {Efficiently harmonizing information sharing for heterogeneous graph contrastive learning},
  volume       = {169},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). More video-relevant paragraph captioning via perturbed attention self-distillation. <em>PR</em>, <em>169</em>, 111871. (<a href='https://doi.org/10.1016/j.patcog.2025.111871'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Two-stage methods have dominated video paragraph captioning for several years. Recently, one-stage methods have gained popularity, eschewing the event detection stage and generating paragraphs directly from untrimmed videos using transformers. Despite consistently improving performance, we argue that current one-stage methods generate paragraphs less relevant to the video content, a shortcoming not adequately reflected by reference-based metrics. In this paper, we aim to enhance the video relevance of generated paragraph captions for one-stage models. Towards this goal, we identify two key issues: (i) one-stage models struggle to detect decisive video content due to insufficient attention, in the absence of an event detection stage; and (ii) current one-stage methods lose some knowledge learned from shallow layers, resulting in missed video content. To address these challenges, we propose the Perturbed Attention Self-Distillation (PASD) framework, which incorporates two key designs: (i) Perturbed Attention: Our key insight is that the removal of important video content results in a decrease in captioning performance. We design a perturbed branch containing a learnable probe to decrease performance. This learned probe then corrects the original attention, helping the model focus on important video content. (ii) Layer-Wise Self-Distillation: We design a layer-wise self-distillation mechanism that transfers knowledge from shallow layers to deep layers, enabling the model to fully exploit multi-level representations learned by different layers. Our novelty lies in integrating these two mechanisms to enhance video relevance without requiring an explicit event detection stage. Experimental results clearly demonstrate that PASD improves video-caption alignment, outperforming both traditional one-stage and two-stage methods. Additionally, PASD maintains computational efficiency, making it suitable for real-world deployment.},
  archive      = {J_PR},
  author       = {Yiqi Gao and Wei Suo and Mengyang Sun and Le Liu and Peng Wang},
  doi          = {10.1016/j.patcog.2025.111871},
  journal      = {Pattern Recognition},
  month        = {1},
  pages        = {111871},
  shortjournal = {Pattern Recognition},
  title        = {More video-relevant paragraph captioning via perturbed attention self-distillation},
  volume       = {169},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Towards robust sentiment analysis with multimodal interaction graph and hybrid contrastive learning. <em>PR</em>, <em>169</em>, 111870. (<a href='https://doi.org/10.1016/j.patcog.2025.111870'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Building a robust Multimodal Sentiment Analysis (MSA) model is critical for affective understanding in complex real-world scenarios. Recent research has highlighted Transformer as a potent MSA architecture, capable of acquiring multimodal representations directly from unaligned streams. However, two challenges persist in achieving robust MSA: (i) inefficiency in capturing intricate interactions among modalities, and (ii) susceptibility to random modalities missing. This manuscript introduces the Multimodal Interaction Graph with Hybrid Contrastive Learning (MIG-HCL) to adaptively integrate multimodal data in various modality-missing situations. MIG primarily comprises two components: a Bidirectional Progressive Feature Promotion Module (BPFPM) and a Mutual Information Selective Module (MISM), responding to exploit complementary information and mitigate redundancy across modalities. Upon transforming multimodal sequences into unimodal and multimodal graphs, BPFPM reinforces bidirectional interactions between the global multimodal context and local unimodal features for semantic compensation. Leveraging the heterogeneous graph structure, BPFPM enables parallel computation across multiple modalities, thus avoiding inefficient separate sets of bimodal correlation computations of prior local–local interaction methods. MISM utilizes mutual information between global semantics and reinforced unimodal representations, incorporating learnable weights to filter the most task-relevant features for redundancy elimination. Moreover, HCL is presented to learn robust representations from incomplete data. HCL treats complete and incomplete data as distinct views for multi-scale feature reconstruction, spanning cross-scale and same-scale approaches, to restore both contextual and structural information while discouraging premature solutions. Extensive experiments on benchmark datasets demonstrate that MIG-HCL outperforms existing methods under random modalities missing.},
  archive      = {J_PR},
  author       = {Peizhu Gong and Jin Liu and Xiliang Zhang and Xingye Li and Lai Wei and Huihua He},
  doi          = {10.1016/j.patcog.2025.111870},
  journal      = {Pattern Recognition},
  month        = {1},
  pages        = {111870},
  shortjournal = {Pattern Recognition},
  title        = {Towards robust sentiment analysis with multimodal interaction graph and hybrid contrastive learning},
  volume       = {169},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). MPM-net: Multi-task interactive network with progressive multi-granularity learning for herbal medicine recognition. <em>PR</em>, <em>169</em>, 111868. (<a href='https://doi.org/10.1016/j.patcog.2025.111868'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Traditional Chinese medicine plays a crucial role in healthcare, with Chinese herbal medicine (CHM) forming its core. Accurate identification of CHM is essential for ensuring their efficacy and safety, but this process often requires specialized expertise. Despite significant advances in deep learning for image recognition, the field of CHM identification lags behind. Specifically, the challenges in automatic image recognition of CHM stem from the lack of fully public CHM datasets and the inherent visual similarity and variability of CHM data. To address these issues, we collect a CHM-100 dataset, which is the first fully public dataset for CHM image recognition. Compared with existing CHM recognition datasets, CHM-100 has more diverse samples and higher quality annotations, thus establishing a new benchmark for the development of CHM recognition models. Furthermore, we propose a multi-task interactive network with progressive multi-granularity learning (MPM-Net), designed to enhance the accuracy of CHM recognition. MPM-Net employs a progressive learning strategy to capture detailed local features and is jointly trained with a task for identifying medicinal parts to learn global features. Additionally, we use the cross-attention interaction fusion module to explore the relationship between CHM recognition and biology domain knowledge to enhance the feature representation. We evaluate MPM-Net using the CHM-100 dataset and compare its performance with various state-of-the-art image classification methods, including popular deep networks and fine-grained approaches. The results demonstrate that MPM-Net outperforms existing methods by significant margins, achieving a Top-1 accuracy of 90.86% (1.29% absolute improvement over current state-of-the-art) on the CHM-100 dataset.},
  archive      = {J_PR},
  author       = {Kaiwen Yang and Guanqun Ding and Leyuan Sun and Jingzhong Chen and Guangying Du and Quan Zheng and Tao Zhang},
  doi          = {10.1016/j.patcog.2025.111868},
  journal      = {Pattern Recognition},
  month        = {1},
  pages        = {111868},
  shortjournal = {Pattern Recognition},
  title        = {MPM-net: Multi-task interactive network with progressive multi-granularity learning for herbal medicine recognition},
  volume       = {169},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Visual complexity guided diffusion defender for video object tracking and recognition. <em>PR</em>, <em>169</em>, 111867. (<a href='https://doi.org/10.1016/j.patcog.2025.111867'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The rapid development of Deep Neural Networks (DNNs) highlights AI’s potential yet introduces the risk of abuse and attack. We focus on the vulnerability of video-based DNNs to adversarial attacks, striving to develop a robust, pluggable, and efficient adversarial defender. In principle, the diffusion model can be considered as a natural plug-in adversarial defender that purifies the adversarial examples before they are fed into downstream DNNs. However, existing diffusion purifiers fix the denoising granularity, eliminating the malicious signals while tolerating the risk of losing important information. Based on the observation that attack mitigation in complex scenarios is more difficult, an adaptive diffusion timestep selection mechanism guided by visual complexity is proposed, which enables the vigour of defence to be controlled. Extensive experiments with various video classifiers and trackers on multiple benchmarks demonstrate that our method achieves state-of-the-art results, outperforming existing adversarial purifiers by a significant margin in most cases. Codes are available on https://github.com/DjangoChaogh/VCGDD .},
  archive      = {J_PR},
  author       = {Shaochuan Zhao and Tianyang Xu and Hui Li and Xiao-Jun Wu and Josef Kittler},
  doi          = {10.1016/j.patcog.2025.111867},
  journal      = {Pattern Recognition},
  month        = {1},
  pages        = {111867},
  shortjournal = {Pattern Recognition},
  title        = {Visual complexity guided diffusion defender for video object tracking and recognition},
  volume       = {169},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Source-free domain adaptation for unsupervised radar-based human activity recognition. <em>PR</em>, <em>169</em>, 111866. (<a href='https://doi.org/10.1016/j.patcog.2025.111866'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Radar-based human activity recognition(HAR) has been widely investigated recently and deep learning techniques are used to analyze practical data. Among these works, domain adaptation(DA) is proven to be useful in training models with scarce labeled datasets and generalizing the trained model to a new environment. However, general DA requires access to source domain datasets during the adaptation process, which is risky and inefficient for private data. In this work, we propose an unsupervised radar HAR system, “weighted pseudo-label source-free unsupervised domain adaptation”(WPL-SFUDA). WPL-SFUDA achieves unsupervised HAR based on DA. Furthermore, it also utilizes the source-free DA technique to eliminate the requirement of the model to access the source domain dataset during the adaptation process. We also propose a novel weighted pseudo-label training strategy and label correction module to further improve the robustness of the system working in a radar-based application scenario. The effectiveness of WPL-SFUDA is demonstrated via the public radar HAR dataset. Compared with other UDA and SFUDA methods, the proposed system achieves both higher accuracy and stronger robustness in unsupervised radar-based HAR tasks.},
  archive      = {J_PR},
  author       = {Xu Si and Chi Zhang and Siwei Li and Jing Liang},
  doi          = {10.1016/j.patcog.2025.111866},
  journal      = {Pattern Recognition},
  month        = {1},
  pages        = {111866},
  shortjournal = {Pattern Recognition},
  title        = {Source-free domain adaptation for unsupervised radar-based human activity recognition},
  volume       = {169},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). DMF-SIM: Dual-model framework for super-resolution reconstruction and denoising in structured illumination microscopy. <em>PR</em>, <em>169</em>, 111865. (<a href='https://doi.org/10.1016/j.patcog.2025.111865'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Super-Resolution Structured Illumination Microscopy (SR-SIM) is usually challenged by the balance between imaging quality and continuous imaging, especially in low-light imaging conditions, where obtaining a high Signal-to-Noise Ratio (SNR) is even tougher due to the intrinsic mechanism of sensor noise and detection limitations. In more recent studies, a promising solution to overcome this limitation is a combination of traditional methods with deep neural networks. However, the incorporation of extrinsic optical information weakens robustness across different imaging conditions due to complex processing requirements. Additionally, the high computational cost and need for extensive training data limit the overall performance from reaching further improvements. Motivated by the complementary strengths of vision transformers and channel attention mechanisms, in this paper, we propose a Dual-Model Framework in Structured Illumination Microscopy (DMF-SIM) to achieve simultaneous super-resolution reconstruction and denoising by integrating a reconstruction model (SIM-Rec) and a denoising model (SIM-Den). DMF-SIM integrates super-resolution and denoising models by effectively combining vision transformer and channel attention mechanisms, and a substantial performance improvement has been achieved over single-task models. The experiments also demonstrate enhanced image quality and noise reduction across various imaging conditions.},
  archive      = {J_PR},
  author       = {Yue Huo and Zhi Deng and Peng Zhang and Zixuan Lu and Feifan Zhang and Meiqi Li and Yiwei Hou and Peng Xi and Wei Huang and Yanning Zhang},
  doi          = {10.1016/j.patcog.2025.111865},
  journal      = {Pattern Recognition},
  month        = {1},
  pages        = {111865},
  shortjournal = {Pattern Recognition},
  title        = {DMF-SIM: Dual-model framework for super-resolution reconstruction and denoising in structured illumination microscopy},
  volume       = {169},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Hybrid region and population hypergraph neural network for mild cognitive impairment detection. <em>PR</em>, <em>169</em>, 111864. (<a href='https://doi.org/10.1016/j.patcog.2025.111864'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Mild Cognitive Impairment (MCI) detection is paramount in Alzheimer’s disease (AD) prevention. Existing methods have not sufficiently leveraged both subject correlation and brain region topological information into MCI detection. To solve the limitation, this study proposes a hybrid region and population hypergraph neural network for MCI detection. Specifically, we use two-view selection on both signals and brain regions, and an improved metric learning-based module to improve the quality of hypergraphs. Additionally, a population-to-region hypergraph neural network is designed to leverage information from brain region topology and subject correlation for MCI detection. Experimental results show that our proposed method achieves accuracies of 87.98%, 81.05%, 86.42% and 82.95% for MCI detection, early MCI detection, late MCI detection and MCI classification on the AD Neuroimaging Initiative dataset, respectively, outperforming several state-of-the-art methods. Furthermore, experimental results on the Autism Brain Imaging Data Exchange dataset validating its generalization to other brain disorder detection tasks.},
  archive      = {J_PR},
  author       = {Jie Wang and Luohuang Wu and Hulin Kuang and Jianxin Wang},
  doi          = {10.1016/j.patcog.2025.111864},
  journal      = {Pattern Recognition},
  month        = {1},
  pages        = {111864},
  shortjournal = {Pattern Recognition},
  title        = {Hybrid region and population hypergraph neural network for mild cognitive impairment detection},
  volume       = {169},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Nighttime image dehazing via a physics-aware dynamic neural model with progressive contrastive regularization. <em>PR</em>, <em>169</em>, 111863. (<a href='https://doi.org/10.1016/j.patcog.2025.111863'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Nighttime hazy enlivenment is a challenging yet often-seen scenario for computer vision systems, as a nighttime hazy image contains complex degradation that significantly decreases the visibility of the scene. Nighttime image dehazing aims at restoring a haze-less image with high visibility from nighttime hazy images. It remains a challenging task due to the complex interplay of multiple light sources, scattering, and inhomogeneous haze. This paper presents PDRNet, a physics-aware dynamic model that addresses the unique characteristics of nighttime haze and improves dehazing performance in three key aspects. Firstly, to handle spatially varying haze and lighting conditions, dynamic filtering is introduced for content-aware feature extraction. Secondly, a triple-branch nighttime physics-aware module based on the nighttime atmospheric scattering model is designed, leading to an effective physically-guided restoration process. Lastly, a progressive contrastive regularization scheme, which introduces negative samples of increasing dehazing difficulty from the nighttime, is proposed for providing robust lower-bound constraints on the clean image solution space. The integration of these three components results in a lightweight, physics-aware dynamic model with excellent generalization capabilities. Extensive experiments on both synthetic and authentic datasets demonstrate the state-of-the-art performance of proposed PDRNet for nighttime image dehazing.},
  archive      = {J_PR},
  author       = {Yun Liang and Xinjie Xiao and Zihan Zhou and Lianghui Li and Yuhui Quan},
  doi          = {10.1016/j.patcog.2025.111863},
  journal      = {Pattern Recognition},
  month        = {1},
  pages        = {111863},
  shortjournal = {Pattern Recognition},
  title        = {Nighttime image dehazing via a physics-aware dynamic neural model with progressive contrastive regularization},
  volume       = {169},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Desensitizing for improving corruption robustness in point cloud classification through adversarial training. <em>PR</em>, <em>169</em>, 111862. (<a href='https://doi.org/10.1016/j.patcog.2025.111862'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Due to scene complexity, sensor inaccuracies, and processing imprecision, point cloud corruption is inevitable. Over-reliance on input features is the root cause of DNN vulnerabilities. It remains unclear whether this issue exists in 3D tasks involving point clouds and whether reducing dependence on these features can enhance the model’s robustness to corrupted point clouds. This study attempts to answer these questions. Specifically, we quantified the sensitivity of the DNN to point cloud features using Shapley values and found that models trained using traditional methods exhibited high sensitivity values for certain features. Furthermore, under an equal pruning ratio, prioritizing the pruning of highly sensitive features causes more severe damage to model performance than random pruning. We propose ‘Desensitized Adversarial Training’ (DesenAT), generating adversarial samples using feature desensitization and conducting training within a self-distillation framework, which aims to alleviate DNN’s over-reliance on point clouds features by smoothing sensitivity. First, data points with high contribution components are eliminated, and spatial transformation is used to simulate corruption scenes, generate adversarial samples, and conduct adversarial training on the model. Next, to compensate for information loss in adversarial samples, we use the self-distillation method to transfer knowledge from clean samples to adversarial samples, and perform adversarial training in a distillation manner. Extensive experiments on ModelNet-C and PointCloud-C demonstrate show that the propose method can effectively improve the robustness of the model without reducing the performance of clean data sets. This code is publicly available at https://github.com/JerkyT/DesenAT .},
  archive      = {J_PR},
  author       = {Zhiqiang Tian and Weigang Li and Chunhua Deng and Junwei Hu and Yongqiang Wang and Wenping Liu},
  doi          = {10.1016/j.patcog.2025.111862},
  journal      = {Pattern Recognition},
  month        = {1},
  pages        = {111862},
  shortjournal = {Pattern Recognition},
  title        = {Desensitizing for improving corruption robustness in point cloud classification through adversarial training},
  volume       = {169},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Twin proximal support vector regression with Gauss–Laplace mixed noise. <em>PR</em>, <em>169</em>, 111860. (<a href='https://doi.org/10.1016/j.patcog.2025.111860'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Both the classic proximal support vector regression (PSVR) and the twin support vector regression (TSVR) assume that the data is affected by a single noise characteristic distribution. However, in some low-quality data, such as wind power and photovoltaic power, it is found that the data does not obey a single noise distribution, but tends to follow a Gauss–Laplace mixed noise distribution. Therefore, This study aims to enhance regression models to improve predictive accuracy over traditional models. Based on the theoretical basis of the PSVR model combined with the TSVR model, we introduce the Gauss–Laplace mixed noise feature, and design a new regressor, called the twin proximal support vector regression with Gauss–Laplace mixed noise (GL-TPSVR). At the same time, the augmented Lagrange multiplier (ALM) method is used to solve the GL-TPSVR model. Experimental results show that the accuracy of the GL-TPSVR model is improved by about 32% and 25% respectively compared with the recent advanced regression models on the artificial dataset and the real short-term wind power dataset, verifying the effectiveness and feasibility of the proposed GL-TPSVR model. The GL-TPSVR model is proposed in this paper, which combines the strengths of PSVR and TSVR with the Gauss–Laplace mixed noise distribution, offering a significant improvement in accuracy for datasets with complex noise distributions.},
  archive      = {J_PR},
  author       = {Chao Liu and Quan Qian},
  doi          = {10.1016/j.patcog.2025.111860},
  journal      = {Pattern Recognition},
  month        = {1},
  pages        = {111860},
  shortjournal = {Pattern Recognition},
  title        = {Twin proximal support vector regression with Gauss–Laplace mixed noise},
  volume       = {169},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). HFA2RE: Enhancing adversarial robustness via hyperspherical feature aggregation. <em>PR</em>, <em>169</em>, 111857. (<a href='https://doi.org/10.1016/j.patcog.2025.111857'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Self-supervised adversarial training has gained growing interest for its dual pursuit of discriminative capability and adversarial robustness. Although single-stage methods jointly optimize these objectives by integrating self-supervised learning (SSL) and adversarial learning (AT), they suffer from intractable optimization problems. To address this, existing two-stage methods decouple training by guiding AT with SSL representations. However, their robustness remains constrained by the inherent vulnerability of SSL, i.e., the uniform dispersion of its features on the hypersphere. To overcome this limitation, we propose a novel method, named Hyperspherical Feature Aggregation for Adversarial Robustness Enhancement (HF A 2 RE). It generates class-specific prototypes in an unsupervised manner and employs them as anchors to aggregate hyperspherical features, thereby enlarging margins between features and decision boundaries for enhanced adversarial robustness. Furthermore, we propose Spherical Aggregation Coefficient, the first interpretable metric which evaluates robustness via hyperspherical feature compactness without requiring adversarial examples. Extensive experiments demonstrate that our method obtains superior robustness.},
  archive      = {J_PR},
  author       = {Heqi Peng and Mingxuan Chen and Yunhong Wang and Yuanfang Guo},
  doi          = {10.1016/j.patcog.2025.111857},
  journal      = {Pattern Recognition},
  month        = {1},
  pages        = {111857},
  shortjournal = {Pattern Recognition},
  title        = {HFA2RE: Enhancing adversarial robustness via hyperspherical feature aggregation},
  volume       = {169},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Detecting and explaining structural changes in an evolving graph using a martingale. <em>PR</em>, <em>169</em>, 111855. (<a href='https://doi.org/10.1016/j.patcog.2025.111855'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Dynamic systems such as sensor networks, social networks, computer networks, and power grids can be modeled as evolving graphs, requiring effective methods to monitor structural changes in real-time. In this paper, we present a change detection framework for detecting global structural changes in evolving graphs. This framework combines multiple martingale-based change detectors using different graph features. We establish a mathematical relationship between an additive martingale (derived from the multiple graph features) and the Shapley Additive Explanations (SHAP) method, demonstrating that martingale values at detected change-points directly quantify each feature’s contribution to the detected change. Thus, graph features with significantly high martingale values provide meaningful explanations for detected structural changes. We demonstrate our approach using three synthetic graph types (random topology, scale-free, and small-world), showing how different graph features vary in detection performance across network types. This highlights the importance of using multiple graph features, as relying on any single feature might not detect certain changes. Additionally, we apply our approach to the MIT Reality dataset to identify changes in social interactions among dormitory students at some special time periods. Our results reveal which graph features best characterize the real-world network changes during these special time periods.},
  archive      = {J_PR},
  author       = {Shen-Shyang Ho and Tarun Teja Kairamkonda and Izhar Ali},
  doi          = {10.1016/j.patcog.2025.111855},
  journal      = {Pattern Recognition},
  month        = {1},
  pages        = {111855},
  shortjournal = {Pattern Recognition},
  title        = {Detecting and explaining structural changes in an evolving graph using a martingale},
  volume       = {169},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Locality-driven flexible consensus graph learning for multi-view clustering. <em>PR</em>, <em>169</em>, 111853. (<a href='https://doi.org/10.1016/j.patcog.2025.111853'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Most graph-based Multi-View Clustering (MVC) learns the consensus graph with raw features, which encounters a performance bottleneck due to the limited representation capacity and noise of the original data. Recently, Graph Convolutional Network (GCN) based graph learning has presented enormous potential to break this bottleneck, while the relevant research (1) neglects to distinguish the significance of samples during graph fusion; (2) requires the learned embeddings to have the same dimension strictly to conduct graph contrastive learning. These issues impede improving graph-based MVC with the GCN-based graph learning technique. In this paper, we propose Locality-driven Flexible Consensus Graph Learning (LFCGL) that designs graph-based MVC into the deep framework. Concretely, a multi-view graph auto-encoder is devised as the backbone to learn the graph embedding for structural learning, so as to reveal the intrinsic node dependence within each view, and suppress the adverse impacts of noise in the raw data. Thereinto, the new sample-wise fusion learns the significance of edges within each view, and produces a consensus graph to achieve cross-view structure consistency. Besides, the internal cluster validity index is introduced to guide multi-view embedding learning with a flexible embedding dimension setting for each view, which is ideally equivalent to graph contrastive learning, and prevents clustering-irrelevant noise features from corrupting the learned embedding. Theoretical analysis and extensive experiments demonstrate the superiority of LFCGL over state-of-the-art competitors.},
  archive      = {J_PR},
  author       = {Bocheng Wang and Jiawei Wang and Chusheng Zeng and Mulin Chen},
  doi          = {10.1016/j.patcog.2025.111853},
  journal      = {Pattern Recognition},
  month        = {1},
  pages        = {111853},
  shortjournal = {Pattern Recognition},
  title        = {Locality-driven flexible consensus graph learning for multi-view clustering},
  volume       = {169},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Two-stage knowledge distillation for visible-infrared person re-identification. <em>PR</em>, <em>169</em>, 111850. (<a href='https://doi.org/10.1016/j.patcog.2025.111850'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Visible-infrared person re-identification (VI-ReID) is an important retrieval task that has recently sparked interest due to the requirements for continuous 24-hour surveillance. VI-ReID aims to retrieve specific visible or infrared person images in one modality based on a query from the other modality. Visible and infrared images have different spectra, leading to huge modality gap that is major challenge for VI-ReID. Recent methods reduce the gap, but they ignore intra-modality discrepancy. Besides, these methods require well-annotated cross-modality data, but gathering such data is time-consuming and labor-intensive. In this paper, we propose a novel Two-Stage Knowledge Distillation method (TSKD) for VI-ReID, which adopts a simple-to-difficult strategy for cross-modality feature alignment and explores a way to reduce annotation costs by using only a small number of labeled data. TSKD consists of three novel components: soft-identity learning (SI), self-mimic learning (SM), and mutual-distillation learning (MD). SI first generates pseudo-labels with confidence for unlabeled data, thereby decreasing the annotation cost. After that, SM learns the prototype for each person in special modality and minimizes the intra-modality discrepancy. Finally, MD performs mutual distillation for cross-modality feature alignment in the set-level measurement rather than the instance measurement for each person. Importantly, we demonstrate that TSKD achieves stronger robustness under weak supervision. Our experimental results on two VI-ReID benchmarks demonstrate the effectiveness of TSKD under both full-supervision and weak-supervision settings. The code is released at https://github.com/shijiangming1/TSKD .},
  archive      = {J_PR},
  author       = {Jiangming Shi and Xiangbo Yin and Demao Zhang and Zhizhong Zhang and Yuan Xie and Yanyun Qu},
  doi          = {10.1016/j.patcog.2025.111850},
  journal      = {Pattern Recognition},
  month        = {1},
  pages        = {111850},
  shortjournal = {Pattern Recognition},
  title        = {Two-stage knowledge distillation for visible-infrared person re-identification},
  volume       = {169},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Perceptual image compression with textual side information. <em>PR</em>, <em>169</em>, 111848. (<a href='https://doi.org/10.1016/j.patcog.2025.111848'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The explosion of data has resulted in more and more associated text being transmitted along with images. Inspired by from distributed source coding, many works utilize image side information to enhance image compression. However, existing methods generally do not consider using text as side information to enhance perceptual compression of images, even though the benefits of multimodal synergy have been widely demonstrated in research. This begs the following question: How can we effectively transfer text-level semantic dependencies to help image compression, which is only available to the decoder? In this work, we propose a novel deep image compression method with text-guided side information to achieve a better rate–perception–distortion tradeoff. Specifically, we employ the CLIP text encoder and an effective Semantic-Spatial Aware block to fuse the text and image features. This is done by predicting a semantic mask to guide the learned text-adaptive affine transformation at the pixel level. Furthermore, we design a text-conditional generative adversarial networks to improve the perceptual quality of reconstructed images. Extensive experiments involving four datasets and ten image quality assessment metrics demonstrate that the proposed approach achieves superior results in terms of rate–perception tradeoff and semantic distortion.},
  archive      = {J_PR},
  author       = {Shi-Yu Qin and Bin Chen and Yu-Jun Huang and Bao-Yi An and Tao Dai and Shu-Tao Xia},
  doi          = {10.1016/j.patcog.2025.111848},
  journal      = {Pattern Recognition},
  month        = {1},
  pages        = {111848},
  shortjournal = {Pattern Recognition},
  title        = {Perceptual image compression with textual side information},
  volume       = {169},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Query-enhanced motion transformer with dilated static query and bridged dynamic query. <em>PR</em>, <em>169</em>, 111847. (<a href='https://doi.org/10.1016/j.patcog.2025.111847'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Motion forecasting is integral to autonomous driving, as it aims to anticipate future trajectory of traffic agents within the complicated environment by comprehending their various motion intentions. Recent effective query-based methods have focused on designing static queries and learning dynamic queries through cascaded Transformer decoders to represent motion multimodality. However, static queries are usually generated by clustering the goal of motion samples, while they suffer from out-of-distribution limitation and thus deviate from the boundary samples, leading to a suboptimal prediction. Additionally, dynamic queries, affected by the sequential connections between decoder layers, lack communication between distant decoders, leading to inconsistent prediction. In this paper, we propose a simple yet effective Query-Enhanced Motion TRansformer (QE-MTR) for motion forecasting. This approach includes both Dilated Static Query (DSQ) and Bridged Dynamic Query (BDQ) to enhance the representation of motion multimodality. Specifically, the DSQ expands the valid range of motion samples from their future sequence to complete sequence, providing more diverse motion patterns for query generation and effectively dilating the coverage of static queries. The BDQ introduces cross connection across decoders instead of direct connections, promoting harmonious dynamic optimization and improving the effectiveness of final dynamic queries. QE-MTR achieves state-of-the-art performance on the Waymo Open Motion Dataset, earning 2nd place finish in the 2023 Waymo Motion Prediction Challenge.},
  archive      = {J_PR},
  author       = {Miao Kang and Liushuai Shi and Ke Ye and Sanping Zhou and Nanning Zheng},
  doi          = {10.1016/j.patcog.2025.111847},
  journal      = {Pattern Recognition},
  month        = {1},
  pages        = {111847},
  shortjournal = {Pattern Recognition},
  title        = {Query-enhanced motion transformer with dilated static query and bridged dynamic query},
  volume       = {169},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Feature matters: Revisiting channel attention for temporal action detection. <em>PR</em>, <em>169</em>, 111846. (<a href='https://doi.org/10.1016/j.patcog.2025.111846'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Temporal Action Detection (TAD) aims to locate the temporal boundaries and recognize categories of actions in videos. Existing single-stage methods, while emphasizing temporal interactions, often neglect the treatment of local features. Given the increasingly complex and overlapping actions in new datasets, relying solely on temporal interactions is insufficient. In this paper, we revisit the channel attention in TAD and propose a novel single-stage convolutional model called Hi ghlighted F eature I nteraction (HiFi). HiFi introduces a fundamental unit called the Expanded Excitation & Interaction Block (EEI), which evaluates the importance of each channel and scales the channels to suppress irrelevant and emphasize valuable channels. Consequently, HiFi can learn effective temporal representations at multi-scales through convolution and pooling operations. Extensive experiments on THUMOS-14, ActivityNet v1.3, HACS Segment, FineAction, Epic-Kitchen, and Ego4D-Moment Queries demonstrate HiFi outperforms the state-of-the-art methods and confirm the efficacy of weak temporal interactions and channel enhancement in TAD.},
  archive      = {J_PR},
  author       = {Guo Chen and Yin-Dong Zheng and Wei Zhu and Jiahao Wang and Tong Lu},
  doi          = {10.1016/j.patcog.2025.111846},
  journal      = {Pattern Recognition},
  month        = {1},
  pages        = {111846},
  shortjournal = {Pattern Recognition},
  title        = {Feature matters: Revisiting channel attention for temporal action detection},
  volume       = {169},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). MGRAG: MultiModal grid-aware retrieval augmentation generation framework for power grid work tickets. <em>PR</em>, <em>169</em>, 111845. (<a href='https://doi.org/10.1016/j.patcog.2025.111845'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In power grid operations, traditional manual methods for issuing work tickets, essential for guiding electrical equipment inspection, repair, and construction, are inefficient, error-prone, and highly dependent on personnel expertise. To automate this process, Large Language Models (LLMs) and Retrieval-Augmented Generation (RAG) techniques have been explored, but they struggle with cross-data type reasoning, limited document quality, and the need for dynamic adaptation. To overcome these challenges, we introduce the MultiModal Grid-Aware Retrieval Augmentation Generation Framework (MGRAG), which emulates the retrieval process of manual work ticket filling using multi-modal data including text, images, and graph topologies. MGRAG employs Text-Graph Mapping for subgraph extraction and retrieval from wiring diagrams, along with a CLIP-based Visual Adapter with Cache Model for equipment image matching. Experiments on the MultiModal Work Ticket (MMWT) dataset, encompassing two years of data from three large-scale substations with diverse grid structures, reveals that MGRAG significantly outperforms existing RAG and LLM baselines. Importantly, MGRAG not only solves the multi-modal retrieval problem for work tickets but also provides a versatile and industrially applicable framework for multi-modal data retrieval and integration.},
  archive      = {J_PR},
  author       = {Jiangbing Mao and Chenfeng Zheng and Wenliang Liu and Zhihong Zhang},
  doi          = {10.1016/j.patcog.2025.111845},
  journal      = {Pattern Recognition},
  month        = {1},
  pages        = {111845},
  shortjournal = {Pattern Recognition},
  title        = {MGRAG: MultiModal grid-aware retrieval augmentation generation framework for power grid work tickets},
  volume       = {169},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Multi-scale feature fusion for breast tumor grading using dual-adaptive attention mechanisms. <em>PR</em>, <em>169</em>, 111838. (<a href='https://doi.org/10.1016/j.patcog.2025.111838'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Breast cancer is the most common malignant tumor in women worldwide. Early detection and timely treatment are crucial for improving survival rates and ensuring effective treatment planning. While existing methods primarily focus on classifying benign versus malignant tumors, accurate tumor grading remains underexplored. The scarcity of high-quality annotated datasets for advanced tumor grades, combined with their inherent similarity, limits model robustness and precise grading. To address these challenges, we propose a novel framework for breast tumor grading diagnosis that leverages multi-scale feature fusion with dual-adaptive attention mechanisms. This framework enhances grading performance by capturing both local and global information. It integrates information across multiple scales, improves feature representation, and enables the detection of subtle differences between grades. The framework consists of three primary modules: deep hybrid feature extraction, multi-scale feature fusion, and dual-adaptive attention mechanisms. Together, these modules enhance feature representation, capture salient information, and highlight discriminative features, ultimately improving grading accuracy. In addition, we constructed two distinct image datasets consisting of 17,058 ultrasound images and 1571 MRI images for breast tumor grading. Experimental results demonstrated the effectiveness of our framework, achieving a test accuracy of 0.89, a macro-average F1-score of 0.82, and a micro-average precision of 0.93, outperforming existing state-of-the-art methods.},
  archive      = {J_PR},
  author       = {Israr Hussain and Zhihui Lai and Heng Kong and Fazil Hussain},
  doi          = {10.1016/j.patcog.2025.111838},
  journal      = {Pattern Recognition},
  month        = {1},
  pages        = {111838},
  shortjournal = {Pattern Recognition},
  title        = {Multi-scale feature fusion for breast tumor grading using dual-adaptive attention mechanisms},
  volume       = {169},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Fast distance-enhanced graph convolutional network for skeleton-based action recognition. <em>PR</em>, <em>169</em>, 111827. (<a href='https://doi.org/10.1016/j.patcog.2025.111827'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Exploring how to use graph structures more effectively for extracting and expressing skeleton information has become a significant area of skeleton-based human action recognition. Existing research mostly focuses on adjacent or local skeletal points, which might limit the receptive field (sampling range of skeleton point influence) of graph convolutional networks. In this paper, we introduce a fast distance-enhanced graph convolutional network (FD-GCN) by expanding the impact of remote skeleton joints for relative skeletons’ topological graph. FD-GCN expresses the skeleton information of the topological graph better by reasonably expanding the receptive field. In addition, skeleton-based action recognition becomes increasingly redundant after adding various modules and runs very slowly. Therefore, FD-GCN introduces a temporal convolution structure that can assign position information and weights to time frames to solve this problem. FD-GCN achieves excellent results on NTU RGB+D, NTU RGB+D 120 and NW-UCLA datasets. FD-GCN matches the SOTA performance on the X-Sub benchmark of NTU120, achieving 89.4% accuracy. In addition, FD-GCN achieves 97.0% accuracy on the NW-UCLA dataset, outperforming the previous SOTA by 0.2% in a 4-stream setting. More importantly, the inference time of FD-GCN is about 1.6 ms, while the inference time of the SOTA methods are mostly more than 10 ms.},
  archive      = {J_PR},
  author       = {Jinze Huo and Haibin Cai and Qinggang Meng},
  doi          = {10.1016/j.patcog.2025.111827},
  journal      = {Pattern Recognition},
  month        = {1},
  pages        = {111827},
  shortjournal = {Pattern Recognition},
  title        = {Fast distance-enhanced graph convolutional network for skeleton-based action recognition},
  volume       = {169},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). A comprehensive survey of oracle character recognition: Challenges, datasets, methodology, and beyond. <em>PR</em>, <em>169</em>, 111824. (<a href='https://doi.org/10.1016/j.patcog.2025.111824'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Oracle character recognition — an analysis of ancient Chinese inscriptions found on oracle bones — has become a pivotal field intersecting archaeology, paleography, and historical cultural studies. Traditional methods of oracle character recognition have relied heavily on manual interpretation by experts, which is not only labor-intensive but also limits broader accessibility to the general public. With recent breakthroughs in pattern recognition and deep learning, there is a growing movement toward the automation of oracle character recognition (OrCR), showing considerable promise in tackling the challenges inherent to these ancient scripts. However, a comprehensive understanding of OrCR still remains elusive. Therefore, this paper presents a systematic and structured survey of the current landscape of OrCR research. We commence by identifying and analyzing the key challenges of OrCR. Then, we provide an overview of the primary benchmark datasets and digital resources available for OrCR. A review of contemporary research methodologies follows, in which their respective efficacies, limitations, and applicability to the complex nature of oracle characters are critically highlighted and examined. Additionally, our review extends to ancillary tasks associated with OrCR across diverse disciplines, providing a broad-spectrum analysis of its applications. We conclude with a forward-looking perspective, proposing potential avenues for future investigations that could yield significant advancements in the field.},
  archive      = {J_PR},
  author       = {Jing Li and Xueke Chi and Qiufeng Wang and Kaizhu Huang and Da-Han Wang and Yongge Liu and Cheng-Lin Liu},
  doi          = {10.1016/j.patcog.2025.111824},
  journal      = {Pattern Recognition},
  month        = {1},
  pages        = {111824},
  shortjournal = {Pattern Recognition},
  title        = {A comprehensive survey of oracle character recognition: Challenges, datasets, methodology, and beyond},
  volume       = {169},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Boosting endomicroscopy image recognition with contrastive learning and bag-of-words. <em>PR</em>, <em>169</em>, 111823. (<a href='https://doi.org/10.1016/j.patcog.2025.111823'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Probe-based confocal laser endomicroscopy (pCLE) is an effective method for early gastrointestinal cancer detection, facing challenges like high annotation costs and inefficient automatic classification. In this paper, we proposed a novel multi-task learning algorithm for endomicroscopy image classification, CB-MTL, consisting of two tasks to learn more robust visual representations from unlabeled data through self-supervised learning. The main task is a contrastive learning task for learning transformation-invariant representations through the forceful alignment of a pair of augmented images of the same sample. The auxiliary task involves a bag-of-words (BoW) objective to generate BoW vectors for each image that captures structural and content-related information from diverse local similarity regions, guiding the main task toward acquiring more effective representations and enhancing generalization. Experimental results demonstrate the effectiveness of CB-MTL across both CNN and Transformer architectures and show that BoW significantly improves the contrastive learning task on both CLE image datasets without requiring additional annotations.},
  archive      = {J_PR},
  author       = {Jingjun Zhou and Gang Zheng and Qian Liu},
  doi          = {10.1016/j.patcog.2025.111823},
  journal      = {Pattern Recognition},
  month        = {1},
  pages        = {111823},
  shortjournal = {Pattern Recognition},
  title        = {Boosting endomicroscopy image recognition with contrastive learning and bag-of-words},
  volume       = {169},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). UniFormaly: Towards task-agnostic unified framework for visual anomaly detection. <em>PR</em>, <em>169</em>, 111820. (<a href='https://doi.org/10.1016/j.patcog.2025.111820'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Visual anomaly detection aims to learn normality from normal images, but existing approaches are fragmented across various tasks: defect detection, semantic anomaly detection, multi-class anomaly detection, and anomaly clustering. This one-task-one-model approach is resource-intensive and incurs high maintenance costs as the number of tasks increases. We present UniFormaly , a universal and powerful anomaly detection framework. We emphasize the necessity of our off-the-shelf approach by pointing out a suboptimal issue in online encoder-based methods. We introduce Back Patch Masking (BPM) and top k -ratio feature matching to achieve unified anomaly detection. BPM eliminates irrelevant background regions using a self-attention map from self-supervised ViTs. This operates in a task-agnostic manner and alleviates memory storage consumption, scaling to tasks with large-scale datasets. Top k -ratio feature matching unifies anomaly levels and tasks by casting anomaly scoring into multiple instance learning. Finally, UniFormaly achieves outstanding results on various tasks and datasets. Codes are available at https://github.com/YoojLee/Uniformaly .},
  archive      = {J_PR},
  author       = {Yujin Lee and Harin Lim and Seoyoon Jang and Hyunsoo Yoon},
  doi          = {10.1016/j.patcog.2025.111820},
  journal      = {Pattern Recognition},
  month        = {1},
  pages        = {111820},
  shortjournal = {Pattern Recognition},
  title        = {UniFormaly: Towards task-agnostic unified framework for visual anomaly detection},
  volume       = {169},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Nonconvex tensor multiview subspace clustering with bipartite graph regularization. <em>PR</em>, <em>169</em>, 111818. (<a href='https://doi.org/10.1016/j.patcog.2025.111818'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we present a nonconvex tensor method with bipartite graph regularization for multiview subspace clustering. Departing from conventional approaches, we employ anchor points to learn coefficient matrices, significantly reducing computational time. Using these matrices, we construct bipartite graphs to capture spatial structural correlations and assemble a tensor, applying a weighted tensor Schatten– p norm 0 < p ≤ 1 to extract high-order information from multi-view data. We integrate self-representation and clustering indicators into a joint optimization framework, enabling efficient cluster identification. An augmented Lagrange multiplier (ALM) algorithm solves the model, with convergence to a Karush–Kuhn–Tucker (KKT) point. Extensive experiments on seven diverse real-world datasets confirm our method’s superiority over state-of-the-art techniques, delivering enhanced clustering accuracy and computational efficiency.},
  archive      = {J_PR},
  author       = {Min Li and Xue Yao and Mingqing Xiao and Weiwei Wang},
  doi          = {10.1016/j.patcog.2025.111818},
  journal      = {Pattern Recognition},
  month        = {1},
  pages        = {111818},
  shortjournal = {Pattern Recognition},
  title        = {Nonconvex tensor multiview subspace clustering with bipartite graph regularization},
  volume       = {169},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). PID: Physics-informed diffusion model for infrared image generation. <em>PR</em>, <em>169</em>, 111816. (<a href='https://doi.org/10.1016/j.patcog.2025.111816'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Infrared imaging technology has gained notable attention for its reliable sensing ability in low visibility conditions, prompting numerous studies to convert the abundant RGB images to infrared images. However, most existing image translation methods treat infrared images as a stylistic variation, neglecting the fundamental principles of physics, which limits their practical application. To address these issues, we propose a P hysics- I nformed D iffusion (PID) model for translating RGB images to infrared images that adhere to physical laws. Our method leverages the iterative optimization of the diffusion model and incorporates strong physical constraints based on prior knowledge of infrared laws during training. This approach enhances the similarity between translated infrared images and the real infrared domain without increasing extra inference parameters. Experimental results demonstrate that PID significantly outperforms existing state-of-the-art methods. Our code is available at https://github.com/fangyuanmao/PID .},
  archive      = {J_PR},
  author       = {Fangyuan Mao and Jilin Mei and Shun Lu and Fuyang Liu and Liang Chen and Fangzhou Zhao and Yu Hu},
  doi          = {10.1016/j.patcog.2025.111816},
  journal      = {Pattern Recognition},
  month        = {1},
  pages        = {111816},
  shortjournal = {Pattern Recognition},
  title        = {PID: Physics-informed diffusion model for infrared image generation},
  volume       = {169},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Open-world weakly-supervised object localization. <em>PR</em>, <em>169</em>, 111808. (<a href='https://doi.org/10.1016/j.patcog.2025.111808'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {While remarkable success has been achieved in weakly supervised object localization (WSOL), current frameworks are not capable of locating objects of novel categories in open-world settings. To address this issue, we are the first to introduce a new weakly supervised object localization task called Open-world Weakly Supervised Object Localization ( OWSOL ). During training, all labeled data comes from known categories and, both known and novel categories exist in the unlabeled data. To handle such data, we propose a novel paradigm of contrastive representation co-learning using both labeled and unlabeled data to perform Generalized Class Activation Mapping ( GCAM ) for object localization, without the requirement of bounding box annotation. As no class label is available for the unlabeled data, we conduct clustering over the full training set and design a novel multiple semantic centroids-driven contrastive loss for representation learning. We re-organize two widely used datasets, i.e., ImageNet-1K and iNatLoc500, and propose OpenImages150 to serve as evaluation benchmarks for OWSOL. Extensive experiments demonstrate that the proposed method can surpass all baselines by a large margin. We believe that this work can shift the close-set localization towards the open-world setting and serve as a foundation for subsequent works. The code and dataset are available at https://github.com/ryylcc/OWSOL .},
  archive      = {J_PR},
  author       = {Jinheng Xie and Zhaochuan Luo and Rouyi Li and Yawen Huang and Haozhe Liu and Yuexiang Li and Yefeng Zheng and Yang Zhang and Linlin Shen and Mike Zheng Shou},
  doi          = {10.1016/j.patcog.2025.111808},
  journal      = {Pattern Recognition},
  month        = {1},
  pages        = {111808},
  shortjournal = {Pattern Recognition},
  title        = {Open-world weakly-supervised object localization},
  volume       = {169},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Graph-empowered text-to-SQL generation on electronic medical records. <em>PR</em>, <em>169</em>, 111800. (<a href='https://doi.org/10.1016/j.patcog.2025.111800'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Electronic Medical Records (EMR) store comprehensive patient data, offering immense potential for improving medical decision-making, education, and research. However, extracting actionable insights from EMR remains challenging due to its complex, structured nature, which requires expertise in Structured Query Language (SQL). To address this, we propose leveraging Large Language Models (LLMs) combined with graph-empowered techniques to enhance Text-to-SQL capabilities in EMR systems. LLMs provide the ability to understand natural language queries, making data access more intuitive for non-technical users, while graph representations enable the modeling of intricate relationships between medical entities. This integration not only simplifies data querying but also captures the contextual and relational nuances inherent in EMR data. In this paper, we proposed a Text-to-SQL method based on graph structure injection, which aims to bridge the gap between medical professionals and complex data systems, promoting more efficient and accessible use of EMR for clinical and research applications. To demonstrate the effectiveness of our proposed method, we conduct extensive experiments on the MIMICSQL dataset. The experimental results show that our proposed method achieves 0.942 execution accuracy, reaching a new SOTA.},
  archive      = {J_PR},
  author       = {Qian Chen and Jun Peng and Baiyang Song and Yiyi Zhou and Rongrong Ji},
  doi          = {10.1016/j.patcog.2025.111800},
  journal      = {Pattern Recognition},
  month        = {1},
  pages        = {111800},
  shortjournal = {Pattern Recognition},
  title        = {Graph-empowered text-to-SQL generation on electronic medical records},
  volume       = {169},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). SAEN-BGS: Energy-efficient spiking autoencoder network for background subtraction. <em>PR</em>, <em>169</em>, 111792. (<a href='https://doi.org/10.1016/j.patcog.2025.111792'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Background subtraction (BGS) is utilized to detect moving objects in a video and is commonly employed at the onset of object tracking and human recognition processes. Nevertheless, existing BGS techniques utilizing deep learning still encounter challenges with various background noises in videos, including variations in lighting, shifts in camera angles, and disturbances like air turbulence or swaying trees. To address this problem, we design a spiking autoencoder network, termed SAEN-BGS, based on noise resilience and time-sequence sensitivity of spiking neural networks (SNNs) to enhance the separation of foreground and background. To eliminate unnecessary background noise and preserve the important foreground elements, we begin by creating the continuous spiking conv-and-dconv block, which serves as the fundamental building block for the decoder in SAEN-BGS. Moreover, in striving for enhanced energy efficiency, we introduce a novel self-distillation spiking supervised learning method grounded in ANN-to-SNN frameworks, resulting in decreased power consumption. In extensive experiments conducted on CDnet-2014 and DAVIS-2016 datasets, our approach demonstrates superior segmentation performance relative to other baseline methods, even when challenged by complex scenarios with dynamic backgrounds.},
  archive      = {J_PR},
  author       = {Zhixuan Zhang and Xiao Peng Li and Qi Liu},
  doi          = {10.1016/j.patcog.2025.111792},
  journal      = {Pattern Recognition},
  month        = {1},
  pages        = {111792},
  shortjournal = {Pattern Recognition},
  title        = {SAEN-BGS: Energy-efficient spiking autoencoder network for background subtraction},
  volume       = {169},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Robust object detection in adverse weather with feature decorrelation via independence learning. <em>PR</em>, <em>169</em>, 111790. (<a href='https://doi.org/10.1016/j.patcog.2025.111790'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Object detection algorithms often experience a decline in recognition accuracy under adverse weather due to the interference caused by haze, rain streaks, and snow. To address this issue, conventional strategies typically utilize image restoration techniques to mitigate the occlusion and distortion of objects resulting from these atmospheric phenomena. However, such methods suffer from spurious correlations arising from the interaction between weather information and instance-level and image-level features. This study aims to decouple these spurious correlations at both levels. At the instance-level, we propose an independence learning module (ILM), which is based on Random Fourier Features and Hilbert–Schmidt Independence Criterion (HSIC). This module achieves decoupling by assigning weights to region proposal features. At the image-level, we design a domain-aware module (DAM). DAM distinguishes weather-specific information and enables efficient separation from image-level features through domain-aware classification loss. Experimental evaluations conducted on diverse benchmark datasets showcase promising performance and demonstrate the effectiveness of our proposed object detector.},
  archive      = {J_PR},
  author       = {Yutao Zhang and Shiyu Xuan and Zechao Li},
  doi          = {10.1016/j.patcog.2025.111790},
  journal      = {Pattern Recognition},
  month        = {1},
  pages        = {111790},
  shortjournal = {Pattern Recognition},
  title        = {Robust object detection in adverse weather with feature decorrelation via independence learning},
  volume       = {169},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Prototype-guided text-based person search on rich chinese descriptions. <em>PR</em>, <em>169</em>, 111781. (<a href='https://doi.org/10.1016/j.patcog.2025.111781'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Text-based person search aims to simultaneously localize and identify the target person based on query text from uncropped scene images, which can be regarded as the unified task of person detection and text-based person retrieval task. In this work, we propose a large-scale benchmark dataset named PRW-TPS-CN 1 based on the widely used person search dataset PRW. Our dataset contains 47,102 sentences, which means there is quite more information than existing dataset. These texts precisely describe the person images from top to bottom, which in line with the natural description order. We also provide both Chinese and English descriptions in our dataset for more comprehensive evaluation. These characteristics make our dataset more applicable. To alleviate the inconsistency between person detection and text-based person retrieval, we take advantage of the rich texts in PRW-TPS-CN dataset. We propose to aggregate multiple texts as text prototypes to maintain the prominent text features of a person, which can better reflect the whole character of a person. The overall prototypes lead to generating the image attention map to eliminate the detection misalignment causing the decrease of text-based person retrieval. Thus, the inconsistency between person detection and text-based person retrieval is largely alleviated. We conduct extensive experiments on the PRW-TPS-CN dataset. The experimental results show the PRW-TPS-CN dataset’s effectiveness and the state-of-the-art performance of our approach.},
  archive      = {J_PR},
  author       = {Ziqiang Wu and Bingpeng Ma},
  doi          = {10.1016/j.patcog.2025.111781},
  journal      = {Pattern Recognition},
  month        = {1},
  pages        = {111781},
  shortjournal = {Pattern Recognition},
  title        = {Prototype-guided text-based person search on rich chinese descriptions},
  volume       = {169},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). DLIENet: A lightweight low-light image enhancement network via knowledge distillation. <em>PR</em>, <em>169</em>, 111777. (<a href='https://doi.org/10.1016/j.patcog.2025.111777'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Learning-based methods have recently made remarkable progress for low-light image enhancement (LIE). However, most of these methods generally suffer from unsatisfactory results with overexposure and noise residual. To address these issues, we propose a distillation-based low-light image enhancement network (DLIENet), where we construct a teacher network for LIE to help train DLIENet with lightweight architecture. DLIENet imitates the low-light enhancement capability of the teacher network through knowledge distillation. Concretely, the distillation in the bottleneck module transfers the feature representation capability from the pre-trained teacher network to DLIENet. The distillation in content attention module forces DLIENet to focus on the key contents for reconstructing normal-light images, promoting noise-free results for the low-light images. Additionally, we employ a two-step joint training strategy to improve the performance of the teacher network for LIE. Moreover, we introduce a feature fusion block in DLIENet to normalize the features globally, avoiding non-uniform illumination and overexposure. Extensive experiments and evaluations on both the synthetic and real-world datasets demonstrate the generalization and generalization ability of our method.},
  archive      = {J_PR},
  author       = {Ling Zhang and Zhenyu Li and Liang Cheng and Qing Zhang and Zheng Liu and Xiaolong Zhang and Chunxia Xiao},
  doi          = {10.1016/j.patcog.2025.111777},
  journal      = {Pattern Recognition},
  month        = {1},
  pages        = {111777},
  shortjournal = {Pattern Recognition},
  title        = {DLIENet: A lightweight low-light image enhancement network via knowledge distillation},
  volume       = {169},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). MMoFusion: Multi-modal co-speech motion generation with diffusion model. <em>PR</em>, <em>169</em>, 111774. (<a href='https://doi.org/10.1016/j.patcog.2025.111774'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The body movements accompanying speech aid speakers in expressing their ideas. Co-speech motion generation is one of the important approaches for synthesizing realistic avatars. Due to the intricate correspondence between speech and motion, generating realistic and synchronous motion is a challenging task. In this paper, we propose MMoFusion , a M ulti-modal co-speech Mo tion generation framework based on dif Fusion model to ensure both the authenticity and diversity of generated motion. We propose the progressive fusion to enhance the interaction of inter-modal and intra-modal, efficiently integrating multi-modal information. Specifically, we employ a masked style matrix based on emotion and identity information to control the generation of different motion styles. Temporal modeling of speech and motion is partitioned into style-guided specific feature encoding and shared feature encoding, aiming to learn both inter-modal and intra-modal features. Besides, we propose a geometric loss to enforce the joints’ velocity and acceleration coherence among frames. Our framework generates vivid, diverse, and style-controllable motion of arbitrary length through inputting speech and editing identity and emotion. Extensive experiments demonstrate that our method outperforms current co-speech motion generation methods including upper body and challenging full body. Our code and model will be released at our website .},
  archive      = {J_PR},
  author       = {Sen Wang and Jiangning Zhang and Xin Tan and Zhifeng Xie and Chengjie Wang and Lizhuang Ma},
  doi          = {10.1016/j.patcog.2025.111774},
  journal      = {Pattern Recognition},
  month        = {1},
  pages        = {111774},
  shortjournal = {Pattern Recognition},
  title        = {MMoFusion: Multi-modal co-speech motion generation with diffusion model},
  volume       = {169},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Quality transformer for human parsing. <em>PR</em>, <em>169</em>, 111736. (<a href='https://doi.org/10.1016/j.patcog.2025.111736'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Human parsing aims to partition humans into multiple pixel-wise semantic parts. The natural hierarchical structure of the human body contains numerous implicit paradigms, and the spatial relationships between parts exhibit strong correlations. This structural information is implicitly encoded into the human features, resulting in significant uncertainty in the per-pixel category probability distribution. Building on this insight, we propose a novel Quality Transformer (QualityFormer) network for human parsing. Our framework utilizes the human features as a key and employs pixel-wise probability maps and IoU scores as queries to derive spatial and category quality distributions through a quality-guided cross-attention mechanism. These distributions are integrated into a unified quality guidance map and concatenated with the baseline network to improve the modeling of inter-part relationships. Extensive experiments demonstrate that QualityFormer outperforms state-of-the-art methods across six benchmark datasets, including ATR, LIP, Pascal-Person-Part, CIHP, MHP-v2, and VIP.},
  archive      = {J_PR},
  author       = {Yao Guo and Lu Yang and Pu Cao and Shan Li and Yilin Zhou and Qing Song},
  doi          = {10.1016/j.patcog.2025.111736},
  journal      = {Pattern Recognition},
  month        = {1},
  pages        = {111736},
  shortjournal = {Pattern Recognition},
  title        = {Quality transformer for human parsing},
  volume       = {169},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Instance-wise distributionally robust nonnegative matrix factorization. <em>PR</em>, <em>169</em>, 111732. (<a href='https://doi.org/10.1016/j.patcog.2025.111732'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Nonnegative matrix factorization (NMF) stands as a prevalent algebraic representation technique deployed across diverse domains such as data mining and machine learning. At its core, NMF aims to minimize the distance between the original input and a lower-rank approximation of it. However, when data is noisy or contains outliers, NMF struggles to provide accurate results. Existing robust methods rely on known distribution assumptions, which limit their effectiveness in real-world situations where the noise distribution is unknown. To address this gap, we introduce a new model, called instance-wise distributionally robust NMF (iDRNMF), that can handle a wide range of noise distributions. By leveraging a weighted sum multi-objective method, iDRNMF can handle multiple noise distributions and their combinations. Furthermore, while the entry-wise models assume noise contamination at the individual matrix entries level, the proposed instance-wise model assumes noise contamination at the entire data instances level (columns of the input matrix). This instance-wise model is often more appropriate for data representation tasks, as it addresses the noise affecting entire feature vectors rather than individual features. To train this model, we develop a unified multi-objective optimization framework based on an iterative reweighted algorithm, which maintains computational efficiency similar to single-objective NMFs. This framework provides flexible updating rules, making it suitable for optimizing a wide range of robust and distributionally robust objectives. Extensive experiments on various datasets with distinct noise distributions and mixtures thereof show the superior performance of iDRNMF compared to state-of-the-art models, showcasing its effectiveness in handling diverse noise profiles on real-world problems.},
  archive      = {J_PR},
  author       = {Wafa Barkhoda and Amjad Seyedi and Nicolas Gillis and Fardin Akhlaghian Tab},
  doi          = {10.1016/j.patcog.2025.111732},
  journal      = {Pattern Recognition},
  month        = {1},
  pages        = {111732},
  shortjournal = {Pattern Recognition},
  title        = {Instance-wise distributionally robust nonnegative matrix factorization},
  volume       = {169},
  year         = {2026},
}
</textarea>
</details></li>
<li><details>
<summary>
(2026). Online task-free continual learning via expansible vision transformer. <em>PR</em>, <em>169</em>, 111730. (<a href='https://doi.org/10.1016/j.patcog.2025.111730'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Vision Transformers (ViTs) have lately shown remarkable data representation capabilities leading to state-of-the-art results in several vision and language learning tasks. Given its powerful representation ability, some recent studies have explored the ViT in continual learning by employing the dynamic expansion mechanism. However, these methods rely on the task information and therefore cannot deal with a more realistic scenario, namely the Task-Agnostic Continual Learning (TACL). Unlike these ViT-based continual learning methods, this paper addresses TACL by proposing the Lifelong Expansible Vision Transformer (LEViT) model, which dynamically increases the model’s capacity to deal with changes in the underlying probability distribution of the data representations learnt during continual learning. LEViT is implemented by an ensemble of transformers, each enabled with a multi-head attention mechanism and a linear classifier. We propose a new dynamic expansion mechanism which incrementally increases the capacity of LEViT without requiring task labels, by evaluating the statistical similarity between the joint distribution modeled by all previously learned components and the probabilistic representation of incoming data samples. The proposed expansion mechanism ensures the diversity of learnt knowledge by the components of LEViT. In addition, we introduce the Dynamic Knowledge Fusion (DKF) approach, aiming to explore the ViT feature representation ability for knowledge transfer. Specifically, we view all previously learnt components as an evolved knowledge base which provides prior knowledge for future learning. The proposed LEViT, when compared to the existing ViT-based methods, does not require any task information and can reuse previously learned representations to promote future task learning.},
  archive      = {J_PR},
  author       = {Fei Ye and Adrian G. Bors},
  doi          = {10.1016/j.patcog.2025.111730},
  journal      = {Pattern Recognition},
  month        = {1},
  pages        = {111730},
  shortjournal = {Pattern Recognition},
  title        = {Online task-free continual learning via expansible vision transformer},
  volume       = {169},
  year         = {2026},
}
</textarea>
</details></li>
</ul>

</body>
</html>
